% Encoding: UTF-8

@Article{Nielsen00trackTimeVarCoeffFuncs,
  author    = {Nielsen, H. A. and Nielsen, T. S. and Joensen, A. K. and Madsen, H. and Holst, J.},
  title     = {Tracking time-varying coefficient-functions},
  journal   = {International Journal of Adaptive Control and Signal Processing, Informatics and Mathematical Modelling},
  year      = {2000},
  volume    = {14},
  number    = {8},
  pages     = {813--828},
  abstract  = {A method for adaptive and recursive estimation in a class of non-linear autoregressive models with external input is proposed. The model class considered is conditionally parametric ARX-models (CPARX-models), which is conventional ARX-models in which the parameters are replaced by smooth, but otherwise unknown, functions of a low-dimensional input process. These coeficient functions are estimated adaptively and recursively without specifying a global parametric form, i.e. the method allows for on-line tracking of the coefficient functions. Essentially, in its most simple form, the method is a combination of recursive least squares with exponential forgetting and local polynomial regression. It is argued, that it is appropriate to let the forgetting factor vary with the value of the external signal which is the argument of the coefficient functions. Some of the key properties of the modified method are studied by simulation.},
  comment   = {Sort-of smoothly switching linear regression, but switch is nonlinear adaptation of a subvector of inputs
* this approach is constantly used by DTU/ENFOR
* used in Christiansen07autoTuneWind
* is Shi08curvePredClustGaussProcMix a more sophisticated way to do this?

The idea:
* changing of linear regression coeffs as a nonlinear function of some "control points"
* control points are additional variables that are thought to affect the regression coeffs
 -- In the regression, each input is weighted twice: by the product of the control variable polynomial and by the linear regression coeff.
 -- ctl pts are fixed, allows regression to be conditioned on the control variables in a smoothly interpolated way
 -- regression coefficient are linear and adaptively learned
* nonlinear control variable to coeff function is sort-of kernel based (better done w/ KNN, SVN, etc?)
* adaptive yet kind-of remembers previous states by only adapting coeffs for a given control point when the control variables are near that point; leaves them alone, otherwise
* claimed to be good for time-varying, lagged autoregressive signals
 -- But correlated perf is bad (sec. 4.1, p. 821). Wouldn't delayed versions of autocorrelated signals like wind power cause probs? Whiten or something?

This is NOT piecewise linear regression: PWLR picks regions of regression inputs, x, small enough that the relationship can be assumed to linear; nonlinearity accommodated by adjusting coeffs=c(x). The approach here assumes linearity over the entire range of x, but only within a small neighborhood around independent control point variables, u; nonlinearity handled by c(u) This paper's approach is a subset of PWLR. Is it better? Maybe it has fewer parameters, or fits wind power problems better?

 General comments:
* picking what inputs are regression inputs and which are control inputs seems tricky or requires problem insight
 -- regression inputs need linear relationship w/ predicted output
 -- control variables pick the linear coeffs
 -- In the end, only predicts linear relationships?
 ---- nonlinearity, but still assumes linearity/gaussianity in the region of a control point
 ---- or does weighted sum of control points make it more nonlinear? (this weighting is not clear)
* lots of notational problems in this paper!

Related?  Shi08curvePredClustGaussProcMix},
  crossref  = {Christiansen07autoTuneWind},
  file      = {Nielsen00trackTimeVarCoeffFuncs.pdf:Nielsen00trackTimeVarCoeffFuncs.pdf:PDF},
  groups    = {Read},
  owner     = {scotto},
  timestamp = {2008.12.27},
  url       = {http://www2.imm.dtu.dk/pubdb/p.php?1043},
}

@Article{Bechrakis04windCorrNeighborStat,
  author    = {Bechrakis, D.A. and Sparis, P.D.},
  title     = {Correlation of wind speed between neighboring measuring stations},
  journal   = {Energy Conversion, IEEE Transaction on},
  year      = {2004},
  volume    = {19},
  number    = {2},
  pages     = {400--406},
  month     = jun,
  issn      = {1558-0059},
  abstract  = {A method for establishing wind speed correlation between neighboring measuring stations is presented in this paper. The aim of this study is to develop a model, in which given the wind speed at a particular site to simulate the wind speed at another, nearby site, in order to estimate the wind power of an area. This method takes into account the evolution of the sample cross correlation function (SCCF) of wind speed in time domain and uses an artificial neural network to perform the wind speed simulation. Four separate pairs of wind data measuring stations at two different regions were examined. Tests showed that the higher the SCCF value between two sites, the better simulation achieved. Also, in a pair of stations under investigation the reference station must be the one that contains more information in its wind speed signal, in order to obtain the optimum simulation performance.},
  comment   = {Crappy paper somehow using a neural net to predict wind speed from distant wind measurements 
* calc cross correlations between measurement sites 
* somehow train a neural net at the max correlation lag 
* not clear
 -- what the NN architecture was
 -- how many inputs and outputs there were
 -- what happens if station-to-station lag doesn't match training lag 
* I don't know if results are good (no baseline) 
* related to Barbounis2007locRecurrentNeuralWind?},
  crossref  = {Barbounis2007locRecurrentNeuralWind},
  doi       = {10.1109/TEC.2004.827040},
  file      = {Bechrakis04windCorrNeighborStat.pdf:Bechrakis04windCorrNeighborStat.pdf:PDF},
  groups    = {Read},
  keywords  = { correlation methods, neural nets, power system simulation, wind power, wind power plants artificial neural network, neighboring measuring stations, sample cross correlation function, wind data measuring station, wind power, wind speed correlation},
  owner     = {sotterson},
  timestamp = {2008.12.05},
}

@Article{Fuentes05spaceTimeStructWind,
  author    = {Montserrat Fuentes and Li Chen and Jerry M. Davis and Gary M. Lackmann},
  title     = {Modeling and predicting complex space-time structures and patterns of coastal wind fields},
  journal   = {Environmetrics},
  year      = {2005},
  volume    = {16},
  pages     = {449--464},
  abstract  = {A statistical technique is developed for wind field mapping that can be used to improve either the assimilation of
surface wind observations into a model initial field or the accuracy of post-processing algorithms run on
meteorological model output. The observed wind field at any particular location is treated as a function of the true
(but unknown) wind and measurement error. The wind field from numerical weather prediction models is treated
as a function of a linear and multiplicative bias and a term which represents random deviations with respect to the
true wind process. A Bayesian approach is taken to provide information about the true underlying wind field,
which is modeled as a stochastic process with a non-stationary and non-separable covariance. The method is
applied to forecast wind fields from a widely used mesoscale numerical weather prediction (NWP) model (MM5).
The statistical model tests are carried out for the wind speed over the Chesapeake Bay and the surrounding region
for 21 July 2002. Coastal wind observations that have not been used in the MM5 initial conditions or forecasts are
used in conjunction with the MM5 forecast wind field (valid at the same time that the observations were available)
in a post-processing technique that combined these two sources of information to predict the true wind field. Based
on the mean square error, this procedure provides a substantial correction to the MM5 wind field forecast over the
Chesapeake Bay region. 

key words: Bayesian inference; Fourier transform; geostatistics; meteorological mesoscale model (MM5);
non-separable models; non-stationary models; wind fields},
  comment   = {Data assimilarion: Combines numerical weather prediction w/ wind observations * could use this either for assessment or for picking met tower locations -- result would be a better estimate of wind fields in places where you haven't observed them -- could use this to pick good location for forecasting -- could use this to pick good location for wind farm * "stationarity" means in space and time * they find regions of stationarity w/ k-means clustering! -- features come from NWP (what and how picked is not explained but maybe they are "stability parameters" as in Nielsen07*?) -- AIC/BIC cluster stopping -- from this they build a mixture, w/ weights coming from spatio/temporal distances -- the model looks a tiny bit like a Gaussian Process (use Plagemann08nonStatGaussProcPoint) * models include -- overall trend in wind -- diurnal and half-diurnal variations * MCMC learning * is a lot better than the traditional Kalman 3D VAR assimilation technique * incorporates both additive and multiplicative bias estimation},
  crossref  = {Plagemann08nonStatGaussProcPoint},
  doi       = {10.1002/env.714},
  file      = {:Fuentes05spaceTimeStructWind.pdf:PDF;Fuentes05spaceTimeStructWind.pdf:Fuentes05spaceTimeStructWind.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2008.08.21},
  url       = {http://www4.stat.ncsu.edu/~fuentes/fuentesenv.pdf},
}

@TechReport{Carney06evalDensForecastPIThisto,
  author      = {Michael Carney and P\'{a}draig Cunningham},
  title       = {Evaluating Density Forecasting Models},
  institution = {Trinity College},
  year        = {2006},
  number      = {TCD-CS-2006-21},
  month       = may,
  abstract    = {Density forecasting in regression is gaining popularity as real world applications demand an estimate of the level of uncertainty in predictions. In this paper we describe the two goals of density forecasting1 sharpness and calibration. We review the evaluation methods available to a density forecaster to assess each of these goals and we introduce a new evaluation method that allows modelers to compare and evaluate their models across both of these goals simultaneously and identify the optimal model.},
  comment     = {Model choice for both sharpness and calibration of density forecsts.

* Also explains the continuous ranked probability score (CRPS)
-- equivalent to mean absolute error (MAE) for point forecasts
* Also, the Probability Integral Transform (PIT) histogram is explained
-- see also Diebold98evalDens
* Could be a criterion for probabilistic forecast feature selection, especially since it evalutates sharpness and calibration simultaneously, avoiding multi-critera optimization (I think)},
  crossref    = {Diebold98evalDens},
  file        = {Carney06evalDensForecastPIThisto.pdf:Carney06evalDensForecastPIThisto.pdf:PDF},
  groups      = {Ensemble, PointDerived},
  location    = {Dublin, Dublin 2, Ireland,},
  owner       = {sotterson},
  timestamp   = {2008.09.30},
  url         = {https://www.cs.tcd.ie/publications/tech-reports/tr-index.06.php},
}

@InProceedings{Plagemann08nonStatGaussProcPoint,
  author    = {Plagemann, C. and Kersting, K. and Burgard, W.},
  title     = {Nonstationary {Gauss}ian Process Regression using Point Estimates of Local Smoothness},
  booktitle = {European Conference on Machine Learning (ECML)},
  year      = {2008},
  abstract  = {Gaussian processes using nonstationary covariance functions are a powerful tool for Bayesian regression with input-dependent smoothness. A common approach is to model the local smoothness by a latent process that is integrated over using Markov chain Monte Carlo approaches. In this paper, we demonstrate that an approximation that uses the estimated mean of the local smoothness yields good results and allows one to employ efficient gradient-based optimization techniques for jointly learning the parameters of the latent and the observed processes. Extensive experiments on both synthetic and real-world data, including challenging problems in robotics, show the relevance and feasibility of our approach.},
  comment   = {spatial smoothing of a model around measurements. Useful for assessments or met tower picking? - a better way to do Fuentes05spaceTimeStructWind?},
  crossref  = {Fuentes05spaceTimeStructWind},
  file      = {:Plagemann08nonStatGaussProcPoint.pdf:PDF;Plagemann08nonStatGaussProcPoint.pdf:Plagemann08nonStatGaussProcPoint.pdf:PDF},
  location  = {Antwerp, Belgium},
  owner     = {sotterson},
  timestamp = {2009.01.27},
  url       = {http://www.cognitivesystems.org/publications/plagemann08ecml.pdf},
}

@InProceedings{Kocijan03caseCompNeuralGaussProc,
  author    = {Kocijan, J. and B. Banko and B. Likar and A. Girard and R. Murray-Smith and C. E. Rasmussen},
  title     = {A case based comparison of identification with neural network and {Gauss}ian process models},
  booktitle = {Proceedings of the International Conference on Intelligent Control Systems and Signal Processing (ICONS)},
  year      = {2003},
  abstract  = {In this paper an alternative approach to black-box identification of non-linear dynamic systems is compared with the more established approach of using artificial neural networks. The Gaussian process prior approach is a representative of non-parametric modelling approaches. It was compared on a pH process modelling case study. The purpose of modelling was to use the model for control design. The comparison revealed that even though Gaussian process models can be effectively used for modelling dynamic systems caution has to be axercised when signals are selected.},
  comment   = {GP's easier to train w/ less data than NNARX neural nets and w/ less data; can warn about overtraining
* test was synthesized data on mixing chemical vats
* Neural Net was NNARX (explained how to use Matlab toolbox so could reproduce this)
* GP k-step ahead prediction: just feed back predicted outputs
* GP's kinda worked on full train data, but were worse than NN on validation (test) data
-- however, GP output variance indicated when it was being asked to predict in a region beyond what it had seen during training. (agrees w/ Tancret99CompNeuralGaussProcAlloy) A good diagnostic.
* With truncated training set, GP's worked much better than NN},
  crossref  = {Tancret99CompNeuralGaussProcAlloy},
  file      = {:Kocijan03caseCompNeuralGaussProc.pdf:PDF;Kocijan03caseCompNeuralGaussProc.pdf:Kocijan03caseCompNeuralGaussProc.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2008.10.03},
  url       = {http://www.kyb.mpg.de/publication.html?publ=2314},
}

@Article{Sorjamaa07methLongTermPred,
  author    = {Antti Sorjamaa and Jin Hao and Nima Reyhani and Yongnan Ji and Amaury Lendasse},
  title     = {Methodology for long-term prediction of time series},
  journal   = {Neurocomputing},
  year      = {2007},
  volume    = {70},
  number    = {16-18},
  pages     = {2861--2869},
  abstract  = {In this paper, a global methodology for the long-term prediction of time series is proposed. This methodology combines direct prediction strategy and sophisticated input selection criteria: k-nearest neighbors approximation method (k-NN), mutual information (MI) and nonparametric noise estimation (NNE). A global input selection strategy that combines forward selection, backward elimination (or pruning) and forward?backward selection is introduced. This methodology is used to optimize the three input selection criteria (k-NN, MI and NNE). The methodology is successfully applied to a real life benchmark: the Poland Electricity Load dataset.},
  comment   = {kNN, kNN-MI, and non-parametric noise estimatation feature selection (forward-backward) with direct LS-SVM training

Three feature selection criteria
* kNN
-- add/delete feature that minimizes
-- train a kNN regressor, picking num. neighbors to min. k-fold regression error.
-- leave-one-out seems very expensive.
-- they mention a bootstrap method of picking neighborhood but I can't tell if the use it
-- seems like the approach in Navot05nearNbrFeatSelRegress would be better
* Mutual Information
-- kNN based (Kraskov04EstMutInfKNN)
-- num. neighbors tuned by leave one out cross-validation, again (not sure if it's max mutual information or min reconstruction error)
--- SEEMS WRONG: decreasing k increases MI for constant sample size (Papana08EvalMutInfoDynSys)
--- AND original paper (Kraskov04EstMutInfKNN) said that MI ests were scaled by k/N so if k decreases MI should decrease?
* Nonparametric noise estimator (NNE, also calls it "GT")
-- minimize to minimize estimate of MSE
-- GT: gamma test: estimates noise variance or MSE that can be achieved w/o overfiing
-- not too well explained
* Comparisons of feaure selection criteria
-- performance is about the same but NNE maybe slightly worse
-- kNN is 10X faster than MI, 20X faster than NNE, so kNN is preferred Selection Strategy
* a hybrid forward/backward approach: at each step, can either remove or add a feature
* not compared against other methods so don't know how good it is, really

Regression
* Polish electricity forecasting
* actual regression is LS-SVM
-- supposedly good b/c no local minima, but does this matter?
* 15 past values included in input
-- 2 weeks, long enough to capture main electricty load dynamics

* direct training is superior (by quite a bit) to recursive but 10X as CPU intensive

Related?  Kraskov04EstMutInfKNN},
  crossref  = {Papana08EvalMutInfoDynSys},
  doi       = {10.1016/j.neucom.2006.06.015},
  file      = {Sorjamaa07methLongTermPred.pdf:Sorjamaa07methLongTermPred.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2008.10.13},
  url       = {http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6V10-4NSMMNN-1&_user=10&_rdoc=1&_fmt=&_orig=search&_sort=d&view=c&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=0a2e93a1a57b608cfb7634ea2377c5b2},
}

@TechReport{Ralaivola04nonLinearKernelKalman,
  author      = {Liva Ralaivola and Florence d'Alch?-Buc},
  title       = {Nonlinear Time series filtering, smoothing and learning using the kernel {Kalman} filter},
  institution = {Universite Pierre et Marie Curie},
  year        = {2004},
  type        = {Monograph (Technical Report)},
  number      = {774},
  month       = dec,
  abstract    = {In this paper, we propose a new model, the Kernel Kalman Filter, to perform various nonlinear time series processing. This model is based on the use of Mercer kernel functions in the framework of the Kalman Filter or Linear Dynamical Systems. Thanks to the kernel trick, all the equations involved in our model to perform filtering, smoothing and learning tasks, only require matrix algebra calculus whilst providing the ability to model complex time series. In particular, it is possible to learn dynamics from some nonlinear noisy time series implementing an exact EM procedure. When predictions in the original input space are needed, an efficient and original preimage learning strategy is proposed.},
  comment     = {NIPS paper here: Ralaivola04dynamicModelKernel},
  crossref    = {Ralaivola04dynamicModelKernel},
  file        = {:Ralaivola04nonLinearKernelKalman.pdf:PDF;Ralaivola04nonLinearKernelKalman.pdf:Ralaivola04nonLinearKernelKalman.pdf:PDF},
  location    = {Paris, France.},
  owner       = {sotterson},
  timestamp   = {2008.09.29},
  url         = {http://eprints.pascal-network.org/archive/00000774/},
}

@Article{Bisgaard07BewareAutoCorrRegress,
  author    = {S. Bisgaard and M. Kulahci},
  title     = {Beware of the Effect of Autocorrelation in Regression},
  journal   = {Quality Engineering},
  year      = {2007},
  volume    = {19},
  number    = {2},
  pages     = {143--148},
  month     = apr,
  abstract  = {INTRODUCTION Usually we prefer to use engineering examples to illustrate important statistical issues. However, in this column we deviate from this practice and use a notorious example from economics to show how regression analysis can be seriously misleading if the assumption of independence of the errors is violated. As illustrated in several previous columns (Bisgaard and Kulahci, 2005, 2006a,b) the issue of autocorrelation is of importance when we use regression to estimate input-output relationships from process data sampled over time that likely may be autocorrelated. Coen et al. (1969) originally discussed the data we use. It is of historical and pedagogical significance because it generated quite a controversy when it was first published; but the discussion is still interesting to read. Box and Newbold (1971) subsequently provided incisive analysis of this example. The reason we like to use this example is that it is a poignant illustration of the problem of autocorrelated errors that needs to be known by users of regression analysis and especially by quality engineers. We first provide an analysis similar to that provided by Coen et al. (1969) followed by reanalysis similar to that provided by Box and Newbold (1971), but with more details, accompanying graphics and commentary. Should you like to follow along (as we recommend), the relevant data, edited for easier use, is provided in Appendix.},
  comment   = {How autocorrelation makes lagged variable LSQ regression inappropriate, what to do if not
* this is maybe useful for feature selection of lagged variables e.g. offsite met towers or other observations

Why linear least squares regression fails for autocorrelated signals when doing lagged variable regression.
* 2 unrelated but individually autocorrelated signals can have large cross correlations!
* can occur by chance
* t-test (assess regression significance) are bogus since they assume indep/normal
-- can check if t-test is meaningful by examining error residuals (they s/b uncorrelated)

LSQ regression also fails for non-stationary signals
* relationship not stable
* cross-correlations (for determining lag) are not even defined
* underlying trend dominates the estimated cross-correlation
* detrending w/ LSQ may not help; authors say use prewhitening instead "partial correlation"
* means, in general, removing effect of previous values (p. 146)
* makes a time-independent error model (as assumed by LSQ linear regression) more realistic
* using linear regression to detrend is not safe, but doesn't quite explain why (p. 144)
* often better to use 1\textsuperscript{st} order difference (p. 145)
* but authors recommend prewhitening instead
* after partial correlation, often see that the regression coeffs have no significance

Non-independent error models can detect appropriateness of least squares regression
* proposed error model is itself autoregressive
* error model coeff estimated from data
* coeff tell you if it's either a noisy random walk or a exponentially weighed moving average of past errors
* if it's either then least squares linear regression not appropriate

* note: can also check by checking for the assumed i.i.d. error residual (s/b uncorrelated)

What to do if LSQ regression not appropriate
* add time-dependent (autoregressive) term to model
-- and check to see if i.i.d. error after using this model
-- do a first order difference
----- I'm not clear on when this is better
----- achieves stationarity

They explain prewhitening and relate it to Box-Jenkins models in Bisgaard06studyInOutRelatI},
  crossref  = {Bisgaard06studyInOutRelatI},
  file      = {Bisgaard07BewareAutoCorrRegress.pdf:Bisgaard07BewareAutoCorrRegress.pdf:PDF;Bisgaard07BewareAutoCorrRegress.pdf:Bisgaard07BewareAutoCorrRegress.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.01.20},
  url       = {http://www2.imm.dtu.dk/pubdb/p.php?5420},
}

@InBook{Sollich05robustModMismatchGaussProc,
  chapter   = {Can {Gauss}ian Process Regression Be Made Robust Against Model Mismatch?},
  pages     = {211--228},
  title     = {Deterministic and Statistical Methods in Machine Learning, Lecture Notes in Artificial Intelligence},
  publisher = {Springer},
  year      = {2005},
  author    = {Peter Sollich},
  editor    = {J Winkler, N Lawrence and M Niranjan},
  volume    = {3635},
  abstract  = {Learning curves for Gaussian process (GP) regression can be strongly affected by a mismatch between the `student' model and the `teacher' (true data generation process), exhibiting e.g. multiple overfitting maxima and logarithmically slow learning. I investigate whether GPs can be made robust against such effects by adapting student model hyperparameters to maximize the evidence (data likelihood). An approximation for the average evidence is derived and used to predict the optimal hyperparameter values and the resulting generalization error. For large input space dimension, where the approximation becomes exact, Bayes-optimal performance is obtained at the evidence maximum, but the actual hyperparameters (e.g. the noise level) do not necessarily reflect the properties of the teacher. Also, the theoretically achievable evidence maximum cannot always be reached with the chosen set of hyperparameters, and maximizing the evidence in such cases can actually make generalization performance worse rather than better. In lower-dimensional learning scenarios, the theory predicts--in excellent qualitative and good quantitative accord with simulations--that evidence maximization eliminates logarithmically slow learning and recovers the optimal scaling of the decrease of generalization error with training set size.},
  comment   = {Hyperparm tuning by max likelihood helps w/ model mismatch; mismatch more a problem in higher dims
* mismatch:
-- wrong covariance matrix, wrong noise model
-- low dim: very slow convergence w/ num. training points
-- high dim: overtraining maxima
* hyperparm tuning adapts cov matrix, mainly w/ l, helps
* A successor to:Sollich02modMismatchGaussProc},
  crossref  = {Sollich02modMismatchGaussProc},
  file      = {Sollich05robustModMismatchGaussProc.pdf:Sollich05robustModMismatchGaussProc.pdf:PDF;Sollich05robustModMismatchGaussProc.pdf:Sollich05robustModMismatchGaussProc.pdf:PDF},
  groups    = {Read},
  location  = {Berlin},
  owner     = {scotto},
  timestamp = {2008.10.06},
  url       = {http://www.mth.kcl.ac.uk/~psollich/publications/node27.html},
}

@InProceedings{Sollich02modMismatchGaussProc,
  author    = {Peter Sollich},
  title     = {{Gauss}ian process regression with mismatched models},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2002},
  editor    = {T G Dietterich, S Becker and Z Ghahramani},
  volume    = {14},
  pages     = {519--526},
  publisher = {MIT Press.},
  abstract  = {Learning curves for Gaussian process regression are well understood when the `student' model happens to match the `teacher' (true data generation process). I derive approximations to the learning curves for the more generic case of mismatched models, and find very rich behaviour: For large input space dimensionality, where the results become exact, there are universal (student-independent) plateaux in the learning curve, with transitions in between that can exhibit arbitrarily many over-fitting maxima. In lower dimensions, plateaux also appear, and the asymptotic decay of the learning curve becomes strongly student-dependent. All predictions are confirmed by simulations.},
  comment   = {Model mismatch causes slow convergence in num. pts, and overtraining if high dims.
* asks if hyperparam max lik selection can improve. Answer is "yes" in Sollich05robustModMismatchGaussProc},
  crossref  = {Sollich05robustModMismatchGaussProc},
  file      = {Sollich02modMismatchGaussProc.pdf:Sollich02modMismatchGaussProc.pdf:PDF},
  groups    = {Read},
  location  = {Cambridge, MA},
  owner     = {scotto},
  timestamp = {2008.10.06},
  url       = {http://www.mth.kcl.ac.uk/~psollich/publications/node27.html},
}

@Article{Aghagolzadeh07hierClustMutInf,
  author    = {Aghagolzadeh, M. and Soltanian-Zadeh, H. and Araabi, B. and Aghagolzadeh, A.},
  title     = {A Hierarchical Clustering Based on Mutual Information Maximization},
  journal   = {Image Processing ({ICIP})},
  year      = {2007},
  volume    = {1},
  pages     = {277--280},
  month     = {16 2007-Oct. 19},
  issn      = {1522-4880},
  abstract  = {Mutual information has been used in many clustering algorithms for measuring general dependencies between random data variables, but its difficulties in computing for small size datasets has limited its efficiency for clustering in many applications. A novel clustering method is proposed which estimates mutual information based on information potential computed pair-wise between data points and without any prior assumptions about cluster density function. The proposed algorithm increases the mutual information in each step in an agglomerative hierarchy scheme. We have shown experimentally that maximizing mutual information between data points and their class labels will lead to an efficient clustering. Experiments done on a variety of artificial and real datasets show the superiority of this algorithm, besides its low computational complexity, in comparison to other information based clustering methods and also some ordinary clustering algorithms.},
  comment   = {Claimed to be better than Kraskov MI clustering [Kraskov03hierClustMutInf, Kraskov05HierClustMutInfo] method for small data sets

Related:  Kraskov05HierClustMutInfo},
  crossref  = {Kraskov03hierClustMutInf},
  doi       = {10.1109/ICIP.2007.4378945},
  file      = {Aghagolzadeh07hierClustMutInf.pdf:Aghagolzadeh07hierClustMutInf.pdf:PDF},
  keywords  = {information theory, pattern clusteringagglomerative hierarchy scheme, cluster density function, computational complexity, hierarchical clustering algorithm, information based clustering, mutual information maximization},
  owner     = {sotterson},
  timestamp = {2008.11.14},
}

@Article{Shi08curvePredClustGaussProcMix,
  author    = {J. Q. Shi and B. Wang},
  title     = {Curve prediction and clustering with mixtures of {Gauss}ian process functional regression models},
  journal   = {Statistics and Computing},
  year      = {2008},
  volume    = {18},
  number    = {3},
  pages     = {267--283},
  issn      = {0960-3174},
  abstract  = {Shi et al. (2006) proposed a Gaussian process functional regression (GPFR) model to model functional response curves with a set of functional covariates. Two main problems are addressed by this method: modelling nonlinear and nonparametric regression relationship and modelling covariance structure and mean structure simultaneously. The method gives very good results for curve fitting and prediction but side-steps the problem of heterogeneity. In this paper we present a new method for modelling functional data with ?spatially? indexed data, i.e., the heterogeneity is dependent on factors such as region and individual patient?s information. For data collected from different sources, we assume that the data corresponding to each curve (or batch) follows a Gaussian process func- tional regression model as a lower-level model, and introduce an allocation model for the latent indicator variables as a higher-level model. This higher-level model is dependent on the information related to each batch. This method takes ad- vantage of both GPFR and mixture models and therefore improves the accuracy of predictions. The mixture model has also been used for curve clustering, but focusing on the problem of clustering functional relationships between response curve and covariates. The model is examined on simulated data and real data. Key words: Curve clustering, Curve prediction, Functional data analysis, Gaussian process, Gaussian process functional regression model, allocation model, batch data},
  comment   = {Use this for power curves, regime switchng, etc? 
* EM algorithm avoids need for initial segmentation as in agglomerative clustering? 
* also does n-step ahead forecasting as an example 
* is this a fancier way to do Nielsen00trackTimeVarCoeffFuncs and Christiansen07autoTuneWind?},
  crossref  = {Nielsen00trackTimeVarCoeffFuncs},
  doi       = {10.1007/s11222-008-9055-1},
  file      = {Shi08curvePredClustGaussProcMix.pdf:Shi08curvePredClustGaussProcMix.pdf:PDF},
  location  = {Hingham, MA, USA},
  owner     = {sotterson},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2009.01.05},
}

@InProceedings{Francois06permTestMutInf,
  author     = {D. Fran{\c c}ois and V. Wertz and M. Verleysen},
  title      = {The permutation test for feature selection by mutual information},
  booktitle  = {European Symposium on Artificial Neural Networks (ESANN)},
  year       = {2006},
  pages      = {239--244},
  abstract   = {The estimation of mutual information for feature selection is often subject to inaccuracies due to noise, small sample size, bad choice of parameter for the estimator, etc. The choice of a threshold above which a feature will be considered useful is thus difficult to make. Therefore, the use of the permutation test to assess the reliability of the estimation is proposed. The permutation test allows performing a non-parametric hypothesis test to select the relevant features and to build a Feature Relevance Diagram that visually synthesizes the result of the test.},
  annotation = {Publication where we propose to use the permutation test to gain knownledge about the mutual information estimator to better set thresholds and to better choose the arameters of the estimator.},
  comment    = {Feature selection by mutual information against a permutation null hypothesis; how to get conf interval on p-value significance

Filter technique limitations
* no attempt to limit redundancy
* also misses variables that are individually irrelevant but relevant together
* BUT, I believe the claim is that this filter technique doesn't suffer these probs b/c of the JOINT MI calculation during feed forward selection.

Null hypothesis
* that feature is independent of dependent variable (a useless feature)
* to measure full independence (null hypothesis) need to do all temporal order interpolations Can't do, so end up w/ confidence on p-value!
* significance test is explained in Opdyke03permTestPairMulti

Advantages of this technique
1.) select MI threshold automatically via p-values (although paper isn't too clear about how to do it)
 -- would the null permutation hypothesis help or hurt w/ the problem of feature autocorrelation?
 -- says that bootstrap resampling not appropriate since a repeated point leads to strong MI overestimation w/ the Kraskov04EstMutInfKNN kNN MI estimator (p. 1278)
 --- in fact, this is why they use the sampling-without-replacement significance test in Opdyke03permTestPairMulti
 --- But how does this comare to subsampling bootstrap?
2.) can detect bad MI estimation, making it possible to adjust MI estimator params until get good estimates
* says NN feature selection is theoretically useless (ideally, weights will be zeroed)
* something about self MI of dependent variable but I don't understand how this is used or calculated???
* MI threshold seems to be different for each feature dimension
* Nor demonstration that this actually helps w/ regression, although on simulated data, it does pick right features.
* something like this technique may be used in Dijck06speedFeatSelMutInf
* related idea: do partial correlation sig. thresholding as in Pellet08markovBlankCausLrn},
  crossref   = {Dijck06speedFeatSelMutInf},
  date       = {April 26-28},
  domains    = {Feature selection:Permutation test},
  file       = {:Francois06permTestMutInf.pdf:PDF;Francois06permTestMutInf.pdf:Francois06permTestMutInf.pdf:PDF},
  groups     = {Read},
  location   = {Bruges (Belgium)},
  owner      = {sotterson},
  timestamp  = {2009.01.28},
}

@TechReport{Christiansen07autoTuneWind,
  author      = {L. E. Christiansen and H. A. Nielsen and T. S. Nielsen and H. Madsen},
  title       = {Automatic selection of tuning parameters in wind power prediction},
  institution = {Informatics and Mathematical Modelling, Technical University of Denmark, {DTU}},
  year        = {2007},
  abstract    = {This document presents frameworks for on-line tuning of adaptive estimation procedures. First, introducing unbounded optimization of variable forgetting factor recursive least squares (RLS) using steepest descent and Gauss-Newton methods. Second, adaptive optimization of the bandwidth in conditional parametric {ARX-}models. It was found that the steepest descent approach was more suitable in the examples considered. Further a large increase in the stability when using the proposed transformation of the forgetting factor as compared to the standard approach using a clipper function is observed. This becomes increasingly important when the optimal forgetting factor approaches unity. Adaptive estimation in conditional parametric models are also considered. A similar approach is used to develop a procedure for on-line tuning of the bandwidth independently for each fitting point. Both Gaussian and tri-cube weight functions have been used and for many applications the tri-cube weight function with a lower bound on the bandwidth is preferred. Overall this work documents that automatic tuning of adaptiveness of tuning parameters is indeed feasible and makes it easier to initialize these classes of systems, e.g. when predicting the power production from new wind farms.},
  comment     = {Online learning parameter tuning for linear regression and conditional auto regressive (CARX) prediction approach:
* wind speed/dir -> pow est.
* combine est w/ observed power
* diurnal correction
* two models: linear regression and conditional autoregression

Different tuning parameters were optimized, depending upon prediction algoritm:
I. Linar Regression
* Recusive Least Squares (RLS) has a forgetting factor that they adapt
* Problem: forgetting factor is (0,1) and optimization is unconstrained
* Bad solution: clip after optimization: causes instability
* Proposed solution: optimize an unconstrained variable that's mapped to (0,1) w/ a differentiable function
* tests not done on real data, though II. CARX
* this seems to be like ARMA w/ extra (exogenous) inputs
* This is a switching auto regressor with multiple inputs (ARX, X is for exogenous).

Coefficients are determined by a second set of inputs , and the forum is some kind of polynomial.
* Second set of inputs can be wind speed and/or direction (example of use on slide 21 here: http://130.226.56.153/zephyr/publ/Zephyr-WPPT-talk.pdf) * but coeffs come from some kind of polynomial (need to look up reference to understand)
* here, they pick "bandwidth" for each "fitting point" Need to look up ref, again.
 -- "bandwidth": how sensitive the weights are to a distance from the setting point. Bigger g means more sensitivity.
* tests not done on real data, though
* Is Shi08curvePredClustGaussProcMix a more sophisticated way to do this?

Steepest Descent vs. Gauss Newton
* also experimented w/ these two optimizers
* steepest descent works better than Gauss Newton on RLS
* they assume it will also be best for CARX
CARX and bandwidth described in Nielsen00trackTimeVarCoeffFuncs

 Related: Shi08curvePredClustGaussProcMix},
  crossref    = {Nielsen00trackTimeVarCoeffFuncs},
  file        = {Christiansen07autoTuneWind.pdf:Christiansen07autoTuneWind.pdf:PDF},
  groups      = {Read},
  location    = {Richard Petersens Plads, Building 321, {DK-}2800 Kgs. Lyngby},
  owner       = {scotto},
  series      = {IMM-Technical Report-2007-12},
  timestamp   = {2008.07.04},
  url         = {http://www2.imm.dtu.dk/pubdb/p.php?5316},
}

@Article{Ferreira08resCompWindPred,
  author    = {Ferreira, A.A. and Ludermir, T.B.},
  title     = {Using Reservoir Computing for Forecasting Time Series: {Brazil}ian Case Study},
  journal   = {Hybrid Intelligent Systems (HIS)},
  year      = {2008},
  pages     = {602--607},
  month     = sep,
  abstract  = {This paper presents a Brazilian case study of forecasting a wind speed time series with reservoir computing (RC). RC is a research area, in which an untrained recurrent network of nodes is used for the recognition of temporal patters. In RC only the weights of the connections in a linear output layer are trained. This reduces the complexity of recurrent neural networks (RNN) training to simple linear regression. In this work we used echo state network (ESN) to create the case study and compare the results with Multilayer Perceptron Networks and persistence method. Our case study concerns forecasting the wind speed, which is fundamental information in the operation planning for electrical wind power systems. The results showed that the RC performed significantly better than multilayer perceptron networks or persistence method, even though it presents a significantly simpler and faster, training algorithm.},
  comment   = {Reservoir Computing wind forecast beats bogus MLP and persistence on day-ahead wind forecast
* 1 hour feature sample rate (combinations of wind,temp,dir)
* 24 hour ahead lookahead time, so captures periodicity (much easier than hour ahead)
* best features: either wind or wind + temp.
-- dir didn't seem to help
* the readout network is straight linear regression
* normalizing inputs apparently helped
* found that reservoir pre-adapatation didn't help
-- don't say what kind of adaptation it was
-- somewhat different conclusion than Schrauwen07reservCompOverview
* good 10X cross validation Recurrent neural net spec
* spectral radius fixed (not the best approach, see Schrauwen07reservCompOverview)
* 400 nodes
* nodes were analog, tanh
* had bias layer
* it's an echo net i.e. analog (matches node spec above)

MLP baseline is bogus
* 3 layer MLP but it didn't have a time delay network: no way to caputure dynamics
* num. hidden units fixed for different num. of features? (hard to tell)
* lost very badly to persistence. Should have at least matched
* RPROP training (didn't some other paper on FANN library find that this didn't work so hot?)

Matlab toolbox they used is here: http://www.elis.ugent.be/rct},
  crossref  = {Schrauwen07reservCompOverview},
  doi       = {10.1109/HIS.2008.61},
  file      = {Ferreira08resCompWindPred.pdf:Ferreira08resCompWindPred.pdf:PDF;Ferreira08resCompWindPred.pdf:Ferreira08resCompWindPred.pdf:PDF},
  groups    = {Read, recurNN},
  keywords  = {forecasting theory, pattern recognition, power engineering computing, power system planning, recurrent neural nets, regression analysis, time series, wind power plantsBrazilian case study, echo state network, electrical wind power systems, linear regression, operation planning, recurrent neural networks training, reservoir computing, temporal patterns recognition, wind speed time series forecasting},
  owner     = {sotterson},
  timestamp = {2008.12.15},
}

@Article{Turbelin09wavletWindXcorrNeural,
  author    = {Gr{\'e}gory Turbelin and Pierre Ngaea and Michel Grignon},
  title     = {Wavelet cross-correlation analysis of wind speed series generated by ANN based models},
  journal   = {Renewable Energy},
  year      = {2009},
  volume    = {34},
  number    = {4},
  pages     = {1024--1032},
  month     = apr,
  abstract  = {To obtain, over medium term periods, wind speed time series on a site, located in the southern part of the Paris region (France), where long recording are not available, but where nearby meteorological stations provide large series of data, use was made of ANN based models. The performance of these models have been evaluated by using several commonly used statistics such as average absolute error, root mean square error, normalized mean square error, and correlation coefficient. Such global criteria are good indicators of the ??robustness?? of the models but are unable to provide useful information about their ??effectiveness?? in accurately generating wind speed fluctuations over a wide range of scales. Therefore a complementary wavelet cross coherence analysis has been performed. Wavelet cross coherence, wavelet cross-correlation and spectral wavelet cross-correlation coefficients, have been calculated and displayed as functions of the equivalent Fourier period. These coefficients provide quantitative measures of the scale-dependence of the model performance. In particular the spectral wavelet cross coherence coefficient can be used to have a rapid and efficient identification of the validity range of the models. The results show that the ANN models employed in this study are only effective in computing large-scale fluctuations of large amplitude. To obtain a more representative time series, with much higher resolution, small-scale fluctuations have to be simulated by a superimposed statistical model. By combining ANN and statistical models, both the high and the low-frequency segments of the wind velocity spectra can be simulated, over a range of several hours, at the target site.},
  comment   = {Modelling wind speed correlations at sites around Europe in Wavelet bands; wavelets only good for large scale fluctuation prediction, still need stat. model.

Use mutual information instead?
* use mutual information instead of cross correlation (Brillinger07mutInfoInFreq)
* but could you use MI to reproduce the fluctuations??

I didn't quite understand the use of this paper. But possibly, it could be used for site planning, upscaling or offsite observations?},
  crossref  = {Brillinger07mutInfoInFreq},
  doi       = {10.1016/j.renene.2008.08.016},
  file      = {Turbelin09wavletWindXcorrNeural.pdf:Turbelin09wavletWindXcorrNeural.pdf:PDF;Turbelin09wavletWindXcorrNeural.pdf:Turbelin09wavletWindXcorrNeural.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.12.05},
  url       = {http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6V4S-4TVY5C2-1&_user=10&_rdoc=1&_fmt=&_orig=search&_sort=d&view=c&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=99a028afcb4973999917fa0e8a5ad670},
}

@InProceedings{Schrauwen07reservCompOverview,
  author    = {Benjamin Schrauwen and David Verstraeten and Jan Van Campenhout},
  title     = {An overview of reservoir computing: theory, applications and implementations},
  booktitle = {European Symposium on Artificial Neural Networks (ESANN)},
  year      = {2007},
  pages     = {471--482},
  abstract  = {Training recurrent neural networks is hard. Recently it has
however been discovered that it is possible to just construct a random
recurrent topology, and only train a single linear readout layer. State-of-
the-art performance can easily be achieved with this setup, called Reservoir
Computing. The idea can even be broadened by stating that any high di-
mensional, driven dynamic system, operated in the correct dynamic regime
can be used as a temporal ?kernel? which makes it possible to solve complex
tasks using just linear post-processing techniques.
This tutorial will give an overview of current research on theory, applica-
tion and implementations of Reservoir Computing.},
  comment   = {Tutorial Paper on reservoir computing
* Recursive Neural Networks (RNN) can capture system dynamics but are very hard to train
-- recursive means that don't need to train weights on a lookahead delay line
* Reservoir Computing Nets (RCN) : has an RNN but don't train it (much). Just train a regression from it to the output
-- related to echo state and liquid state machines
-- both are best when the internal RNN is on the edge of instability
-- (did the authors extend this to RCN in Schrauwen09ReservCompPhasePower)
* they say linear regression: regresssion figure shows ARMA, so FANCY linear regression
-- regression inputs: RCN inputs, RNN states, past RCN outputs
-- however Ferreira08resCompWindPred does non-recursvie LR
* Similar to kernel trick (input to high dim feature space) BUT

l.) Reservoirs explicity compute feature space
2.) Reservoirs can handle temporal signals.
* when train, use desired outputs in recursive part; during test, use actual RCN outputs
-- good b/c can then use for forecasting
-- but IS THERE A ONE STEP AHEAD PROBLEM HERE?
* could train multiple forecasters if get multiple time scales right in RNN (p. 471)
* best node type not clear
* how to pick connections not clear but people do random and other things
* people have actually done speech recognition w/ a bucket of water (water is the RNN)
* RCN dynamics depend upon input mags (?) so must normalize them (?)


Applications
* outperforms state of the art on digit recognition
* used to foreacst wind in Ferreira08resCompWindPred and better than MLP
* There's a matlab toolbox!
http://mail.elis.ugent.be/pipermail/reservoir_computing/2007-January/000006.html

(required toolboxes: robust control, statistics, signal processing)


Adaptation: Don't really leave reservoir untrained:
l.) pick from ensemble of random RCN's
2) Prune nodes in random RCN
3.) adjust RCB gain and thresh. Best seems to be Intrinsic Plasticity (better than spectral radius)
* Doesn't work w/o Intrinsic Plasticity tuning, sometimes
* adapt works better than spectral bodies pick.

IDEA: Bayesian Model Averaging (BMA) on an ensemble of reservoirs.},
  crossref  = {Schrauwen09ReservCompPhasePower},
  file      = {Schrauwen07reservCompOverview.pdf:Schrauwen07reservCompOverview.pdf:PDF;Schrauwen07reservCompOverview.pdf:Schrauwen07reservCompOverview.pdf:PDF},
  groups    = {Read, recurNN},
  owner     = {sotterson},
  timestamp = {2008.12.16},
  url       = {http://serv2.ist.psu.edu:8080/viewdoc/summary?doi=10.1.1.95.1215},
}

@Article{Galka06whiteMutInfoSpatioTemp,
  author    = {Galka, Andreas and Ozaki, Tohru and Bayard, Jorge and Yamashita, Okito},
  title     = {Whitening as a Tool for Estimating Mutual Information in Spatiotemporal Data Sets},
  journal   = {Journal of Statistical Physics},
  year      = {2006},
  volume    = {124},
  pages     = {1275--131541},
  abstract  = {We address the issue of inferring the connectivity structure of spatially extended dynamical systems by estimation of mutual information between pairs of sites. The well-known problems resulting from correlations within and between the time series are addressed by explicit temporal and spatial modelling steps which aim at approximately removing all spatial and temporal correlations, i.e. at whitening the data, such that it is replaced by spatiotemporal innovations; this approach provides a link to the maximum-likelihood method and, for appropriately chosen models, removes the problem of estimating probability distributions of unknown, possibly complicated shape. A parsimonious multivariate autoregressive model based on nearest-neighbour interactions is employed. Mutual information can be reinterpreted in the framework of dynamical model comparison (i.e. likelihood ratio testing), since it is shown to be equivalent to the difference of the log-likelihoods of coupled and uncoupled models for a pair of sites, and a parametric estimator of mutual information can be derived. We also discuss, within the framework of model comparison, the relationship between the coefficient of linear correlation and mutual information. The practical application of this methodology is demonstrated for simulated multivariate time series generated by a stochastic coupled-map lattice. The parsimonious modelling approach is compared to general multivariate autoregressive modelling and to Independent Component Analysis (ICA).},
  comment   = {* a way of assessing causality (so know which observations predict future) w/o doing giant lagged mutual information * Can solve MAR (good tuorial: Hytti06multVarAutoRegTut) * compares correlation coeff w/ mutual information * interactions between grid points is instantaneous (not lagged) but says this technique can be extended (p. 1293)},
  crossref  = {Hytti06multVarAutoRegTut},
  doi       = {10.1007/s10955-006-9131-x},
  file      = {Galka06whiteMutInfoSpatioTemp.pdf:Galka06whiteMutInfoSpatioTemp.pdf:PDF;Galka06whiteMutInfoSpatioTemp.pdf:Galka06whiteMutInfoSpatioTemp.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.01.20},
  url       = {http://www.ingentaconnect.com/content/klu/joss/2006/00000124/00000005/00009131},
}

@InCollection{Ralaivola04dynamicModelKernel,
  author    = {Liva Ralaivola and Florence {d'Alch\'{e}-Buc}},
  title     = {Dynamical Modeling with Kernels for Nonlinear Time Series Prediction},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  publisher = {MIT Press},
  year      = {2004},
  editor    = {Sebastian Thrun and Lawrence Saul and Bernhard {Sch\"{o}lkopf}},
  volume    = {16},
  abstract  = {We consider the question of predicting nonlinear time series. Kernel Dynamical Modeling (KDM), a new method based on kernels, is proposed as an extension to linear dynamical models. The kernel trick is used twice: first, to learn the parameters of the model, and second, to compute preimages of the time series predicted in the feature space by means of Support Vector Regression. Our model shows strong connection with the classic Kalman Filter model, with the kernel feature space as hidden state space. Kernel Dynamical Modeling is tested against two benchmark time series and achieves high quality predictions.},
  comment   = {Short paper describing Kalman in kernel space for nonlinear time prediction Longer version here: Ralaivola04nonLinearKernelKalman},
  crossref  = {Ralaivola04nonLinearKernelKalman},
  file      = {:Ralaivola04dynamicModelKernel.pdf:PDF;Ralaivola04dynamicModelKernel.pdf:Ralaivola04dynamicModelKernel.pdf:PDF},
  keywords  = {kernel methods, dynamic Bayesian networks, chaotic time series, preimage determination, Kalman filter, support vector regression},
  location  = {Cambridge, MA},
  owner     = {sotterson},
  timestamp = {2008.09.29},
  url       = {http://books.nips.cc/nips16.html},
}

@InProceedings{Navot05nearNbrFeatSelRegress,
  author    = {Amir Navot and Lavi Shpigelman and Naftali Tishby and Eilon Vaadia},
  title     = {Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2005},
  abstract  = {We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data.},
  comment   = {Regression using KNN error gradients and a mix of hard and soft features selection. gets delays right, could be online
* regression is K nearest neighbor, but the k is a little soft.
* instead of 1/0 neigborhood selection in KNN, include extra neighbors \& for KNN output, compute weighted average
-- weight based on distance and trained weights.
-- actually, does some hard selection but works OK if hard-select more than idea and let weights zero them out.
* delays: feature vectors are binned across 10 time intervals
* does better than other methods
* picks more dispersed delays than other methods (maybe correct ones, since this works better)
* says this could be extended to an online approach
* avoids forward/backward "one at a time" feature adding or deleting

* use this to improve method in Sorjamaa07methLongTermPred

features: 64 channels of neural spike counts at 10 delays (640 dim vector)
outputs: X or Y component of hand velocity
Has Matlab at: http://www.cs.huji.ac.il/labs/learning/code/fsr/},
  crossref  = {Sorjamaa07methLongTermPred},
  file      = {Navot05nearNbrFeatSelRegress.pdf:Navot05nearNbrFeatSelRegress.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2008.10.13},
  url       = {http://books.nips.cc/papers/files/nips18/NIPS2005_0190.pdf},
}

@Article{Kraskov04EstMutInfKNN,
  author    = {Kraskov, Alexander and Stogbauer, Harald and Grassberger, Peter},
  title     = {Estimating mutual information},
  journal   = {Physical Review E},
  year      = {2004},
  volume    = {69},
  number    = {6},
  pages     = {066138},
  month     = jun,
  abstract  = {We present two classes of improved estimators for mutual information M(X,Y) , from samples of random points distributed according to some joint probability density ?(x,y) . In contrast to conventional estimators based on binnings, they are based on entropy estimates from k -nearest neighbor distances. This means that they are data efficient (with k=1 we resolve structures down to the smallest possible scales), adaptive (the resolution is higher where data are more numerous), and have minimal bias. Indeed, the bias of the underlying entropy estimates is mainly due to nonuniformity of the density at the smallest resolved scale, giving typically systematic errors which scale as functions of k?N for N points. Numerically, we find that both families become exact for independent distributions, i.e. the estimator M?(X,Y) vanishes (up to statistical fluctuations) if ?(x,y)=?(x)?(y) . This holds for all tested marginal distributions and for all dimensions of x and y . In addition, we give estimators for redundancies between more than two random variables. We compare our algorithms in detail with existing algorithms. Finally, we demonstrate the usefulness of our estimators for assessing the actual independence of components obtained from independent component analysis (ICA), for improving ICA, and for estimating the reliability of blind source separation.},
  comment   = {Estimates mutual information using K-nearest neighbors.
* doesn't deteriorate as quickly at high dim as kernel density estimation (KDE), may be faster (p. 11)
* Is the method tested in Papana08EvalMutInfoDynSys
* good for small num. of points (says it works for only 30 in the conclusion)
* bigger study in [Kraskov03hierClustMutInf]
* Bessa10infoWindPow uses an info-theory-inspired KNN-like pdf estimation, kind-of related to this approach
* Ma11mutInfoCopulaEnt *may* be equivalent but less computationally intensive
* KNN Entropy and MI estimation algorithm
  - (roughly) finds distance to k neighbors in joint space (X,Y)
  - then compares the number of neighbors found at that distance in the individual spaces, X, Y
  - Possibly better explanations are in
   --- Kraskov04syncIndepMeasMIphd, Section 3.2.2
   --- Victor02binlessInfo
   --- Kraskov08MIChierClustMutInf

-- does MILCA use the bad form (A3) or the one used in this paper?

* Victor02binlessInfo may be related
* Doquire12compMutInfoFeatSel says it's the best MI estimator for MI featsel

* Francois07resampParamFreeFeatSel notes that bootstrap resampling screws up this kNN estimator (p. 1278)
* Kraskov says duplicated points (as in bootstrap) break contin. dist. assumption (here, p. 066138-6).
* Kraskov says rank ordering (like in a copula) improve accuracy, although they say it's not required in general (in this paper, p. 066138-6)
* Sales11parMutInfoEst: Kraskov04 method (this one's) is least biased, most accurate, for network building problem
* simple mod for missing data: Doquire11mutInfMissDat
* is "best of breed" MI estimator: Lizier14JIDTinfoToolkit
, slides or paper?

* is extended to partial or conditional MI here: Frenzel07partMutInfo
 - and this is used for variable selection here: Kugiumtzis13directCplngPMI and here Vlachos10nonUnifStSpcMI

Bias/variance vs. k and npts
* bias generaly gets more negative w/ increasing k, decreasing npts (Fig 4)
* like k=2-4 when not looking for strict zer-valued dependence
* MI est. variance goes up roughly ~ dim/sqrt(npts). Fig. 15
* Frenzel07partMutInfo points out that partial MI also has a bias/variance tradeoff w.r.t. k
* possible proof the unbiasedness of the "Kozachenko and Leonenko " estimator is in :  Singh03knnEntrpyEst, at least  Gao15mutInfoStrongDep seems to think it's in there.
* Looks good up to dim=8. k=1 to minimize bias (I suppose) and then averaged over 100 realizations to get rid of random errors (Fig 14). This is for a normal dist. This was also w/ a special form of MI of a group vs. a scalar

* doesn't work for a mix of discrete and continous variables: Ross14MutInfoCntnAndDscrt
* not too k-sensitive but s/ consider it (Doquire12compMutInfoFeatSel)
 - est. MI w/ avg. result coming from several ks
 - use permutation and sampling tests

* Dimension-dependent bias
 - MI underestimate (neg. bias) w/ few points: Tested on 2D Gaussian. Always underestimates, and underestimate increases w/ decreasing num. samples (increasing 1/N) The exception is if the correlation is actually zero, whre there is little(?) bias, regardless of num. samples. (p. 066138-7,8)
 - several papers also mention dimension bias e.g. Blumentritt11mutInfoAssoc, Francois06permTestMutInf and others
 - Use 2nd method (equation (9)) for high dimensions.  This is the one used in Stogbauer04leastDepMutInfo, which is rrefernced on the MILCA page.
 - dimension dependent bias might be improved by in trucation, as in Blumentritt11mutInfoAssoc
 - Vlachos10nonUnifStSpcMI shows how Kraskov-like PMI/CMI has less dimension dependent bias than Kraskov MI
 - more on dimension-dependent bias here: Kraskov08MIChierClustMutInf* IDEA: use Schnitzer12Localandglobal hub processing to reduce dim-dependent bias?

* MI estimate for streaming data, varying lag and window length: Keller15EstMutualInfStrm

* estimation of strongly dependent variables requires a lot of data.  See: Gao15mutInfoStrongDep
 - this is because of the assumption of uniformity, I think, in eq (18)

Implementations
* pure Matlab: here: https://www.mathworks.com/matlabcentral/fileexchange/50818-kraskov-mutual-information-estimator
* pure Matlab: Information Theoretical Estimators (ITE) Toolbox (maybe does Kraskov): Szabo13infoTheoEstTlbx
* Java/Matlab/etc  implementation: Lizier14JIDTinfoToolkit
* Python: Gao15mutInfoStrongDep (improved algorithm)
* is implemented in Matlab/C in Sorjamaa05mutInfKNNpred
* also a subset of MI funcs also parallelly implemented in R in the parmigene package: http://www.inside-r.org/packages/cran/parmigene/docs

Related:  Sorjamaa05mutInfKNNpred},
  crossref  = {Papana08EvalMutInfoDynSys},
  doi       = {10.1103/PhysRevE.69.066138},
  file      = {Kraskov04EstMutInfKNN.pdf:Kraskov04EstMutInfKNN.pdf:PDF},
  numpages  = {16},
  owner     = {sotterson},
  publisher = {American Physical Society},
  timestamp = {2008.10.10},
}

@Article{Diebold98evalDens,
  author    = {Francis X. Diebold and Todd A. Gunther and Anthony S. Tay},
  title     = {Evaluating Density Forecasts, with Applications to Financial Risk Management},
  journal   = {International Economic Review},
  year      = {1998},
  volume    = {39},
  pages     = {863--883},
  abstract  = {We propose methods for evaluating density forecasts. We focus primarily on methods that are applicable regardless of the particular user?s loss function. We illustrate the methods with a detailed simulation example, and then we present an application to density forecasting of daily stock market returns. We discuss extensions for improving suboptimal density forecasts, multi-step-ahead density forecast evaluation, multivariate density forecast evaluation, monitoring for structural change and its relationship to density forecasting, and density forecast evaluation with known loss function.},
  comment   = {Theoretical treatment of the probability inetgral transform (PIT)

(PDF I annoated seems to match the pdf with the official publication marks on it. Must have been a pre-published version?)

* Carney06evalDensForecastPIThisto kind of explains it too.
* but this paper explains why the IT should be uniform for a correctly estimated distribution},
  crossref  = {Carney06evalDensForecastPIThisto},
  file      = {Diebold98evalDens.pdf:Diebold98evalDens.pdf:PDF;Diebold98evalDens.pdf:Diebold98evalDens.pdf:PDF},
  groups    = {Test, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2008.10.01},
  url       = {http://www.ssc.upenn.edu/~fdiebold/papers/paper16/paper16.pdf},
}

@Article{Stogbauer04leastDepMutInfo,
  author    = {Stogbauer, Harald and Kraskov, Alexander and Astakhov, Sergey A. and Grassberger, Peter},
  title     = {Least-dependent-component analysis based on mutual information},
  journal   = {Physical Review E},
  year      = {2004},
  volume    = {70},
  number    = {6},
  pages     = {066123},
  month     = dec,
  abstract  = {We propose to use precise estimators of mutual information (MI) to find least dependent components in a linearly mixed signal. On the one hand this seems to lead to better blind source separation than with any other presently available algorithm. On the other hand it has the advantage, compared to other implementations of `independent' component analysis (ICA) some of which are based on crude approximations for MI, that the numerical values of the MI can be used for: (i) estimating residual dependencies between the output components; (ii) estimating the reliability of the output, by comparing the pairwise MIs with those of re-mixed components; (iii) clustering the output according to the residual interdependencies. For the MI estimator we use a recently proposed k-nearest neighbor based algorithm. For time sequences we combine this with delay embedding, in order to take into account non-trivial time correlations. After several tests with artificial data, we apply the resulting MILCA (Mutual Information based Least dependent Component Analysis) algorithm to a real-world dataset, the ECG of a pregnant woman. The software implementation of the MILCA algorithm is freely available at this http URL},
  comment   = {ICA using kNN MI estimator. Can group multi-dim signals, useful mutual information facts kNN Mutual Information (MI) estimation * benefits 1.) any dim signal 2.) more robust * selection of k: -- k selection a bias vs. variance tradeoff -- if have more data, use smaller k -- THE OPPOSITE: of Papana08EvalMutInfoDynSys, where larger k worked worse for less data -- for fetal ECG, they used k=30 for 5s of 8 channel 50Hz data (2500 samples) * is the MILCA, for which I think matlab is distributed http://www.klab.caltech.edu/~kraskov/MILCA/ * MILCA code probably uses Kraskov04 method having least high dim problems (p. 3) Clustering used in ICA * for case when sources are actually dependent -- this can detect them (residual information after linear ICA is detected by mtualin formation clustering algorithm) -- example: mother/fetus hearts s/ have 3 indep sources each * don't use MI directly so can allow multi-Dim variables w/ different dims (equ. 25) -- use the dim-confounder trick in Francois07resampParamFreeFeatSel instead of explicit dims? Somehow? -- This doesn't seemto mean that multi-dim MI is wrong; this just makes clustering do something sensible * clustering is hierarchical and disjoint. * MI grouping property (eq 9) means that can cluster w/ fully joint MI, not just incremental pairwise (p. 11) Clustering w/ delays * allows unraveling and detecting more complex time relationship * considers set of lags from a single input to be a single RV * use multi-D MI estimator to determine MI -- same MI method as in: Kraskov04EstMutInfKNN, apparently (but does it use Kraskov04EstMutInfKNN formula A3 or the other one used in the Kraskov04EstMutInfKNN tests?) * I COULD USE THIS for turbine clustering w/o looking for max MI delays -- dependencies would be more general, wouldn't depend so much on speed of front. -- then, models trained on the outputs would be more general too (if they learn w/ all delays) -- would need to do something about noise at input when here isn't a relationship, though. Interesting MI facts * minimum MI determined by correlation coeff (eq 7, and is there a muli-D version too?) * MI increases w/ variable dimensionality * pairwise independence doesn't imply global independence -- but can min global MI pairwise if do factoring -- decomposition of MI into pairwise MI's},
  crossref  = {Papana08EvalMutInfoDynSys},
  doi       = {10.1103/PhysRevE.70.066123},
  file      = {Stogbauer04leastDepMutInfo.pdf:Stogbauer04leastDepMutInfo.pdf:PDF;Stogbauer04leastDepMutInfo.pdf:Stogbauer04leastDepMutInfo.pdf:PDF},
  groups    = {Read},
  numpages  = {17},
  owner     = {sotterson},
  publisher = {American Physical Society},
  timestamp = {2008.10.16},
  url       = {http://arxiv.org/abs/physics/0405044},
}

@Article{Quiroga02eegSynch,
  author               = {Quiroga, Quian R. and Kraskov, A. and Kreuz, T. and Grassberger, P.},
  title                = {Performance of different synchronization measures in real data: a case study on electroencephalographic signals.},
  journal              = {Physical Review E},
  year                 = {2002},
  volume               = {65},
  number               = {4 Pt 1},
  month                = apr,
  abstract             = {We study the synchronization between left and right hemisphere rat electroencephalographic (EEG) channels by using various synchronization measures, namely nonlinear interdependences, phase synchronizations, mutual information, cross correlation, and the coherence function. In passing we show a close relation between two recently proposed phase synchronization measures and we extend the definition of one of them. In three typical examples we observe that except mutual information, all these measures give a useful quantification that is hard to be guessed beforehand from the raw data. Despite their differences, results are qualitatively the same. Therefore, we claim that the applied measures are valuable for the study of synchronization in real data. Moreover, in the particular case of EEG signals their use as complementary variables could be of clinical relevance.},
  citeulike-article-id = {944871},
  comment              = {Linear/non-linear synchronization detection between brain halves
* EEG's of two brain halves: idea is to detect if synchronization breaks down lesions are added

Techniques:
1.) linear cross correlation
2.) spectral coherence in 9Hz FFT bin (where syncrhony believed to exist)
3.) A couple Hilbert transform phase locking detectors (not relevant for broadband)
4.) S: a kind of swapped k-nearest neighbors.
 -- Double sided, which is odd
-- more sensitive than mutual information, since it's linear instead of quadratic? Only for linear corr?
-- + or - sign can indicate which variable is forcing the other (relate to wind direction??)
5.) H: logged version of S (more sensitive)
6.) N: normalized H (more sensitve, but

Conclusions:
* nonlinear is more sensitive than linear (including Hilbert xform)
* problems with mutual information estimation
-- but Duckrow03eegSynchComment shows how to do it
 -- authors agree in Quiroga05replyCommentEEGsynch
* is a dynamical system thing
 * embedding dimension is the number of lags considered; embedding lag is the sample rate
 http://books.google.com/books?id=_qX5oAK6_54C&pg=PT108&lpg=PT108&dq=embedding+dimension+time+lag&source=web&ots=uqbjDxm9LF&sig=Q5UtGXaAyWoGrOtxPnu_Gc2psvc&hl=en&sa=X&oi=book_result&resnum=9&ct=result\#PPT109,M1

Related: Duckrow03eegSynchComment},
  crossref             = {Quiroga05replyCommentEEGsynch},
  doi                  = {10.1103/PhysRevE.65.041903},
  file                 = {Quiroga02eegSynch.pdf:Quiroga02eegSynch.pdf:PDF},
  keywords             = {absencegenetics, acidtoxicity, animal, animals, brain, comparative, cortical, data, disease, dynamics, effectsinjuries, effectsmethodsstatistics, epilepsy, factors, govt, humans, ibotenic, inbred, inducedphysiopathology, injurieschemically, male, models, non-phs, nonlinear, nucleidrug, numerical, rats, research, strains, study, support, synchronizationdrug, thalamic, time, us},
  posted-at            = {2006-11-15 15:56:42},
  timestamp            = {2008.09.09},
  url                  = {http://www.vis.caltech.edu/~rodri/papers/performance.pdf},
}

@InProceedings{Alonso08windCtlManage,
  author    = {Javier Revuelta Alonso},
  title     = {Wind Energy: Control and operation management issues},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  year      = {2008},
  comment   = {Spanish wind control, maybe good for optimization (related to Alonso08windLargeScaleIntegrat)

* almost as much wind as hydro (17.3 pct vs. 19.3 pct)
* solar PV is 2X bigger than thermal (1GW vs. 0.5GW, interesting b/c the do thermal)
* totally confusing graphs of peak/max production/demand
* Wind DROPS in the morning at the same time as demand is RISING! Power cutoffs due to high winds
* Cut off production for > 25 m/s ("tripping")
* Actually, Cam says that this might be partly about the lack of undervoltage protection in early Spanish turbines. They used to just cut off if there was low voltage
* Spanish regs require wind to adjust production withing 15 minutes

* In 2007 -- caused 1.5GW something -- 94 trips ==> 2573 hours of lost production
-- do some kind of "N-1 contingency analysis" before doing something about trips
---- N-1 contingency analysis: Cam says this is a standard thing in power: something like losing biggest load and most critical transmission line. Or something like that.

Forecast errors
* they're getting about 20 pct mean relative errors w/ HIRLAM model
* thermal powerplants require 5 hours startup: 15 pct chance chance of 570MW windpower shortfall within 5 hours b/c of prediction errors Power balance feasibility (goals?)
* cover peak hour w/ reserve (demand reserve, I think he means)
* try to avoid curtailments during off peak hours
* slope mismatch: cover case of fastest wind drop AND fastest demand rise Wind Power factor is also a thing to optimize for (in Spain, anyway)
* get a preimimum for good PF during times of day
* Cam says early Spanish wind farms had poor power factor control, but new turbines don't have this problem Congestion management uses optimal probabilisic reschedulers
* startups, shutdowns, pumps
* also something about optimal voltage controls},
  crossref  = {Alonso08windLargeScaleIntegrat},
  file      = {Alonso08windCtlManage.pdf:Alonso08windCtlManage.pdf:PDF;Alonso08windCtlManage.pdf:Alonso08windCtlManage.pdf:PDF},
  groups    = {IWFTM08, Read},
  owner     = {sotterson},
  timestamp = {2008.09.19},
  url       = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
}

@InProceedings{Alonso08windLargeScaleIntegrat,
  author    = {Javier Revuelta Alonso},
  title     = {Integration of largescale wind in the grid ??? The Spanish Experience},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  year      = {2008},
  comment   = {Transmission line control in Spain. Probably good for optimization (related to Alonso08windCtlManage)
* more numerical detail on power factor bonus, tarrifs, etc.
* conclusion: wind energy saves fuel but not investment in capacity},
  crossref  = {Alonso08windCtlManage},
  file      = {Alonso08windLargeScaleIntegrat.pdf:Alonso08windLargeScaleIntegrat.pdf:PDF;Alonso08windLargeScaleIntegrat.pdf:Alonso08windLargeScaleIntegrat.pdf:PDF},
  groups    = {IWFTM08, Read},
  owner     = {sotterson},
  timestamp = {2008.09.19},
  url       = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
}

@Article{Barbounis2007locRecurrentNeuralWind,
  author    = {T. G. Barbounis and J. B. Theocharis},
  title     = {Locally recurrent neural networks for wind speed prediction using spatial correlation},
  journal   = {Information Sciences},
  year      = {2007},
  volume    = {177},
  number    = {24},
  pages     = {5775--5797},
  issn      = {0020-0255},
  comment   = {Related to Bechrakis04windCorrNeighborStat?},
  crossref  = {Bechrakis04windCorrNeighborStat},
  doi       = {10.1016/j.ins.2007.05.024},
  file      = {Barbounis2007locRecurrentNeuralWind.pdf:Barbounis2007locRecurrentNeuralWind.pdf:PDF;Barbounis2007locRecurrentNeuralWind.pdf:Barbounis2007locRecurrentNeuralWind.pdf:PDF},
  location  = {New York, NY, USA},
  owner     = {sotterson},
  publisher = {Elsevier Science Inc.},
  timestamp = {2008.07.03},
  url       = {http://portal.acm.org/citation.cfm?id=1293634&jmp=cit&coll=GUIDE&dl=GUIDE#},
}

@Article{Bisgaard06studyInOutRelatI,
  author    = {S. Bisgaard and M. Kulahci},
  title     = {Studying Input Output Relationships I},
  journal   = {Quality Engineering},
  year      = {2006},
  volume    = {18},
  number    = {2},
  pages     = {273--281},
  comment   = {Whitening eliminates autocorrelation artifacts in linear regression When estimating an input/output relationship, a correlated input causes problems * violates i.i.d. error assumption of least squared input/output linear regression * when estimating I/O delay, causes smeared lags in I/O cross correlation -- anti-causal! correlation smearing makes output appear be be correlated w/ inputs that haven't happened yet -- can see why: xcorr is just autocorr -- can see that symmetric autocorr will smear to anticausality Recommend pre-whitening * build model of input by itself (AR, MA, ARMA) * subtract off the prediction residual from the input * use INPUT model to predict output and subtract residual * I DON'T QUITE UNDERSTAND THE RATIONALE Determining signifcant I/O lags after pre-whitening * argues that using fewer lags in model will avoid ill-conditioning and bias * pick statistically significant lags from xcorr of whitened input and whitened output (explains how) * can also roughly estimate an FIR input/output relationship w/ whitened correlations * also recommend autocorrelated error modeling (see Bisgaard07BewareAutoCorrRegress for an example)},
  crossref  = {Bisgaard07BewareAutoCorrRegress},
  file      = {Bisgaard06studyInOutRelatI.pdf:Bisgaard06studyInOutRelatI.pdf:PDF;Bisgaard06studyInOutRelatI.pdf:Bisgaard06studyInOutRelatI.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.01.20},
  url       = {http://www2.imm.dtu.dk/pubdb/p.php?4785},
}

@Article{Brillinger07mutInfoInFreq,
  author    = {David R. Brillinger and Apratim Guha},
  title     = {Mutual information in the frequency domain},
  journal   = {Journal of Statistical Planning and Inference},
  year      = {2007},
  volume    = {137},
  number    = {3},
  pages     = {1076--1084},
  comment   = {MI in FFT bins. Is like spectral coherence (for comparing different waveforms) but could have advantages?
* samples are the spectrum in frequency bins
* uses histogram technique, w/ 10 bins, I think
* appears to be using just the power spectrum, no phase (wouldn't this lose too much info? wouldn't the power spectrum of indentially shaped pink noise have the high MI even if the signals are independent?)

* use to improve Turbelin09wavletWindXcorrNeural

Related: Lan07eegChanSelMutInfoICA},
  crossref  = {Turbelin09wavletWindXcorrNeural},
  doi       = {10.1016/j.jspi.2006.06.026},
  file      = {Brillinger07mutInfoInFreq.pdf:Brillinger07mutInfoInFreq.pdf:PDF},
  owner     = {scotto},
  timestamp = {2008.09.09},
  url       = {http://www.sciencedirect.com.offcampus.lib.washington.edu/science?_ob=ArticleURL&_udi=B6V0M-4KKNKY2-1&_user=582538&_coverDate=03%2F01%2F2007&_rdoc=34&_fmt=high&_orig=browse&_srch=doc-info(%23toc%235650%232007%23998629996%23636584%23FLA%23display%23Volume)&_cdi=5650&_sort=d&_docanchor=&_ct=37&_acct=C000029718&_version=1&_urlVersion=0&_userid=582538&md5=848f7694baaacc6c4757b2502e502917},
}

@Article{Duckrow03eegSynchComment,
  author    = {Duckrow, R. B. and Albano, A. M.},
  title     = {Comment on: Performance of different synchronization measures in real data: A case study on electroencephalographic signals},
  journal   = {Physical Review E},
  year      = {2003},
  volume    = {67},
  number    = {6},
  pages     = {063901},
  month     = jun,
  comment   = {Mutual Info estimation for time delay, high dim signals * mainly interesting for the adaptive partitioning for histogram bin size * better than fixed-bin histo technique -- better answer for fewer num. pts -- important b/c ECoG signal are nonstationary: can't have too long of a time window * bit-wise embedding * avoids erroneous (?) increase in mutual information w/ increasing dim - Quiroga05replyCommentEEGsynch says increase is theoretically expected, not really avoided? Code: * original: ftp://ftp.ee.pdx.edu/pub/users/andy/code/pollypack.shar * supposedly improved: http://www.physics.emory.edu/~weeks/research/tseries3.html

Related:  Fraser86indepCoordMutInfo},
  crossref  = {Quiroga05replyCommentEEGsynch},
  doi       = {10.1103/PhysRevE.67.063901},
  file      = {Duckrow03eegSynchComment.pdf:Duckrow03eegSynchComment.pdf:PDF},
  groups    = {Read},
  numpages  = {3},
  owner     = {sotterson},
  publisher = {American Physical Society},
  timestamp = {2008.09.05},
  url       = {http://prola.aps.org.offcampus.lib.washington.edu/abstract/PRE/v67/i6/e063901},
}

@Article{Fraser86indepCoordMutInfo,
  author    = {Andrew M. Fraser and Harry L. Swinney},
  title     = {Independent coordinates for strange attractors from mutual information},
  journal   = {Physical Review A},
  year      = {1986},
  volume    = {33},
  number    = {2},
  pages     = {1134--1140},
  comment   = {An adaptive historgram bin sized altorithm for computing mutual information technique used in: Duckrow03eegSynchComment Code: * original: ftp://ftp.ee.pdx.edu/pub/users/andy/code/pollypack.shar * supposedly improved: http://www.physics.emory.edu/~weeks/research/tseries3.html},
  crossref  = {Duckrow03eegSynchComment},
  file      = {Fraser86indepCoordMutInfo.pdf:Fraser86indepCoordMutInfo.pdf:PDF;Fraser86indepCoordMutInfo.pdf:Fraser86indepCoordMutInfo.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.10.10},
  url       = {http://prola.aps.org.offcampus.lib.washington.edu/pdf/PRA/v33/i2/p1134_1},
}

@InProceedings{Grimit08windStateOfArt,
  author    = {Eric Grimit and Cameron Potter},
  title     = {State-of-the-Art Wind Energy Forecasting (and the Future of Forecasting)},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  year      = {2008},
  comment   = {Really, a 3Tier sales talk * best forecast error measure vs. use * overscheduling risk analysis * discussion of ramp forecasting -- includes probabilistic power conversion of some kind (like in Collier08windForecastPowerModel) -- "ramp" defined -- etc.},
  crossref  = {Collier08windForecastPowerModel},
  file      = {Grimit08windStateOfArt.pdf:Grimit08windStateOfArt.pdf:PDF;Grimit08windStateOfArt.pdf:Grimit08windStateOfArt.pdf:PDF},
  groups    = {IWFTM08, Read},
  owner     = {sotterson},
  timestamp = {2008.09.19},
  url       = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
}

@InProceedings{Herrera06effectInputSelFuncApprox,
  author    = {Luis Javier Herrera and H{\'e}ctor Pomares and Ignacio Rojas and Michel Verleysen and Alberto Guill{\'e}n},
  title     = {Effective Input Variable Selection for Function Approximation},
  booktitle = {International Conference on Artificial Neural Networks (ICANN)},
  year      = {2006},
  pages     = {41--50},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comment   = {Select features by estimating and iteratively removing Markov blankets. * "Markov blanket" estimated here: the p variables w/ the hghest mutual information w/ a given variable * there's an undefined variable (M) in eq. 14 so I'm not sure how the loss function is calculated, and can't understnad the iteration * used kNN mutual information to determine MI with output variable. (how to pick k is undefined) * beats other methods, but I'm not sure they're good methods * some (all?) is implemented in Matlab Hmmm... * seems like the Markov Blanket (MB) s/b w.r.t. the OUTPUT variable * use MI clustering (heirarchical or spectral) to determine MB's * I don't quite understand the rationale of removing a variable * I don't like the fixed 'p'. -- Could use BIC or MDL in hierarchial or spectral to automatically determine it? -- use permutation method in Francois07resampParamFreeFeatSel},
  crossref  = {Sorjamaa05mutInfKNNpred},
  doi       = {10.1007/11840817},
  file      = {Herrera06effectInputSelFuncApprox.pdf:Herrera06effectInputSelFuncApprox.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2008.10.13},
  url       = {http://www.springerlink.com/content/84m5l430725l1664/},
}

@Article{Hytti06multVarAutoRegTut,
  author               = {Hytti and Heli and Takalo and Reijo and Ihalainen and Heimo},
  title                = {Tutorial on Multivariate Autoregressive Modelling},
  journal              = {The Journal of Clinical Monitoring and Computing},
  year                 = {2006},
  volume               = {20},
  number               = {2},
  pages                = {101--108},
  month                = apr,
  issn                 = {1387-1307},
  citeulike-article-id = {744710},
  comment              = {Use for feature selection, offsite obs? * problem for modeling is doing multi-step-ahead (this paper discusses one-step ahead) * matlab available. There's a paper comparing the methods: http://portal.acm.org/citation.cfm?id=1161586 * MAR can be done w/ mutual information/whitening. See Galka06whiteMutInfoSpatioTemp},
  crossref             = {Galka06whiteMutInfoSpatioTemp},
  doi                  = {10.1007/s10877-006-9013-4},
  file                 = {Hytti06multVarAutoRegTut.pdf:Hytti06multVarAutoRegTut.pdf:PDF;Hytti06multVarAutoRegTut.pdf:Hytti06multVarAutoRegTut.pdf:PDF},
  keywords             = {multivariate\_analysis, regression\_analysis, signal\_analysis},
  owner                = {sotterson},
  posted-at            = {2008-01-20 13:59:11},
  publisher            = {Springer},
  timestamp            = {2009.01.21},
}

@Article{Kraskov05HierClustMutInfo,
  author    = {A. Kraskov and H. Stogbauer and R. G. Andrzejak and P. Grassberger},
  title     = {Hierarchical clustering using mutual information},
  journal   = {Europhysics Letters (EPL)},
  year      = {2005},
  volume    = {70},
  number    = {2},
  pages     = {278--284},
  month     = apr,
  comment   = {Theoretical treament of using kNN MI estimate to do hierarchical clustering
* some good MI facts
 -- log correction for relating binned info to differential
 -- Shanon vs. Algorithmic info:
 ---- Shannon: random variables
 ---- Algorithmic: non-random symbols, exact.
 -- differential information (the non-binned kind) not invariant under transforms
* improved in [Aghagolzadeh07hierClustMutInf] for short data sets (claim)
* bigger study in [Kraskov03hierClustMutInf]
* more robustness study in Papana08EvalMutInfoDynSys

Related: Kraskov08MIChierClustMutInf.Papana08EvalMutInfoDynSys},
  crossref  = {Aghagolzadeh07hierClustMutInf},
  doi       = {10.1209/epl/i2004-10483-y},
  file      = {Kraskov05HierClustMutInfo.pdf:Kraskov05HierClustMutInfo.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.10.17},
}

@Article{Pellet08markovBlankCausLrn,
  author    = {Pellet,, Jean-Philippe and Elisseeff,, Andr\'{e}},
  title     = {Using {Markov} Blankets for Causal Structure Learning},
  journal   = {Journal of Machine Learning Research},
  year      = {2008},
  volume    = {9},
  pages     = {1295--1342},
  issn      = {1533-7928},
  comment   = {* uses (and defines) partial correlation to determine causal relationships * also shows how to partial correlation significant coeff tests -- could use to detect "significant lags" -- something like MI thresholding done in Francois06permTestMutInf -- do something like max power beamforming w/ sig tests on filter coeffs? ---- i.e. convoltive cannonical correlation analysis (which would find max correlation of inputs w/ output to be predicted)},
  crossref  = {Francois06permTestMutInf},
  file      = {:Pellet08markovBlankCausLrn.pdf:PDF;Pellet08markovBlankCausLrn.pdf:Pellet08markovBlankCausLrn.pdf:PDF},
  location  = {Cambridge, MA, USA},
  owner     = {sotterson},
  publisher = {MIT Press},
  timestamp = {2009.01.30},
}

@Article{Qi01modelSelNNforecast,
  author    = {Qi, Min and Zhang, Guoqiang Peter},
  title     = {An investigation of model selection criteria for neural network time series forecasting},
  journal   = {European Journal of Operational Research},
  year      = {2001},
  volume    = {132},
  number    = {3},
  pages     = {666--680},
  month     = aug,
  comment   = {Selecting forecasting inputs w/ AIC, BIC, CAC(?) * beaten by techniqe in Huang06inputSelStockForecast},
  crossref  = {Huang06inputSelStockForecast},
  file      = {Qi01modelSelNNforecast.pdf:Qi01modelSelNNforecast.pdf:PDF;Qi01modelSelNNforecast.pdf:Qi01modelSelNNforecast.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.01.29},
  url       = {http://ideas.repec.org/a/eee/ejores/v132y2001i3p666-680.html},
}

@Article{Quiroga05replyCommentEEGsynch,
  author    = {R. Quian Quiroga and A. Kraskov and P. Grassberger},
  title     = {Reply to "Comment on `Performance of different synchronization measures in real data: A case study on electroencephalographic signals' "},
  year      = {2005},
  volume    = {72},
  doi       = {10.1103/PhysRevE.72.063902},
  url       = {http://scitation.aip.org.offcampus.lib.washington.edu/getabs/servlet/GetabsServlet?prog=normal&id=PLEEE8000072000006063902000001&idtype=cvips&gifs=yes},
  crossref  = {Duckrow03eegSynchComment},
  file      = {Quiroga05replyCommentEEGsynch.pdf:Quiroga05replyCommentEEGsynch.pdf:PDF},
  journal   = {Physical Review Letters},
  owner     = {sotterson},
  timestamp = {2008.09.05},
}

@InCollection{Schrauwen09ReservCompPhasePower,
  author    = {Benjamin Schrauwen and Lars Buesing and Robert Legenstein},
  title     = {On Computational Power and the Order-Chaos Phase Transition in Reservoir Computing},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2009},
  editor    = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
  volume    = {21},
  comment   = {When does reservoir computing work? Also liquid state and echo state networks.
* related to echo state and liquid state stabilities discussed in Schrauwen07reservCompOverview?},
  crossref  = {Schrauwen07reservCompOverview},
  file      = {Schrauwen09ReservCompPhasePower.pdf:Schrauwen09ReservCompPhasePower.pdf:PDF;Schrauwen09ReservCompPhasePower.pdf:Schrauwen09ReservCompPhasePower.pdf:PDF},
  groups    = {recurNN},
  owner     = {sotterson},
  timestamp = {2008.12.16},
  url       = {http://snn.elis.ugent.be/node/230},
}

@Article{Tancret99CompNeuralGaussProcAlloy,
  author    = {F. Tancret and H. K. D. H. Bhadeshia and D. J. C. MacKay},
  title     = {Comparison of artificial neural networks with {Gauss}ian processes to model the yield strength of nickel-base superalloys},
  journal   = {ISIJ International},
  year      = {1999},
  volume    = {39},
  pages     = {1020--1026},
  comment   = {GP's and Neural Nets perform aboutt the same, but GP's are easier to train; NN's more efficient, sometimes better at extrapolation (agrees w/ Kocijan03caseCompNeuralGaussProc, I think) * comparing BP's to a laboriously hand-picked committee of NN regressors Gaussian Process background * they're transduction ==> big runtime cost, not just training (are newer algorithms not transductive?) * GP's assume that the output probs are jointly Gaussian distributed -- params come from the training data -- when joint Gaussian is marginalized, you end up with a 1-D gaussian for the test point -- mean and variance at that point are different than at other points * No idea of time or order dependence: just of closeness of one point to another -- dependence smoothing is scaled by a "length scale" which is unique for each input dimension -- hyperparms like length scale chosen based on ML criteria, not by MDL or BIC scheme -- how does this avoid overtraining? * no notion of "time" so how forecast ahead? * training is mainly picking hyperparams? Performance * NN's and GP's give about the same results, w/ same confidences * GP's easier and faster to train (in human time) * GP's computationally expensive, roughly cubic in num. of training points (test or train!) Extrapolation * NN's will extend trends * GP's will extend trends for a litle bit, but decay to a default b/c of Gaussian covariance decay w/ length scale * So, NN's MAY BE BETTER FOR MONOTONIC TRENDS!},
  crossref  = {Kocijan03caseCompNeuralGaussProc},
  file      = {:Tancret99CompNeuralGaussProcAlloy.pdf:PDF;Tancret99CompNeuralGaussProcAlloy.pdf:Tancret99CompNeuralGaussProcAlloy.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2008.10.03},
  url       = {http://www.msm.cam.ac.uk/phase-trans/abstracts/superalloy.gaussian.html},
}

@Article{Lyon16advtgsGrpMnLorenzGINI,
  author    = {Merritt Lyon and Li C. Cheung and Joseph L. Gastwirth},
  title     = {The Advantages of Using Group Means in Estimating the Lorenz Curve and Gini Index From Grouped Data},
  journal   = {The American Statistician},
  year      = {2016},
  volume    = {70},
  number    = {1},
  pages     = {25-32},
  abstract  = { A recent article proposed a histogram-based method for estimating the Lorenz curve and Gini index from grouped data that did not use the group means reported by government agencies. When comparing their method to one based on group means, the authors assume a uniform density in each grouping interval, which leads to an overestimate of the overall average income. After reviewing the additional information in the group means, it will be shown that as the number of groups increases, the bounds on the Gini index obtained from the group means become narrower. This is not necessarily true for the histogram method. Two simple interpolation methods using the group means are described and the accuracy of the estimated Gini index they yield and the histogram-based one are compared to the published Gini index for the 1967–2013 period. The average absolute errors of the estimated Gini index obtained from the two methods using group means are noticeably less than that of the histogram-based method. Supplementary materials for this article are available online.[Received August 2014. Revised September 2015.] },
  comment   = {Calculating GINI coefficients from anonymized summary stats, like quantiles, interval means, etc.   Estimtimates pdf by line slope between bin means, right hand tail fit some some kind of exponential.  R-code attached at end of paper.

See Fancier: Prendergast16quantLorenzGINI
See Simpler: Tille12histoLorenzGini},
  doi       = {10.1080/00031305.2015.1105152},
  eprint    = {https://doi.org/10.1080/00031305.2015.1105152},
  file      = {:Lyon16advtgsGrpMnLorenzGINI.pdf:PDF},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/00031305.2015.1105152
    
},
}

@Article{Liu11confWgtKNNimbal,
  author    = {Liu, Wei and Chawla, Sanjay},
  title     = {Class confidence weighted knn algorithms for imbalanced data sets},
  journal   = {Advances in Knowledge Discovery and Data Mining},
  year      = {2011},
  pages     = {345--356},
  abstract  = { In this paper, a novel k-nearest neighbors (kNN) weighting
strategy is proposed for handling the problem of class imbalance. When
dealing with highly imbalanced data, a salient drawback of existing kNN
algorithms is that the class with more frequent samples tends to dominate
the neighborhood of a test instance in spite of distance measurements,
which leads to suboptimal classification performance on the minority
class. To solve this problem, we propose CCW (class confidence weights)
that uses the probability of attribute values given class labels to weight
prototypes in kNN. The main advantage of CCW is that it is able to
correct the inherent bias to majority class in existing kNN algorithms
on any distance measurement. Theoretical analysis and comprehensive
experiments confirm our claims},
  file      = {:Liu11confWgtKNNimbal.pdf:PDF},
  publisher = {Springer},
}

@Article{King02logistRgrssnMLvsExact,
  author    = {Elizabeth N King and Thomas P Ryan},
  title     = {A Preliminary Investigation of Maximum Likelihood Logistic Regression versus Exact Logistic Regression},
  journal   = {The American Statistician},
  year      = {2002},
  volume    = {56},
  number    = {3},
  pages     = {163-170},
  abstract  = { Logistic regression is used by practitioners and researchers in many fields, but is undoubtedly used most frequently in medical and biostatistical applications. Maximum likelihood is generally the estimation method of choice, but we show that maximum likelihood can produce very poor results under certain conditions. Specifically, the poor performance of maximum likelihood in the case of rare events is known and we review research on this topic. We primarily examine the performance of maximum likelihood in the presence of near separation, which has apparently not been studied. Exact logistic regression is the logical alternative to maximum likelihood. We offer a comparison of the two methods of estimation. },
  comment   = {ML logistic regression fails for rare events.  An alternative is "exact" logistic regression.  This paper explains how and when to do exact instead of ML.},
  doi       = {10.1198/00031300283},
  eprint    = {https://doi.org/10.1198/00031300283},
  file      = {:King02logistRgrssnMLvsExact.pdf:PDF},
  publisher = {Taylor \& Francis},
  url       = { 
        https://doi.org/10.1198/00031300283
    
},
}

@InProceedings{Malhotra15lstmAnomDetTS,
  author       = {Malhotra, Pankaj and Vig, Lovekesh and Shroff, Gautam and Agarwal, Puneet},
  title        = {Long short term memory networks for anomaly detection in time series},
  booktitle    = {European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
  year         = {2015},
  pages        = {89},
  organization = {Presses universitaires de Louvain},
  abstract     = { Long Short Term Memory (LSTM) networks have been
demonstrated to be particularly useful for learning sequences containing
longer term patterns of unknown length, due to their ability to maintain
long term memory. Stacking recurrent hidden layers in such networks also
enables the learning of higher level temporal features, for faster learning
with sparser representations. In this paper, we use stacked LSTM net-
works for anomaly/fault detection in time series. A network is trained on
non-anomalous data and used as a predictor over a number of time steps.
The resulting prediction errors are modeled as a multivariate Gaussian
distribution, which is used to assess the likelihood of anomalous behav-
ior. The efficacy of this approach is demonstrated on four datasets: ECG,
space shuttle, power demand, and multi-sensor engine dataset.},
  comment      = {LSTM NN does a point prediction of non-anomalous data, flexibly modeling temporal dependence.  A multivariate Gaussian then models the likelihood of the error time series via a Kalman filter . A low likelihood is an anomaly.

There is no trained abnormality detector.

Obviously, this could be a Gaussian Copula or whatever...  The coolness of this idea is that the dependency inside the model e.g. Gaussian could capture temporal dependence.  The weakness is that the Gaussian dimension determines the max analysis window width but I think this method also a finite analysis window of length l.},
  file         = {:papers\\Malhotra15lstmAnomDetTS.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2017.07.04},
  url          = {https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2015-56.pdf},
}

@Article{Yu18deepSolarDB,
  author    = {Yu, Jiafan and Wang, Zhecheng and Majumdar, Arun and Rajagopal, Ram},
  title     = {DeepSolar: A Machine Learning Framework to Efficiently Construct a Solar Deployment Database in the United States},
  journal   = {Joule},
  year      = {2018},
  volume    = {2},
  number    = {12},
  pages     = {2605--2617},
  abstract  = {-- NIPS pdf abstract --
We have developed DeepSolar, a deep learning framework that analyzes satellite
imagery to identify the GPS locations and sizes of solar photovoltaic (PV) panels.
Leveraging its high accuracy and scalability, we constructed a comprehensive high-
fidelity solar deployment database for the contiguous U.S. We demonstrated its
value by discovering that residential solar deployment density peaks at a population
density of 1000 capita/mile2, increases with annual household income asymptoting
at ∼\$150K, and has an inverse correlation with the Gini index representing income
inequality. We uncovered a solar radiation threshold (4.5 kWh/m2/day) above
which the solar deployment is “triggered”. Furthermore, we built an accurate
machine learning-based predictive model to estimate the solar deployment density
at the census-tract level. We offer DeepSolar database as a publicly-available
resource for researchers, utilities, solar developers and policymakers to further
uncover solar deployment patterns, build comprehensive economic and behavioral
models, and ultimately support the adoption and management of solar electricity.

-- From Joule website ---

Highlights

* An accurate deep learning model for detecting solar panel on satellite imagery
* Built a nearly complete solar installation database for the contiguous US
* Identified key socioeconomic factors correlating with solar deployment density
* A predictive model to estimate solar deployment density at census tract level

Context \& Scale

We built a nearly complete solar installation database for the contiguous US utilizing a novel deep learning model applied to satellite imagery. The data are published as the first publicly available, high-fidelity solar installation database in the contiguous US. We plan to update it annually and add other countries and regions of the world. We demonstrated the value of this database by identifying key environmental and socioeconomic factors correlated with solar deployment. We also developed high-accuracy machine learning models to predict solar deployment density utilizing these factors as input. We hope the data produced by DeepSolar can aid researchers, policymakers, and the industry in gaining a better understanding of solar adoption and its impacts.
Summary

We developed DeepSolar, a deep learning framework analyzing satellite imagery to identify the GPS locations and sizes of solar photovoltaic panels. Leveraging its high accuracy and scalability, we constructed a comprehensive high-fidelity solar deployment database for the contiguous US. We demonstrated its value by discovering that residential solar deployment density peaks at a population density of 1,000 capita/mile2, increases with annual household income asymptoting at ∼\$150k, and has an inverse correlation with the Gini index representing income inequality. We uncovered a solar radiation threshold (4.5 kWh/m2/day) above which the solar deployment is “triggered.” Furthermore, we built an accurate machine learning-based predictive model to estimate the solar deployment density at the census tract level. We offer the DeepSolar database as a publicly available resource for researchers, utilities, solar developers, and policymakers to further uncover solar deployment patterns, build comprehensive economic and behavioral models, and ultimately support the adoption and management of solar electricity.},
  comment   = {Satellite imagery and big CNN finds 95\% of PV panels in US. Uses census-tract level to estimate what triggers solar adoption (existence of panels in one slice of time).   Their random forest (I think) socio-economic factor panel predictor fits better than traditional log or log-linear models, like Clean Power Research is using.

POINTS FROM 4 PAGE SUMMARY

CNN panel recognizer
-  semi-supervised transfer learning: 
    - initial CNN learns PV panel presence/absence (370K labeled images)
    - branching new layer off middele extracts boundaries w/o supervision 
    - activation map is thresholded to produce boundaries
    - don't run segmentation on images predicted to have no panels
- performance "comparable" to supervised, best is rural areas
  - residential: 93/89\% precision/recall
  - rural:       94/91\%
  - lower error if aggregate to larger area (uncorrelated/unbiased errors, reported results on census tracts, I think)
- targets are @ census tract, not zipcode, which is better: see my Evernote page for why)

Features predicting solar adoption (# solar systems / household)
- irradiation: sharp increase above 4.5-5 kWh/m^2/d
   -- since was so imp. the binned by irradiance and report other facts layered by this
- household income increases adoption, maybe plateauing \@ \$150K/yr
- pop. density: very sharp peak at 10000/sqmi
- education: increases adoption but redundant w/ income @ high irrad (yet has predictive value for low irrad)
- econ. equalilty (GINI coeff): the more equal, the more panels
- probably a lot more: I downloaded the tract data and there are a huge # of variables

PV Deployment Prediction
- traditional is linear or log-linear, < 10K samples
- trained a random forest model on 70K census  tracts (SolarForest)
- better 10-fold CV Rsq=0.72 than any other approach


A paper I found from Google Scholar says that this paper was published in NIPS 2018 but I can't find it there.  So, I've attached it to this bibtex entry.  The paywalled version is probably here: https://www.cell.com/joule/pdfExtended/S2542-4351(18)30570-1

Project Page: http://web.stanford.edu/group/deepsolar/home.html

Project page says it's better than OpenPV (https://openpv.nrel.gov/), which has worse data and relies on self-reports.

which has links to code and data on this page.

Evernote: https://www.evernote.com/l/AA2LIbgfOqFKcZTCg4PTvSnwfS5f3fbyM7w/

Quite relevant to Clean Power Research.},
  file      = {NIPS 2018 Paper:Yu18deepSolarDB_conf.pdf:PDF},
  publisher = {Elsevier},
  url       = {https://www.sciencedirect.com/science/article/pii/S2542435118305701},
}

@Article{Bass94bassWithoutDecVars,
  author   = {Bass, Frank M. and Krishnan, Trichy V. and Jain, Dipak C.},
  title    = {Why the Bass Model Fits without Decision Variables},
  journal  = {Marketing Science},
  year     = {1994},
  volume   = {13},
  number   = {3},
  pages    = {203-223},
  abstract = { Over a large number of new products and technological innovations, the Bass diffusion model (Bass 1969) describes the empirical adoption curve quite well. In this study, we generalize the Bass model to include decision variables such as price and advertising. The generalized model reduces to the Bass model as a special case and explains why the Bass model works so well without including decision variables. We compare our generalized Bass model to other approaches from the literature for including decision variables into diffusion models, and our results provide both theoretical and empirical support for the generalized Bass model. We also show how our generalized Bass model can be used for product planning purposes. },
  comment  = {The Bass Model used in Dong17resPVdeployFrcst to predict PV adoption.  Generalizes it to (I think) multivariate inputs, but then finds that they don't matter.  Also has a decent derivation.    Cosguner18dynPriceDynBass says it is an improvment

My first impression is that the original idea was that product adoption followed an S curve.  Then the guy decided to make it a diffy Q.  But I had better check this.

Also see this homepage: http://www.bassbasement.org/BassModel/Default.aspx

ANYWAY, maybe this could be cast as a stochastic differential equation problem??  Impose temporal smoothness or something?  
SURE ENOUGH: It's been done: Kapur12bassDiffuseSDE, Nafidi18inhomogDiffProcBassSDE

Spatial bass: Duggan17bassMetaSpace},
  doi      = {10.1287/mksc.13.3.203},
  eprint   = {https://doi.org/10.1287/mksc.13.3.203},
  file     = {:Bass94bassWithoutDecVars.pdf:PDF},
  url      = { 
        https://doi.org/10.1287/mksc.13.3.203
    
},
}

@InProceedings{Grunow04weakLightPV,
  author    = {Grunow, P and Lust, S and Sauter, D and Hoffmann, V and Beneking, C and Litzenburger, B and Podlowski, L},
  title     = {Weak light performance and annual yields of PV modules and systems as a result of the basic parameter set of industrial solar cells},
  booktitle = {19th European Photovoltaic Solar Energy Conference},
  year      = {2004},
  pages     = {2190--2193},
  abstract  = { The weak light performance of multi- and mono-crystalline PV modules are known to be dependent on 
the used cell type, but also vary from cell supplier to cell supplier using even the same cell type. It is shown, that this 
is a result from the characteristic distribution of the parasitic resistances. This paper shows that these differences can 
lead to 10% difference in annual energy yields of photovoltaic systems. This itself provides a major optimisation 
opportunity. The corresponding energy yields of identical designed reference PV systems are differing in a range, 
which is roughly of the same order as the efficiency variation for actual crystalline silicon modules found on the 
market. This underlines the on-going need for an extension of the STC parameters to characterize the potential 
energy yield of PV modules.  For practical use, state-of-the-art industrial cell and module testing equipment provides 
reliable values for the parasitic resistances and currents by extracting the basic parameter set from the measured IV 
curves at different intensity levels as well as in the dark, i.e. values for the shunt resistances, 2nd diode contributions 
and series resistances in the cells and the European module efficiency for the modules. 
Keywords: Multi-Crystalline, Performance, Shunts},
  comment   = {Shows where low light drop in PV panel efficiency is, and what circuitry affects it.  Can see that nonlinearity of electrical watts vs. solar intensity starts at around 400 W/m^2.  Depends a lot upon circuitry, though.  At 200 W/m^2, see that best case efficiency loss is about 2.5%; worst case example given is about 7.5%

Low light panel design can lead up to 10% difference in annual energy yields


For schematic drawing of linearity, see Electropaedia19solarPower

Can see some commercial modules in low light at:  Aleo19pvPanelsLowLight},
  file      = {:Grunow04weakLightPV.pdf:PDF},
  url       = {http://www.solon.com/export/sites/default/solonse.com/_downloads/global/article-pid/Grunow_et_al_Weak_Light_Performance.pdf},
}

@Article{Hansen01mdlSelMDL,
  author    = {Mark H Hansen and Bin Yu},
  title     = {Model Selection and the Principle of Minimum Description Length},
  journal   = {Journal of the American Statistical Association},
  year      = {2001},
  volume    = {96},
  number    = {454},
  pages     = {746-774},
  abstract  = { This article reviews the principle of minimum description length (MDL) for problems of model selection. By viewing statistical modeling as a means of generating descriptions of observed data, the MDL framework discriminates between competing models based on the complexity of each description. This approach began with Kolmogorov's theory of algorithmic complexity, matured in the literature on information theory, and has recently received renewed attention within the statistics community. Here we review both the practical and the theoretical aspects of MDL as a tool for model selection, emphasizing the rich connections between information theory and statistics. At the boundary between these two disciplines we find many interesting interpretations of popular frequentist and Bayesian procedures. As we show, MDL provides an objective umbrella under which rather disparate approaches to statistical modeling can coexist and be compared. We illustrate the MDL principle by considering problems in regression, nonparametric curve estimation, cluster analysis, and time series analysis. Because model selection in linear regression is an extremely common problem that arises in many applications, we present detailed derivations of several MDL criteria in this context and discuss their properties through a number of examples. Our emphasis is on the practical application of MDL, and hence we make extensive use of real datasets. In writing this review, we tried to make the descriptive philosophy of MDL natural to a statistics audience by examining classical problems in model selection. In the engineering literature, however, MDL is being applied to ever more exotic modeling situations. As a principle for statistical modeling in general, one strength of MDL is that it can be intuitively extended to provide useful tools for new problems. },
  comment   = {Information based model selection.  Describes AIC and BIC in terms of  MDL.  Has equations for linear regression selection.

A partial entry for this article is in speakerClust.bib (hansenXXmdl).},
  doi       = {10.1198/016214501753168398},
  eprint    = {https://doi.org/10.1198/016214501753168398},
  file      = {:Hansen01mdlSelMDL.pdf:PDF},
  keywords  = {A IC; Bayesian methods; Bayes information criterion; Cluster analysis; Code length; Coding redundancy; Information theory; Model selection; Pointwise and minimax lower bounds; Regression; time series.},
  publisher = {Taylor \& Francis},
  url       = { 
        https://doi.org/10.1198/016214501753168398
    
},
}

@Article{Lim15lrnInteractHierGrpLasso,
  author    = {Michael Lim and Trevor Hastie},
  title     = {Learning Interactions via Hierarchical Group-Lasso Regularization},
  journal   = {Journal of Computational and Graphical Statistics},
  year      = {2015},
  volume    = {24},
  number    = {3},
  pages     = {627-654},
  note      = {PMID: 26759522},
  abstract  = { We introduce a method for learning pairwise interactions in a linear regression or logistic regression model in a manner that satisfies strong hierarchy: whenever an interaction is estimated to be nonzero, both its associated main effects are also included in the model. We motivate our approach by modeling pairwise interactions for categorical variables with arbitrary numbers of levels, and then show how we can accommodate continuous variables as well. Our approach allows us to dispense with explicitly applying constraints on the main effects and interactions for identifiability, which results in interpretable interaction models. We compare our method with existing approaches on both simulated and real data, including a genome-wide association study, all using our R package glinternet. 

Keywords
hierarchical; interaction; computer intensive; regression; logistic},
  doi       = {10.1080/10618600.2014.938812},
  eprint    = {https://doi.org/10.1080/10618600.2014.938812},
  file      = {:Lim15lrnInteractHierGrpLasso.pdf:PDF},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/10618600.2014.938812
    
},
}

@Article{Morshedizadeh17impPowCurvMon,
  author    = {Majid Morshedizadeh and Mojtaba Kordestani and Rupp Carriveau and David S-K Ting and Mehrdad Saif},
  title     = {Improved power curve monitoring of wind turbines},
  journal   = {Wind Engineering},
  year      = {2017},
  volume    = {41},
  number    = {4},
  pages     = {260-271},
  abstract  = { Wind turbine power output monitoring can detect anomalies in turbine performance which have the potential to result in unexpected failure. This study examines common Supervisory Control And Data Acquisition data over a period of 20 months. It is common to have more than 150 signals acquired by Supervisory Control And Data Acquisition systems, and applying all is neither practical nor useful. Thus, to address the issue, correlation coefficients analysis has been applied in this work to reveal the most influential parameters on wind turbine active power. Then, radial basis function and multilayer perception artificial neural networks are set up, and their performance is compared in two static and dynamic states. The proposed combination of the feature selection method and the dynamic multilayer perception neural network structure has performed well with favorable prediction error levels compared to other methods. Thus, the combination may be a valuable tool for turbine power curve monitoring. 

},
  comment   = {Some kinda RBF feature selection for turbine monitoring.  NN and some kinda dynamic input.

Paper downloaded from: https://www.researchgate.net/profile/Mojtaba_Kordestani/publication/317239656_Improved_power_curve_monitoring_of_wind_turbines/links/592de2710f7e9beee7337b09/Improved-power-curve-monitoring-of-wind-turbines.pdf},
  doi       = {10.1177/0309524X17709730},
  eprint    = {http://dx.doi.org/10.1177/0309524X17709730},
  file      = {:papers\\Morshedizadeh17impPowCurvMon.pdf:PDF},
  keywords  = {Performance monitoring, wind turbines, artificial neural networks, Supervisory Control And Data Acquisition, power curve},
  owner     = {sotterson},
  timestamp = {2017.07.24},
  url       = { 
        http://dx.doi.org/10.1177/0309524X17709730
    
},
}

@InProceedings{Flexer13shrKNNhubRdc,
  author       = {Flexer, Arthur and Schnitzer, Dominik},
  title        = {Can Shared Nearest Neighbors Reduce Hubness in High-Dimensional Spaces?},
  booktitle    = {2013 IEEE 13\textsuperscript{th} International Conference on Data Mining Workshops (ICDMW),},
  year         = {2013},
  pages        = {460--467},
  organization = {IEEE},
  abstract     = {?Hubness? is a recently discovered general problem
of machine learning in high dimensional data spaces. Hub objects
have a small distance to an exceptionally large number of data
points, and anti-hubs are far from all other data points. It
is related to the concentration of distances which impairs the
contrast of distances in high dimensional spaces. Computation of
secondary distances inspired by shared nearest neighbor (SNN)
approaches has been shown to reduce hubness and concentration
and there already exists some work on direct application of SNN
in the context of hubness in image recognition. This study applies
SNN to a larger number of high dimensional real world data sets
from diverse domains and compares it to two other secondary
distance approaches (local scaling and mutual proximity). SNN
is shown to reduce hubness but less than other approaches and,
contrary to its competitors, it is only able to improve classification
accuracy for half of the data sets.},
  comment      = {Local Scaling (LS) and Mutual Proximity (MP) are better than Shared Nearest Neighbors (SNN) for reducing hubness, and especially for KNN classification. MP may be slightly best, more computationally efficient, and (0,1) scaling may make it best for locLinQR weighting. Nice plots. Has Matlab.

A refinement of Schnitzer12Localandglobal

Concentration of distances with increased dimension
* points all start to look like each other
 - mean cross-point, variance normalized distance decreases with dimension
 - so can't tell them apart
* euclidean: mean dist increase; constant dist variance
* cosine: constant dist; decreasing variance
* Nice plots of this: Figs 3,4

Hubness
* hub points are too close to every other point, in KNN, making classification impossible
* related to actual data dimension (intrinsic dimension) not literal feature dimension
* detected by distance skewness. Nice plot in fig 2.

Solutions tested
* Shared Nearest Neighbhor (SNN): distance from common NN's
* Local Scaling (LS)
 - distance normalized by scale of NN's for each point
 - mentioned in in Schnitzer12Localandglobal but not compared. Why now?
 - originally from spectral clustering
 - comp. cost goes up w/ sq. of dist. (but speculatively, could be better)
* Mutual Proximity (MP): kinda probabilistic, see Schnitzer12Localandglobal
 - approximations can make it very computationally cheap, compared to other 2 methods (linear vs. dist)
 - seems a little more reliable and smooth (?)
 - is a global technique, not local, like the other two

RESULTS : MP ~ LS > SNN
* all techniques can decisively reduce hubness (MP, LS are better than SNN)
* MP always shows a classification improvement, but not always large
* I'm not sure if LS always does (didn't look carefully)
* SNN does not always improve, can sometimes make it considerably worse
 - reason may be that it quantizes distance by rank
 - can see that all values are not used
 - can see that it reduces intrinsic dimension, while the other two methods don't
* MP approx can great reduce computation but..
 - In Schnitzer12Localandglobal MP Gaussian approx seemed to have no loss
 - here the say it's small
* not clear if tests were on balanced or imbalanced data
 - see Tomasev13imbalKnnHubs for specific imbalanced data tests.

Matlab: http://ofai.at/research/impml/projects/hubology.html
 but it may have moved to location mentioned in Schnitzer12Localandglobal},
  file         = {Flexer13shrKNNhubRdc.pdf:Flexer13shrKNNhubRdc.pdf:PDF},
  groups       = {Read},
  owner        = {sotterson},
  timestamp    = {2014.04.04},
  url          = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6753957},
}

@Article{Schnitzer12Localandglobal,
  author    = {Schnitzer, Dominik and Flexer, Arthur and Schedl, Markus and Widmer, Gerhard},
  title     = {Local and global scaling reduce hubs in space},
  journal   = {The Journal of Machine Learning Research},
  year      = {2012},
  volume    = {13},
  number    = {1},
  pages     = {2871--2902},
  abstract  = {'Hubness' has recently been identified as a general problem of high dimensional data spaces, manifesting itself in the emergence of objects, so-called hubs, which tend to be among the k nearest neighbors of a large number of data items. As a consequence many nearest neighbor relations in the distance space are asymmetric, that is, object y is amongst the nearest neighbors of x but not vice versa. The work presented here discusses two classes of methods that try to symmetrize nearest neighbor relations and investigates to what extent they can mitigate the negative effects of hubs. We evaluate local distance scaling and propose a global variant which has the advantage of being easy to approximate for large data sets and of having a probabilistic interpretation. Both local and global approaches are shown to be effective especially for high-dimensional data sets, which are affected by high hubness. Both methods lead to a strong decrease of hubness in these data sets, while at the same time improving properties like classification accuracy. We evaluate the methods on a large number of public machine learning data sets and synthetic data. Finally we present a real-world application where we are able to achieve significantly higher retrieval quality.},
  comment   = {High dim. KNN classification greatly improved with local and unsupervised global hub reduction. MP technique might be good for locLinQR, spectral or KNN clustering. MP also has probabilstic interpretation, maybe interesting for Kraskov MI estimation too? SNN not as reliable or effiencient (potentially) as MP and LS. Has Matlab

updated in Flexer13shrKNNhubRdc, which has some nice plots

Manifestations of the Curse of High Dimensionality

* Hubs
 - points that are the nearest neighbors of way too many other points
* Orphans
 - data points that are not the nearest neighbor of any other point
 - this made 27\% of songs unreachable in music recommender system.
* Concentration:
 - all points points are at about the same distance from each other
 - any set of points: this is a consequence of high dim spaces
 - measurement: ratio of spread to magnitude, e.g. std(all distances)/mean(all distances)
 ---- points closer to data mean are more likely to be hubs
 ---- points further awy from mean are more likey to be orphans
* Asymmetry
 - y is the NN of x but y's nearest neighbor is some other point, a, not x
 - causes KNN to fail.
 - the techniques in this paper make adjacency more symmetric.
* Pairwise stability (the lack of it):
 - y is the NN of x
 - a is he NN of y
 - class(x) == class(y) != class(a) [this is the lack of stability]
* Bad hubs
 -- have different classes than the majority of data points they are nearest neighbors to

Fixes for C.0.D and Hubs (not local, not global?)
* SNN: Shared Nearest Neighbors
 - helps with "non-globular" clusters
 - similarity is the num. of KNN's that x and y have in common (NN overlap)
* RNN Reverse Nearest Neighbor
 - finds fartherst neighbors?? (not described)
 - used for outlier detection
* Proximity verification: based on the NN rank similarity

Local Scaling Methods
* LS: Local Scaling (Zelnik-Manor)
 - initially used for spectral clustering
 - tunes pairwise distances to account for different local densities (good for KNN density estimation?)
 - x and y "close" iff dist. dx,y is small rel. to both local x and y scales (dist to x or y's kth NN)
 - calls it "LS" (section 3.1)
* Contextual dissimilarity measure (CDM), not explained
* NICDM: non-iterative contextual dissimilarity measure
 - scales by avg. dist to ith NN, for all x or y
 - more stable than LS
 - the local alternative this paper tries

Global Scaling: Mutual Proximity (MP)
* what the experiment with in this paper
* converts distance to probability of being x and y being eachothers NN (k==1 in KNN)
 - is a similarity measure now, scaled to (0,1)
 --- IDEA: try this for spectral clustering, as LS already worked and this is already a similarity w/ nice scaling
 --- IDEA: seem like it would be smooth, unlike KNN probs, so a good locLin weight function?
 - can estimate w/ joint empirical distribution of all points but not necessary
 --- almost, or just as, good to model joint distance by independent Gaussians/Gamma for each point
 ----- but in Flexer13shrKNNhubRdc, the say the loss is small
 --- Params come from subsamples (30 points, but I am not quite convinced b/c they avg. results)
 --- Gaussian may be best: Gamma and Gauss perf. the same (?) but they drop gamma (Sec 4.4, p. 2889)
 - scaling means that can combine multiple distance measures with ease
 --- example is music timbre and rythmn distances
* makes relationships symmetric and (I think) autoscales out hubs
* is unsupervised so useful for more than classification!
* IDEA: Globalness of MP is good b/c for KNN locLinQR neighborhoods
 - with a dimension reduction loop, the adjacency fix would have to work at full dimension.
* Compare with p-Gaussians global techniques e.g. Francois05locKernHiDim

Performance Metrics
* KNN classifer error
* k-occurrence: num. times x occurs in the KNN lists of all other points
* Hubness: k-occurrence distribution skewness
 - postive skew (>1.4) is "hubby" (p. 2884)
 - "very hubby": >2 (p. 2892)
 --- see that dimensions as low as 30 are very hubby (p. 2892)
 --- IDEA: would hub reduction reduce dimension-dependent bias in Kraskov04EstMutInfKNN
 - see that "badness" of hubs about as bad as orphans (on tests, p. 2894)
 - IDEA: justify need for special metrics by showing hubness of locLin QR input data
 - IDEA: use hubness/orphanness as a probabilistic forecast input (for whole dataset, somehow or locally?)
 -- maybe plot error vs. local region "hubness" or "orphanness"
* Goodman-Kruksal Index: roughly, average class matching between pairs (pair stability)
* Intrinsic dimensionality (ref to Levina04maxLikIntrinsDim )
 - num. dims really needed to model the data (feature space dimension is "embedded dimension"
 - ID and embedded dimension can be vastly different (9 vs. 15154 in one case, pl 2884)
 - IDEA: use ID as a lower bound on dimension search for PCA, when the goal is regression?
 ---- could there still be dims that aren't relevant to the regression targ, even at lower ID?
 - IDEA: use this for dim reduction prior (final dim, post reduction, e.g. in PCA, if didn't have Minka00AutoPCAdim)
 - IDEA: use to justify local regression dimension reduction (plot intrinsic dim. of each neighborhood)
* Percent symmetric relations (want this to be high)

RESULTS
* Test set is large, very diverse
 - image, audio, text, statistical, biological
 - 3 kinds of distance measures: Euclidean, cosine, Kullback-Leibler divergence (KL-dist?)
* initial tests done using empirical distribution
* Performance Improvement
 - quite large reduction in KNN classification error, and hubness
 - best improvements on high intrinsic or embedded dimensions
 - best improvements if data was already hubby (some high dims aren't hubby)
 - hubness decreases most on most hubby original data (obvious?)
 - hubness reduction never hurts, even if data is not hubby
* Neither MP or NICDM reduce intrinsic dimensionality
 - Good, b/c intrinsic dimensionality may be needed for discrimination
 - This explanation is kind of given in Flexer13shrKNNhubRdc
 - interesting multidimensional distance scaling (MDS) measurement technique!
* MP seems equal or slightly better than NICDM on Godman-Kruskal (concordance) and symmetry
* simple Gaussian indepence modelling of distribution performs as well as anything else
 - but loss is called "small" in Flexer13shrKNNhubRdc
* subsampling of distribution said to be as good, but I am not so sure about this
* Seems like they like MP the best
* not clear if tests were on balanced or imbalanced data
 - see Tomasev13imbalKnnHubs for specific imbalanced data tests.

Has MATLAB: http://www.ofai.at/~dominik.schnitzer/mp.},
  file      = {Schnitzer12Localandglobal.pdf:Schnitzer12Localandglobal.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {JMLR. org},
  timestamp = {2014.04.04},
  url       = {http://dl.acm.org/citation.cfm?id=2503333},
}

@TechReport{Dokic15systIntegLrgData,
  author      = {Tatjana Dokic and Yingzhong Gu and Arjun Anand and Shang-Tse Cheng and Mladen Kezunovic},
  title       = {Systematic Integration of Large Data Sets for Improved Decision Making},
  institution = {PSERC},
  year        = {2015},
  number      = {T-51},
  abstract    = {(PSERC) is a multi-university Center conducting research on challenges facing the electric power industry and educating the next generation of power engineers. More information about PSERC can be found at the Center's website: Notice Concerning Copyright Material PSERC members are given permission to copy without fee all or part of this publication for internal use if appropriate attribution is given to this document as the source material. This report is available for downloading from the PSERC website.Making " (project T-51). We express our appreciation for the support provided by PSERC's industry members and by the National Science Foundation under the Industry/University Cooperative Research Center program. In particular, we wish to thank industry advisors Executive Summary This project is focused on the Big Data uses in power systems. To be able to research this topic that spans across many applications and yet to stay within the scope of the project, three issues were addressed by the team of researchers: Outage and Asset Management (Mladen Kezunovic and his students), Short-term Spatio-temporal wind forecast (Le Xie and his students), and Distributed Database for Future Grid (Santiago Grijalva and Polo Chau and their students). Before specific results of the study are addressed, the report has offered a definition of the Big Data problem in the utility industry in Section 2. In Section 3, the improved fault location using lightning data for transmission network is presented. In Section 4, a methodology for evaluating risk of insulation breakdown is discussed. Section 5 describes big data application to wind power forecast and look-ahead power system dispatch. Section 6 describes implementation of distributed database for future grid using smart meter data as an example. Conclusions are given in Section 7. As an introduction to the Big Data problem, it was pointed out that spatial and temporal aspects are most critical part of the use of Big Data. This is particularly challenging when the data from various sources are integrated. This is illustrated by an example where the data from various power system field measurement infrastructures is merged with data that comes from the sources outside the utility owned database. Examples of the utility sources of field measurement data of interest were supervisory control and data acquisition system, synchrophasor measurement system, automated smart metering system, etc. Examples of the non-utility owned databases are the lightning detection network, weather data coming from the various government services, landscape and vegetation data, ?},
  comment     = {A huge "big data" grid operations project report.  Of interest are, at least:

1. Two short term statio-temporal wind speed forecasting methods that model advection of surface winds and also include geostropic winds!
 - Does a kind of diurnal persistence for both, kind of like Pritchard10varQuantProbFrcst
 - these assume wind speed is a truncated Gaussian, and do learning with a CRPS based method!
 - learns spatial covariances with neighboring wind farms
 - compare with Tastu10spatTempErr and my paper

2. "Robust" dispatch equations, which may be probabilistic.
 - compare with Bertsimas13adptRobustOptUC ?

3. Locates line faults using lightning data

4. Lots more but I didn't read it.


Project page is here http://pserc.wisc.edu/research/current_projects/t51.aspx},
  file        = {Dokic15systIntegLrgData.pdf:Dokic15systIntegLrgData.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2017.03.11},
  url         = {http://pserc.wisc.edu/research/public_reports/td.aspx},
}

@InBook{Khattree99multiVarCncpt,
  chapter   = {Chapter 1: Multivariate Analysis Concepts},
  title     = {Applied Multivariate Statistics with SAS Software, Second Edition},
  publisher = {SAS Publishing},
  year      = {1999},
  author    = {Ravindra Khattree and Dayanand Naik},
  abstract  = {* Introduction * Random Vectors, Means, Variances, and Covariances * Multivariate Normal Distribution * Sampling from Multivariate Normal Populations * Some Important Sample Statistics and Their Distributions * Tests for Multivariate Normality * Random Vector and Matrix Generation},
  comment   = {Clear description of generalized variance, partial correlation, multivariate normal and other multivariate tests, etc. From SAS.},
  file      = {Khattree99multiVarCncpt.pdf:Khattree99multiVarCncpt.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2012.03.09},
  url       = {https://support.sas.com/pubscat/bookdetails.jsp?pc=56903},
}

@TechReport{Milligan11CostCausIntegVarGen,
  author      = {Milligan, Michael and Ela, Erik and Hodge, Bri-Mathias and Kirby, Brendan and Lew, Debra and Clark, Charlton and DeCesaro, Jennifer and Lynn, Kevin},
  title       = {Cost-Causation and Integration Cost Analysis for Variable Generation},
  institution = {NREL},
  year        = {2011},
  type        = {Tech Report},
  number      = {NREL/TP-5500-51860},
  month       = jun,
  abstract    = {1
Cost-Causation and Integration Cost Analysis
for Variable Generation

Michael Milligan, Erik Ela, Bri-Mathias Hodge, Brendan Kirby (consultant), Debra Lew
National Renewable Energy Laboratory

Charlton Clark, Jennifer DeCesaro, Kevin Lynn
United States Department of Energy
Introduction

Wind and solar power generation are prized for their environmental benefits, their low and stable
operating costs, and their help in reducing fuel imports. Advances in both technologies are
reducing capital costs and providing significant control capabilities. Still, the primary energy
source for both technologies is variable and uncertain and a power system with significant wind
or solar penetration must be operated differently than a power system based exclusively on
conventional resources. It is very natural to ask what the additional cost of accommodating wind
and solar generation is. However, calculating the integration cost of variable generation turns out
to be surprisingly difficult.

Integration cost analysis has progressed significantly over the past ten years. There is also a
much better understanding of the cost drivers among the system stakeholders now than there was
when wind and solar generation were in their infancy. This report examines how wind and solar
integration studies have evolved, what analysis techniques work, what common mistakes are still
made, what improvements are likely to be made in the near future, and why calculating
integration costs is such a difficult problem and should be undertaken carefully, if at all.

Analysis techniques are now very good at simulating power system operations with time-
synchronized load, wind, and solar data. The best studies model security-constrained unit
commitment and economic dispatch with hourly (or shorter) time steps covering one year or
longer. They account for forecast errors for wind, solar, and load as well as actual output and
consumption. Cases with and without wind and solar can be compared. Total system costs with
and without renewables can be calculated fairly accurately under a range of system conditions.
These cost differences are typically dominated by the fuel cost savings that renewables provide,
however. Calculating an ?integration cost? that only includes the added cost the power system
incurs dealing with the variability and uncertainty of wind and solar, and excludes the fuel cost
savings, is much more difficult. The many complex interactions among components of the power
system and assumptions regarding the no-wind base case all have important influences on
integration cost estimates, and in fact raise questions of whether cost components that are commonly thought to be integration costs can be correctly untangled. We discuss many of these
concerns and implications, shedding some light on the difficulties involved in measuring and
interpreting integration cost estimates.},
  comment     = {How integration costs are attributed to various energy sources. Nuclear and large fossil fuel plants have higher integration costs than wind and solar (according to the article that said this paper was a reference for that factoid).

The article referencing this one was:
http://www.greentechmedia.com/articles/read/Grid-Integration-of-Wind-and-Solar-is-Cheap},
  file        = {Milligan11CostCausIntegVarGen.pdf:Milligan11CostCausIntegVarGen.pdf:PDF},
  journal     = {Contract},
  location    = {Colorado, USA},
  owner       = {sotterson},
  pages       = {275--3000},
  timestamp   = {2014.12.15},
  url         = {http://www.nrel.gov/docs/fy11osti/51860.pdf},
  volume      = {303},
}

@Book{Ackermann05WindPowSysBook,
  title     = {Wind power in power systems},
  publisher = {Wiley Online Library},
  year      = {2005},
  author    = {Ackermann, Thomas and others},
  volume    = {140},
  abstract  = {1 Introduction 1
Part A Theoretical Background and Technical Regulations 5
2 Historical Development and Current Status of Wind Power 7
Thomas Ackermann
2.1 Introduction 7
2.2 Historical Background 8
2.2.1 Mechanical power generation 8
2.2.2 Electrical power generation 9
2.3 Current Status of Wind Power Worldwide 11
2.3.1 Overview of grid-connected wind power generation 11
2.3.2 Europe 11
2.3.3 North America 13
2.3.4 South and Central America 16
2.3.5 Asia and Pacific 16
2.3.6 Middle East and Africa 17
2.3.7 Overview of stand-alone generation 18
2.3.8 Wind power economics 18
2.3.9 Environmental issues 20
2.4 Status of Wind Turbine Technology 21
2.4.1 Design approaches 22
2.5 Conclusions 23
Acknowledgements 23
References 23
3 Wind Power in Power Systems: An Introduction 25
Lennart So?der and Thomas Ackermann
3.1 Introduction 25
3.2 Power System History 25
3.3 Current Status of Wind Power in Power Systems 26
3.4 Network Integration Issues for Wind Power 28
3.5 Basic Electrical Engineering 29
3.6 Characteristics of Wind Power Generation 32
3.6.1 The wind 32
3.6.2 The physics 33
3.6.3 Wind power production 34
3.7 Basic Integration Issues Related to Wind Power 40
3.7.1 Consumer requirements 40
3.7.2 Requirements from wind farm operators 41
3.7.3 The integration issues 41
3.8 Conclusions 46
Appendix: A Mechanical Equivalent to Power System Operation with
Wind Power 47
Introduction 47
Active power balance 48
Reactive power balance 49
References 50
4 Generators and Power Electronics for Wind Turbines 53
Anca D. Hansen
4.1 Introduction 53
4.2 State-of-the-art Technologies 53
4.2.1 Overview of wind turbine topologies 53
4.2.2 Overview of power control concepts 55
4.2.3 State-of-the-art generators 55
4.2.4 State-of-the-art power electronics 59
4.2.5 State-of-the-art market penetration 62
4.3 Generator Concepts 65
4.3.1 Asynchronous (induction) generator 66
4.3.2 The synchronous generator 69
4.3.3 Other types of generators 70
4.4 Power Electronic Concepts 72
4.4.1 Soft-starter 72
4.4.2 Capacitor bank 72
4.4.3 Rectifiers and inverters 73
4.4.4 Frequency converters 74
4.5 Power Electronic Solutions in Wind Farms 75
4.6 Conclusions 77
References 77
5 Power Quality Standards for Wind Turbines 79
John Olav Tande
5.1 Introduction 79
5.2 Power Quality Characteristics of Wind Turbines 80
viii Contents
5.2.1 Rated data 81
5.2.2 Maximum permitted power 81
5.2.3 Maximum measured power 81
5.2.4 Reactive power 81
5.2.5 Flicker coefficient 82
5.2.6 Maximum number of wind turbine switching operations 83
5.2.7 Flicker step factor 83
5.2.8 Voltage change factor 84
5.2.9 Harmonic currents 84
5.2.10 Summary power quality characteristics for various wind turbine types 84
5.3 Impact on Voltage Quality 85
5.3.1 General 85
5.3.2 Case study specifications 86
5.3.3 Slow voltage variations 87
5.3.4 Flicker 89
5.3.5 Voltage dips 91
5.3.6 Harmonic voltage 92
5.4 Discussion 93
5.5 Conclusions 94
References 95
6 Power Quality Measurements 97
Fritz Santjer
6.1 Introduction 97
6.2 Requirements for Power Quality Measurements 98
6.2.1 Guidelines 98
6.2.2 Specification 99
6.2.3 Future aspects 104
6.3 Power Quality Characteristics of Wind Turbines and Wind Farms 105
6.3.1 Power peaks 105
6.3.2 Reactive power 106
6.3.3 Harmonics 106
6.3.4 Flicker 108
6.3.5 Switching operations 109
6.4 Assessment Concerning the Grid Connection 111
6.5 Conclusions 112
References 113
7 Technical Regulations for the Interconnection of Wind Farms to
the Power System 115
Julija Matevosyan, Thomas Ackermann and Sigrid M. Bolik
7.1 Introduction 115
7.2 Overview of Technical Regulations 115
7.2.1 Regulations for networks below 110 kV 117
7.2.2 Regulations for networks above 110 kV 119
7.2.3 Combined regulations 120
7.3 Comparison of Technical Interconnection Regulations 121
7.3.1 Active power control 122
7.3.2 Frequency control 123
Contents ix
7.3.3 Voltage control 124
7.3.4 Tap changers 128
7.3.5 Wind farm protection 128
7.3.6 Modelling information and verification 133
7.3.7 Communication and external control 133
7.3.8 Discussion of interconnection regulations 134
7.4 Technical Solutions for New Interconnection Rules 136
7.4.1 Absolute power constraint 136
7.4.2 Balance control 136
7.4.3 Power rate limitation control approach 136
7.4.4 Delta control 137
7.5 Interconnection Practice 138
7.6 Conclusions 140
References 140
8 Power System Requirements for Wind Power 143
Hannele Holttinen and Ritva Hirvonen
8.1 Introduction 143
8.2 Operation of the Power System 144
8.2.1 System reliability 145
8.2.2 Frequency control 146
8.2.3 Voltage management 147
8.3 Wind Power Production and the Power System 149
8.3.1 Production patterns of wind power 149
8.3.2 Variations of production and the smoothing effect 151
8.3.3 Predictability of wind power production 155
8.4 Effects of Wind Energy on the Power System 156
8.4.1 Short-term effects on reserves 156
8.4.2 Other short-term effects 160
8.4.3 Long-term effects on the adequacy of power capacity 162
8.4.4 Wind power in future power systems 164
8.5 Conclusions 164
References 165
9 The Value of Wind Power 169
Lennart So?der
9.1 Introduction 169
9.2 The Value of a Power Plant 169
9.2.1 Operating cost value 169
9.2.2 Capacity credit 170
9.2.3 Control value 170
9.2.4 Loss reduction value 170
9.2.5 Grid investment value 170
9.3 The Value of Wind Power 170
9.3.1 The operating cost value of wind power 171
9.3.2 The capacity credit of wind power 171
9.3.3 The control value of wind power 174
9.3.4 The loss reduction value of wind power 177
9.3.5 The grid investment value of wind power 180
x Contents
9.4 The Market Value of Wind Power 180
9.4.1 The market operation cost value of wind power 180
9.4.2 The market capacity credit of wind power 181
9.4.3 The market control value of wind power 182
9.4.4 The market loss reduction value of wind power 188
9.4.5 The market grid investment value of wind power 189
9.5 Conclusions 194
References 195
Part B Power System Integration Experience 197
10 Wind Power in the Danish Power System 199
Peter Borre Eriksen and Carl Hilger
10.1 Introduction 199
10.2 Operational Issues 203
10.2.1 The Nordic market model for electricity trading 205
10.2.2 Different markets 207
10.2.3 Interaction between technical rules and the market 209
10.2.4 Example of how Eltra handles the balance task 210
10.2.5 Balancing via Nord Pool: first step 211
10.2.6 The accuracy of the forecasts 213
10.2.7 Network controller and instantaneous reserves 215
10.2.8 Balancing prices in the real-time market 215
10.2.9 Market prices fluctuating with high wind production 217
10.2.10 Other operational problems 217
10.3 System Analysis and Modelling Issues 219
10.3.1 Future development of wind power 219
10.3.2 Wind regime 220
10.3.3 Wind power forecast models 221
10.3.4 Grid connection 223
10.3.5 Modelling of power systems with large-scale wind
power production 224
10.3.6 Wind power and system analysis 226
10.3.7 Case study CO2 reductions according to the Kyoto
Protocol 228
10.4 Conclusions and Lessons Learned 231
References 232
11 Wind Power in the German Power System: Current Status and Future
Challenges of Maintaining Quality of Supply 233
Matthias Luther, Uwe Radtke and Wilhelm R. Winter
11.1 Introduction 233
11.2 Current Performance of Wind Energy in Germany 234
11.3 Wind Power Supply in the E.ON Netz Area 236
11.4 Electricity System Control Requirements 237
11.5 Network Planning and Connection Requirements 238
11.6 Wind Turbines and Dynamic Performance Requirements 241
11.7 Object of Investigation and Constraints 241
Contents xi
11.8 Simulation Results 244
11.8.1 Voltage quality 244
11.8.2 Frequency stability 248
11.9 Additional Dynamic Requirements of Wind Turbines 252
11.10 Conclusions 254
References 255
12 Wind Power on Weak Grids in California and the US Midwest 257
H. M. Romanowitz
12.1 Introduction 257
12.2 The Early Weak Grid: Background 259
12.2.1 Tehachapi 66 kV transmission 259
12.2.2 VARs 260
12.2.3 FACTS devices 260
12.2.4 Development of wind energy on the Tehachapi 66 kV grid 261
12.2.5 Reliable generation 262
12.2.6 Capacity factor improvement: firming intermittent wind generation 263
12.3 Voltage Regulation: VAR Support on a Wind-dominated Grid 264
12.3.1 Voltage control of a self-excited induction machine 264
12.3.2 Voltage regulated VAR control 264
12.3.3 Typical wind farm PQ operating characteristics 265
12.3.4 Local voltage change from VAR support 267
12.3.5 Location of supplying VARs within a wind farm 268
12.3.6 Self-correcting fault condition: VAR starvation 269
12.3.7 Efficient-to-use idle wind turbine component capacity
for low-voltage VARs 270
12.3.8 Harmonics and harmonic resonance: location on grid 271
12.3.9 Islanding, self-correcting conditions and speed of response
for VAR controls 274
12.3.10 Self-correcting fault condition: VAR starvation 275
12.3.11 Higher-speed grid events: wind turbines that stay connected through
grid events 276
12.3.12 Use of advanced VAR support technologies on weak grids 278
12.3.13 Load flow studies on a weak grid and with induction machines 279
12.4 Private Tehachapi Transmission Line 280
12.5 Conclusions 281
References 282
13 Wind Power on the Swedish Island of Gotland 283
Christer Liljegren and Thomas Ackermann
13.1 Introduction 283
13.1.1 History 283
13.1.2 Description of the local power system 285
13.1.3 Power exchange with the mainland 286
13.1.4 Wind power in the South of Gotland 286
13.2 The Voltage Source Converter Based High-voltage Direct-current Solution 287
13.2.1 Choice of technology 287
13.2.2 Description 287
13.2.3 Controllability 288
xii Contents
13.2.4 Reactive power support and control 288
13.2.5 Voltage control 288
13.2.6 Protection philosophy 289
13.2.7 Losses 290
13.2.8 Practical experience with the installation 290
13.2.9 Tj?reborg Project 291
13.3 Grid Issues 291
13.3.1 Flicker 292
13.3.2 Transient phenomena 292
13.3.3 Stability issues with voltage control equipment 293
13.3.4 Validation 294
13.3.5 Power flow 295
13.3.6 Technical responsibility 296
13.3.7 Future work 296
13.4 Conclusions 296
Further Reading 297
References 297
14 Isolated Systems with Wind Power 299
Per Lundsager and E. Ian Baring-Gould
14.1 Introduction 299
14.2 Use of Wind Energy in Isolated Power Systems 300
14.2.1 System concepts and configurations 300
14.2.2 Basic considerations and constraints for wind?diesel power stations 305
14.3 Categorisation of Systems 310
14.4 Systems and Experience 311
14.4.1 Overview of systems 312
14.4.2 Hybrid power system experience 312
14.5 Wind Power Impact on Power Quality 315
14.5.1 Distribution network voltage levels 316
14.5.2 System stability and power quality 316
14.5.3 Power and voltage fluctuations 317
14.5.4 Power system operation 317
14.6 System Modelling Requirements 320
14.6.1 Requirements and applications 321
14.6.2 Some numerical models for isolated systems 322
14.7 Application Issues 322
14.7.1 Cost of energy and economics 324
14.7.2 Consumer demands in isolated communities 325
14.7.3 Standards, guidelines and project development approaches 325
14.8 Conclusions and Recommendations 327
References 328
15 Wind Farms in Weak Power Networks in India 331
Poul S?rensen
15.1 Introduction 331
15.2 Network Characteristics 334
15.2.1 Transmission capacity 334
15.2.2 Steady-state voltage and outages 335
Contents xiii
15.2.3 Frequency 337
15.2.4 Harmonic and interharmonic distortions 337
15.2.5 Reactive power consumption 338
15.2.6 Voltage imbalance 338
15.3 Wind Turbine Characteristics 338
15.4 Wind Turbine Influence on Grids 339
15.4.1 Steady-state voltage 339
15.4.2 Reactive power consumption 339
15.4.3 Harmonic and interharmonic emission 342
15.5 Grid Influence on Wind Turbines 343
15.5.1 Power performance 343
15.5.2 Safety 345
15.5.3 Structural lifetime 346
15.5.4 Stress on electric components 346
15.5.5 Reactive power compensation 346
15.6 Conclusions 347
References 347
16 Practical Experience with Power Quality and Wind Power 349
A ?
ke Larsson
16.1 Introduction 349
16.2 Voltage Variations 349
16.3 Flicker 352
16.3.1 Continuous operation 352
16.3.2 Switching operations 354
16.4 Harmonics 358
16.5 Transients 360
16.6 Frequency 361
16.7 Conclusions 363
References 363
17 Wind Power Forecast for the German and Danish Networks 365
Bernhard Ernst
17.1 Introduction 365
17.2 Current Development and Use of Wind Power Prediction Tools 366
17.3 Current Wind Power Prediction Tools 367
17.3.1 Prediktor 367
17.3.2 Wind Power Prediction Tool 368
17.3.3 Zephyr 370
17.3.4 Previento 370
17.3.5 eWind 370
17.3.6 SIPREO?LICO 371
17.3.7 Advanced Wind Power Prediction Tool 372
17.3.8 HONEYMOON project 376
17.4 Conclusions and Outlook 377
17.4.1 Conclusions 377
17.4.2 Outlook 380
References 380
Useful websites 381
xiv Contents
18 Economic Aspects of Wind Power in Power Systems 383
Thomas Ackermann and Poul Erik Morthorst
18.1 Introduction 383
18.2 Costs for Network Connection and Network Upgrading 384
18.2.1 Shallow connection charges 384
18.2.2 Deep connection charges 387
18.2.3 Shallowish connection charges 388
18.2.4 Discussion of technical network limits 388
18.2.5 Summary of network interconnection and upgrade costs 389
18.3 System Operation Costs in a Deregulated Market 390
18.3.1 Primary control issues 391
18.3.2 Treatment of system operation costs 392
18.3.3 Secondary control issues 392
18.3.4 Electricity market aspects 395
18.4 Example: Nord Pool 395
18.4.1 The Nord Pool power exchange 396
18.4.2 Elspot pricing 397
18.4.3 Wind power and the power exchange 398
18.4.4 Wind power and the balancing market 403
18.5 Conclusions 408
References 409
Part C Future Concepts 411
19 Wind Power and Voltage Control 413
J. G. Slootweg, S. W. H. de Haan, H. Polinder and W. L. Kling
19.1 Introduction 413
19.2 Voltage Control 414
19.2.1 The need for voltage control 414
19.2.2 Active and reactive power 416
19.2.3 Impact of wind power on voltage control 417
19.3 Voltage Control Capabilities of Wind Turbines 420
19.3.1 Current wind turbine types 420
19.3.2 Wind turbine voltage control capabilities 421
19.3.3 Factors affecting voltage control 425
19.4 Simulation Results 425
19.4.1 Test system 425
19.4.2 Steady-state analysis 426
19.4.3 Dynamic analysis 428
19.5 Voltage Control Capability and Converter Rating 430
19.6 Conclusions 431
References 432
20 Wind Power in Areas with Limited Transmission Capacity 433
Julija Matevosyan
20.1 Introduction 433
20.2 Transmission Limits 434
20.2.1 Thermal limit 434
20.2.2 Voltage stability limit 435
Contents xv
20.2.3 Power output of wind turbines 438
20.2.4 Transient stability 439
20.2.5 Summary 439
20.3 Transmission Capacity: Methods of Determination 440
20.3.1 Determination of cross-border transmission capacity 440
20.3.2 Determination of transmission capacity within the country 441
20.3.3 Summary 442
20.4 Measures to Increase Transmission Capacity 442
20.4.1 ?Soft? measures 442
20.4.2 Possible reinforcement measures: thermal limit 443
20.4.3 Possible reinforcement measures: voltage stability limit 444
20.4.4 Converting AC transmission lines to DC for higher transmission ratings 444
20.5 Impact of Wind Generation on Transmission Capacity 445
20.6 Alternatives to Grid Reinforcement for the Integration of Wind Power 446
20.6.1 Regulation using existing generation sources 447
20.6.2 Wind energy spillage 447
20.6.3 Summary 457
20.7 Conclusions 458
References 458
21 Benefits of Active Management of Distribution Systems 461
Goran Strbac, Predrag Djapic?, Thomas Bopp and Nick Jenkins
21.1 Background 461
21.2 Active Management 462
21.2.1 Voltage-rise effect 462
21.2.2 Active management control strategies 464
21.3 Quantification of the Benefits of Active Management 465
21.3.1 Introduction 465
21.3.2 Case studies 466
21.4 Conclusions 476
References 476
22 Transmission Systems for Offshore Wind Farms 479
Thomas Ackermann
22.1 Introduction 479
22.2 General Electrical Aspects 481
22.2.1 Offshore substations 482
22.2.2 Redundancy 483
22.3 Transmission System to Shore 484
22.3.1 High-voltage alternating-current transmission 485
22.3.2 Line-commutated converter based high-voltage direct-current transmission 486
22.3.3 Voltage source converter based high-voltage direct-current transmission 488
22.3.4 Comparison 490
22.4 System Solutions for Offshore Wind Farms 497
22.4.1 Use of low frequency 497
22.4.2 DC solutions based on wind turbines with AC generators 498
22.4.3 DC solutions based on wind turbines with DC generators 498
22.5 Offshore Grid Systems 499
22.6 Alternative Transmission Solutions 500
22.7 Conclusions 500
xvi Contents
Acknowledgement 501
References 501
23 Hydrogen as a Means of Transporting and Balancing Wind Power Production 505
Robert Steinberger-Wilckens
23.1 Introduction 505
23.2 A Brief Introduction to Hydrogen 506
23.3 Technology and Efficiency 507
23.3.1 Hydrogen production 507
23.3.2 Hydrogen storage 508
23.3.3 Hydrogen transport 509
23.4 Reconversion to Electricity: Fuel Cells 510
23.5 Hydrogen and Wind Energy 512
23.6 Upgrading Surplus Wind Energy 514
23.6.1 Hydrogen products 516
23.7 A Blueprint for a Hydrogen Distribution System 516
23.7.1 Initial cost estimates 518
23.8 Conclusions 519
References 519
Part D Dynamic Modelling of Wind Turbines for power System Studies 523
24 Introduction to the Modelling of Wind Turbines 525
Hans Knudsen and J?rgen Nyga?rd Nielsen
24.1 Introduction 525
24.2 Basic Considerations regarding Modelling and Simulations 526
24.3 Overview of Aerodynamic Modelling 526
24.3.1 Basic description of the turbine rotor 527
24.3.2 Different representations of the turbine rotor 532
24.4 Basic Modelling Block Description of Wind Turbines 534
24.4.1 Aerodynamic system 535
24.4.2 Mechanical system 536
24.4.3 Generator drive concepts 536
24.4.4 Pitch servo 539
24.4.5 Main control system 539
24.4.6 Protection systems and relays 541
24.5 Per Unit Systems and Data for the Mechanical System 541
24.6 Different Types of Simulation and Requirements for Accuracy 546
24.6.1 Simulation work and required modelling accuracy 546
24.6.2 Different types of simulation 547
24.7 Conclusions 552
References 553
25 Reduced-order Modelling of Wind Turbines 555
J. G. Slootweg, H. Polinder and W. L. Kling
25.1 Introduction 555
25.2 Power System Dynamics Simulation 556
25.3 Current Wind Turbine Types 557
25.4 Modelling Assumptions 557
Contents xvii
25.5 Model of a Constant-speed Wind Turbine 559
25.5.1 Model structure 559
25.5.2 Wind speed model 559
25.5.3 Rotor model 562
25.5.4 Shaft model 564
25.5.5 Generator model 565
25.6 Model of a Wind Turbine with a Doubly fed Induction Generator 567
25.6.1 Model structure 567
25.6.2 Rotor model 568
25.6.3 Generator model 568
25.6.4 Converter model 570
25.6.5 Protection system model 572
25.6.6 Rotor speed controller model 573
25.6.7 Pitch angle controller model 574
25.6.8 Terminal voltage controller model 575
25.7 Model of a Direct drive Wind Turbine 576
25.7.1 Generator model 577
25.7.2 Voltage controller model 578
25.8 Model Validation 579
25.8.1 Measured and simulated model response 579
25.8.2 Comparison of measurements and simulations 582
25.9 Conclusions 584
References 584
26 High-order Models of Doubly-fed Induction Generators 587
Eva Centeno Lo?pez and Jonas Persson
26.1 Introduction 587
26.2 Advantages of Using a Doubly-fed Induction Generator 588
26.3 The Components of a Doubly-fed Induction Generator 588
26.4 Machine Equations 589
26.4.1 The vector method 590
26.4.2 Notation of quantities 592
26.4.3 Voltage equations of the machine 592
26.4.4 Flux equations of the machine 594
26.4.5 Mechanical equations of the machine 595
26.4.6 Mechanical equations of the wind turbine 597
26.5 Voltage Source Converter 597
26.6 Sequencer 599
26.7 Simulation of the Doubly-fed Induction Generator 599
26.8 Reducing the Order of the Doubly-fed Induction Generator 600
26.9 Conclusions 601
References 602
27 Full-scale Verification of Dynamic Wind Turbine Models 603
Vladislav Akhmatov
27.1 Introduction 603
27.1.1 Background 604
27.1.2 Process of validation 605
27.2 Partial Validation 607
27.2.1 Induction generator model 607
27.2.2 Shaft system model 611
xviii Contents
27.2.3 Aerodynamic rotor model 613
27.2.4 Summary of partial validation 618
27.3 Full-scale Validation 619
27.3.1 Experiment outline 619
27.3.2 Measured behaviour 621
27.3.3 Modelling case 622
27.3.4 Model validation 623
27.3.5 Discrepancies between model and measurements 625
27.4 Conclusions 625
References 626
28 Impacts of Wind Power on Power System Dynamics 629
J. G. Slootweg and W. L. Kling
28.1 Introduction 629
28.2 Power System Dynamics 630
28.3 Actual Wind Turbine Types 631
28.4 Impact of Wind Power on Transient Stability 632
28.4.1 Dynamic behaviour of wind turbine types 632
28.4.2 Dynamic behaviour of wind farms 636
28.4.3 Simulation results 638
28.5 Impact of Wind Power on Small Signal Stability 645
28.5.1 Eigenvalue?frequency domain analysis 645
28.5.2 Analysis of the impact of wind power on small signal stability 646
28.5.3 Simulation results 647
28.5.4 Preliminary conclusions 648
28.6 Conclusions 650
References 651
29 Aggregated Modelling and Short-term Voltage Stability of Large Wind Farms 653
Vladislav Akhmatov
29.1 Introduction 653
29.1.1 Main outline 654
29.1.2 Area of application 655
29.1.3 Additional requirements 655
29.2 Large Wind Farm Model 656
29.2.1 Reactive power conditions 657
29.2.2 Faulting conditions 658
29.3 Fixed-speed Wind Turbines 658
29.3.1 Wind turbine parameters 661
29.3.2 Stabilisation through power ramp 661
29.4 Wind Turbines with Variable Rotor Resistance 663
29.5 Variable-speed Wind Turbines with Doubly-fed Induction Generators 665
29.5.1 Blocking and restart of converter 667
29.5.2 Response of a large wind farm 668
29.6 Variable-speed Wind Turbines with Permanent Magnet Generators 670
29.7 A Single Machine Equivalent 672
29.8 Conclusions 673},
  comment   = {System level book on wind power: economics, electrical networks, storage, forecasting, markets, aeronautic and electro-mechanic design of turbines. Seems to have been used for some classes.},
  file      = {Ackermann05WindPowSysBook.pdf:Ackermann05WindPowSysBook.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.01.28},
  url       = {http://iie.fing.edu.uy/simsee/curso2010/wind_power_in_power_systems.pdf},
}

@Article{Ohara2010noLogCountDat,
  author    = {O’hara, Robert B and Kotze, D Johan},
  title     = {Do not log-transform count data},
  journal   = {Methods in ecology and Evolution},
  year      = {2010},
  volume    = {1},
  number    = {2},
  pages     = {118--122},
  abstract  = {1. Ecological count data (e.g. number of individuals or species) are often log‐transformed to satisfy parametric test assumptions.

2. Apart from the fact that generalized linear models are better suited in dealing with count data, a log‐transformation of counts has the additional quandary in how to deal with zero observations. With just one zero observation (if this observation represents a sampling unit), the whole data set needs to be fudged by adding a value (usually 1) before transformation.

3. Simulating data from a negative binomial distribution, we compared the outcome of fitting models that were transformed in various ways (log, square root) with results from fitting models using quasi‐Poisson and negative binomial models to untransformed count data.

4. We found that the transformations performed poorly, except when the dispersion was small and the mean counts were large. The quasi‐Poisson and negative binomial models consistently performed well, with little bias.

5. We recommend that count data should not be analysed by log‐transforming it, but instead models based on Poisson and negative binomial distributions should be used.

Key-words: generalized linear models, linear models, overdispersion, Poisson, transformation},
  comment   = {It's better to model count data with a quasi-poission (has an adjustable over-dispersion param)  or a neg. binomial GLM than it is to log transform the data to make it normal and then (I think) use linear regression.  The log transform will blow up on zero-count-values, and even if it didn't, the resulting mean estimate will be biased (although where/how the bias is calcualted is not terribly clear).

Estimating the mean of true count data -- here from a neg binomial dist -- is best done by a quasi-poisson or neg. binomial glm.  I guess what people might commonly do is to notice that the data is not normally distributed, and therefore, they would log transform it before estimating the mean (and then inverse transforming?)  Anyway, using the glms yield much better results.

On this data a quasi poisson is just as good as a neg binomial, but the authors say that you can't expect that in real life.  Then the give a technique for chosing between the two models, as well as references to a couple of other papers on the topic.},
  file      = {:Ohara2010noLogCountDat.pdf:PDF},
  publisher = {Wiley Online Library},
  url       = {https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.2041-210X.2010.00021.x},
}

@Article{Gregorcic07LocalIDgaussProc,
  author    = {Gregorcic, G. and Lightbody, G.},
  title     = {Local Model Network Identification With {Gauss}ian Processes},
  journal   = {Neural Networks, IEEE Transactions on},
  year      = {2007},
  volume    = {18},
  number    = {5},
  pages     = {1404--1423},
  month     = sep,
  issn      = {1045-9227},
  abstract  = {A {Bayes}ian {Gauss}ian process (GP) modeling approach has recently been introduced to model-based control strategies. The estimate of the variance of the predicted output is the most useful advantage of GPs in comparison to neural networks (NNs) and fuzzy models. However, the GP model is computationally demanding and nontransparent. To reduce the computation load and increase transparency, a local linear GP model network is proposed in this paper. The proposed methodology combines the local model network principle with the GP prior approach. A novel algorithm for structure determination and optimization is introduced, which is widely applicable to the training of local model networks. The modeling procedure of the local linear GP (LGP) model network is demonstrated on an example of a nonlinear laboratory scale process rig.},
  comment   = {Local linear models with Gaussian Process Prior. Automatically clusters the model (\& decides how many?)},
  doi       = {10.1109/TNN.2007.895825},
  file      = {Gregorcic07LocalIDgaussProc.pdf:Gregorcic07LocalIDgaussProc.pdf:PDF;Gregorcic07LocalIDgaussProc.pdf:Gregorcic07LocalIDgaussProc.pdf:PDF},
  keywords  = {Bayes methods, Gaussian processes, neurocontrollers, nonlinear control systemsBayesian Gaussian process, local model network identification, model-based control strategies, neural networks},
  owner     = {scotto},
  timestamp = {2008.10.04},
  url       = {http://ieeexplore.ieee.org.offcampus.lib.washington.edu/search/srchabstract.jsp?arnumber=4298114&isnumber=4298102&punumber=72&k2dockey=4298114@ieeejrns&query=((local+model+network+identification+with+gaussian+processes)%3Cin%3Emetadata)&pos=0&access=no},
}

@Article{Schuhen12ensembleMOS,
  author    = {Schuhen, Nina and Thorarinsdottir, Thordis L and Gneiting, Tilmann},
  title     = {Ensemble model output statistics for wind vectors},
  journal   = {arXiv},
  year      = {2012},
  abstract  = {A bivariate ensemble model output statistics (EMOS) technique for the postprocessing of
ensemble forecasts of two-dimensional wind vectors is proposed, where the postprocessed
probabilistic forecast takes the form of a bivariate normal probability density function. The
postprocessed means and variances of the wind vector components are linearly bias-corrected
versions of the ensemble means and ensemble variances, respectively, and the conditional correlation between the wind components is represented by a trigonometric function of the ensemble
mean wind direction. In a case study on 48-hour forecasts of wind vectors over the
North American Pacific Northwest with the University of Washington Mesoscale Ensemble,
the bivariate EMOS density forecasts were calibrated and sharp, and showed considerable improvement over the raw ensemble and reference forecasts, including ensemble copula coupling.},
  comment   = {DWD's ensemble MOS. Does wind vector ensembles, which might be interesting for Use for an Eweline baseline?},
  file      = {Schuhen12ensembleMOS.pdf:Schuhen12ensembleMOS.pdf:PDF},
  groups    = {Ensemble, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2013.04.22},
}

@InProceedings{Solomatine04adaBoostContin,
  author    = {Solomatine, D.P. and Shrestha, D.L.},
  title     = {AdaBoost.RT: a boosting algorithm for regression problems},
  booktitle = {Proceedings of the 2004 IEEE International Joint Conference on Neural Networks},
  year      = {2004},
  volume    = {2},
  pages     = {1163--11682},
  abstract  = {A boosting algorithm, AdaBoost.RT, is proposed for regression problems. The idea is to filter out examples with a relative estimation error that is higher than the pre-set threshold value, and then follow the AdaBoost procedure. Thus it requires to select the sub-optimal value of relative error threshold to demarcate predictions from the predictor as correct or incorrect. Some experimental results using the M5 model tree as a weak learning machine for benchmark data sets and for hydrological modeling are reported, and compared to other boosting methods, bagging and artificial neural networks, and to a single M5 model tree. AdaBoost.Rt is proved to perform better on most of the considered data sets.},
  comment   = {A continous output valued version of adaboost.

Could use with the feature construction algorithm in Lillywhite13featCnstrct

Kankanala14adaBoostPlus might be an improvement, also Tian09newAdaboost_RT and others...},
  doi       = {10.1109/IJCNN.2004.1380102},
  file      = {Solomatine04adaBoostContin.pdf:Solomatine04adaBoostContin.pdf:PDF},
  issn      = {1098-7576},
  keywords  = {estimation theory;hydrology;learning (artificial intelligence);neural nets;pattern classification;regression analysis;trees (mathematics);AdaBoost.RT;M5 model tree;artificial neural networks;benchmark data sets;boosting algorithm;estimation error filtering;hydrological modeling;preset threshold value;regression problems;relative error threshold;weak learning machine;Accuracy;Artificial neural networks;Bagging;Boosting;Classification algorithms;Error correction;Estimation error;Filtering;Machine learning;Training data},
  owner     = {sotterson},
  timestamp = {2014.01.21},
}

@Article{Palmer97ECMWFensemblePredSys,
  author    = {T N Palmer and J Barkmeijer and R Buizza and T Petroliagis},
  title     = {ECMWF Ensemble Prediction System},
  year      = {1997},
  volume    = {4},
  pages     = {301--304},
  doi       = {10.1017/S1350482797000649},
  url       = {http://journals.cambridge.org/action/displayAbstract;jsessionid=1E77E49C35CA06C6FBD791EEDCA78ECA.tomcat1?fromPage=online&aid=48055},
  abstract  = {A brief description of the ECWMF Ensemble Prediction System is given, including the method for producing initial perturbations, products and verification, and planned developments.},
  journal   = {Meteorological Applications},
  owner     = {sotterson},
  timestamp = {2008.07.03},
}

@InProceedings{Pavlovski17skipTrainRgrssnTempNet,
  author    = {Martin Pavlovski and Fang Zhou and Ivan Stojkovic and Ljupco Kocarev and Zoran Obradovic},
  title     = {Adaptive Skip-Train Structured Regression Temporal Networks},
  booktitle = {ECML PKDD. European Conference On Machine Learning \& Principles And Practice Of Knowledge Discovery In Databases},
  year      = {2017},
  abstract  = {A broad range of high impact applications involve learning
a predictive model in a temporal network environment. In weather fore-
casting, predicting effectiveness of treatments, outcomes in healthcare
and in many other domains, networks are often large, while intervals
between consecutive time moments are brief. Therefore, models are re-
quired to forecast in a more scalable and efficient way, without compro-
mising accuracy. The Gaussian Conditional Random Field (GCRF) is a
widely used graphical model for performing structured regression on net-
works. However, GCRF is not applicable to large networks and it cannot
capture different network substructures (communities) since it consid-
ers the entire network while learning. In this study, we present a novel
model, Adaptive Skip-Train Structured Ensemble (AST-SE), which is a
sampling-based structured regression ensemble for prediction on top of
temporal networks. AST-SE takes advantage of the scheme of ensemble
methods to allow multiple GCRFs to learn from several subnetworks.
The proposed model is able to automatically skip the entire training or
some phases of the training process. The prediction accuracy and effi-
ciency of AST-SE were assessed and compared against alternatives on
synthetic temporal networks and the H3N2 Virus Influenza network. The
obtained results provide evidence that (1) AST-SE is ∼140 times faster
than GCRF as it skips retraining quite frequently; (2) It still captures
the original network structure more accurately than GCRF while op-
erating solely on partial views of the network; (3) It outperforms both
unweighted and weighted GCRF ensembles which also operate on sub-
networks but require retraining at each timestep},
  comment   = {And adaptive structured graph regression that updates all or an automatically chosen part of itself on each step.  Is fast and can handle big networks.   I talked to the ECML PKDD 2017 poster's author and it seemed to me that this could be applied to learning spatial dependencies across wind farms.  On thing I'm not sure of is if it can retain past lessons (weather regimes) or if it wipes them out as it adapts.    But since it updates substructures, maybe old regime info could be retained in the non-updated part of the graph?},
  file      = {:Pavlovski17skipTrainRgrssnTempNet.pdf:PDF},
  url       = {http://ecmlpkdd2017.ijs.si/program.html},
}

@Article{Stark06partCrossCorrResAmbig,
  author    = {Eran Stark and Rotem Drori and Moshe Abeles},
  title     = {Partial cross-correlation analysis resolves ambiguity in the encoding of multiple movement features},
  journal   = {Journal of Neurophysiology},
  year      = {2006},
  volume    = {95},
  number    = {3},
  pages     = {1966--75},
  abstract  = {A classical question in neuroscience is which features of a stimulus or of an action are represented in brain activity. When several features are interdependent either at a given point in time or at distinct points in time, neural activity related to one feature appears to be correlated with other features. Thus techniques that simultaneously consider multiple features cannot account for delayed interdependencies between features. The result is an ambiguity with respect to the encoded features. Here, we resolve this ambiguity by applying a novel statistical method based on partial cross-correlations. The method yields estimates of linear correlations between neural activity and a given feature that are not affected by linear correlations with other features at multiple time delays. The method also provides a graphical output measured on a scale that allows for comparisons between different features, neurons, and experiments. We use real movement data and neural activity simulated according to a wide range of tuning models to illustrate the method. When applied to real neural activity, the procedure yields results that indicate which of the considered features the neural activity is related to and at what time delays.},
  comment   = {Figures out correct lags for multivariate partial cross correlation},
  doi       = {10.1152/jn.00981},
  file      = {Stark06partCrossCorrResAmbig.pdf:Stark06partCrossCorrResAmbig.pdf:PDF;Stark06partCrossCorrResAmbig.pdf:Stark06partCrossCorrResAmbig.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.03.10},
}

@Article{Hild06featExtractInfoLrn,
  author    = {Hild, K.E. and Erdogmus, D. and Torkkola, K. and Principe, J.C.},
  title     = {Feature extraction using information-theoretic learning},
  journal   = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year      = {2006},
  volume    = {28},
  number    = {9},
  pages     = {1385--1392},
  month     = sep,
  issn      = {0162-8828},
  abstract  = {A classification system typically consists of both a feature extractor (preprocessor) and a classifier. These two components can be trained either independently or simultaneously. The former option has an implementation advantage since the extractor need only be trained once for use with any classifier, whereas the latter has an advantage since it can be used to minimize classification error directly. Certain criteria, such as minimum classification error, are better suited for simultaneous training, whereas other criteria, such as mutual information, are amenable for training the feature extractor either independently or simultaneously. Herein, an information-theoretic criterion is introduced and is evaluated for training the extractor independently of the classifier. The proposed method uses nonparametric estimation of Renyi's entropy to train the extractor by maximizing an approximation of the mutual information between the class labels and the output of the feature extractor. The evaluations show that the proposed method, even though it uses independent training, performs at least as well as three feature extraction methods that train the extractor and classifier simultaneously},
  comment   = {Feature extraction using quadtratic mutual information
* could be applied to technique in Francois07resampParamFreeFeatSel},
  doi       = {10.1109/TPAMI.2006.186},
  file      = {Hild06featExtractInfoLrn.pdf:Hild06featExtractInfoLrn.pdf:PDF;Hild06featExtractInfoLrn.pdf:Hild06featExtractInfoLrn.pdf:PDF},
  keywords  = {estimation theory, feature extraction, information theory, pattern classificationfeature extraction, information-theoretic learning, minimum classification error, mutual information, nonparametric estimation},
  owner     = {sotterson},
  timestamp = {2009.02.11},
}

@Article{Beaver06clustWindRegime,
  author    = {Scott Beaver and Ahmet Palazoglu},
  title     = {Cluster Analysis of Hourly Wind Measurements to Reveal Synoptic Regimes Affecting Air Quality},
  year      = {2006},
  volume    = {45},
  number    = {12},
  pages     = {17101726},
  doi       = {DOI: 10.1175/JAM2437.1},
  abstract  = {A clustering algorithm is developed to study hourly, ground-level wind measurements obtained from a network of monitoring stations positioned throughout the San Francisco Bay Area of California. A statistical model based on principal components analysis (or empirical orthogonal functions) is used to cluster these autocorrelated and cross-correlated observations. Patterns at the synoptic time scale are isolated by using windowing and scaling operations to treat the data. Four dominant wind patterns that affect air quality are identified for the study region, and summer days from 8 yr of historical data are assigned to these modes. One cluster captures a high pressure system over the western United States, the anticyclonic winds of which block the typical marine flow through the study region. Differential heating convects a polluted air mass to a nearby valley in which severe episodes of higher-than-average ozone composition occur. A second pattern represents a seasonal, offshore ridge of high pressure that reduces marine flow and produces a shallow boundary layer. These two clusters capture distinct meteorological regimes that are favorable to ozone buildup and account for nearly all ?exceedances? of ozone air-quality standards. Two other clusters have strong marine flow that inhibits ozone buildup.},
  file      = {Beaver06clustWindRegime.pdf:Beaver06clustWindRegime.pdf:PDF;Beaver06clustWindRegime.pdf:Beaver06clustWindRegime.pdf:PDF},
  journal   = {Journal of Applied Meteorology and Climatology},
  owner     = {sotterson},
  timestamp = {2009.05.14},
}

@Article{Estevez09normMutInfoFeatSel,
  author    = {Estevez, P.A. and Tesmer, M. and Perez, C.A. and Zurada, J.M.},
  title     = {Normalized Mutual Information Feature Selection},
  journal   = {Neural Networks, IEEE Transactions on},
  year      = {2009},
  volume    = {20},
  number    = {2},
  pages     = {189--201},
  month     = feb,
  issn      = {1045-9227},
  abstract  = {A filter method of feature selection based on mutual information, called normalized mutual information feature selection (NMIFS), is presented. NMIFS is an enhancement over Battiti's MIFS, MIFS-U, and mRMR methods. The average normalized mutual information is proposed as a measure of redundancy among features. NMIFS outperformed MIFS, MIFS-U, and mRMR on several artificial and benchmark data sets without requiring a user-defined parameter. In addition, NMIFS is combined with a genetic algorithm to form a hybrid filter/wrapper method called GAMIFS. This includes an initialization procedure and a mutation operator based on NMIFS to speed up the convergence of the genetic algorithm. GAMIFS overcomes the limitations of incremental search algorithms that are unable to find dependencies between groups of features.},
  comment   = {Explains why normalize MI for discrete variables * MI is between min(Hx,Hy) \& max(Hx,Hy) -- so it's biased towards variables w/ more possible values -- can show that can make MI higher by adding random, meaningless codes * but how does this relate to dimensions?, as in Kraskov clustering? * uses MI to select variable lags},
  doi       = {10.1109/TNN.2008.2005601},
  file      = {Estevez09normMutInfoFeatSel.pdf:Estevez09normMutInfoFeatSel.pdf:PDF},
  keywords  = {Battiti MIFS-U method;filter method;genetic algorithm;hybrid filter;incremental search algorithm;initialization procedure;mRMR method;mutation operator;normalized mutual information feature selection;wrapper method;feature extraction;genetic algorithms;search problems;},
  owner     = {scotto},
  timestamp = {2011.05.21},
}

@Article{Pya15shapeCnstrAddMdl,
  author    = {Pya, Natalya and Wood, Simon N},
  title     = {Shape constrained additive models},
  journal   = {Statistics and Computing},
  year      = {2015},
  volume    = {25},
  number    = {3},
  pages     = {543--559},
  abstract  = {A framework is presented for generalized additive modelling under shape constraints on the component functions of the linear predictor of the GAM. We represent shape constrained model components by mildly non-linear extensions of P-splines. Models can contain multiple shape constrained and unconstrained terms as well as shape constrained multi-dimensional smooths. The constraints considered are on the sign of the first or/and the second derivatives of the smooth terms. A key advantage of the approach is that it facilitates efficient estimation of smoothing parameters as an integral part of model estimation, via GCV or AIC, and numerically robust algorithms for this are presented. We also derive simulation free approximate Bayesian confidence intervals for the smooth components, which are shown to achieve close to nominal coverage probabilities. Applications are presented using real data examples including the risk of disease in relation to proximity to municipal incinerators and the association between air pollution and health.
Keywords
Monotonic smoothing Convex smoothing Generalized additive model P-splines},
  comment   = {An extension of penalized splines (Eilers96FlexSmthBsplnPnlty, Eilers10SplnsKntsPnlties) to multi-dimensions and a big selection of possible onstraints beyond roughness e.g. monotonicity.
  The intro has a really short bit about just scalar monotonic penalized splines -- which is all I need for distribution regression cdf interpolation.

* nice table w/ coefficients for many constraint configurations
* Supplemental Material with derivations also attached as PDF
* the bummer is that it ends up having an iterative model that they had to mess with to get convergence
 - mulitple starts, etc.
 - but it seems to have worked in the end
 - maybe for the simple scalar monotonic case, it's less tricky?
* expression for effective degrees of freedom on p. 8.  Use for scanning lambda, as in ridge regression?

Implemented in R scam library.  First scam recommendation I saw was here:
http://stackoverflow.com/questions/28582145/how-to-fit-a-smooth-line-to-some-points-but-preserve-monotonicity},
  file      = {Paper:Pya15shapeCnstrAddMdl.pdf:PDF;Supplemental Material:Pya15shapeCnstrAddMdl_suppMat.pdf:PDF},
  publisher = {Springer},
  url       = {http://link.springer.com/article/10.1007%2Fs11222-013-9448-7},
}

@Article{Politis01asympSubsamp,
  author    = {Politis, DN and Romano, JP and Wolf, M},
  title     = {On the asymptotic theory of subsampling},
  journal   = {Statistica Sinica},
  year      = {2001},
  volume    = {11},
  number    = {4},
  pages     = {1105--1124},
  issn      = {1017-0405},
  abstract  = {A general approach to constructing confidence intervals by subsampling was presented in Politis and Romano (1994). The crux of the method is recomputing a statistic over subsamples of the data, and these recomputed values are used to build up an estimated sampling distribution. The method works under extremely weak conditions, it applies to independent, identically distributed (i.i.d.) observations as well as to dependent data situations, such as time series (possibly nonstationary), random fields, and marked point processes. In this article, we present some theorems showing: a new construction for confidence intervals that removes a previous condition, a general theorem showing the validity of subsampling for data-dependent choices of the block size, and a general theorem for the construction of hypothesis tests (not necessarily derived from a confidence interval construction). The arguments apply to both the i.i.d. setting and the dependent data case.},
  comment   = {How to pick the m in m out of n subsampling bootstrap * useful for Geyer06subSampBootStrap * seems to be an alternative to: Bickel08choiceOfMsubsampBtstrp * recommended by: Simar10mofnBtstrp * I'm not sure, but maybe Jach11subSampInfTS is a better method for subsamp size selection --- since Politis was a coauthor of that paper and the method here wasn't used.},
  file      = {Politis01asympSubsamp.pdf:Politis01asympSubsamp.pdf:PDF},
  keywords  = {STATISTICS, BOOTSTRAP, JACKKNIFE, TESTS, confidence intervals, data-dependent block size choice, hypothesis tests, large sample theory, resampling},
  owner     = {scot},
  timestamp = {2011.06.28},
}

@Article{Pinson10condPredInt,
  author    = {Pinson, P. and Kariniotakis, G.},
  title     = {Conditional Prediction Intervals of Wind Power Generation},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2010},
  volume    = {25},
  number    = {4},
  pages     = {1845--1856},
  issn      = {0885-8950},
  abstract  = {A generic method for the providing of prediction intervals of wind power generation is described. Prediction intervals complement the more common wind power point forecasts, by giving a range of potential outcomes for a given probability, their so-called nominal coverage rate. Ideally they inform of the situation-specific uncertainty of point forecasts. In order to avoid a restrictive assumption on the shape of forecast error distributions, focus is given to an empirical and nonparametric approach named adapted resampling. This approach employs a fuzzy inference model that permits to integrate expertise on the characteristics of prediction errors for providing conditional interval forecasts. By simultaneously generating prediction intervals with various nominal coverage rates, one obtains full predictive distributions of wind generation. Adapted resampling is applied here to the case of an onshore Danish wind farm, for which three point forecasting methods are considered as input. The probabilistic forecasts generated are evaluated based on their reliability and sharpness, while compared to forecasts based on quantile regression and the climatology benchmark. The operational application of adapted resampling to the case of a large number of wind farms in Europe and Australia among others is finally discussed.},
  booktitle = {Power Systems, IEEE Transactions on},
  comment   = {Bootstrap model of point forecast errors produces a probabilistic forecast. Manually picked segmentation and fuzzy rules weight bootstrap samples. Could be improved.

* is a generic error model, for any point forecast
* adaptive resample models whole dist., avoids quantile crossing coming from multi-model quantile regression
* actual alg. works only on power prediction.
-- Seems like should at least include wind speed so can tell where they're at on the power curve
-- my guess is that they couldn't tune the fuzzy rules in 2D
* prob forecast distribution by sampling past forecast errors, centering them about the median
-- if past errors come from sliding window, the window may not contain relevant samples for bootstrap
-- solution: save historical sample sets from many forecast conditions
-- draw samples from these sets, and weight by fuzzy logic (weight ~ forecast condition match)
-- note: hand tuned fuzzy rules and set partitioning would be almost impossible for high dim inputs
---- e.g. offsite observations and multiple lags
---- I think this needs clustering and data-based tuning.

RESULTS
* tested via reliability and sharpness, but I couldn't fully understand either explanation
* the 3 test point forecasts were equally reliable
* but the approach had better skill on some point forecast methods than others
-- for the M2 model, results were the same as for quantile regression
-- mostly, though, this approach had better skill than QR
-- they speculate that results will also vary with the site being forecasted and weather condition

CONCLUSIONS
* adaptive resampling should be run after multiple point forecasts are aggregated
---- not individually on each one of them, with the densities being aggregated
* in future should skip pt. frcsts and directly model pred. dists.


WAYS I THINK THIS COULD BE IMPROVED
1.) Better bootstrap subsample size, replacement: Quantilies come from empirical c.d.f. which comes from simple counts of samples -- without replacement. But full sample bootstrap is a known bad way to est a dist. max (and min?) with replacement. When they do subsampling, it's not clear how they picked the subsample size. In sum, they need an approach something like Geyer06subSampBootStrap
2.) Fix problems with censoring. They have to clip (censor) out-of-bounds predictions. Could censoring problem could be reduced by something in Chay01semiParmCensRgrsn ?
3.) Bootstrap weighting should be statistically principled, probably an EM-learned mixture model By weighting subsamples, they're essentially assuming that the data comes from a mixture, but instead of probabilstic mixture weights, they're using hand-tuned fuzzy weights. Instead, they should use a real mixture model. I think this could be done w/ EM, and still retain the advantages of a bootstrap approach. An EM mixture approach would solve both the weighting and the partitioning in a statistically principled and data-driven manner.

See also Pinson04OonlinePredRiskWind},
  doi       = {10.1109/TPWRS.2010.2045774},
  file      = {Pinson10condPredInt.pdf:Pinson10condPredInt.pdf:PDF},
  groups    = {Read, PointDerived, doReadWPV_1},
  keywords  = {fuzzy reasoning, load forecasting, production engineering computing, regression analysis, wind power, wind power plants, adapted resampling, climatology benchmark, conditional prediction intervals, forecast error distributions, fuzzy inference model, nominal coverage rate, onshore wind farm, quantile regression, situation-specific uncertainty, wind power generation, wind power point forecasts},
  owner     = {scot},
  timestamp = {2010.12.01},
}

@InProceedings{Coates11ntwkUnSupFtLrn,
  author    = {Coates, Adam and Ng, Andrew Y and Lee, Honglak},
  title     = {An analysis of single-layer networks in unsupervised feature learning},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  year      = {2011},
  pages     = {215--223},
  abstract  = {A great deal of research has focused on algorithms
for learning features from unlabeled
data. Indeed, much progress has been
made on benchmark datasets like NORB and
CIFAR by employing increasingly complex
unsupervised learning algorithms and deep
models. In this paper, however, we show that
several simple factors, such as the number of
hidden nodes in the model, may be more important
to achieving high performance than
the learning algorithm or the depth of the
model. Specifically, we will apply several offthe-
shelf feature learning algorithms (sparse
auto-encoders, sparse RBMs, K-means clustering,
and Gaussian mixtures) to CIFAR,
NORB, and STL datasets using only singlelayer
networks. We then present a detailed
analysis of the effect of changes in the model
setup: the receptive field size, number of hidden
nodes (features), the step-size (?stride?)
between extracted features, and the effect
of whitening. Our results show that large
numbers of hidden nodes and dense feature
extraction are critical to achieving high
performance?so critical, in fact, that when
these parameters are pushed to their limits,
we achieve state-of-the-art performance on
both CIFAR-10 and NORB using only a single
layer of features. More surprisingly, our
best performance is based on K-means clustering,
which is extremely fast, has no hyperparameters
to tune beyond the model structure
itself, and is very easy to implement. Despite
the simplicity of our system, we achieve
accuracy beyond all previously published results
on the CIFAR-10 and NORB datasets
(79.6\% and 97.2\% respectively).},
  comment   = {Basis learning less important that tuning of the encoder that does "deep learning" -- said to obsolete deep autoencoding. Good for regime learning?

Blogger who said it obsoleted deep learning: http://danluu.com/linear-hammer/
2 other papers he said obsoleted deep autoencoding:
Ngiam11sparseFiltFtLrn, Coates11encodeTrnVQ

For regime learning, compare with adaboost methods, starting with Lillywhite13featCnstrct and the papers it references.},
  file      = {Coates11ntwkUnSupFtLrn.pdf:Coates11ntwkUnSupFtLrn.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.01.28},
  url       = {http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf},
}

@InProceedings{Dijck06speedFeatSelMutInf,
  author      = {Gert Van Dijck and Marc M. Van Hulle},
  title       = {Speeding Up the Wrapper Feature Subset Selection in Regression by Mutual Information Relevance and Redundancy Analysis.},
  booktitle   = {International Conference on Artificial Neural Networks (ICANN)},
  year        = {2006},
  editor      = {Stefanos D. Kollias and Andreas Stafylopatis and Wlodzislaw Duch and Erkki Oja},
  volume      = {4131},
  series      = {Lecture Notes in Computer Science},
  pages       = {31--40},
  publisher   = {Springer},
  abstract    = {A hybrid filter/wrapper feature subset selection algorithm for regression is proposed. First, features are filtered by means of a relevance and redundancy filter using mutual information between regression and target variables. We introduce permutation tests to find statistically significant relevant and redundant features. Second, a wrapper searches for good candidate feature subsets by taking the regression model into account. The advantage of a hybrid approach is threefold. First, the filter provides interesting features independently from the regression model and, hence, allows for an easier interpretation. Secondly, because the filter part is computationally less expensive, the global algorithm will faster provide good candidate subsets compared to a stand-alone wrapper approach. Finally, the wrapper takes the bias of the regression model into account, because the regression model guides the search for optimal features. Results are shown for the 'Boston housing' and 'orange juice' benchmarks based on the multilayer perceptron regression model.},
  comment     = {Thin w/ sig. thresh MI filter, cluster \& select representative for each cluster, then do wrapper selection w/ MLP regressor

Relevance
* means sufficiently more mutual info w/ output than the null hypothesis of independence, (H0)
* MI for H0: MI with a variable's time order permuted.
* permutation significance test may be better explained in Francois06permTestMutInf

Redundant feature removal
* done after relevance thresholding
* agglomerative cluster w/ mutual info distance function
* they do HUGE reduction: 700 --> 13 or 5 features
* IDEA for wind: use HMM agglom clust to enforce some temporal continuity

Final wrapper search
* use MLP regressor
* genetic algorithm search
 -- oldish 2000 paper found this better than sequential search
 -- not sure this is still true or may have been problem specific

Problems for wind forecasting
1.) wind autocorr ==> some MI after permute. Maybe small?
2.) need dependence at lag
3.) dep. pattern won't be stationary

Solution for wind forecasting?
a.) clust 1\textsuperscript{st}, by crosscorr peak lag/mag since cheap.
b.) inside cluster, find sel by MI at lag, somehow

Related, if exists:   conf/icann/2006-1, Francois06permTestMutInfr
},
  date        = {2006-11-17},
  description = {dblp},
  doi         = {10.1007/11840817_4},
  file        = {Dijck06speedFeatSelMutInf.pdf:Dijck06speedFeatSelMutInf.pdf:PDF},
  groups      = {Read},
  isbn        = {3-540-38625-4},
  keywords    = {dblp },
  owner       = {sotterson},
  timestamp   = {2009.01.08},
  url         = {http://dblp.uni-trier.de/db/conf/icann/icann2006-1.html#DijckH06},
}

@Article{Thorarinsdottir11probWndNonHomGR,
  author       = {Thorarinsdottir, Thordis L. and Johnson, Matthew S.},
  title        = {Probabilistic Wind Gust Forecasting Using Nonhomogeneous Gaussian Regression},
  journal      = {Monthly Weather Review},
  year         = {2011},
  volume       = {140},
  number       = {3},
  pages        = {889--897},
  issn         = {0027-0644},
  abstract     = {A joint probabilistic forecasting framework is proposed for maximum wind speed, the probability of gust, and, conditional on gust being observed, the maximum gust speed in a setting where only the maximum wind speed forecast is available. The framework employs the nonhomogeneous Gaussian regression ({NGR}) statistical postprocessing method with appropriately truncated Gaussian predictive distributions. For wind speed, the distribution is truncated at zero, the location parameter is a linear function of the wind speed ensemble forecast, and the scale parameter is a linear function of the ensemble variance. The gust forecasts are derived from the wind speed forecast using a gust factor, and the predictive distribution for gust speed is truncated according to its definition. The framework is applied to 48-h-ahead forecasts of wind speed over the North American Pacific Northwest obtained from the University of Washington mesoscale ensemble. The resulting density forecasts for wind speed and gust speed are calibrated and sharp, and offer substantial improvement in predictive performance over the raw ensemble or climatological reference forecasts.},
  comment      = {An extreme (max) regression thing.  Use for ReWP?  Also an example of non-homogenious Gaussian Regression},
  date         = {2011-08-05},
  doi          = {10.1175/MWR-D-11-00075.1},
  file         = {Full Text PDF:Thorarinsdottir11probWndNonHomGR:PDF},
  owner        = {sotterson},
  shortjournal = {Mon. Wea. Rev.},
  timestamp    = {2016.11.10},
  urldate      = {2016-11-10},
}

@MastersThesis{Linnet05toolsWindTrade,
  author      = {Linnet, U.},
  title       = {Tools supporting wind energy trade in deregulated markets},
  year        = {2005},
  month       = jul,
  abstract    = {A large share of the wind energy produced in Scandinavia is sold at deregulated electricity markets. The main market, Elspot, is a day-ahead market where energy is sold up to 36 hours before delivery. Failure in delivering exactly the quantity which was sold results in a fine, called regulation cost. As wind energy comes from an uncontrollable energy source - the wind - producers can not always fulfil their sales obligations and must, therefore, often pay high regulation costs. In this thesis it is examined how producers can increase their profit by bidding on the market in such a way that the regulation cost is minimised. The methods developed rely on new production forecasts which provide better probabilistic information about the prediction uncertainty than many forecasting systems currently in use.
The problem is formulated in two different ways. One, originally presented by John B. Bremnes, where only a part of the market is included, gives a simple method that can be applied using only statistical tools. The other method is
more exible at the cost of complexity. It uses both statistics and stochastic programming. This method can be changed and applied in other markets with a structure different from that of the Scandinavian market, NordPool.
Keywords: Electricity market, Wind energy, NordPool, Quantile Regression, Stochastic Programming.},
  comment     = {Henrik's master's student describing how to reduce regulation costs on the nordpool markets. Uses probabilstic forecasts.

Could be a good example of Nordpool use.},
  file        = {Linnet05toolsWindTrade.pdf:Linnet05toolsWindTrade.pdf:PDF},
  groups      = {Use, CitaviImport1, doReadWPV_2},
  institution = {Department of Informatics and Mathematical Modelling (IMM) of the Technical University of Denmark (DTU)},
  keywords    = {market integration, trading},
  location    = {Lyngby, Denmark},
  owner       = {sotterson},
  timestamp   = {2013.09.26},
}

@Article{Pan10xferLrnSurvey,
  author    = {Sinno Jialin Pan and Qiang Yang},
  title     = {A Survey on Transfer Learning},
  journal   = IEEE_J_KDE,
  year      = {2010},
  volume    = {22},
  number    = {10},
  pages     = {1345--1359},
  abstract  = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the
same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For
example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain
of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge
transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In
recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing
and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we
discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask
learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning
research.
Index Terms: Transfer learning, survey, machine learning, data mining.},
  comment   = {General adaptation, maybe a way to adapt to a new NWP model without throwing away old training data.},
  doi       = {10.1109/TKDE.2009.191},
  file      = {Pan10xferLrnSurvey.pdf:Pan10xferLrnSurvey.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.07.24},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5288526},
}

@Unpublished{Cameron13instrVarEconCrsNotes,
  author    = {A. Colin Cameron},
  title     = {Instrumental Variables: Course Notes, Econometrics 240A: Winter 2013. U.C. Davis, Dept. of Economics},
  abstract  = {A major complication that is emphasized in microeconometrics is the possibility of
inconsistent parameter estimation due to endogenous regressors. Then regression
estimates measure only the magnitude of association, rather than the magnitude
and direction of causation which is needed for policy analysis.
The instrumental variables estimator provides a way to nonetheless obtain con-
sistent parameter estimates. This method, widely used in econometrics and rarely
used elsewhere, is conceptually difficult and easily misused.
We provide a lengthy expository treatment that defines an instrumental variable
and explains how the instrumental variables method works in a simple setting.},
  comment   = {Instrumental variables are a way of removing confounding due to correlated regression errors.  They don't necessarily improve upon OLS regression prediction accuracy (they might reduce "efficiency," the accuracy you get vs. # of samples) but they remove bias in the estimate of the linear causal effect of a regression input variable on y.  It seems like Graphical Models do the same thing but better: they can be nonlinear, and are inherently probabilistic.

Anyway, if you stick to linear and LSQ, this could be useful for ModernWindABS, where root causes are being detected from linear functions (at least in Dienst16AnomOffshrWnd)

OVERVIEW

OLS directly estimates y given x, computing dy/dx, and it does a good job of predicting y from x; it predicts the "total effect" in eq. (4.44).

But if the goal is to estimate the linear effect of x on y, OLS gives a biased result.  In this example and the example in Section 4.8.8, y is effected by both x and u (u is the error that results if you model y using only x).  If x is correlated with u, then OLS can reduce the y prediction error making use of that correlation, but that exaggerates the effect of x alone.

In order to improve the estimate of  the impact of x, you need to model the other variables that affect y and are correlated with x.  These are generally unknown, but their effect can be approximating by finding other variables that are both correlated with the x and uncorrelated with the error.  These are the instrumental variables.

OTHER DEFINITIONS
* exogenous variable: seems to mean something that is being assessed in isolation
  - e.g.  predictor variable the effect of which on the prediction output is being measured.
   - on p. 40, it's defined as the regression in puts that aren't correlated with the regression error, u
* endogenous variable: a thing you're not measuring that effects the system; a confounder, I think.
   - on p. 40, it's defined as variables that are correlated with u (not suitable for use as instruments)
* instrumental variable: a variable indended to reduce to bias caused by endogenous variables
* Wald Estimator: a regression done with a binary instrument.  also called the grouping estimator.  I guess it factors out categories.
* just-identifed: have exactly as many instrumental variables as have endogenous variables
  - actually, I'm not totally sure about this, see pp. 40-41
  - in this condiiton, must used the Instrumental Variables Estimator on p. 41
* over-identified: more instruments than K (the size of endogenous variables?)
   - In this condition, can use the Instrumental Variables Estimator
   - or can use the 2SLS (2 stage least squared) regresson
  - or use the White (1982) scheme: two-stage instrumental variables estimator (p. 43)},
  file      = {Cameron13instrVarEconCrsNotes.pdf:Cameron13instrVarEconCrsNotes.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2016.12.18},
  url       = {http://cameron.econ.ucdavis.edu/e240a/ch04iv.pdf},
}

@Article{Reikard08tempStateRampWind,
  author    = {Gordon Reikard},
  title     = {Using temperature and state transitions to forecast wind speed},
  journal   = {Wind Energy},
  year      = {2008},
  volume    = {11},
  number    = {r},
  pages     = {431--443},
  abstract  = {A major issue in forecasting wind speed is non-linear variability. The probability distribution of wind speed series shows heavy tails, while there are frequent state transitions, in which wind speed changes by large magnitudes, over relatively short time periods. These so-called large ramp events are one of the critical issues currently facing the wind energy community. Two forecasting algorithms are analyzed here. The first is a regression on lags, including temperature as a causal factor, with time-varying parameters. The second augments the first using state transition terms. The main innovation in state transition models is that the cumulative density function from regressions on the states is used as a right-hand side variable in the regressions for wind speed. These two methods are tested against a persistence forecast and several non-linear models, using eight hourly wind speed series. On average, these two models produce the best results. The state transition model improves slightly over the regression. However, the improvement achieved by both models relative to the persistence forecast is fairly small. These results argue that there are limits to the accuracy that can be achieved in forecasting wind speed data.},
  comment   = {Wind forecast w/ sliding linear regression on wind+temp and sorta-state switching via CDF beats non-linear methods; fractal limits?
* this is 1 hour ahead forecasting
* studies show wind,temp combo important for ramps
-- when cdf's are included (below) temp sensitivity in model drops sharply
-- temperature is forecasted ahead to time of prediction (I think, somehow using regression on lags)
* want to do state sensitive regression so ramps don't get washed out
* "state" is percentile of current wind velocity in 20 hour sliding window
-- just a term w/ a regression coeff (binary states didn't work, IDEA: use mixture w/ hidden variable?)
-- rest of inputs are not wind state sensitive
-- so not really state switching,
* diurnal stuff handled by a term w/ a 24 hr lag
-- could have used sinusoids; they just say this is simpler (no experimental comparison)
* does log-log for speed prediction w/ lagged spd and temp: WHY?
-- wouldn't speed (at least) be linear w/ speed?
-- Simonoff09transfRegrsn says log-log is for exponential relationships (like a power curve)

State switching sliding update linear regression better than:
* Kalman filter (especially bad
* GARCH/EGARH (middling)
* neural network (middling BUT the net isn't explained, for example, is it allowed to adapt?? (Or should it have learned to switch?))


Note: Kristin reviewed something by this guy; Ken may have recommended this article.

Fractality and Limits to Prediction
* there's evidence of fractality
* typically comes from multiplicative relationship between stochastic processes
* no known algorithm can predict when only outcome is observed
* what about the delay embedding stuff? See all the mutual info papers + Simon06UnfoldMeaningTimeSeriesClust

IDEAS/SOLUTIONS
* this paper is a good baseline for my first hour-ahead system
* add offsite observations * add NWP, like Kristin's paper
* diurnal detection or 24 hour ahead or sinusoidal features that are selected by feature selection?
* should "switching" be determined by wind only? How about wind+temperature
-- s/ look up refs 22-24 since they note relationship between temp and speed
-- suggests more complex automatic state algorithm could be better
-- mixture of regressors? output is is weighted by prob of being in a state like a GMM
-- i.e. is there an EM algorithm here?
* I thought I've seen papers on prediction of multipicative factors??

* better switching regression, maybe w/ Bayesian averaging (they reference a paper that does this)
* pre-trained states via clustering, or maybe multiple regressors w/ different inputs, derived from clustering
* seems like they might do better if had derivatives so could get dynamics into linear regression
-- like my 5-minute BPA stuff
-- esp. for temperature, which seems to be separately forecasted ahead (why not just lump it in?)},
  doi       = {10.1002/we.263},
  file      = {Reikard08tempStateRampWind.pdf:Reikard08tempStateRampWind.pdf:PDF;Reikard08tempStateRampWind.pdf:Reikard08tempStateRampWind.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2008.09.24},
  url       = {http://www3.interscience.wiley.com.offcampus.lib.washington.edu/journal/117868262/abstract},
}

@Article{Peirolo11infoGainFrcstScore,
  author    = {Peirolo, R.},
  title     = {Information gain as a score for probabilistic forecasts},
  journal   = {Meteorological Applications},
  year      = {2011},
  volume    = {18},
  number    = {1},
  pages     = {9--17},
  abstract  = {A measure of the information added by a probabilistic forecast to that contained in the climatological distribution is presented in this paper. This measure, called information gain, is mathematically closely related to the traditional ignorance score, but is more intuitive. Its advantages over other scores for probabilistic forecasts are also shown. The information gain score is tested on ECMWF ensemble forecasts of 500 hPa geopotential and 850 hPa temperature. The trends observed are in good agreement with those seen in other verification measures applied to the same data. In particular, the information gain decays with increasing lead time and increases over the years, in agreement with the improvement of the model. KEY WORDS forecast verification; information theory; intuitive accuracy measure},
  comment   = {Better than CRPS probabilistic forecast for evaluating probabilistic forecasts against climatology
* mainly, it's more sensitve to accurate variance prediction than CRPS
* IG has has easy to interpret props.},
  doi       = {10.1002/met.188/abstract;jsessionid=DF5F5E930E81400B76DBF2FB30BAAE76.d03t01},
  file      = {Peirolo11infoGainFrcstScore.pdf:Peirolo11infoGainFrcstScore.pdf:PDF},
  groups    = {Test, doReadWPV_2},
  owner     = {scot},
  publisher = {Wiley Online Library},
  timestamp = {2011.05.17},
}

@Article{Kugiumtzis13directCplngPMI,
  author    = {Kugiumtzis, Dimitris},
  title     = {Direct-coupling information measure from nonuniform embedding},
  journal   = {Physical Review E},
  year      = {2013},
  volume    = {87},
  number    = {6},
  pages     = {062918},
  abstract  = {A measure to estimate the direct and directional coupling in multivariate time series is proposed. The measure is an extension of a recently published measure of conditional mutual information from mixed embedding (MIME) for bivariate time series. In the proposed measure of partial MIME (PMIME), the embedding is on all observed variables and it is optimized in explaining the response variable. It is shown that PMIME detects correctly direct coupling and outperforms the (linear) conditional Granger causality and the partial transfer entropy. We demonstrate that PMIME does not rely on significance test and embedding parameters and the number of observed variables has no effect on its statistical accuracy; it may only slow the computations. The importance of these points is shown in simulations and in an application to epileptic multichannel scalp electroencephalograms.},
  comment   = {* Matlab: http://users.auth.gr/dkugiu/Software_en.html
* earlier paper by these guys picked Kraskov as the best MI estimator: Papana08EvalMutInfoDynSys
* some Kraskov MI stuff used here is explained  in more detail in Vlachos10nonUnifStSpcMI
* Vlachos10nonUnifStSpcMI explains why you'd want to use this instead of straight MI
* Vlachos10nonUnifStSpcMI explains the problem being solved much better than this paper.},
  doi       = {10.1103/PhysRevE.87.062918},
  file      = {Kugiumtzis13directCplngPMI.pdf:Kugiumtzis13directCplngPMI.pdf:PDF},
  publisher = {APS},
}

@InProceedings{Nielsen02windPowVarCoeff,
  author    = {Nielsen, Torben Skov and Nielsen, Henrik Aalborg and Madsen, Henrik},
  title     = {Prediction of wind power using time-varying coefficient functions},
  booktitle = {Proceedings of the XV IFAC World Congress},
  year      = {2002},
  abstract  = {A method for adaptive and recursive estimation in a class of non-linear autore-
gressive models with external input is proposed. The model class considered is conditionally
parametric ARX-models (CPARX-models), which is conventional ARX-models in which the
parameters are replaced by smooth, but otherwise unknown, functions of a low-dimensional
input process. These coef?cient-functions are estimated adaptively and recursively without
specifying a global parametric form, i.e. the method allows for on-line tracking of the
coef?cient-functions. The usefulness of the method is illustrated using prediction of power
production from wind farms as an example. A CPARX model for predicting the power pro-
duction is suggested and the coef?cient-functions are estimated using the proposed method.
The new models are evaluated for ?ve wind farms in Denmark as well as one wind farm in
Spain. It is shown that the predictions based on conditional parametric models are superior to
the predictions obtained by previously identi?ed parametric models.
Keywords: Adaptive algorithms; Recursive algorithms; Estimation algorithms, Nonlinear
models; Time-varying systems; Nonparametric regression; Forecasts; Windmills.},
  comment   = {DTU/ENFOR's time-varying coeff wind power algorithm. Shows that boosting could also use it for quantile regression, etc., as in Fenske11Identifyingriskfactors},
  file      = {Nielsen02windPowVarCoeff.pdf:Nielsen02windPowVarCoeff.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.02},
  url       = {http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=14&ved=0CDMQFjADOAo&url=http%3A%2F%2Fwww.researchgate.net%2Fpublication%2F233400475_Prediction_of_Wind_Power_Using_Time-varying_Coefficient-functions%2Flinks%2F0046352d3a924a8901000000&ei=iYxWVMbqINDcaKf4gsAH&usg=AFQjCNHy4eFT92TCjnulF12WMl_p9iCPpQ&bvm=bv.78677474,d.ZWU&cad=rja},
}

@Article{Alfonso10monMutInfoOpt,
  author    = {Alfonso, L. and Lobbrecht, A. and Price, R.},
  title     = {Optimization of water level monitoring network in polder systems using information theory},
  journal   = {Water Resources Research},
  year      = {2010},
  volume    = {46},
  number    = {12},
  abstract  = {A method for siting water level monitors based on information theory measurements is presented. The first measurement is joint entropy, which evaluates the amount of information content that a monitoring set is able to collect, and the second measurement is total correlation, which evaluates the level of dependency or redundancy among monitors in the set. In order to find the most convenient set of places to put monitors from a large number of potential sites, a multiobjective optimization problem is posed under two different considerations: (1) taking into account the costs of placing new monitors and (2) considering the cost of placing monitors too close to hydraulic structures. In both cases, the joint entropy of the set is maximized and its total correlation is minimized. The costs are considered in terms of information theory units, for which additional terms affecting the objective functions are introduced. The proposed method is applied in a case study of the Delfland region, Netherlands. Results show that total correlation is an effective way to measure multivariate independency and that it must be combined with joint entropy to get results that cover a significant proportion of the total information content of the system. The maximization of joint entropy gives results that cover between 82 pct and 85 pct of the total information content. Citation: Alfonso, L., A. Lobbrecht, and R. Price (2010), Optimization},
  comment   = {feature sel. by information, kind of like mRmR, but w/ total correlation (avoids multivar. dist. est)
* decent explanation of total correlation (I was looking for explanation of total information)
* avoid giant joint entropy calc by computing total correlation instead

* want to persue two goals in optimal sensor location picking:
-- max Joint Entropy: max information
-- min Total Correlation: redundant information
* do so by thresholding out sites below a certain entropy (an approx)

* kind of like peng05featSelMutInfo but not exaclty (?)
* some discussion of how symbols can be concatenated w/ calc joint entropy, but I don't get it
* is total correlation and mutual information being confused?},
  doi       = {10.1029/2009wr008953},
  file      = {Alfonso10monMutInfoOpt.pdf:Alfonso10monMutInfoOpt.pdf:PDF},
  owner     = {scotto},
  publisher = {American Geophysical Union},
  timestamp = {2011.05.15},
  url       = {http://www.agu.org.globalproxy.cvt.dk/pubs/crossref/2010/2009WR008953.shtml},
}

@Article{Soder93rsrvWindHydroThrm,
  author    = {Soder, L.},
  title     = {Reserve margin planning in a wind-hydro-thermal power system},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {1993},
  volume    = {8},
  number    = {2},
  pages     = {564--571},
  month     = may,
  issn      = {0885-8950},
  abstract  = {A method for studying the effect of wind power on power system reserve margins, need of extra resources, etc. is presented. A conventional model for short-term operation planning for a hydrothermal power system is supplemented with a wind power model. The wind power model includes the forecast of total wind power generation and the uncertainty of the forecast. The conventional hydro-thermal model is extended to take into account load forecast uncertainty and reserve margins of the generation units. The requirements of instantaneous, fast, and slow reserves, depending on possible forced outages of thermal units and uncertain load and wind speed forecasts etc., are calculated together with the available capacities of the corresponding reserve type. The results include an estimation of whether there is a deficit or excess of instantaneous, fast, or slow reserves during each hour of the planning period. A numerical example shows an application of the method to the Swedish power system},
  comment   = {Simple unit commitment for wind/hydro/thermal system. Models net load forecast, after wind included.
* Models load/wind forecast errors as independent. Tryggvi and Henrik think they're correlated.},
  doi       = {10.1109/59.260826},
  file      = {Soder93rsrvWindHydroThrm.pdf:Soder93rsrvWindHydroThrm.pdf:PDF},
  keywords  = {Sweden;capacities;forced outages;hydrothermal power system;load forecasting;model;power stations;reserve margins;short-term operation planning;wind power;hydrothermal power systems;load forecasting;power system planning;wind power;wind power plants;},
  owner     = {scot},
  timestamp = {2011.06.09},
}

@Article{Zahn72fourDesc,
  author    = {Zahn, Charles T. and Roskies, Ralph Z.},
  title     = {{Fourier} Descriptors for Plane Closed Curves},
  journal   = {Computers, IEEE Transactions on},
  year      = {1972},
  volume    = {C-21},
  number    = {3},
  pages     = {269--281},
  month     = mar,
  issn      = {0018-9340},
  abstract  = {A method for the analysis and synthesis of closed curves in the plane is developed using the Fourier descriptors FD's of Cosgriff [1]. A curve is represented parametrically as a function of arc length by the accumulated change in direction of the curve since the starting point. This function is expanded in a Fourier series and the coefficients are arranged in the amplitude/phase-angle form. It is shown that the amplitudes are pure form invariants as well as are certain simple functions of phase angles. Rotational and axial symmetry are related directly to simple properties of the Fourier descriptors. An analysis of shape similarity or symmetry can be based on these relationships; also closed symmetric curves can be synthesized from almost arbitrary Fourier descriptors. It is established that the Fourier series expansion is optimal and unique with respect to obtaining coefficients insensitive to starting point. Several examples are provided to indicate the usefulness of Fourier descriptors as features for shape discrimination and a number of interesting symmetric curves are generated by computer and plotted out.},
  comment   = {Kinda like (the same as?) circular harmonics
* 1\textsuperscript{st} paper on Fourier Descriptors, I think
* use for lagged wind velocity basis?
* Jouan87fourDescCardiac may be a better explanation
* has R package: FourierDescriptors http://www.johnmyleswhite.com/notebook/2010/09/04/the-fourierdescriptors-package/},
  doi       = {10.1109/TC.1972.5008949},
  file      = {Zahn72fourDesc.pdf:Zahn72fourDesc.pdf:PDF},
  owner     = {scotto},
  timestamp = {2010.09.06},
}

@Article{Squire98cmplxDerivsCSD,
  author    = {Squire, William and Trapp, George},
  title     = {Using complex variables to estimate derivatives of real functions},
  journal   = {Siam Review},
  year      = {1998},
  volume    = {40},
  number    = {1},
  pages     = {110--112},
  abstract  = {A method to approximate derivatives of real functions using complex variables
which avoids the subtractive cancellation errors inherent in the classical derivative approximations
is described. Numerical examples illustrating the power of the approximation are presented.
Key words. divided difference, subtractive cancellation},
  comment   = {Numerical partial derivatives (e.g. in a Jacobian) for any real function that can accept complex inputs. Super simple and accurate. Has Matlab library

* The first paper with the modern complex step differentiation algorithm, according the Clive Moler (another guy who might be first author, see blog post in Shampine07AccNumerDiffMatlab)

* Matlab PMAD library is even more user friendly: Shampine07AccNumerDiffMatlab

* Sensitivity analysis was done in: Martins03cmplxStepDerivCSD},
  doi       = {10.1137/S003614459631241X},
  file      = {Squire98cmplxDerivsCSD.pdf:Squire98cmplxDerivsCSD.pdf:PDF},
  owner     = {sotterson},
  publisher = {SIAM},
  timestamp = {2014.07.02},
}

@Article{Schlueter86windArrayPred,
  author    = {Schlueter, R. A. and Sigari, G. and Costi, A.},
  title     = {Wind Array Power Prediction for Improved Operating Economics and Reliability},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {1986},
  volume    = {1},
  number    = {1},
  pages     = {137--142},
  month     = feb,
  issn      = {0885-8950},
  abstract  = {A methodology for predicting wind power variations one or more hours ahead is developed. This prediction method is required for unit commitment and generation control strategies that have been developed to provide economic and reliable operation for utilities with large wind penetrations. The methodology utilizes a set of meteorological towers that encircle the wind turbine clusters at a radius of 100 miles. The prediction methodology must determine (1) the direction of propagation of the meteorological event, (2) the subset of meteorological towers that are in the direction of motion of the meteorological event and thus are to be used in the prediction, (3) the delays between the prediction sites in the wind turbine cluster and the selected subset of reference sites in the ring of meteorological towers to be used for prediction, and (4) the parameters of the predictive model that predicts wind speed based on weighted delayed wind speed measurements at the selected reference sites.},
  comment   = {Prediction using off-site observations: correlation-lag clustering; wind change not necess. in direction of wind; met tower rings 3 Models proposed, best is correlation echelon model for wind (the site/site method on p. 141. Others are group/site and group/group) * regress on scaled deviance from offsite mean wind speed (i.e. covariance) * scale comes from off/on correlation at peak mag lag * bias is onsite wind speed mean * NOTE: don't set lookahead; get whatever lookahead you get from cross-corr. TRUE: forecaster can only see if correlation at lookahead time is good enough. (if to predict at a longer lookahead weight by correlation at that lag) * Recursive Least Squared algorithm used (eq. 3), rather than actually weighting by xcorr -- RLS adjusts weights only -- delay figured out by delay estimation/feature selection -- delay determination harder for shorter lookaheads (washed out by self-correlation?) * This is also a kind of lag forecast: assumes future trip from obs to site will take T seconds, like it did T seconds ago, as measured by corr. * Need to model/constrain lag too? * This method assumes wind at met towers is statistically independent (not true). So use something like max power beamforming? i.e. convoltive cannonical correlation analysis (which would find max correlation of inputs w/ output to be predicted) Feature Selection (of offiste measurement towers) 1.) heuristic groups by by intra-correlation (w/ intra-lag delays) 2.) uses known distances from turbines to find groups w/ lags consistent w/ a direction of propagation 3.) selects all measurement groups w/ lags > lookahead time * Authors would prefer to use meterological information to find delays/direction * Is there some Bayesian prior equivalent here? ie. the prob that this lag is correct? * For selected obs w/ lags > lookahead time, must just advance by lag-tahead (but don't say this) Met Tower Rings * Study in OK suggests need met tower ring of radius > 100 miles for one hour ahead forecasting Direction of Propagation * two examples (or more?) where wind speed change propagation NOT in direction of prevailing wind velocity * I don't get why they allow the possibility of circular feature selection (wind speed change propagating in a ring towards turbines?, p. 141)},
  doi       = {10.1109/TPWRS.1986.4334859},
  file      = {Schlueter86windArrayPred.pdf:Schlueter86windArrayPred.pdf:PDF;Schlueter86windArrayPred.pdf:Schlueter86windArrayPred.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.01.15},
}

@Article{Duque09scenarioBeta,
  author    = {Alvaro Jaramillo Duque and Ismael S\'{a}nchez and Edgardo Castronuovo and Julio Usaola},
  title     = {Simulation scenarios and prediction intervals in wind power forecasting with the Beta distribution},
  journal   = {Electrical Engineering Electronic Journal},
  year      = {2009},
  volume    = {1},
  number    = {1},
  month     = jul,
  abstract  = {A methodology for the simulation of the wind power scenario for a short term horizon (one or two days in advance) is proposed. The covariance of the historical errors and the wind power forecast are use to generate a conditional random variable that represent the power wind production as a scenario. With the information provided by the scenario simulation, the energy deviation during a period and the prediction interval for each hour are obtained. The Beta distribution is used to represent the behaviour of the wind power production due to its better performance. With the results, it is possible to quantify the uncertainty of wind energy production. Finally, comparing the covariance and correlation of the simulated errors with historical errors, the procedure of the methodology is validated.},
  comment   = {forecast error scenarios via MVN model in linear domain, transformed to Beta. Seems wrong.

Similar idea to Pinson09probFrcstStatScenWind
* forecast errors across horizons put in vector, modeled w/ MVN
-- a problem since MVN is applied w/ no transform
-- means that linear correlation has already made non-recoverable errors
* Then marginal transforms to beta

Not really tested
* errors calculated by scenario "difference" from actual power production

* "difference" not defined
* but probably doesn't model a distribution match
* is not actually shown anyway
* should be compared against something e.g. Pierre's 09 paper

Still, this was Pierre's recommendation for spinning reserves},
  file      = {Duque09scenarioBeta.pdf:Duque09scenarioBeta.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.11.24},
  url       = {http://www.aedie.org/eeej/webrevista/articulos/num1/Vol1num1.html},
}

@Article{Milligan85numClust,
  author      = {Milligan, Glenn and Cooper, Martha},
  title       = {An examination of procedures for determining the number of clusters in a data set},
  journal     = {Psychometrika},
  year        = {1985},
  volume      = {50},
  pages       = {159--179},
  issn        = {0033-3123},
  abstract    = {A Monte Carlo evaluation of 30 procedures for determining the number of clusters was conducted on artificial data sets which contained either 2, 3, 4, or 5 distinct nonoverlapping clusters. To provide a variety of clustering solutions, the data sets were analyzed by four hier-archical clustering methods. External criterion measures indicated excellent recovery of the true cluster structure by the methods at the correct hierarchy level. Thus, the clustering present in the data was quite strong. The simulation results for the stopping rules revealed a wide range in their ability to determine the correct number of clusters in the data. Several procedures worked fairly well, whereas others performed rather poorly. Thus, the latter group of rules would appear to have little validity, particularly for data sets containing distinct clusters. Applied researchers are urged to select one or more of the better criteria. However, users are cautioned that the per-formance of some of the criteria may be data dependent. Key words : classification, stopping rules, numerical taxonomy},
  affiliation = {The Ohio State University Faculty of Management Sciences 301 Hagerty Hall 43210 Columbus OH 301 Hagerty Hall 43210 Columbus OH},
  comment     = {highly cited paper on classic methods for num of cluster determination},
  doi         = {10.1007/BF02294245},
  file        = {Milligan85numClust.pdf:Milligan85numClust.pdf:PDF},
  issue       = {2},
  keyword     = {Humanities, Social Sciences and Law},
  owner       = {sotterson},
  publisher   = {Springer New York},
  timestamp   = {2011.11.17},
}

@InProceedings{Zhao11MultiLinTensorSVDandPLS,
  author    = {Zhao, Qibin and Caiafa, Cesar F and Mandic, Danilo P and Zhang, Liqing and Ball, Tonio and Schulze-Bonhage, Andreas and Cichocki, Andrzej},
  title     = {Multilinear Subspace Regression: An Orthogonal Tensor Decomposition Approach.},
  booktitle = {NIPS},
  year      = {2011},
  volume    = {2011},
  pages     = {1269--1277},
  abstract  = {A multilinear subspace regression model based on so called latent variable decomposition
is introduced. Unlike standard regression methods which typically
employ matrix (2D) data representations followed by vector subspace transformations,
the proposed approach uses tensor subspace transformations to model
common latent variables across both the independent and dependent data. The
proposed approach aims to maximize the correlation between the so derived latent
variables and is shown to be suitable for the prediction of multidimensional
dependent data from multidimensional independent data, where for the estimation
of the latent variables we introduce an algorithm based on Multilinear Singular
Value Decomposition (MSVD) on a specially defined cross-covariance tensor. It
is next shown that in this way we are also able to unify the existing Partial Least
Squares (PLS) and N-way PLS regression algorithms within the same framework.
Simulations on benchmark synthetic data confirm the advantages of the proposed
approach, in terms of its predictive ability and robustness, especially for small
sample sizes. The potential of the proposed technique is further illustrated on a
real world task of the decoding of human intracranial electrocorticogram (ECoG)
from a simultaneously recorded scalp electroencephalograph (EEG).},
  comment   = {Multilinear SVD and PLS, tensor based, as in Lu09UncorrMultiLinPCA . Could use to do high dimensional quantile regression, where high dim. interaction terms are squashed using PLS against point forecast errors or empirical quantiles.

* maybe can use some of the matlab tools metioned in Cichocki14TensorNetworksBigOpt ?},
  file      = {Zhao11MultiLinTensorSVDandPLS.pdf:Zhao11MultiLinTensorSVDandPLS.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.02.09},
  url       = {http://papers.nips.cc/paper/4328-multilinear-subspace-regression-an-orthogonal-tensor-decomposition-approach-spotlight.pdf},
}

@Article{Moon08nonParaForecastGSlake,
  author    = {Young-Il Moon and Upmanu Lall and Hyun-Han Kwon},
  title     = {Non-parametric short-term forecasts of the Great Salt Lake using atmospheric indices},
  journal   = {International Journal of Climatology},
  year      = {2008},
  volume    = {28},
  pages     = {361370},
  abstract  = {A multivariate, non-parametric model for approximating the non-linear dynamics of hydroclimatic variables is developed and applied for forecasting the volume of the Great Salt Lake (GSL) of Utah. The monthly volume of the GSL is presumed to depend on recent volumes of the lake, and on three atmospheric circulation indices. The indices considered are the Southern oscillation index (SOI), the pacific/North America (PNA) climatic index, and the central North Pacific (CNP) climatic index. Locally weighted polynomials with automatically and locally chosen parameters are used for developing a non-linear forecasting model. Estimated average mutual information (M.I) is used to select appropriate lags across each time series. Iterated and direct multi-step predictions of lake volumes for up to 2 years in the future with and without the atmospheric indices are compared. The atmospheric circulation information can lead to significant improvements in the predictability of the lake.},
  comment   = {K-neighbors, kernel-like regression, autoregressive with exogenous inputs. Dynamical system stuff mentioned but not truly used

How does this compare to KNN regression in: Navot05nearNbrFeatSelRegress


KNN or kernel-like regressor: could have just used neural net w/ standard lagged feature selection?
* state variables are inputs at multiple lags
* when forecast, pick k-nearest input neighbors in historical record (and their outputs)
* weight the neigbhors' outputs based on distance (tried several types)

* also added a special month-of-the-year input for a sort-of seasonality

* k and distance function chosen by brute force cross validation
* some inputs removed when have poor t-test
-- is this a good test? ie. white regression errors? See e.g. Bisgaard06studyInOutRelatI, Bisgaard07BewareAutoCorrRegress,
* seems like could have done this all w/ a neural network
* iterated instead of direct forecasting may have been slightly better but very short test data

Dynamical system theory quoted but not truly used
* fixed lag selected by max MI
-- shouldn't they search for min MI? (eg. Simon07hiDimDelSelMutInfo)
-- maybe had do do this b/c dimension not selected yet
* dimension selected by brute force cross validation error parameter search
* really, could have selected arbitrary lags w/ same procedure
-- as in Simon07hiDimDelSelMutInfo
-- dimension choice could be cross-validated as before, or selected w/ standard tech.

Results
* much better than AR},
  doi       = {10.1002/joc.1533},
  file      = {Moon08nonParaForecastGSlake.pdf:Moon08nonParaForecastGSlake.pdf:PDF;Moon08nonParaForecastGSlake.pdf:Moon08nonParaForecastGSlake.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.02.09},
  url       = {http://www3.interscience.wiley.com/journal/114298145/abstract},
}

@InBook{Valpola15neurPCA2deepUnsupLrnBk,
  chapter   = {From neural PCA to deep unsupervised learning},
  pages     = {143--171},
  title     = {Advances in Independent Component Analysis and Learning Machines},
  publisher = {Academic Press},
  year      = {2015},
  author    = {Valpola, Harri},
  editor    = {Ella Bingham and Samuel Kaski and Jorma Laaksonen and Jouko Lampinen},
  abstract  = {A network supporting deep unsupervised learning is presented. The network is an
autoencoder with lateral shortcut connections from the encoder to decoder at each
level of the hierarchy. The lateral shortcut connections allow the higher levels of
the hierarchy to focus on abstract invariant features. While standard autoencoders
are analogous to latent variable models with a single layer of stochastic variables,
the proposed network is analogous to hierarchical latent variables models.
Learning combines denoising autoencoder and denoising sources separation
frameworks. Each layer of the network contributes to the cost function a term
which measures the distance of the representations produced by the encoder and
the decoder. Since training signals originate from all levels of the network, all
layers can learn efficiently even in deep networks.
The speedup offered by cost terms from higher levels of the hierarchy and the
ability to learn invariant features are demonstrated in experiments.},
  comment   = {A new way of semisupervised learning that allows unsupervised learning part to compress and forget information.

Useful because it combines unsupervised and supervised learning.  Seems to be faster than plain unsupervised and also more compact.  Also, it's not just for classification but for continuous regression.

Extended in Rasmus15semiSupLadder
and again in Pezeshki16dcnstrLadderNet

Tutorial slides: Raiko15supUnsupCombLadder},
  file      = {Valpola15neurPCA2deepUnsupLrnBk.pdf:Valpola15neurPCA2deepUnsupLrnBk.pdf:PDF},
  journal   = {Advances in Independent Component Analysis and Learning Machines},
}

@Article{McVinish12impABCquantDist,
  author    = {McVinish, R.},
  title     = {Improving ABC for quantile distributions},
  journal   = {Statistics and Computing},
  year      = {2012},
  volume    = {22},
  number    = {6},
  pages     = {1199--1207},
  issn      = {0960-3174},
  abstract  = {A new approximate Bayesian computation (ABC) algorithm is proposed specifically designed for models involving quantile distributions. The proposed algorithm compares favourably with two other ABC algorithms when applied to examples involving quantile distributions.},
  comment   = {Somehow useful for quantile forecast, I would guess. Related post from X'ian blog post (bibtex'ed)},
  doi       = {10.1007/s11222-010-9209-9},
  file      = {McVinish12impABCquantDist.pdf:McVinish12impABCquantDist.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_2},
  keywords  = {Approximate Bayesian computation; Likelihood-free inference; Markov chain Monte Carlo; Quantile distributions; Quantile regression},
  language  = {English},
  owner     = {sotterson},
  publisher = {Springer US},
  timestamp = {2013.10.04},
}

@TechReport{RodriguezAlvarez13fastSmthMultiPspline,
  author      = {Rodr{\'\i}guez-{\'A}lvarez, Mar{\'\i}a Xos{\'e} and Lee, Dae-Jin and Kneib, Thomas and Durb{\'a}n, Mar{\'\i}a and Eilers, Paul},
  title       = {Fast algorithm for smoothing parameter selection in multidimensional generalized P-splines},
  institution = {Universidad Carlos III de Madrid. Departamento de Estad?stica},
  year        = {2013},
  type        = {UC3M Working papers. Statistics and Econometrics},
  number      = {13-26},
  abstract    = {A new computational algorithm for estimating the smoothing parameters of a multidimensional penalized
spline generalized model with anisotropic penalty is presented. This new proposal is based on the mixed
model representation of a multidimensional P-spline, in which the smoothing parameter for each
covariate is expressed in terms of variance components. On the basis of penalized quasi-likelihood
methods (PQL), closed-form expressions for the estimates of the variance components are obtained. This
formulation leads to an efficient implementation that can considerably reduce the computational load. The
proposed algorithm can be seen as a generalization of the algorithm by Schall (1991) - for variance
components estimation - to deal with non-standard structures of the covariance matrix of the random
effects. The practical performance of the proposed computational algorithm is evaluated by means of
simulations, and comparisons with alternative methods are made on the basis of the mean square error
criterion and the computing time. Finally, we illustrate our proposal with the analysis of two real datasets:
a two dimensional example of historical records of monthly precipitation data in USA and a three
dimensional one of mortality data from respiratory disease according to the age at death, the year of death
and the month of death.
Keywords: Smoothing; P-splines; Tensor product; Anisotropic penalty; Mixed Models.},
  comment     = {Multidimensional p-spline smoothing. Said to be fast. Are tensor splines, maybe tensor PLS better?},
  file        = {RodriguezAlvarez13fastSmthMultiPspline.pdf:RodriguezAlvarez13fastSmthMultiPspline.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2015.02.09},
  url         = {http://orff.uc3m.es/handle/10016/17544},
}

@InCollection{FerrariTrecate02pcsWiseLR,
  author    = {Ferrari-Trecate, Giancarlo and Muselli, Marco},
  title     = {A New Learning Method for Piecewise Linear Regression},
  booktitle = {Artificial Neural Networks ??? ICANN 2002},
  publisher = {Springer Berlin Heidelberg},
  year      = {2002},
  editor    = {Dorronsoro, Jos??R.},
  volume    = {2415},
  series    = {Lecture Notes in Computer Science},
  pages     = {444--449},
  isbn      = {978-3-540-44074-1},
  abstract  = {A new connectionist model for the solution of piecewise linear regression problems is introduced; it is able to reconstruct both con-
tinuous and non continuous real valued mappings starting from a ?nite
set of possibly noisy samples. The approximating function can assume a
di?erent linear behavior in each region of an unknown polyhedral parti-
tion of the input domain.
The proposed learning technique combines local estimation, clustering in
weight space, multicategory classification and linear regression in order
to achieve the desired result. Through this approach piecewise afine
solutions for general nonlinear regression problems can also be found.},
  comment   = {Simple almost non-iterative method for piecewise multiinear regression. Could be adapted for quantile regression (local linear) nieghborhood size picking. Could also be used for regime learning w/ offsite observations.

ALGORITHM

1. For every point, pick the nearest (in magnitude, not time) c -1 points and
 a. compute linear regression coeffs
 b. .make a feature vector for each point from the coeffs concatenated w/ the inputs
 c. But LR coeffs aren't unique? esp. when noisy. Is clusterer mainly using inputs?
2. cluster feature vectors w/ kmeans to get S clusters
 a. distance factor related to covariance matrices somehow but I don't understand it
3. try to predict the kmeans class labels from the inputs in that class. It's multi-category, so use an SVN trick to get unique assignment.
4. Using the new input classification, calculate the regression coeffs for each local neighborhood. The SVN style classifier could be used to also classify new, separate test inputs.

Done. No iteration, except for kmeans.

TWEAK PARAMETERS
c: the number of points needed to buid the point regression
S: the number of neighborhoods

IDEAS
* spectral clustering might better capture the I/O relationships than kmeans/
* if use for picking local linear neighbhorhoods, don't need to do the final classification?
 -- for new inputs, neighborhood weights could come from distance from the mean of the kmeans/spectral clusters
 -- the final linear regresion might be better if the inputs were weighted by distance to other neighbhorhoods.
* BUT, is there something important about the SVN disjoint point classification? I don't see it.
* somehow make sure there are enough points in each class to do the LR},
  doi       = {10.1007/3-540-46084-5_72},
  file      = {FerrariTrecate02pcsWiseLR.pdf:FerrariTrecate02pcsWiseLR.pdf:PDF},
  groups    = {Read},
  language  = {English},
  owner     = {sotterson},
  timestamp = {2014.03.08},
}

@InCollection{Sugiyama09densRatEst,
  author      = {Sugiyama, Masashi},
  title       = {Density Ratio Estimation: A New Versatile Tool for Machine Learning},
  booktitle   = {Advances in Machine Learning},
  publisher   = {Springer Berlin / Heidelberg},
  year        = {2009},
  editor      = {Zhou, Zhi-Hua and Washio, Takashi},
  volume      = {5828},
  series      = {Lecture Notes in Computer Science},
  pages       = {6--9},
  isbn        = {978-3-642-05223-1},
  abstract    = {A new general framework of statistical data processing based on the ratio of probability densities has been proposed recently and gathers a great deal of attention in the machine learning and data mining communities [1?17]. This density ratio framework includes various statistical data processing tasks such as non-stationarity adaptation [18, 1, 2, 4, 13], outlier detection [19?21, 6], and conditional density estimation [22?24, 15]. Furthermore, mutual information?which plays a central role in information theory [25]?can also be estimated via density ratio estimation. Since mutual information is a measure of statistical independence between random variables [26?28], density ratio estimation can be used also for variable selection [29, 7, 11], dimensionality reduction [30, 16], and independent component analysis [31, 12]. Accurately estimating the density ratio is a key issue in the density ratio framework. A naive approach is to estimate each density separately and then take the ratio of the estimated densities. However, density estimation is known to be a hard problem and thus this two-step approach may not be accurate in practice. A promising approach would be to estimate the density ratio directly without going through density estimation. Several direct density ratio estimation methods have been proposed so far, including kernel mean matching [3], logistic regression [32, 33, 5], the Kullback-Leibler importance estimation procedure [8, 9], least-squares importance ?tting [10, 17], and unconstrained least-squares importance ?tting [10, 17]. Note that the importance refers to the density ratio, derived from importance sampling [34]. Furthermore, a density ratio estimation method incorporating dimensionality reduction has also be developed [35], which is shown to be useful in high-dimensional cases. A review of the density ratio framework is available in [36]. The density ratio approach was shown to be useful in various real-world applications including brain-computer interface [4, 37], robot control [38?41, 15], speaker identi?- cation [42, 43], natural language processing [14], bioinformatics [11], visual inspe},
  affiliation = {Department of Computer Science, Tokyo Institute of Technology},
  comment     = {overview of the many applications of density ratio estimation e.g. mutual information. Good for prob forecast featsel?

Useful for SPRT anomalie detection too, since it has a density ratio in it?

I can't find the pdf, but have some slides

Maybe an improvement here: Direct density-ratio estimation with dimensionality reduction via least-squares hetero-distributional subspace search http://www.sciencedirect.com.globalproxy.cvt.dk/science/article/pii/S0893608010001917},
  doi         = {10.1007/978-3-642-05224-8_2},
  file        = {slides:Sugiyama09densRatEst_Slides.pdf:PDF;Abstract and Bibliography:Sugiyama09densRatEst_AbsAndBib.pdf:PDF},
  groups      = {Ensemble, doReadNonWPV_2},
  keyword     = {Computer Science},
  owner       = {sotterson},
  timestamp   = {2011.11.30},
}

@Article{Hart11genProfLrgVarRen,
  author    = {Elaine K. Hart and Mark Z. Jacobson},
  title     = {A {Monte Carlo} approach to generator portfolio planning and carbon emissions assessments of systems with large penetrations of variable renewables},
  journal   = {Renewable Energy},
  year      = {2011},
  volume    = {36},
  number    = {8},
  pages     = {2278--2286},
  issn      = {0960-1481},
  abstract  = {A new generator portfolio planning model is described that is capable of quantifying the carbon emissions associated with systems that include very high penetrations of variable renewables. The model combines a deterministic renewable portfolio planning module with a Monte Carlo simulation of system operation that determines the expected least-cost dispatch from each technology, the necessary reserve capacity, and the expected carbon emissions at each hour. Each system is designed to meet a maximum loss of load expectation requirement of 1 day in 10 years. The present study includes wind, centralized solar thermal, and rooftop photovoltaics, as well as hydroelectric, geothermal, and natural gas plants. The portfolios produced by the model take advantage of the aggregation of variable generators at multiple geographically disperse sites and the incorporation of meteorological and load forecasts. Results are presented from a model run of the continuous two-year period, 2005???2006 in the California ISO operating area. A low-carbon portfolio is produced for this system that is capable of achieving an 80\% reduction in electric power sector carbon emissions from 2005 levels and supplying over 99\% of the annual delivered load with non-carbon sources. A portfolio is also built for a projected 2050 system, which is capable of providing 96\% of the delivered electricity from non-carbon sources, despite a projected doubling of the 2005 system peak load. The results suggest that further reductions in carbon emissions may be achieved with emerging technologies that can reliably provide large capacities without necessarily providing positive net annual energy generation. These technologies may include demand response, vehicle-to-grid systems, and large-scale energy storage.},
  comment   = {California 2050 low carbon scenario w/ 25\% peak gas, 2.6\% gas capacity factor. Huge C02 reductions. Maybe scenario generation is worth looking at. Compare with Nikolakakis11optWindSolMix and the 2012 IEEE intermittency issue.},
  doi       = {10.1016/j.renene.2011.01.015},
  file      = {Hart11genProfLrgVarRen.pdf:Hart11genProfLrgVarRen.pdf:PDF},
  keywords  = {Renewable energy},
  owner     = {sotterson},
  timestamp = {2011.12.01},
  url       = {http://www.sciencedirect.com/science/article/pii/S0960148111000371},
}

@Article{Dette06nonParamMonoRgrssn,
  author    = {Dette, Holger and Neumeyer, Natalie and Pilz, Kay F and others},
  title     = {A simple nonparametric estimator of a strictly monotone regression function},
  journal   = {Bernoulli},
  year      = {2006},
  volume    = {12},
  number    = {3},
  pages     = {469--490},
  abstract  = {A new method for monotone estimation of a regression function is proposed, which is potentially attractive to users of conventional smoothing methods. The main idea of the new approach is to construct a density estimate from the estimated values m?(i/N) (i=1,?,N) of the regression function and to use these `data' for the calculation of an estimate of the inverse of the regression function. The final estimate is then obtained by a numerical inversion. Compared to the currently available techniques for monotone estimation the new method does not require constrained optimization. We prove asymptotic normality of the new estimate and compare the asymptotic properties with the unconstrained estimate. In particular, it is shown that for kernel estimates or local polynomials the bandwidths in the procedure can be chosen such that the monotone estimate is first-order asymptotically equivalent to the unconstrained estimate. We also illustrate the performance of the new procedure by means of a simulation study.},
  comment   = {A rearrangement type montonicity enforcer, useful for undoing quantile crossover, according to Chernozhukov10qrNoCross. Also useful for other monotone regression problems, I suppose.

As a side effect, it shows how to make a fake density for regression results -- useful when want to get a density out of a quantile estimate!

* a different monotizer by the authors is in: Dette08quantRgrsNonCross},
  file      = {Dette06nonParamMonoRgrssn.pdf:Dette06nonParamMonoRgrssn.pdf:PDF},
  owner     = {sotterson},
  publisher = {Bernoulli Society for Mathematical Statistics and Probability},
  timestamp = {2015.07.10},
  url       = {http://projecteuclid.org/euclid.bj/1151525131},
}

@MastersThesis{Brown12powCurvVerifSDE,
  author      = {C. Brown},
  title       = {Fast Verification of Wind Turbine Power Curves: Summary of Project Results},
  year        = {2012},
  note        = {Supervised by Associate Professor Pierre Pinson, pp\@imm.dtu.dk, {DTU} Informatics, and Mike Courtney, {DTU} Wind Energy},
  abstract    = {A new method has been developed for analyzing power performance measurements using a stochastic differential equation called the Langevin equation on high frequency wind turbine measurement data sampled at one sample per second or more. The aim of this method is to provide an alternative to the current industry standard described in {IEC} 61400-12-1. This new method was implemented and analyzed in a number of test cases to evaluate its effectiveness and robustness as a part of the FastWind project at the Technical University of Denmark?s (DTU) Wind Energy department. The FastWind project aims to develop and test this method for use in the Danish wind energy industry. This new method was successfully implemented for use in this project as well as future work in the FastWind project. Several methods of estimating the power curve uncertainty were developed and implemented; including an analytical equation and two data resampling methods. Using measurement data from {DTU}?s Nordtank wind turbine at the Ris{\o} site, the practical application of this new method was tested. It was found that the method is sensitive to some of the input parameters chosen by the user. Additionally, it was found to be sensitive to the coherence of high frequency wind speed and power measurements. These sensitivities indicate that the base method is not ready for application in industry but the method needs some further development to improve the robustness of the method?s results. However, potential solutions to these difficulties were proposed and show some promise for improving the outputs of the method. Proponents of this new method claim that the method for effectively uses measurement data and therefore will require less data, and therefore shorter measurement campaigns, to obtain a power curve with a desired level of certainty. Additionally, it has been claimed that the method is insensitive to turbulence intensity and site specific conditions; unlike the standard {IEC} method. The potential benefits of this new method were tested and some positive results were obtained. The method was shown that it may converge faster than the standard {IEC} method requiring less data to obtain the same level of certainty. Furthermore, the method shows some insensitivity to turbulence intensity indicating that site specific turbulence will not affect the power curve obtained. Although this new method has some challenges associated with it, they are not unsolvable and these benefits show that the method may be worth the time invested to overcome these difficulties.},
  comment     = {Power curve learning with stochastic differentifal equations (SDE's) is supposed to allow separation of the effect of local turbulence and other things. Tests suggest that that the idea promising but not quite baked.},
  file        = {Brown12powCurvVerifSDE.pdf:Brown12powCurvVerifSDE.pdf:PDF},
  groups      = {PointDerived, doReadWPV_2},
  institution = {Technical University of Denmark, {DTU} Informatics, {E-}mail: reception\@imm.dtu.dk},
  location    = {Asmussens Alle, Building 305, {DK-}2800 Kgs. Lyngby, Denmark},
  owner       = {sotterson},
  timestamp   = {2014.02.25},
  url         = {http://www.imm.dtu.dk/English.aspx},
}

@Article{Villiers08sagCarrierSig,
  author    = {de Villiers, W. and Cloete, J.H. and Wedepohl, L.M. and Burger, A.},
  title     = {Real-Time Sag Monitoring System for High-Voltage Overhead Transmission Lines Based on Power-Line Carrier Signal Behavior},
  year      = {2008},
  volume    = {23},
  number    = {1},
  month     = jan,
  pages     = {389--395},
  issn      = {0885-8977},
  doi       = {10.1109/TPWRD.2007.905550},
  abstract  = {A new method of measuring the change in the average height above ground of horizontal high-voltage overhead transmission-lines (OHTLs) phase conductors is introduced. The new technique, called power-line carrier sag (PLC-SAG) for short, determines average overhead conductor height variations in real time by correlating sag with measured variations in the amplitude of signals propagating between power-line carrier (PLC) stations. The multifrequency PLC-SAG monitoring signals are injected onto the operational PLC system in the 50- to 500-kHz band, but without interfering with the operational integrity of the PLC teleprotection system. The feasibility of the method was examined using the theory of natural modes for multiconductor systems, and tested by extensive field experiments on two horizontal 400-kV lines operated in South Africa by Eskom. It is concluded that the average height of a horizontal OHTL can be tracked accurately for continuous periods by the PLC-SAG technique.},
  file      = {Villiers08sagCarrierSig.pdf:Villiers08sagCarrierSig.pdf:PDF;Villiers08sagCarrierSig.pdf:Villiers08sagCarrierSig.pdf:PDF},
  groups    = {Read},
  journal   = {Power Delivery, IEEE Transactions on},
  keywords  = {power overhead lines, power supply quality, power system measurementEskom, South Africa, high-voltage overhead transmission lines, multiconductor systems, operational integrity, overhead conductor height variations, power-line carrier sag, power-line carrier signal behavior, power-line carrier stations, real-time sag monitoring system, teleprotection system},
  owner     = {sotterson},
  timestamp = {2009.02.23},
}

@Article{Lee07rbstAdaptPLS,
  author    = {Lee, Hae Woo and Lee, Min Woo and Park, Jong Moon},
  title     = {Robust Adaptive Partial Least Squares Modeling of a Full-Scale Industrial Wastewater Treatment Process},
  journal   = {Industrial \& Engineering Chemistry Research},
  year      = {2007},
  volume    = {46},
  number    = {3},
  pages     = {955--964},
  abstract  = {A new scheme of robust adaptive partial least squares (PLS) method was proposed for the purpose of prediction and monitoring of an industrial wastewater treatment process that has highly complex and time-varying process dynamics. The essential feature of this method is that all incoming process data are preliminarily screened on the basis of a combined monitoring index and each observation identified as an outlier is simply eliminated (hard threshold) or suppressed by using a weight function (soft threshold) prior to model update. To elucidate the feasibility of the proposed scheme, various PLS modeling approaches, including conventional ones, were evaluated and their results were compared with each other. While the conventional approaches clearly revealed their limitations such as the inflexibility of the model to process changes and the misleading model update by high leverage outliers, most robust adaptive PLS approaches based on the proposed scheme exhibited fairly good performances both in the prediction and monitoring aspects. Among the tested methods, the robust adaptive PLS method using Fair weight function showed the best performances, reasonably maintaining the robustness of the PLS model.},
  comment   = {use instead of sliding window covariance estimation, etc.?},
  doi       = {10.1021/ie061094+},
  eprint    = {http://pubs.acs.org/doi/pdf/10.1021/ie061094%2B},
  file      = {Lee07rbstAdaptPLS.pdf:Lee07rbstAdaptPLS.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.11.03},
}

@Article{Bracale13bayesPVprobFrcst,
  author    = {Bracale, A.},
  title     = {A {Bayes}ian Method for Short-Term Probabilistic Forecasting of Photovoltaic Generation in Smart Grid Operation and Control},
  year      = {2013},
  number    = {6},
  pages     = {733--747},
  abstract  = {A new short-term probabilistic forecasting method is proposed to predict the
probability density function of the hourly active power generated by a photovoltaic system.
Firstly, the probability density function of the hourly clearness index is forecasted making
use of a Bayesian auto regressive time series model; the model takes into account the
dependence of the solar radiation on some meteorological variables, such as the cloud
cover and humidity. Then, a Monte Carlo simulation procedure is used to evaluate the
predictive probability density function of the hourly active power by applying the
photovoltaic system model to the random sampling of the clearness index distribution. A
numerical application demonstrates the effectiveness and advantages of the proposed
forecasting method.},
  booktitle = {Energies},
  comment   = {A multivariate input method for PV},
  file      = {Bracale13bayesPVprobFrcst.pdf:Bracale13bayesPVprobFrcst.pdf:PDF},
  groups    = {Ensemble, PointDerived, CitaviImport1, doReadWPV_1, Use},
  ncite     = {2},
  owner     = {sotterson},
  timestamp = {2013.09.26},
}

@Article{Kirch11timeFreqBtStrp,
  author    = {Kirch, C. and Politis, D.N.},
  title     = {{TFT}-bootstrap: Resampling time series in the frequency domain to obtain replicates in the time domain},
  journal   = {The Annals of Statistics},
  year      = {2011},
  volume    = {39},
  number    = {3},
  pages     = {1427--1470},
  abstract  = {A new time series bootstrap scheme, the time frequency toggle (TFT)-bootstrap, is proposed. Its basic idea is to bootstrap the Fourier coefficients of the observed time series, and then to back-transform them to obtain a bootstrap sample in the time domain. Related previous proposals, such as the "surrogate data" approach, resampled only the phase of the Fourier coefficients and thus had only limited validity. By contrast, we show that the appropriate resampling of phase and magnitude, in addition to some smoothing of Fourier coefficients, yields a bootstrap scheme that mimics the correct second-order moment structure for a large class of time series processes. As a main result we obtain a functional limit theorem for the TFT-bootstrap under a variety of popular ways of frequency domain bootstrapping. Possible applications of the TFT-bootstrap naturally arise in change-point analysis and unit-root testing where statistics are frequently based on functionals of partial sums. Finally, a small simulation study explores the potential of the TFT-bootstrap for small samples showing that for the discussed tests in change-point analysis as well as unit-root testing, it yields better results than the corresponding asymptotic tests if measured by size and power.},
  comment   = {Bootstrap in freq. domain, where Fourier coeffs are independent normal distributions; an advanced version of surrogate method.

Could maybe use for missing feature fill-in
* note that this can't be used to estimate confidence intervals for mean value
-- throws away the DC coeff but I'm not sure why
-- but this does make the approach robust for addititive transforms

* improvement on surrogate method used in: Pinson10relDiagSerCorr and in Jornsten07btstrpClassNotes
* for smooth matches in time domain when using for missing feature fill-in, proper phase could be set as in Smaragdis11timeFreqImpute

* approximates signal by Gaussian process even when data is covariance is really non-linear (I think this was good)

BASIC IDEA: Fourier coeffs are independent and normally distributed so can generate bootstraps with the proper statistics by drawing from those distributions and inverse FFT'ing. Also, the real component is indepent of the imaginary component. The procedure is:
1. x(n) --> FFT --> X(k)
2. delete DC term and half-term if even-sized
3. use X(k) to generate a new Fourier coeff bootstrap sample, X*, enforcing symmetry for real signals
4. X* --> IFFT x* (the time domain bootstrap)

WAYS TO GENERATE THE FOURIER COEFF BOOTSTRAP:
1. Residual bootstrap: whitening, as in Jornsten07btstrpClassNotes

2. Wild Bootstrap: actually create a Gaussian for each Fourier coeff

3. Local boostrap: model each coeff as a kernel density along with its neighbors, exploiting freq continuity.
-- Draw from that.
-- is an improvement over Residual bootstrap
-- better results if spectrum not too smoothed (I think he concludes at the end)

APPLICATIONS
* Statistics based on periodograms e.g. ratio stats, spetral estimates, Yule-Walker AR estimators
* Change point tests ("structural changes" but only mentions mean change, not dependence change via CUSUM)
* Unit root testing: is the signal non-stationary but is stationary after 1\textsuperscript{st} order differencing?
* Use this, combined w/ Fourier Regression to do Ameilia-like bootstrapped missing feature fill-in. Lags would be handled by Fourier coeffs, with proper autocorrelation functions instead of Amelia's single lag thing. Could perhaps do better spectral template matching of historical data, getting something more like regime-switching missing feature fill-in.

PROOFS
* most of the paper is proofs that this is a principled estimator, etc.

RESULTS
* tests on simulated data. Supposed to be better in some ways, especially on critical values test (for change det?)},
  file      = {Kirch11timeFreqBtStrp.pdf:Kirch11timeFreqBtStrp.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2012.12.03},
  url       = {http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1305292042},
}

@Article{Murphy73scoreVecPart,
  author    = {Murphy, Allan H.},
  title     = {A New Vector Partition of the Probability Score},
  journal   = {Journal of Applied Meteorology},
  year      = {1973},
  volume    = {12},
  number    = {4},
  pages     = {595--600},
  issn      = {0021-8952},
  abstract  = {A new vector partition of the probability, or Brier, score (PS) is formulated and the nature and properties
of this partition are described. The relationships between the terms in this partition and the terms in the
original vector partition of the PS are indicated. The new partition consists of three terms: 1) a measure of
the uncertainty inherent in the events, or states, on the occasions of concern (namely, the PS for the sample
relative frequencies); 2) a measure of the reliability of the forecasts; and 3) a new measure of the resolution
of the forecasts. These measures of reliability and resolution are and are not, respectively, equivalent (i.e.,
linearly related) to the measures of reliability and resolution provided by the original partition. Two sample
collections of probability forecasts are used to illustrate the differences and relationships between these
partitions. Finally, the two partitions are compared, with particular reference to the attributes of the forecasts
with which the partitions are concerned, the interpretation of the partitions in geometric terms, and
the use of the partitions as the bases for the formulation of measures to evaluate probability forecasts. The
results of these comparisons indicate that the new partition offers certain advantages vis-a-vis the original
partition.},
  booktitle = {Journal of Applied Meteorology},
  comment   = {There are two big papers on partitioning some prob. score. This is the first.},
  doi       = {10.1175/1520-0450%281973%29012%3C0595%3AANVPOT%3E2.0.CO%3B2},
  file      = {Murphy73scoreVecPart.pdf:Murphy73scoreVecPart.pdf:PDF},
  groups    = {Test, CitaviImport1, doReadWPV_2},
  keywords  = {probability score},
  ncite     = {500},
  owner     = {sotterson},
  timestamp = {2013.09.26},
}

@Article{Solomatine09errParamEst,
  author    = {Solomatine, Dimitri P. and Shrestha, Durga Lal},
  title     = {A novel method to estimate model uncertainty using machine learning techniques},
  journal   = {Water Resources Research},
  year      = {2009},
  volume    = {45},
  number    = {12},
  issn      = {1944-7973},
  abstract  = {A novel method is presented for model uncertainty estimation using machine learning techniques and its application in rainfall runoff modeling. In this method, first, the probability distribution of the model error is estimated separately for different hydrological situations and second, the parameters characterizing this distribution are aggregated and used as output target values for building the training sets for the machine learning model. This latter model, being trained, encapsulates the information about the model error localized for different hydrological conditions in the past and is used to estimate the probability distribution of the model error for the new hydrological model runs. The M5 model tree is used as a machine learning model. The method is tested to estimate uncertainty of a conceptual rainfall runoff model of the Bagmati catchment in Nepal. In this paper the method is extended further to enable it to predict an approximation of the whole error distribution, and also the new results of comparing this method to other uncertainty estimation approaches are reported. It can be concluded that the method generates consistent, interpretable and improved model uncertainty estimates.},
  comment   = {Model errors are estimated depending upon input space, then a new model estimates to input-space-dependent error distribution parameters. This is a little like: Cannon12nnCaDENCE},
  doi       = {10.1029/2008WR006839},
  groups    = {PointDerived, doReadNonWPV_2},
  keywords  = {rainfall runoff modeling, uncertainty analysis, machine learning method, clustering},
  owner     = {sotterson},
  timestamp = {2014.01.21},
}

@Article{Park14windPowCurveMon,
  author    = {J. Y. Park and J. K. Lee and K. Y. Oh and J. S. Lee},
  title     = {Development of a Novel Power Curve Monitoring Method for Wind Turbines and Its Field Tests},
  journal   = IEEE_J_EC,
  year      = {2014},
  volume    = {29},
  number    = {1},
  pages     = {119--128},
  month     = mar,
  issn      = {0885-8969},
  abstract  = {A novel power curve monitoring method for wind turbines was developed to prevent a turbine failure in a wind farm. Compared with the existing methods, this algorithm automatically calculates the power curve limits for power curve monitoring, even when a considerable number of abnormal data are included in wind speed-output power data measured at a wind turbine. In addition, the proposed algorithm automatically generates an alarm message when the wind speed-power data measured at the wind turbine deviate from the power curve limits, particularly considering their degree of deviation from the power curve limits and the cases when the measured data hover between the Warning Zones and the Alarm Zones. We confirmed its effectiveness through its field tests.},
  comment   = {
Referenced by: Leahy16windTurbFaultMachLrn},
  doi       = {10.1109/TEC.2013.2294893},
  file      = {:C\:\\Users\\sotterson\\Documents\\Tempo Box\\ref\\papers\\Park14windPowCurveMon.pdf:PDF},
  keywords  = {condition monitoring, wind turbines, alarm zones, power curve monitoring method, turbine failure prevention, warning zones, wind farm, wind speed-output power data, wind turbines, Interpolation, Monitoring, Power measurement, Standards, Wind farms, Wind speed, Wind turbines, Alarm generation, condition monitoring, fault data queue, power curve, turbine monitoring, wind turbine},
  owner     = {sotterson},
  timestamp = {2017.06.19},
}

@Article{Taylor98cmbinFrcstQuants,
  author    = {Taylor, James W and Bunn, Derek W},
  title     = {Combining forecast quantiles using quantile regression: Investigating the derived weights, estimator bias and imposing constraints},
  journal   = {Journal of Applied Statistics},
  year      = {1998},
  volume    = {25},
  number    = {2},
  pages     = {193--206},
  abstract  = {A novel proposal for combining forecast distributions is to use quantile regression to combine quantile estimates. We consider the usefulness of the resultant linear combining weights. If the quantile estimates are unbiased, then there is strong intuitive appeal for omitting the constant and constraining the weights to sum to unity in the quantile regression. However, we show that suppressing the constant renders one of the main attractive features of quantile regression invalid. We establish necessary and sufficient conditions for unbiasedness of a quantile estimate, and show that a combination with zero constant and weights that sum to unity is not necessarily unbiased.},
  comment   = {Combining expert distribution forecasts into a single forecast based on weighted sums of quantiles. Not quite predicting the quantiles of the sum of a pool (ReWP) but maybe it works anyway?},
  doi       = {10.1080/02664769823188},
  file      = {Taylor98cmbinFrcstQuants.pdf:Taylor98cmbinFrcstQuants.pdf:PDF},
  publisher = {Taylor \& Francis},
}

@InProceedings{Meyers19pvDgrdAnlysisNoPhysMdl,
  author    = {Bennet Meyers and Michael Deceglie and Chris Deline, and Dirk Jordan},
  title     = {Signal Processing on PV Time-Series Data: Robust Degradation Analysis without Physical Models},
  booktitle = {{IEEE} Photovoltaic Specialists Conference (PSVC 46)},
  year      = {2019},
  address   = {Chicago, IL},
  month     = jun,
  abstract  = {A novel unsupervised machine learning approach
for analyzing time-series data is applied to the topic of pho-
tovoltaic (PV) system degradation rate estimation, sometimes
referred to as energy-yield degradation analysis. This approach
only requires a measured power signal as an input—no irra-
diance data, temperature data, or system configuration infor-
mation. We present results on a data set that was previously
analyzed and presented by NREL using RdTools, validating the
accuracy of the new approach and showing increased robustness
to data anomalies while reducing the data requirements to carry
out the analysis.
Index Terms—photovoltaic systems, distributed power gener-
ation, computer aided analysis, data analysis, statistical learning.},
  comment   = {Estimation of PV degradation rates.  From Patrick Keelin

Has both paper and slides},
  file      = {Paper:Meyers19pvDgrdAnlysisNoPhysMdl.pdf:PDF;Slides:Meyers19pvDgrdAnlysisNoPhysMdl_slides.pdf:PDF},
  url       = {https://www.ieee-pvsc.org/ePVSC/},
}

@Book{Orwig12EconEvalSTwindPowFrcstERCOT,
  author    = {Orwig, Kirsten and Hodge, BM and Brinkman, G and Ela, E and Milligan, M and Banunarayanan, V and Nasir, S and Freedman, J},
  title     = {Economic evaluation of short-term wind power forecasts in ERCOT: Preliminary results},
  year      = {2012},
  publisher = {National Renewable Energy Laboratory},
  url       = {http://www.nrel.gov/docs/fy12osti/56257.pdf},
  abstract  = {A number of wind energy integration studies have
investigated the monetary value of using day-ahead wind power
forecasts for grid operation decisions. Historically, these studies
have shown that large cost savings could be gained by grid
operators implementing the forecasts in their system operations.
To date, none of these studies have investigated the value of
shorter term (0- to 6-h ahead) wind power forecasts. In 2010, the
Department of Energy and the National Oceanic and
Atmospheric Administration partnered to form the Wind
Forecasting Improvement Project (WFIP) to fund improvements
in short-term wind forecasts and determine the economic value
of these improvements to grid operators. In this work, we
discuss the preliminary results of the economic benefit analysis
portion of the WFIP for the Electric Reliability Council of
Texas. The improvements seen in the wind forecasts are
examined and the economic results of a production cost model
simulation are analyzed.
Keywords-wind power forecasting; numerical weather
prediction; economic value; grid operations},
  file      = {Orwig12EconEvalSTwindPowFrcstERCOT.pdf:Orwig12EconEvalSTwindPowFrcstERCOT.pdf:PDF},
}

@Article{Gneiting14ProbFrcsting,
  author    = {Gneiting, Tilmann and Katzfuss, Matthias},
  title     = {Probabilistic Forecasting},
  journal   = {Annual Review of Statistics and Its Application},
  year      = {2014},
  volume    = {1},
  number    = {1},
  pages     = {125--151},
  abstract  = {A probabilistic forecast takes the form of a predictive probability distribution over future quantities or events of interest. Probabilistic forecasting aims to maximize the sharpness of the predictive distributions, subject to calibration, on the basis of the available information set. We formalize and study notions of calibration in a prediction space setting. In practice, probabilistic calibration can be checked by examining probability integral transform (PIT) histograms. Proper scoring rules such as the logarithmic score and the continuous ranked probability score serve to assess calibration and sharpness simultaneously. As a special case, consistent scoring functions provide decision-theoretically coherent tools for evaluating point forecasts. We emphasize methodological links to parametric and nonparametric distributional regression techniques, which attempt to model and to estimate conditional distribution functions; we use the context of statistically postprocessed ensemble forecasts in numerical weather prediction as an example. Throughout, we illustrate concepts and methodologies in data examples.

Keywords
calibration, consistent scoring function, ensemble forecast, proper scoring
rule, distributional regression},
  comment   = {General Overvview of 2014 state of the art. Interesting topics: ensemble calibration, copulas for multidim, scenarios, aaggregation. onditional distributions (instead of quantiles), etc. Probably worth reading to catch up.},
  doi       = {10.1146/annurev-statistics-062713-085831},
  eprint    = {http://dx.doi.org/10.1146/annurev-statistics-062713-085831},
  file      = {Gneiting14ProbFrcsting.pdf:Gneiting14ProbFrcsting.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.07},
}

@InProceedings{Bessa11quantCplaWindPow,
  author    = {R. J. Bessa and J. Mendes and V. Miranda and A. Botterud and J. Wang and Z. Zhou},
  title     = {Quantile-copula density forecast for wind power uncertainty modeling},
  booktitle = {2011 IEEE Trondheim PowerTech},
  year      = {2011},
  pages     = {1-8},
  month     = {June},
  abstract  = {A probabilistic forecast, in contrast to a point forecast, provides to the end-user more and valuable information for decision-making problems such as wind power bidding into the electricity market or setting adequate operating reserve levels in the power system. One important requirement is to have flexible representations of wind power forecast (WPF) uncertainty, in order to facilitate their inclusion in several problems. This paper reports results of using the quantile-copula conditional Kernel density estimator in the WPF problem, and how to select the adequate kernels for modeling the different variables of the problem. The method was compared with splines quantile regression for a real wind farm located in the U.S. Midwest.},
  comment   = {Is made time-adaptive in Bessa12adaptQuantCopulaFrcst},
  doi       = {10.1109/PTC.2011.6019180},
  file      = {Bessa11quantCplaWindPow.pdf:Bessa11quantCplaWindPow.pdf:PDF},
  keywords  = {decision making;power generation economics;power markets;probability;splines (mathematics);weather forecasting;wind power plants;WPF problem;WPF uncertainty modelling;decision-making problems;electricity market;power system;probabilistic forecast;quantile regression;quantile-copula conditional Kernel density estimator;quantile-copula density forecast;splines;wind farm;wind power bidding;wind power uncertainty modeling;Calibration;Density functional theory;Kernel;Uncertainty;Wind forecasting;Wind power generation;Wind speed;Wind power;copula;forecasting;kernel density;probabilistic;uncertainty},
}

@Article{Scheufele14CombNwcstTmLagEns,
  author    = {Scheufele, Katrin and Kober, Kirstin and Craig, George C and Keil, Christian},
  title     = {Combining probabilistic precipitation forecasts from a nowcasting technique with a time-lagged ensemble},
  journal   = {Meteorological Applications},
  year      = {2014},
  volume    = {21},
  number    = {2},
  pages     = {230--240},
  abstract  = {A probabilistic nowcasting technique based on the Local Lagrangian method is combined with probabilistic
forecasts derived from a time-lagged convection-permitting model to produce seamless short-term probabilistic precipitation
forecasts. The fraction, the neighbourhood and the mean method are used to derive probabilistic information from this eight
member ensemble. The model-based forecasts are calibrated with the reliability diagram statistics method. The skill of the
probabilistic nowcasts and forecasts is evaluated with three quality measures. Probabilistic model-based forecasts are found
to outperform probabilistic radar-based nowcasts after 2.25?3.5 h. Weighting functions derived in a lead time dependent
evaluation of forecast skill are used to combine nowcasts and forecasts additively. The resulting seamless blended forecasts
maintain or exceed the skill of the respective best component. In comparison with similar studies, the application of the
time-lagged approach increases the skill of the numerical forecasts and hence the blended forecasts.
KEY WORDS high-resolution numerical weather prediction; seamless blending; COSMO-DE; neighbourhood method; radar
data},
  comment   = {Forecasts via a blend of nowcasting (probabilistic persisitence?) and time-lagged ensemble forecasts. Maybe use for very short term quantile forecasts. Can this be somehow modified for very short term nodal forecasts?},
  doi       = {10.1002/met.1381/abstract},
  file      = {Scheufele14CombNwcstTmLagEns.pdf:Scheufele14CombNwcstTmLagEns.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.12.19},
}

@Article{Olson18sensRndFrstProbKern,
  author   = {Olson, Matthew A and Wyner, Abraham J},
  title    = {Making Sense of Random Forest Probabilities: a Kernel Perspective},
  journal  = {arXiv:1812.05792v1},
  year     = {2018},
  month    = dec,
  abstract = {A random forest is a popular tool for estimating probabilities in machine learning classification tasks. However, the means by which this is accomplished is unprincipled: one simply counts the fraction of trees in a forest that vote for a certain class. In this paper, we forge a connection between random forests and kernel regression. This places random forest probability estimation on more sound statistical footing. As part of our investigation, we develop a model for the proximity kernel and relate it to the geometry and sparsity of the estimation problem. We also provide intuition and recommendations for tuning a random forest to improve its probability estimates.},
  comment  = {A way to get better (class?) probabilities from a random forest.  Getting random forest probabilities by counting leaves is a good classifier but not reliable as a source of proabilities.  This paper has a method for tuning.  Seems to be RMSE of probability instead of Breir Score, but I have only skimmed the paper so far.

Recommends the tutorial in: Biau16randFrstGuideTour},
  file     = {:Olson18sensRndFrstProbKern.pdf:PDF},
  url      = {https://arxiv.org/abs/1812.05792v1},
}

@InBook{Jaakkola96cmprDiffsnMdlAdopt,
  pages     = {65--82},
  title     = {Comparison and Analysis of Diffusion Models},
  publisher = {Springer US},
  year      = {1996},
  author    = {Jaakkola, Hannu},
  editor    = {Kautz, Karlheinz and Pries-Heje, Jan},
  address   = {Boston, MA},
  isbn      = {978-0-387-34982-4},
  abstract  = {A real diffusion process consists of a huge amount of interrelated variables. This complexity can be modelled by diffusion models building a simplified mathematical representation of the main features of the process as a time series of indicators describing the phenomenon in interest. Mathematical models are mostly used for technological forecasting purposes; forecasting is based on the best fit of the empirical data to the model formula and trend extrapolation outside the empirical period. The fit gives numeric values to the parameters of the model. Some of the models include also explanative factors --- values of parameters describe behavioral properties of the process. The paper concentrates on mathematical diffusion models. The variety of models derived from the literature is introduced and analyzed. As an application of mathemat ical approach to the diffusion the diffusion of mobile phones is discussed. The focus of this paper is to point out problems in mathematical analysis and to discuss about alternative approaches.},
  booktitle = {Diffusion and Adoption of Information Technology: Proceedings of the first IFIP WG 8.6 working conference on the diffusion and adoption of information technology, Oslo, Norway, October 1995},
  comment   = {Compares many Bass-like models, doesn't like them.  Thinks they're only useful for intuition and understanding.  Might be interesting to see how they help w/ intuition and understanding.},
  doi       = {10.1007/978-0-387-34982-4_6},
  file      = {:Jaakkola96cmprDiffsnMdlAdopt.pdf:PDF},
  url       = {https://doi.org/10.1007/978-0-387-34982-4_6},
}

@Article{Hill16lrgSclWindMaxPlanckMiss,
  author    = {Joshua S Hill},
  title     = {Max Planck Institute Study On Large-Scale Wind Energy Misses The Mark},
  journal   = {Clean Technica},
  year      = {2016},
  abstract  = {A recent study from the Max Planck Institute investigating the efficiency of large-scale wind farms claims that large-scale wind energy deployment slows down winds in and around a wind farm, therefore reducing wind turbine efficiency.

However, the assumptions made in this study bear little to no relation to the reality of future wind energy deployment, subsequently casting unnecessary doubt on the issue.},
  comment   = {A rebuttal of Miller16windSpdRedLargeScl

Also see evernote:
http://www.evernote.com/l/AA0V1xHzL29PgYhOrFprhksnanhADGk1XAI/},
  file      = {Hill16lrgSclWindMaxPlanckMiss.pdf:Hill16lrgSclWindMaxPlanckMiss.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.26},
  url       = {https://cleantechnica.com/2016/11/25/max-planck-institute-study-large-scale-wind-energy-misses-mark/},
}

@Article{Chay01semiParmCensRgrsn,
  author    = {Chay, K.Y. and Powell, J.L.},
  title     = {Semiparametric censored regression models},
  journal   = {The Journal of Economic Perspectives},
  year      = {2001},
  volume    = {15},
  number    = {4},
  pages     = {29--42},
  abstract  = {A regression model is censored when the recorded data on the dependent variable cuts off outside a certain range with multiple observations at the endpoints of that range. When the data are censored, variation in the observed dependent variable will understate the effect of the regressors on the ?true? dependent variable. As a result, standard ordinary least squares regression using censored data will typically result in coefficient estimates that are biased toward zero. Traditional statistical analysis uses maximum likelihood or related procedures to deal with the problem of censored data. However, the validity of such methods requires correct specification of the error distribution, which can be problematic in practice. In the past two decades, a number of semiparametric alternatives for dealing with censored data have been proposed. In a semiparametric approach, part of the functional form of the model?usually the regression function?is parametrically specified by the researcher based upon plausible assumptions, while the rest of the model is not parameterized.1 While the theoretical literature has produced several semiparametric estimators for the censored data model, published applications of these estimators to empirical problems in economics have lagged far behind. This paper reviews the intuition and computation of a handful of semiparametric estimators proposed for the censored regression model. The various estimators are used to examine changes in black-white earnings inequality during the 1960s, around the time of the passage of the Civil Rights Act of 1964, based on longitudinal Social Security Administration (SSA) earnings records. These earnings records are censored at the taxable maximum; that is, anyone earning more than the maximum that was taxable under Social Security is recorded as having earned at the maximum. Thus, above the maximum, the data on earnings do not accurately reflect actual earnings. Ordinary least squares analysis of these data implies little convergence in the earnings of black and white workers during the 1960s. On the other hand, the estimates from the semiparametric models that account for censoring suggest that significant black-white earnings convergence did occur after 1964. Comparisons of the results from parametric and semiparametric procedures help pinpoint sources of misspecification in the parametric approach.},
  comment   = {Easy/old explanation of censored regression. Use for regression on power-curved data, where clip at zero and max power?

Maybe an improvement for pdf estimate on point forecasts (Pinson10condPredInt) or for BMA ensemble forecasting (censoring the pdf on each ensemble member)},
  file      = {Chay01semiParmCensRgrsn.pdf:Chay01semiParmCensRgrsn.pdf:PDF},
  groups    = {Ensemble, PointDerived, doReadNonWPV_1},
  owner     = {scotto},
  publisher = {JSTOR},
  timestamp = {2011.10.05},
}

@Misc{Focken10extrmEvRampFrcst,
  author       = {Ulrich Focken},
  title        = {Experiences with Extreme Event Warning and Ramp Forecasting for US Wind Farm},
  howpublished = {Fourth Workshop on Best Practice in the Use of Short-term Forecasting of Wind Power},
  month        = oct,
  year         = {2010},
  abstract     = {A reliable ramp forecast and warnings that indicate extreme events, like shut-downs due to high wind speeds or icing are of high value for a system operator. We present recent advances and practical experiences with ramp event forecasting and warnings for wind farms in the United States (in particular, the BPA ramp event project in the Pacific Northwest) and from south-eastern Australia.},
  comment      = {On the need for ramp forecasts},
  file         = {Focken10extrmEvRampFrcst.pdf:Focken10extrmEvRampFrcst.pdf:PDF},
  groups       = {Use, doReadWPV_2},
  owner        = {scot},
  timestamp    = {2011.06.16},
  url          = {http://powwow.risoe.dk/BestPracticeWorkshop4.htm},
}

@Book{Holttinen16Designoperationpower,
  title     = {Design and operation of power systems with large amounts of wind power. Final summary report, IEA WIND Task 25, Phase three 2012?2014},
  publisher = {Julkaisija -- Utgivare},
  year      = {2016},
  author    = {Hannele Holttinen and Juha Kiviluoma and Alain Forcione and Michael Milligan and Charles J. Smith and Jody Dillon and Jan Dobschinski and Serafin van Roon and Nicolaos Cutululis and Antje Orths and Peter B{\o}rre Eriksen and Enrico Maria Carlini and Ana Estanqueiro and Ricardo Bessa and Lennart S{\"o}der and Hossein Farahmand and Jose Rueda Torres and Bai Jianhua and Junji Kondoh and Ivan Pineda and Goran Strbac},
  abstract  = {A research and development (R\&D) task on the Design and Operation of Power Systems with Large Amounts of Wind Power was formed in 2006 within the International Energy Agency (IEA) Implementing Agreement for Co-operation in the Research, Development, and Deployment of Wind Turbine Systems (http://www.ieawind.org) as Wind Task 25. The aim of this R\&D task is to collect and share information on the experiences gained and the studies made on power system impacts of wind power and to review methodologies, tools, and data used.  he following countries and institutes have been involved in the collaboration:

...

IEA Wind Task 25 produced a report in 2007 on the state-of-the-art knowledge and
results that had been gathered so far that was published in the VTT Working Papers series. Summary reports of two subsequent phases have also been published by VTT: 2009 (VTT Research Notes 2493) and 2013 (VTT Technology T75). These reports presented summaries of selected, recently finished studies. In addition, IEA Wind Task 25 developed guidelines on the recommended methodologies when estimating the system impacts and costs of wind power integration; this was published in 2013 as RP16 of IEA Wind. All of these reports are available on the IEA Wind Task 25 website: http://www.ieawind.org/task_25.html. This report summarises the results of the third three-year phase. The work continues with a fourth three-year period (2015?2017). Sintef would like to acknowledge the NOWITECH research centre for financially supporting our participation in the IEA Task25 activities. June 2016, Authors},
  comment   = {Read before doing ModernABWind work?},
  file      = {Book:Holttinen16lrgWindIEA25final.pdf:PDF},
  url       = {http://www.vtt.fi/inf/pdf/technology/2016/T268.pdf},
}

@Article{Kober12probNowcastBlendNWP,
  author    = {Kober, K and Craig, GC and Keil, C and D{\"o}rnbrack, A},
  title     = {Blending a probabilistic nowcasting method with a high-resolution numerical weather prediction ensemble for convective precipitation forecasts},
  journal   = {Quarterly Journal of the Royal Meteorological Society},
  year      = {2012},
  volume    = {138},
  number    = {664},
  pages     = {755--768},
  abstract  = {A seamless prediction of convective precipitation for a continuous range of lead
times from 0?8 h requires the application of different approaches. Here, a nowcasting
method and a high-resolution numerical weather prediction ensemble are combined
to provide probabilistic precipitation forecasts. For the nowcast, an existing
deterministic extrapolation technique was modified by the local Lagrangian method
to calculate the probability of exceeding a threshold value in radar reflectivity.
Numerical forecasts were obtained from an experimental high-resolution ensemble
that provides 20 different deterministic forecasts of synthetic radar reflectivity.
Probabilistic information was calculated by different approaches from the ensemble
output. The probabilistic forecasts based on the ensemble were calibrated with
the reliability diagram statistics method. The skill of the probabilistic nowcasts and
forecasts was evaluated using three quality measures. Finally, a seamless probabilistic
forecast was generated as an additive combination of nowcast and forecast, using a
weighting function based on their relative skills. The skill of the seamless forecast
was greater than or equal to that of the nowcast or ensemble forecast in all quality
measures and at all lead times.
Key Words: short-range forecasting; blending; ensemble prediction; forecast calibration},
  comment   = {Forecasts via a blend of nowcasting (probabilistic persisitence?) and ensemble forecasts. Maybe use for very short term quantile forecasts. Can this be somehow modified for very short term nodal forecasts?

Referenced by Richard Keane, DWD},
  file      = {Kober12probNowcastBlendNWP.pdf:Kober12probNowcastBlendNWP.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.12.19},
}

@Article{Gower68addPtMultivar,
  author    = {Gower, John C},
  title     = {Adding a point to vector diagrams in multivariate analysis},
  journal   = {Biometrika},
  year      = {1968},
  volume    = {55},
  number    = {3},
  pages     = {582--585},
  abstract  = {A set of n base points P; (i == 1, 2, ... ,n), with known co-ordinates rela.tive to orthogonal axes, and
a. further point P,.+1 , with known distance from each of the base set, are given. The co-ordinates of P,.+l
relative to the axes of the base set are found. The formula. is particularly simple when the base set is
referred to ita principal axes, when the co-ordinates of P,.+l for a subset of all the axes can be calculated
from the co-rdinates of the P 1 in this subset only. The classical results for adding a point to a principal
components or canonica.l variates analyses are obtained when the base set is derived using the appro?
pria.te distance functions. An example is given.},
  comment   = {Online PCA, CCA (if this is the same as CVA in this paper) and I think, MDS (so says Bengio04OutOfSmplExt). Avoids totally retraining the covariance or whatever matrix},
  file      = {Gower68addPtMultivar.pdf:Gower68addPtMultivar.pdf:PDF},
  owner     = {sotterson},
  publisher = {Biometrika Trust},
  timestamp = {2014.06.26},
}

@Article{Billings07sparseIDorthMI,
  author    = {Stephen A. Billings and Hua-Liang Wei},
  title     = {Sparse Model Identification Using a Forward Orthogonal Regression Algorithm Aided by Mutual Information},
  journal   = {Neural Networks, IEEE Transactions on},
  year      = {2007},
  volume    = {18},
  number    = {1},
  pages     = {306--310},
  abstract  = {A sparse representation, with satisfactory approximation accuracy, is usually desirable in any nonlinear system identification and signal processing problem. A new forward orthogonal regression algorithm, with mutual information interference, is proposed for sparse model selection and parameter estimation. The new algorithm can be used to construct parsimonious linear-in-the-parameters models.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comment   = {A way of creating orthogonal basis functions of NARX models. I didn't quite understand this paper. I thought this was a feature selection paper but it's kind of about feature compression, or maybe model output compression. Anyway, there's a trick to avoid selecting correlated inputs (after transformation?).},
  doi       = {10.1109/TNN.2006.886356},
  file      = {Billings07sparseIDorthMI.pdf:Billings07sparseIDorthMI.pdf:PDF;Billings07sparseIDorthMI.pdf:Billings07sparseIDorthMI.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.02.09},
}

@Article{Khouider10stochCloudCnvct,
  author    = {Khouider, Boualem and Biello, Joseph and Majda, Andrew J and others},
  title     = {A stochastic multicloud model for tropical convection},
  journal   = {Communications in Mathematical Sciences},
  year      = {2010},
  volume    = {8},
  number    = {1},
  pages     = {187--216},
  abstract  = {A stochastic model for representing the missing variability in global climate models
due to unresolved features of organized tropical convection is presented here. We use a
Markov chain lattice model to represent small scale convective elements which interact
with each other and with the large scale environmental variables through convective
available potential energy (CAPE) and middle troposphere dryness. Each lattice site
is either occupied by a cloud of a certain type (congestus, deep or stratiform) or it
is a clear sky site. The lattice sites are assumed to be independent from each other
so that a coarse-grained stochastic birth-death system, which can be evolved with
a very low computational overhead, is obtained for the cloud area fractions alone.
The stochastic multicloud model is then coupled to a simple tropical climate model
consisting of a system of ode?s, mimicking the dynamics over a single GCM grid box.
Physical intuition and observations are employed here to constrain the design of the
models. Numerical simulations showcasing some of the dynamical features of the
coupled model are presented below.},
  comment   = {Maybe for calibration/stochastic models that preserve temporal correlations. In this case, convection models. Reference from Richarde Keane, in response to the question of how to do calibration w/o messing up temporal derivatives. Besides ensemble calibration, the grid aspect might be somehow useful for NWP grid upscaling or nodal forecasts, and maybe probabilsitic persistence (w/ the correct temporal dependencies).

2009 paper (not 2010 journal paper) is probably from here:
http://www.math.nyu.edu/faculty/majda/Submitted/CMS%20%28A%20Stochastic%20Multicloud%29%20Majda%20Khouider%20Biello%20%282008%29.pdf},
  file      = {2009 submitted version:Khouider10stochCloudCnvct.pdf:PDF},
  owner     = {sotterson},
  publisher = {International Press of Boston},
  timestamp = {2014.12.19},
  url       = {https://projecteuclid.org/euclid.cms/1266935019},
}

@Article{Hsu08SlassoVARsubset,
  author    = {Hsu, Nan-Jung and Hung, Hung-Lin and Chang, Ya-Mei},
  title     = {Subset selection for vector autoregressive processes using Lasso},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {2008},
  volume    = {52},
  number    = {7},
  pages     = {3645--3657},
  issn      = {0167-9473},
  abstract  = {A subset selection method is proposed for vector autoregressive (VAR) processes using the Lasso [Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B 58, 267-288] technique. Simply speaking, Lasso is a shrinkage method in a regression setup which selects the model and estimates the parameters simultaneously. Compared to the conventional information-based methods such as AIC and BIC, the Lasso approach avoids computationally intensive and exhaustive search. On the other hand, compared to the existing subset selection methods with parameter constraints such as the top-down and bottom-up strategies, the Lasso method is computationally efficient and its result is robust to the order of series included in the autoregressive model. We derive the asymptotic theorem for the Lasso estimator under VAR processes. Simulation results demonstrate that the Lasso method performs better than several conventional subset selection methods for small samples in terms of prediction mean squared errors and estimation errors under various settings. The methodology is applied to modeling U.S. macroeconomic data for illustration.},
  comment   = {Picking lags with LASSO -- does 1\textsuperscript{st} order difference to remove non-stationarity -- HUH Ahmed07empCompForecast says differencing doesn't work for non-stationary signals -- but Bisgaard papers say to do it},
  doi       = {10.1016/j.csda.2007.12.004},
  file      = {Hsu08SlassoVARsubset.pdf:Hsu08SlassoVARsubset.pdf:PDF;Hsu08SlassoVARsubset.pdf:Hsu08SlassoVARsubset.pdf:PDF},
  location  = {Amsterdam, The Netherlands, The Netherlands},
  owner     = {sotterson},
  publisher = {Elsevier Science Publishers B. V.},
  timestamp = {2009.08.17},
}

@Patent{Mihnev16anomDetParamSysPtnt,
  nationality = {US},
  number      = {US 9,379,951 B2},
  year        = {2016},
  yearfiled   = {2014},
  author      = {Aldimir Mihnev},
  title       = {Method and Apparatus for Detection of Anomalies in Integrated Parameter Systems},
  language    = {English},
  assignee    = {Instep Software, LLC},
  address     = {Chicago, IL (US)},
  type        = {patentus},
  day         = {28},
  dayfiled    = {10},
  month       = jun,
  monthfiled  = jan,
  abstract    = {A system, method, and tang1ble computmg apparatus is disclosed for the detection of anomalies in an integrated data network. Said system, method and apparatus comprises the creation and construction of a mathematical model that utilizes multi-dimensional mutual information to detect interactions and interrelationships between pairs ofdata streams and among pluralities of data streams. Real-time analysis of the operations of an integrated data network is enhanced and expedited via use of locality sensitive hashing that relies on density detenninations ofclusters of data.},
  comment     = {Peter Deeskow suggested I look at it.  It uses mutual information.  Peter says it's related to the STEAG SPRT approach.  Looks interesting.},
  file        = {:Mihnev16anomDetParamSysPtnt.pdf:PDF},
}

@Article{Huang14hrtfTensorPLS,
  author    = {Huang, Qinghua and Li, Lin},
  title     = {Modeling individual HRTF tensor using high-order partial least squares},
  journal   = {EURASIP Journal on Advances in Signal Processing},
  year      = {2014},
  volume    = {2014},
  number    = {1},
  pages     = {1--14},
  abstract  = {A tensor is used to describe head-related transfer functions (HRTFs) depending on frequencies, sound directions,
and anthropometric parameters. It keeps the multi-dimensional structure of measured HRTFs. To construct a
multi-linear HRTF personalization model, an individual core tensor is extracted from the original HRTFs using
high-order singular value decomposition (HOSVD). The individual core tensor in lower-dimensional space acts as
the output of the multi-linear model. Some key anthropometric parameters as the inputs of the model are selected
by Laplacian scores and correlation analyses between all the measured parameters and the individual core tensor.
Then, the multi-linear regression model is constructed by high-order partial least squares (HOPLS), aiming to seek a
joint subspace approximation for both the selected parameters and the individual core tensor. The numbers of
latent variables and loadings are used to control the complexity of the model and prevent overfitting feasibly.
Compared with the partial least squares regression (PLSR) method, objective simulations demonstrate the better
performance for predicting individual HRTFs especially for the sound directions ipsilateral to the concerned ear. The
subjective listening tests show that the predicted individual HRTFs are approximate to the measured HRTFs for the
sound localization.
Keywords: Head-related transfer function; Tensor; Multi-linear model; Laplacian score; High-order partial least squares},
  comment   = {Tensor PLS, maybe good for compressing high dim interaction terms of spline projections before doing linear QR?

Note that I had a lot of papers on HRTF in my Thesis bibtex file, speakerclust.bib},
  file      = {Huang14hrtfTensorPLS.pdf:Huang14hrtfTensorPLS.pdf:PDF},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2015.02.09},
}

@Article{Lu07lagEnsemble,
  author    = {Lu, Chungu and Yuan, Huiling and Schwartz, Barry E. and Benjamin, Stanley G.},
  title     = {Short-Range Numerical Weather Prediction Using Time-Lagged Ensembles},
  journal   = {Weather and Forecasting},
  year      = {2007},
  volume    = {22},
  number    = {3},
  pages     = {580--595},
  month     = jun,
  issn      = {0882-8156},
  abstract  = {A time-lagged ensemble forecast system is developed using a set of hourly initialized Rapid Update Cycle model deterministic forecasts. Both the ensemble-mean and probabilistic forecasts from this time-lagged ensemble system present a promising improvement in the very short-range weather forecasting of 1?3 h, which may be useful for aviation weather prediction and nowcasting applications. Two approaches have been studied to combine deterministic forecasts with different initialization cycles as the ensemble members. The first method uses a set of equally weighted time-lagged forecasts and produces a forecast by taking the ensemble mean. The second method adopts a multilinear regression approach to select a set of weights for different time-lagged forecasts. It is shown that although both methods improve short-range forecasts, the unequally weighted method provides the best results for all forecast variables at all levels. The timelagged ensembles also provide a sample of statistics, which can be used to construct probabilistic forecasts.},
  booktitle = {Weather and Forecasting},
  comment   = {doi: 10.1175/WAF999.1
Review:
Better 1-3hr forecasts using ensembles built from overlapping NWP time series from older start times.

* main reason is that, at short horizons, NWP spinup is averaged out
* but smoothing due to averaging helps a little bit.
* Wind speed point forecasts are improved the least
* but wind speed probabilsitic forecast are improved about the same as other parameters
* "time lagged" means using earlier NWP time series, not just the latest

Test setup
* test data is from RUC, which is updated w/ new assimilation data every hour.
* randomly chosen test time
* four parameters forcasted (at 3 heights)
-- geopotential height
-- temperature
-- horizontal wind speed
-- relative humidity

RESULTS

Ensemble mean
* increased or mostly non-decreasing skill
* less improvement for wind at upper levels, but is 'upper much higher than hub height?
* For non-wind variables, newer is not always better. Reason is model spin-uptime.
* spinup errors come from
1.) convection and precipitation fields
2.) ingestion of wind or mas-related observations
* most improvement from spinup improvement, w/ some from smoothing due to averaging

Linear regression
* regess ensembles against the thing to predict
* better than ensemble mean
* wind looks better
* for spinup-affected variables, see that latest NWP is deemphasized
* for wind, see that latest is emphasized and that regression coeffs "ring" on older NWP's

Probabilistic forecast
* I don't understand where the probabilities came from
* But see that probabilistic forecasts are improved.
* Surprisingly, wind speed probabilistic forecast is improved nearly as much as the other parameters are!},
  doi       = {10.1175/WAF999.1},
  file      = {Lu07lagEnsemble.pdf:Lu07lagEnsemble.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {American Meteorological Society},
  timestamp = {2013.02.20},
}

@Article{Jonsson13spotFrcstWind,
  author    = {Jonsson, T. and Pinson, P. and Nielsen, H.A. and Madsen, H. and Nielsen, T.S.},
  title     = {Forecasting Electricity Spot Prices Accounting for Wind Power Predictions},
  journal   = {Sustainable Energy, IEEE Transactions on},
  year      = {2013},
  volume    = {4},
  number    = {1},
  pages     = {210--218},
  issn      = {1949-3029},
  abstract  = {A two-step methodology for forecasting of electricity spot prices is introduced, with focus on the impact of predicted system load and wind power generation. The nonlinear and nonstationary influence of these explanatory variables is accommodated in a first step based on a nonparametric and time-varying regression model. In a second step, time-series models, i.e., ARMA and Holt-Winters, are applied to account for residual autocorrelation and seasonal dynamics. Empirical results are presented for out-of-sample forecasts of day-ahead prices in the Western Danish price area of Nord Pool's Elspot, during a two year period covering 2010-2011. These results clearly demonstrate the practical benefits of accounting for the complex influence of these explanatory variables.

Index Terms: Adaptivity, electricity prices, forecasting, nonlinear modeling, nonparametric modeling, robustness.},
  comment   = {Tryggvi's spot price forecast algorithm considering wind power as an input. Two stages: adaptive (exponential weighting) kernel forecast (Huber influence distance), followed with ARIMA and/or Holt Winters to touch up the error correlations. A point, not a probabilistic forecast.

Might be a clue for plain wind power forecasting too.

Holt Winters: Winters60exponMvAvgSalesFrcst or Meyer02naiveFrcstMethR

H-W compared w/ exponential weighting: Taylor07frcstSprMktPriceQR},
  doi       = {10.1109/TSTE.2012.2212731},
  file      = {Jonsson13spotFrcstWind.pdf:Jonsson13spotFrcstWind.pdf:PDF},
  groups    = {PointDerived, doReadWPV_2},
  keywords  = {autoregressive moving average processes;correlation theory;load forecasting;power generation economics;power markets;pricing;regression analysis;time series;wind power plants;ARMA model;Holt-Winters model;Nord Pool Elspot;Western Danish price area;day-ahead price;electricity spot price accounting forecasting;nonparametric regression model;residual autocorrelation;seasonal dynamics;system load prediction;time-series model;time-varying regression model;two-step methodology;wind power generation;Electricity;Forecasting;Load modeling;Predictive models;Production;Wind forecasting;Wind power generation;Adaptivity;electricity prices;forecasting;nonlinear modeling;nonparametric modeling;robustness},
  owner     = {sotterson},
  timestamp = {2013.10.29},
}

@Article{Yu99quantregLocLinKNN,
  author    = {Yu, Keming},
  title     = {Smoothing regression quantile by combining k-NN estimation with local linear kernel fitting},
  journal   = {Statistica Sinica},
  year      = {1999},
  volume    = {9},
  number    = {3},
  pages     = {759--774},
  abstract  = {A two-step nonparametric regression quantile smoothing technique is presented here, combining a standard k-NN technique and a locally linear kernel smoother. There are many advantages to this approach: an asymptotically optimal mean square error (Fan, Hu and Truong (1995)), a ready-made bandwidth selection rule (Yu and Jones (1998)), and simple computation and flexible estimation under variable transformations and distributional assumptions. The method is tested on a simulated example, and applied to data.
Key words and phrases: Bandwidth selection, correlated regression model, double
kernel method, k-NN method, local linear kernel smoothing, mean square error,
regression quantile.},
  comment   = {Two step method estimates local cdf/quantiles using KNN of each poionts input (X) coordinates, then smooths this with local linear regression. Is fast to compute but loses out to some benchmark methods. May not work for high dims seems to assume 2\textsuperscript{nd} order kernel (bad for disjoint, non-ball neighborhoods).

Past work
* references to early kernel QR
* references to early papers on fitting step function QR with local linear models (1995).

Why two step?
* two step method has same asymptotic pointwise mean square error (ASME) as "directly minimizing the check function"
* also has same efficiency as spline
* allows them to use rule-of-thumb bandwidth pickers
 -- these rules of them might be useful for speeding up agglomerative regression clustering
 -- Hansen09nonParamE718notes also has some rules of thumb

First step
* estimate a cdf/quantile from the k nearest neighbhors of each point, just like I was doing, except maybe they don't interpolate
* the call this HRK, from Healy's rule
* selection of k not too sensitive
* is really noisy, with long tails at RHS boundary (1Dim)

Second step
* rule of thumb for 2\textsuperscript{nd} step locLinQR smoothing
* locLinQR has some
KNN conditional quantile regression w/ local linear kernel, somehow. Seems like everything, but only 12 cites.

RESULTS
* tests on some real data, and quantiles look pretty good
* test on analytical function w/ known quantiles
 -- could use this for other work too!
 -- doesn't beat all other methods but is mostly the same at the median and the extremes},
  file      = {Yu99quantregLocLinKNN.pdf:Yu99quantregLocLinKNN.pdf:PDF},
  groups    = {Read, PointDerived, Test, doReadNonWPV_1},
  owner     = {sotterson},
  publisher = {C/O DR HC HO, INST STATISTICAL SCIENCE, ACADEMIA SINICA, TAIPEI 115, TAIWAN},
  timestamp = {2014.03.30},
}

@Misc{Wicklin12choleskyUseCorr,
  author       = {Rick Wicklin},
  title        = {Use the Cholesky transformation to correlate and uncorrelate variables},
  howpublished = {The DO Loop: Blog Post},
  month        = feb,
  year         = {2012},
  abstract     = {A variance-covariance matrix expresses linear relationships between variables. Given the covariances between variables, did you know that you can write down an
invertible linear transformation that "uncorrelates" the variables? Conversely, you can
transform a set of uncorrelated variables into variables with given covariances. The
transformation that works this magic is called the Cholesky transformation; it is
represented by a matrix that is the "square root" of the covariance matrix.},
  comment      = {Cholesky produces a "square root matrix": L*L'=A, where L is lower triangular and A is a positive semidifinite square matrix (like a covariance matrix).  Can be used to make correlated data, uncorrelate data, and solve linear systems with covariance matrices.  It's comutationally efficient.

Uses:
* To generate multivariate normal data with a given covariance structure from
uncorrelated normal variables.
* To remove the correlations between variables. This task requires using the
inverse Cholesky transformation.
* To quickly solve linear systems that involve a covariance matrix.},
  file         = {Wicklin12choleskyUseCorr.pdf:Wicklin12choleskyUseCorr.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2015.12.22},
  url          = {http://blogs.sas.com/content/iml/2012/02/08/use-the-cholesky-transformation-to-correlate-and-uncorrelate-variables.html},
}

@Article{Rubin15techLrnRateElecSply,
  author    = {Edward S. Rubin and In{\^{e}}s M.L. Azevedo and Paulina Jaramillo and Sonia Yeh},
  title     = {A review of learning rates for electricity supply technologies},
  journal   = {Energy Policy},
  year      = {2015},
  volume    = {86},
  pages     = {198--218},
  month     = {nov},
  abstract  = {A variety of mathematical models have been proposed to characterize and quantify the dependency of
electricity supply technology costs on various drivers of technological change. The most prevalent model
form, called a learning curve, or experience curve, is a log-linear equation relating the unit cost of a
technology to its cumulative installed capacity or electricity generated. This one-factor model is also the
most common method used to represent endogenous technical change in large-scale energy-economic
models that inform energy planning and policy analysis. A characteristic parameter is the “learning rate,”
defined as the fractional reduction in cost for each doubling of cumulative production or capacity. In this
paper, a literature review of the learning rates reported for 11 power generation technologies employing
an array of fossil fuels, nuclear, and renewable energy sources is presented. The review also includes
multi-factor models proposed for some energy technologies, especially two-factor models relating cost to
cumulative expenditures for research and development (R&D) as well as the cumulative installed ca-
pacity or electricity production of a technology. For all technologies studied, we found substantial
variability (as much as an order of magnitude) in reported learning rates across different studies. Such
variability is not readily explained by systematic differences in the time intervals, geographic regions,
choice of independent variable, or other parameters of each study. This uncertainty in learning rates,
together with other limitations of current learning curve formulations, suggests the need for much more
careful and systematic examination of the influence of how different factors and assumptions affect
policy-relevant outcomes related to the future choice and cost of electricity supply and other energy
technologies.},
  comment   = {Read this first, maybe?},
  doi       = {10.1016/j.enpol.2015.06.011},
  publisher = {Elsevier {BV}},
}

@TechReport{Jones11renewEnrgyStrat,
  author      = {Lawrence E. Jones},
  title       = {Strategies and Decision Support Systems for Integrating Variable Energy Resources in Control Centers for Reliable Grid Operations: Global Best Practices, Examples of Excellence and Lessons Learned},
  institution = {U.S. Dept. of Energy},
  year        = {2011},
  abstract    = {A variety of studies have recently evaluated the opportunities for the large-scale integration of wind energy
into the U.S. power system. These studies have included, but are not limited to, ?20 Percent Wind Energy by
2030: Increasing Wind Energy?s Contribution to U.S. Electricity Supply?, the ?Western Wind and Solar
Integration Study?, and the ?Eastern Wind Integration and Transmission Study.? Each of these U.S. based
studies have evaluated a variety of activities that can be undertaken by utilities to help integrate wind energy.
The integration of wind energy into the power grid introduces additional variability and uncertainty into grid
operations beyond that created by system load. It has been said by some in the utility industry that the
integration of large amounts of wind energy into the power grid requires a paradigm shift in how the grid is
operated. The following report provides an evaluation of how system operations worldwide are changing in
response to increases in wind penetration. It provides unique insights into what has worked well, what has not,
and how operators see the future of their responsibilities changing as we introduce more wind into the grid.
While there is no "silver bullet" solution to successful wind integration that applies to all power systems,
experience from actual operators provides some of the most valuable feedback on how operations are
changing, what tools are needed and which actions are providing the most benefit. This report constitutes the
first time grid operators from across the globe have provided consolidated feedback on how wind energy is
affecting them and what concrete measures are being taken to manage the system in the face of the increased
challenges posed by the increased variability and uncertainty presented by wind generation. The U.S.
Department of Energy is pleased to support this project through the American Recovery and Reinvestment
Act, to enable such valuable information to be provided to the utility industry as a whole.},
  comment     = {Comprehensive 2011 review of control rooms
-- includes several refs to prob. forecasts.

Seems like a good starting point.},
  file        = {Jones11renewEnrgyStrat.pdf:Jones11renewEnrgyStrat.pdf:PDF},
  groups      = {Use, doReadWPV_1},
  owner       = {sotterson},
  timestamp   = {2013.09.27},
  url         = {http://apps1.eere.energy.gov/news/progress_alerts.cfm/pa_id=656},
}

@InCollection{An10voteAvgEns,
  author    = {An, Kun and Meng, Jiang},
  title     = {Voting-Averaged Combination Method for Regressor Ensemble},
  booktitle = {Advanced Intelligent Computing Theories and Applications},
  publisher = {Springer Berlin Heidelberg},
  year      = {2010},
  editor    = {Huang, De-Shuang and Zhao, Zhongming and Bevilacqua, Vitoantonio and Figueroa, JuanCarlos},
  volume    = {6215},
  series    = {Lecture Notes in Computer Science},
  pages     = {540--546},
  isbn      = {978-3-642-14921-4},
  abstract  = {A voting-averaged (VOA) method is presented to combine an ensemble for the regression tasks. VOA can select ensemble components dynamically using the hidden selectivity mechanism of voting, and hence VOA can be regarded as an improvement and extension of both voting and average methods. The experiment results of ten regression tasks show VOA and a representative selective average (SEA) method of GASEN (genetic algorithm-based selective ensemble), are of similar performances to each other, and both of better performance than simple average (SIA) in Bagging ensemble. SEA produces the ensemble subset in the using genetic optimization with validation datasets after the individuals are trained well; however, VOA combines a selective ensemble directly according to the cluster of the component outputs, not to determine ensemble subsets beforehand.},
  comment   = {Maybe a regime switching algorithm since weights can be shifted "dynamically." Could also be adaboost compatible, I think.},
  doi       = {10.1007/978-3-642-14922-1_67},
  owner     = {sotterson},
  timestamp = {2014.01.21},
}

@Article{Lave13solarVarWvlt,
  author    = {Lave, M. and Kleissl, J. and Stein, J.S.},
  title     = {A Wavelet-Based Variability Model (WVM) for Solar {PV} Power Plants},
  journal   = {Sustainable Energy, IEEE Transactions on},
  year      = {2013},
  volume    = {4},
  number    = {2},
  pages     = {501--509},
  month     = apr,
  issn      = {1949-3029},
  abstract  = {A wavelet variability model (WVM) for simulating solar photovoltaic (PV) power plant output given a single irradiance point sensor timeseries using spatio-temporal correlations is presented. The variability reduction (VR) that occurs in upscaling from the single point sensor to the entire PV plant at each timescale is simulated, then combined with the wavelet transform of the point sensor timeseries to produce a simulated power plant output. The WVM is validated against measurements at a 2-MW residential rooftop distributed PV power plant in Ota City, Japan and at a 48-MW utility-scale power plant in Copper Mountain, NV. The WVM simulation matches the actual power output well for all variability timescales, and the WVM compares well against other simulation methods.},
  comment   = {A wavelet based method for estimating the spatio-temporal variance of a large solar plant given point irradiance measurements. Could be good for PV or Wind (see LNEG paper). Could also be a compliment to DTU spatio-temporal techniques. Or, the aggregation techniques in Cormode10histoWvltProb or Zhao10wvltUncTseries

Easy to read but not too detailed article evernoted here:
https://www.evernote.com/Home.action?csrfBusterToken=7ca63c71\#st=p&n=734eed2b-d535-4923-a8a3-a255e2ec5dd7},
  doi       = {10.1109/TSTE.2012.2205716},
  file      = {Lave13solarVarWvlt.pdf:Lave13solarVarWvlt.pdf:PDF},
  groups    = {doReadWPV_2, PV},
  keywords  = {photovoltaic power systems;solar power stations;wavelet transforms;Copper Mountain;Japan;NV;Ota City;WVM;power 2 MW;power 48 MW;residential rooftop distributed PV power plant;single irradiance point sensor timeseries;single point sensor;solar photovoltaic power plant;spatio-temporal correlations;utility-scale power plant;variability reduction;wavelet transform;wavelet-based variability model;Cities and towns;Clouds;Copper;Correlation;Indexes;Power generation;Wavelet transforms;Geographic dispersion;photovoltaic (PV);solar;upscaling;variability;wavelets},
  owner     = {sotterson},
  timestamp = {2014.03.25},
}

@Article{Xu10wgtPLSgrp,
  author    = {Xu, Heng and Cai, Wensheng and Shao, Xueguang},
  title     = {Weighted partial least squares regression by variable grouping strategy for multivariate calibration of near infrared spectra},
  journal   = {Analytical Methods},
  year      = {2010},
  volume    = {2},
  number    = {3},
  pages     = {289--294},
  abstract  = {A weighted partial least squares (PLS) regression method for multivariate calibration of near infrared (NIR) spectra is proposed. In the method, the spectra are split into groups of variables according to the statistic values of variables, i.e., the stability, which has been used to evaluate the importance of variables in a calibration model. Because the stability reflects the relative importance of the variables for modeling, these groups present different spectral information for construction of PLS models. Therefore, if a weight which is proportional to the stability is assigned to each sub-model built with different group variables, a combined model can be built by a weighted combination of the sub-models. This method is different from the commonly used variable selection strategies, making full use of the variables according to their importance, instead of only the important ones. To validate the performance of the proposed method, it was applied to two different NIR spectral data sets. Results show that the proposed method can effectively utilize all variables in the spectra and enhance the prediction ability of the PLS model.},
  comment   = {weighted pls, maybe easy way to incorporate into local learning. Grouping would work w/ dir and spline basis?},
  file      = {Xu10wgtPLSgrp.pdf:Xu10wgtPLSgrp.pdf:PDF},
  owner     = {sotterson},
  publisher = {The Royal Society of Chemistry},
  timestamp = {2014.04.27},
  url       = {http://pubs.rsc.org/en/content/articlehtml/2010/ay/b9ay00257j},
}

@Article{Lydia13powCurvModelAdvncd,
  author    = {Lydia, M. and Selvakumar, A.I. and Kumar, S.S. and Kumar, G.E.P.},
  title     = {Advanced Algorithms for Wind Turbine Power Curve Modeling},
  journal   = {IEEE Transactions on Sustainable Energy},
  year      = {2013},
  volume    = {4},
  number    = {3},
  pages     = {827--835},
  abstract  = {A wind turbine power curve essentially captures the performance of the wind turbine. The power curve depicts the relationship between the wind speed and output power of the turbine. Modeling of wind turbine power curve aids in performance monitoring of the turbine and also in forecasting of power. This paper presents the development of parametric and nonparametric models of wind turbine power curves. Parametric models of the wind turbine power curve have been developed using four and five parameter logistic expressions. The parameters of these expressions have been solved using advanced algorithms like genetic algorithm (GA), evolutionary programming (EP), particle swarm optimization (PSO), and differential evolution (DE). Nonparametric models have been evolved using algorithms like neural networks, fuzzy c-means clustering, and data mining. The modeling of wind turbine power curve is done using five sets of data; one is a statistically generated set and the others are real-time data sets. The results obtained have been compared using suitable performance metrics and the best method for modeling of the power curve has been obtained.},
  comment   = {Simple physical model of power curve, including blade pitch angle. Improved on this w/ machine learning methods.

Physical model could be logged to make angle, etc. additive, dep on speed could still be a 1D spline if but in log domain. Could be good for ReWP.},
  doi       = {10.1109/TSTE.2013.2247641},
  file      = {Lydia13powCurvModelAdvncd.pdf:Lydia13powCurvModelAdvncd.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.10},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6491505},
}

@Article{Cutler07rampWPPT,
  author    = {Cutler, Nicholas and Kay, Merlinde and Jacka, Kieran and Nielsen, Torben Skov},
  title     = {Detecting, categorizing and forecasting large ramps in wind farm power output using meteorological observations and WPPT},
  journal   = {Wind Energy},
  year      = {2007},
  volume    = {10},
  number    = {5},
  pages     = {453--470},
  issn      = {1099-1824},
  abstract  = {Abstract 10.1002/we.235.abs The Wind Power Prediction Tool (WPPT) has been installed in Australia for the first time, to forecast the power output from the 65MW Roaring 40s Renewable Energy P/L Woolnorth Bluff Point wind farm. This article analyses the general performance of WPPT as well as its performance during large ramps (swings) in power output. In addition to this, detected large ramps are studied in detail and categorized. WPPT combines wind speed and direction forecasts from the Australian Bureau of Meteorology regional numerical weather prediction model, MesoLAPS, with real-time wind power observations to make hourly forecasts of the wind farm power output. The general performances of MesoLAPS and WPPT are evaluated over 1 year using the root mean square error (RMSE). The errors are significantly lower than for basic benchmark forecasts but higher than for many other WPPT installations, where the site conditions are not as complicated as Woolnorth Bluff Point. Large ramps are considered critical events for a wind power forecast for energy trading as well as managing power system security. A methodology is developed to detect large ramp events in the wind farm power data. Forty-one large ramp events are detected over 1 year and these are categorized according to their predictability by MesoLAPS, the mechanical behaviour of the wind turbine, the power change observed on the grid and the source weather event. During these events, MesoLAPS and WPPT are found to give an RMSE only roughly equivalent to just predicting the mean (climatology forecast). Copyright ?? 2007 John Wiley \& Sons, Ltd.},
  comment   = {Ramp detection heuristic with some tuning. Could use for phase error detection (only detect on ramps) Doesn't really forecast ramps. Just evaluates WPPT performance on them. Also categorizes what happened during ramps.},
  doi       = {10.1002/we.235},
  file      = {Cutler07rampWPPT.pdf:Cutler07rampWPPT.pdf:PDF},
  keywords  = {wind power forecast, short-term prediction, large ramp, swing, meteorology, weather event, power system security, energy trading},
  owner     = {scot},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2010.11.23},
}

@Article{Steiner17critWeathWindPow,
  author   = {Andrea Steiner and Carmen K{\"o}hler and Isabel Metzinger and Axel Braun and Mathias Zirkelbach and Dominique Ernst and Peter Tran and Bodo Ritter},
  title    = {Critical weather situations for renewable energies -- Part A: Cyclone detection for wind power},
  journal  = {Renewable Energy},
  year     = {2017},
  volume   = {101},
  pages    = {41 - 50},
  issn     = {0960-1481},
  abstract = {Abstract A constantly increasing share of weather dependent renewable energies in Germany's power mix poses new challenges concerning grid management and security of energy supply. An evaluation of the three year period from 2012 to 2014 reveals, that 60\% of days with largest errors in the day-ahead wind power forecasts for Germany are linked to cyclones and troughs traversing the North Sea, the Baltic Sea or Germany. A cyclone detection algorithm has been developed to automatically indicate these critical weather situations. The algorithm is based on Numerical Weather Prediction model forecasts of mean sea level pressure. The cyclone detection is used to design an automated weather information tool for end-users such as Transmission System Operators (TSOs). For 2014, it identified a critical weather development in 38\% of all days. The root mean square error of day-ahead wind power forecasts increased by 1\% of installed capacity during these periods. A real time application of the tool is being implemented in order to support a sustainable and save integration of the increasing wind power production. It will then be provided to, and will be tested by, three German \{TSOs\} with the purpose of an operative usage to guarantee the security of supply.},
  doi      = {http://doi.org/10.1016/j.renene.2016.08.013},
  file     = {:Steiner17critWeathWindPow.pdf:PDF},
  keywords = {Wind energy, Weather dependency, Cyclone detection, Net stability, Transmission system operator },
  url      = {http://www.sciencedirect.com/science/article/pii/S0960148116307145},
}

@Article{Larson16dayAheadSolarPVfrcst,
  author    = {David P. Larson and Lukas Nonnenmacher and Carlos F.M. Coimbra},
  title     = {Day-ahead forecasting of solar power output from photovoltaic plants in the American Southwest},
  journal   = {Renewable Energy},
  year      = {2016},
  volume    = {91},
  pages     = {11 - 20},
  issn      = {0960-1481},
  abstract  = {Abstract A forecasting method for hourly-averaged, day-ahead power output (PO) from photovoltaic (PV) power plants based on least-squares optimization of Numerical Weather Prediction (NWP) is presented. Three variations of the forecasting method are evaluated against \{PO\} data from two non-tracking, 1??MWp \{PV\} plants in California for 2011???2014. The method performance, including the inter-annual performance variability and the spatial smoothing of pairing the two plants, is evaluated in terms of standard error metrics, as well as in terms of the occurrence of severe forecasting error events. Results validate the performance of the proposed methodology as compared with previous studies. We also show that the bias errors in the irradiance inputs only have a limited impact on the \{PO\} forecast performance, since the method corrects for systematic errors in the irradiance forecast. The relative root mean square error (RMSE) for \{PO\} is in the range of 10.3%???14.0% of the nameplate capacity, and the forecast skill ranges from 13% to 23% over a persistence model. Over three years, an over-prediction of the daily \{PO\} exceeding 40% only occurs twice at one of the two plants under study, while the spatially averaged \{PO\} of the paired plants never exceeds this threshold. },
  comment   = {Some kind of PV upscaling, regional averaging.  Has graphs, simple?  Use for Colombia GIZ course?},
  doi       = {https://doi.org/10.1016/j.renene.2016.01.039},
  file      = {Larson16dayAheadSolarPVfrcst.pdf:Larson16dayAheadSolarPVfrcst.pdf:PDF},
  keywords  = {Forecasting, Solar power output, Photovoltaics, Numerical weather prediction, Interannual performance variability },
  owner     = {sotterson},
  timestamp = {2017.04.30},
  url       = {http://www.sciencedirect.com/science/article/pii/S0960148116300398},
}

@Article{Koehler17critWeathSolarPow,
  author   = {Carmen K{\"o}hler and Andrea Steiner and Yves-Marie Saint-Drenan and Dominique Ernst and Anja Bergmann-Dick and Mathias Zirkelbach and Zied Ben Bouall{\'e}gue and Isabel Metzinger and Bodo Ritter},
  title    = {Critical weather situations for renewable energies -- Part B: Low stratus risk for solar power},
  journal  = {Renewable Energy},
  year     = {2017},
  volume   = {101},
  pages    = {794 - 803},
  issn     = {0960-1481},
  abstract = {Abstract Accurately predicting the formation, development and dissipation of fog and low stratus (LS) still poses a challenge for numerical weather prediction (NWP) models. Errors in the low cloud cover \{NWP\} forecasts directly impact the quality of photovoltaic (PV) power prediction. On days with LS, day-ahead forecast errors of Germany-wide \{PV\} power frequently lie within the magnitude of the balance energy and thus pose a challenge for maintaining grid stability. An indication in advance about the possible occurrence of a critical weather situation such as \{LS\} would represent a helpful tool for transmission system operators (TSOs) in their day-to-day business. In the following, a detection algorithm for low stratus risk (LSR) is developed and applied as post-processing to the \{NWP\} model forecasts of the regional non-hydrostatic model COSMO-DE, operational at the German Weather Service. The aim of the \{LSR\} product is to supply day-ahead warnings and to support the decision making process of the TSOs. The quality of the \{LSR\} is assessed by comparing the computed regions of \{LSR\} occurrence with a satellite based cloud classification product from the Nowcasting Satellite Facility (NWCSAF). The results show that the \{LSR\} provides additional information that should in particular be useful for risk adverse users. },
  comment  = {First day writeup for a weather forecasting course.  Gives a broad overview of NWP.

NWP solves dynamical equations (TODO: which ones?)},
  doi      = {http://doi.org/10.1016/j.renene.2016.09.002},
  file     = {Koehler17critWeathSolarPow.pdf:Koehler17critWeathSolarPow.pdf:PDF},
  keywords = {Low stratus, Fog, Photovoltaic, Power forecast, Numerical weather prediction, Solar radiation },
  url      = {http://www.sciencedirect.com/science/article/pii/S0960148116307844},
}

@Article{SaezGallego14probRsrvRqtsDet,
  author    = {Javier Saez-Gallego and Juan M. Morales and Henrik Madsen and Tryggvi J??nsson},
  title     = {Determining reserve requirements in \{DK1\} area of Nord Pool using a probabilistic approach},
  journal   = {Energy},
  year      = {2014},
  volume    = {74},
  number    = {0},
  pages     = {682--693},
  issn      = {0360-5442},
  note      = {International Conference on Effi ciency, Cost, Optimization, Simulation and Environmental Impact of Energy Systems ??? \{ECOS\} 2013},
  abstract  = {Abstract Allocation of electricity reserves is the main tool for transmission system operators to guarantee a reliable and safe real-time operation of the power system. Traditionally, a deterministic criterion is used to establish the level of reserve. Alternative criteria are given in this paper by using a probabilistic framework where the reserve requirements are computed based on scenarios of wind power forecast error, load forecast errors and power plant outages. Our approach is first motivated by the increasing wind power penetration in power systems worldwide as well as the current market design of the \{DK1\} area of Nord Pool, where reserves are scheduled prior to the closure of the day-ahead market. The risk of the solution under the resulting reserve schedule is controlled by two measures: the \{LOLP\} (Loss-of-Load Probability) and the \{CVaR\} (Conditional Value at Risk). Results show that during the case study period, the \{LOLP\} methodology produces more costly and less reliable reserve schedules, whereas the solution from the CVaR-method increases the safety of the overall system while decreasing the associated reserve costs, with respect to the method currently used by the Danish \{TSO\} (Transmission System Operator).},
  comment   = {Scenario based reserve requirements opt. Very related to ReWP

Could use Rockafellar14superQuantRgrssn for the conditional value at risk part.},
  doi       = {10.1016/j.energy.2014.07.034},
  keywords  = {Reserve determination},
  owner     = {sotterson},
  timestamp = {2014.11.07},
  url       = {http://www.sciencedirect.com/science/article/pii/S0360544214008561},
}

@InCollection{Patton13copulaMVtimeSerFrcstBkCh,
  author    = {Andrew Patton},
  title     = {Chapter 16 - Copula Methods for Forecasting Multivariate Time Series},
  booktitle = {Handbook of Economic Forecasting},
  publisher = {Elsevier},
  year      = {2013},
  editor    = {Graham Elliott and Allan Timmermann},
  volume    = {2, Part B},
  series    = {Handbook of Economic Forecasting},
  pages     = {899 - 960},
  abstract  = {Abstract Copula-based models provide a great deal of flexibility in modeling multivariate distributions, allowing the researcher to specify the models for the marginal distributions separately from the dependence structure (copula) that links them to form a joint distribution. In addition to flexibility, this often also facilitates estimation of the model in stages, reducing the computational burden. This chapter reviews the growing literature on copula-based models for economic and financial time series data, and discusses in detail methods for estimation, inference, goodness-of-fit testing, and model selection that are useful when working with these models. A representative data set of two daily equity index returns is used to illustrate all of the main results.},
  comment   = {Book chapter so maybe worth reading.

Time series forecasting w/ Copulas.
 Linear Correlation in Copula-Based Multivariate Models
time-varying
tail dependence,
...},
  doi       = {http://dx.doi.org/10.1016/B978-0-444-62731-5.00016-6},
  file      = {Patton13copulaMVtimeSerFrcstBkCh.pdf:Patton13copulaMVtimeSerFrcstBkCh.pdf:PDF},
  issn      = {1574-0706},
  keywords  = {Dependence, Correlation, Tail risk, Volatility, Density forecasting },
  url       = {http://www.sciencedirect.com/science/article/pii/B9780444627315000166},
}

@Article{Zhang16detProbIntrvVari,
  author    = {Yachao Zhang and Kaipei Liu and Liang Qin and Xueli An},
  title     = {Deterministic and probabilistic interval prediction for short-term wind power generation based on variational mode decomposition and machine learning methods},
  journal   = {Energy Conversion and Management},
  year      = {2016},
  volume    = {112},
  pages     = {208 - 219},
  issn      = {0196-8904},
  abstract  = {Abstract Due to the increasingly significant energy crisis nowadays, the exploitation and utilization of new clean energy gains more and more attention. As an important category of renewable energy, wind power generation has become the most rapidly growing renewable energy in China. However, the intermittency and volatility of wind power has restricted the large-scale integration of wind turbines into power systems. High-precision wind power forecasting is an effective measure to alleviate the negative influence of wind power generation on the power systems. In this paper, a novel combined model is proposed to improve the prediction performance for the short-term wind power forecasting. Variational mode decomposition is firstly adopted to handle the instability of the raw wind power series, and the subseries can be reconstructed by measuring sample entropy of the decomposed modes. Then the base models can be established for each subseries respectively. On this basis, the combined model is developed based on the optimal virtual prediction scheme, the weight matrix of which is dynamically adjusted by a self-adaptive multi-strategy differential evolution algorithm. Besides, a probabilistic interval prediction model based on quantile regression averaging and variational mode decomposition-based hybrid models is presented to quantify the potential risks of the wind power series. The simulation results indicate that: (1) the normalized mean absolute errors of the proposed combined model from one-step to three-step forecasting are 4.34\%, 6.49\% and 7.76\%, respectively, which are much lower than those of the base models and the hybrid models based on the signal decomposition techniques; (2) the interval forecasting model proposed can provide reliable and excellent prediction results for a certain expectation probability, which is an effective and reliable tool for the short-term wind power probabilistic interval prediction. },
  doi       = {http://dx.doi.org/10.1016/j.enconman.2016.01.023},
  file      = {Zhang16detProbIntrvVari.pdf:Zhang16detProbIntrvVari.pdf:PDF},
  keywords  = {Wind power, Variational mode decomposition, Combined forecasting model, Self-adaptive multi-strategy differential evolution, Probabilistic interval prediction, Quantile regression averaging },
  owner     = {sotterson},
  timestamp = {2017.01.16},
  url       = {http://www.sciencedirect.com/science/article/pii/S0196890416000406},
}

@Article{Chen17copulaTimeTrav,
  author    = {Min Chen and Guizhen Yu and Peng Chen and Yunpeng Wang},
  title     = {A copula-based approach for estimating the travel time reliability of urban arterial},
  journal   = {Transportation Research Part C: Emerging Technologies},
  year      = {2017},
  volume    = {82},
  pages     = {1 - 23},
  issn      = {0968-090X},
  abstract  = {Abstract Estimating the travel time reliability (TTR) of urban arterial is critical for real-time and reliable route guidance and provides theoretical bases and technical support for sophisticated traffic management and control. The state-of-art procedures for arterial \{TTR\} estimation usually assume that path travel time follows a certain distribution, with less consideration about segment correlations. However, the conventional approach is usually unrealistic because an important feature of urban arterial is the dependent structure of travel times on continuous segments. In this study, a copula-based approach that incorporates the stochastic characteristics of segments travel time is proposed to model arterial travel time distribution (TTD), which serves as a basis for \{TTR\} quantification. First, segments correlation is empirically analyzed and different types of copula models are examined. Then, fitting marginal distributions for segment \{TTD\} is conducted by parametric and non-parametric regression analysis, respectively. Based on the estimated parameters of the models, the best-fitting copula is determined in terms of the goodness-of-fit tests. Last, the model is examined at two study sites with \{AVI\} data and \{NGSIM\} trajectory data, respectively. The results of path \{TTD\} estimation demonstrate the advantage of the proposed copula-based approach, compared with the convolution model without capturing segments correlation and the empirical distribution fitting methods. Furthermore, when considering the segments correlation effect, it was found that the estimated path \{TTR\} is more accurate than that by the convolution model. },
  comment   = {Traffic route time is represented as a copula for the individual path segments, thus modelling the correlation between different path lengths better.  Also compares Pearson's Spearman's and Kendal's Tau correlation.

Pearson's, Spearman's, and Kendal's Tau correlation.
* linear (Pearson's) not invariant under nonlinear strictly increasing transformations
   - e.g. a copula marginal transform
   -  so can't preserve linear correlation in original space
* Rank based correlation
   - options: Spearman's rho, Kendall's tau.
   - Spearman confidence intervals less relilable and interpretable than Kendall
      - this is from a paper they cite:  Kendall and Gibbons (1990)
      - they do a test and agree with it (See 4.2.1, but they don't really explain their reasoning)
      - anyway, they ultimatly use Kendall's tau (},
  doi       = {https://doi.org/10.1016/j.trc.2017.06.007},
  file      = {:papers\\Chen17copulaTimeTrav.pdf:PDF},
  keywords  = {Urban arterial, Travel time distribution, Travel time reliability, Segment correlation, Copula},
  owner     = {sotterson},
  timestamp = {2017.07.05},
  url       = {http://www.sciencedirect.com/science/article/pii/S0968090X17301602},
}

@Article{Bennasar15jointMImaxFeatSel,
  author   = {Mohamed Bennasar and Yulia Hicks and Rossitza Setchi},
  title    = {Feature selection using Joint Mutual Information Maximisation},
  journal  = {Expert Systems with Applications},
  year     = {2015},
  volume   = {42},
  number   = {22},
  pages    = {8520--8532},
  issn     = {0957-4174},
  abstract = {Abstract Feature selection is used in many application areas relevant to expert and intelligent systems, such as data mining and machine learning, image processing, anomaly detection, bioinformatics and natural language processing. Feature selection based on information theory is a popular approach due its computational efficiency, scalability in terms of the dataset dimensionality, and independence from the classifier. Common drawbacks of this approach are the lack of information about the interaction between the features and the classifier, and the selection of redundant and irrelevant features. The latter is due to the limitations of the employed goal functions leading to overestimation of the feature significance. To address this problem, this article introduces two new nonlinear feature selection methods, namely Joint Mutual Information Maximisation (JMIM) and Normalised Joint Mutual Information Maximisation (NJMIM); both these methods use mutual information and the ???maximum of the minimum??? criterion, which alleviates the problem of overestimation of the feature significance as demonstrated both theoretically and experimentally. The proposed methods are compared using eleven publically available datasets with five competing methods. The results demonstrate that the \{JMIM\} method outperforms the other methods on most tested public datasets, reducing the relative average classification error by almost 6% in comparison to the next best performing method. The statistical significance of the results is confirmed by the \{ANOVA\} test. Moreover, this method produces the best trade-off between accuracy and stability.},
  comment  = {Feature selection with Joint Mutual Information -- a one-to-one MI low dim MI technique like Deng&Peng mRMR but better.

* Is not the same as "JMI" method mentioned in Vergara14revFeatSelMutInfo, and found to be best in Brown12condLikInfoFeatSel

Is tested on classification problems, many algorithms compared.
* Overall, it seems that JMIM or JMI are the best,
* JMI better on some problems but oveall JMIM is more accurate
* JMIM possibly having a better accuracy/stability tradeoff.
Weaknesses
* Does not use high dim MI like Frenzel07partMutInfo and later use.
* Is a Filter method so doesn't consider learning algorithm interaction (like a wrapper, I suppose)},
  doi      = {10.1016/j.eswa.2015.07.007},
  file     = {Bennasar15jointMImaxFeatSel.pdf:Bennasar15jointMImaxFeatSel.pdf:PDF},
  keywords = {Feature selection, Mutual information, Joint mutual information, Conditional mutual information, Subset feature selection, Classification, Dimensionality reduction, Feature selection stability },
}

@Article{Asir16hiDimFeatSelLitRev,
  author    = {Asir, D and Appavu, S and Jebamalar, E},
  title     = {Literature Review on Feature Selection Methods for High-Dimensional Data},
  journal   = {International Journal of Computer Applications},
  year      = {2016},
  volume    = {136},
  number    = {1},
  pages     = {9--17},
  abstract  = {ABSTRACT
Feature selection plays a significant role in improving the
performance of the machine learning algorithms in terms of
reducing the time to build the learning model and increasing
the accuracy in the learning process. Therefore, the
researchers pay more attention on the feature selection to
enhance the performance of the machine learning algorithms.
Identifying the suitable feature selection method is very
essential for a given machine learning task with high-
dimensional data. Hence, it is required to conduct the study on
the various feature selection methods for the research
community especially dedicated to develop the suitable
feature selection method for enhancing the performance of the
machine learning tasks on high-dimensional data. In order to
fulfill this objective, this paper devotes the complete literature
review on the various feature selection methods for high-
dimensional data.
General Terms
Literature review on feature selection methods, study on
feature selection, wrapper-based feature selection, embedded-
based feature selection, hybrid feature selection, filter-based
feature selection, feature subset-based feature selection,
feature ranking-based feature selection, attribute selection,
dimensionality reduction, variable selection, survey on feature
selection, feature selection for high-dimensional data,
introduction to variable and feature selection, feature selection
for classification.
Keywords
Introduction to variable and feature selection, information
gain-based feature selection, gain ratio-based feature
selection, symmetric uncertainty-based feature selection,
subset-based feature selection, ranking-based feature
selection, wrapper-based feature selection, embedded-based
feature selection, filter-based feature selection, hybrid feature
selection, selecting feature from high-dimensional data.},
  comment   = {I don't know if it's good but it's new.  Maybe worth skimming},
  doi       = {10.5120/ijca2016908317},
  file      = {Asir16hiDimFeatSelLitRev.pdf:Asir16hiDimFeatSelLitRev.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2016.06.22},
}

@Article{Zhang15solFrcstMetrics,
  author   = {Jie Zhang and Anthony Florita and Bri-Mathias Hodge and Siyuan Lu and Hendrik F. Hamann and Venkat Banunarayanan and Anna M. Brockway},
  title    = {A suite of metrics for assessing the performance of solar power forecasting},
  journal  = {Solar Energy},
  year     = {2015},
  volume   = {111},
  pages    = {157 - 175},
  issn     = {0038-092X},
  abstract = {Abstract Forecasting solar energy generation is a challenging task because of the variety of solar power systems and weather regimes encountered. Inaccurate forecasts can result in substantial economic losses and power system reliability issues. One of the key challenges is the unavailability of a consistent and robust set of metrics to measure the accuracy of a solar forecast. This paper presents a suite of generally applicable and value-based metrics for solar forecasting for a comprehensive set of scenarios (i.e., different time horizons, geographic locations, and applications) that were developed as part of the U.S. Department of Energy SunShot Initiative???s efforts to improve the accuracy of solar forecasting. In addition, a comprehensive framework is developed to analyze the sensitivity of the proposed metrics to three types of solar forecasting improvements using a design-of-experiments methodology in conjunction with response surface, sensitivity analysis, and nonparametric statistical testing methods. The three types of forecasting improvements are (i) uniform forecasting improvements when there is not a ramp, (ii) ramp forecasting magnitude improvements, and (iii) ramp forecasting threshold changes. Day-ahead and 1-hour-ahead forecasts for both simulated and actual solar power plants are analyzed. The results show that the proposed metrics can efficiently evaluate the quality of solar forecasts and assess the economic and reliability impacts of improved solar forecasting. Sensitivity analysis results show that (i) all proposed metrics are suitable to show the changes in the accuracy of solar forecasts with uniform forecasting improvements, and (ii) the metrics of skewness, kurtosis, and R??nyi entropy are specifically suitable to show the changes in the accuracy of solar forecasts with ramp forecasting improvements and a ramp forecasting threshold. },
  doi      = {http://doi.org/10.1016/j.solener.2014.10.016},
  file     = {Zhang15solFrcstMetrics.pdf:Zhang15solFrcstMetrics.pdf:PDF},
  keywords = {Grid integration, Nonparametric statistical testing, Solar power forecasting, Solar power ramps, Sensitivity analysis },
  url      = {http://www.sciencedirect.com/science/article/pii/S0038092X14005027},
}

@Article{SaintDrenan17probRgnlPVfrcst,
  author   = {Y.M. Saint-Drenan and G.H. Good and M. Braun},
  title    = {A probabilistic approach to the estimation of regional photovoltaic power production},
  journal  = {Solar Energy},
  year     = {2017},
  volume   = {147},
  pages    = {257-276},
  issn     = {0038-092X},
  abstract = {Abstract Forecasting the total photovoltaic (PV) power generated in the control areas of the transmission system operators (TSO) is an important step in the integration of the large amounts of \{PV\} energy into the German electricity supply system. A standard approach for evaluating the regional \{PV\} power generation from weather forecast consists in upscaling the forecast of a limited set of reference plants to the complete area. Previous studies shown that this method can lead to large errors when the set of reference plants has different characteristics or weather conditions than the set of unknown plants. In this paper, an alternative to the upscaling approach is proposed. In this method, called a probabilistic regional \{PV\} model, an average \{PV\} model with a very limited number of inputs (two module orientation angles) is used to calculate the power generation of the most frequent module orientation angles. The resulting power values are finally weighted according to their probability of occurrence to estimate the actual power generation. The implementation of this model thus only requires information on the location and peak capacity of the plant installed in a region and no \{PV\} plant measurement is necessary. The proposed method has been evaluated against the estimate of the total power generation provided by the German TSOs, which shows that an \{RMSE\} ranging from 4.2 to 4.9\% can be obtained with this method using on \{IFS\} meteorological forecast. The regional power forecasted with the probabilistic approach was also compared to the day-ahead forecast disseminated by the TSO. This analysis shows that the forecast evaluated with the proposed approach has an \{RMSE\} less than 0.5\% higher than the reference forecasts. This is considered a promising result given that the forecast evaluated with the probabilistic model is based on one single weather model and that ??? at the exception of the model calibration - no statistical post-processing method is used to optimize its performance. },
  comment  = {Yve-Marie's probabilistic PV panel upscaling algorithm.  Use for GIZ colombia?

At least look at the intro for a summary of solar forecast state of the art.},
  doi      = {http://dx.doi.org/10.1016/j.solener.2017.03.007},
  file     = {SaintDrenan17probRgnlPVfrcst.pdf:SaintDrenan17probRgnlPVfrcst.pdf:PDF},
  keywords = {Photovoltaic, Forecast, Regional model },
  url      = {http://www.sciencedirect.com/science/article/pii/S0038092X17301676},
}

@Article{Spaeth86ClustLinRgrsnLED,
  author    = {Sp{\"a}th, H},
  title     = {Clusterwise linear least absolute deviations regression},
  journal   = {Computing},
  year      = {1986},
  volume    = {37},
  number    = {4},
  pages     = {371--377},
  abstract  = {Abstract from a technical manual that uses the RMSE version of this algorithm:

This algorithm is fairly simple to describe. The number of clusters, K, for a given run is fixed. The rows are randomly sorted into the groups to form K initial clusters. An exchange algorithm is applied to this initial configuration which searches for the rows of data that would produce a maximum decrease in a least-squares penalty function (that is, maximizing the increase in R-squared at each step). The algorithm continues until no beneficial exchange of rows can be found.
Our experience with this algorithm indicates that its success depends heavily upon the initial-random configuration. For this reason, we suggest that you try many different configurations. In one test, we found that the optimum resulted from only one in about fifteen starting configurations. Hence, we suggest that you repeat the process twenty-five or thirty times. The program lets you specify the number of repetitions.},
  comment   = {Clusters datas to the points fit on separate lines. Fortran available on net. It's also in a commercial package, which shows some results (attached too). Could use this for local linear neighbhorhood sizing.

Very simple, heuristic, and computationally not so efficient. Must restart many times, according to the software manual advice.},
  file      = {paper:Spaeth86ClustLinRgrsnLED.pdf:PDF;Software manual w/ advice:Spaeth86ClustLinRgrsnLED_manual.pdf:PDF},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2014.03.31},
}

@Article{Aguilera13psplinePCA,
  author    = {A.M. Aguilera and M.C. Aguilera-Morillo},
  title     = {Penalized \{PCA\} approaches for B-spline expansions of smooth functional data},
  journal   = {Applied Mathematics and Computation},
  year      = {2013},
  volume    = {219},
  number    = {14},
  pages     = {7805--7819},
  issn      = {0096-3003},
  abstract  = {Abstract Functional principal component analysis (FPCA) is a dimension reduction technique that explains the dependence structure of a functional data set in terms of uncorrelated variables. In many applications the data are a set of smooth functions observed with error. In these cases the principal components are difficult to interpret because the estimated weight functions have a lot of variability and lack of smoothness. The most common way to solve this problem is based on penalizing the roughness of a function by its integrated squared d-order derivative. Two alternative forms of penalized \{FPCA\} based on B-spline basis expansions of sample curves and a simpler discrete penalty that measures the roughness of a function by summing squared d-order differences between adjacent B-spline coefficients (P-spline penalty) are proposed in this paper. The main difference between both smoothed \{FPCA\} approaches is that the first uses the P-spline penalty in the least squares approximation of the sample curves in terms of a B-spline basis meanwhile the second introduces the P-spline penalty in the orthonormality constraint of the algorithm that computes the principal components. Leave-one-out cross-validation is adapted to select the smoothing parameter for these two smoothed \{FPCA\} approaches. A simulation study and an application with chemometric functional data are developed to test the performance of the proposed smoothed approaches and to compare the results with non penalized \{FPCA\} and regularized FPCA.

Keywords:
Functional data
Principal component analysis
B-spline expansion
Roughness penalty
P-splines},
  comment   = {PCA assuming data fits multivariate roughness-penalized splines. Good for cfpcQR when do PCA on directly on spline bases first, rather than doing splineQR on each dimension first (could do it in the next step). Has R. Mentioned function PLS in [15], which might also be interesting

PCA vs. functional (example: spline) PCA
http://en.wikipedia.org/wiki/Functional_principal_component_analysis\#Applications

* analytical matrix equation for psplines (eq 3)
* mentions that Fourier and periodic wavelet basis have been successfully used instead of splines in functional PCA (just says they've been used, doesn't say if that's better)
* a rule of thumb for num. knots suggestion comes from: Ruppert02numKntsPenSpln
* author's PhD thesis: Morillo13PenEstFDAphd

Could these be compressed by: Lu09UncorrMultiLinPCA ?},
  doi       = {10.1016/j.amc.2013.02.009},
  file      = {Aguilera13psplinePCA.pdf:Aguilera13psplinePCA.pdf:PDF},
  keywords  = {Functional data},
  owner     = {sotterson},
  timestamp = {2014.11.07},
  url       = {http://www.sciencedirect.com/science/article/pii/S0096300313001252},
}

@Article{Luoma14valFrcstCA,
  author   = {Jennifer Luoma and Patrick Mathiesen and Jan Kleissl},
  title    = {Forecast value considering energy pricing in California},
  journal  = {Applied Energy},
  year     = {2014},
  volume   = {125},
  pages    = {230 - 237},
  issn     = {0306-2619},
  abstract = {Abstract In this study, production forecast value is investigated using day-ahead market (DAM) and real-time market (RTM) locational marginal prices (LMP) at 63 sites in California. Using the North American Mesoscale (NAM) Model, day-ahead global horizontal irradiance (GHI) forecasts are established and converted to power assuming that a 1 \{MW\} solar photovoltaic plant is co-located at each observation site. Using this forecast, energy is hypothetically sold in the DAM. As the \{RTM\} occurs, deviations between forecast and observation are settled by hypothetically purchasing or selling energy at the \{RTM\} price. Total revenue is calculated by the sum of these two transactions. Comparison of \{NAM\} forecast revenue to perfect day-ahead forecast revenue shows that perfect forecast revenue is always greater. However, yearly \{NAM\} forecast revenue is as much as 98\% of the perfect forecast revenue for some sites. After a bias-correction is applied to \{NAM\} forecasts, \{NAM\} forecast revenue decreases. This demonstrates that based on the observed DAM-RTM price spread, biased forecasts can have a higher forecast value than more accurate forecasts. However, when a deviation penalty is assessed, the most accurate forecasts always yield the highest total revenue. },
  comment  = {A value of forecast paper, surprisingly showing that a bias-corrected DA forecast resulted in lower revenue!  Not bad for the California but it doesn't generalize perfectly to continental Europe because the US has a better market (Locational Marginal Pricing, which half of Europe considers immoral).  But the introduction has references to some canonical earlier papers, some European.},
  doi      = {http://dx.doi.org/10.1016/j.apenergy.2014.03.061},
  file     = {Luoma14valFrcstCA.pdf:Luoma14valFrcstCA.pdf:PDF},
  keywords = {Solar energy, Forecast value, Energy market, Energy prices },
  url      = {http://www.sciencedirect.com/science/article/pii/S0306261914003018},
}

@Article{Hashemi16multivarCopulaBaysNet,
  author   = {Seyed Javad Hashemi and Faisal Khan and Salim Ahmed},
  title    = {Multivariate probabilistic safety analysis of process facilities using the Copula Bayesian Network model},
  journal  = {Computers \& Chemical Engineering},
  year     = {2016},
  volume   = {93},
  pages    = {128 - 142},
  issn     = {0098-1354},
  abstract = {Abstract Integrated safety analysis of hazardous process facilities calls for an understanding of both stochastic and topological dependencies, going beyond traditional Bayesian Network (BN) analysis to study cause-effect relationships among major risk factors. This paper presents a novel model based on the Copula Bayesian Network (CBN) for multivariate safety analysis of process systems. The innovation of the proposed \{CBN\} model is in integrating the advantage of copula functions in modelling complex dependence structures with the cause-effect relationship reasoning of process variables using BNs. This offers a great flexibility in probabilistic analysis of individual risk factors while considering their uncertainty and stochastic dependence. Methods based on maximum likelihood evaluation and information theory are presented to learn the structure of \{CBN\} models. The superior performance of the \{CBN\} model and its advantages compared to traditional \{BN\} models are demonstrated by application to an offshore managed pressure drilling case study.},
  comment  = {High dim copula + bayesian network does causal analysis.},
  doi      = {http://dx.doi.org/10.1016/j.compchemeng.2016.06.011},
  file     = {Hashemi16multivarCopulaBaysNet.pdf:Hashemi16multivarCopulaBaysNet.pdf:PDF},
  keywords = {Correlation, Dependence structure, Multivariate probabilistic model, Akaike's information criterion},
  url      = {//www.sciencedirect.com/science/article/pii/S0098135416302010},
}

@Article{Gallo16multiAgentElecMkt,
  author   = {Giulia Gallo},
  title    = {Electricity market games: How agent-based modeling can help under high penetrations of variable generation},
  journal  = {The Electricity Journal},
  year     = {2016},
  volume   = {29},
  number   = {2},
  pages    = {39--46},
  issn     = {1040-6190},
  abstract = {Abstract Integrating increasingly high levels of variable generation in U.S. electricity markets requires addressing not only power system and grid modeling challenges but also an understanding of how market participants react and adapt to them. Key elements of current and future wholesale power markets can be modeled using an agent-based approach, which may prove to be a useful paradigm for researchers studying and planning for power systems of the future.},
  comment  = {From Rafael},
  doi      = {10.1016/j.tej.2016.02.001},
  file     = {Gallo16multiAgentElecMkt.pdf:Gallo16multiAgentElecMkt.pdf:PDF},
  keywords = {Electricity market design, Agent-based modeling, Variable generation, Uplift payments, Missing money problem, Revenue sufficiency },
}

@Article{Hong16probLoadFrcstTutorial,
  author   = {Tao Hong and Shu Fan},
  title    = {Probabilistic electric load forecasting: A tutorial review},
  journal  = {International Journal of Forecasting},
  year     = {2016},
  volume   = {32},
  number   = {3},
  pages    = {914 - 938},
  issn     = {0169-2070},
  abstract = {Abstract Load forecasting has been a fundamental business problem since the inception of the electric power industry. Over the past 100 plus years, both research efforts and industry practices in this area have focused primarily on point load forecasting. In the most recent decade, though, the increased market competition, aging infrastructure and renewable integration requirements mean that probabilistic load forecasting has become more and more important to energy systems planning and operations. This paper offers a tutorial review of probabilistic electric load forecasting, including notable techniques, methodologies and evaluation methods, and common misunderstandings. We also underline the need to invest in additional research, such as reproducible case studies, probabilistic load forecast evaluation and valuation, and a consideration of emerging technologies and energy policies in the probabilistic load forecasting process.},
  comment  = {Tutorial on prob. load forecasting.  If it's good, also read Wang16loadFrcstRcncyBigDat for an up to date, unified treatment by same set of authors.},
  doi      = {http://dx.doi.org/10.1016/j.ijforecast.2015.11.011},
  file     = {Hong16probLoadFrcstTutorial.pdf:Hong16probLoadFrcstTutorial.pdf:PDF},
  keywords = {Short term load forecasting, Long term load forecasting, Probabilistic load forecasting, Regression analysis, Artificial neural networks, Forecast evaluation },
  url      = {http://www.sciencedirect.com/science/article/pii/S0169207015001508},
}

@Article{Tascikaraoglu16loadFrcstSpatTempCmprs,
  author   = {Akin Tascikaraoglu and Borhan M. Sanandaji},
  title    = {Short-term residential electric load forecasting: A compressive spatio-temporal approach},
  journal  = {Energy and Buildings},
  year     = {2016},
  volume   = {111},
  pages    = {380 - 392},
  issn     = {0378-7788},
  abstract = {Abstract Load forecasting is an essential step in power systems operations with important technical and economical impacts. Forecasting can be done both at aggregated and stand-alone levels. While forecasting at the aggregated level is a relatively easier task due to smoother load profiles, residential forecasting (stand-alone level) is a more challenging task due to existing diurnal, weekly, and annual cycles effects in the corresponding time series data and fluctuations caused by the random usage of appliances by end-users. Exploring the available historical load data, it has been discovered that there usually exists an interesting trend between the data from a target house and the data from its surrounding houses. This trend can be exploited for improving the forecast accuracy. One can define several different features for each house, including house size, occupancy level, and usage behavior of appliances. While the number of such features can be large, the main challenge is how to determine the best candidates (features) for an input set without increasing the forecasting computational costs. With this objective in mind, we present a forecasting approach which combines ideas from Compressive Sensing (CS) and data decomposition. The idea is to provide a framework which facilitates exploiting the existing low-dimensional structures governing the interactions among residential houses. The effectiveness of the proposed algorithm is evaluated using real data collected from residential houses in TX, USA. The comparisons against benchmark methods show that the proposed approach significantly improves the short-term forecasts. },
  doi      = {http://dx.doi.org/10.1016/j.enbuild.2015.11.068},
  file     = {Tascikaraoglu16loadFrcstSpatTempCmprs.pdf:Tascikaraoglu16loadFrcstSpatTempCmprs.pdf:PDF},
  keywords = {Electric load forecasting, Compressive Sensing, Spatial correlation, Residential buildings, Data decomposition },
  url      = {http://www.sciencedirect.com/science/article/pii/S037877881530431X},
}

@InBook{Lorenz12predSolIrradPV,
  chapter   = {1.13 - Prediction of Solar Irradiance and Photovoltaic Power},
  pages     = {239 - 292},
  title     = {Comprehensive Renewable Energy},
  publisher = {Elsevier},
  year      = {2012},
  author    = {E. Lorenz and D. Heinemann},
  editor    = {Sayigh, Ali},
  address   = {Oxford},
  isbn      = {978-0-08-087873-7},
  abstract  = {Abstract Power generation from solar and wind energy systems is highly variable due to its dependence on meteorological conditions. An efficient use of these fluctuating energy sources requires reliable forecast information for management and operation strategies. We give an overview of different applications and state-of-the-art models for solar irradiance and photovoltaic power prediction, including time series models based on on-site measured data, models based on the detection of cloud motion in satellite images, and numerical weather prediction-based models. In the second part of this chapter, we show evaluation results for selected irradiance and power prediction schemes. },
  comment   = {Reference used for Colombia GIZ solar part.  Couldn't get the paper.},
  doi       = {http://doi.org/10.1016/B978-0-08-087872-0.00114-1},
  keywords  = {Accuracy assessment, Confidence intervals, Grid integration of \{PV\} power, Irradiance from satellite data, Irradiance prediction, Motion vector fields, Numerical weather prediction, PV power prediction, PV simulation, Tilted irradiance models, Time series models },
  url       = {http://www.sciencedirect.com/science/article/pii/B9780080878720001141},
}

@Article{Kraus17copulaDvineQR,
  author    = {Daniel Kraus and Claudia Czado},
  title     = {D-vine copula based quantile regression},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {2017},
  volume    = {110},
  pages     = {1 - 18},
  issn      = {0167-9473},
  abstract  = {Abstract Quantile regression, that is the prediction of conditional quantiles, has steadily gained importance in statistical modeling and financial applications. A new semiparametric quantile regression method is introduced. It is based on sequentially fitting a likelihood optimal D-vine copula to given data resulting in highly flexible models with easily extractable conditional quantiles. As a subclass of regular vine copulas, D-vines enable the modeling of multivariate copulas in terms of bivariate building blocks, a so-called pair-copula construction (PCC). The proposed algorithm works fast and accurate even in high dimensions and incorporates an automatic variable selection by maximizing the conditional log-likelihood. Further, typical issues of quantile regression such as quantile crossing or transformations, interactions and collinearity of variables are automatically taken care of. In a simulation study the improved accuracy and reduced computation time of the approach in comparison with established quantile regression methods is highlighted. An extensive financial application to international credit default swap (CDS) data including stress testing and Value-at-Risk (VaR) prediction demonstrates the usefulness of the proposed method. },
  comment   = {Pairwise copula (D-vines) are good for high dim (Highest example is only 5, though) and don't have quantile crossing problems.},
  doi       = {https://doi.org/10.1016/j.csda.2016.12.009},
  file      = {Kraus17copulaDvineQR.pdf:Kraus17copulaDvineQR.pdf:PDF},
  keywords  = {Quantile regression, Conditional distribution, Vine copula, Conditional copula quantile, Stress testing },
  owner     = {sotterson},
  timestamp = {2017.05.23},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167947316303073},
}

@Article{Ding16gaussCopulaMissDatEM,
  author    = {Wei Ding and Peter X.-K. Song},
  title     = {\{EM\} algorithm in Gaussian copula with missing data},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {2016},
  volume    = {101},
  pages     = {1 - 11},
  issn      = {0167-9473},
  abstract  = {Abstract Rank-based correlation is widely used to measure dependence between variables when their marginal distributions are skewed. Estimation of such correlation is challenged by both the presence of missing data and the need for adjusting for confounding factors. In this paper, we consider a unified framework of Gaussian copula regression that enables us to estimate either Pearson correlation or rank-based correlation (e.g. Kendall???s tau or Spearman???s rho), depending on the types of marginal distributions. To adjust for confounding covariates, we utilize marginal regression models with univariate location-scale family distributions. We establish the \{EM\} algorithm for estimation of both correlation and regression parameters with missing values. For implementation, we propose an effective peeling procedure to carry out iterations required by the \{EM\} algorithm. We compare the performance of the \{EM\} algorithm method to the traditional multiple imputation approach through simulation studies. For structured types of correlations, such as exchangeable or first-order auto-regressive (AR-1) correlation, the \{EM\} algorithm outperforms the multiple imputation approach in terms of both estimation bias and efficiency. },
  comment   = {Better that Amelia (Honaker10missValTseries)

See also: Hollenbach14missDatImpCopula},
  doi       = {https://doi.org/10.1016/j.csda.2016.01.008},
  file      = {Ding16gaussCopulaMissDatEM.pdf:Ding16gaussCopulaMissDatEM.pdf:PDF},
  keywords  = {Gaussian copula, EM algorithm, Misaligned missing data, Regression },
  owner     = {sotterson},
  timestamp = {2017.05.23},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167947316000177},
}

@Article{Remillard17cplaCondQuantile,
  author   = {Bruno R{\'e}millard and Bouchra Nasri and Taoufik Bouezmarni},
  title    = {On copula-based conditional quantile estimators},
  journal  = {Statistics \& Probability Letters},
  year     = {2017},
  volume   = {128},
  pages    = {14 - 20},
  issn     = {0167-7152},
  abstract = {Abstract Recently, two different copula-based approaches have been proposed to estimate the conditional quantile function of a variable Y with respect to a vector of covariates X : the first estimator is related to quantile regression weighted by the conditional copula density, while the second estimator is based on the inverse of the conditional distribution function written in terms of margins and the copula. Using empirical processes, we show that even if the two estimators look quite different, their estimation errors have the same limiting distribution. Also, we propose a bootstrap procedure for the limiting process in order to construct uniform confidence bands around the conditional quantile function. },
  comment  = {Two kinds of conditional copulas: QR weighted by cond copula density; and one based on the inverse distribution function, in termns of margins.  They have similar errors, which is assessed by a bootstrapping method for quantile estimators (could be used for Prime).},
  doi      = {https://doi.org/10.1016/j.spl.2017.04.014},
  file     = {Remillard17cplaCondQuantile.pdf:Remillard17cplaCondQuantile.pdf:PDF},
  keywords = {Conditional quantile function, Copula, Quantile regression, Bootstrap },
  url      = {http://www.sciencedirect.com/science/article/pii/S0167715217301530},
}

@Article{Kaur16benefitsSolPowFrcstImbalMkt,
  author   = {Amanpreet Kaur and Lukas Nonnenmacher and Hugo T.C. Pedro and Carlos F.M. Coimbra},
  title    = {Benefits of solar forecasting for energy imbalance markets},
  journal  = {Renewable Energy},
  year     = {2016},
  volume   = {86},
  pages    = {819 - 830},
  issn     = {0960-1481},
  abstract = {Abstract Short term electricity trading to balance generation and demand provides an economic opportunity to integrate larger shares of variable renewable energy sources in the power grid. Recently, many regulatory market environments are reorganized to allow short term electricity trading. This study seeks to quantify the benefits of solar forecasting for energy imbalance markets (EIM). State-of-the-art solar forecasts, covering forecast horizons ranging from 24??h to 5??min are proposed and compared against the currently used benchmark models, persistence (P) and smart persistence (SP). The implemented reforecast of numerical weather prediction time series achieves a skill of 14.5\% over the smart persistence model. Using the proposed forecasts for a forecast horizon of up to 75??min for a single 1??MW power plant reduces required flexibility reserves by 21\% and 16.14\%, depending on the allowed trading intervals (5 and 15??min). The probability of an imbalance, caused through wrong market bids from \{PV\} solar plants, can be reduced by 19.65\% and 15.12\% (for 5 and 15??min trading intervals). All \{EIM\} stakeholders benefit from accurate forecasting. Previous estimates on the benefits of EIMs, based on persistence model are conservative. It is shown that the design variables regulating the market time lines, the bidding and the binding schedules, drive the benefits of forecasting. },
  doi      = {http://doi.org/10.1016/j.renene.2015.09.011},
  file     = {Kaur16benefitsSolPowFrcstImbalMkt.pdf:Kaur16benefitsSolPowFrcstImbalMkt.pdf:PDF},
  keywords = {Solar forecasting, Real-time market, Energy imbalance market, Reserves },
  url      = {http://www.sciencedirect.com/science/article/pii/S0960148115302901},
}

@Article{Furman16gaussCopulaTailDep,
  author    = {Edward Furman and Alexey Kuznetsov and Jianxi Su and Ri{\v{c}}ardas Zitikis},
  title     = {Tail dependence of the Gaussian copula revisited},
  journal   = {Insurance: Mathematics and Economics},
  year      = {2016},
  volume    = {69},
  pages     = {97 - 103},
  issn      = {0167-6687},
  abstract  = {Abstract Tail dependence refers to clustering of extreme events. In the context of financial risk management, the clustering of high-severity risks has a devastating effect on the well-being of firms and is thus of pivotal importance in risk analysis. When it comes to quantifying the extent of tail dependence, it is generally agreed that measures of tail dependence must be independent of the marginal distributions of the risks but rather solely copula-dependent. Indeed, all classical measures of tail dependence are such, but they investigate the amount of tail dependence along the main diagonal of copulas, which has often little in common with the concentration of extremes in the copulas??? domain of definition. In this paper we urge that the classical measures of tail dependence may underestimate the level of tail dependence in copulas. For the Gaussian copula, however, we prove that the classical measures are maximal. The implication of the result is two-fold: On the one hand, it means that in the Gaussian case, the (weak) measures of tail dependence that have been reported and used are of utmost prudence, which must be a reassuring news for practitioners. On the other hand, it further encourages substitution of the Gaussian copula with other copulas that are more tail dependent.},
  comment   = {Tail dependence measures actually upperbound the amount that can be expressed by a Gaussian Copula, and underestimate tail dependence of others.  So, moral of the story, get off the Gaussian Copula.},
  doi       = {https://doi.org/10.1016/j.insmatheco.2016.04.009},
  file      = {Furman16gaussCopulaTailDep.pdf:Furman16gaussCopulaTailDep.pdf:PDF},
  keywords  = {Diagonal, Gaussian copula, Maximal tail dependence, Tail independence, Index of tail dependence },
  owner     = {sotterson},
  timestamp = {2017.05.23},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167668715301104},
}

@Article{Wang16loadFrcstRcncyBigDat,
  author   = {Pu Wang and Bidong Liu and Tao Hong},
  title    = {Electric load forecasting with recency effect: A big data approach},
  journal  = {International Journal of Forecasting},
  year     = {2016},
  volume   = {32},
  number   = {3},
  pages    = {585 - 597},
  issn     = {0169-2070},
  abstract = {Abstract Temperature plays a key role in driving the electricity demand. We adopt the recency effec, a term drawn from psychology, to represent the fact that the electricity demand is affected by the temperatures of the preceding hours. In the load forecasting literature, the temperature variables are often constructed in the form of lagged hourly temperatures and moving average temperatures. In the past, computing power has limited the amount of temperature variables that can be used in a load forecasting model. In this paper, we present a comprehensive study to model the recency effect using a big data approach. We take advantage of modern computing power to answer a fundamental question: how many lagged hourly temperatures and/or moving average temperatures are needed in a regression model in order to capture the recency effect fully without compromising the forecasting accuracy? Using a case study based on data from the load forecasting track of the Global Energy Forecasting Competition 2012, we first demonstrate that a model with the recency effect outperforms its counterpart (a.k.a. Tao's Vanilla Benchmark Model) by 18\% to 21\% for forecasting the load series at the top (aggregated) level. We then model the recency effect in order to customize load forecasting models at the bottom level of a geographic hierarchy, again showing a superiority over the benchmark model of 12\% to 15\% on average. Finally, we discuss four different implementations of the recency effect modeling by hour of a day. In addition, this paper also presents two interesting findings: 1) the naive models are not useful for benchmark purposes in load forecasting at aggregated level due to their lack of accuracy; and 2) slicing the data into 24 pieces to develop one model for each hour is not necessarily better than building one interaction regression model using all 24 hours together.},
  comment  = {Forecasts with lagged variables.  Find that one model/horizion is not necessarily beter than a single interaction regression (in contrast, apparently to e.g. Crawford16loadFrcstERCOT)

 If it's good, also read  for an up to date, unified treatment Hong16probLoadFrcstTutorial by same set of authors.},
  doi      = {http://dx.doi.org/10.1016/j.ijforecast.2015.09.006},
  file     = {Wang16loadFrcstRcncyBigDat.pdf:Wang16loadFrcstRcncyBigDat.pdf:PDF},
  keywords = {Interaction regression, Group analysis, Lagged temperature, Linear models, Moving average temperature, Multiple linear regression, Naive models, Benchmark },
  url      = {http://www.sciencedirect.com/science/article/pii/S0169207015001557},
}

@Article{Santos15MultiAgentElecMkt,
  author   = {Gabriel Santos and Tiago Pinto and Hugo Morais and Tiago M. Sousa and Ivo F. Pereira and Ricardo Fernandes and Isabel Pra??a and Zita Vale},
  title    = {Multi-agent simulation of competitive electricity markets: Autonomous systems cooperation for European market modeling},
  journal  = {Energy Conversion and Management},
  year     = {2015},
  volume   = {99},
  pages    = {387--399},
  issn     = {0196-8904},
  abstract = {Abstract The electricity market restructuring, and its worldwide evolution into regional and even continental scales, along with the increasing necessity for an adequate integration of renewable energy sources, is resulting in a rising complexity in power systems operation. Several power system simulators have been developed in recent years with the purpose of helping operators, regulators, and involved players to understand and deal with this complex and constantly changing environment. The main contribution of this paper is given by the integration of several electricity market and power system models, respecting to the reality of different countries. This integration is done through the development of an upper ontology which integrates the essential concepts necessary to interpret all the available information. The continuous development of Multi-Agent System for Competitive Electricity Markets platform provides the means for the exemplification of the usefulness of this ontology. A case study using the proposed multi-agent platform is presented, considering a scenario based on real data that simulates the European Electricity Market environment, and comparing its performance using different market mechanisms. The main goal is to demonstrate the advantages that the integration of various market models and simulation platforms have for the study of the electricity markets??? evolution.},
  comment  = {From Rafael... maybe Henry M could use.},
  doi      = {10.1016/j.enconman.2015.04.042},
  file     = {paper:Santos15MultiAgentElecMkt.pdf:PDF},
  keywords = {Decision support in power systems, Electricity market simulation, Multi-agent systems, Ontologies },
}

@Article{Hong16probEnrgyFrcstCompet14,
  author   = {Tao Hong and Pierre Pinson and Shu Fan and Hamidreza Zareipour and Alberto Troccoli and Rob J. Hyndman},
  title    = {Probabilistic energy forecasting: Global Energy Forecasting Competition 2014 and beyond},
  journal  = {International Journal of Forecasting},
  year     = {2016},
  volume   = {32},
  number   = {3},
  pages    = {896 - 913},
  issn     = {0169-2070},
  abstract = {Abstract The energy industry has been going through a significant modernization process over the last decade. Its infrastructure is being upgraded rapidly. The supply, demand and prices are becoming more volatile and less predictable than ever before. Even its business model is being challenged fundamentally. In this competitive and dynamic environment, many decision-making processes rely on probabilistic forecasts to quantify the uncertain future. Although most of the papers in the energy forecasting literature focus on point or single-valued forecasts, the research interest in probabilistic energy forecasting research has taken off rapidly in recent years. In this paper, we summarize the recent research progress on probabilistic energy forecasting. A major portion of the paper is devoted to introducing the Global Energy Forecasting Competition 2014 (GEFCom2014), a probabilistic energy forecasting competition with four tracks on load, price, wind and solar forecasting, which attracted 581 participants from 61 countries. We conclude the paper with 12 predictions for the next decade of energy forecasting. },
  doi      = {http://doi.org/10.1016/j.ijforecast.2016.02.001},
  file     = {:Hong16probEnrgyFrcstCompet14.pdf:PDF},
  keywords = {Electric load forecasting, Electricity price forecasting, Wind power forecasting, Solar power forecasting, Probabilistic forecasting, Forecasting competition },
  url      = {http://www.sciencedirect.com/science/article/pii/S0169207016000133},
}

@Article{Alessandrini15anlogEnsPrbSolPowFrcst,
  author    = {Alessandrini, S. and Delle Monache, L. and Sperati, S. and Cervone, G.},
  title     = {An analog ensemble for short-term probabilistic solar power forecast},
  year      = {2015},
  volume    = {157},
  pages     = {95--110},
  month     = nov,
  issn      = {0306-2619},
  abstract  = {Abstract The energy produced by photovoltaic farms has a variable nature depending on astronomical and meteorological factors. The former are the solar elevation and the solar azimuth, which are easily predictable without any uncertainty. The amount of liquid water met by the solar radiation within the troposphere is the main meteorological factor influencing the solar power production, as a fraction of short wave solar radiation is reflected by the water particles and cannot reach the earth surface. The total cloud cover is a meteorological variable often used to indicate the presence of liquid water in the troposphere and has a limited predictability, which is also reflected on the global horizontal irradiance and, as a consequence, on solar photovoltaic power prediction. This lack of predictability makes the solar energy integration into the grid challenging. A cost-effective utilization of solar energy over a grid strongly depends on the accuracy and reliability of the power forecasts available to the Transmission System Operators (TSOs). Furthermore, several countries have in place legislation requiring solar power producers to pay penalties proportional to the errors of day-ahead energy forecasts, which makes the accuracy of such predictions a determining factor for producers to reduce their economic losses. Probabilistic predictions can provide accurate deterministic forecasts along with a quantification of their uncertainty, as well as a reliable estimate of the probability to overcome a certain production threshold. In this paper we propose the application of an analog ensemble (AnEn) method to generate probabilistic solar power forecasts (SPF). The AnEn is based on an historical set of deterministic numerical weather prediction (NWP) model forecasts and observations of the solar power. For each forecast lead time and location, the ensemble prediction of solar power is constituted by a set of past production data. These measurements are those concurrent to past deterministic NWP forecasts for the same lead time and location, chosen based on their similarity to the current forecast and, in the current application, are represented by the one-hour average produced solar power.},
  comment   = {I think I used this for GIZ colombia class (it was actually Alessandrini15AppAnlgEnsSolPowFrcst) but maybe this has the details.},
  file      = {Alessandrini15anlogEnsPrbSolPowFrcst.pdf:Alessandrini15anlogEnsPrbSolPowFrcst.pdf:PDF},
  keywords  = {Analog ensemble, Short-term solar power forecasting, Probabilistic predictions, Uncertainty quantification, Ensemble verification},
  owner     = {sotterson},
  timestamp = {2017.04.27},
  url       = {http://www.sciencedirect.com/science/article/pii/S0306261915009368},
}

@Article{GonzalezAparicio15windPowFrcstUncertMktSpain,
  author   = {I. Gonz{\'a}lez-Aparicio and A. Zucker},
  title    = {Impact of wind power uncertainty forecasting on the market integration of wind energy in {Spain}},
  journal  = {Applied Energy},
  year     = {2015},
  volume   = {159},
  pages    = {334--349},
  issn     = {0306-2619},
  abstract = {Abstract The growing share of electricity production from variable renewable energy sources increases the stochastic nature of the power system. This has repercussions on the markets for electricity. Deviations from forecasted production schedules require balancing of a generator???s position within a day. Short term products that are traded on power and/or reserve markets have been developed for this purpose, providing opportunities to actors who can offer flexibility in the short term. The value of flexibility is typically modelled using stochastic scenario extensions of dispatch models which requires, as a first step, understanding the nature of forecast uncertainties. This study provides a new approach for determining the forecast errors of wind power generation in the time period between the closure of the day ahead and the opening of the first intraday session using Spain as an example. The methodology has been developed using time series analysis for the years 2010-2013 to find the explanatory variables of the wind error variability by applying clustering techniques to reduce the range of uncertainty, and regressive techniques to forecast the probability density functions of the intra-day price. This methodology has been tested considering different system actions showing its suitability for developing intra-day bidding strategies and also for the generation of electricity generated from Renewable Energy Sources scenarios. This methodology could help a wind power producer to optimally bid into the intraday market based on more accurate scenarios, increasing their revenues and the system value of wind.},
  comment  = {How to do mixed intraday/interday bidding.  Also, value of forecasts.},
  doi      = {10.1016/j.apenergy.2015.08.104},
  file     = {GonzalezAparicio15windPowFrcstUncertMktSpain.pdf:GonzalezAparicio15windPowFrcstUncertMktSpain.pdf:PDF},
  keywords = {Wind power uncertainty, Multivariate Gaussian mixture model, Wind integration, Market design },
  url      = {http://www.sciencedirect.com/science/article/pii/S0306261915010405},
}

@Article{Maghami16pwrLossSolSoil,
  author    = {Mohammad Reza Maghami and Hashim Hizam and Chandima Gomes and Mohd Amran Radzi and Mohammad Ismael Rezadad and Shahrooz Hajighorbani},
  title     = {Power loss due to soiling on solar panel: A review},
  journal   = {Renewable and Sustainable Energy Reviews},
  year      = {2016},
  volume    = {59},
  pages     = {1307 - 1316},
  issn      = {1364-0321},
  abstract  = {Abstract The power output delivered from a photovoltaic module highly depends on the amount of irradiance, which reaches the solar cells. Many factors determine the ideal output or optimum yield in a photovoltaic module. However, the environment is one of the contributing parameters which directly affect the photovoltaic performance. The authors review and evaluate key contributions to the understanding, performance effects, and mitigation of power loss due to soiling on a solar panel. Electrical characteristics of \{PV\} (Voltage and current) are discussed with respect to shading due to soiling. Shading due to soiling is divided in two categories, namely, soft shading such as air pollution, and hard shading which occurs when a solid such as accumulated dust blocks the sunlight. The result shows that soft shading affects the current provided by the \{PV\} module, but the voltage remains the same. In hard shading, the performance of the \{PV\} module depends on whether some cells are shaded or all cells of the \{PV\} module are shaded. If some cells are shaded, then as long as the unshaded cells receive solar irradiance, there will be some output although there will be a decrease in the voltage output of the \{PV\} module. This study also present a few cleaning method to prevent from dust accumulation on the surface of solar arrays. },
  doi       = {https://doi.org/10.1016/j.rser.2016.01.044},
  file      = {Maghami16pwrLossSolSoil.pdf:Maghami16pwrLossSolSoil.pdf:PDF},
  keywords  = {Photovoltaic, Losses, Environment, Power output, Performance loss, Shadow },
  owner     = {sotterson},
  timestamp = {2017.04.29},
  url       = {http://www.sciencedirect.com/science/article/pii/S1364032116000745},
}

@Article{Schnabel13quantSheets,
  author    = {Schnabel, Sabine K and Eilers, Paul HC},
  title     = {Simultaneous estimation of quantile curves using quantile sheets},
  journal   = {AStA Advances in Statistical Analysis},
  year      = {2013},
  volume    = {97},
  number    = {1},
  pages     = {77--87},
  abstract  = {Abstract The results of quantile smoothing often showcrossing curves, in particular,
for small data sets. We define a surface, called a quantile sheet, on the domain of
the independent variable and the probability. Any desired quantile curve is obtained
by evaluating the sheet for a fixed probability. This sheet is modeled by P-splines
in form of tensor products of B-splines with difference penalties on the array of
coefficients. The amount of smoothing is optimized by cross-validation.Anapplication
for reference growth curves for children is presented.
Keywords P-splines ? Quantiles ? Smoothing ? Tensor product},
  comment   = {Estimate quantiles on 2D sheet: one direction is tau, the other is the measurement. Avoids crossover. Uses a p-spline instead of a b-spline.

Psplines: some kinda polynomial. Supposed to give same result as b-spline but estimation is much more unstable:
http://support.sas.com/documentation/cdl/en/statug/63347/HTML/default/viewer.htm\#statug_transreg_sect041.htm},
  doi       = {10.1007/s10182-012-0198-1},
  file      = {Schnabel13quantSheets.pdf:Schnabel13quantSheets.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_1},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2014.06.24},
}

@Article{Zilko16mixedDscrtContinCopula,
  author    = {Aurelius A. Zilko and Dorota Kurowicka},
  title     = {Copula in a multivariate mixed discrete -- continuous model},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {2016},
  volume    = {103},
  pages     = {28 - 55},
  issn      = {0167-9473},
  abstract  = {Abstract The use of different copula-based models to represent the joint distribution of an eight-dimensional mixed discrete and continuous problem consisting of five discrete and three continuous variables is investigated. The discussion starts with the theoretical properties of the copula-based models. Four different models are constructed for the data collected for the purpose of predicting the length of disruption caused by problems with the train detection system in the Dutch railway network and their performance is tested. The more complex models turn out to represent the data better. Nevertheless, it is shown that the simpler eight dimensional Normal copula still constitutes a statistically sound model for the data.},
  comment   = {A normal copula is a good model for 8-d mixed discrete and continuous inputs!  They have a fancier copula that also works.},
  doi       = {https://doi.org/10.1016/j.csda.2016.02.017},
  file      = {Zilko16mixedDscrtContinCopula.pdf:Zilko16mixedDscrtContinCopula.pdf:PDF},
  keywords  = {Copula, Vine, Mixed models },
  owner     = {sotterson},
  timestamp = {2017.05.23},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167947316300895},
}

@Article{BrancucciMartinezAnido16valDAsolPowFrcstImp,
  author   = {Carlo Brancucci Martinez-Anido and Benjamin Botor and Anthony R. Florita and Caroline Draxl and Siyuan Lu and Hendrik F. Hamann and Bri-Mathias Hodge},
  title    = {The value of day-ahead solar power forecasting improvement},
  journal  = {Solar Energy},
  year     = {2016},
  volume   = {129},
  pages    = {192 - 203},
  issn     = {0038-092X},
  abstract = {Abstract The value of day-ahead solar power forecasting improvements was analyzed by simulating the operation of the Independent System Operator ??? New England (ISO-NE) power system under a range of scenarios with varying solar power penetrations and solar power forecasting improvements. The results showed how the integration of solar power decreased operational electricity generation costs, by decreasing fuel and variable operation and maintenance costs, while decreasing start and shutdown costs of fossil fueled conventional generators. Solar power forecasting improvements changed the impacts that the uncertainty of solar power has on bulk power system operations; electricity generation from the fast start and lower efficiency power plants, ramping of all generators, start and shutdown costs, and solar power curtailment were all reduced. These impacts led to a reduction in overall operational electricity generation costs in the system that translates into an annual economic value for improving solar power forecasting. },
  doi      = {http://doi.org/10.1016/j.solener.2016.01.049},
  file     = {BrancucciMartinezAnido16valDAsolPowFrcstImp.pdf:BrancucciMartinezAnido16valDAsolPowFrcstImp.pdf:PDF},
  keywords = {Solar power forecasting, Solar grid integration, Bulk power system operations },
  url      = {http://www.sciencedirect.com/science/article/pii/S0038092X16000736},
}

@Article{Dionne15LiquidAdjIntraday,
  author   = {Georges Dionne and Maria Pacurar and Xiaozhou Zhou},
  title    = {Liquidity-adjusted Intraday Value at Risk modeling and risk management: An application to data from Deutsche B{\"o}rse},
  journal  = {Journal of Banking \& Finance},
  year     = {2015},
  volume   = {59},
  pages    = {202--219},
  issn     = {0378-4266},
  abstract = {Abstract This paper develops a high-frequency risk measure: the Liquidity-adjusted Intraday Value at Risk (LIVaR). Our objective is to explicitly consider the endogenous liquidity dimension associated with order size. By reconstructing the open Limit Order Book of Deutsche B??rse, changes in the tick-by-tick (ex-ante) frictionless return and actual return are modeled jointly. The risk related to the ex-ante liquidity premium is then quantified. Our model can be used to identify the impact of ex-ante liquidity risk on total risk, and to provide an estimation of the VaR for the actual return at a point in time. In our sample, liquidity risk can account for up to 32% of total risk depending on order size.},
  comment  = {Maybe how to model liquidity risk in German Intraday Electricity market trading?},
  doi      = {10.1016/j.jbankfin.2015.06.005},
  file     = {Dionne15LiquidAdjIntraday.pdf:Dionne15LiquidAdjIntraday.pdf:PDF},
  keywords = {Liquidity-adjusted Intraday Value at Risk},
}

@Article{Pircalabu17priceVolRiskWndTrdCpla,
  author    = {A. Pircalabu and T. Hvolby and J. Jung and E. H??g},
  title     = {Joint price and volumetric risk in wind power trading: A copula approach},
  journal   = {Energy Economics},
  year      = {2017},
  volume    = {62},
  pages     = {139 - 154},
  issn      = {0140-9883},
  abstract  = {Abstract This paper examines the dependence between wind power production and electricity prices and discusses its implications for the pricing and the risk distributions associated with contracts that are exposed to joint price and volumetric risk. We propose a copula model for the joint behavior of prices and wind power production, which is estimated to data from the Danish power market. We find that the marginal behavior of the individual variables is best described by ARMA???GARCH models with non-Gaussian error distributions, and the preferred copula model is a time-varying Gaussian copula. As an application of our joint model, we consider the case of an energy trading company entering into longer-term agreements with wind power producers, where the fluctuating future wind power production is bought at a predetermined fixed price. We find that assuming independence between prices and wind power production leads to an underestimation of risk, as the profit distribution becomes left-skewed when the negative dependence that we find in the data is accounted for. By performing a simple static hedge in the forward market, we show that the risk can be significantly reduced. Furthermore, an out-of-sample study shows that the choice of copula influences the price of correlation risk, and that time-varying copulas are superior to the constant ones when comparing actual profits generated with different models.},
  comment   = {* A copula model is proposed for electricity spot prices and wind power production.
* Evidence of time-varying dependence between prices and production is found.
* The pricing of contracts exposed to joint price and volumetric risk is investigated.
* We show that assuming independence leads to an underestimation of risk.
* We find that the choice of copula model influences the price of correlation risk.
* 2D conditional copula with Markov dependendence upon previous time step cdf.
* Marginal models: seems to be a fixed cdf of the error of a time-variying ARMA-GARCH deterministic prediction},
  doi       = {https://doi.org/10.1016/j.eneco.2016.11.023},
  file      = {Pircalabu17priceVolRiskWndTrdCpla.pdf:Pircalabu17priceVolRiskWndTrdCpla.pdf:PDF},
  keywords  = {Volumetric risk, Spot electricity price, Wind power production, Time-varying copula model, Risk management, Correlation risk },
  owner     = {sotterson},
  timestamp = {2017.05.23},
  url       = {http://www.sciencedirect.com/science/article/pii/S0140988316303450},
}

@Article{Munkhammar17solarIrradSpatCpla,
  author    = {Joakim Munkhammar and Joakim Wid??n and Laura M. Hinkelman},
  title     = {A copula method for simulating correlated instantaneous solar irradiance in spatial networks},
  journal   = {Solar Energy},
  year      = {2017},
  volume    = {143},
  pages     = {10 - 21},
  issn      = {0038-092X},
  abstract  = {Abstract This paper presents a method for generating correlated instantaneous solar irradiance data for an arbitrary set of spatially dispersed locations. Based on the empirical clear-sky index distribution for one location and the cross-correlation between clear-sky index data at all location pairs, a copula is used to represent the dependence between locations. The method is primarily intended for probabilistic simulations of electricity distribution grids with high penetrations of photovoltaic (PV) systems, in which solar irradiance data for nodes in the grid can be sampled from the model. The method is validated against a 10-s resolution solar irradiance data set for 14 locations, dispersed within an array of approximately 1 km ?? 1.2 km, at the Island of Oahu, Hawai???i, USA. The results are compared with previous results for along- and cross-wind pairs of locations, and with models for adjacent (completely correlated) and dispersed (completely uncorrelated) locations. It is shown that the copula approach performs better than the adjacent model for a majority of all location pairs and for all but one pair of locations separated more than 500 m. It outperforms the dispersed model for all pairs of locations. In conclusion, the proposed method can generate correlated data and estimate the aggregate clear-sky index for any set of locations based only on the distribution of the clear-sky index for a single location. },
  comment   = {See also: Wu15pvGenPairCpla},
  doi       = {https://doi.org/10.1016/j.solener.2016.12.022},
  file      = {Munkhammar17solarIrradSpatCpla.pdf:Munkhammar17solarIrradSpatCpla.pdf:PDF},
  keywords  = {Instantaneous solar irradiance, Clear-sky index, Aggregate clear-sky index, Copula },
  owner     = {sotterson},
  timestamp = {2017.05.23},
  url       = {http://www.sciencedirect.com/science/article/pii/S0038092X16306168},
}

@Article{Donnat16genericRVrep,
  author    = {Philippe Donnat and Gautier Marti and Philippe Very},
  title     = {Toward a generic representation of random variables for machine learning},
  journal   = {Pattern Recognition Letters},
  year      = {2016},
  volume    = {70},
  pages     = {24 - 31},
  issn      = {0167-8655},
  abstract  = {Abstract This paper presents a pre-processing and a distance which improve the performance of machine learning algorithms working on independent and identically distributed stochastic processes. We introduce a novel non-parametric approach to represent random variables which splits apart dependency and distribution without losing any information. We also propound an associated metric leveraging this representation and its statistical estimate. Besides experiments on synthetic datasets, the benefits of our contribution is illustrated through the example of clustering financial time series, for instance prices from the credit default swaps market. Results are available on the website http://www.datagrapple.com and an \{IPython\} Notebook tutorial is available at http://www.datagrapple.com/Tech for reproducible research.},
  comment   = {Time series clustering based on copula distances (roughly).  I think it assumes temporal independence?},
  doi       = {https://doi.org/10.1016/j.patrec.2015.11.004},
  file      = {Donnat16genericRVrep.pdf:Donnat16genericRVrep.pdf:PDF},
  keywords  = {Financial time series, i.i.d. random processes, Normalizations and preprocessings, Clustering random variables },
  owner     = {sotterson},
  timestamp = {2017.05.23},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167865515003906},
}

@Article{Che17neuralNetTensorRnk1,
  author    = {Maolin Che and Andrzej Cichocki and Yimin Wei},
  title     = {Neural networks for computing best rank-one approximations of tensors and its applications},
  journal   = {Neurocomputing},
  year      = {2017},
  pages     = {-},
  issn      = {0925-2312},
  abstract  = {Abstract This paper presents the neural dynamical network to compute a best rank-one approximation of a real-valued tensor. We implement the neural network model by the ordinary differential equations (ODE), which is a class of continuous-time recurrent neural network. Several new properties of solutions for the neural network are established. We prove that the locally asymptotic stability of solutions for \{ODE\} by constructive an appropriate Lyapunov function under mild conditions. Furthermore, we also discuss how to use the proposed neural networks for solving the tensor eigenvalue problem including the tensor H-eigenvalue problem, the tensor Z-eigenvalue problem, and the generalized eigenvalue problem with symmetric-definite tensor pairs. Finally, we generalize the proposed neural networks to the computation of the restricted singular values and the associated restricted singular vectors of real-valued tensors. We illustrate and validate theoretical results via numerical simulations. },
  comment   = {Use this for high dim covariance matrix learning by tensors?  Could the fact that it's NN based be a bridge to a deep learning approach?

Tensors for covariance learning:  Li11tensoCovMatObjTrck},
  doi       = {https://doi.org/10.1016/j.neucom.2017.04.058},
  file      = {:Che17neuralNetTensorRnk1.pdf:PDF},
  keywords  = {Neural network, Ordinary differential equations, Lyapunov function, Lyapunov stability theory, Rank-one tensor, Best rank-one approximation, Z-eigenpair, Symmetric-definite tensor pair, H-eigenpair, The local maximal generalized eigenpair, The local minimal generalized eigenpair, Generalized tensor eigenpair, Local optimal rank-one approximation, Restricted singular value, Restricted singular vector},
  owner     = {sotterson},
  timestamp = {2017.06.26},
  url       = {http://www.sciencedirect.com/science/article/pii/S0925231217308263},
}

@Article{Ren15ensWindSolPowFrcstRev,
  author   = {Ye Ren and P.N. Suganthan and N. Srikanth},
  title    = {Ensemble methods for wind and solar power forecasting -- A state-of-the-art review},
  journal  = {Renewable and Sustainable Energy Reviews},
  year     = {2015},
  volume   = {50},
  pages    = {82 - 91},
  issn     = {1364-0321},
  abstract = {Abstract This paper reviews state-of-the-art on wind speed/power forecasting and solar irradiance forecasting with ensemble methods. The ensemble forecasting methods are grouped into two main categories: competitive ensemble forecasting and cooperative ensemble forecasting. The competitive ensemble forecasting is further categorized based on data diversity and parameter diversity. The cooperative ensemble forecasting is divided according to pre-processing and post-processing. Typical articles are discussed according to each category and their characteristics are highlighted. We also conduct comparisons based on reported results and comparisons based on simulations conducted by us. Suggestions for future research include ensemble of different paradigms and inter-category ensemble methods among others. },
  doi      = {http://doi.org/10.1016/j.rser.2015.04.081},
  file     = {Ren15ensWindSolPowFrcstRev.pdf:Ren15ensWindSolPowFrcstRev.pdf:PDF},
  keywords = {Ensemble method, Wind speed forecasting, Wind power forecasting, Solar irradiance forecasting },
  url      = {http://www.sciencedirect.com/science/article/pii/S1364032115003512},
}

@Article{Kenari13cmtteMchn,
  author    = {Seyed Ali Jafari Kenari and Syamsiah Mashohor},
  title     = {Robust committee machine for water saturation prediction},
  journal   = {Journal of Petroleum Science and Engineering},
  year      = {2013},
  volume    = {104},
  number    = {0},
  pages     = {1--10},
  issn      = {0920-4105},
  abstract  = {Abstract Water saturation is one of the important physical properties of the petroleum reservoir which are usually determined by core analysis. An accurate determination of this parameter is significant to execute a realistic evaluation of hydrocarbon reserves in the formation and also decreasing the economic risk. In this study, a robust technique is proposed to determine an accurate value of this parameter from well log data in un-cored well or at un-cored interval of the same well by combining different types of machine learning techniques. The final results (sub-CM outputs) demonstrated that integrating these techniques using proposed method provides an accurate, fast and cost-effective method for estimating the target value.},
  comment   = {An ensemble continous regression technique, perhaps like adaboost, although I haven't checked.},
  doi       = {10.1016/j.petrol.2013.03.009},
  file      = {Kenari13cmtteMchn.pdf:Kenari13cmtteMchn.pdf:PDF},
  keywords  = {committee machine},
  owner     = {sotterson},
  timestamp = {2014.01.21},
}

@Article{Hyndman96estCondDens,
  author    = {Hyndman, Rob J. and Bashtannyk, David M. and Grunwald, Gary K.},
  title     = {Estimating and Visualizing Conditional Densities},
  journal   = {Journal of Computational and Graphical Statistics},
  year      = {1996},
  volume    = {5},
  number    = {4},
  pages     = {315--336},
  abstract  = {Abstract We consider the kernel estimator of conditional density and derive its asymptotic bias, variance, and mean-square error. Optimal bandwidths (with respect to integrated mean-square error) are found and it is shown that the convergence rate of the density estimator is order n ?2/3. We also note that the conditional mean function obtained from the estimator is equivalent to a kernel smoother. Given the undesirable bias properties of kernel smoothers, we seek a modified conditional density estimator that has mean equivalent to some other nonparametric regression smoother with better bias properties. It is also shown that our modified estimator has smaller mean square error than the standard estimator in some commonly occurring situations. Finally, three graphical methods for visualizing conditional density estimators are discussed and applied to a data set consisting of maximum daily temperatures in Melbourne, Australia.

Key Words: Bandwidth; Conditional density; Data visualization; DenSity estimation; Kernel smoothing; Nonparametric regression},
  comment   = {The conditional density estimator used in the hdrcde R package. I also used it for the Eweline reference point forecast to prob. forecast system. I think this was used to (at least) pick kernel bandwidth in [<Mendes11statWindFrcst>]},
  doi       = {10.1080/10618600.1996.10474715},
  eprint    = {http://www.tandfonline.com/doi/pdf/10.1080/10618600.1996.10474715},
  file      = {Hyndman96estCondDens.pdf:Hyndman96estCondDens.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2014.03.25},
}

@Article{Kim16sprsCondCplaMultiOut,
  author    = {Minyoung Kim},
  title     = {Sparse conditional copula models for structured output regression},
  journal   = {Pattern Recognition},
  year      = {2016},
  volume    = {60},
  pages     = {761 - 769},
  issn      = {0031-3203},
  abstract  = {Abstract We deal with the multiple output regression task where the central theme is to capture the sparse output correlation among the output variables. Sparse inverse covariance learning of linear Gaussian conditional models has been recently studied, shown to achieve superb prediction performance. However, it can fail when the underlying true input???output process is non-Gaussian and/or non-linear. We introduce a novel sparse conditional copula model to represent the joint density of the output variables. By incorporating a Gaussian copula function, yet modeling univariate marginal densities by (non-Gaussian) mixtures of experts, we achieve high flexibility in representation that admits non-linear and non-Gaussian densities. We then propose a sparse learning method for this copula-based model that effectively imposes sparsity in the conditional dependency among output variables. The learning optimization is efficient as it can be decomposed into gradient-descent marginal density estimation and the sparse inverse covariance learning for the copula function. Improved performance of the proposed approach is demonstrated on several interesting image/vision tasks with high dimensions.},
  comment   = {Multiple output regression task with output correlations modeled w/ conditional copulas.  Outputs could be NWP grids for upscaling, or forecast aggregation.},
  doi       = {https://doi.org/10.1016/j.patcog.2016.03.027},
  file      = {Kim16sprsCondCplaMultiOut.pdf:Kim16sprsCondCplaMultiOut.pdf:PDF},
  keywords  = {Multiple output regression, Sparse inverse covariance estimation, Copula models, Gaussian random fields },
  owner     = {sotterson},
  timestamp = {2017.05.23},
  url       = {http://www.sciencedirect.com/science/article/pii/S0031320316300127},
}

@Article{SantosAlamillos14windVarBsld,
  author   = {F.J. Santos-Alamillos and D. Pozo-V??zquez and J.A. Ruiz-Arias and V. Lara-Fanego and J. Tovar-Pescador},
  title    = {A methodology for evaluating the spatial variability of wind energy resources: Application to assess the potential contribution of wind energy to baseload power},
  journal  = {Renewable Energy},
  year     = {2014},
  volume   = {69},
  pages    = {147 - 156},
  issn     = {0960-1481},
  abstract = {Abstract We propose a method for analyzing the potential contribution of wind energy resources to stable (baseload) power within a region. The method uses principal component analysis (PCA) to analyze spatiotemporal balancing of wind energy resources and then assesses the optimal wind farm location to reduce wind power fluctuations. The ability of different reference wind turbines, alone or interconnected, to provide stable power is ultimately evaluated at selected locations. The method was tested in the southern Iberian Peninsula, including offshore areas. We used hourly wind energy estimates from the \{WRF\} mesoscale model at 3-km spatial resolution for the period 2008???2010. First, results show a valuable spatial balancing pattern between the wind energy resources in the northeast study region and Strait of Gibraltar area. The pattern was found to result from the interaction of mesoscale zonal flow with the complex topography of the region. Second, the results indicate that by taking advantage of the spatial balancing pattern, the optimal allocation and interconnection of wind farms across the region, can substantially reduce wind power fluctuations. This optimal allocation can in some cases generate stable power, thereby contributing to baseload power. },
  comment  = {Usefor for ReWP reserve power allocation, also aggregation},
  doi      = {http://dx.doi.org/10.1016/j.renene.2014.03.006},
  file     = {SantosAlamillos14windVarBsld.pdf:SantosAlamillos14windVarBsld.pdf:PDF},
  keywords = {Wind energy variability, PCA, Andalusia, Firm capacity, Balancing },
  url      = {//www.sciencedirect.com/science/article/pii/S0960148114001499},
}

@Article{Dunsmuir15glarma,
  author    = {Dunsmuir, William TM and Scott, David J and others},
  title     = {The glarma Package for Observation Driven Time Series Regression of Counts},
  journal   = {Journal of Statistical Software},
  year      = {2015},
  volume    = {67},
  number    = {7},
  pages     = {1--36},
  abstract  = {Abstract
We review the theory and application of generalized linear autoregressive moving av-
erage observation-driven models for time series of counts with explanatory variables and
describe the estimation of these models using the R package glarma. Forecasting, diag-
nostic and graphical methods are also illustrated by several examples.

Keywords: observation-driven count time series, generalized linear ARMA models, glarma, R.},
  comment   = {An autoregressive GLM (generalized linear model).  Possibly useful for Henry Martin's thesis, modeling the autoregressive  "Hart" (sp?) distribution of the offer time-of-arrival (which is at heart, a kth order all-pole model, I think)..},
  file      = {Dunsmuir15glarma.pdf:Dunsmuir15glarma.pdf:PDF},
  owner     = {sotterson},
  publisher = {Foundation for Open Access Statistics},
  timestamp = {2016.12.05},
  url       = {https://www.jstatsoft.org/article/view/v067i07/v67i07.pdf},
}

@Article{Poncela13autoTuneKalman,
  author    = {Poncela, Marta and Poncela, Pilar and Per{\'a}n, Jos{\'e} Ram{\'o}n},
  title     = {Automatic tuning of {Kalman} filters by maximum likelihood methods for wind energy forecasting},
  journal   = {Applied Energy},
  year      = {2013},
  volume    = {108},
  number    = {0},
  pages     = {349--362},
  issn      = {0306-2619},
  abstract  = {Abstract Wind energy has the advantages of being clean, having a zero-cost primary energy source (wind) and having low operating and maintenance costs. Despite these advantages, it is difficult to manage due to its variable condition. Recently, there has been an explosion of forecasting tools to integrate wind energy into the electrical grid. This paper is devoted to improve the performance of statistical tools based on Kalman filter models. We substitute the traditional way of setting the values of the model parameters by estimating them by quasi maximum likelihood methods for a certain forecast horizon. It produces an automatic self-tuning of the model parameters for each particular wind farm. We show that this brings the models close to an optimum for all the horizons. We also propose new multivariate models to capture the effect of missing inputs on the predicted power. We have applied our methodology in several wind farms and the results show that these two approaches always provide more accurate predictions, with up to 60% of improvement for the RMSE. Finally, we propose a real-time estimation strategy.},
  comment   = {Adaptive multivariate Kalman filter. Multistep, though....},
  doi       = {10.1016/j.apenergy.2013.03.041},
  file      = {Poncela13autoTuneKalman.pdf:Poncela13autoTuneKalman.pdf:PDF},
  keywords  = {Wind energy forecasting},
  owner     = {sotterson},
  timestamp = {2014.11.18},
  url       = {http://www.sciencedirect.com/science/article/pii/S0306261913002377},
}

@Article{Kou13sprsOnlineWrpGausProc,
  author    = {Peng Kou and Feng Gao and Xiaohong Guan},
  title     = {Sparse online warped {Gauss}ian process for wind power probabilistic forecasting},
  journal   = {Applied Energy},
  year      = {2013},
  volume    = {108},
  number    = {0},
  pages     = {410--428},
  issn      = {0306-2619},
  abstract  = {Abstract Wind generation has experienced rapid growth around the world in the past decade. This highlights the importance of the short-term wind power forecasting. This paper focuses on the probabilistic short-term wind power forecasting. An online sparse Bayesian model is established. The key features of the proposed model are its non-Gaussian predictive distributions and its time-adaptiveness. This model based on the warped Gaussian process (WGP), which handles the non-Gaussian uncertainties in wind power series by automatically transforming it to a latent series. The transformed series is well-modeled by a Gaussian process (GP), then the non-Gaussian uncertainty associated with the wind power can be predicted in a standard \{GP\} framework. Wind generation is a process whose characteristics change with time, so a wind power forecasting model should exhibit adaptive features. To address this, we introduce an online learning algorithm to WGP, thus permitting \{WGP\} to track the time-varying characteristic of wind generation. Moreover, since the high computational costs of \{WGP\} hinder its practical application on large-scale problems such as wind power forecast, the proposed model also employs a sparsification method to reduce its computational costs, thus enhancing its practical applicability. The simulation on actual data validates the effectiveness of the proposed model. The data used in the simulation are obtained in the real operation of a wind farm in China.},
  comment   = {Adaptive probabilistic forecast algorithm using Gaussian Processes. Sparsity makes it less computationally burdensome.},
  doi       = {10.1016/j.apenergy.2013.03.038},
  file      = {Kou13sprsOnlineWrpGausProc.pdf:Kou13sprsOnlineWrpGausProc.pdf:PDF},
  keywords  = {Wind energy},
  owner     = {sotterson},
  timestamp = {2014.11.18},
  url       = {http://www.sciencedirect.com/science/article/pii/S0306261913002341},
}

@Article{Monteiro16intrdyPrcFrcstIber,
  author    = {Monteiro, Claudio and Ramirez-Rosado, Ignacio and Fernandez-Jimenez, L. and Conde, Pedro},
  title     = {Short-Term Price Forecasting Models Based on Artificial Neural Networks for Intraday Sessions in the Iberian Electricity Market},
  journal   = {Energies},
  year      = {2016},
  volume    = {9},
  number    = {9},
  pages     = {721},
  month     = {Sep},
  issn      = {1996-1073},
  abstract  = {Abstract: This paper presents novel intraday session models for price forecasts (ISMPF models) for hourly price forecasting in the six intraday sessions of the Iberian electricity market (MIBEL) and the analysis of mean absolute percentage errors (MAPEs) obtained with suitable combinations of their input variables in order to find the best ISMPF models. Comparisons of errors from different ISMPF models identified the most important variables for forecasting purposes. Similar analyses were applied to determine the best daily session models for price forecasts (DSMPF models) for the day-ahead price forecasting in the daily session of the MIBEL, considering as input variables extensive hourly time series records of recent prices, power demands and power generations in the previous day, forecasts of demand, wind power generation and weather for the day-ahead, and chronological variables. ISMPF models include the input variables of DSMPF models as well as the daily session prices and prices of preceding intraday sessions. The best ISMPF models achieved lower MAPEs for most of the intraday sessions compared to the error of the best DSMPF model; furthermore, such DSMPF error was very close to the lowest limit error for the daily session. The best ISMPF models can be useful for MIBEL agents of the electricity intraday market and the electric energy industry.
Keywords: short-term forecasting; electricity market prices; Iberian electricity market (MIBEL); daily session prices; intraday session prices},
  comment   = {Useful for German intraday price forecasting?},
  doi       = {10.3390/en9090721},
  file      = {Monteiro16intrdyPrcFrcstIber.pdf:Monteiro16intrdyPrcFrcstIber.pdf:PDF},
  publisher = {MDPI AG},
  url       = {http://dx.doi.org/10.3390/en9090721},
}

@Article{Karegowda12kmeansKNNcasc,
  author    = {Karegowda, Asha Gowda and Jayaram, MA and Manjunath, AS},
  title     = {Cascading K-means clustering and K-nearest neighbor classifier for categorization of diabetic patients},
  journal   = {International Journal of Engineering and Advanced Technology},
  year      = {2012},
  volume    = {1},
  number    = {3},
  pages     = {2249--8958},
  abstract  = {Abstract? Medical Data mining is the process of extracting hidden patterns from medical data. This paper presents the development of a hybrid model for classifying Pima Indian diabetic database (PIDD). The model consists of three stages. In the first stage, K-means clustering is used to identify and eliminate incorrectly classified instances. In the second stage Genetic algorithm (GA) and Correlation based feature selection (CFS) is used in a cascaded fashion for relevant feature extraction, where GA rendered global search of attributes with fitness evaluation effected by CFS. Finally in the third stage a fine tuned classification is done using K-nearest neighbor (KNN) by taking the correctly clustered instance of first stage and with feature subset identified in the second stage as inputs for the KNN. Experimental results signify the cascaded K-means clustering and KNN along with feature subset identified GA_CFS has enhanced classification accuracy of KNN. The proposed model obtained the classification accuracy of 96.68 pct. for diabetic dataset.
Index Terms: Genetic algorithm, Correlation based feature selection ,K-nearest neighbor, K-means clustering , Pima Indian Diabetics.},
  comment   = {Kmeans --> featsel --> KNN: Almost what I'm doing for local linear QR. An update of the idea in Ding04KknnInKmeansClust},
  file      = {Karegowda12kmeansKNNcasc.pdf:Karegowda12kmeansKNNcasc.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.04.04},
  url       = {http://www.ijeat.org/attachments/File/V1Issue3/C0211021312.pdf},
}

@Article{Dahlhaus00graphInterMultiVar,
  author               = {Dahlhaus, Rainer},
  title                = {Graphical interaction models for multivariate time series},
  journal              = {Metrika},
  year                 = {2000},
  volume               = {51},
  number               = {2},
  pages                = {157--172},
  abstract             = {Abstract. In this paper we extend the concept of graphical models for multivariate data to multivariate time series. We define a partial correlation graph for time series and use the partial spectral coherence between two components given the remaining components to identify the edges of the graph. As an example we consider multivariate autoregressive processes. The method is applied to air pollution data.},
  citeulike-article-id = {274871},
  comment              = {Use spectral partial coherence to draw correlation graph

* is partial correlation in the freq domain
 -- all lags are considered, which is nice

* Spectrum C-code is distributed here: http://www.statlab.uni-heidelberg.de/people/eichler/manual.html

* highly cited (100, last time I checked): http://scholar.google.com/scholar?q=Graphical+Interaction+Models+for+Multivariate+Time+Series&oe=UTF-8&rls=org.mozilla:en-US:official&client=firefox-a&um=1&ie=UTF-8&sa=N&tab=ws&ei=Yz-7SfK8LIrIM-nuwZsI&oi=property_suggestions&resnum=0&ct=property-revision&cd=1

* look up refs? See if mutual information is there?
 -- partial spectral coherence is analagous to conditional mutual information of signals w/ vectorized lags},
  doi                  = {10.1007/s001840000055\%20},
  file                 = {Dahlhaus00graphInterMultiVar.pdf:Dahlhaus00graphInterMultiVar.pdf:PDF;Dahlhaus00graphInterMultiVar.pdf:Dahlhaus00graphInterMultiVar.pdf:PDF},
  keywords             = {mts},
  owner                = {sotterson},
  posted-at            = {2005-08-05 14:05:49},
  timestamp            = {2009.03.13},
}

@InCollection{Toke11mktMkingOBsprd,
  author    = {Toke, Ioane Muni},
  title     = {"Market making" in an order book model and its impact on the spread},
  booktitle = {Econophysics of Order-driven Markets},
  publisher = {Springer},
  year      = {2011},
  pages     = {49--64},
  abstract  = {Abstract. It has been suggested that marked point processes might be good can-
didates for the modelling of financial high?frequency data. A special class of point
processes, Hawkes processes, has been the subject of various investigations in the
financial community. In this paper, we propose to enhance a basic zero-intelligence
order book simulator with arrival times of limit and market orders following mutu?
ally (asymmetrically) exciting Hawkes processes. Modelling is based on empirical
observations on time intervals between orders that we verify on several markets (eq-
uity, bond futures, index futures). We show that this simple feature enables a much
more realistic treatment of the bid?ask spread of the simulated order book.},
  comment   = {See also updated version: Toke16modelIntsLimOrdrBk
Uses a Hawkes process to model market order book.  Here's what Henry Martin said about it:

Concerning the Toke paper: I talked about it because it is cited in the review 2013Gould_limitOrderBooks (This is the awesome paper) but I did not read the Toke paper, so I can not say if it is worth the time or not. I just used it to see what they predict using the hawkes process and how they validate it but you should definitely have a look at the Gould paper.

I wonder: would autogregressive quantile regression be better?


Gould Paper: Gould13limitOrderBks


See Smirni16cs426dscrtEvntSimCrsNotes for Poisson arrival stuff},
  file      = {Toke11mktMkingOBsprd.pdf:Toke11mktMkingOBsprd.pdf:PDF},
  url       = {http://link.springer.com/chapter/10.1007%2F978-88-470-1766-5_4},
}

@Article{Koenker96intPtQR,
  author    = {Roger Koenker and Beum J. Park},
  title     = {An interior point algorithm for nonlinear quantile regression},
  journal   = {Journal of Econometrics},
  year      = {1996},
  volume    = {71},
  number    = {1???2},
  pages     = {265--283},
  issn      = {0304-4076},
  abstract  = {Abstract

A new algorithm for computing quantile regression estimates for problems in which the response function is nonlinear in parameters is described. The nonlinear l1 estimation problem is a special (median) case. The algorithm is closely related to recent developments on interior point methods for solving linear programs. Performance of the algorithm on a variety of test problems including the censored linear quantile regression problem of Powell (1986) is reported.

Keywords
 Quantile regression;
 Nonlinear regression;
 Linear programming;
 Interior point algorithms;
 Nonlinear programming},
  comment   = {The classic QR interior point method. Better on nonlinear problems than simplex but not as computationally efficient. MMqr may be even more stable.

This algorithm is used in the matlab code (ipqr.m) here:

http://sites.stat.psu.edu/~dhunter/code/qrmatlab/

The purpose of this site was to compare Koenker's method (the one in this paper) to a new one: the Majorize-Minimize method (Hunter00qrMMalg), called mmqr.m on the site. MMQR may be a bit more robust on nonlinear problems (convergence succeeds) but sometimes a bit slow.},
  doi       = {10.1016/0304-4076(96)84507-6},
  file      = {Koenker96intPtQR.pdf:Koenker96intPtQR.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_1},
  keywords  = {Quantile regression},
  owner     = {sotterson},
  timestamp = {2014.02.28},
  url       = {http://www.sciencedirect.com/science/article/pii/0304407696845076},
}

@Article{Oh15mdlDepHiDimFactrCopula,
  author    = {Dong Hwan Oh and Andrew J. Patton},
  title     = {Modelling Dependence in High Dimensions with Factor Copulas},
  journal   = {Journal of Business \& Economic Statistics},
  year      = {2015},
  volume    = {0},
  number    = {ja},
  pages     = {0-0},
  abstract  = {AbstractThis paper presents flexible new models for the dependence structure, or copula, of economic variables based on a latent factor structure. The proposed models are particularly attractive for relatively high dimensional applications, involving fifty or more variables, and can be combined with semiparametric marginal distributions to obtain flexible multivariate distributions. Factor copulas generally lack a closed-form density, but we obtain analytical results for the implied tail dependence using extreme value theory, and we verify that simulation-based estimation using rank statistics is reliable even in high dimensions. We consider ???scree??? plots to aid the choice of the number of factors in the model. The model is applied to daily returns on all 100 constituents of the S\&P 100 index, and we find significant evidence of tail dependence, heterogeneous dependence, and asymmetric dependence, with dependence being stronger in crashes than in booms. We also show that factor copula models provide superior estimates of some measures of systemic risk.

Keywords: correlation, dependence, copulas, tail risk.},
  comment   = {Factor (vine) copula for high dims.  Also handles tail correlation, I think.

This is a 2015 paper but the copy I have in this bibtex entry is a 2011 version.  See if I can get a newer one.

Maybe read the  tech report instead.  It seems to be pretty similar to this one, and it's 2015:  Oh15mdlDepHiDimFactrCopulaTechRep},
  doi       = {10.1080/07350015.2015.1062384},
  eprint    = {http://dx.doi.org/10.1080/07350015.2015.1062384},
  file      = {Oh15mdlDepHiDimFactrCopula.pdf:Oh15mdlDepHiDimFactrCopula.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2016.12.17},
}

@Article{Cao12crvDerivParaPenSpln,
  author    = {Cao, Jiguo and Cai, Jing and Wang, Liangliang},
  title     = {Estimating curves and derivatives with parametric penalized spline smoothing},
  journal   = {Statistics and Computing},
  year      = {2012},
  volume    = {22},
  number    = {5},
  pages     = {1059--1067},
  abstract  = {Accurate estimation of an underlying function and its derivatives is one of the
central problems in statistics. Parametric forms are often proposed based on
the expert opinion or prior knowledge of the underlying function. However,
these strict parametric assumptions may result in biased estimates when they
are not completely accurate. Meanwhile, nonparametric smoothing methods,
which do not impose any parametric form, are quite flexible. We propose a
parametric penalized spline smoothing method, which has the same flexibility
as the nonparametric smoothing methods. It also uses the prior knowledge
of the underlying function by defining an additional penalty term using the
distance of the fitted function to the assumed parametric function. Our simulation
studies show that the parametric penalized spline smoothing method
can obtain more accurate estimates of the function and its derivatives than
the penalized spline smoothing method. The parametric penalized spline
smoothing method is also demonstrated by estimating the human height
function and its derivatives from the real data.
Keywords: growth curve, nonlinear regression, parameter cascading},
  comment   = {A two-penalty spline fit: one penalty is the usual roughness fit, and the second is the difference from an assumed known function e.g. an approximately known fit. Has Matlab. Could be useful fo power curve learning, when the curve comes from an analytic wind power curve (or PV power curve) or from a measured, generic curve, like Stephan Vogt's (Vogt15HybridPhysMLrgnFrcst). So could be good for NWP-grid-based algorithms.

Matlab:
http://people.stat.sfu.ca/~cao/Research/PPSS.html},
  file      = {Cao12crvDerivParaPenSpln.pdf:Cao12crvDerivParaPenSpln.pdf:PDF},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2015.01.28},
  url       = {http://people.stat.sfu.ca/~cao/Research/PPSS.html},
}

@InProceedings{Sinn14EnergyDemandFrcst,
  author       = {Mathieu Sinn},
  title        = {Energy Demand Forecasting: Industry Practices and Challenges},
  booktitle    = {e-Energy '14 Proceedings of the 5th international conference on Future energy systems},
  year         = {2014},
  pages        = {121-121},
  address      = {Cambridge},
  organization = {IBM Research},
  abstract     = {Accurate forecasting of energy demand plays a key role for utility companies, network operators, producers and suppliers of energy. Demand forecasts are utilized for unit commitment, market bidding, network operation and maintenance, integration of renewable energy sources, and for novel dynamic pricing mechanisms, e.g., demand response. In order to achieve accurate forecasts with high spatial and temporal resolution, data from various sources needs to be integrated: Smart meters, SCADA, weather forecasts, physical, statistical and geographical models. In this talk I will give an overview of recent work within IBM Research on an intelligent large-scale energy demand forecasting solution which provides forecasts at different aggregation levels, quantifies uncertainty in demand, and estimates the amount of distributed renewable energy behind the meters. The solution can be seamlessly integrated with external applications for network planning and decision support, and has been validated with leading electric utility companies world-wide.},
  comment      = {Demand forecasting overview slides, culminating in the Vermont Project, which appears to be a net demand forecast.  Mentions transfer learning as a challenge.},
  doi          = {10.1145/2602044.2602086},
  file         = {Slides:Sinn14EnergyDemandFrcst.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2017.03.13},
  url          = {http://dl.acm.org/citation.cfm?id=2602086},
}

@InCollection{Barber10arHMMwindFrcst,
  author    = {Chris Barber and Joseph Bockhorst and Paul Roebber},
  title     = {Auto-Regressive {HMM} Inference with Incomplete Data for Short-Horizon Wind Forecasting},
  booktitle = {Advances in Neural Information Processing Systems 23},
  year      = {2010},
  editor    = {J. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R.S. Zemel and A. Culotta},
  pages     = {136--144},
  abstract  = {Accurate short-term wind forecasts (STWFs), with time horizons from 0.5 to 6 hours, are essential for efficient integration of wind power to the electrical power grid. Physical models based on numerical weather predictions are currently not competitive, and research on machine learning approaches is ongoing. Two major challenges confronting these efforts are missing observations and weather-regime induced dependency shifts among wind variables. In this paper we introduce approaches that address both of these challenges. We describe a new regime-aware approach to STWF that use auto-regressive hidden Markov models (AR-HMM), a subclass of conditional linear Gaussian (CLG) models. Although AR-HMMs are a natural representation for weather regimes, as with CLG models in general, exact inference is NP-hard when observations are missing (Lerner and Parr, 2001). We introduce a simple approximate inference method for AR-HMMs, which we believe has applications in other problem domains. In an empirical evaluation on publicly available wind data from two geographically distinct regions, our approach makes signi?cantly more accurate predictions than baseline models, and uncovers meteorologically relevant regimes.},
  comment   = {Does distant data, missing features and regime detection. Joe Bockhort's sort-of graphical model (the guy I gave the reference to for a research project, while I was at 3TIER).

Related to Dereszynski12sensNtwkMissDat},
  file      = {Barber10arHMMwindFrcst.pdf:Barber10arHMMwindFrcst.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2012.10.08},
  url       = {http://books.nips.cc/papers/files/nips23/NIPS2010_1284.pdf},
}

@Article{Larson06shortTermWindOffSite,
  author    = {Kristin A. Larson and Kenneth Westrick},
  title     = {Short-term wind forecasting using off-site observations},
  journal   = {Wind Energy},
  year      = {2006},
  volume    = {9},
  number    = {1-2},
  pages     = {55--62},
  abstract  = {Accurate wind energy forecasts can be an essential component for economic viability of a wind project. Timely and accurate short-term (hours) forecasts can increase the electric grid efficiency and minimize ancillary or other firming requirements, ultimately resulting in reduced costs. This article investigates the use of off-site observations, at distances up to 200?km from the wind farm, as predictors in statistical forecast techniques. In combination with on-site and off-site observations, fine-scale numerical weather predictions can also be used to further increase forecast accuracy at these short forecast horizons. An example from the Pacific Northwest of the USA is described. It is shown that significant forecast improvements are feasible when using off-site observations and/or mesoscale numerical weather predictions in statistical forecast algorithms.},
  comment   = {3TIR 2 Hour wind forecasts w/ offsite met towers and NWP. Oracle experiments hint at 3TIER needs * met towers up to 200km away useful for 2 hour forecasts (I'm not sure which were 200km away, though) Regression on combinations of: * 1 hour NWP (could be interpolated somehow. Do they just copy it?) * 10 minute met tower wind speed on site * 10 minute offsite met tower wind data * time window width not specified * lag from offsite not explained but probably fixed, heuristically (seems like direction used only to condition neural nets) Regressors trired * linear * fixed neural * conditional neural (threshold switches baseed on direction and wind speed) -- is the conditional part really needed? Just have these as NN inputs? * Support Vector Regression * these are retrained once a month, could be more often but they don't Time lag between site and off-site * not explained, either the lag or the width of the time window Oracle conclusions (2 hour lookahead, oracle across features AND regressor types!) * NWP worse than persistence but still helps in conjunction w/ other features * adding offsite observations helps more often than NWP * but site, offsite and NWP features can be better (oracle feature selection) * conditional neural nets consistently best (switched on wind speed, direction thresholds) -- just put those vars into the NN? -- means that a switching regressor is needed? * SVR no better than neural nets My conclusion: 3TIER could use: 1.) switching regression 2.) Switching feature selection 3.) A way tipsier the right lag (switching?) 4.) A way to pick the input window width (If it's not fixed to one -- can't tell from this) 5.) A way to upsample 1 hour NWP to 10 minute intervals. 6.) Missing feature handling},
  doi       = {10.1002/we.179},
  file      = {Larson06shortTermWindOffSite.pdf:Larson06shortTermWindOffSite.pdf:PDF;Larson06shortTermWindOffSite.pdf:Larson06shortTermWindOffSite.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2008.09.30},
}

@Article{Khodayar17RoughDeepNNShrtWndSpdFrcst,
  author   = {M. Khodayar and O. Kaynak and M. E. Khodayar},
  title    = {Rough Deep Neural Architecture for Short-term Wind Speed Forecasting},
  journal  = {IEEE Transactions on Industrial Informatics},
  year     = {2017},
  volume   = {PP},
  number   = {99},
  pages    = {1},
  issn     = {1551-3203},
  abstract = {Accurate wind speed forecasting is a fundamental requirement for large-scale integration of wind power generation. However, the intermittent and stochastic nature of wind speed makes this task challenging. Artificial neural networks (ANNs) are widely used in this area; however, they may fail to provide the accuracy that may be required. This is due to applying shallow architectures with error-prone hand-engineered features. This paper proposes a deep learning network (DNN) architecture with stacked auto-encoder (SAE) and stacked denoising auto-encoder (SDAE) for ultra-short-term and short-term wind speed forecasting. Auto-encoders (AEs) are applied for unsupervised feature learning from the unlabeled wind data and a supervised regression layer is applied at the top of the AEs for wind speed forecasting. There are several uncertain factors exist in the wind data that degrade the accuracy of current methodologies. In order to improve the accuracy, rough neural networks (RNNs) is incorporated in the proposed deep learning models to develop novel rough extensions of SAE and SDAE that are robust to wind uncertainties. Experimental results show that the proposed rough DNN models outperform classic DNNs and previous models that apply shallow architectures in the view of lower RMSE and MAE measurements.},
  doi      = {10.1109/TII.2017.2730846},
  file     = {:Khodayar17RoughDeepNNShrtWndSpdFrcst.pdf:PDF},
  keywords = {Autoregressive processes, Forecasting, Neural networks, Predictive models, Uncertainty, Wind forecasting, Wind speed, Deep Neural Network, Forecasting, Rough Neural Network, Uncertainty, Wind},
}

@Article{openEnergi15machLrnSmrtGrid,
  author    = {OpenEnergi},
  title     = {How Can Machine Learning Create a Smarter Grid?},
  journal   = {openenergi.com},
  year      = {2016},
  month     = nov,
  abstract  = {Across the globe, energy systems are changing and creating unprecedented challenges for the organisations tasked with ensuring the lights stay on. In the UK, National Grid is facing shrinking margins, looming capacity shortages and unpredictable peaks and troughs in energy supply caused by increasing levels of renewable penetration.

At the recent Reinventing Energy Summit, Michael Bironneau, Head of Technology Development at Open Energi, explored how the same machine learning techniques that have let machines defeat chess and Go masters, can also be leveraged to orchestrate massive amounts of flexible demand-side capacity ? from industrial equipment, co-generation and battery storage systems ? towards the one goal of creating a smarter grid; one that is cleaner, cheaper, more secure and more efficient.},
  comment   = {Optimization of Demand Management, uses Deep Reinforcement Learning to learn optimal sequence of actions for rescheduling power consumption.

Sources of flexible demand, examples:
* fridges
* water pumps
* bitumen tanks

Data
* Lack of data is a problem
* Want anonymised 1/2 hourly power data
* Want APIs for reporting and accessing flexibility (I ask: "block chain?")},
  file      = {openEnergi15machLrnSmrtGrid.pdf:openEnergi15machLrnSmrtGrid.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.30},
  url       = {http://www.openenergi.com/creating-a-smarter-grid-with-machine-learning/},
}

@Article{Koenker11addQRfeatsel,
  author    = {Koenker, Roger and others},
  title     = {Additive models for quantile regression: model selection and confidence bandaids},
  journal   = {Brazilian Journal of Probability and Statistics},
  year      = {2011},
  volume    = {25},
  number    = {3},
  pages     = {239--262},
  abstract  = {Additive models for conditional quantile functions provide an attractive frame-
work for non-parametric regression applications focused on features of the response be-
yond its central tendency. Total variation roughness penalities can be used to control
the smoothness of the additive components much as squared Sobelev penalties are used
for classical L 2 smoothing splines. We describe a general approach to estimation and
inference for additive models of this type. We focus attention primarily on selection of
smoothing parameters and on the construction of confidence bands for the nonparametric
components. Both pointwise and uniform confidence bands are introduced; the uniform
bands are based on the Hotelling (1939) tube approach. Some simulation evidence is pre-
sented to evaluate finite sample performance and the methods are also illustrated with an
application to modeling childhood malnutrition in India.},
  comment   = {Implementation of the automatically chosen additive linear QR penalty analyzed in Belloni11sparseQRl1

This is in R, and Belloni11sparseQRl1 has it in matlab},
  file      = {Koenker11addQRfeatsel.pdf:Koenker11addQRfeatsel.pdf:PDF},
  owner     = {sotterson},
  publisher = {Brazilian Statistical Association},
  timestamp = {2014.03.30},
  url       = {http://www.econ.uiuc.edu/~roger/research/bandaids/bandaids.pdf},
}

@Article{Christmann14lrnRateSVMaddQR,
  author      = {{Christmann}, A. and {Zhou}, D.-X.},
  title       = {{Learning rates for the risk of kernel based quantile regression estimators in additive models}},
  journal     = {ArXiv e-prints},
  year        = {2014},
  month       = may,
  abstract    = {Additive models play an important role in semiparametric statis-
tics. This paper gives learning rates for regularized kernel based meth-
ods for additive models. These learning rates compare favourably in
particular in high dimensions to recent results on optimal learning
rates for purely nonparametric regularized kernel based quantile re-
gression using the Gaussian radial basis function kernel, provided the
assumption of an additive model is valid. Additionally, a concrete ex-
ample is presented to show that a Gaussian function depending only
on one variable lies in a reproducing kernel Hilbert space generated by
an additive Gaussian kernel, but does not belong to the reproducing
kernel Hilbert space generated by the multivariate Gaussian kernel of
the same variance.
Key words and phrases. Additive model, kernel, quantile regression,
semiparametric, rate of convergence, support vector machine.},
  adsnote     = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl      = {http://adsabs.harvard.edu/abs/2014arXiv1405.3379C},
  comment     = {SVM additive QR model learns faster than SVN RBF QR model (a surprise?) but the main thing is that there is such a thing as an SVM RBF QR model. Look it up!},
  doi         = {http://arxiv.org/abs/1405.3379},
  eprint      = {1405.3379},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {Christmann14lrnRateSVMaddQR.pdf:Christmann14lrnRateSVMaddQR.pdf:PDF},
  keywords    = {Statistics - Machine Learning},
  owner       = {sotterson},
  timestamp   = {2014.07.04},
}

@Article{Gajowniczek18clustElecLoadFrcst,
  author    = {Gajowniczek, Krzysztof and Z{\k{a}}bkowski, Tomasz},
  title     = {Simulation study on clustering approaches for short-term electricity forecasting},
  journal   = {Complexity},
  year      = {2018},
  volume    = {2018},
  abstract  = {Advanced metering infrastructures such as smart metering have begun to attract increasing attention; a considerable body of
research is currently focusing on load profiling and forecasting at different scales on the grid. Electricity time series clustering
is an effective tool for identifying useful information in various practical applications, including the forecasting of electricity usage,
which is important for providing more data to smart meters. This paper presents a comprehensive study of clustering methods for
residential electricity demand profiles and further applications focused on the creation of more accurate electricity forecasts for
residential customers. The contributions of this paper are threefold: (1) using data from 46 homes in Austin, Texas, the similarity
measures from different time series are analyzed; (2) the optimal number of clusters for representing residential electricity use
profiles is determined; and (3) an extensive load forecasting study using different segmentation-enhanced forecasting algorithms
is undertaken. Finally, from the operator’s perspective, the implications of the results are discussed in terms of the use of clustering
methods for grouping electrical load patterns.},
  comment   = {Load clustering algs,  # clustering picked compared for forecasting a large region. Similar to MNSP.  Many evaluation criteria.

Could be a good guide for how to do load forecast aggregation.},
  doi       = {https://doi.org/10.1155/2018/3683969},
  file      = {:Gajowniczek18clustElecLoadFrcst.pdf:PDF},
  publisher = {Hindawi},
}

@InCollection{Kitigawa08btstrpInfoCrit,
  author    = {Konishi, Sadanori and Kitagawa, Genshiro},
  title     = {Bootstrap Information Criterion},
  booktitle = {Information Criteria and Statistical Modeling},
  publisher = {Springer New York},
  year      = {2008},
  series    = {Springer Series in Statistics},
  pages     = {187--209},
  isbn      = {978-0-387-71887-3},
  abstract  = {Advances in computing now allow numerical methods to be used for modeling complex systems, instead of analytic methods. Complex Bayesian models can now be used for practical applications by using numerical methods such as the Markov chain Monte Carlo (MCMC) technique. Also, when the maximum likelihood estimator cannot be obtained analytically, it is possible to obtain it by a numerical optimization method. In conjunction with the development of numerical methods, model evaluation must now deal with extremely complex and increasingly diverse models. The bootstrap information criterion [Efron (1983), Wong (1983), Konishi and Kitagawa (1996), Ishiguro et al. (1997), Cavanaugh and Shumway (1997), and Shibata (1997)], obtained by applying the bootstrap methods originally proposed by Efron (1979), permits the evaluation of models estimated through complex processes.},
  comment   = {overview of techniques similar to BIC or AIC but is bootstrapped and non-parametric improved methods in: Kitigawa09BiVarRedBtstrpIC},
  doi       = {10.1007/978-0-387-71887-3_8},
  file      = {Konishi08btstrpInfoCrit.pdf:Konishi08btstrpInfoCrit.pdf:PDF},
  keyword   = {Statistics},
  owner     = {sotterson},
  timestamp = {2011.11.30},
}

@Article{Bauer15quietRevNWP,
  author    = {Bauer, Peter and Thorpe, Alan and Brunet, Gilbert},
  title     = {The quiet revolution of numerical weather prediction},
  journal   = {Nature},
  year      = {2015},
  volume    = {525},
  number    = {7567},
  pages     = {47--55},
  month     = sep,
  issn      = {0028-0836},
  abstract  = {Advances in numerical weather prediction represent a quiet revolution because they have resulted from a steady accumulation of scientific knowledge and technological advances over many years that, with only a few exceptions, have not been associated with the aura of fundamental physics breakthroughs. Nonetheless, the impact of numerical weather prediction is among the greatest of any area of physical science. As a computational problem, global weather prediction is comparable to the simulation of the human brain and of the evolution of the early Universe, and it is performed every day at major operational centres across the world.},
  comment   = {What's new in NWP ca 2015.  I borrowed some images from it for GIZ Colombia.  See also: Bauer16tdysWeathFrcstImprv

4D VAR assimilation
* Global ensemble forecast trajectories, which have been
  + initialized by a previous analysis ensemble, are produced over a time window (for example, 09:00?21:00 UTC).
  + These provide estimates of the current weather (first guesses).
    - The difference between these forecasts and available observations (shown as data points with error bars) is the short-range forecast error.

* Minimization difference between this and observatons
  + four dimensions employing variational techniques
  + Result 4D-Var trajectories

* Next cycle of ensemble forecasts is then initialized from these refined analyses.},
  file      = {Bauer15quietRevNWP.pdf:Bauer15quietRevNWP.pdf:PDF},
  owner     = {sotterson},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2017.04.25},
  url       = {http://dx.doi.org/10.1038/nature14956},
}

@Article{Lu08TransLineModLidarAcc,
  author    = {Lu, M.L. and Kieloch, Z.},
  title     = {Accuracy of Transmission Line Modeling Based on Aerial {LiDAR} Survey},
  journal   = {Power Delivery, IEEE Transactions on},
  year      = {2008},
  volume    = {23},
  number    = {3},
  pages     = {1655--1663},
  month     = jul,
  issn      = {0885-8977},
  abstract  = {Aerial LiDAR survey is receiving wide application in transmission-line modeling due to its efficiency. The technique is particularly useful for modeling of existing lines for the purpose of thermal rating, upgrading, or vegetation management. An accurate modeling of an existing line depends largely on proper determination of the base conductor temperature, i.e. the conductor temperature at the time of the aerial light detection and ranging (LiDAR) survey. In this paper, an acceptable accuracy for the base conductor temperature is first established. Extensive parametric studies are then conducted to reveal the effects of all the potentially major factors: ambient air temperature, electrical load, solar radiation, wind, and conductor size on the base conductor temperature. As a result, recommendations are made on the proper practice of performing an aerial LiDAR survey and determining the base conductor temperature so that the resulting transmission line modeling is within an acceptable accuracy. It is demonstrated that a wide error can easily be introduced without following a proper procedure for the LiDAR survey.},
  comment   = {Also see: Armstrong07sagLidarNonContactTemp},
  doi       = {10.1109/TPWRD.2007.911164},
  file      = {Lu08TransLineModLidarAcc.pdf:Lu08TransLineModLidarAcc.pdf:PDF;Lu08TransLineModLidarAcc.pdf:Lu08TransLineModLidarAcc.pdf:PDF},
  keywords  = {airborne radar, optical radar, transmission line theoryaerial LiDAR, aerial light detection and ranging, thermal rating, transmission line modeling, vegetation management},
  owner     = {sotterson},
  timestamp = {2009.02.23},
}

@Article{Wu09featSelQR,
  author    = {Wu, Yichao and Liu, Yufeng},
  title     = {Variable selection in quantile regression},
  journal   = {Statistica Sinica},
  year      = {2009},
  volume    = {19},
  number    = {2},
  pages     = {801},
  abstract  = {After its inception in Koenker and Bassett (1978), quantile regression has
become an important and widely used technique to study the whole conditional
distribution of a response variable and grown into an important tool of applied
statistics over the last three decades. In this work, we focus on the variable se-
lection aspect of penalized quantile regression. Under some mild conditions, we
demonstrate the oracle properties of the SCAD and adaptive-LASSO penalized
quantile regressions. For the SCAD penalty, despite its good asymptotic proper-
ties, the corresponding optimization problem is non-convex and, as a result, much
harder to solve. In this work, we take advantage of the decomposition of the SCAD
penalty function as the difference of two convex functions and propose to solve the
corresponding optimization using the Difference Convex Algorithm (DCA).
Key words and phrases: DCA, LASSO, oracle, quantile regression, SCAD, variable
selection.},
  comment   = {Variable selection for QR},
  file      = {Wu09featSelQR.pdf:Wu09featSelQR.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.05.13},
}

@Article{Lai08extCSD1stAnd2nd,
  author    = {Lai, K-L and Crassidis, JL},
  title     = {Extensions of the first and second complex-step derivative approximations},
  journal   = {Journal of Computational and Applied Mathematics},
  year      = {2008},
  volume    = {219},
  number    = {1},
  pages     = {276--293},
  abstract  = {Ageneral framework for the first and second complex-step derivative approximation to compute numerical derivatives is presented.
For first derivatives the complex-step approach does not suffer roundoff errors as in standard numerical finite-difference approaches.
Therefore, since an arbitrarily small step size can be chosen, the complex-step approach can achieve near analytical accuracy.
However, for second derivatives straight implementation of the complex-step approach does suffer from roundoff errors. Therefore,
an arbitrarily small step size cannot be chosen. In this paper the standard complex-step approach is expanded by using general
complex-step sizes to provide a wider range of accuracy for both the first- and second-derivative approximations. Even higher
accuracy formulations are obtained by repetitively applying Richardson extrapolations. The new extensions can allow the use of
one step size to provide optimal accuracy for both derivative approximations.

Keywords: Complex step; Jacobian; Hessian; Finite-difference},
  comment   = {How to compute 2\textsuperscript{nd} order partial derivatives. The journal paper

Conference paper with kalman filter application: Lai08Extensionsfirstand

Maybe fancier stuff in: Abreu13genCmplxStepDiff},
  file      = {Lai08extCSD1stAnd2nd.pdf:Lai08extCSD1stAnd2nd.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.07.06},
  url       = {http://www.sciencedirect.com/science/article/pii/S0377042707004086},
}

@Article{Rai15agentEnrgyTechAdoptFactors,
  author   = {Varun Rai and Scott A. Robinson},
  title    = {Agent-based modeling of energy technology adoption: Empirical integration of social, behavioral, economic, and environmental factors},
  journal  = {Environmental Modelling \& Software},
  year     = {2015},
  volume   = {70},
  pages    = {163 - 177},
  issn     = {1364-8152},
  abstract = {Agent-based modeling (ABM) techniques for studying human-technical systems face two important challenges. First, agent behavioral rules are often ad hoc, making it difficult to assess the implications of these models within the larger theoretical context. Second, the lack of relevant empirical data precludes many models from being appropriately initialized and validated, limiting the value of such models for exploring emergent properties or for policy evaluation. To address these issues, in this paper we present a theoretically-based and empirically-driven agent-based model of technology adoption, with an application to residential solar photovoltaic (PV). Using household-level resolution for demographic, attitudinal, social network, and environmental variables, the integrated ABM framework we develop is applied to real-world data covering 2004–2013 for a residential solar PV program at the city scale. Two applications of the model focusing on rebate program design are also presented.},
  doi      = {https://doi.org/10.1016/j.envsoft.2015.04.014},
  file     = {:Rai15agentEnrgyTechAdoptFactors.pdf:PDF},
  keywords = {Agent-based modeling, Solar photovoltaic (PV), Complex systems, Technology adoption, Social networks, Bounded rationality},
  url      = {http://www.sciencedirect.com/science/article/pii/S1364815215001231},
}

@InProceedings{Bhowmik19estimaggLrnAggDat,
  author    = {Avradeep Bhowmik and Minmin Chen and Zhengming Xing and Suju Rajan},
  title     = {{E}st{I}m{A}gg: A Learning Framework for Groupwise Aggregated Data},
  booktitle = {Proc. SIAM International Conference on Data Mining},
  year      = {2019},
  pages     = {477-485},
  abstract  = {Aggregation is a common technique in data-driven applications for handling issues like privacy, scalability and reliability in a vast range of domains including healthcare, sensor networks and web applications. However, despite the ubiquitousness, extending machine learning methods to the aggregation context is unfortunately not well-studied. In this work, we consider the problem of learning individual level predictive models when the target variables used for training are only available as aggregates. In particular, this problem is a critical bottleneck in designing effective bidding strategies in the context of online advertising where ground-truth cost-per-click (CPC) data is aggregated before being released to advertisers. We introduce a novel learning framework that can use aggregates computed at varying levels of granularity for building individual-level predictive models. We generalise our modelling and algorithmic framework to handle data from diverse domains, and extend our techniques to cover arbitrary aggregation paradigms like sliding windows and overlapping/non-uniform aggregation. We show empirical evidence for the efficacy of our techniques with experiments on both synthetic data and real data from the online advertising domain as well as healthcare to demonstrate the wider applicability of our framework.
},
  comment   = {Say you have a sum of outcomes from some population, and that you have a vector of predictors for each individual in the population.  This paper shows how to use the global sum and individual predictors to estimate the outcome of each individual, using a GLM.

Does this with a histogram: Bhowmik15glmForAggDat

Uses?
* disaggregation problems: have total electricity consumption on a feeder.  Use this to estimate each customer's consumption.
* imputation: for missing customer data},
  doi       = {10.1137/1.9781611975673.54},
  eprint    = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611975673.54},
  url       = {https://epubs.siam.org/doi/abs/10.1137/1.9781611975673.54},
}

@InProceedings{Tian09newAdaboost_RT,
  author    = {Huixin Tian and Wang, A. and Zhizhong Mao},
  title     = {A new soft sensor modeling method based on modified AdaBoost with incremental learning},
  booktitle = {Proceedings of the 48\textsuperscript{th} IEEE Conference on Decision and Control, 2009 held jointly with the 2009 28\textsuperscript{th} Chinese Control Conference},
  year      = {2009},
  pages     = {8375--8380},
  abstract  = {Aiming at the characteristics of soft sensors, an ensemble learning algorithm AdaBoost.RT is used to establish the soft sensor models. According to the shortcoming of AdaBoost.RT and the difficulties of on-line updating for soft sensor models, a self-adaptive modifying threshold ?? and an incremental learning method are proposed for improving the performance of original AdaBoost.RT. The new modified AdaBoost.RT can overcome the disadvantages of original AdaBoost.RT and update the soft sensor model in real time. The new method is used to establish the soft sensor model of molten steel temperature in 300t LF. Practical production data are used to test the model. The results demonstrate that the new soft sensor model based on modified AdaBoost.RT can improve the prediction accuracy and has good ability of update.},
  comment   = {an improved version of Adaboost.RT (Solomatine04adaBoostContin) w/ adaptive thresholds, applied to sensors

Tian10elmAdaboost_RT seems to be the same idea},
  doi       = {10.1109/CDC.2009.5400292},
  file      = {Tian09newAdaboost_RT.pdf:Tian09newAdaboost_RT.pdf:PDF},
  issn      = {0191-2216},
  keywords  = {inference mechanisms;learning (artificial intelligence);neural nets;AdaBoost modification;incremental learning method;molten steel temperature;self adaptive modifying threshold;soft sensor modeling method;Accuracy;Artificial intelligence;Intelligent sensors;Machine learning;Predictive models;Production;Sensor phenomena and characterization;Steel;Temperature sensors;Training data},
  owner     = {sotterson},
  timestamp = {2014.01.21},
}

@InProceedings{Direito11classMDSepi,
  author    = {Direito, B. and Teixeira, C. and Dourado, A.},
  title     = {On the benefits of classical multidimensional scaling in Epileptic seizure prediction studies},
  booktitle = {Bioengineering (ENBENG), 1\textsuperscript{st} Portuguese Meeting in},
  year      = {2011},
  pages     = {1--4},
  abstract  = {Algorithms for Epileptic seizure prediction using various features extracted from the multichannel Electroencephalographic (EEG) signals, need to work in high dimensional spaces, leading to increased difficulties in computational time and convergence conditions. Multidimensional Scaling (MDS) is a technique to surpass this curse of dimensionality in classification problems. In this work we investigate the influence of dimensional reduction in classification performance by previously applying Multidimensional Scaling and then applying Support Vector Machines (SVM) to classify the brain state. Data from five patients of the European Database on Epilepsy of the FP7 EPILEPSIAE Project is used. The results show that dimension reduction improves less than expected the SVM performance.},
  comment   = {Dim reduce with classical MDS and then use SVM classifier to predict seisures. Not transductive: they use an MDS transform trained on the training data only, and then reuse it on the test, with no further training. Don't say how, though.

* A heaver user of the Matlab Toolbox for Dimensionality Reduction: Maaten09DimRedCmprTechNote
* out of sample extension for MDS (and other stuff) seems to be here: Bengio04OutOfSmplExt},
  doi       = {10.1109/ENBENG.2011.6026063},
  file      = {Direito11classMDSepi.pdf:Direito11classMDSepi.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.06.26},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6026063},
}

@InProceedings{Maggio10frcstERCOT,
  author    = {Maggio, D. and D'Annunzio, C. and Shun-Hsien Huang and Thompson, C.},
  title     = {Utilization of forecasts for Wind-powered Generation Resources in ERCOT operations},
  booktitle = {IEEE Power \& Energy Society (PES)},
  year      = {2010},
  pages     = {1--5},
  month     = jul,
  abstract  = {Along with many other Independent System Operators, the ERCOT ISO has been charged with the task of integrating renewable resources into the electric power system. A significant amount of this effort has been centered on wind energy. Wind power forecasting has been a key factor in being able to incorporate large Wind-powered Generation Resources (WGRs) into normal operations. Consequently, this paper will present the wind power forecasting tools used by the ERCOT ISO and describe how they are being integrated in System Operations. As part of this discussion, the challenges that have surfaced in implementing these wind power forecasting tools will be explored. Finally, there will be a brief look into anticipated future work and goals for the ERCOT ISO.},
  comment   = {ERCOT doesn't use full probabilstic forecast (only levels). No direct use of ramp forecast.

Forecast is semi-probabilistic:
* Two interval forecasts: 50\% and 80\%
* use single prob. contours, one or the other, not both
* don't optimize over whole pdf.

2008:
* ERCOT forced wind producing use its "80\% over" forecast.
* For day ahead forecast.
* May eventually use 50\%.
* Producers can only subtract from it for maintenance, etc.
* Their forcast still doesn't use turbine availability, maybe not weather station data.

2009:
* forced them to produce hour-ahead forecasts at least as accruate as ERCOT's 50\% forecast
* Why less accurate than day ahead?
* B/C tighter bounds on ERCOT's hour ahead frcst means less absolute power error potential?

Near-real-time balancing seems to be mostly manual control
* manual control of 15 minute ahead balancing
* manual hack for curtailment

Non-Spinning reserve
* non-spinning: can respond in 30 minute
* historial forecast/load error histogram to determine 95\% confidence

* seems to be fixed: isn't "situation-dependent". Called "resolution" in Pinson06qualValProbFrcst

Lack of data problems
* turbine
* site met obs (I think)
* cutoffs Ramp forecast
* used for "situational awareness" and "education" * not numerically in balancing.},
  doi       = {10.1109/PES.2010.5589494},
  file      = {Maggio10frcstERCOT.pdf:Maggio10frcstERCOT.pdf:PDF},
  groups    = {Read, Use, doReadWPV_1},
  issn      = {1944-9925},
  keywords  = {ERCOT ISO operations;Electric Reliability Council of Texas;wind power forecasting;wind power generation resources;load forecasting;power generation reliability;wind power plants;},
  owner     = {scot},
  timestamp = {2011.05.03},
}

@InProceedings{Liu10probLoadFlow,
  author    = {Yifang Liu and Buhan Zhang and Jianghong Wang and Junfang Li and Xu Zheng and Kai Wang and Bingying Wu and Biao Mao},
  title     = {Static security analysis of smart grid adapting to new energy supply injection},
  booktitle = {Power System Technology (POWERCON)},
  year      = {2010},
  pages     = {1--6},
  abstract  = {Along with the advance of the smart grid, more and more new energy is introduced into the power system, especially wind power. Their natural characteristics like stochastic capacity and intermittent fluctuation produce an impact on the power grid's operation security. This paper takes many uncertain factors into account such as the variation of loads and generator outages. In this paper, a Beta distribution model is built to describe the short-term production of the wind power and probabilistic load flow which combines the concept of semi-invariant and Improved Von Mises method is presented to obtain the probability distribution function (PDF) and the cumulative distribution function(CDF) of node voltages and transmission line flows. Three static security evaluation indices are introduced in this paper for planning and real time operation respectively. The calculation on the IEEE RTS-24 node test system not only shows the static security evaluation indices, but also provides preparation for the location of wind power plant and maximum wind power capacity. The case indicates that this method can analyse the real time static security of smart grid with new energy injection rapidly and accurately, and is with practical application value.},
  comment   = {stochatistic load flow security analysis (might be a probabilistic forecast use that German TSO's could use).

* uses Beta distribution for wind power.g},
  doi       = {10.1109/POWERCON.2010.5666402},
  file      = {Liu10probLoadFlow.pdf:Liu10probLoadFlow.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  keywords  = {power distribution;power system security;probability;smart power grids;wind power;Beta distribution model;IEEE RTS-24 node test system;Von Mises method;cumulative distribution function;energy supply injection;planning;power system;probability distribution function;short-term production;smart grid;static security;static security analysis;wind power plant;Fluctuations;Green products;Production;Real time systems;Security;Wind forecasting;Wind power generation;new energy integration;probabilistic load flow;static security analysis;wind power},
  owner     = {sotterson},
  timestamp = {2013.10.01},
}

@Article{Menn09smthTruncStblDist,
  author    = {Menn, Christian and Rachev, Svetlozar T},
  title     = {Smoothly truncated stable distributions, GARCH-models, and option pricing},
  journal   = {Mathematical Methods of Operations Research},
  year      = {2009},
  volume    = {69},
  number    = {3},
  pages     = {411--438},
  abstract  = {Although asset return distributions are known to be conditionally leptokur-

tic, this fact was rarely addressed in the recent GARCH model literature. For this

reason, we introduce the class of smoothly truncated stable distributions (STS dis-

tributions) and derive a generalized GARCH option pricing framework based on

non-Gaussian innovations. Our empirical results show that (1) the model?s perfor-

mance in the objective as well as the risk-neutral world is substantially improved

by allowing for non-Gaussian innovations and (2) the model?s best option pricing

performance is achieved with a new estimation approach where all model param-

eters are obtained from time-series information whereas the market price of risk

and the spot variance are inverted from market prices of options.},
  comment   = {How to make Axel's alpha(?)-stable distribution bounded so that extreme values can be forecasted that fit within actual wind power limits.

Paper is a 2005 draft; I couldn't get the published article so I HOPE it's accurate enough.

* Stable distribution intro is here: Nolan15StableDistBootCh1
* another intro w/ mention of trunc., est, from quantiles: Borak05stableDist
},
  file      = {Menn09smthTruncStblDist.pdf:Menn09smthTruncStblDist.pdf:PDF},
  publisher = {Springer},
  url       = {http://www.pstat.ucsb.edu/research/papers/sts-option.pdf},
}

@Article{Kim06quantRgrsnShapeConstr,
  author    = {Kim, Mi-Ok},
  title     = {Quantile Regression With Shape-Constrained Varying Coefficients},
  journal   = {Sankhya - the Indian Journal of Statistics},
  year      = {2006},
  volume    = {68},
  number    = {3},
  pages     = {369--391},
  issn      = {0972-7671},
  abstract  = {Although much research has been devoted to shape-constrained function estimation, the efforts have been practically confined to the case of univariate smoothing where the unknown function is a function of a single variable. We extend shape-constrained function estimation to a general class of constrained nonparametric or semi-parametric regression where the nonparamet- ric component can be described by one-dimensional smooth functions. Built on the ideas of He and Shi (1998) and He and Ng (1999), we consider quantile regression with shape constrained coefficient functions. B-splines are used to approximate the unknown coefficient functions, and shape constraints are imposed on the spline coefficients. The method can be implemented with any existing linear program and knot selection algorithm. We show that the method does not compromise smoothness of the estimators, flexibility of the model or computational efficiency. Asymptotic results show that the con- strained B-spline estimators have the same rate of convergence and the same normal limiting distribution as the unconstrained estimators. The method can accommodate a general class of linearizable shape constraints such as convexity/concavity, monotonicity, periodicity and pointwise constraints. AMS (2000) subject classification. Primary 62G08, 62G10, 62G35, 62G20. Keywords and phrases. Convexity, concavity, monotonicity, asymptotic con- sistency.},
  comment   = {quantile regression w/ constraints like montonicity, and others. Useful for general multivariate constraints.

Could also be a way to get a pdf forecast that to does the right things at the edges for wind power.

Use, somehow, for Tryggvi's spinning reserves price model?},
  file      = {Kim06quantRgrsnShapeConstr.pdf:Kim06quantRgrsnShapeConstr.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_2},
  owner     = {scot},
  timestamp = {2010.12.22},
}

@Article{Koenker94QuantSmthSplns,
  author    = {Koenker, Roger and Ng, Pin and Portnoy, Stephen},
  title     = {Quantile smoothing splines},
  journal   = {Biometrika},
  year      = {1994},
  volume    = {81},
  number    = {4},
  pages     = {673--680},
  abstract  = {Although nonparametric regression has traditionally focused on the estimation of conditional
mean functions, nonparametric estimation of conditional quantile functions is
often of substantial practical interest. We explore a class of quantile smoothing splines,
defined as solutions to
xxx
with Pt(u) = u { r- I(u  0)}, p;?: 1, and appropriately chosen r For the particular choices
p = 1 and p = oo we characterise solutions g as splines, and discuss computation by standard
lrtype linear programming techniques. At A= 0, g interpolates the rth quantiles at
the distinct design points, and for A sufficiently large g is the linear regression quantile fit
(Koenker & Bassett, 1978) to the observations. Because the methods estimate conditional
quantile functions they possess an inherent robustness to extreme observations in the y;'s.
The entire path of solutions, in the quantile parameter r, or the penalty parameter A, may
be efficiently computed by parametric linear programming methods. We note that the
approach may be easily adapted to impose monotonicity and/or convexity constraints on
the fitted function. An example is provided to illustrate the use of the proposed methods.
Some key words: Bandwidth selection; Non parametric regression; Quantile; Smoothing; Spline.},
  comment   = {SIC(lambda): A BIC-like model complexity penalized cost function for quantile regression. Is used in He99multiVarQRsplines

Also suggestion to use quantile smoothing splines for multivariate QR, but people later used regression splines rather than putting a knot at every point e.g. He99multiVarQRsplines},
  file      = {Koenker94QuantSmthSplns.pdf:Koenker94QuantSmthSplns.pdf:PDF},
  owner     = {sotterson},
  publisher = {Biometrika Trust},
  timestamp = {2014.11.07},
}

@InCollection{Marx10varyCoeffPsplineCmplx,
  author    = {Marx, Brian D},
  title     = {P-spline varying coefficient models for complex data},
  booktitle = {Statistical Modelling and Regression Structures},
  publisher = {Springer},
  year      = {2010},
  pages     = {19--43},
  abstract  = {Although the literature on varying coefficient models (VCMs) is vast, we
believe that there remains room to make these models more widely accessible and
provide a unified and practical implementation for a variety of complex data settings.
The adaptive nature and strength of P-spline VCMs allow a full range of
models: from simple to additive structures, from standard to generalized linear models,
from one-dimensional coefficient curves to two-dimensional (or higher) coefficient
surfaces, among others, including bilinear models and signal regression. As Pspline
VCMs are grounded in classical or generalized (penalized) regression, fitting
is swift and desirable diagnostics are available. We will see that in higher dimensions,
tractability is only ensured if efficient array regression approaches are implemented.
We also motivate our approaches through several examples, most notably
the German deep drill data, to highlight the breadth and utility of our approach.},
  comment   = {Building variable coefficient models w/ psplines. Has Fourier basis periodicity, 2 dimensional, etc.},
  doi       = {10.1007/978-3-7908-2413-1_2},
  file      = {Marx10varyCoeffPsplineCmplx.pdf:Marx10varyCoeffPsplineCmplx.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.10},
}

@Article{Yiou10precipRegimeCCA,
  author    = {Mathieu Vrac and Pascal Yiou},
  title     = {Weather regimes designed for local precipitation modeling: Application to the Mediterranean basin},
  journal   = {Journal of Geophysical Research},
  year      = {2010},
  volume    = {115},
  number    = {D12103},
  abstract  = {Although weather regimes are often used as a primary step in many statistical downscaling processes, they are usually defined solely in terms of atmospheric variables and seldom to maximize their correlation to observed local meteorological phenomena. This paper compares different clustering methods to perform such a task. The correlation clustering model is introduced to define regimes that are well correlated to local?scale precipitation observed on seven French Mediterranean rain gauges. This clustering method is compared to other approaches such as the k?means and ?expectation?maximization? (EM) algorithms. The two latter are applied either to the main principal components of large?scale reanalysis data (geopotential height at 500 mbar and sea level pressure) covering the Mediterranean basin or to the canonical variates associated with large scale and resulting from a canonical correlation analysis performed on reanalyses and local precipitation. The weather regimes obtained by the different approaches are compared, with a focus on the ?extreme content? captured within the regimes. Then, cost functions are developed to quantify the errors due to misclassification, in terms of local precipitation. The different clustering approaches show different misclassification and costs. EM applied to canonical variates appears as a good compromise between the other approaches, with high discrimination, overall for extreme precipitation, while the precipitation costs due to bad classification are acceptable. This paper provides tools to help the users choose the clustering method to be used according to the expected goal and the use of the weather regimes.},
  comment   = {CCA relates wide area met model to 7 rain gauges, looking for extreme rain. A mixture of CCA's. - focus on extreme events relevant to Radar\@Sea or Safewind Has R package, CCMtools http://cran.r-project.org/web/packages/CCMtools/index.html},
  doi       = {10.1029/2009JD012871},
  file      = {Yiou10precipRegimeCCA.pdf:Yiou10precipRegimeCCA.pdf:PDF},
  owner     = {scotto},
  timestamp = {2010.08.01},
  url       = {http://www.agu.org.globalproxy.cvt.dk/pubs/crossref/2010/2009JD012871.shtml},
}

@Article{Koenker05ineqConstrQR,
  author    = {Koenker, Roger and Ng, Pin},
  title     = {Inequality constrained quantile regression},
  journal   = {Sankhy{\=a}: The Indian Journal of Statistics},
  year      = {2005},
  pages     = {418--440},
  abstract  = {An algorithm for computing parametric linear quantile regression estimates
subject to linear inequality constraints is described. The algorithm is a
variant of the interior point algorithm described in Koenker and Portnoy
(1997) for unconstrained quantile regression and is consequently quite efficient
even for large problems, particularly when the inherent sparsity of
the resulting linear algebra is exploited. Applications to qualitatively constrained
nonparametric regression are described in the penultimate sections.
Implementations of the algorithm are available in MATLAB and R.

Keywords and phrases. Quantile regression, qualitative constraints, interior
point algorithm, sparse matrices, smoothing.},
  comment   = {Frisch-Newton QR optimization: (roughness, mono, etc.) in matlab:

rq_fnm.m (although they call it rq.m)
http://www.econ.uiuc.edu/~roger/research/rq/rq.m

More software description:
http://www.econ.uiuc.edu/~roger/research/rq/rq.html},
  file      = {Koenker05ineqConstrQR.pdf:Koenker05ineqConstrQR.pdf:PDF},
  owner     = {sotterson},
  publisher = {JSTOR},
  timestamp = {2014.11.04},
  url       = {http://www.jstor.org/stable/25053440},
}

@Article{Moller08adaptQuantRegr,
  author    = {M{\o}ller, Jan Kloppenborg and Nielsen, Henrik Aalborg and Madsen, Henrik},
  title     = {Time-adaptive quantile regression},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {2008},
  volume    = {52},
  number    = {3},
  pages     = {1292--1303},
  issn      = {0167-9473},
  abstract  = {An algorithm for time-adaptive quantile regression is presented. The algorithm is based on the simplex algorithm, and the linear optimization formulation of the quantile regression problem is given. The observations have been split to allow a direct use of the simplex algorithm. The simplex method and an updating procedure are combined into a new algorithm for time-adaptive quantile regression, which generates new solutions on the basis of the old solution, leading to savings in computation time. The suggested algorithm is tested against a static quantile regression model on a data set with wind power production, where the models combine splines and quantile regression. The comparison indicates superior performance for the time-adaptive quantile regression in all the performance parameters considered.},
  comment   = {DTU guy's quantile regression algorithm, including what I think is its Matlab representation. Adaptation keeps a sliding time window of points in an analysis window, only shifting old points with the newest sample is in the same magnitude bin and that bin is already full (also some constraint for current state of simplex algorithm solution). It's 2X-5X faster than doing a full model train at every time step.

The matlab is in the technical report, which came from here: http://orbit.dtu.dk/en/publications/id(541b9fb7-6425-4bb4-9bdc-780a5721fada).html},
  doi       = {10.1016/j.csda.2007.06.027},
  file      = {2008 paper:Moller08adaptQuantRegr.pdf:PDF;2006 Matlab, in Technical Report:Moller06adaptQuantRegrTechRep.pdf:PDF},
  groups    = {DOE-PNL09, PointDerived, doReadWPV_2},
  location  = {Amsterdam, The Netherlands, The Netherlands},
  owner     = {sotterson},
  publisher = {Elsevier Science Publishers B. V.},
  timestamp = {2009.03.03},
}

@InProceedings{Nissen12analogEnsPowFrcst,
  author    = {Jesper Nissen and Luca Delle Monache and Sue Ellen Haupt and Tomislav Maric and Line Gulstad},
  title     = {Analog Ensemble based power forecasting},
  booktitle = {Wire 1002},
  year      = {2012},
  month     = may,
  abstract  = {An analog ensemble approached for probabilistically power forecasting is tested for Sprog? Offshore Wind Park in Great Belt Denmark and the performance verified for a set of Numerical Weather Prediction models, a range of analogs searched for and finally is the method benchmarked against a few baseline industry standard methods over a 2.5 months period. The Analog Ensemble approach proves to be a strong candidate as a power forecasting algorithm as the baseline methods are clearly outperformed in terms of significant lower RMS and superior Spearman rank correlation - The system stochastic performance is verified in terms of relevant skill scores and a close to perfectly reliable performance is found when forecasting the chance of power production to be below 50 \% of nominal power on the 12-36 lead time.},
  comment   = {An ad-hoc KNN regression deterministic forecast. -- outperforms linear regression! A bit like my local linear idea.

Called an "analog ensemble technique," it's evaluated "stochastically" by ranking probability of getting power less than the 50\% quantile -- a kind of prob. frcst, then.

Also computed RMSE and spearman corr.

could also somehow use <<Distribution to Distribution Regression>>, as in Oliva13dist2distRgrssn

analog forecasts are the future: Lew12tusconFrcstWrkshp
},
  file      = {Nissen12analogEnsPowFrcst.pdf:Nissen12analogEnsPowFrcst.pdf:PDF},
  groups    = {Ensemble, Use, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2013.10.10},
}

@Article{Junk15AnalogEnsmblKNN,
  author   = {Junk, Constantin and Delle Monache, Luca and Alessandrini, Stefano},
  title    = {Analog-based Ensemble Model Output Statistics},
  journal  = {Monthly Weather Review},
  year     = {2015},
  volume   = {143},
  number   = {2015},
  pages    = {29092917},
  month    = jul,
  abstract = {An analog-based ensemble model output statistics (EMOS) is proposed to improve EMOS for the calibration of ensemble forecasts. Given a set of analog predictors and corresponding weights, which are optimized with a brute-force continuous ranked probability score (CRPS) minimization, forecasts similar to a current ensemble forecast (i.e., analogs) are searched. The best analogs and the corresponding observations form the training dataset for estimating the EMOS coefficients. To test the new approach for renewable energy applications, wind speed measurements at 100-m height from six measurement towers and wind ensemble forecasts at 100-m height from the European Centre for Medium-Range Weather Forecasts (ECMWF) Ensemble Prediction System (EPS) are used. The analog-based EMOS is compared against EMOS, an adaptive and recursive wind vector calibration (AUV), and an analog ensemble applied to ECMWF EPS. It is shown that the analog-based EMOS outperforms EMOS, AUV, and the analog ensemble at all measurement sites in terms of CRPS and Brier score for common and rare events. The CRPS improvements relative to EMOS reach up to 11% and are statistically significant at almost all sites. The reliability of the analog-based EMOS ensemble for rare events is better compared to EMOS and AUV and is similar compared to the analog ensemble.

Keywords: Statistical techniques, Ensembles, Forecast verification/skill, Forecasting techniques, Probability forecasts/models/distribution},
  comment  = {A KNN based analog ensemble forecast method. Presented at the Kassel Eweline meeting in June 2015.},
  doi      = {10.1175/MWR-D-15-0095.1},
  file     = {Junk15AnalogEnsmblKNN.pdf:Junk15AnalogEnsmblKNN.pdf:PDF},
}

@Article{Grimit06continRankProbCircVar,
  author    = {Grimit, E. P. and T. Gneiting and V. J. Berrocal and N. A. Johnson},
  title     = {The continuous ranked probability score for circular variables and its application to mesoscale forecast ensemble verification},
  journal   = {Quarterly Journal of the Royal Meteorological Society},
  year      = {2006},
  volume    = {132},
  pages     = {2925--2942},
  abstract  = {An analogue of the linear continuous ranked probability score is introduced that applies to probabilistic forecasts of circular quantities, such as wind direction. This scoring rule is proper and thereby discourages hedging. The circular continuous ranked probability score reduces to angular distance when the forecast is deterministic, just as the linear continuous ranked probability score generalizes the absolute error. Furthermore, the circular continuous ranked probability score provides a direct way of comparing deterministic forecasts, discrete forecast ensembles, and post-processed forecast ensembles that can take the form of circular probability density functions. The circular continuous ranked probability score is used in this study to compare predictions of 10 m wind direction for 361 cases of mesoscale, short-range ensemble forecasts over the North American Pacific Northwest. Simple, calibrated probability forecasts based on the ensemble mean and its forecast error history over the period outperform probability forecasts constructed directly from the ensemble sample statistics. These results suggest that short-term forecast uncertainty is not yet well predicted at mesoscale resolutions near the surface, despite the inclusion of multi-scheme physics diversity and surface boundary parameter perturbations in the mesoscale ensemble design.},
  comment   = {Circular probability statistics, as in wind direction
* CPRS

Might be important b/c wind DIRECTION is important for windpower prediction. Possibly, DWD's ensemble testing will only consider speed?},
  doi       = {10.1256/qj.05.235},
  file      = {Grimit06continRankProbCircVar.pdf:Grimit06continRankProbCircVar.pdf:PDF},
  groups    = {Test, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2009.04.08},
}

@TechReport{Hoogerheide08frcstRiskImp,
  author      = {Lennart Hoogerheide and Herman K. van Dijk},
  title       = {{Bayes}ian Forecasting of Value at Risk and Expected Shortfall using Adaptive Importance Sampling},
  institution = {Tinbergen Institute},
  year        = {2008},
  type        = {Tinbergen Institute Discussion Papers},
  number      = {08-092/4},
  month       = oct,
  abstract    = {An efficient and accurate approach is proposed for forecasting Value at Risk [VaR] and Expected Shortfall [ES] measures in a Bayesian framework. This consists of a new adaptive importance sampling method for Quantile Estimation via Rapid Mixture of t approximations [QERMit]. As a first step the optimal importance density is approximated, after which multi-step ?high loss? scenarios are efficiently generated. Numerical standard errors are compared in simple illustrations and in an empirical GARCH model with Student-t errors for daily S\&P 500 returns. The results indicate that the proposed QERMit approach outperforms several alternative approaches in the sense of more accurate VaR and ES estimates given the same amount of computing time, or equivalently requiring less computing time for the same numerical accuracy.},
  comment     = {Forecasts of various risk measures. Use for asymmetric cost function for ramp forecasts?},
  file        = {Hoogerheide08frcstRiskImp.pdf:Hoogerheide08frcstRiskImp.pdf:PDF;Hoogerheide08frcstRiskImp.pdf:Hoogerheide08frcstRiskImp.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2009.02.10},
  url         = {http://ideas.repec.org/p/dgr/uvatin/20080092.html},
}

@Article{Klucher79Evaluationmodelspredict,
  author    = {T.M. Klucher},
  title     = {Evaluation of models to predict insolation on tilted surfaces},
  journal   = {Solar Energy},
  year      = {1979},
  volume    = {23},
  number    = {2},
  pages     = {111 - 114},
  issn      = {0038-092X},
  abstract  = {An empirical study was performed to evaluate the validity of various insolation models which employ either an isotropic or an anisotropic distribution approximation for sky light when predicting insolation on tilted surfaces. Data sets of measured hourly insolation values were obtained over a 6-month period using pyranometers which received diffuse and total solar radiation on a horizontal plane and total radiation on surfaces tilted toward the equator at 37?? and 60?? angles above the horizon. Data on the horizontal surfaces were used in the insolation models to predict insolation on the tilted surface; comparisons of measured vs calculated insolation on the tilted surface were examined to test the validity of the sky light approximations. It was found that the Liu-Jordan isotropic distribution model provides a good fit to empirical data under overcast skies but underestimates the amount of solar radiation incident on tilted surfaces under clear and partly cloudy conditions. The anisotropic-clear-sky distribution model by Temps and Coulson provides a good prediction for clear skies but overstimates the solar radiation when used for cloudy days. An anisotropic-all-sky model was formulated in this effort which provided excellent agreement between measured and predicted insolation throughout the 6-month period.},
  comment   = {An alternative method for IWES to map radiation onto an inclined surface.  But the alternative used seems to be Perez90ModelDayLightAvail

Referred to in: Saint-Drenan14commentsGenPVpow

Attached is a tech note from the previous year.  Seems to be the same, or at least has the same title.},
  doi       = {http://dx.doi.org/10.1016/0038-092X(79)90110-5},
  file      = {1978 tech note:Klucher79Evaluationmodelspredict_TR.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.30},
  url       = {http://www.sciencedirect.com/science/article/pii/0038092X79901105},
}

@Article{Souza09gpBoostFrcst,
  author    = {Luzia Vidal de Souza and Aurora T. R. Pozo},
  title     = {Genetic Programming and Boosting Technique to Improve Time Series Forecasting},
  journal   = {Evolutionary Computation},
  year      = {2009},
  abstract  = {An essential element for many management decisions is an accurate forecasting. There are
several methods and techniques to forecast time series that include traditional forecasting
techniques with theoretical foundations in statistics. These methods present some obstacles
and complexities to overcome; one of the most important ones is the difficulty to select the
model that can provide the best adjustment for a specific dataset, many attempts have to be
usually done until the best model can be obtained. Considering this scenario, different
machine learning techniques have been recently used in this problem, such as Artificial
Neural Network (ANN), Evolutionary Computation (EC), in particular, Genetic
Programming (GP), which is considered a promising approach to forecast noisy complex
series (Kaboudan, 2000), there are many other works founded in the literature that use (GP)
to Time Series Prediction. On the other hand, recently advances in the machine learning
field show that the application of the Boosting algorithm is a powerful approach to increase
the accuracy of forecasting methods. Boosting algorithm was proposed and developed by
Freund and Schapire (1996). According to Allwein et al. (2000), Boosting is a method of
finding a highly accurate hypothesis by combining many "weak" hypotheses, each of which
is only moderately accurate. Paris et al. (2004) proposed GPBoost that uses the Boosting
algorithm with the GP as base learner. We have proposed a new formula for the updating of
the weights and for obtain the final hypothesis of the predictor. This algorithm was called of
Boosting Correlation Coefficients (BCC) and it is based on the correlation coefficient instead
of the loss function used by traditional Boosting algorithms. To evaluate this approach we
conducted three experiments. In the first one, the BCC was used to forecast real time series,
in this experiment the mean squared error (MSE) has been used to compare the accuracy of
the proposed method against the results obtained by GP, GPBoost and the traditional
statistical methodology (ARMA). In the second, to prove the efficiency of the proposed
methodology a widespread Monte Carlo simulation was done covering the entire ARMA
spectrum, in which artificial series were generated from the parametric space of the
principal ARMA models, they are AR(1), AR(2), MA(1), MA(2) e ARMA(1,1). The database
generated was composed by 214.000 time series with 150 observations each one. The
training set was composed by 90 pct of date and the others 10\% composes the test set. The
results were compared out of sample and the BCC showed better performance than ARMA
methodology, Genetic Programming and GPBoost. Finally, the BCC algorithm was also
applied to multiple regressions problem and the results obtained from this method were
compared with the results from Artificial Neural Network, Model Tree and Boosting. This
comparison showed that the BCC supplied better results than other ones. In way compare
the performance of the BCC methodology with other methods, many statistical tests were
performed such as Median Square Error (MSE), Root Median Square Error (RMSE) and a
non parametric test Friedman. The results were compared out of sample and the BCC
methodology had been presented accurate forecasts. Future research Considering that GP is
able to provide solutions of high quality, and after the success of our own experiments
(Souza et al., 2007a), we are encouraged to further explore GP towards finding solutions to
the problem of modeling and pattern discovery of complex time series and in additional we
will investigate the procedure BCC using GP as a base learner to analyze probabilistic and
stochastic processes. We will investigate new tools that can work GP to more effectively
solve this problem. One of the most important applications for the time series analysis is in
stock markets. The goal of this task is to choose the best stocks when making an investment,
and to decide which is the best strategy at the moment. Therefore, we will investigate the
appropriate means for using GP in this task, as well as other general problems in financial
time series. An another application that we must investigate is in Biological Networks, for
example, gene regulatory network.},
  book      = {Evolutionary Computation},
  comment   = {continous forecasting boosting w/ genetic programming.

Similar to adaboosting Lillywhite13featCnstrct ?

This was in an open source journal, so I'm not sure how good it is.},
  file      = {Souza09gpBoostFrcst.pdf:Souza09gpBoostFrcst.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.01.21},
  url       = {http://www.intechopen.com/books/export/citation/BibTex/evolutionary-computation/genetic-programming-and-boosting-technique-to-improve-time-series-forecasting},
}

@Article{Garcia-Bustamante08weibullWind,
  author    = {E. Garc?a-Bustamante and J. F. Gonz?lez-Rouco and P. A. Jim?nez and J. Navarro and J. P. Mont?vez},
  title     = {The influence of the Weibull assumption in monthly wind energy estimation},
  journal   = {Wind Energy},
  year      = {2008},
  volume    = {11},
  number    = {5},
  pages     = {483--502},
  abstract  = {An estimation of the monthly wind energy output for the period 1999-2003 at five wind farms in northeastern Spain was evaluated. The methodology involved the calculation of wind speed histograms and the observed average wind power versus wind relation obtained from hourly data. The energy estimation was based on the cumulated contribution of the wind power from each wind speed interval. The impact of the Weibull distribution assumption as a substitute of the actual histogram in the wind energy estimation was evaluated. Results reveal that the use of a Weibull probability distribution has a moderate impact in the energy calculation as the largest estimation errors are, on average, no larger than 10\% of the total monthly energy produced. However, the evaluation of the goodness of fit through the 2 statistics shows that the Weibull assumption is not strictly substantiated for most of the sites. This apparent discrepancy is based on the partial cancellation of the positive and negative departures of the Weibull fitted and the actual wind frequency distributions. Further investigation of the relation between the 2 and the error contribution exposes a tendency of the Weibull distribution to underestimate (overestimate) the observed histograms in the lower and upper (intermediate) wind speed intervals. This fact, together with the larger wind power weight over the highest winds, results in a systematic total wind energy underestimation.},
  comment   = {Reccomnended by Ken?},
  file      = {Garcia-Bustamante08weibullWind.pdf:Garcia-Bustamante08weibullWind.pdf:PDF;Garcia-Bustamante08weibullWind.pdf:Garcia-Bustamante08weibullWind.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.09.24},
  url       = {http://www3.interscience.wiley.com.offcampus.lib.washington.edu/journal/117947264/abstract},
}

@Article{Chen16DynamicCovarianceModels,
  author    = {Ziqi Chen and Chenlei Leng},
  title     = {Dynamic Covariance Models},
  journal   = {Journal of the American Statistical Association},
  year      = {2016},
  volume    = {111},
  number    = {515},
  pages     = {1196-1207},
  abstract  = {An important problem in contemporary statistics is to understand the relationship among a large number of variables based on a dataset, usually with p, the number of the variables, much larger than n, the sample size. Recent efforts have focused on modeling static covariance matrices where pairwise covariances are considered invariant. In many real systems, however, these pairwise relations often change. To characterize the changing correlations in a high-dimensional system, we study a class of dynamic covariance models (DCMs) assumed to be sparse, and investigate for the first time a unified theory for understanding their nonasymptotic error rates and model selection properties. In particular, in the challenging high-dimensional regime, we highlight a new uniform consistency theory in which the sample size can be seen as n4/5 when the bandwidth parameter is chosen as h proportional to n exp -1/5 for accounting for the dynamics. We show that this result holds uniformly over a range of the variable used for modeling the dynamics. The convergence rate bears the mark of the familiar bias-variance trade-off in the kernel smoothing literature. We illustrate the results with simulations and the analysis of a neuroimaging dataset. Supplementary materials for this article are available online.

Key Words: Covariance model; Dynamic covariance; Functional connectivity; High Dimen-
sionality; Marginal independence; Rate of convergence; Sparsity; Uniform consistency},
  comment   = {Big covariance model for which cov. coeffs are conditional on some data.  Good for wind power level, wind speed direction, etc...},
  doi       = {10.1080/01621459.2015.1077712},
  eprint    = {http://dx.doi.org/10.1080/01621459.2015.1077712},
  file      = {Chen16DynamicCovarianceModels.pdf:Chen16DynamicCovarianceModels.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.16},
  url       = {http://dx.doi.org/10.1080/01621459.2015.1077712},
}

@Conference{Biller04scenarioDepModel,
  author       = {Biller, B. and Ghosh, S.},
  title        = {Dependence modeling for stochastic simulation},
  booktitle    = {Conference on Winter Simulation},
  year         = {2004},
  pages        = {153--161},
  organization = {Winter Simulation Conference},
  abstract     = {An important step in designing stochastic simulation is modeling the uncertainty in the input environment of the system being studied. Obtaining a reasonable representation of this uncertainty can be challenging in the presence of dependencies in the input process. This tutorial attempts to provide a coherent narrative of the central principles that underlie methods that aim to model and sample a wide variety of dependent input processes.

Could be used for prob forecast feature selection too.},
  comment      = {Classic methods for determing stochastic dependence. Could use this for scenario forecast feature selection.},
  file         = {Biller04scenarioDepModel.pdf:Biller04scenarioDepModel.pdf:PDF},
  groups       = {Ensemble, PointDerived, Use, doReadNonWPV_2},
  isbn         = {0780387864},
  owner        = {scot},
  timestamp    = {2010.11.24},
}

@Article{Aflalo13SpecMDS,
  author    = {Aflalo, Yonathan and Kimmel, Ron},
  title     = {Spectral multidimensional scaling},
  journal   = {Proceedings of the National Academy of Sciences},
  year      = {2013},
  volume    = {110},
  number    = {45},
  pages     = {18052--18057},
  abstract  = {An important tool in information analysis is dimensionality reduction. There are various approaches for large data simplification by scaling its dimensions downthat play a significant role in recognition and classification tasks. The efficiency of dimension reduction tools is measured in terms of memory and computational complexity, which are usually a function of the number of the given data points. Sparse local operators that involve substantially less than quadratic complexity at one end, and faithfulmultiscalemodelswith quadratic cost at the other end,make the design of dimension reduction procedure
a delicate balance between modeling accuracy and efficiency. Here, we combine the benefits of both and propose a low-dimensional multiscale modeling of the data, at a modest computational cost.
The idea is to project the classical multidimensional scaling problem into the data spectral domain extracted from its Laplace-Beltrami operator. There, embedding into a small dimensional Euclidean space is accomplished while optimizing for a small number of coefficients.
We provide a theoretical support and demonstrate that working in the natural eigenspace of the data, one could reduce the process complexity while maintaining the model fidelity. As examples, we efficiently canonize nonrigid shapes by embedding their intrinsic metric into R3, a method often used for matching and classifying almost isometric articulated objects. Finally, we demonstrate the method by exposing the style in which handwritten digits appear in a large collection of images. We also visualize clustering of digits by treating images as feature points that we map to a plane.
flat embedding | distance maps | big data | diffusion geometry},
  comment   = {MDS with what seems to be an extra eigen calc works better on clustering problems with less data than classical MDS. Reason (I think) is that there are fewer coeffs to optimize.},
  file      = {Aflalo13SpecMDS.pdf:Aflalo13SpecMDS.pdf:PDF},
  owner     = {sotterson},
  publisher = {National Acad Sciences},
  timestamp = {2014.06.27},
  url       = {http://www.pnas.org/content/110/45/18052.short},
}

@InProceedings{Singh14incWindDEtrans,
  author    = {A. Singh and D. Willi and N. Chokani and R. S. Abhari},
  title     = {Increasing On-Shore Wind Generated Electricity In {G}ermany?s Transmission Grid},
  booktitle = {Proc. {ASME Turbo Expo 2014}: Turbine Technical Conference and Exposition},
  year      = {2014},
  month     = jun,
  abstract  = {An increase in the penetration of renewables generated electricity has technical and economic impacts on power transmission systems because of the renewables? variable characteristics. However, due to concerns of energy security, operational information of power infrastructure is scarce, making it challenging for policy-makers and independent power producers to assess these systems for the development of new energy projects. This paper presents an analysis of Germany?s power generation and transmission infrastructure using integrated, geographically-indexed production, demand and grid models. The paper assesses the impact of growth of renewables on Germany?s grid in a scenario of slow growth of grid infrastructure to show that the length of transmission lines needing reinforcement increases from 650 km in 2011 to 1090 km in 2020, if Germany?s transmission grid is to keep pace with the increased penetration of renewable energy. Mesoscale model simulations of the weather are used in the year 2020 scenario to assess the economic development of the competing renewables ? wind and solar ? in relation to the available grid capacity. It is shown that if the grid development lags the development of then targeted 35\% renewables portfolio, then 6.5\% of generated power by wind and solar energy will face risk of curtailment.},
  comment   = {Data-sparse curtailment study, maybe useful for upscaling? Simulation of German grid using little real data comes close to curtailment studies that used detailed measurements. Paper came to me from coauthor, N. Chokani, who was a judge for IRPWIND in 2013. Some matlab is available. Some non-intuitive conclusions are reached.

Potential uses
* grid expansion studies where little data is available (Africa, or even Germany?!?(
* somehow use this approach for IWES upscaling algorithm?

Interesting facts
* DE energy plan will require a 50 pct increase in installed wind capacity in 7 years!
* There is ALREADY transmission (congestion?) in DE transmission and distribution grids
* an increase in wind is preferable to PV in several ways noted below

Experiment: predict grid power flows as RES increase w/ varying levels of transmission, using little data

Approach:
* roughly approximate the distribution grid using tesslated regions centered on grid transformers
* include uniform PV panel distribution and wind distribution based on existing farms or future farm locations designated in other studied (strongest wind, I think)
* transmission at boundaries of Germany simulated using scaled power measurements
* storage simulated, not increased
* consumption mostly assumed to be proportional to population in the tesselated region
* compute net flows MATPOWER optimal power flow simulation
-- freeware in Matlab
-- I'm not sure what "optimum" is but must have something to do w/ price
* Unit committment simulated using standard marginal costs
-- How were primary/secondary/tertiary reserves handled?
-- Is only unit committment simulated?
-- perfect forecasts assumed?

Data:
* network topology info from ENTSO-E
* Generator info from "a commercial source"
* wind/solar irradiance data from weather model
* 288 sub-day-length cases simulated using 2011 power data

Conclusions
* wind is most likely to be curtailed
-- mostly in N. DE, far from consumption so transmission constraints matter
-- solar arrives when demand is highest
* wind produces more energy than solar at same capacity
-- 75 pct greater than PV at equal capacities in 2012 even though there was more PV capacity than wind
-- because it works at night
-- note that wind capacity also costs less than solar, currently (I say)
* if DE doesn't expand grid, wind will get curtailed a lot more
* baseload is met by nukes, hydro, and bio
-- even though hydro and bio are dispatchable?
-- Wouldn't regulation be a more profiitable market for them?
* coal and oil plants are displaced by increasing RES but isn't coal displacing natural gas in DE right now?
* wind is the quickest, cheapest way to increase RES
* grid expansion necessary if RES increases more
-- but couldn't curtailment be cheaper, as in that CA Hart&Jacobson study?
* some agreement w/ detailed TSO Dena study, in terms of what lines could be congested w/o grid expansion
* but Ecofys study proposed adding most storage, so I'm not sure there is "agreement"},
  file      = {Singh14incWindDEtrans.pdf:Singh14incWindDEtrans.pdf:PDF},
  groups    = {Read},
  location  = {D{\"u}sseldorf, Germany},
  owner     = {sotterson},
  timestamp = {2014.01.20},
  url       = {http://www.asmeconferences.org/TE2014//},
}

@InBook{Cooke10vinesArise,
  chapter   = {Vines Arise},
  title     = {Vine Copula Method Handbook},
  publisher = {World Scientific},
  year      = {2010},
  author    = {Cooke, R.M., Joe, H. and Aas, K.},
  editor    = {D. Kurowicka and H. Joe},
  abstract  = {An introduction to the main idea of vines as graphical models is presented,
with various notation and graphs for representing vines. The
early history of vines is summarized, together with motivation for their
construction. The relation to compatibility of subsets of marginal distributions
is given to provide some intuition. Important properties and
applications of vines are included.},
  comment   = {vine copula tranform like tensor decomposition? e.g. Cichocki14TensorNetworksBigOpt},
  file      = {Cooke10vinesArise.pdf:Cooke10vinesArise.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.11.24},
  url       = {http://dutiosc.twi.tudelft.nl/~risk/index.php?option=com_docman&task=doc_details&gid=203&&Itemid=13},
}

@Article{Zhang02bayesLineThermRisk,
  author               = {Zhang, Jun and Pu, Jian and Mccalley, J. D. and Stern, H. and Gallus, W. A.},
  title                = {A {Bayes}ian approach for short-term transmission line thermal overload risk assessment},
  journal              = {Power Delivery, IEEE Transactions on},
  year                 = {2002},
  volume               = {17},
  number               = {3},
  pages                = {770--778},
  abstract             = {An on-line conductor thermal overload risk assessment method is presented in this paper. Bayesian time series models are used to model weather conditions along the transmission lines. An estimate of the thermal overload risk is obtained by Monte Carlo (MC) simulation. We predict the thermal overload risk for the next hour based on the current weather conditions and power system operating conditions. The predicted risk of thermal overload is useful for on-line decision making in a stressed operational environment.},
  citeulike-article-id = {1244264},
  comment              = {Hour ahead powerline thermal overload risk with statistical weather extrapolation; intersesting for weather param models and heat equ. plugin * MCMC risk estimate (it's time varying) * Wasn't tested * statistical models are plugged into heat balance equation Wind speed/dir models * speed: don't use normal Weibul b/c measurments have spike at anemometer stall spee -- instead use normal w/ mean at stall speed and a Weibul for speeds greater than stall -- somehow also use logistic regression * angle: Von Mises (VM) wind dir distribution used since it has a range of 0? 2pi. -- use 1\textsuperscript{st} order autoregressive * Also stat models for Ambient and Current temp, solar radiation, * Use temperature regions, assuming line is constant for long distances * assume spatial correlation for weather data},
  file                 = {Zhang02bayesLineThermRisk.pdf:Zhang02bayesLineThermRisk.pdf:PDF;Zhang02bayesLineThermRisk.pdf:Zhang02bayesLineThermRisk.pdf:PDF},
  groups               = {Read},
  keywords             = {line-sag},
  owner                = {sotterson},
  posted-at            = {2007-04-23 05:56:15},
  timestamp            = {2009.02.23},
  url                  = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1022802},
}

@Book{Revelle17psychThryR_eBook,
  title     = {An introduction to psychometric theory with applications in R (eBook)},
  publisher = {Northwestern University},
  year      = {2017},
  author    = {William Revelle},
  abstract  = {An Overview
This page is devoted to teaching others about psychometric theory as well as R. It consists of chapters
of an in progress text as well as various short courses on R.
The e-book is a work in progress. Chapters will appear sporadically. Parts of it are from the dra of a
book being prepared for the Springer series on using R, other parts are just interesting tid-bits that
would not be appropriate as chapters.
It is written in the hope that I can instill in a new generation of psychologists the love for quantitative
methodology imparted to me by reading the popular and then later the scientific texts of Ray Cattell
[Cattell, 1966b] and Hans Eysenck [Eysenck, 1964, Eysenck, 1953, Eysenck, 1965]. Those Penguin and
Pelican paperbacks by Cattell and Eysenck were the first indications that I had that it was possible to
study personality and psychology with a quantitative approach.
My course in psychometric theory, on which much of this book is based, was inspired by a course of the
same name by Warren Norman. The organizational structure of this text owes a great deal to the
structure of Warren's course. Warren introduced me, as well as a generation of graduate students at the
University of Michigan, to the role of theory and measurement in the study of psychology. He
introduced to me to the "bible" of psychometrics: Jum Nunnally's Psychometric Theory [Nunnally,
1967].
The students in my psychometric theory classes over the years, by their continuing questions and
sometimes confusion, have given me the motivation to try to make this text as understandable and
useful as I can. The members of the Society of Multivariate Experimental Psychology, by their
willingness to share cutting (and sometimes bleeding) edge ideas freely and with respect for
alternative interpretations have been a never ending source of new and exciting ideas.
This book would not be possible without the amazing contributions of the R-Core Team and the many
contributers to R and the R-Help listserve.
Lecture notes to accompany these chapters are found in the syllabus for my course on psychometric
theory.},
  comment   = {Has a decent explanation of the relationship between scalar correlation and linear regression coefficient but it's not clear on multivariate regression or partial correlation.  Doesn't cover precision matrix, which is what I was looking for because of the models that handle high dim covaraince with a sparse precision matrix.  Has some good basic/interesting stuff, though:

INTERESTING STUFF
* IQ, SAT and ACT are all transforms of the deviation from the mean. (Table 3.11)
* Correlation Coefficient vs. Regression CoefficientIs
  - just the linear prediction coefficient -- byx in eq. (4.5) -- but normalized by both stx and sty instead of just var_x, where y is the predictand.
  - Looked at the other way:byx = rxy * stdy / stdx
* proof for why correlation coefficient is the cosine between two variables
  - i.e. rxy=cos(angle x and y vectors)
  - see just below eq. 4.11
* Table 4.8: correlation-like meaasures and their relation to linear correlation (Pearson)},
  file      = {:Revelle17psychThryR_eBook.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.23},
  url       = {http://www.personality-project.org/r/book/},
}

@Article{Sherwood13wgtQRmiss,
  author    = {Sherwood, Ben and Wang, Lan and Zhou, Xiao-Hua},
  title     = {Weighted quantile regression for analyzing health care cost data with missing covariates},
  journal   = {Statistics in Medicine},
  year      = {2013},
  volume    = {32},
  number    = {28},
  pages     = {4967--4979},
  issn      = {1097-0258},
  abstract  = {Analysis of health care cost data is often complicated by a high level of skewness, heteroscedastic variances and the presence of missing data. Most of the existing literature on cost data analysis have been focused on modeling the conditional mean. In this paper, we study a weighted quantile regression approach for estimating the conditional quantiles health care cost data with missing covariates. The weighted quantile regression estimator is consistent, unlike the naive estimator, and asymptotically normal. Furthermore, we propose a modified BIC for variable selection in quantile regression when the covariates are missing at random. The quantile regression framework allows us to obtain a more complete picture of the effects of the covariates on the health care cost and is naturally adapted to the skewness and heterogeneity of the cost data. The method is semiparametric in the sense that it does not require to specify the likelihood function for the random error or the covariates. We investigate the weighted quantile regression procedure and the modified BIC via extensive simulations. We illustrate the application by analyzing a real data set from a health care cost study. Copyright 2013 John Wiley & Sons, Ltd.},
  comment   = {How to handle missing data in linear quantile regression. Local linear too, I suppose.},
  doi       = {10.1002/sim.5883},
  file      = {Sherwood13wgtQRmiss.pdf:Sherwood13wgtQRmiss.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_2},
  keywords  = {health care cost data, missing data, inverse probability weighting, quantile regression},
  owner     = {sotterson},
  timestamp = {2014.03.03},
}

@Article{Li09predInfSuffDimRed,
  author    = {Li, Lexin},
  title     = {Exploiting predictor domain information in sufficient dimension reduction},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {2009},
  volume    = {53},
  number    = {7},
  pages     = {2665--2672},
  month     = may,
  abstract  = {Analysis of high-dimensional data is becoming the norm in a variety of scientific studies and dimension reduction methods are widely employed. As the predictor domain knowledge is often available, it is useful to incorporate such domain information into dimension reduction and subsequent model formulation. Existing solutions such as simple average, principal components analysis and partial least squares cannot assure preservation of full regression information when reducing the dimension. In this article we investigate sufficient dimension reduction strategies that can retain full regression information meanwhile utilizing prior domain knowledge. Both simulations and a real data analysis demonstrate that the new methods are effective and often superior than the existing solutions.},
  comment   = {Supervised dimension reduction works better than unsupervised in this case; SIR sufficient dimension reduction might be best

* This is supervised dimension reduction, where you consider the regression target, unlike e.g. PCA, which is unsupervised)

Performance of various dim reduction techniques for prediction (classfication, I think):
* I think the fact that inputs were "grouped" is significant in this problem, but I haven't read carefully enough to know for sure.
* SDR is seen to achieve the smallest mis-classification
-- SDR: sufficient dimension reduction
-- kind that worked was "grouped SIR"
-- SIR: sliced inverse regression
* PLS performs better than PCA,
* PCA slightly better than simple average
* clear overfitting without dimension reduction

Possible uses
* use for lagged velocity basis coeffs?
* alternatives compared consider basis coeffs w/ and w/o grouping
* how to deal with 1K ensembles

Note: not all supervised dim reduction strategies were tested, eg. regression that preserves input/output mutual information (in this bib file somewhere)},
  file      = {Li09predInfSuffDimRed.pdf:Li09predInfSuffDimRed.pdf:PDF},
  groups    = {Ensemble, doReadNonWPV_1},
  owner     = {scot},
  timestamp = {2010.08.31},
  url       = {http://ideas.repec.org/a/eee/csdana/v53y2009i7p2665-2672.html},
}

@Article{Chandola09anomalyDetSurv,
  author    = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  title     = {Anomaly detection: A survey},
  journal   = {ACM computing surveys (CSUR)},
  year      = {2009},
  volume    = {41},
  number    = {3},
  pages     = {15},
  abstract  = {Anomaly detection is an important problem that has been researched within diverse research areas
and application domains. Many anomaly detection techniques have been specifically developed
for certain application domains, while others are more generic. This survey tries to provide a
structured and comprehensive overview of the research on anomaly detection. We have grouped
existing techniques into different categories based on the underlying approach adopted by each
technique. For each category we have identified key assumptions, which are used by the techniques
to differentiate between normal and anomalous behavior. When applying a given technique to a
particular domain, these assumptions can be used as guidelines to assess the effectiveness of the
technique in that domain. For each category, we provide a basic anomaly detection technique, and
then show how the different existing techniques in that category are variants of the basic tech-
nique. This template provides an easier and succinct understanding of the techniques belonging
to each category. Further, for each category, we identify the advantages and disadvantages of the
techniques in that category. We also provide a discussion on the computational complexity of the
techniques since it is an important issue in real application domains. We hope that this survey
will provide a better understanding of the different directions in which research has been done on
this topic, and how techniques developed in one area can be applied in domains for which they
were not intended to begin with.
Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications?
Data Mining
General Terms: Algorithms
Additional Key Words and Phrases: Anomaly Detection, Outlier Detection},
  comment   = {Huge num. cites on GS.  For ModernWindABS},
  file      = {Chandola09anomalyDetSurv.pdf:Chandola09anomalyDetSurv.pdf:PDF},
  publisher = {ACM},
}

@Article{Goldstein16cmprUnsupNovDetMV,
  author    = {Goldstein, Markus and Uchida, Seiichi},
  title     = {A Comparative Evaluation of Unsupervised Anomaly Detection Algorithms for Multivariate Data},
  journal   = {PloS one},
  year      = {2016},
  volume    = {11},
  number    = {4},
  pages     = {e0152173},
  month     = {apr},
  abstract  = {Anomaly detection is the process of identifying unexpected items or events in datasets, which differ from the norm. In contrast to standard classification tasks, anomaly detection is often applied on unlabeled data, taking only the internal structure of the dataset into account. This challenge is known as unsupervised anomaly detection and is addressed in many practical applications, for example in network intrusion detection, fraud detection as well as in the life science and medical domain. Dozens of algorithms have been proposed in this area, but unfortunately the research community still lacks a comparative universal evaluation as well as common publicly available datasets. These shortcomings are addressed in this study, where 19 different unsupervised anomaly detection algorithms are evaluated on 10 different datasets from multiple application domains. By publishing the source code and the datasets, this paper aims to be a new well-funded basis for unsupervised anomaly detection research. Additionally, this evaluation reveals the strengths and weaknesses of the different approaches for the first time. Besides the anomaly detection performance, computational effort, the impact of parameter settings as well as the global/local anomaly detection behavior is outlined. As a conclusion, we give an advise on algorithm selection for typical real-world tasks.},
  comment   = {For ModernWIndABS},
  doi       = {10.1371/journal.pone.0152173},
  editor    = {Dongxiao Zhu},
  file      = {Goldstein16cmprUnsupNovDetMV.pdf:Goldstein16cmprUnsupNovDetMV.pdf:PDF},
  owner     = {sotterson},
  publisher = {Public Library of Science},
  timestamp = {2016.12.17},
  url       = {http://dx.doi.org/10.1371/journal.pone.0152173},
}

@Article{Thorarinsdottir13highDimCalRankHist,
  author    = {Thorarinsdottir, Thordis L and Scheuerer, Michael and Heinz, Christopher},
  title     = {Assessing the calibration of high-dimensional ensemble forecasts using rank histograms},
  journal   = {arXiv},
  year      = {2013},
  abstract  = {Any decision making process that relies on a probabilistic forecast of future events necessarily
requires a calibrated forecast. This paper proposes new methods for empirically assessing
forecast calibration in a multivariate setting where the probabilistic forecast is given by an
ensemble of equally probable forecast scenarios. Multivariate properties are mapped to a single
dimension through a pre-rank function and the calibration is subsequently assessed visually
through a histogram of the ranks of the observation?s pre-ranks. Average ranking assigns a
pre-rank based on the average univariate rank while band depth ranking employs the concept
of functional band depth where the centrality of the observation within the forecast ensemble
is assessed. Several simulation examples and a case study of temperature forecast trajectories
at Berlin Tegel Airport in Germany demonstrate that both multivariate ranking methods can
successfully detect various sources of miscalibration and scale efficiently to high dimensional
settings.
Keywords: average rank; band depth; forecast trajectory; forecast verification; modified band
depth; multivariate forecast},
  comment   = {Multivariate ensemble ranking reduced to 1D. Checks for deviations from correlation structure, maybe temporal too, but final cal is done visually? Could be an improvement upon energy function metric (Pinson12scenQualWind)},
  file      = {Thorarinsdottir13highDimCalRankHist.pdf:Thorarinsdottir13highDimCalRankHist.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.18},
  url       = {http://arxiv.org/abs/1310.0236},
}

@Article{Sjoestrand12spasmMatlab,
  author    = {Sj{\"o}strand, Karl and Clemmensen, Line Harder and Larsen, Rasmus and Ersb{\o}ll, Bjarne},
  title     = {Spasm: A matlab toolbox for sparse statistical modeling},
  journal   = {Journal of Statistical Software Accepted for publication},
  year      = {2012},
  abstract  = {Applications in biotechnology such as gene expression analysis and image processing
have led to a tremendous development of statistical methods with emphasis on reliable
solutions to severely underdetermined systems, and interpretation, solutions where the
surplus of inputs have been reduced to a concise model. At the core of this development
are methods which augments the standard linear models for regression, classication and
decomposition such that sparse solutions are obtained. This toolbox aims at making
public carefully implemented and well-tested variants of the most popular such methods
for the Matlab programming environment. The toolbox builds on code made public in
2005 and which has since been used in several studies.
Keywords: Least angle regression, LASSO, elastic net, sparse principal component analysis,
sparse discriminant analysis, Matlab.},
  comment   = {Matlab for several classic sparse algorithms

Sparse:
* PCA (Zou06SparsePCA), sp
* discrim analysis (LDA?)
* LARS
* elastic net

Code:
* download: http://www.imm.dtu.dk/projects/spasm/},
  file      = {Sjoestrand12spasmMatlab.pdf:Sjoestrand12spasmMatlab.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.10.23},
  url       = {http://www.imm.dtu.dk/projects/spasm/},
}

@Article{Honaker10missValTseries,
  author    = {Honaker, James and King, Gary},
  title     = {What to Do about Missing Values in Time-Series Cross-Section Data},
  journal   = {American Journal of Political Science},
  year      = {2010},
  volume    = {54},
  pages     = {561--581},
  month     = apr,
  abstract  = {Applications of modern methods for analyzing data with missing values, based primarily on multiple imputation, have in the last half-decade become common in American politics and political behavior. Scholars in this subset of political science have thus increasingly avoided the biases and inefficiencies caused by ad hoc methods like listwise deletion and best guess imputation. However, researchers in much of comparative politics and international relations, and others with similar data, have been unable to do the same because the best available imputation methods work poorly with the time-series cross-section data structures common in these fields. We attempt to rectify this situation with three related developments. First, we build a multiple imputation model that allows smooth time trends, shifts across cross-sectional units, and correlations over time and space, resulting in far more accurate imputations. Second, we enable analysts to incorporate knowledge from area studies experts via priors on individual missing cell values, rather than on difficult-to-interpret model parameters. Third, because these tasks could not be accomplished within existing imputation algorithms, in that they cannot handle as many variables as needed even in the simpler cross-sectional data for which they were designed, we also develop a new algorithm that substantially expands the range of computationally feasible data types and sizes for which multiple imputation can be used. These developments also make it possible to implement the methods introduced here in freely available open source software that is considerably more reliable than existing algorithms.},
  comment   = {Time series missing data, by authors of R package, "Amelia II"

Note that a coupula technique is generally faster and better (except not always for binary inputs): Hollenbach14missDatImpCopula


Is multiple imputation based:
* EM handles missingness; bootstrap handles fill-in uncertainty:
1.) Bootstrap m samples of n points each (n is the original sample size).
2.) Use EM to estimate m normal distributions (so linear dependence assumed).
3.) Estimate missing values from the estimated normal distributions.

* learns statistical model of cross variable dependence
-- Temporal continuity: Very complex lagged (and lead) basis function model,
-- can include interactions.
-- Lot's of parameters but their alg. (EMB) is supposed to handle it.
-- can include predicted variable while imputing (but not for later learning)
-- linear normal dependence is assumed
---- WARNING: To avoid bias, imputation model s/ contain as much info as analysis model
---- Does this mean just that you don't lose information??
---- Still OK to train a complex MLP on this stuff??
---- the say: For say that the risk of imputation errors smaller than not using the data (p. 562)
* runs the model to produce M guesses at missing values (imputations)
-- Entire data set is repeated M times
-- learner is trained on the M impuations
-- Use EM to fill missing values form imputations, each the same size as the data.
-- does a kind of bootstrapping
-- easy to parallelize
* better than replacing missing data w/ single estimates (final estimate confidence too high)
* constraints are handled inside the EM algorithm by placing priors on the estimated parameters. This is more principled than some kinda truncation, and maybe (I say) gives better results.

Boundary handling
* called logical bounds
* not explained here; kind of explained here: Drechsler11multiImputeSlides

* if random draw of normal dist produces an outabounds variable, draw another one.
* can draw a lot (so there's a param specifying how many times to do this)
* can bias the results, since will force other missing variables in a vector with an out of bounds value
Advantages
* standard multiple imputation works for 30-40 variables
* this has worked for up to 240 variables, 32K samples

Disadvantages
* at least the standard algorithm has no idea of test/train sets
-- always uses all the data
-- for forecasting, this would be the future, and therefore cheating
-- but could get cov matrix from standard alg. on the training set, and then use PCA method as in Oba03pcaImp to fill test set missing values.
* assumes a normal distribution (although they say this works in practice)

* broundaries handled by redrawing:
-- may have to redraw a lot if meeting bound is improbable
-- continual redrawing can bias the distribution

Could an SVD trick further improve this? See Asari05rgrsnSvdRidgeInfo
Could do Copula: Hollenbach14missDatImpCopula or Ding16gaussCopulaMissDatEM},
  doi       = {10.1111/j.1540-5907.2010.00447.x},
  file      = {Honaker10missValTseries.pdf:Honaker10missValTseries.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  timestamp = {2010.09.09},
}

@Article{Fearnhead12semiAutoABCconstr,
  author    = {Fearnhead, Paul and Prangle, Dennis},
  title     = {Constructing summary statistics for approximate {Bayes}ian computation: semi-automatic approximate {Bayes}ian computation},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year      = {2012},
  volume    = {74},
  number    = {3},
  pages     = {419--474},
  abstract  = {Approximate Bayesian Computation (ABC)
methods are a family of algorithms for
`likelihood-free' Bayesian inference.
The domain of use is models where numerical
evaluation of the likelihood is impossible or
impractical, but from which data can easily be
simulated.
This includes many situations of inference for
complex stochastic models e.g. population genetics,
systems biology and infectious disease models.
ABC operates by simulating data Ysim from the
model of interest for many parameter values and
constructing an approximation to the posterior
from those values for which the associated Ysim
closely matches the observations Yobs.
A simple ABC algorithm using rejection sampling is
shown below. The literature contains more efficient
ABC algorithms based},
  comment   = {An Approximate {Bayes}ian Computation paper that does some kinda feature selection to pick the best summary stats. Is much more accurate than ad hoc methods in the literature. Combined w/ auto-bandwidth selection, ABC might be usable.

ABC is good when don't know the true error distribution. Maybe good for forecasting. Could be another way of evaluating probabilistic forecasts.

Was originally published here: Fearnhead10semiAutoABC

Paper for this on order from Subito

A poster on this topic is attached.

Some intro slides by Prangle are here: Prangle11sumStatSeqABC},
  file      = {Slides:Fearnhead12semiAutoABCconstr_slides.pdf:PDF;Fearnhead12semiAutoABCconstr.pdf:Fearnhead12semiAutoABCconstr.pdf:PDF},
  groups    = {Ensemble, PointDerived, Test, doReadNonWPV_2},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2013.10.04},
}

@Article{Blum13dimRedABC,
  author    = {Blum, MGB and Nunes, MA and Prangle, Dennis and Sisson, SA},
  title     = {A comparative review of dimension reduction methods in approximate {Bayes}ian computation},
  journal   = {Statistical Science},
  year      = {2013},
  volume    = {28},
  number    = {2},
  pages     = {189--208},
  abstract  = {Approximate Bayesian computation (ABC) methods make use of
comparisons between simulated and observed summary statistics to overcome
the problem of computationally intractable likelihood functions. As the
practical implementation of ABC requires computations based on vectors of
summary statistics, rather than full data sets, a central question is how to
derive low-dimensional summary statistics from the observed data with minimal
loss of information. In this article we provide a comprehensive review
and comparison of the performance of the principal methods of dimension
reduction proposed in the ABC literature. The methods are split into three
nonmutually exclusive classes consisting of best subset selection methods,
projection techniques and regularization. In addition, we introduce two new
methods of dimension reduction. The first is a best subset selection method
based on Akaike and Bayesian information criteria, and the second uses ridge
regression as a regularization procedure. We illustrate the performance of
these dimension reduction techniques through the analysis of three challenging
models and data sets.

Key words and phrases: Approximate Bayesian computation, dimension reduction,
likelihood-free inference, regularization, variable selection.},
  comment   = {Possibly good for ensemble analog search. Needed b/c ABC has probs w/ high dimensions (in some article I'\ve bibtexed). There are probably helpers for this method in other articles I've bibtexed, including an R implementation of ABC.

It seems like the feature selection and dimension reduction transforms used here could be improved upon by other machine learning techniques.},
  file      = {Blum13dimRedABC.pdf:Blum13dimRedABC.pdf:PDF},
  groups    = {Ensemble, PointDerived, doReadNonWPV_2},
  owner     = {sotterson},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2013.10.04},
  url       = {http://arxiv.org/abs/1202.3819},
}

@Article{Blum10nonlinApproxBayesComp,
  author    = {Blum, Michael G. and Fran\c{c}ois, Olivier},
  title     = {Non-linear regression models for Approximate {Bayes}ian Computation},
  journal   = {Statistics and Computing},
  year      = {2010},
  volume    = {20},
  number    = {1},
  pages     = {63--73},
  issn      = {0960-3174},
  abstract  = {Approximate Bayesian inference on the basis of
summary statistics is well-suited to complex problems for
which the likelihood is either mathematically or computa-
tionally intractable. However the methods that use rejection
suffer from the curse of dimensionality when the number of
summary statistics is increased. Here we propose a machine-
learning approach to the estimation of the posterior density
by introducing two innovations. The new method ?ts a non-
linear conditional heteroscedastic regression of the parame-
ter on the summary statistics, and then adaptively improves
estimation using importance sampling. The new algorithm
is compared to the state-of-the-art approximate Bayesian
methods, and achieves considerable reduction of the com-
putational burden in two examples of inference in statistical
genetics and in a queueing model.

Keywords Likelihood-free inference ? Curse of
dimensionality ? Feed forward neural networks ?
Heteroscedasticity ? Coalescent models ? Approximate
Bayesian computation ? Conditional density estimation ?
Implicit statistical models ? Importance sampling ?
Non-linear regression ? Indirect inference},
  comment   = {Efficient Bayesian statistical computations w/o distribution estimation: has R package
* Is a general monte-carlo-type technique for doing any Bayesian thing, I think.
* "approximate Bayesian computation" (ABC): do bayesian inference to est. params, etc.
* efficient
-- instead of the usual distribution estimation, works on summary statistics, like mean, variance
-- and maybe quantiles?
-- paper's trick is to do fewer montecarlo runs using neural net distribution parameter correction

Slides are a nice overview from: www.ceremade.dauphine.fr/~xian/ABCOF.pdf


POSSIBLE USES I CAN THINK OF
- is lately being used for dynamical systems (ABC wikipedial)
- good for high dimensions
- spinning reserve optimization
- better use of quantile regression output?
- optimal bidding (given forecast distribution)
- better/adaptive forecasting?
- non-linear twist in this paper makes ABC much more efficient
- slides say there's an R package to be available soon in CRAN
- analog ensemble forecastion (see Prangle11sumStatSeqABC)},
  doi       = {10.1007/s11222-009-9116-0},
  file      = {Paper:Blum10nonlinApproxBayesComp.pdf:PDF;Slides:Blum10nonlinApproxBayesComp_slides.pdf:PDF},
  groups    = {Ensemble, PointDerived},
  location  = {Hingham, MA, USA},
  ncite     = {110},
  owner     = {scot},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2010.07.27},
}

@Article{Bessa17imprvUnderstandUncertFrcstPow,
  author    = {Ricardo Bessa and Corinna Möhrlen and Vanessa Fundel and Malte Siefert and Jethro Browell and Sebastian Haglund El Gaidi and Bri-Mathias Hodge and Umit Cali and George Kariniotakis},
  title     = {Towards Improved Understanding of the Applicability of Uncertainty Forecasts in the Electric Power Industry},
  journal   = {Energies},
  year      = {2017},
  volume    = {10},
  number    = {9, 1402},
  month     = {sep},
  abstract  = {Around the world wind energy is starting to become a major energy provider in electricity markets, as well as participating in ancillary services markets to help maintain grid stability. The reliability of system operations and smooth integration of wind energy into electricity markets has been strongly supported by years of improvement in weather and wind power forecasting systems. Deterministic forecasts are still predominant in utility practice although truly optimal decisions and risk hedging are only possible with the adoption of uncertainty forecasts. One of the main barriers for the industrial adoption of uncertainty forecasts is the lack of understanding of its information content (e.g., its physical and statistical modeling) and standardization of uncertainty forecast products, which frequently leads to mistrust towards uncertainty forecasts and their applicability in practice. This paper aims at improving this understanding by establishing a common terminology and reviewing the methods to determine, estimate, and communicate the uncertainty in weather and wind power forecasts. This conceptual analysis of the state of the art highlights that: (i) end-users should start to look at the forecast’s properties in order to map different uncertainty representations to specific wind energy-related user requirements; (ii) a multidisciplinary team is required to foster the integration of stochastic methods in the industry sector. A set of recommendations for standardization and improved training of operators are provided along with examples of best practices.

Keywords: wind energy; uncertainty; decision-making; quantiles; ensembles; forecast; statistics; weather},
  comment   = {Use of forecasts paper.  Maybe it has stuff from Eweline.},
  doi       = {10.3390/en10091402},
  file      = {:Bessa17imprvUnderstandUncertFrcstPow.pdf:PDF},
  keywords  = {Keywords: wind energy; uncertainty; decision-making; quantiles; ensembles; forecast; statistics; weather},
  publisher = {{MDPI} {AG}},
}

@Article{Battaglia18biasDpLrnGraphNet,
  author      = {Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andrew Ballard and Justin Gilmer and George Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
  title       = {Relational inductive biases, deep learning, and graph networks},
  abstract    = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  comment     = {Deep Learning doesn't represent the world; this paper talks about graph networdks as a way to represent reality and learn more efficiently, although this approach can't easily describe recursion, conditional iteration, or control flow.  The authors have a new graph networks library.

* can't easily describe recursion, conditional iteration, or control flow :
  (from https://www.zdnet.com/article/google-ponders-the-shortcomings-of-machine-learning)
* Graph Nets Library (also has a nice intro on Github):
  https://github.com/deepmind/graph_nets


},
  date        = {2018-06-04},
  eprint      = {http://arxiv.org/abs/1806.01261v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Battaglia18biasDpLrnGraphNet.pdf:PDF},
  keywords    = {cs.LG, cs.AI, stat.ML},
  url         = {https://arxiv.org/abs/1806.01261},
}

@Article{May08varSelpartMutInfo,
  author    = {May,, Robert J. and Maier,, Holger R. and Dandy,, Graeme C. and Fernando,, T.M.K. Gayani},
  title     = {Non-linear variable selection for artificial neural networks using partial mutual information},
  journal   = {Environ. Model. Software},
  year      = {2008},
  volume    = {23},
  number    = {10-11},
  pages     = {1312--1326},
  issn      = {1364-8152},
  abstract  = {Artificial neural networks (ANNs) have been widely used to model environmental processes. The ability of ANN models to accurately represent the complex, non-linear behaviour of relatively poorly understood processes makes them highly suited to this task. However, the selection of an appropriate set of input variables during ANN development is important for obtaining high-quality models. This can be a difficult task when considering that many input variable selection (IVS) techniques fail to perform adequately due to an underlying assumption of linearity, or due to redundancy within the available data. This paper focuses on a recently proposed IVS algorithm, based on estimation of partial mutual information (PMI), which can overcome both of these issues and is considered highly suited to the development of ANN models. In particular, this paper addresses the computational efficiency and accuracy of the algorithm via the formulation and evaluation of alternative techniques for determining the significance of PMI values estimated during selection. Furthermore, this paper presents a rigorous assessment of the PMI-based algorithm and clearly demonstrates the superior performance of this non-linear IVS technique in comparison to linear correlation-based techniques.},
  comment   = {Kernel regression based PMI:
 (+) has a stopping criteria (Akaike Information Criteria)
 (-) higher complexity
 (than Frenzel07partMutInfo)
(says http://stats.stackexchange.com/questions/35089/feature-selection-using-mutual-information-in-matlab)
 than

But I don't understand why it computes TWO residuals.

* original partial mutual information paper here: Frenzel07partMutInfo
* applied to forecasting here: May08partMutInfoWaterFrcst (where it works on a large num. of variables (500))
* another PMI feat sel paper: Dalto14NshrtWindFrcstNNpmi
* author has a general featsel review in May11revVarSelNN
* kernel regression inside of PMI loop not explained but is probably a GRNN, as in May08partMutInfoWaterFrcst
 - this is kind of like a wrapper technique but he calls it a "filter" in May11revVarSelNN
* Does wrapper PMI greedy loop avoid interatction missing problems of pure PMI loop
  - problems are listed in Lizier12compNetsTransEnt

But why compute two residuals?
* residual of  prediction using current features, u, makes sense: it's what X doesn't know about y
  - if I(u,Cs) is high then candidate Cs can add information about Y that wasn't already in X
* but they computer I(u,v), where v is the residual of Cs predicted with X
  - say that v is the info in Cs not already in X, which is true, BUT
  - redundant info relevant about Y that was already in X would not be in the residual u
  ==> Cs would not be selected due to low I(u,Cs), if that was computed
* seems computaton of u doesn't add anything
* only thing I can think of is if wanted only scalar MI calc and Cs was multivariate, so that
   I(u,Cs) would be multivariate
  - still could do that w/ Kraskov MI
  - but would have to account for dimension-dependent bias (add a dimension w/ noise in it when computing MI lower dim candidates?)
* but see below...

In fact, why computer ANY residuals?
* can calc multivariate PMI in  Frenzel07partMutInfo and Lizier14JIDTinfoToolkit
* this is kind of done in Kugiumtzis13directCplngPMI
* would make it a filter that avoids regression in the middle
* lower computational cost if PMI calc is fast
  - and it's Kraskov-style, so I think it is
  - PMI calc high dim only if there are really lot of relevant non-redundant features
* Kraskov dimension-bias could still be a problem b/c candidate feats would have different dims
* but see below...

Why predict candidate feature with previously selected. feats?
Paper says because it captures info not in already selected features.  BUT if Cs had a lot of redundancy w/ previous X, then it would not be any better at predicting Y than X, and therefore, I(u,Cs) would be low.  Redundancy is ALREADY captured in u=Y|X regression?
Possible explanation: if regression alg. Is bad, selected feats have high MI w/ target and also w/ candidate, then candidate MI w/ resid. would be high even though its redundant.  This method avoids that possibility.  Also see Vlachos10nonUnifStSpcMI: explains expression & makes bias argument (different MI algorithm)},
  doi       = {10.1016/j.envsoft.2008.03.007},
  file      = {May08varSelpartMutInfo.pdf:May08varSelpartMutInfo.pdf:PDF;May08varSelpartMutInfo.pdf:May08varSelpartMutInfo.pdf:PDF},
  location  = {Amsterdam, The Netherlands, The Netherlands},
  owner     = {sotterson},
  publisher = {Elsevier Science Publishers B. V.},
  timestamp = {2009.03.16},
}

@Article{Hanel08runoffPartialMutInfoNeural,
  author    = {M. Hanel and P. Maca},
  title     = {Runoff prediction with partial mutual information as tool for identification and construction of input variables for artificial neural networks},
  journal   = {Geophysical Research Abstracts},
  year      = {2008},
  volume    = {10},
  abstract  = {Artificial neural networks are very flexible and often used tools for runoff modeling. They can combine any reasonable set of input variables and learn the way how to map it to output variable(s). Among other parameters (in particular parameters that influence speed and quality of learning process) there are two crucial things that have to be chosen: the architecture of the network and the set of input variables. The increasing number of neurons leads to greater flexibility of the network but can cause tendency of overfitting the data, similarly increasing number of inputs brings more information into the model but together with noise. Finally, more complex structure of neural network costs more computational time. The key issue is then to choose the smallest architecture that is able to represent the process and the smallest set of input variables that includes as much necessary information for reliable prediction as possible. In the presented study the main attention was paid to input variable selection. Because neural networks are often chosen because of their ability to represent nonlinear relationships between variables the measure of nonlinear dependence called partial mutual information (PMI) was applied. PMI is based on entropy and express the reduction of uncertainty in one variable due to the knowledge of other variable. Compared to basic mutual information PMI counts for dependence between the variable of interest and already chosen variables as well and thus prevents from including variables that bring no extra information into input data set. Significance of input variable is tested against hypothesis of no dependence by standard bootstrap technique. The method was applied to prediction of runoff in experimental catchment Modrava III in the Czech republic for 1 and 12 hours leading times. Runoff was predicted using hourly data on precipitation and past runoff. The aim of the study was to identify set of input variables (time lags of runoff and precipitation) that significantly contains information about predicted runoff. It was found that the variables of significance vary a lot between different runoff events. Therefore the mean dependence between PMI and time lag was explored for both precipitation and runoff and both leading times. Resulting relation between time lag and PMI was parametrized and used for construction of new input variables. In this way the information about past is summarized in few new variables instead of including all of these variables into input data set. The results shows that comparing the neural networks with same number of inputs, the network that uses input constructed with PMI produces more stable output. As a part of this study freely available software tools for calculation of PMI (PMI Estimator) and application of neural networks (ANN) were developed.},
  comment   = {I got the code from Martin Hanel: /Users/sotterson/proj/regime/contrib/pmi He says it implements the method in: Bowden05inputNeuralWater},
  file      = {Hanel08runoffPartialMutInfoNeural.pdf:Hanel08runoffPartialMutInfoNeural.pdf:PDF;Hanel08runoffPartialMutInfoNeural.pdf:Hanel08runoffPartialMutInfoNeural.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.03.26},
  url       = {http://www.cosis.net/abstracts/EGU2008/05387/EGU2008-A-05387.pdf},
}

@InProceedings{Tao14WindPowPredPattrnDeepLrn,
  author    = {Y. Tao and H. Chen and C. Qiu},
  title     = {Wind power prediction and pattern feature based on deep learning method},
  booktitle = {Proc. IEEE PES Asia-Pacific Power and Energy Engineering Conf. (APPEEC)},
  year      = {2014},
  pages     = {1--4},
  month     = dec,
  abstract  = {As a type of clean and renewable energy source, wind power is being widely used all around the world. However, owing to the uncertainty and instability of the wind power, it is essential to build an accurate prediction model for wind power. In order to build the model, the hidden rules of wind power patterns is extracted by historical data from wind farm based on deep belief network (DBN). Several experiments are conducted to compare different solutions to DBN. The experimental results show that prediction errors are significantly reduced using the proposed technique. Depth learning theory has a strong scientific and engineering practical value in the field of wind power prediction.},
  comment   = {Deep learning wind power forecast.},
  doi       = {10.1109/APPEEC.2014.7066166},
  file      = {:Tao14WindPowPredPattrnDeepLrn.pdf:PDF},
  issn      = {2157-4839},
  keywords  = {belief networks, learning (artificial intelligence), load forecasting, power engineering computing, wind power plants, DBN, accurate prediction model, deep belief network, depth learning theory, historical data, prediction errors, renewable energy source, wind farm, wind power patterns, wind power prediction, Artificial neural networks, Predictive models, Renewable energy sources, Training, Wind power generation, Wind speed, Boltzmann machine, deep belief network, neural network, pattern features, wind power prediction},
}

@Article{Ding16condMultivarTdist,
  author    = {Ding, Peng},
  title     = {On the conditional distribution of the multivariate t distribution},
  journal   = {The American Statistician},
  year      = {2016},
  volume    = {70},
  number    = {3},
  pages     = {293--295},
  abstract  = {As alternatives to the normal distributions, t distributions are widely applied in robust analysis for data with outliers or heavy tails. The properties of the multivariate t distribution are well documented in Kotz and Nadarajah's book, which, however, states a wrong conclusion about the conditional distribution of the multivariate t distribution. Previous literature has recognized that the conditional distribution of the multivariate t distribution also follows the multivariate t distribution. We provide an intuitive proof without directly manipulating the complicated density function of the multivariate t distribution.

Keywords: Bayes? theorem, Data augmentation, Mahalanobis distance, Normal mixture, Representation},
  comment   = {Correction for multivariate T-dist calc.

Also see: Roth13multivarTdist},
  file      = {Ding16condMultivarTdist.pdf:Ding16condMultivarTdist.pdf:PDF},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2017.03.20},
  url       = {http://amstat.tandfonline.com/doi/abs/10.1080/00031305.2016.1164756},
}

@BOOK{Burton2001windHandBk,
  Title                    = {Wind Energy Handbook: Handbook},
  Publisher                = {John Wiley and Sons},
  Year                     = {2001},
  Author                   = {Tony Burton and David Sharpe and Nick Jenkins and Ervin Bossanyi},
  Abstract                 = {As environmental concerns have focused attention on the generation of electricity from clean and renewable sources wind energy has become the world's fastest growing energy source. The Wind Energy Handbook draws on the authors' collective industrial and academic experience to highlight the interdisciplinary nature of wind energy research and provide a comprehensive treatment of wind energy for electricity generation. Features include: ndash; An authoritative overview of wind turbine technology and wind farm design and development ndash; In-depth examination of the aerodynamics and performance of land-based horizontal axis wind turbines ndash; A survey of alternative machine architectures and an introduction to the design of the key components ndash; Description of the wind resource in terms of wind speed frequency distribution and the structure of turbulence ndash; Coverage of site wind speed prediction techniques ndash; Discussions of wind farm siting constraints and the assessment of environmental impact ndash; The integration of wind farms into the electrical power system, including power quality and system stability ndash; Functions of wind turbine controllers and design and analysis techniques With coverage ranging from practical concerns about component design to the economic importance of sustainable power sources, the Wind Energy Handbook will be an asset to engineers, turbine designers, wind energy consultants and graduate engineering students.},
  Owner                    = {sotterson},
  Timestamp                = {2008.07.03},
  URL                      = {http://books.google.com/books?id=4UYm893y-34C}
}

@Article{Siemens14sennFrcstsRES,
  author    = {Siemens},
  title     = {A Better Forecast for Renewable Energy Sources},
  journal   = {Pictures of the Future: The Magazine for Research and Innovation},
  year      = {2014},
  abstract  = {As increasing amounts of wind and solar power enter transmission networks, it is not just demand for energy that is fluctuating, but also its supply. Siemens has created neural network-based forecasting software that predicts fluctuations, thus helping to increase the efficiency of electricity markets.
Everything was simpler in the past. Power plants were distributed throughout countries and their output was adjusted according to energy demand. Power plants used calendars and weather forecasts, among other means, to predict the power needs of regions and large production plants.
Today, the situation is more complicated. Depending on the weather, wind farms and solar parks produce varying amounts of electricity, and conventional power plants must make up for fluctuations. The greater the share of fluctuating renewable energy sources, the more difficult it becomes to manage power supply ? an issue that affects power suppliers and grid operators alike.},
  comment   = {A Siemens tools seems to be doing solar/wind/netdemand/reserve/price forecasts using a "deep learning-related" neural net (SENN).  See pdf, which is a printout of my Evernote notes.

Siemens is using a generic NN system (SENN) to do many times of forecasts
  First developed over 20 years ago, used to forecast electricity prices since 2005
has it been modernized (this article seems to call it "deep learning-related")?
Danish offshore wind farm accuracy is "7.2%" 3 days ahead -- seems very good!
50 MW S. African solar farm accuracy is "7%" at 5 days ahead -- also seems very good!
will also do a dust forecast, telling operators when to clean PV panels
Seem to be doing a net demand forecast
Need for balancing energy
Are now predicting transmission loss ==> must be net demand

More about Siemens AI and RES: Machine Learning: Getting Machines to Mimic Intuition
Evernote Notes:
http://www.evernote.com/l/AA1xHg70hUhJ6LbsMQZL-RqecWmtlxfq-cU/  (this article)
http://www.evernote.com/l/AA05QepHHVlLMJiB6pxDMLuP_nI41T8nlMk/  (related)},
  file      = {Siemens14sennFrcstsRES.pdf:Siemens14sennFrcstsRES.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.26},
  url       = {https://www.siemens.com/innovation/en/home/pictures-of-the-future/energy-and-efficiency/sustainable-power-generation-neural-networks.html},
}

@Article{Biller09copulaMultiVar,
  author    = {Biller, Bahar},
  title     = {Copula-Based Multivariate Input Models for Stochastic Simulation},
  journal   = {Operations Research},
  year      = {2009},
  volume    = {57},
  number    = {4},
  pages     = {878--892},
  abstract  = {As large-scale discrete-event stochastic simulation becomes a tool that is used routinely for the design and analysis of stochastic systems, the need for input-modeling support with the ability to represent complex interactions and interdependencies among the components of multivariate time-series input processes is more critical than ever. Motivated by the failure of independent and identically distributed random variables to represent such input processes, a comprehensive framework called Vector-Autoregressive-To-Anything (VARTA) has been introduced for multivariate time-series input modeling. Despite its flexibility in capturing a wide variety of distributional shapes, we show that VARTA falls short in representing dependence structures that arise in situations where extreme component realizations occur together. We demonstrate that it is possible to extend VARTA to work for such dependence structures via the use of the copula theory, which has been used primarily for random vectors in the simulation input-modeling literature, for multivariate time-series input modeling. We show that our copula-based multivariate time-series input model, which includes VARTA as a special case, allows the development of statistically valid fitting and fast sampling algorithms well suited for driving large-scale stochastic simulations.},
  comment   = {spinning reserves, works better than VARTA, which I think is like Juan Mi's compare to Deler01genMultiMargCorr (VARTA)},
  date      = {2009-07-01},
  doi       = {10.1287/opre.1080.0669},
  file      = {Biller09copulaMultiVar.pdf:Biller09copulaMultiVar.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.11.24},
  url       = {http://or.journal.informs.org/cgi/content/abstract/57/4/878},
}

@TechReport{LondonEcon13ValueLostLoad,
  author      = {Anoymous},
  title       = {The Value of Lost Load ({VoLL}) for Electricity in {Great Britain}},
  institution = {London Economics},
  year        = {2013},
  month       = jul,
  abstract    = {As part of the documents for the Consultation on the draft Electricity Market Reform Delivery, the Department of Energy and Climate Change has published the report prepared by London Economics which estimates the value of lost load (VoLL) for domestic, small and medium sized businesses and industrial and commercial electricity consumers in Great Britain. VoLL represents the value that electricity users attribute to security of electricity supply and the estimates could be used to provide a price signal about the adequate level of security of supply in Great Britain. The research is based on a variety of methods, but the major work element involved estimation of VoLL using choice experiments (CE).},
  comment     = {How the UK calculated the value of a capacity reserve be estimating the value of load lost during a power shortage. Uses questionnaires of some sort (I think) and Value at Risk estimation.},
  file        = {LondonEcon13ValueLostLoad.pdf:LondonEcon13ValueLostLoad.pdf:PDF},
  location    = {London, UK},
  owner       = {sotterson},
  timestamp   = {2014.12.04},
  url         = {http://londoneconomics.co.uk/blog/publication/the-value-of-lost-load-voll-for-electricity-in-great-britain/},
}

@PhdThesis{Kristof13opRsrvWind,
  author      = {Kristof, DE},
  title       = {SIZING {AND} ALLOCATION OF OPERATING RESERVES FOLLOWING WIND POWER INTEGRATION},
  year        = {2013},
  type        = {PhD. Thesis},
  month       = apr,
  abstract    = {As part of the European policy goals aiming towards a sustainable energy supply, the
share of renewable energy sources in the electricity system is increasing. The two
technologies driving this development in Europe, i.e. wind and PV, are characterised
by a variable energy output, facing a limited predictability. These unexpected output
variations challenge the real-time balance between electricity demand and supply
and require appropriate operating reserve capacity. In the absence of large-scale
storage or price responsive demand, the balancing of the power system is the main
bottleneck for a large-scale deployment of renewable electricity generation in the
power system.
This dissertation deals with the management of operating reserves for balancing the
power system with high shares of wind power. Firstly, a numerical model is
developed to generate realistic time series of wind power generation and predictions
over a region. This data is used to determine the short-term variations and prediction
errors impacting the system balance. These are assessed by means of a flexibility
assessment tool quantifying the operational thermal flexibility which is available for
covering real-time system imbalances. Results show that without additional reserve
requirements, this flexibility is insufficient to ensure a reliable integration of wind
power. Particularly the upward fast-response flexibility is found to be scarce.
Secondly, a statistical methodology is used to size and allocate additional reserve
capacity to maintain stable reliability standards while integrating wind power. A
strategy which minimises total capacity while maximising the allocation of reserve
capacity towards slow-response reserves is put forward as a cost-efficient strategy.
This is verified by means of a model simulating the day-ahead scheduling of power
plants to meet the demand for power and reserve capacity. It is found that reserve
requirements, and in particular the fast-response upward reserves, impact electricity
generation cost. This cost increase is minimised when deploying slow-response
reserve strategies. Results indicate the importance of peak power plants and base
load flexibility to obtain a cost-efficient procurement of thermal reserve capacity.
Finally, two reserve strategies are presented based on probabilistic wind power
forecasting tools. Information concerning the uncertainty of the wind power forecast
is used to obtain time-varying reserve requirements and enables the active
participation of wind power in reserves. It is shown that both strategies achieve
substantial cost savings without impacting system security. In particular the dynamic
reserve strategy is particularly useful for reducing upward reserve capacity. In
contrast, wind power participation remains a last-resort measure when facing low
downward flexibility and elevated reserve requirements.},
  comment     = {Overview of mostly deterministic wind grid integration optimization. But some reference to probabalistic forecasts and dynamic reserve sizing.

References might be a good way to find latest uses. See in the refs I have noted in my bookmarks.

Also could be a good way to comprehend how optimization is used in European power systems.},
  file        = {Kristof13opRsrvWind.pdf:Kristof13opRsrvWind.pdf:PDF},
  groups      = {Use, doReadWPV_1, doReadWPV_2, Wind},
  institution = {Katholieke Universiteit Leuven},
  owner       = {sotterson},
  timestamp   = {2013.09.27},
}

@Article{McLe08WP26EquivWndPowCrvs,
  author      = {J. R. McLean},
  title       = {WP2.6 -- Equivalent Wind Power Curves},
  journal     = {EU Tradewind Project Technical Report No.11914/BT/02. Garrad Hassan and Partners.},
  year        = {2008},
  number      = {11914/BT/02},
  month       = jul,
  abstract    = {As part of the Tradewind project, Garrad Hassan and Partners (GH) has been asked to produce an equivalent wind power curve (EPC), as detailed in WP2.6, in order to convert wind data to power data for wind farms across each region. The hub height wind speed time series (HWS) described in WP2.4 [1] will be combined with the EPC to estimate the power time series for each region. Future developments in wind turbine design and the implications for the EPC are considered up to the year 2030. As detailed in [2] the EPC will include factors such as array losses, topographic losses, electrical losses, availability and similar, and will take into account possible future developments in wind turbine power curves and hub heights.},
  comment     = {The generic power curves that Stefan V. used for NWP based grid forecasting. Could be a good initial condition for an algorithm that learns power curves. Or could be could when only a simple, rough curve is needed.},
  file        = {McLe08WP26EquivWndPowCrvs.pdf:McLe08WP26EquivWndPowCrvs.pdf:PDF},
  institution = {Garrad Hassan and Partners Ltd.},
  owner       = {sotterson},
  timestamp   = {2015.01.28},
  type        = {TradeWind Project Report},
}

@TechReport{Kamath10statHistRamps,
  author      = {Kamath, C.},
  title       = {Using Simple Statistical Analysis of Historical Data to Understand Wind Ramp Events},
  institution = {Lawrence Livermore National Laboratory (LLNL), Livermore, CA},
  year        = {2010},
  number      = {LLNL-TR-423242},
  abstract    = {As renewable resources start providing an increasingly larger percentage of our energy needs, we need to improve our understanding of these intermittent resources so we can manage them better. In the case of wind resources, large unscheduled changes in the energy output, called ramp events, make it challenging to keep the load and the generation balanced. In this report, we show that simple statistical analysis of the historical data on wind energy generation can provide insights into these ramp events. In particular, this analysis can help answer questions such as the time period during the day when these events are likely to occur, the relative severity of positive and negative ramps, and the frequency of their occurrence. As there are several ways in which ramp events can be defined and counted, we also conduct a detailed study comparing different options. Our results indicate that the statistics are relatively insensitive to these choices, but depend on utility-specific factors, such as the magnitude of the ramp and the time interval over which this change occurs. These factors reflect the challenges faced by schedulers and operators in keeping the load and generation balanced and can change over the years. We conduct our analysis using data from wind farms in the Tehachapi Pass region in Southern California and the Columbia Basin region in Northern Oregon; while the results for other regions are likely to be different, the report describes the benefits of conducting simple statistical analysis on wind generation data and the insights that can be gained through such analysis.},
  comment     = {Histograms and stuff of ramp stats. No obvious conclusions from this study.
* used BPA and CA wind data

Ramp definitions
* 3 ramp definitions p. 5-
* no explanation of how an operator would use these ramps
* ramps are merged when they touch (does this make sense?)

Somewhat useful opinion
* says ramp definition threshold and time should not be max capacity but a funciton of reserves, time of year, etc.

Conclusions
* no conclusion about month of year ramp stats
* maybe more pos. than neg. for small ramps
* neg. can be as severe as pos.
* not clear if there are more large neg. than large pos.
* no conclusions about time of day
* more ramps in winter but isn't there just more wind then?},
  file        = {Kamath10statHistRamps.pdf:Kamath10statHistRamps.pdf:PDF},
  groups      = {Read},
  owner       = {scot},
  timestamp   = {2011.04.20},
  url         = {http://www.osti.gov/bridge/purl.cover.jsp?purl=/972427-5UaxYH/},
}

@InProceedings{Lv16faultDiagDeepLrn,
  author    = {F. Lv and C. Wen and Z. Bao and M. Liu},
  title     = {Fault diagnosis based on deep learning},
  booktitle = {Proc. American Control Conf. (ACC)},
  year      = {2016},
  pages     = {6851--6856},
  month     = jul,
  abstract  = {As representation scheme can severely limit the window by which the system observes its world, deep learning for fault diagnosis is put forward in this paper. It is a real time online scheme that can enhance the accuracy of detection, classification and prediction, and efficient for incipient faults that cannot be detected by traditional statistic technology. A stacked sparse auto encoder is used to learn the deep architectures of fault data to minimize the loss of information. Experiment results show that the proposed method not only improves the divisibility between faults and normal process, but also exhibits a better performance on the accuracy of fault classification for the chemical benchmark, Tennessee Eastman Process (TEP) data.},
  comment   = {Stacked sparse autoencoder detects, classificies and predicts faults.  Paper also references a standard fault detection database that can be downloaded for baseline studies (useful!)},
  doi       = {10.1109/ACC.2016.7526751},
  file      = {Lv16faultDiagDeepLrn.pdf:Lv16faultDiagDeepLrn.pdf:PDF},
  keywords  = {chemical engineering computing, chemical industry, encoding, fault diagnosis, learning (artificial intelligence), pattern classification, TEP, Tennessee Eastman process, chemical production system, deep learning, fault data classification, fault diagnosis, sparse autoencoder, Classification algorithms, Encoding, Fault detection, Fault diagnosis, Machine learning, Support vector machines, Training, deep learning, fault classification, fault detection, sparse auto encoding},
  owner     = {sotterson},
  timestamp = {2017.01.26},
}

@TechReport{Kleissl10solarFrcstStateOfArt,
  author      = {Jan Kleissl},
  title       = {A Current State of the Art in Solar Forecasting. Appendix A of California Renewable Energy Forecasting, Resource Data and Mapping},
  institution = {Regents of the University of California},
  year        = {2010},
  number      = {Contract Number: 500-99-013, Work Authorization Number: BOA-99-248-R},
  abstract    = {As solar thermal and photovoltaic generation begin to have a larger role in electrical
generation in California, the California Independent System Operators needs to accommodate their
variable nature in its forecasting and dispatching. This project reviews and evaluates current knowledge
and models for forecasting solar resources and considers options for improving forecasts through RD&D
and additional measurements.
Satellite and numerical weather prediction (NWP) have been shown to be the best tools for hour ahead
and day ahead forecasts at this time. However, NWP solar forecast performance has yet to be evaluated
for California, where the coastal microclimate especially may present a significant challenge. To validate
and calibrate such forecasts, an aggregated real-time production database for all metered PV systems is
deemed to be the most spatially dense and economical set of ?measurements.? A research roadmap for
improving Direct Normal Irradiance forecasts is provided.
Keywords: solar thermal, photovoltaic systems, energy, renewable, forecast, NWP, modeling},
  comment     = {Solar forecasting literature review. Pelland13pvSolarFrcstStateOfArt is newer, has author in common. Read that instead?},
  file        = {Kleissl10solarFrcstStateOfArt.pdf:Kleissl10solarFrcstStateOfArt.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2015.02.19},
  url         = {http://uc-ciee.org/all-documents/a/451/113/nested},
}

@InProceedings{Pearce09solarNukeIns,
  author    = {Pearce, J.M.},
  title     = {Increasing {PV} velocity by reinvesting the nuclear energy insurance subsidy in large-scale photovoltaic production},
  booktitle = {Photovoltaic Specialists Conference (PVSC), 2009 34\textsuperscript{th} IEEE},
  year      = {2009},
  pages     = {001338--001343},
  month     = jun,
  abstract  = {As the debate over the future of energy grows, often nuclear energy production is pitted against solar photovoltaic energy conversion. There is a widespread belief that solar cannot compete with nuclear energy economically without government subsidies. The continued and widespread belief in the economic viability of nuclear energy, however, is predicated in part on government-mandated limitation on the liability of the nuclear industry. To demonstrate the magnitude of this nuclear energy insurance subsidy, this paper considers a shift in policy to reinvest only the premiums of the nuclear energy insurance subsidy into large scale solar photovoltaic production. The current insurance subsidy for a single nuclear power plant in the U.S. is reviewed along with the investment requirements for a one GigaWatt thin film amorphous silicon solar photovoltaic manufacturing plant. The available power and energy are then compared for an ensemble of nuclear power plants and solar photovoltaic arrays produced by the manufacturing plants over a nuclear plant life cycle. The startling results show that only the premiums for nuclear energy insurance would result in both more installed power and energy produced by mid-century if these funds were invested in large scale photovoltaic manufacturing. This study clearly shows that policies to transfer the nuclear energy insurance subsidy to large-scale manufacturing would increase the PV velocity to push the PV industry over 1 TW in under fifty years.},
  comment   = {Nuclear insurance subsidies are huge, so large that even very expensive solar would produce more energy if given the same amount of subsidy.},
  doi       = {10.1109/PVSC.2009.5411268},
  file      = {Pearce09solarNukeIns.pdf:Pearce09solarNukeIns.pdf:PDF},
  issn      = {0160-8371},
  keywords  = {PV velocity;energy grows;insurance subsidy;large-scale photovoltaic production;nuclear energy production;nuclear power plant;insurance;nuclear power stations;photovoltaic power systems;},
  owner     = {sotterson},
  timestamp = {2012.03.02},
}

@Article{Greaves09tempUncertRamps,
  author    = {Greaves, B. and Collins, J. and Parkes, J. and Tindal, A.},
  title     = {Temporal Forecast Uncertainty for Ramp Events},
  journal   = {Wind Engineering},
  year      = {2009},
  volume    = {33},
  number    = {4},
  pages     = {309--319},
  issn      = {0309-524X},
  abstract  = {As the penetration of wind energy continues to increase around the world, the impact of wind energy on the management of electrical grids is becoming increasingly evident. The challenge for the grid operator of integrating wind energy, or for the energy trader to maximise the market value of the energy, is toughest during periods of rapid change in wind farm production, or ramp events. These events are also tough to forecast accurately, and it is therefore essential to understand the uncertainty of forecasting such events. To date the majority of work on the uncertainty of wind energy forecasts has been focused on the possible amplitude of wind production that might occur at a given time. However, there has been limited investigation into effectively defining the possible timing of significant wind energy events. This paper aims to focus on methodologies for generating temporal forecast uncertainty for rapid changes in wind farm production. The first challenge is to define ramp events, secondly the forecast uncertainty needs to be calculated and finally this information needs to be presented clearly to the end user. This paper covers these three areas, with a focus on the method of calculating forecast uncertainty using multiple NWP inputs, statistical processing and adaptive algorithms. The results are based on GH Forecaster services for both individual and portfolios of wind farms. The outcome of the investigation demonstrates that temporal forecast uncertainty can be calculated and clearly presented to indicate the likely timing and amplitude of wind energy ramp events.},
  comment   = {Another ramp forecast paper. Why not just correctly model temporal dependencies in a probabilistic forecast?},
  file      = {Greaves09tempUncertRamps.pdf:Greaves09tempUncertRamps.pdf:PDF},
  groups    = {doReadNonWPV_1},
  owner     = {scot},
  publisher = {Multi-Science},
  timestamp = {2011.05.11},
}

@Article{Sengupta17bestPracticesSolarRsrc,
  author   = {Sengupta, Manajit and Habte, Aron and Gueymard, Christian and Wilbert, Stefan and Renne, Dave},
  title    = {Best Practices Handbook for the Collection and Use of Solar Resource Data for Solar Energy Applications: Second Edition},
  year     = {2017},
  month    = {12},
  abstract = {As the world looks for low-carbon sources of energy, solar power stands out as the single most abundant energy resource on Earth. Harnessing this energy is the challenge for this century. Photovoltaics, solar heating and cooling, and concentrating solar power (CSP) are primary forms of energy applications using sunlight. These solar energy systems use different technologies, collect different fractions of the solar resource, and have different siting requirements and production capabilities. Reliable information about the solar resource is required for every solar energy application. This holds true for small installations on a rooftop as well as for large solar power plants; however, solar resource information is of particular interest for large installations, because they require substantial investment, sometimes exceeding 1 billion dollars in construction costs. Before such a project is undertaken, the best possible information about the quality and reliability of the fuel source must be made available. That is, project developers need reliable data about the solar resource available at specific locations, including historic trends with seasonal, daily, hourly, and (preferably) subhourly variability to predict the daily and annual performance of a proposed power plant. Without this data, an accurate financial analysis is not possible. Additionally, with the deployment of large amounts of distributed photovoltaics, there is an urgent need to integrate this source of generation to ensure the reliability and stability of the grid. Forecasting generation from the various sources will allow for larger penetrations of these generation sources because utilities and system operators can then ensure stable grid operations. Developed by the foremost experts in the field who have come together under the umbrella of the International Energy Agency's Solar Heating and Cooling Task 46, this handbook summarizes state-of-the-art information about all the above topics.},
  comment  = {NREL's solar assessment handbook.

Low light levels
* Example shows that, in Arizona, ~20% of GHI is below 228 W/m^2.  
* At that level, PV panel efficiency drops, is no longer linear w.r.t. GHI 
   (Electropaedia19solarPower, Aleo19pvPanelsLowLight, Grunow04weakLightPV)},
  doi      = {10.2172/1411856},
  place    = {United States},
}

@TechReport{Crabtree14srvyCmrclCondMonWindTurb,
  author      = {C.J. Crabtree and D. Zappala? and P.J. Tavner},
  title       = {Survey of commercially available condition monitoring systems for wind turbines.},
  institution = {DU},
  year        = {2014},
  type        = {Technical Report},
  month       = {May},
  abstract    = {As wind energy assumes greater importance in remote and offshore locations, effective and reliable condition monitoring techniques are required. Failure rate and downtime studies have also highlighted a need for condition monitoring of particular wind turbine drive train components. This survey discusses the reliability of wind turbines and different monitoring configurations currently in use. The document contains a survey of commercially available condition monitoring systems for wind turbines including information on their monitoring technologies based on available literature and discussion with the companies responsible. Observations are made concerning the nature of systems that are currently available and the apparent direction of future monitoring systems.},
  comment     = {MondernWindABS},
  file        = {Crabtree14srvyCmrclCondMonWindTurb.pdf:Crabtree14srvyCmrclCondMonWindTurb.pdf:PDF},
  owner       = {sotterson},
  publisher   = {Durham University School of Engineering and Computing Sciences and the SUPERGEN Wind Energy Technologies Consortium},
  timestamp   = {2016.12.19},
  url         = {http://dro.dur.ac.uk/12497/},
}

@InProceedings{Grimit08frcstWndRamp,
  author    = {Eric Grimit and Cameron Potter},
  title     = {A Prototype Day-Ahead Forecast System for Rapid Wind Ramp Events},
  booktitle = {American Wind Energy Association (AWEA)},
  year      = {2008},
  abstract  = {As wind energy penetration levels increase, it is increasingly important to anticipate rapid fluctuations in wind energy output resulting from large changes in wind speed. These sudden changes in wind energy production, which we call rapid ramp events (RREs), pose a challenge to grid operators, plant managers, and energy traders. Unscheduled wind ramps consume high-cost ancillary services, reduce system reliability, and negatively impact revenues. RREs affect the entire wind facility and even adjacent wind projects nearly coincidentally. Geographic dispersion of wind energy projects can smooth, but does not eliminate, wind energy ramps. Accurate day-ahead forecasts of RREs can be useful in mitigating their negative impacts. Unfortunately, state-of-the-art forecast systems for day-ahead wind energy prediction are currently tuned to reduce the overall forecast error, by minimizing a bulk error metric like the root-mean-squared error over a long period. Such a forecast system is not generally optimized to capture the large hou/rly fluctuations in wind project output. A specialized day-ahead forecast system that is tuned for prediction of large changes in wind speed could yield substantial benefits. A prototype day-ahead RRE forecast system has been designed and built for Bonneville Power Administration (BPA) by 3TIER. This experimental RRE forecast system is fully probabilistic, allowing the scheduler to evaluate the level of acceptable risk in real-time and make an informed decision. The forecast system is based on an ensemble of mesoscale numerical weather predictions that assesses the uncertainty in the timing and magnitude of wind speed changes. The system also directly accounts for power conversion uncertainties.},
  comment   = {3TIER's ramp forecasting prototype.},
  file      = {Grimit08frcstWndRamp.pdf:Grimit08frcstWndRamp.pdf:PDF;Grimit08frcstWndRamp.pdf:Grimit08frcstWndRamp.pdf:PDF},
  groups    = {DOE-PNL09},
  owner     = {sotterson},
  timestamp = {2009.03.04},
}

@Article{Meinshausen08pValHiDim,
  author     = {Meinshausen, N. and Meier, L. and {B{\"u}hlmann}, P.},
  title      = {P-values for high-dimensional regression},
  journal    = {ArXiv e-prints},
  year       = {2008},
  month      = nov,
  abstract   = {Assigning significance in high-dimensional regression is challenging. Most computationally efficient selection algorithms cannot guard against inclusion of noise variables. Asymptotically valid p-values are not available. An exception is a recent proposal by Wasserman and Roeder (2008) which splits the data into two parts. The number of variables is then reduced to a manageable size using the first split, while classical variable selection techniques can be applied to the remaining variables, using the data from the second split. This yields asymptotic error control under minimal conditions. It involves, however, a one-time random split of the data. Results are sensitive to this arbitrary choice: it amounts to a `p-value lottery' and makes it difficult to reproduce results. Here, we show that inference across multiple random splits can be aggregated, while keeping asymptotic control over the inclusion of noise variables. We show that the resulting p-values can be used for control of both family-wise error (FWER) and false discovery rate (FDR). In addition, the proposed aggregation is shown to improve power while reducing the number of falsely selected variables substantially.},
  comment    = {How to select p-values in order to use p-values to assess significance. Could be useful for matlab's stepwisefit.m
  * this was version 3 of the paper, submitted in June 2009

Compare with Nogueira18featSelStbl},
  eprint     = {0811.2177},
  eprinttype = {arXiv},
  file       = {Meinshausen08pValHiDim.pdf:Meinshausen08pValHiDim.pdf:PDF;Meinshausen08pValHiDim.pdf:Meinshausen08pValHiDim.pdf:PDF},
  keywords   = {Statistics - Methodology, Statistics - Machine Learning},
  owner      = {sotterson},
  timestamp  = {2009.08.12},
  url        = {http://arxiv.org/abs/0811.2177v3},
}

@Article{Bremnes04precipQuantsVerif,
  author    = {Bremnes, John Bj{\o}rnar},
  title     = {Probabilistic forecasts of precipitation in terms of quantiles using NWP model output.},
  journal   = {Monthly Weather Review},
  year      = {2004},
  volume    = {132},
  number    = {1},
  abstract  = {At sites with observations it is often possible to improve or enrich NWP model forecasts by means of statistical
methods. Such forecasts are almost exclusively deterministic or probabilities of discrete events. In this paper a
flexible approach for making reliable precipitation forecasts in terms of quantiles is described. The approach is
essentially in two steps: (i) estimation of probability of precipitation and (ii) estimation of selected quantiles in
the distribution of precipitation amounts given occurrence of precipitation. Estimates are obtained by means of
probit regression and local quantile regression, respectively. By applying the laws of probability, the steps are
combined to make unconditional quantile forecasts. Examples of daily precipitation forecasting using single
deterministic forecasts and ensemble forecasts as input are given.},
  comment   = {More explanation of the chi-squared quantile test used in Bremnes04quantFrcstVerif and Bremnes04windLocQR},
  file      = {Bremnes04precipQuantsVerif.pdf:Bremnes04precipQuantsVerif.pdf:PDF},
  groups    = {Test, doReadWPV_1},
  owner     = {sotterson},
  timestamp = {2014.04.01},
  url       = {http://search.ebscohost.com/login.aspx?direct=true&amp;profile=ehost&amp;scope=site&amp;authtype=crawler&amp;jrnl=00270644&amp;AN=11912142&amp;h=%2FtKF7i6ozE7pnViVjg5bpsXbZfDi1HH1Oi18gbTSt%2FjydThGjXU7z4X4o67lD3y3KDIj984AOBVBiDuXJm7owQ%3D%3D&amp;crl=c},
}

@Article{Kinney14mutMaxInfoEquit,
  author    = {Kinney, Justin B. and Atwal, Gurinder S.},
  title     = {Equitability, mutual information, and the maximal information coefficient},
  journal   = {Proceedings of the National Academy of Sciences},
  year      = {2014},
  volume    = {111},
  number    = {9},
  pages     = {3354--3359},
  issn      = {0027-8424},
  abstract  = {Attention has recently focused on a basic yet unresolved problem in statistics: How can one quantify the strength of a statistical association between two variables without bias for relationships of a specific form? Here we propose a way of mathematically formalizing this {\textquotedblleft}equitability{\textquotedblright} criterion, using core concepts from information theory. This criterion is naturally satisfied by a fundamental information-theoretic measure of dependence called {\textquotedblleft}mutual information.{\textquotedblright} By contrast, a recently introduced dependence measure called the {\textquotedblleft}maximal information coefficient{\textquotedblright} is seen to violate equitability. We conclude that estimating mutual information provides a natural and practical method for equitably quantifying associations in large datasets.How should one quantify the strength of association between two random variables without bias for relationships of a specific form? Despite its conceptual simplicity, this notion of statistical {\textquotedblleft}equitability{\textquotedblright} has yet to receive a definitive mathematical formalization. Here we argue that equitability is properly formalized by a self-consistency condition closely related to Data Processing Inequality. Mutual information, a fundamental quantity in information theory, is shown to satisfy this equitability criterion. These findings are at odds with the recent work of Reshef et al. [Reshef DN, et al. (2011) Science 334(6062):1518{\textendash}1524], which proposed an alternative definition of equitability and introduced a new statistic, the {\textquotedblleft}maximal information coefficient{\textquotedblright} (MIC), said to satisfy equitability in contradistinction to mutual information. These conclusions, however, were supported only with limited simulation evidence, not with mathematical arguments. Upon revisiting these claims, we prove that the mathematical definition of equitability proposed by Reshef et al. cannot be satisfied by any (nontrivial) dependence measure. We also identify artifacts in the reported simulation evidence. When these artifacts are removed, estimates of mutual information are found to be more equitable than estimates of MIC. Mutual information is also observed to have consistently higher statistical power than MIC. We conclude that estimating mutual information provides a natural (and often practical) way to equitably quantify statistical associations in large datasets.},
  comment   = {Defines the strength of a pairwise variable association with "equitability", shows that the maximal info coeff (MIC) does not really have it, but that (apparently) plain on mutual info is more equitable.

In 2018, Donati18maxInfCoeffTl may have come up with a better MIC coeff.},
  doi       = {10.1073/pnas.1309933111},
  eprint    = {https://www.pnas.org/content/111/9/3354.full.pdf},
  publisher = {National Academy of Sciences},
  url       = {https://www.pnas.org/content/111/9/3354},
}

@InProceedings{Zhou16recurCNNcntinPain,
  author    = {J. Zhou and X. Hong and F. Su and G. Zhao},
  title     = {Recurrent Convolutional Neural Network Regression for Continuous Pain Intensity Estimation in Video},
  booktitle = {Proc. IEEE Conf. Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year      = {2016},
  pages     = {1535--1543},
  month     = jun,
  abstract  = {Automatic pain intensity estimation possesses a significant position in healthcare and medical field. Traditional static methods prefer to extract features from frames separately in a video, which would result in unstable changes and peaks among adjacent frames. To overcome this problem, we propose a real-time regression framework based on the recurrent convolutional neural network for automatic frame-level pain intensity estimation. Given vector sequences of AAM-warped facial images, we used a slidingwindow strategy to obtain fixed-length input samples for the recurrent network. We then carefully design the architecture of the recurrent network to output continuousvalued pain intensity. The proposed end-to-end pain intensity regression framework can predict the pain intensity of each frame by considering a sufficiently large historical frames while limiting the scale of the parameters within the model. Our method achieves promising results regarding both accuracy and running speed on the published UNBCMcMaster Shoulder Pain Expression Archive Database.},
  comment   = {An example of continuous valued CNN learning (deep learning).  Could use for nowcasting from NWP grids?  Or forecast upscaling?  Input could be wind variance, irradance,... and output target could be power within the region.

Could maybe pretrain the NWP forecast features with a CNN  autocode approach:
Input: a forecasted time series of NWP "images" (24 hours, in 15 min steps, for a DA forecast)
           an input could also the horizon for  each "image"
           would be X dim, like RGB images.  Dims would be:

           - wind speed, angle
          - temp
          - ...
          - irradiance (for PV)
Output: the corresponding measured regiona powers, repeated for every 15 min step},
  doi       = {10.1109/CVPRW.2016.191},
  file      = {Zhou16recurCNNcntinPain.pdf:Zhou16recurCNNcntinPain.pdf:PDF},
  keywords  = {feature extraction, health care, recurrent neural nets, regression analysis, AAM-warped facial images, UNBC-McMaster Shoulder Pain Expression Archive Database, automatic frame-level pain intensity estimation, automatic pain intensity estimation, continuous pain intensity estimation, end-to-end pain intensity regression framework, feature extraction, healthcare, recurrent convolutional neural network regression, vector sequences, Active appearance model, Estimation, Feature extraction, Neural networks, Pain, Support vector machines, Training},
  owner     = {sotterson},
  timestamp = {2017.03.08},
}

@InProceedings{Zareipour11rampClassFrcstDatMin,
  author    = {H. Zareipour and D. Huang and W. D. Rosehar},
  title     = {Wind Power Ramp Events Classification and Forecasting: A Data Mining Approach},
  booktitle = {IEEE Power \& Energy Society (PES)},
  year      = {2011},
  location  = {Detroit, USA},
  month     = jul,
  url       = {http://www.ucalgary.ca/hzareipo/node/57},
  abstract  = {Available wind power forecasting tools predict the future values of wind power production. System operators use those predictions to estimate the severity of wind power ramp up/down events, and determine the set of actions needed to manage those events. In this paper, a direct approach for predicting the severity of wind power ramp events is presented. Ramp events are categorized into ?classes?, and available data are used to predict the class of future ramps. Support vector machines (SVM) are used as classifiers and an elaborate model for forming the set of inputs to the classifier is proposed. Numerical results based on the wind power data in Alberta, Canada, is presented. Index Terms: Wind power, ramp event, data mining, classification, feature selection},
  file      = {Zareipour11rampClassFrcstDatMin.pdf:Zareipour11rampClassFrcstDatMin.pdf:PDF},
  groups    = {doReadNonWPV_1},
  owner     = {scot},
  timestamp = {2011.04.27},
}

@Article{Qin14geneRgrsnClustTime,
  author    = {Qin, Li-Xuan and Breeden, Linda and Self, Steven G},
  title     = {Finding gene clusters for a replicated time course study},
  journal   = {BMC research notes},
  year      = {2014},
  volume    = {7},
  number    = {1},
  pages     = {60},
  abstract  = {Background: Finding genes that share similar expression patterns across samples is an important question that is
frequently asked in high-throughput microarray studies. Traditional clustering algorithms such as K-means clustering
and hierarchical clustering base gene clustering directly on the observed measurements and do not take into
account the specific experimental design under which the microarray data were collected. A new model-based
clustering method, the clustering of regression models method, takes into account the specific design of the
microarray study and bases the clustering on how genes are related to sample covariates. It can find useful gene
clusters for studies from complicated study designs such as replicated time course studies.
Findings: In this paper, we applied the clustering of regression models method to data from a time course study
of yeast on two genotypes, wild type and YOX1 mutant, each with two technical replicates, and compared the
clustering results with K-means clustering. We identified gene clusters that have similar expression patterns in wild
type yeast, two of which were missed by K-means clustering. We further identified gene clusters whose expression
patterns were changed in YOX1 mutant yeast compared to wild type yeast.
Conclusions: The clustering of regression models method can be a valuable tool for identifying genes that are
coordinately transcribed by a common mechanism.
Keywords: Clustering, Microarray, Regression, Replications, Time course},
  comment   = {This seems identical to Qin07rgrsnClustVSkmeans. Although this pdf is nicer, I have already marked up the pdf for Qin07rgrsnClustVSkmeans, so see that review.

Cited by: Kun14windIntrvlClustSOM (irrelevantly?)

This is a use of the R corm package.},
  file      = {Qin14geneRgrsnClustTime.pdf:Qin14geneRgrsnClustTime.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {BioMed Central Ltd},
  timestamp = {2014.10.24},
  url       = {http://www.biomedcentral.com/1756-0500/7/60/},
}

@Article{Jombart10discrimPCA,
  author    = {Jombart, T. and Devillard, S. and Balloux, F.},
  title     = {Discriminant analysis of principal components: a new method for the analysis of genetically structured populations},
  journal   = {BMC genetics},
  year      = {2010},
  volume    = {11},
  number    = {1},
  pages     = {94},
  issn      = {1471-2156},
  abstract  = {Background: The dramatic progress in sequencing technologies offers unprecedented prospects for deciphering the organization of natural populations in space and time. However, the size of the datasets generated also poses some daunting challenges. In particular, Bayesian clustering algorithms based on pre-defined population genetics models such as the STRUCTURE or BAPS software may not be able to cope with this unprecedented amount of data. Thus, there is a need for less computer-intensive approaches. Multivariate analyses seem particularly appealing as they are specifically devoted to extracting information from large datasets. Unfortunately, currently available multivariate methods still lack some essential features needed to study the genetic structure of natural populations. Results: We introduce the Discriminant Analysis of Principal Components (DAPC), a multivariate method designed to identify and describe clusters of genetically related individuals. When group priors are lacking, DAPC uses sequential K-means and model selection to infer genetic clusters. Our approach allows extracting rich information from genetic data, providing assignment of individuals to groups, a visual assessment of between-population differentiation, and contribution of individual alleles to population structuring. We evaluate the performance of our method using simulated data, which were also analyzed using STRUCTURE as a benchmark. Additionally, we illustrate the method by analyzing microsatellite polymorphism in worldwide human populations and hemagglutinin gene sequence variation in seasonal influenza. Conclusions: Analysis of simulated data revealed that our approach performs generally better than STRUCTURE at characterizing population subdivision. The tools implemented in DAPC for the identification of clusters and graphical representation of between-group structures allow to unravel complex population structures. Our approach is also faster than Bayesian clustering algorithms by several orders of magnitude, and may be applicable to a wider range of datasets.},
  comment   = {PCA/K-means/LCA linear dim red \& clust for p>>n, has R library
* clustering w/ PCA (dim red., decorr), k-means (cluster ID), LDA (loadings in original space)
* good for high dim. feature space w/ few samples
* use for regime detection?
* R library: adegenet
* nice, funny slides (attached)
* more info and papers at URL
* related to between-group PCA, but whole approach called "discriminative PCA" or "DPCA"
* also related to multivariate ANOVA (MANOVA?)
* can use for supervised discrimination too: PCA dim reduction/decorr makes LDA possible
* k-means used when groups are unknown
* replace PCA w/ set oriented PCA, as in Horenko06locPCAhmm ?
* relation to correlation clustering? (see energytop.org)},
  file      = {Paper:Jombart10discrimPCA.pdf:PDF;Slides:Jombart10discrimPCA_slides.pdf:PDF},
  owner     = {scot},
  publisher = {BioMed Central Ltd},
  timestamp = {2011.01.19},
  url       = {http://adegenet.r-forge.r-project.org/},
}

@Article{Kramer09gaussGraphModelsLasso,
  author    = {Kr{\"a}mer, Nicole and Schafer, Juliane and Boulesteix, Anne-Laure},
  title     = {Regularized estimation of large-scale gene association networks using graphical {Gauss}ian models},
  journal   = {BMC Bioinformatics},
  year      = {2009},
  volume    = {10},
  number    = {1},
  pages     = {384},
  issn      = {1471-2105},
  abstract  = {BACKGROUND:Graphical Gaussian models are popular tools for the estimation of (undirected) gene association networks from microarray data. A key issue when the number of variables greatly exceeds the number of samples is the estimation of the matrix of partial correlations. Since the (Moore-Penrose) inverse of the sample covariance matrix leads to poor estimates in this scenario, standard methods are inappropriate and adequate regularization techniques are needed. Popular approaches include biased estimates of the covariance matrix and high-dimensional regression schemes, such as the Lasso and Partial Least Squares. RESULTS:In this article, we investigate a general framework for combining regularized regression methods with the estimation of Graphical Gaussian models. This framework includes various existing methods as well as two new approaches based on ridge regression and adaptive lasso, respectively. These methods are extensively compared both qualitatively and quantitatively within a simulation study and through an application to six diverse real data sets. In addition, all proposed algorithms are implemented in the R package "parcor", available from the R repository CRAN. CONCLUSION:In our simulation studies, the investigated non-sparse regression methods, i.e. Ridge Regression and Partial Least Squares, exhibit rather conservative behavior when combined with (local) false discovery rate multiple testing in order to decide whether or not an edge is present in the network. For networks with higher densities, the difference in performance of the methods decreases. For sparse networks, we confirm the Lasso's well known tendency towards selecting too many edges, whereas the two-stage adaptive Lasso is an interesting alternative that provides sparser solutions. In our simulations, both sparse and non-sparse methods are able to reconstruct networks with cluster structures. On six real data sets, we also clearly distinguish the results obtained using the non-sparse methods and those obtained using the sparse methods where specification of the regularization parameter automatically means model selection. In five out of six data sets, Partial Least Squares selects very dense networks. Furthermore, for data that violate the assumption of uncorrelated observations (due to replications), the Lasso and the adaptive Lasso yield very complex structures, indicating that they might not be suited under these conditions. The shrinkage approach is more stable than the regression based approaches when using subsampling.},
  comment   = {Describes algorithms in the R "parcor" package
Lasso and adaptive lasso generate too many bayesian network connections when features are correlated; shrinkage approaches better?

The shrinkage method is described in: Schafer05shrinkCov and Opgen-Rhein07rankShrink (I think)},
  doi       = {10.1186/1471-2105-10-384},
  file      = {Kramer09gaussGraphModelsLasso.pdf:Kramer09gaussGraphModelsLasso.pdf:PDF},
  owner     = {scot},
  pubmedid  = {19930695},
  timestamp = {2010.08.02},
  url       = {http://www.biomedcentral.com/1471-2105/10/384},
}

@Article{Li11randKNNfeatSelVsRandFrst,
  author    = {Li, Shengqiao and Harner, E James and Adjeroh, Donald A},
  title     = {Random KNN feature selection-a fast and stable alternative to Random Forests},
  journal   = {BMC bioinformatics},
  year      = {2011},
  volume    = {12},
  number    = {1},
  pages     = {450},
  abstract  = {BACKGROUND:

Successfully modeling high-dimensional data involving thousands of variables is challenging. This is especially true for gene expression profiling experiments, given the large number of genes involved and the small number of samples available. Random Forests (RF) is a popular and widely used approach to feature selection for such "small n, large p problems." However, Random Forests suffers from instability, especially in the presence of noisy and/or unbalanced inputs.
RESULTS:

We present RKNN-FS, an innovative feature selection procedure for "small n, large p problems." RKNN-FS is based on Random KNN (RKNN), a novel generalization of traditional nearest-neighbor modeling. RKNN consists of an ensemble of base k-nearest neighbor models, each constructed from a random subset of the input variables. To rank the importance of the variables, we define a criterion on the RKNN framework, using the notion of support. A two-stage backward model selection method is then developed based on this criterion. Empirical results on microarray data sets with thousands of variables and relatively few samples show that RKNN-FS is an effective feature selection approach for high-dimensional data. RKNN is similar to Random Forests in terms of classification accuracy without feature selection. However, RKNN provides much better classification accuracy than RF when each method incorporates a feature-selection step. Our results show that RKNN is significantly more stable and more robust than Random Forests for feature selection when the input data are noisy and/or unbalanced. Further, RKNN-FS is much faster than the Random Forests feature selection method (RF-FS), especially for large scale problems, involving thousands of variables and multiple classes.
CONCLUSIONS:

Given the superiority of Random KNN in classification performance when compared with Random Forests, RKNN-FS's simplicity and ease of implementation, and its superiority in speed and stability, we propose RKNN-FS as a faster and more stable alternative to Random Forests in classification problems involving feature selection for high-dimensional datasets.},
  comment   = {better than random forests?

Is implemplented in the R package:

http://cran.r-project.org/web/packages/rknn/index.html},
  file      = {Li11randKNNfeatSelVsRandFrst.pdf:Li11randKNNfeatSelVsRandFrst.pdf:PDF},
  owner     = {sotterson},
  publisher = {BioMed Central Ltd},
  timestamp = {2014.05.02},
  url       = {http://www.biomedcentral.com/1471-2105/12/450},
}

@Article{Tondel11locLinClustPLS,
  author    = {T{\o}ndel, Kristin and Indahl, Ulf G and Gjuvsland, Arne B and Vik, Jon O and Hunter, Peter and Omholt, Stig W and Martens, Harald},
  title     = {Hierarchical Cluster-based Partial Least Squares Regression (HC-PLSR) is an efficient tool for metamodelling of nonlinear dynamic models},
  journal   = {BMC systems biology},
  year      = {2011},
  volume    = {5},
  number    = {1},
  pages     = {90},
  abstract  = {Background

Deterministic dynamic models of complex biological systems contain a large number of parameters and state variables, related through nonlinear differential equations with various types of feedback. A metamodel of such a dynamic model is a statistical approximation model that maps variation in parameters and initial conditions (inputs) to variation in features of the trajectories of the state variables (outputs) throughout the entire biologically relevant input space. A sufficiently accurate mapping can be exploited both instrumentally and epistemically. Multivariate regression methodology is a commonly used approach for emulating dynamic models. However, when the input-output relations are highly nonlinear or non-monotone, a standard linear regression approach is prone to give suboptimal results. We therefore hypothesised that a more accurate mapping can be obtained by locally linear or locally polynomial regression. We present here a new method for local regression modelling, Hierarchical Cluster-based PLS regression (HC-PLSR), where fuzzy C-means clustering is used to separate the data set into parts according to the structure of the response surface. We compare the metamodelling performance of HC-PLSR with polynomial partial least squares regression (PLSR) and ordinary least squares (OLS) regression on various systems: six different gene regulatory network models with various types of feedback, a deterministic mathematical model of the mammalian circadian clock and a model of the mouse ventricular myocyte function.
Results

Our results indicate that multivariate regression is well suited for emulating dynamic models in systems biology. The hierarchical approach turned out to be superior to both polynomial PLSR and OLS regression in all three test cases. The advantage, in terms of explained variance and prediction accuracy, was largest in systems with highly nonlinear functional relationships and in systems with positive feedback loops.
Conclusions

HC-PLSR is a promising approach for metamodelling in systems biology, especially for highly nonlinear or non-monotone parameter to phenotype maps. The algorithm can be flexibly adjusted to suit the complexity of the dynamic model behaviour, inviting automation in the metamodelling of complex systems.},
  comment   = {local linear regression with dimension-reduced neighborhoods deteremined by partial least squares (PLSR). Maybe good for loc lin QR?},
  file      = {Tondel11locLinClustPLS.pdf:Tondel11locLinClustPLS.pdf:PDF},
  owner     = {sotterson},
  publisher = {BioMed Central Ltd},
  timestamp = {2014.04.22},
  url       = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3127793/},
}

@Article{Milligan2008balAreaRampRqts,
  author               = {Michael Milligan and Brendan Kirby},
  title                = {The Impact of Balancing Area Size and Ramping Requirements on Wind Integration},
  journal              = {Wind Engineering},
  year                 = {2008},
  volume               = {32},
  number               = {4},
  pages                = {379--398},
  month                = jun,
  issn                 = {0309-524X},
  abstract             = {Balancing area reserve sharing2 may significantly reduce wind integration costs. It also reduces utility costs without wind. Some recent studies indicate that large balancing areas can integrate wind more easily than small ones. The ?hockey stick? pattern of dramatically increasing wind integration cost above some threshold wind penetration may not be as pronounced as expected. The existence and location of this threshold could have important implications regarding the cost of integrating significant wind penetrations. We examine wind integration impacts as a function of balancing area size to determine if the larger system size mitigates wind integration impacts at high penetrations. Using data from Minnesota, we show that ramping requirements can be reduced by balancing area consolidation. In a companion paper3, we examine electricity market data in the United States that show how ramping capability is provided at low or no cost, and discuss its relevance to wind integration.},
  citeulike-article-id = {3472773},
  comment              = {combine w/ Milligan08subHrRampBal ?},
  file                 = {paper:Milligan2008balAreaRampRqts.pdf:PDF},
  owner                = {egrimit},
  posted-at            = {2008-11-02 02:21:22},
  publisher            = {Multi-Science Publishing Co Ltd},
  timestamp            = {2009.03.04},
  url                  = {http://www.metapress.com.globalproxy.cvt.dk/content/9514587106h67112/},
}

@TechReport{Hirth13balPowRnwblGerman,
  author      = {Hirth, Lion and Ziegenhagen, Inka},
  title       = {Balancing Power and Variable Renewables: a Glimpse at German Data},
  institution = {USAEE},
  year        = {2013},
  type        = {Working Paper},
  number      = {No. 13-154},
  month       = dec,
  abstract    = {Balancing power (regulating power, control power) is used to quickly restore the supply-demand balance in power systems. Variable renewable energy sources (VRE) such as wind and solar power, being stochastic in nature, ceteris paribus increase the need for short-term balancing. Their impact on reserve requirements is heavily discussed in academic and policy circles and often thought to be large. The paper contrasts a literature survey and model results with descriptive statistics of empirical market data from Germany, providing surprising insights: all models predict VRE to increase balancing reserve requirements - however, despite German VRE capacity doubled during the last five years, balancing reserves decreased by 20\%, and procurement cost fell by 50\%. Other factors, such as increased TSO cooperation and the recession, must have overcompensated for the growth of renewables. To the extent this specific German experience can be generalized, we interpret this as an indication that balancing power is not necessarily a major barrier to VRE integration at moderate penetration rates. Next to reserve requirements, the paper discusses two additional links between renewables and balancing systems: the supply of balancing power by renewables; and the role of the imbalance price as incentive for forecast improvements. Reviewing these three links, the paper also provides a comprehensive overview of balancing systems.


Keywords: balancing power; control power; regulating power; variable renewables; wind power; solar power; market design; renewables system integration},
  comment     = {Costs of control power in Germany, where renewable doubled yet control power cost decreased by 50\% -- shouldn't have happened according to econ. models. Also a reference for the Graf/Haubrich methodology for reserve sizing. Perhaps a good literature review.
  This is the paper I annotated.

* DE renewable energy greatly increased (doubled over 5 years) and balancing reserves decreased by 20\%, and procurement cost fell by 50\%reserve costs (much like in ERCOT: Andrade17ercotRsrvLessMore)

* balancing power is only about 2.5% of wholesale market

See also: Hirth13ctlPowRnwblGerman},
  file        = {Hirth13balPowRnwblGerman.pdf:Hirth13balPowRnwblGerman.pdf:PDF},
  groups      = {Read, Use},
  owner       = {sotterson},
  publisher   = {USAEE Working Paper},
  timestamp   = {2014.12.11},
  url         = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2371752},
}

@Article{Wallstrom08bayesSplineR,
  author                     = {Wallstrom, Garrick and Liebner, Jeffrey and Kass, Robert E.},
  title                      = {An implementation of {Bayes}ian adaptive regression splines (BARS) in C with S and R wrappers},
  journal                    = {Journal of Statistical Software},
  year                       = {2008},
  volume                     = {26},
  number                     = {1},
  pages                      = {1--21},
  month                      = jun,
  issn                       = {1548-7660},
  abstract                   = {BARS (DiMatteo, Genovese, and Kass 2001) uses the powerful reversible-jump MCMC engine to perform spline-based generalized nonparametric regression. It has been shown to work well in terms of having small mean-squared error in many examples (smaller than known competitors), as well as producing visually-appealing fits that are smooth (filtering out high-frequency noise) while adapting to sudden changes (retaining high-frequency signal). However, BARS is computationally intensive. The original implementation in S was too slow to be practical in certain situations, and was found to handle some data sets incorrectly. We have implemented BARS in C for the normal and Poisson cases, the latter being important in neurophysiological and other point-process applications. The C implementation includes all needed subroutines for fitting Poisson regression, manipulating B-splines (using code created by Bates and Venables), and finding starting values for Poisson regression (using code for density estimation created by Kooperberg). The code utilizes only freely-available external libraries (LAPACK and BLAS) and is otherwise self-contained. We have also provided wrappers so that BARS can be used easily within S or R.},
  affiliation                = {Wallstrom, G (Reprint Author), Univ Pittsburgh, Dept Biomed Informat, Suite M-183 Parkvale Bldg, Pittsburgh, PA 15213 USA. {[}Wallstrom, Garrick] Univ Pittsburgh, Dept Biomed Informat, Pittsburgh, PA 15213 USA. {[}Liebner, Jeffrey; Kass, Robert E.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.},
  author-email               = {garrick\@cbmi.pitt.edu},
  comment                    = {R implementation of Kaufman05perSplnRgrs (I think) and other techniques. R packages avaiable from two links on the bottom of the bibtex URL page

Note that Kaufman05perSplnRgrs also has some kinda matlab},
  doc-delivery-number        = {321QT},
  file                       = {Wallstrom08bayesSplineR.pdf:Wallstrom08bayesSplineR.pdf:PDF},
  journal-iso                = {J. Stat. Softw.},
  keywords                   = {curve-fitting; free-knot splines; nonparametric regression; peri-stimulus time histogram; Poisson process},
  keywords-plus              = {LINEAR ALGEBRA SUBPROGRAMS; SCHWARZ CRITERION; MODELS; SET},
  language                   = {English},
  location                   = {UCLA DEPT STATISTICS, 8130 MATH SCIENCES BLDG, BOX 951554, LOS ANGELES, CA 90095-1554 USA},
  number-of-cited-references = {23},
  owner                      = {scot},
  publisher                  = {Journal of Statistical Software},
  subject-category           = {Computer Science, Interdisciplinary Applications; Statistics \& Probability},
  times-cited                = {1},
  timestamp                  = {2010.08.10},
  type                       = {Article},
  unique-id                  = {ISI:000257322300001},
  url                        = {http://apps.isiknowledge.com.globalproxy.cvt.dk/full_record.do?product=UA&search_mode=GeneralSearch&qid=2&SID=Z2Boj1I61m6IibEPiHd&page=1&doc=1&colname=WOS},
}

@Misc{Harsha14invntryRevOptDemand,
  author    = {Harsha, Pavithra and Natarajan, Ramesh and Subramanian, Dharmashankar},
  title     = {Data-Driven Inventory and Revenue Optimization for Uncertain Demand Driven by Multiple Factors},
  month     = dec,
  year      = {2014},
  note      = {US Patent 20,140,365,276},
  abstract  = {Based on a time series history of a random variable representing
demand for at least one of a good and a service as a
function of at least one controllable demand driver, obtain a
quantile regression function that estimates a quantile of a
demand distribution function; obtain a mixed- and/or superquantile
regression function that estimates conditional value
at risk; and obtain a regression function that estimates mean
of the demand distribution function. Joint optimization of:
inventory of the at least one of a good and a service, and the at
least one controllable demand driver, is undertaken based on
the quantile regression function and the mixed- and/or superquantile
regression function, to obtain an optimal value for
the at least one controllable demand driver and an implied
optimal value for a stocking level. One or more exogenous
demand drivers can optionally be taken into account.},
  comment   = {Some kinda stochastic optimzation patent that may be relevant to ReWP inter and intraday reserve offers.

Somewhat close to jointly estimating noisy supply and noisy demand price in response?},
  file      = {Harsha14invntryRevOptDemand.pdf:Harsha14invntryRevOptDemand.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.01.26},
  url       = {http://www.freepatentsonline.com/20140365276.pdf},
}

@Article{Fang02metaElipDistMargKendalPears,
  author    = {Hong-Bin Fang and Kai-Tai Fang and Samuel Kotz},
  title     = {The Meta-elliptical Distributions with Given Marginals},
  journal   = {Journal of Multivariate Analysis},
  year      = {2002},
  volume    = {82},
  number    = {1},
  pages     = {1 - 16},
  issn      = {0047-259X},
  abstract  = {Based on an analysis of copulas of elliptically contoured distributions, joint densities of continuous variables with given strictly increasing marginal distributions are constructed. A method utilized for this procedure is to embed the spherical distribution quantile transformation of each variable into an elliptically contoured distribution. The new class of distributions is then called meta-elliptical distributions. The corresponding analytic forms of the density, conditional distribution functions, and dependence properties are derived. This new class of distributions has the same Kendall's rank correlation coefficient as meta-Gaussian distributions. As an extension of elliptically contoured distributions, some new classes of distributions are also obtained.},
  comment   = {Proof of arcsin relationship between Kendall's tau and Pearson'ts Rho for Guassian Copula

According to: https://stats.stackexchange.com/questions/133460/proof-of-the-relation-between-kendalls-tau-and-pearsons-rho-for-the-gaussian-c

It is proven as Theorem 3.2 in Fang, Fang, & Kotz, The Meta-elliptical Distributions with Given Marginals Journal of Multivariate Analysis, Elsevier, 2002, 82, 1?16 but that relies on Theorem 2.22 in [K. T. Fang, Kotz, and Ng, "Symmetric Multivariate and Related Distribution," Chapman & Hall, London, 1990.] (to which I do not have access).

},
  doi       = {http://dx.doi.org/10.1006/jmva.2001.2017},
  file      = {:papers\\Fang02metaElipDistMargKendalPears.pdf:PDF},
  keywords  = {conditional quantile, copulas, elliptically contoured distributions, Kendall's, likelihood ratio dependence, multivariate distribution, regression dependence},
  owner     = {sotterson},
  timestamp = {2017.07.05},
  url       = {http://www.sciencedirect.com/science/article/pii/S0047259X01920172},
}

@Article{Gottschall06stochModelWindCorr,
  author    = {Julia Gottschall and Edgar Anahua and Stephan Barth and Joachim Peinke},
  title     = {Stochastic Modelling of Wind Speed Power Production Correlations},
  year      = {2006},
  volume    = {6},
  number    = {1},
  pages     = {665--666},
  doi       = {10.1002/pamm.200610313},
  url       = {http://www3.interscience.wiley.com.offcampus.lib.washington.edu/journal/114084507/abstract},
  abstract  = {Based on measurements we investigate the velocity-power characteristic of a 2 MW wind turbine. We apply a stochastic
analysis where we describe the evolution of the power output with a Langevin equation, with special respect to short-time
fluctuations in wind speed.
Standard procedures, such as the IEC 61400-12 standard, are limited due to the fact that only mean values over several
minutes of wind speed and power output are considered. According to this, short-time dynamics of wind and power fluctuations
are usually not taken into account. We introduce an improved method which enables us to extract these dynamics of the
power characteristic from the measured data.
In particular, we get the response dynamics of the power L(u(t)) via the estimation of Kramers-Moyal coefficients,
describing its evolution in time ? L(t) with a Langevin equation where we separate the power output into a relaxation and a
noise part. A fixed-point analysis provides the required power characteristic.},
  file      = {Gottschall06stochModelWindCorr.pdf:Gottschall06stochModelWindCorr.pdf:PDF},
  journal   = {Proceedings, Applied Mathematics and Mechanics (PAMM)},
  owner     = {scotto},
  timestamp = {2008.07.04},
}

@Article{Muecke14LngvnHiFreqPowCrv,
  author    = {M{\"u}cke, Tanja A and W{\"a}chter, Matthias and Milan, Patrick and Peinke, Joachim},
  title     = {Langevin power curve analysis for numerical wind energy converter models with new insights on high frequency power performance},
  journal   = {Wind Energy},
  year      = {2014},
  abstract  = {Based on the Langevin equation, it has been proposed to obtain power curves for wind turbines from high-frequency data of wind speed measurements u(t) and power output P(t). The two parts of the Langevin approach, power curve and drift field, give a comprehensive description of the conversion dynamic over the whole operating range of the wind turbine. The method deals with high- frequent data instead of 10???min means. It is therefore possible to gain a reliable power curve already from a small amount of data per wind speed. Furthermore, the method is able to visualize multiple fixed points, which is, e.g. characteristic for the transition from partial to full load or in case the conversion process deviates from the standard procedures. In order to gain a deeper knowledge, it is essential that the method works not only for measured data but also for numerical wind turbine models and synthetic wind fields. Here, we characterize the dynamics of a detailed numerical wind turbine model and calculate the Langevin power curve for different data samplings. We show how to obtain reliable results from synthetic data and verify the applicability of the method for field measurements with ultra-sonic, cup, and Lidar measurements. The independence of the fixed points on site- specific turbulence effects is also confirmed with the numerical model. Furthermore, we demonstrate the potential of the Langevin approach to detect failures in the conversion process and thus show the potential of the Langevin approach for a condition monitoring system.
Keywords:

 Langevin power curve;
 condition monitoring;
 WEC modelling;
 high frequency power dynamics},
  comment   = {Stochastic differential equation approach learns power curve on very little data per-power. Data has to be high frequency sample rate. Good for ReWP, maybe Eweline scenario generation?},
  doi       = {10.1002/we.1799/full},
  file      = {Muecke14LngvnHiFreqPowCrv.pdf:Muecke14LngvnHiFreqPowCrv.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.11.08},
}

@Article{Massiani2015useOfBassEV,
  author   = {Jérôme Massiani and Andreas Gohs},
  title    = {The choice of Bass model coefficients to forecast diffusion for innovative products: An empirical investigation for new automotive technologies},
  journal  = {Research in Transportation Economics},
  year     = {2015},
  volume   = {50},
  pages    = {17 - 28},
  issn     = {0739-8859},
  note     = {Electric Vehicles: Modelling Demand and Market Penetration},
  abstract = {Bass diffusion models are one of the competing paradigms to forecast the diffusion of innovative products or technologies. This approach posits that diffusion patterns can be modeled through two mechanisms: Innovators adopt the new product and imitators purchase the new product when getting in contact with existing users. Crucial for the implementation of the method are the values assigned to the two parameters, usually referred to as p and q, which mathematically describe innovation and imitation mechanisms. The present paper is based on the findings of a research project about policy measures to promote the diffusion of Electric Vehicles in Germany. It investigates how practitioners could choose adequate values for the Bass model parameters to forecast new automotive technologies diffusion with a focus on Electric Vehicles. It considers parameters provided by the literature as well as ad hoc parameter estimations based on real market data for Germany. Our investigation suggests that researchers may be in trouble in electing adequate parameter values since the different eligible parameter values exhibit dramatic variations. Literature values appear discussible and widely variable while ad hoc estimates appear poorly conclusive. A serious problem is that ad-hoc estimates of the Bass p value are highly sensitive to the assumed market potential M. So for plausible values of M, p varies on a high scale. Unless more consolidation takes place in this area, or more confidence can be placed on ad hoc estimates, these findings issue a warning for the users of such approaches and on the policy recommendations that would derive from their use.},
  comment  = {Recommends a two step approach to getting Bass diffusion adoption model coefficients using a mix of numeric and judgement-based parameter choice.  Several big tables of Bass parameters found in the literature and this paper -- maybe useful.  However, the overall negative tone  regarding the model is summarized in the conclusion: "there generally appears a warning to the prospective users of Bass parameters for the electric vehicles' diffusion."  

Standard Bass Model overview with and without market size estimation
* Using published Bass coefficients
   - fouind in many papers (there's a good table of them
    - seem to be inconsistent, and unsourced folklore
* Using empirically ("ad hoc") estimated Bass coeffs
   -  OLS solution w/ externally provided market size
       - p very sensitive to choice of M
       - q not so sensitive
       -  NLS and MLE solutions: NLS seems best but does not seem better than OLS
   - OLS solution w/ built-in market size estimate 
      - pretty easy
      - often get imaginary coeffs, or negative values that should be positive
      - market tends to be underestimated, is size of final cumulative adoption in the training data
   - says (or cites) that you can't get a good estimate until time has progressed beyond the peak of the density function (and then it's too late for the forecast to be useful) 

Recommend two step approach
1. estimate q using exogenously given market potential
2. get p (and and optionally M)
    a. Keep given M, estimate p,
    b. OR... jointly (re)estimate p and M

q could also come from the literature, as could all the other params, although, as the author notes, these values are often inconsistent and unsourced.
},
  doi      = {https://doi.org/10.1016/j.retrec.2015.06.003},
  file     = {:Massiani15useOfBassEV.pdf:PDF},
  keywords = {Bass diffusion model, Innovation, Electric vehicles},
  url      = {http://www.sciencedirect.com/science/article/pii/S0739885915000220},
}

@Article{Kapur12bassDiffSDEadopt,
  author    = {Kapur, PK and Chaudhary, Kuldeep and Aggarwal, Anu G and Jha, PC},
  title     = {On the development of innovation diffusion model using stochastic differential equation incorporating change in the adoption rate},
  journal   = {International Journal of Operational Research},
  year      = {2012},
  volume    = {14},
  number    = {4},
  pages     = {472--484},
  abstract  = {Bass innovation and diffusion model and many of its extended forms have been reported in marketing literature and applied successfully for depicting and predicting adoption curve for products from different sectors of economy, segments of markets and strata of society. All these models assume the adoption process as a discrete counting process. However, if the potential adopter population is large and product is in the market with greater life cycle length, it is quite likely that adoption process is a stochastic process with continuous state space. In this paper, we propose a new innovation and diffusion model based on type of stochastic differential equation (SDE). It also incorporates the change-point concept, where the rate of product adoption per remaining potential adopter might change due shift in marketing/promotional strategy, entry/exit of some of the competitors in the market. The applicability and accuracy of the proposed model are illustrated using new product sales data. Predictive validity and mean squared error have been used to check the validity of the model. It has been shown that SDE-based model with change point performs comparatively better than Bass innovation and diffusion model.
Keywords: innovation and diffusion model, promotional effort, SDEs, stochastic differential equations, change point},
  publisher = {Inderscience Publishers},
  url       = {https://www.inderscienceonline.com/doi/pdf/10.1504/IJOR.2012.047516},
}

@Article{Kapur12bassDiffuseSDE,
  author   = {P.K. Kapur and Kuldeep Chaudhary and Anu G. Aggarwal and P.C. Jha},
  title    = {On the development of innovation diffusion model using stochastic differential equation incorporating change in the adoption rate},
  journal  = {Inderscience Online},
  year     = {2012},
  abstract = {Bass innovation and diffusion model and many of its extended forms have been reported in marketing literature and applied successfully for depicting and predicting adoption curve for products from different sectors of economy, segments of markets and strata of society. All these models assume the adoption process as a discrete counting process. However, if the potential adopter population is large and product is in the market with greater life cycle length, it is quite likely that adoption process is a stochastic process with continuous state space. In this paper, we propose a new innovation and diffusion model based on type of stochastic differential equation (SDE). It also incorporates the change-point concept, where the rate of product adoption per remaining potential adopter might change due shift in marketing/promotional strategy, entry/exit of some of the competitors in the market. The applicability and accuracy of the proposed model are illustrated using new product sales data. Predictive validity and mean squared error have been used to check the validity of the model. It has been shown that SDE-based model with change point performs comparatively better than Bass innovation and diffusion model.
Keywords: innovation and diffusion model, promotional effort, SDEs, stochastic differential equations, change point},
  comment  = {Mabye a way to improve the Bass Innovation diffusion model with SDEs, but see cautions about a model just slightly more complicated than classic Bass in Bass94bassWithoutDecVars.

Could be useful b/c scenarios could get a rate of change of adoption.  A max rate you'd have to add distribution network gear to handle adoption rate.

Bass models used to predict solar PV adoption in Dong17resPVdeployFrcst},
  doi      = {https://doi.org/10.1504/IJOR.2012.047516},
  file     = {Slides:Kapur12bassDiffuseSDE_slides.pdf:PDF},
  url      = {https://www.inderscienceonline.com/doi/abs/10.1504/IJOR.2012.047516},
}

@InCollection{Cook10distFromQuant,
  author    = {Cook, John D},
  title     = {Determining distribution parameters from quantiles},
  publisher = {bepress},
  year      = {2010},
  month     = jan,
  abstract  = {Bayesian statistics often requires eliciting prior probabilities from sub-
ject matter experts who are unfamiliar with statistics. While most people
an intuitive understanding of the mean of a probability distribution, fewer
people understand variance as well, particularly in the context of asym-
metric distributions. Prior beliefs may be more accurately captured by
asking experts for quantiles rather than for means and variances.
This note will explain how to solve for parameters so that common
distributions satisfy two quantile conditions. We present algorithms for
computing these parameters and point to corresponding software.
The distributions discussed are normal, log normal, Cauchy, Weibull,
gamma, and inverse gamma. The method given for the normal and
Cauchy distributions applies more generally to any location-scale family.},
  comment   = {How to compute wind or pv forecast errors from error quantiles in a Bayesian way. Distributions covered are: log normal, Cauchy, Weibull,gamma, and inverse gamma.

Cauchy has been shown elsewhere (Hodge11frcstErrDistTime) to fit wind power forecast error.
Alternative matlab library for estimating Cauchy as a subset of alpha stable distributions: Veillette10AlphaStableDistMatlab

Is this better than Johnson? George11estJohnsonDistBnd
Is it better than a stable distribution? Nolan15StableDistBootCh1

This paper is related but I couldn't get the pdf:
http://da.journal.informs.org/content/8/3/206.short},
  file      = {Cook10distFromQuant.pdf:Cook10distFromQuant.pdf:PDF},
  groups    = {ErrDistProps, CitaviImport1, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2013.09.27},
}

@InBook{West13bayesDynMod,
  chapter   = {{Bayes}ian Dynamic Modelling},
  title     = {{Bayes}ian Theory and Applications},
  publisher = {Clarendon: Oxford University Press},
  year      = {2013},
  author    = {Mike West},
  abstract  = {Bayesian time series and forecasting is a very broad field and any attempt at
other than a very selective and personal overview of core and recent areas would
be foolhardy. This chapter therefore selectively notes some key models and ideas,
leavened with extracts from a few time series analysis and forecasting examples.
For definitive development of core theory and methodology of Bayesian statespace
models, readers are referred to [74,46] and might usefully read this chapter
with one or both of the texts at hand for delving much further and deeper.
The latter parts of the chapter link into and discuss a range of recent developments
on specific modelling and applied topics in exciting and challenging areas
of Bayesian time series analysis.},
  comment   = {Says that Approximate Bayesian Computation when combined with other SMC methods, seems likely to emerge in coming years as a central approach to computational approximation for sequential analysis in increasingly complex dynamic models.

This stuff is good when don't know what the residual density should be. One of my bibtexed articles has a R implementation associated with it.},
  file      = {West13bayesDynMod.pdf:West13bayesDynMod.pdf:PDF},
  groups    = {Ensemble, PointDerived, Test, doReadNonWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.04},
}

@Article{Kun14windIntrvlClustSOM,
  author    = {Kun, Ren and Jihong, Qu},
  title     = {Wind power interval prediction utilizing SOM-{BP} artificial neural network.},
  journal   = {Journal of Chemical \& Pharmaceutical Research},
  year      = {2014},
  volume    = {6},
  number    = {4},
  abstract  = {Because of randomness and uncertainty of the wind, it is difficult to predict wind power output accurately.In order
to objectify the wind power output, we proposed a new interval prediction method. Firstly, we utilize the SOM
neural network to cluster the measured data to smooth the fluctuations and reduce the noise. Secondly, we establish
the classic BPNN prediction model .and record the upper and lower of each cluster.Then we regard them as several
intervals to enhance the performance. Experiments are constructed on a set of measured data. The results
demonstrate that our method has an excellent performance and can be reasonable applied to predict wind power
output.
Keywords: Wind Power Prediction; SOM Clustering; BP Neural Network; Interval Prediction.},
  comment   = {Turbulence intensity seems useful for forecasting upper and lower quantiles, using some kind of SOM clustering. Paper is hard to read.

Cities Qin14geneRgrsnClustTime, I think, just because it talks about K-means clustering instability, but I'm not sure if that paper even mentioned that.},
  file      = {Kun14windIntrvlClustSOM.pdf:Kun14windIntrvlClustSOM.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.10.24},
  url       = {http://connection.ebscohost.com/c/articles/95959150/wind-power-interval-prediction-utilizing-som-bp-artificial-neural-network},
}

@Article{Rudervall00hvdcTechRev,
  author    = {Rudervall, R. and Charpentier, JP and Sharma, R.},
  title     = {High voltage direct current ({HVDC}) transmission systems technology review paper},
  journal   = {Energy Week},
  year      = {2000},
  volume    = {2000},
  abstract  = {Beginning with a brief historical perspective on the development of High Voltage Direct Current (HVDC) transmission systems, this paper presents an overview of the status of HVDC systems in the world today. It then reviews the underlying technology of HVDC systems, and discusses the HVDC systems from a design, construction, operation and maintenance points of view. The paper then discusses the recent developments in HVDC technologies. The paper also presents an economic and financial comparison of HVDC system with those of an AC system; and provides a brief review of reference installations of HVDC systems. The paper concludes with a brief set of guidelines for choosing HVDC systems in today?s electricity system development.},
  comment   = {HVDC facts, somewhat old, but some theory},
  file      = {Rudervall00hvdcTechRev.pdf:Rudervall00hvdcTechRev.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2012.05.30},
}

@Article{Gruen12ExtBetaInR,
  author    = {Bettina Gr{\"u}n and Ioannis Kosmidis and Achim Zeileis},
  title     = {Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned},
  journal   = {Journal of Statistical Software},
  year      = {2012},
  volume    = {48},
  number    = {11},
  pages     = {1--25},
  month     = may,
  issn      = {1548-7660},
  abstract  = {Beta regression ? an increasingly popular approach for modeling rates and proportions ? is extended in various directions: (a) bias correction/reduction of the maximum likelihood estimator, (b) beta regression tree models by means of recursive partitioning, (c) latent class beta regression by means of finite mixture models. All three extensions may be of importance for enhancing the beta regression toolbox in practice to provide more reliable inference and capture both observed and unobserved/latent heterogeneity in the data. Using the analogy of Smithson and Verkuilen (2006), these extensions make beta regression not only ?a better lemon squeezer? (compared to classical least squares regression) but a full-fledged modern juicer offering lemon-based drinks: shaken and stirred (bias correction and reduction), mixed (finite mixture model), or partitioned (tree model). All three extensions are provided in the R package betareg (at least 2.4-0), building on generic algorithms and implementations for bias correction/reduction, model-based recursive partioning, and finite mixture models, respectively. Specifically, the new functions betatree() and betamix() reuse the object-oriented flexible implementation from the R packages party and flexmix, respectively.

Keywords: beta regression, bias correction, bias reduction, recursive partitioning, nite mixture,
R.},
  accepted  = {2012-02-14},
  bibdate   = {2012-02-14},
  coden     = {JSSOBK},
  comment   = {Beta mixtures, trees, and R code.

Is the R package: betareg
TWO pdfs.  2012 and 2013.  Not sure which one is better.
Code from paper is here: http://www.jstatsoft.org/v48/i11/},
  day       = {24},
  file      = {Gruen12ExtBetaInR.pdf:Gruen12ExtBetaInR.pdf:PDF;:Gruen13extendedBeta.pdf:PDF},
  owner     = {sotterson},
  submitted = {2011-10-15},
  timestamp = {2014.07.18},
  url       = {http://www.jstatsoft.org/v48/i11},
}

@InProceedings{Pinson08probFrcstMarkovSwitch,
  author      = {Pinson, Pierre and Madsen, Henrik},
  title       = {Probabilistic forecasting of wind power at the minute time-scale with {Markov}-switching autoregressive models},
  booktitle   = {Probabilistic Methods Applied to Power Systems (PMAPS)},
  year        = {2008},
  abstract    = {Better modelling and forecasting of very short-term power fluctuations at large offshore wind farms may significantly enhance control and management strategies of their power output. The paper introduces a new methodology for modelling and forecasting such very short-term fluctuations. The proposed methodology is based on a Markov-switching autoregressive model with time-varying coefficients. An advantage of the method is that one can easily derive full predictive densities. The quality of this methodology is demonstrated from the test case of 2 large offshore wind farms in Denmark. The exercise consists in 1-step ahead forecasting exercise on time-series of wind generation with a time resolution of 10 minute. The quality of the introduced forecasting methodology and its interest for better understanding power fluctuations are finally discussed.},
  affiliation = {Technical University of Denmark, Department of Informatics and Mathematical Modeling, Mathematical Statistics and Technical University of Denmark, Department of Informatics and Mathematical Modeling, Mathematical Statistics},
  comment     = {Switching autoregressive models for 10 minute ahead wind pow forecast. I think I remember that this didn't actually work.},
  file        = {Pinson08probFrcstMarkovSwitch.pdf:Pinson08probFrcstMarkovSwitch.pdf:PDF},
  groups      = {PointDerived, doReadWPV_2},
  language    = {English},
  location    = {Presented at: IEEE PMAPS 2008, 'Probabilistic Methods Appllied to power Systems', 2008},
  owner       = {sotterson},
  timestamp   = {2009.03.04},
}

@Article{Mayr14ExtStatBoost,
  author    = {Mayr, Andreas and Binder, Harald and Gefeller, Olaf and Schmid, Matthias},
  title     = {Extending statistical boosting-an overview of recent methodological developments},
  journal   = {arXiv preprint},
  year      = {2014},
  abstract  = {Boosting algorithms to simultaneously estimate and select predictor effects in statistical models have gained substantial interest during the last decade. This review article aims to highlight recent methodological developments regarding boosting algorithms for statistical modelling especially focusing on topics relevant for biomedical research. We suggest a unified framework for gradient boosting and likelihood-based boosting (statistical boosting) which have been addressed strictly separated in the literature up to now. Statistical boosting algorithms have been adapted to carry out unbiased variable selection and automated model choice during the fitting process and can nowadays be applied in almost any possible type of regression setting in combination with a large amount of different types of predictor effects. The methodological developments on statistical boosting during the last ten years can be grouped into three different lines of research: (i) efforts to ensure variable selection leading to sparser models, (ii) developments regarding different types of predictor effects and their selection (model choice), (iii) approaches to extend the statistical boosting framework to new regression settings.},
  comment   = {The latest in boosting, including boosted quantile regression. Read after current boosting update},
  file      = {Mayr14ExtStatBoost.pdf:Mayr14ExtStatBoost.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.07.18},
  url       = {http://arxiv.org/abs/1403.1692},
}

@Article{Gao13adaboost_SVC_R,
  author    = {Feng Gao and Peng Kou and Lin Gao and Xiaohong Guan},
  title     = {Boosting regression methods based on a geometric conversion approach: Using \{SVMs\} base learners},
  journal   = {Neurocomputing},
  year      = {2013},
  volume    = {113},
  number    = {0},
  pages     = {67--87},
  issn      = {0925-2312},
  abstract  = {Boosting is one of the most important developments in ensemble learning during the past decade. Among different types of boosting methods, AdaBoost is the earliest and the most prevailing one that receives lots of attention for its effectiveness and practicality. Hitherto the research on boosting is dominated by classification problems. Conversely, the extension of boosting to regression is not as successful as that on classification. In this paper, we propose a new approach to extending boosting to regression. This approach first converts a regression sample to a binary classification sample from a geometric point of view, and performs AdaBoost with support vector machines base learner on the converted classification sample. Then the separating hypersurface ensemble obtained from AdaBoost is equivalent to a regression function for the original regression sample. Based on this approach, two new boosting regression methods are presented. The first method adopts the explicit geometric conversion while the second method adopts the implicit geometric conversion. Since both these methods essentially run on the binary classification samples, the convergence property of the standard AdaBoost still holds for them. Experimental results validate the effectiveness of the proposed methods.},
  comment   = {continuous valued regression based on adaboost, interesting b/c it actually uses SVM's

I think they call this AdaBoost.SVC.R},
  doi       = {10.1016/j.neucom.2013.01.031},
  keywords  = {Boosting},
  owner     = {sotterson},
  timestamp = {2014.01.21},
}

@Article{Chai14rmseOrMae,
  author    = {{Chai}, T. and {Draxler}, R.~R.},
  title     = {{Root mean square error (RMSE) or mean absolute error (MAE)?}},
  journal   = {Geoscientific Model Development Discussions},
  year      = {2014},
  volume    = {7},
  pages     = {1525-1534},
  month     = feb,
  abstract  = {Both the root mean square error (RMSE) and the
mean absolute error (MAE) are regularly employed in model
evaluation studies. Willmott and Matsuura (2005) have sug-
gested that the RMSE is not a good indicator of average
model performance and might be a misleading indicator of
average error, and thus the MAE would be a better metric for
that purpose. While some concerns over using RMSE raised
by Willmott and Matsuura (2005) and Willmott et al. (2009)
are valid, the proposed avoidance of RMSE in favor of MAE
is not the solution. Citing the aforementioned papers, many
researchers chose MAE over RMSE to present their model
evaluation statistics when presenting or adding the RMSE
measures could be more beneficial. In this technical note, we
demonstrate that the RMSE is not ambiguous in its mean-
ing, contrary to what was claimed by Willmott et al. (2009).
The RMSE is more appropriate to represent model perfor-
mance than the MAE when the error distribution is expected
to be Gaussian. In addition, we show that the RMSE satis-
fies the triangle inequality requirement for a distance metric,
whereas Willmott et al. (2009) indicated that the sums-of-
squares-based statistics do not satisfy this rule. In the end, we
discussed some circumstances where using the RMSE will be
more beneficial. However, we do not contend that the RMSE
is superior over the MAE. Instead, a combination of metrics,
including but certainly not limited to RMSEs and MAEs, are
often required to assess model performance.},
  adsnote   = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl    = {http://adsabs.harvard.edu/abs/2014GMDD....7.1525C},
  comment   = {Covers situations when RMSE is a good model performance metric e.g. when have Gaussian errors, and when it isn't.},
  doi       = {10.5194/gmdd-7-1525-2014},
  file      = {Chai14rmseOrMae.pdf:Chai14rmseOrMae.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.01.16},
}

@InProceedings{Lofstrom09ensmblSel,
  author    = {L{\"o}fstr{\"o}m, T. and Johansson, U. and Bostr{\"o}m, H.},
  title     = {Ensemble member selection using multi-objective optimization},
  booktitle = {IEEE Symposium on Computational Intelligence and Data Mining},
  year      = {2009},
  pages     = {245--251},
  month     = mar,
  abstract  = {Both theory and a wealth of empirical studies have established that ensembles are more accurate than single predictive models. Unfortunately, the problem of how to maximize ensemble accuracy is, especially for classification, far from solved. In essence, the key problem is to find a suitable criterion, typically based on training or selection set performance, highly correlated with ensemble accuracy on novel data. Several studies have, however, shown that it is difficult to come up with a single measure, such as ensemble or base classifier selection set accuracy, or some measure based on diversity, that is a good general predictor for ensemble test accuracy. This paper presents a novel technique that for each learning task searches for the most effective combination of given atomic measures, by means of a genetic algorithm. Ensembles built from either neural networks or random forests were empirically evaluated on 30 UCI datasets. The experimental results show that when using the generated combined optimization criteria to rank candidate ensembles, a higher test set accuracy for the top ranked ensemble was achieved, compared to using ensemble accuracy on selection data alone. Furthermore, when creating ensembles from a pool of neural networks, the use of the generated combined criteria was shown to generally outperform the use of estimated ensemble accuracy as the single optimization criterion.},
  comment   = {Classification ensembles selected for both accuracy and diversity using genetic algorithm and multi-criteria optimization. This might be somehow related to the continous Adaboost papers, which need a criteria for weighting their ensembles.},
  doi       = {10.1109/CIDM.2009.4938656},
  file      = {Lofstrom09ensmblSel.pdf:Lofstrom09ensmblSel.pdf:PDF},
  groups    = {Ensemble, doReadWPV_2},
  keywords  = {genetic algorithms;neural nets;classifier selection set accuracy;ensemble member selection;genetic algorithm;multi-objective optimization;neural networks;Atomic measurements;Difference equations;Diversity reception;Genetic algorithms;Informatics;Neural networks;Optimization methods;Predictive models;Testing;Weight measurement},
  owner     = {sotterson},
  timestamp = {2014.01.29},
}

@Article{Guolo14betaRgrsTseries,
  author    = {Guolo, Annamaria and Varin, Cristiano and others},
  title     = {Beta regression for time series analysis of bounded data, with application to {Canada} Google{\textregistered} Flu Trends},
  journal   = {The Annals of Applied Statistics},
  year      = {2014},
  volume    = {8},
  number    = {1},
  pages     = {74--88},
  abstract  = {Bounded time series consisting of rates or proportions are often
encountered in applications. This manuscript proposes a practical
approach to analyze bounded time series, through a beta regression
model. The method allows the direct interpretation of the regression
parameters on the original response scale, while properly accounting
for the heteroskedasticity typical of bounded variables. The serial de-
pendence is modeled by a Gaussian copula, with a correlation matrix
corresponding to a stationary autoregressive and moving average pro-
cess. It is shown that inference, prediction, and control can be carried
out straightforwardly, with minor modifications to standard analysis
of autoregressive and moving average models. The methodology is
motivated by an application to the influenza-like-illness incidence es-
timated by the GoogleR Flu Trends project.},
  comment   = {How to model temporal dependence with beta regression -- useful for quantile regression since it's bounded?},
  file      = {Guolo14betaRgrsTseries.pdf:Guolo14betaRgrsTseries.pdf:PDF},
  owner     = {sotterson},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2014.07.18},
  url       = {http://projecteuclid.org/euclid.aoas/1396966279},
}

@Book{Madsen11glmBook,
  title     = {Introduction to general and generalized linear models},
  publisher = {Chapman \& Hall/CRC},
  year      = {2011},
  author    = {Madsen, Henrik and Thyregod, Poul},
  isbn      = {978-1420091557},
  abstract  = {Bridging the gap between theory and practice for modern statistical model building, Introduction to General and Generalized Linear Models presents likelihood-based techniques for statistical modelling using various types of data. Implementations using R are provided throughout the text, although other software packages are also discussed. Numerous examples show how the problems are solved with R.

After describing the necessary likelihood theory, the book covers both general and generalized linear models using the same likelihood-based methods. It presents the corresponding/parallel results for the general linear models first, since they are easier to understand and often more well known. The authors then explore random effects and mixed effects in a Gaussian context. They also introduce non-Gaussian hierarchical models that are members of the exponential family of distributions. Each chapter contains examples and guidelines for solving the problems via R.

Providing a flexible framework for data analysis and model building, this book focuses on the statistical methods and models that can help predict the expected value of an outcome, dependent, or response variable. It offers a sound introduction to general and generalized linear models using the popular and powerful likelihood techniques.

Ancillary materials are available at http://www.imm.dtu.dk/~hm/GLM. This material includes solutions to exercises, additional exercises and assignments, which are intended to be solved using a computer, and data used in the book.},
  comment   = {The GLM book that Henrik game me (I have a  paper copy).

Slides and Exercises are here:
http://www.imm.dtu.dk/~hmad/GLM/

Has invertible link functions on p. 103 (useful for bounded output regression, etc.)

Fox08glmBookChap has nearly the same table, but w/ some plots},
  owner     = {sotterson},
  timestamp = {2014.02.08},
  url       = {https://www.amazon.com/Introduction-General-Generalized-Chapman-Statistical/dp/1420091557},
}

@Book{Legler19broadenGLMmultiLev,
  title        = {Broaden your statistical horizons: generalized linear models and multilevel models},
  publisher    = {bookdown.org},
  year         = {2019},
  author       = {Legler, J and Roback, P},
  month        = jan,
  abstract     = {Broadening Your Statistical Horizons (BYSH): Generalized Linear Models and Multilevel Models is intended to be accessible to undergraduate students who have successfully completed a regression course through, for example, a textbook like Stat2 (Cannon et al. 2019). We started teaching this course at St. Olaf in 2003 so students would be able to deal with the non-normal, correlated world we live in. It has been offered at St. Olaf every year since; in fact, it is required for all statistics concentrators. Even though there is no mathematical prerequisite, we still introduce fairly sophisticated topics such as likelihood theory, zero-inflated Poisson, and parametric bootstrapping in an intuitive and applied manner. We believe strongly in case studies featuring real data and real research questions; thus, most of the data in the textbook and available at our GitHub repo arises from collaborative research conducted by the authors and their students, or from student projects. Our goal is that, after working through this material, students will not necessarily be expert in these methods and associated theory, but that they will develop an expanded toolkit and a greater appreciation for the wider world of data and statistical modeling.},
  comment      = {Used for a picture reference in MNSP talk, June 2019.

Also, maybe for MNSP, I should try zero-inflation.},
  howpublished = {Online},
  url          = {https://bookdown.org/roback/bookdown-bysh/},
}

@Article{Eilers96FlexSmthBsplnPnlty,
  author    = {Eilers, Paul HC and Marx, Brian D},
  title     = {Flexible smoothing with B-splines and penalties},
  journal   = {Statistical science},
  year      = {1996},
  pages     = {89--102},
  abstract  = {B-splines are attractive for nonparametric modelling, but
choosing the optimal number and positions of knots is a complex task.
Equidistant knots can be used, but their small and discrete number allows
only limited control over smoothness and fit. We propose to use a
relatively large number of knots and a difference penalty on coefficients
of adjacent B-splines. We show connections to the familiar spline penalty
on the integral of the squared second derivative. A short overview of Bsplines,
of their construction and of penalized likelihood is presented. We
discuss properties of penalized B-splines and propose various criteria for
the choice of an optimal penalty parameter. Nonparametric logistic regression,
density estimation and scatterplot smoothing are used as examples.
Some details of the computations are presented.
Key words and phrases: Generalized linear models, smoothing, nonparametric
models, splines, density estimation.},
  comment   = {Roughness penalized regression and smoothing B-splines. Includes sample R and Matlab.

* Eilers10SplnsKntsPnlties explains (partly) how to extend for periodic bases
* extended w/ multi-dims, constraints: Pya15shapeCnstrAddMdl},
  doi       = {10.2307/2246049},
  file      = {Eilers96FlexSmthBsplnPnlty.pdf:Eilers96FlexSmthBsplnPnlty.pdf:PDF},
  owner     = {sotterson},
  publisher = {JSTOR},
  timestamp = {2015.01.28},
}

@Article{Racine12rgrsnSplnPrim,
  author    = {Racine, Jeffrey S},
  title     = {A primer on regression splines},
  journal   = {CRAN, R-Project. Package crs: Categorical Regression Splines},
  year      = {2012},
  abstract  = {B-splines constitute an appealing method for the nonparametric estimation of a range of statis-
tical objects of interest. In this primer we focus our attention on the estimation of a conditional
mean, i.e. the ?regression function?.
A ?spline? is a function that is constructed piece-wise from polynomial functions. The term
comes from the tool used by shipbuilders and drafters to construct smooth shapes having desired
properties. Drafters have long made use of a bendable strip fixed in position at a number of
points that relaxes to form a smooth curve passing through those points. The malleability of the
spline material combined with the constraint of the control points would cause the strip to take
the shape that minimized the energy required for bending it between the fixed points, this being
the smoothest possible shape. We shall rely on a class of splines called ?B-splines? (?basis-splines?).
A B-spline function is the maximally differentiable interpolative basis function. The B-spline is a
generalization of the B?ezier curve (a B-spline with no ?interior knots? is a B?ezier curve). B-splines
are defined by their ?order? m and number of interior ?knots? N (there are two ?endpoints? which are
themselves knots so the total number of knots will be N+2). The degree of the B-spline polynomial
will be the spline order m minus one (degree = m ? 1).
To best appreciate the nature of B-splines, we shall first consider a simple type of spline, the
B?ezier function, and then move on to the more flexible and powerful generalization, the B-spline
itself. We begin with the univariate case in Section 2 where we consider the univariate B?ezier
function. In Section 3 we turn to the univariate B-spline function, and then in Section 4 we turn to
the multivariate case where we also briefly mention how one could handle the presence of categorical
predictors.
We presume that interest lies in ?regression spline? methodology which differs in a number of
ways from ?smoothing splines?, both of which are popular in applied settings. The fundamen-
tal difference between the two approaches is that smoothing splines explicitly penalize roughness
and use the data points themselves as potential knots whereas regression splines place knots at
equidistant/equiquantile points. We direct the interested reader to Wahba (1990) for a treatment
of smoothing splines.},
  comment   = {How R can use regression (not smoothing) splines, including multivariate. Explanation is OK and there are code samples. Good for quantile regression (maybe a replacement for local linear). Also covers multivariate tensor splines (including w/ categorical inputs, but these can be ignored).

Definitions
* knots: boundaries of input space over which local curve fitting is done.
 -- must be montonically increasing: what is that in N-dim space?
 -- need some idea of what knots are "adjacent"
 -- tensor splines (below) avoid this question
* smoothing spline: uses the data points as potential knots (not what regression splines do)
* interior knots: boundaries within input space (if they exist, the fit is more local than Bezier (below))
* Bezier curve: a recursively derived polynomial smoother that fits without knots, or at least without interior knots
* B-spline: a Beziier curve with interior knots added
* Spline degree: the highest exponent in the spline polynmial
* Spline order: the number of polynomical coeffs (== degree+1)
* augmented knots: extra knots added at input range limits, needed for easy recursive programming

Building pline basis functions:
* these are polynomial fits to the data composed of weighed averages of left and right lower degree splines
* lower degree bases cover less input range
 * knots are chosen so that spatial extent increases as basis fucntions are built, finally covering the entire input range at the desired polynomial degree.
* 0\textsuperscript{th} order basis function is just 1 if x is between the knots i and i+t
* Basis functions are the linearly weighed average of the two lower order basis function on the left and right sides of x
* The (non-adjustable) basis function coeffs weights do linear interpolation over the x range between knots t_i and t_i+j, where the polynomial order is j+1
* leaving out intercept terms can avoid perfect collinearity

Spline derivatives (output w.r.t. input, not w.r.t. coeffs)
* Derivative is built recursively from derivatives of lower order splines
* derivative function coefficients are also built recursively, from lower order derivative coefficients.

Spline intercept term
* this is the zeroth basis, which I guess is sometimes omitted in regression settings
 -- spline basis sums to one
 -- this causes collinearity
 -- I didn't quite understand this
* this book said the saem thing about omitting it:: http://tinyurl.com/mv5j3bb

Splines for regression
* Basis functions don't change with changing regression coeffs (they're fixed purely by knot locations). I guess that's what makes them "basis functions!"
*Jacobian (partial derivative w.r.t. coeffs) looks easy to calculate (for 1D) so it would be easy to plug into a quantile regression n interior point solvers:

Jacobians for derivative-based optimization (as in Hunter00's iprq and mmqr)

Partial derivative w.r.t. the ith coefficient, Beta_i:

BetaPartial = Beta
BetaPartial(i) = 1

d/dBeta_i(x) = Sum(BetaPartial .* BasisFuncs(x))

ORIGINAL CODE SOURCE
* The R code here is a port of R and Fortran in the 2\textsuperscript{nd} attached notes
* according to Racien

MULTIVARIATE SPLINES
* these are tensor splines
* require gridded input space with the same divisions in each dimension
* not good for high dim quantile regression, where lots of space ie empty (are thin plate splines any better?)
* This paper explains how to do with when categorical variables are mixed in
 - but if these are ignored, then it's an ordinary linear equation, w/ a least squares (or linQR) solution.},
  file      = {Racine12rgrsnSplnPrim.pdf:Racine12rgrsnSplnPrim.pdf:PDF;Original Code Notes from Samiran Sinha:Racine12rgrsnSplnPrim_SinhaNotes.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2014.06.12},
  url       = {http://cran.r-project.org/web/packages/crs/vignettes/spline_primer.pdf},
}

@Article{Smith13quantRgrsBSquare,
  author    = {Smith, Luke B and Reich, Brian J},
  title     = {BSquare: An {R} package for {Bayes}ian simultaneous quantile regression},
  year      = {2013},
  abstract  = {BSquare in an R package to conduct Bayesian quantile regression for continuous, discrete, and
censored data. Quantile regression provides a comprehensive analysis of the relationship between
covariates and a response. In quantile regression, by specifying different covariate effects at different
quantile levels we allow covariates to affect not only the center of the distribution, but also its spread
and the magnitude of extreme events. Unlike most approaches to quantile regression, such as those
implemented in package quantreg and bayesQR, BSquare analyzes all quantile levels simultaneously.
Therefore, this approach can borrow strength across nearby quantile levels to reduce uncertainty in
the estimated quantile function, which can be advantageous when the covariate effects are similar
across quantile levels. BSquare takes a model-based approach to quantile regression, which we
briefly review in Section 1; for thorough descriptions of the model and its properties we refer to
Reich et al. (2011); Reich (2012); Reich and Smith (2013); Smith et al. (2013). We then illustrate
the use of the package for continuous (Section 2), survival (Section 3), and discrete (Section 4)
data. We conclude with an example using splines (Section 5).},
  comment   = {Quantile regression based on basis functions, has R. Several nice things about it e.g. basis function can be designed to always produce valid quantiles (avoiding crossover, I think). Also does spatio-temporal modeling.

These are worth reading for overview, if not for the code (which might also be a good reason).

The real papers are here:

http://www4.stat.ncsu.edu/~reich/QR/

which I got to from here (also has links to matlab algorithms)

http://www.ral.ucar.edu/~ericg/softextreme.php

IDEA: add basis learning to this as well, or would that make for impossible quantiles, or just duplicate NN quantile learner? Anyway, I have a lot of papers already in energy.bib about learning monotonic functions (not necessarily bases); maybe worth borrowing some ideas?},
  file      = {Smith13quantRgrsBSquare.pdf:Smith13quantRgrsBSquare.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2013.10.15},
}

@PhdThesis{Frain09alphaDistAppsThesis,
  author      = {Frain, John C},
  title       = {Studies on the Application of the Alpha-stable Distribution in Economics},
  year        = {2009},
  abstract    = {Bubbles, booms and busts in asset prices give rise to a considerable misallocation of
resources when they are growing and the subsequent adjustment can be very long
and painful. Yet, there is no accepted diagnosis of a bubble. In effect, there is a
sense in which a bubble and a bust can not occur in the usual econometric models.
These models, almost always, depend on the normal or Gaussian distribution. Yet
when one looks at data for asset prices the number and size of extreme losses and
gains are orders of magnitude greater than a normal distribution would predict. The
very existence of these extreme values must lead one to question the validity of the
normality assumption and to look for an alternative.
From time to time several alternatives have been proposed. A common pro-
posal is to use mixtures of normal distributions. The simplest such solution is to
have a mixture of two normal distributions ? the first, with low volatility, repre-
sents the fundamental state with no bubble and the second, with high volatility, the
bubble. The price of the asset in question is seen as switching from one state to
the other with the switching being determined by some form of deterministic or
stochastic process. Other solutions involve what are, in effect, infinite mixtures of
normal distributions. Chief amongst these are the various GARCH proceses and the
t-distribution. Various other ?fat-tailed? distributions have been proposed but these
have not received universal acceptance and probably never will. While such distribu-
tions often fit the data well, We have not seen any convincing theoretical arguments
why they should.
The purpose of this thesis is to examine the use of the ?-stable distribution in
this context and to determine some of the consequences of its use. The ?-stable
distribution is a generalisation of the normal distribution. It was first proposed as
a distribution for asset returns and commodity prices by Mandelbrot in the early
1960s. It attracted a lot of attention up to the early 1970s and then interest faded.
There were two reasons for the waning interest. First the advances made at the time
in portfolio and option pricing theory were dependent on the normal distribution. At
the time almost all of this work could not have been replicated without the normality
assumption. Secondly for actual application the computer power available at the
time was simply not sufficient to properly use the ?-stable distribution. Thus ?-
stable analysis was primitive relative to the corresponding normal analysis.
Section 2.1 is a brief history of the application of the ?-stable distribution to fi-
nancial economics. Appendix A contains an account of the theory of such processes.
The ?-stable distribution allows for the type of extreme and skewed values observed
in asset prices. The theoretical arguments that can be used to justify the assump-
tion of a normal distribution can also be used to justify an ?-stable distribution. We
discuss the relevance of a generalised central limit theorem, domains of attraction
and scaling to asset pricing. Statistically, the ?-stable distribution is a much better
fit to the six total return equity indices that we use to illustrate this study. We then
report on three studies that use an assumption of an ?-stable distribution.
The first study examines the problem of regression when the disturbances have
an ?-stable distribution. OLS estimates are not optimum. The maximum likelihood
estimator of the regression coefficients is a form of robust estimator that gives less
weight to extreme observations. The theory is applied to the estimation of day
of week effects in the equity indices. The methodology is feasible and there are
sufficient differences in the results to justify the use of the new methodology when
sufficient data are available and ?fat tails? are suspected. The results support the
conclusion that day of week effects no longer exist.
The second study is a simulation exercise to assess the power of normality tests
when the alternative is an ?-stable distribution. Such tests are sometimes applied
to monthly equity returns and when normality can not be rejected it is concluded
that the data can not be non-normal ?-stable. We show that the power of these test
is often so poor that these conclusions can not be sustained.
The third study concerns the use of the ?-stable distribution in the measure-
ment of Value at Risk (VaR). We find that a static ?-stable distribution gives good
measures of VaR at conventional levels for the equity indices examined. The ?-
stable distribution and a GARCH process with ?-stable innovations can give very
good measures of VaR.
We may draw two types of conclusion from the studies:
1. The use of the ?-stable distribution is feasible in many situations. In the situa-
tions examined here it appears to give better results than traditional methods
that rely on the normal distribution. It can only be used when there is a large
sample of data such as is available in the daily equity return series considered
here.

2. From a policy viewpoint there are two consequences of this analysis:

(a) If economic variables follow an ?-stable distribution then we must accept
that extremes do occur and must make provision where appropriate.
(b) It would appear that policy can not reduce the stability parameter. It can
change the scale parameter and considerable reductions in the probabil-
ity of extreme events can be brought about by reductions in the scale
parameter. Such policies ought to be designed to be sustainable and ef-
fective in the long run.},
  comment     = {Thesis on alpha-stable distribution applications, including how to do regression with assuming alpha-stable error dist.

* main point made is that these distributions catch extreme tails better than with Gaussian assumptions
* main stable dist paper: Nolan15StableDistBootCh1},
  file        = {Frain09alphaDistAppsThesis.pdf:Frain09alphaDistAppsThesis.pdf:PDF},
  institution = {University of Dublin},
  publisher   = {Trinity College},
  url         = {http://www.tcd.ie/Economics/staff/frainj/Stable_Distribution/thesis_main_5.pdf},
}

@TECHREPORT{Schluter10wvltFrcstPayoff,
  Author                   = {Schl{\"u}ter, Stephan and Deuschle, Carola},
  Title                    = {Using wavelets for time series forecasting: Does it pay off?},
  Institution              = {Friedrich-Alexander-Universit{\"a}t Erlangen-N{\"u}rnberg, Institut f{\"u}r Wirtschaftspolitik und Quantitative Wirtschaftsforschung (IWQW)},
  Year                     = {2010},
  Type                     = {IWQW Discussion Paper Series},
  Number                   = {04/2010},
  Abstract                 = {By means of wavelet transform a time series can be decomposed into a time dependent sum of frequency components. As a result we are able to capture seasonalities with time-varying period and intensity, which nourishes the belief that incorporating the wavelet transform in existing forecasting methods can improve their quality. The article aims to verify this by comparing the power of classical and wavelet based techniques on the basis of four time series, each of them having individual characteristics. We find that wavelets do improve the forecasting quality. Depending on the data's characteristics and on the forecasting horizon we either favour a denoising step plus an ARIMA forecast or an multiscale wavelet decomposition plus an ARIMA forecast for each of the frequency components. --},
  File                     = {Schluter10wvltFrcstPayoff.pdf:Schluter10wvltFrcstPayoff.pdf:PDF},
  Keywords                 = {Forecasting; Wavelets; ARIMA; Denoising; Multiscale Analysis},
  Owner                    = {sotterson},
  Timestamp                = {2013.03.16},
  URL                      = {http://ideas.repec.org/p/zbw/iwqwdp/042010.html}
}

@InProceedings{Kawahara09chngPtDensRat,
  author    = {Kawahara, Y. and Sugiyama, M.},
  title     = {Change-point detection in time-series data by direct density-ratio estimation},
  booktitle = {Proceedings of 2009 SIAM International Conference on Data Mining (SDM2009)},
  year      = {2009},
  pages     = {389--400},
  abstract  = {Change-point detection is the problem of discovering time points at which properties of time-series data change. This covers a broad range of real-world problems and has been actively discussed in the community of statistics and data mining. In this paper, we present a novel non-parametric approach to detecting the change of probability distributions of sequence data. Our key idea is to estimate the ratio of probability densities, not the probability densities themselves. This formulation allows us to avoid non-parametric density estimation, which is known to be a di?cult problem. We provide a change-point detection algorithm based on direct density-ratio estimation that can be computed very e?ciently in an online manner. The usefulness of the proposed method is demonstrated through experiments using arti?cial and real datasets. keywords: change-point detection, direct density-ratio estimation, kernel methods, time-series dat},
  comment   = {overview slides in Sugiyama09densRatEst},
  file      = {Kawahara09chngPtDensRat.pdf:Kawahara09chngPtDensRat.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2011.11.30},
}

@Article{Jonathan14nonStatCondXtrmPenSpln,
  author    = {Jonathan, P. and Ewans, K. and Randell, D.},
  title     = {Non-stationary conditional extremes of northern North Sea storm characteristics},
  journal   = {Environmetrics},
  year      = {2014},
  volume    = {25},
  number    = {3},
  pages     = {172--188},
  issn      = {1099-095X},
  abstract  = {Characterising the joint structure of extremes of environmental variables is important for improved understanding of those environments. Yet, many applications of multivariate extreme value analysis adopt models that assume a particular form of extremal dependence between variables without justification, or restrict attention to regions in which all variables are extreme. The conditional extremes model of Heffernan and Tawn provides one approach to avoiding these particular restrictions.Extremal marginal and dependence characteristics of environmental variables typically vary with covariates. Reliable descriptions of extreme environments should also therefore characterise any non-stationarity. A recent article by the current authors extends the conditional extremes model of Heffernan and Tawn to include covariate effects, using Fourier representations of model parameters for single periodic covariates.Here, we further extend our recent work, introducing a general purpose spline representation for model parameters as functions of multidimensional covariates, common to all inference steps. We use a non-crossing quantile regression to estimate appropriate non-stationary marginal quantiles simultaneously as functions of covariate; these are necessary as thresholds for extreme value modelling and for standardisation of marginal distributions prior to application of the conditional extremes model. Then, we perform marginal extreme value and conditional extremes modelling within a roughness-penalised likelihood framework, with cross-validation to estimate suitable model parameter roughness. Finally, we use a bootstrap re-sampling procedure, encompassing all inference steps, to quantify uncertainties in, and dependence structure of, parameter estimates and estimates of conditional extremes of one variate given large values of another.We validate the approach using simulations from known joint distributions, the extremal dependence structures of which change with covariate. We apply the approach to joint modelling of storm peak significant wave height and associated storm peak period for extra-tropical storms at a northern North Sea location, with storm direction as covariate. We evaluate the impact of incorporating directional effects on estimates for conditional return values.},
  comment   = {3D tensor penalized, periodic Bspline for extreme quantile regression. Is said to be too slow in Yu14extrmValGrpMdlThinMemb

Would continue my tensor spline penalization ideas, and would also be an example of building up an extreme quantile forecast from the ground theory.


Slides from: http://www.lancaster.ac.uk/people/jonathan/JntHnc13.pdf},
  doi       = {10.1002/env.2262},
  file      = {paper submitted:Jonathan14nonStatCondXtrmPenSpln.pdf:PDF;slides:Jonathan14nonStatCondXtrmPenSpln_slides.pdf:PDF},
  keywords  = {non-stationarity, conditional extremes, covariate, spline, non-crossing quantile regression, cross-validation, bootstrap},
  owner     = {sotterson},
  timestamp = {2015.04.17},
}

@Article{Simila09combInSelModCmplx,
  author    = {Simil{\"a}, Timo and Tikka, Jarkko},
  title     = {Combined input variable selection and model complexity control for nonlinear regression},
  journal   = {Pattern Recognition Letters},
  year      = {2009},
  volume    = {30},
  number    = {3},
  pages     = {231--236},
  issn      = {0167-8655},
  abstract  = {Choosing a useful combination of input variables and an appropriate complexity of the model is an essential task in nonlinear regression analysis because of the risk of overfitting. This article provides a workable solution for the multilayer perceptron model. An initial structure of the model, including all the input variables, is fixed in the beginning. Only the most useful input variables and hidden nodes remain effective when the model is fitted with the proposed penalization method. The method is tested on three benchmark data sets. Experimental results show that the removal of useless input variables and hidden nodes from the model improves its generalization capability. In addition, the proposed method compares favorably with respect to other penalization methods.},
  comment   = {Backwards feature selection, kinda like automatic releveance determination but w/ different theory, I think
* could use for regime detection
* since backwards, could probably not use offsite obs selection (can't build a big enough neural net)},
  doi       = {10.1016/j.patrec.2008.09.009},
  file      = {Simila09combInSelModCmplx.pdf:Simila09combInSelModCmplx.pdf:PDF;Simila09combInSelModCmplx.pdf:Simila09combInSelModCmplx.pdf:PDF},
  location  = {New York, NY, USA},
  owner     = {sotterson},
  publisher = {Elsevier Science Inc.},
  timestamp = {2009.02.11},
}

@Article{Haralick07clustLinManLMCLUST,
  author    = {Robert M. Haralick and Rave Harpaz},
  title     = {Linear manifold clustering in high dimensional spaces by stochastic search},
  journal   = {Pattern Recognition},
  year      = {2007},
  volume    = {40},
  number    = {10},
  pages     = {2672--2684},
  abstract  = {Classical clustering algorithms are based on the concept that a cluster center is a single point. Clusters which are not compact around a
single point are not candidates for classical clustering approaches. In this paper we present a new clustering paradigm in which the cluster
center is a linear manifold. Clusters are groups of points compact around a linear manifold. A linear manifold of dimension 0 is a point.
So clustering around a center point is a special case of linear manifold clustering. Linear manifold clustering (LMCLUS) identifies subsets
of the data which are embedded in arbitrary oriented lower dimensional linear manifolds. Minimal subsets of points are repeatedly sampled
to construct trial linear manifolds of various dimensions. Histograms of the distances of the points to each trial manifold are computed. The
sampling corresponding to the histogram having the best separation between a mode near zero and the rest is selected and the data points
are partitioned on the basis of the best separation. The repeated sampling then continues recursively on each block of the partitioned data. A
broad evaluation of some 100 experiments over real and synthetic data sets demonstrates the general superiority of this algorithm over any of
the competing algorithms in terms of accuracy and computation time. Its expected computational time is linearly proportional to the data set
dimension and data set size. Its accuracy ranges from near 0.90 to 0.99 depending on the experiment and is generally much higher than the
accuracy of the competing clustering algorithms.
2007 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.
Keywords: Clustering; Linear manifold; Subspace; Histogram thresholding; Data exploration; Random projections},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comment   = {Use to pick local linear regression neighborhoods, or regime detection. Has R

R: https://github.com/wildart/lmclus/blob/master/R/lmclus.R},
  doi       = {10.1016/j.patcog.2007.01.020},
  file      = {Haralick07clustLinManLMCLUST.pdf:Haralick07clustLinManLMCLUST.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.03.31},
}

@Article{Li08quantRgrsnLasso,
  author    = {Li, Youjuan and Zhu, Ji},
  title     = {L1\mbox{-}{N}orm Quantile Regression},
  journal   = {Journal of Computational and Graphical Statistics},
  year      = {2008},
  volume    = {17},
  number    = {1},
  pages     = {163--185},
  abstract  = {Classical regression methods have focused mainly on estimating conditional mean
functions. In recent years, however, quantile regression has emerged as a comprehensive
approach to the statistical analysis of response models. In this article we consider
the L1-norm (LASSO) regularized quantile regression (L1-norm QR), which uses the
sum of the absolute values of the coefficients as the penalty. The L1-norm penalty
has the advantage of simultaneously controlling the variance of the fitted coefficients
and performing automatic variable selection. We propose an efficient algorithm that
computes the entire solution path of the L1-norm QR. Furthermore, we derive an estimate
for the effective dimension of the L1-norm QR model, which allows convenient
selection of the regularization parameter.
Key Words: Effective dimension; LASSO; Linear programming; L1-norm penalty;
Variable selection.},
  comment   = {LASSO feature selection for quantile regression. Should be good for a huge number of variables. Maybe some of the non-linear extensions to ordinary LASSO would work here. At least interaction terms or some kind of spline...},
  doi       = {10.1198/106186008X289155},
  eprint    = {http://amstat.tandfonline.com/doi/pdf/10.1198/106186008X289155},
  file      = {Li08quantRgrsnLasso.pdf:Li08quantRgrsnLasso.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.22},
}

@Article{Tasche16quantifWOadjst,
  author      = {Dirk Tasche},
  title       = {Does quantification without adjustments work?},
  abstract    = {Classification is the task of predicting the class labels of objects based on the observation of their features. In contrast, quantification has been defined as the task of determining the prevalences of the different sorts of class labels in a target dataset. The simplest approach to quantification is Classify & Count where a classifier is optimised for classification on a training set and applied to the target dataset for the prediction of class labels. In the case of binary quantification, the number of predicted positive labels is then used as an estimate of the prevalence of the positive class in the target dataset. Since the performance of Classify & Count for quantification is known to be inferior its results typically are subject to adjustments. However, some researchers recently have suggested that Classify & Count might actually work without adjustments if it is based on a classifer that was specifically trained for quantification. We discuss the theoretical foundation for this claim and explore its potential and limitations with a numerical example based on the binormal model with equal variances. In order to identify an optimal quantifier in the binormal setting, we introduce the concept of local Bayes optimality. As a side remark, we present a complete proof of a theorem by Ye et al. (2012).},
  comment     = {Very relevant to WattPlan Grid.  Predicting the number of true labels in a population could be done by predicting individual labels and summing ("Classify & Count") but this is known to be inaccurate (I think) especially if there is a dataset shift.  The results can be improved with adjustments, but one cited paper says that's not necessary.  This paper suggests that the applicatations where adjustments don't deliver better accuracy are rare.},
  date        = {2016-02-28},
  eprint      = {http://arxiv.org/abs/1602.08780v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:Tasche16quantifWOadjst.pdf:PDF},
  keywords    = {stat.ML, cs.LG, math.ST, stat.TH, 62C10},
  url         = {https://arxiv.org/abs/1602.08780},
}

@Article{Aghabozorgi15tsClustDecRvw,
  author   = {Saeed Aghabozorgi and Ali Seyed Shirkhorshidi and Teh Ying Wah},
  title    = {Time-series clustering – A decade review},
  journal  = {Information Systems},
  year     = {2015},
  volume   = {53},
  pages    = {16 - 38},
  issn     = {0306-4379},
  abstract = {Clustering is a solution for classifying enormous data when there is not any early knowledge about classes. With emerging new concepts like cloud computing and big data and their vast applications in recent years, research works have been increased on unsupervised solutions like clustering algorithms to extract knowledge from this avalanche of data. Clustering time-series data has been used in diverse scientific areas to discover patterns which empower data analysts to extract valuable information from complex and massive datasets. In case of huge datasets, using supervised classification solutions is almost impossible, while clustering can solve this problem using un-supervised approaches. In this research work, the focus is on time-series data, which is one of the popular data types in clustering problems and is broadly used from gene expression data in biology to stock market analysis in finance. This review will expose four main components of time-series clustering and is aimed to represent an updated investigation on the trend of improvements in efficiency, quality and complexity of clustering time-series approaches during the last decade and enlighten new paths for future works.},
  comment  = {Good (many cites) top level 2015 review of TS clustering.  Read before starting on MNSP load profile clustering.  Paper to read after this one: Jin17cmprClustResidEnrgy},
  doi      = {https://doi.org/10.1016/j.is.2015.04.007},
  file     = {:Aghabozorgi15tsClustDecRvw.pdf:PDF},
  keywords = {Clustering, Time-series, Distance measure, Evaluation measure, Representations},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437915000733},
}

@Article{Bubeck09knnClustArbitObj,
  author    = {Bubeck, S{\'e}bastien and Ulrike von Luxburg},
  title     = {Nearest neighbor clustering: A baseline method for consistent clustering with arbitrary objective functions},
  journal   = {The Journal of Machine Learning Research},
  year      = {2009},
  volume    = {10},
  pages     = {657--698},
  abstract  = {Clustering is often formulated as a discrete optimization problem. The objective is to find, among all partitions of the data set, the best one according to some quality measure. However, in the statistical setting where we assume that the finite data set has been sampled from some underlying space, the goal is not to find the best partition of the given sample, but to approximate the true partition of the underlying space. We argue that the discrete optimization approach usually does not achieve this goal, and instead can lead to inconsistency. We construct examples which provably have this behavior. As in the case of supervised learning, the cure is to restrict the size of the function classes under consideration. For appropriate "small" function classes we can prove very general consistency theorems for clustering optimization schemes. As one particular algorithm for clustering with a restricted function space we introduce "nearest neighbor clustering". Similar to the k-nearest neighbor classifier in supervised learning, this algorithm can be seen as a general baseline algorithm to minimize arbitrary clustering objective functions. We prove that it is statistically consistent for all commonly used clustering objective functions.},
  comment   = {Not sure, but maybe arbitrary objective function would allow incorporation of linear quantile regression. Or at least for picking neighborhoods of linear quantile regression.

Same author has earlier tutorial on spectral clustering VonLuxburg07specClustTut, so maybe KNN clustering is not obsolete?},
  file      = {Bubeck09knnClustArbitObj.pdf:Bubeck09knnClustArbitObj.pdf:PDF},
  owner     = {sotterson},
  publisher = {JMLR. org},
  timestamp = {2014.04.03},
  url       = {http://dl.acm.org/citation.cfm?id=1577092},
}

@Article{Simon06UnfoldMeaningTimeSeriesClust,
  author    = {Geoffroy Simon and John A. Lee and Michel Verleysen},
  title     = {Unfolding preprocessing for meaningful time series clustering},
  journal   = {Neural Networks},
  year      = {2006},
  volume    = {19},
  number    = {6},
  pages     = {877--888},
  issn      = {0893-6080},
  abstract  = {Clustering methods are commonly applied to time series, either as a preprocessing stage for other methods or in their own right. In this paper, it is explained why time series clustering may sometimes be considered as meaningless. This problematic situation is illustrated for various raw time series. The unfolding preprocessing methodology is then introduced. The usefulness of unfolding preprocessing is illustrated for various time series. The experimental results show the meaningfulness of the clustering when applied on adequately unfolded time series.},
  comment   = {Subsampling time window to min. temporal correlation makes clusters based on SOM more distinct for distinct generative models * this is a paper on picking the embedding lag; seems like dynamical systems papers in physics picked this w/ fancier methods (MI?) * good refs for how to pick sliding window length for prediction (p. 878, LOOK THESE UP) Rebuts some claim that time series clustering is meaningless * clustering algoritthm is SOM, which is just a template matching algorithm * for smooth, short sequences SOM clusters are meaningless: -- similar models, regardless of generative model b/c of strong temporal correlations * not clear if these results apply to some other algorithm, for example a cosine freq clusterer, which would be very smooth but the clusters of which could still have very different params * picking the right lag -- some have used 1\textsuperscript{st} zero of autocorrelation function (min correlation) ---- only works for linear stuff ---- only works for comparing two scalars -- they propose to pick lag that maximizes "distance from main diagonal" ---- is kind of like "distance from perfectly correlated" ---- works for N-dim variables ---- is still linear-based, but probably works OK for their definition of "meaningful" ------ is avg min distance between templates ------ there's no prediction involved. ---- IDEA: why not instead minimize Krakov multi-dim mutual information? Does not directly address prediction * Seems to assert that clusters picked w/ this method would work better for prediction (p. 887 but is this true?) * IDEA: Peng mRMR-like algorithm which figures out num. of models, lag length by maximizing MI(in,out,lag) - MI(cluster models,lag)},
  doi       = {10.1016/j.neunet.2006.05.020},
  file      = {:Simon06UnfoldMeaningTimeSeriesClust.pdf:PDF;Simon06UnfoldMeaningTimeSeriesClust.pdf:Simon06UnfoldMeaningTimeSeriesClust.pdf:PDF},
  groups    = {Read},
  location  = {Oxford, UK, UK},
  owner     = {sotterson},
  publisher = {Elsevier Science Ltd.},
  timestamp = {2008.12.12},
  url       = {http://www.dice.ucl.ac.be/~verleyse/papers/neuralnetworks06gs.pdf},
}

@INCOLLECTION{Mylonas04knnFeatSelClust,
  Author                   = {Mylonas, Phivos and Wallace, Manolis and Kollias, Stefanos},
  Title                    = {Using k-nearest neighbor and feature selection as an improvement to hierarchical clustering},
  Booktitle                = {Methods and Applications of Artificial Intelligence},
  Publisher                = {Springer},
  Year                     = {2004},
  Pages                    = {191--200},
  Abstract                 = {Clustering of data is a difficult problem that is related to various fields and applications. Challenge is greater, as input space dimensions become larger and feature scales are different from each other. Hierarchical clustering methods are more flexible than their partitioning counterparts, as they do not need the number of clusters as input. Still, plain hierarchical clustering does not provide a satisfactory framework for extracting meaningful results in such cases. Major drawbacks have to be tackled, such as curse of dimensionality and initial error propagation, as well as complexity and data set size issues. In this paper we propose an unsupervised extension to hierarchical clustering in the means of feature selection, in order to overcome the first drawback, thus increasing the robustness of the whole algorithm. The results of the application of this clustering to a portion of dataset in question are then refined and extended to the whole dataset through a classification step, using k-nearest neighbor classification technique, in order to tackle the latter two problems. The performance of the proposed methodology is demonstrated through the application to a variety of well known publicly available data sets.},
  DOI                      = {10.1007/978-3-540-24674-9_21},
  File                     = {Mylonas04knnFeatSelClust.pdf:Mylonas04knnFeatSelClust.pdf:PDF},
  Owner                    = {sotterson},
  Timestamp                = {2014.05.02}
}

@Article{Still04numClustInfo,
  author    = {Still, Susanne and Bialek, William},
  title     = {How Many Clusters? An Information-Theoretic Perspective.},
  year      = {2004},
  volume    = {16},
  number    = {12},
  pages     = {2483--2506},
  issn      = {0899-7667},
  url       = {http://search.ebscohost.com.globalproxy.cvt.dk/login.aspx?direct=true&db=afh&AN=14848270&site=ehost-live},
  abstract  = {Clustering provides a common means of identifying structure in complex data, and there is renewed interest in clustering as a tool for the analysis of large data sets in many fields. A natural question is how many clusters are appropriate for the description of a given system. Traditional approaches to this problem are based on either a framework in which clusters of a particular shape are assumed as a model of the system or on a two-step procedure in which a clustering criterion determines the optimal assignments for a given number of clusters and a separate criterion measures the goodness of the classification to determine the number of clusters. In a statistical mechanics approach, clustering can be seen as a trade-off between energy- and entropy-like terms, with lower temperature driving the proliferation of clusters to provide a more detailed description of the data. For finite data sets, we expect that there is a limit to the meaningful structure that can be resolved and therefo},
  file      = {Still04numClustInfo.pdf:Still04numClustInfo.pdf:PDF},
  journal   = {Neural Computation},
  keywords  = {CLUSTER analysis (Statistics), SAMPLING (Statistics), TEMPERATURE, STATISTICS, STATISTICAL mechanics, ENTROPY},
  owner     = {sotterson},
  timestamp = {2011.11.17},
}

@Article{Ombao08evCoherNonStat,
  author    = {Ombao, H. and Van Bellegem, S.},
  title     = {Evolutionary Coherence of Nonstationary Signals},
  journal   = {Signal Processing, IEEE Transactions on},
  year      = {2008},
  volume    = {56},
  number    = {6},
  pages     = {2259--2266},
  month     = jun,
  issn      = {1053-587X},
  abstract  = {Coherence is a widely used measure for characterizing linear dependence between a pair of signals. For nonstationary signals, the autospectrum, cross spectrum, and coherence between signals may evolve over time. A standard approach is to divide the signals into overlapping blocks of fixed width and then smooth (over frequency) the periodogram matrix at each time block. In this paper, a consistent estimation procedure is developed using time-localized linear filtering. The proposed method automatically selects, via repeated tests of homogeneity, the optimal window width for estimating local coherence. It is pointwise adaptive in the sense that the width of the optimal interval is allowed to change across time. Under the locally stationary process framework, we develop a central limit theorem on the Fisher-z transform of our time-localized band coherence. We apply our method to a pair of highly dynamic brain waves signals whose coherence is shown to evolve during an epileptic seizure.},
  comment   = {Estimates coherence w/ adaptive analysis window, based on minRMSE.


Use for regime boundaries?
* window sel. is probably the interesting part, but it's not very surprising...
* lag relationships not captured so really is just inst. corr. in some freq. band
-- they say that could capture lag relationships (p. 2261) but don't show how
* is between pairs of signals (or pairs, given a 3\textsuperscript{rd}, if partial coherence)

* intended for narrow, single band coherence (not useful for broadband?)

* is point-wise, w/ flexible analysis window, no piecewise stationarity assumption
* optimal window width for coherence is automatically chosen
-- window is chosen for every point, is asymmetric about point of interest
-- window goal is time region w/ constant (stationary) coherence
-- a central limit theorem method (eq. 8) is over-conservative
-- so, develops data-driven approach which minimizes linear prediction error
-- basically, expand for min RMSE, not clear if it's a bruteforce search between all endpoint combos.
* done in coherence space, so picks all lags
* can do online
-- requires complex filter
-- but not fully developed
* can be generalized to partial coherence, if want to measure changes in that},
  doi       = {10.1109/TSP.2007.914341},
  file      = {Ombao08evCoherNonStat.pdf:Ombao08evCoherNonStat.pdf:PDF},
  keywords  = {EEG signal;Fisher-z transform;consistent estimation procedure;dynamic brain wave signal;evolutionary coherence;nonstationary signal;periodogram matrix smoothing;time-localized linear filtering;brain;electroencephalography;estimation theory;medical signal processing;neurophysiology;smoothing methods;transforms;},
  owner     = {scot},
  timestamp = {2011.03.08},
}

@Article{Tian10elmAdaboost_RT,
  author    = {Hui-Xin Tian and Zhi-zhong Mao},
  title     = {An Ensemble ELM Based on Modified AdaBoost.RT Algorithm for Predicting the Temperature of Molten Steel in Ladle Furnace},
  journal   = {Automation Science and Engineering, IEEE Transactions on},
  year      = {2010},
  volume    = {7},
  number    = {1},
  pages     = {73--80},
  issn      = {1545-5955},
  abstract  = {Combined the modified AdaBoost.RT with extreme learning machine (ELM), a new hybrid artificial intelligent technique called ensemble ELM is developed for regression problem in this study. First, a new ELM algorithm is selected as ensemble predictor due to its rapid speed and good performance. Second, a modified AdaBoost.RT is proposed to overcome the limitation of original AdaBoost.RT by self-adaptively modifying the threshold value. Then, an ensemble ELM is presented by using the modified AdaBoost.RT for better accuracy of predictability than individual method. Finally, this new hybrid intelligence method is used to establish a temperature prediction model of molten steel by analyzing the metallurgic process of ladle furnace (LF). The model is examined by data of production from 300t LF in Baoshan Iron and Steel Co., Ltd. and compared with the models that established by single ELM, GA-BP (combined genetic algorithm with BP network), and original AdaBoost.RT. The experiments demonstrated that the hybrid intelligence method can improved generalization performance and boost the accuracy, and the accuracy of the temperature prediction is satisfied for the process of practical producing.},
  comment   = {an improved version of Adaboost.RT (Solomatine04adaBoostContin) w/ adaptive thresholds, applied to process estimation ensemble regression

Tian09newAdaboost_RT seems to be the same idea},
  doi       = {10.1109/TASE.2008.2005640},
  file      = {Tian10elmAdaboost_RT.pdf:Tian10elmAdaboost_RT.pdf:PDF},
  keywords  = {artificial intelligence;regression analysis;ensemble ELM based;extreme learning machine;genetic algorithm BP network;hybrid artificial intelligent technique;hybrid intelligence method;improved generalization performance;metallurgic process ladle furnace;modified AdaBoost.RT algorithm;molten steel ladle furnace;practical producing process;rapid speed ensemble predictor;regression problem studies;self adaptively modifying threshold value;temperature prediction;temperature prediction model;AdaBoost.RT;ensemble algorithm;extreme learning machine (ELM);ladle furnace;self-adaptive},
  owner     = {sotterson},
  timestamp = {2014.01.21},
}

@InCollection{Barutccuouglu03rgrsBagVsAdaBoost,
  author    = {Barut{\c{c}}uo{\u{g}}lu, Zafer and Alpayd{\i}n, Ethem},
  title     = {A comparison of model aggregation methods for regression},
  booktitle = {Artificial Neural Networks and Neural Information Processing?ICANN/ICONIP 2003},
  publisher = {Springer},
  year      = {2003},
  pages     = {76--83},
  abstract  = {Combining machine learning models is a means of improving overall
accuracy.Various algorithms have been proposed to create aggregate models from
other models, and two popular examples for classification are Bagging and AdaBoost.
In this paper we examine their adaptation to regression, and benchmark
them on synthetic and real-world data. Our experiments reveal that different types
of AdaBoost algorithms require different complexities of base models. They outperform
Bagging at their best, but Bagging achieves a consistent level of success
with all base models, providing a robust alternative.},
  comment   = {Adaboost for continuous regression vs. bagging (bagging seems comparatively good).

Useful for continous adaboost ideas spawned off in Lillywhite13featCnstrct},
  file      = {Barutccuouglu03rgrsBagVsAdaBoost.pdf:Barutccuouglu03rgrsBagVsAdaBoost.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.01.21},
}

@Article{Francois07resampParamFreeFeatSel,
  author    = {D. Fran{\cc}ois and F. Rossi and V. Wertz and M. Verleysen},
  title     = {Resampling methods for parameter-free and robust feature selection with mutual information},
  journal   = {Neurocomputing},
  year      = {2007},
  volume    = {70},
  number    = {7-9},
  abstract  = {Combining the mutual information criterion with a forward feature selection strategy offers a good trade-off between optimality of the selected feature subset and computation time. However, it requires to set the parameter(s) of the mutual information estimator and to determine when to halt the forward procedure. These two choices are difficult to make because, as the dimensionality of the subset increases, the estimation of the mutual information becomes less and less reliable. This paper proposes to use resampling methods, a Kfold cross-validation and the permutation test, to address both issues. The resampling methods bring information about the variance of the estimator, information which can then be used to automatically set the parameter and to calculate a threshold to stop the forward procedure. The procedure is illustrated on a synthetic data set as well as on the real-world examples.},
  comment   = {Mutual info feature selection with KNN k selection and stopping via cross validation and permutation test
* a pretty good paper w/ thorough tests Forward feature selection
-- faster than wrapper
-- MI is calculated jointly across already-selected features, so catches interdependencies missed by relevance ranking
-- shows how single feature ranking can fail w/ examples (also how getting the right KNN k is important) Picking best k for KNN mutual information estimator
* increasing k generally decreases mut info variance but also blurs distinction between good and bad features
* k is chosen once at start of feature selection process and then fixed

* pick k that best separates MI estimate for most meaninful distribution from its null hypothesis (random permutation)
* use student-like measure (tik) which uses estimates of MI estimator variance and mean
-- MI estimator mean/var: divide dat into K groups (20-30) and calc MI on each, leaving one group out
-- MI calcs are done for individual features in normal order and permuted (an irrelevant variable hypothesis)
-- Best k is the one yielding max tik across all single features, i
---- Does focus on max risk causing problem w/ low relevance variables that are as significant?
* notes that bootstrap resampling screws up knn MI est! So don't use it
-- uses "K-fold sampling", which seems like a naive form of susampling bootstrap (size choice seems arbitrary. p. 1278)
* also notes that the MI estimator has a dimension-dependent bias, so must compare same-sized dimensions.

Feature selection stopping
* when to stop the iterations?
* theoretically may not need to do this, but need trick to avoid artifiacts in KNN mut info estimator
* mut info does not generally drop when start picking irrelevant features
* could try permutation in Lizier12compNetsTransEnt or CMI ratio in Vlachos10nonUnifStSpcMI

* comparing mut information across dimension d and dimension d+1 is tricky (but could use dim scale in Stogbauer04leastDepMutInfo? Given the plots in this paper, my guess is not)
* find next feauture that would add most MI
* stop if < 95\% certainty (p-value) that an irrelevant (this one, permuted) feature would not add as much MI
-- significance test is explained in Francois06permTestMutInf Could use same alg w/ other relevance measures e.g.
* gamma: Jones04newToolsNLpred, Sorjamaa07methLongTermPred
* quadratic mutual information: Hild06featExtractInfoLrn, Xu03recursEntEstAdaptFIlt Results
* toy example: picks right features better than max MI or using all features
* real data w/ RBFN regression: does as well or sometimes much better than max MI or all features},
  file      = {:Francois07resampParamFreeFeatSel.pdf:PDF;Francois07resampParamFreeFeatSel.pdf:Francois07resampParamFreeFeatSel.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  page      = {1265-1275},
  publisher = {Elsevier},
  timestamp = {2009.01.28},
}

@Article{Khan07perfMutInfoShortNoise,
  author    = {Khan, S. and Bandyopadhyay, S. and Ganguly, A.~R. and Saigal, S. and Erickson, III, D.~J. and Protopopescu, V. and Ostrouchov, G.},
  title     = {Relative performance of mutual information estimation methods for quantifying the dependence among short and noisy data},
  journal   = {Physical Review E},
  year      = {2007},
  volume    = {76},
  number    = {2},
  month     = aug,
  abstract  = {Commonly used dependence measures, such as linear correlation, cross-correlogram, or Kendall's , cannot capture the complete dependence structure in data unless the structure is restricted to linear, periodic, or monotonic. Mutual information MI has been frequently utilized for capturing the complete dependence structure including nonlinear dependence. Recently, several methods have been proposed for the MI estimation, such as kernel density estimators KDEs, k-nearest neighbors KNNs, Edgeworth approximation of differential entropy, and adaptive partitioning of the XY plane. However, outstanding gaps in the current literature have precluded the ability to effectively automate these methods, which, in turn, have caused limited adoptions by the application communities. This study attempts to address a key gap in the literature? specifically, the evaluation of the above methods to choose the best method, particularly in terms of their robustness for short and noisy data, based on comparisons with the theoretical MI estimates, which can be computed analytically, as well with linear correlation and Kendall?s . Here we consider smaller data sizes, such as 50, 100, and 1000, and within this study we characterize 50 and 100 data points as very short and 1000 as short. We consider a broader class of functions, specifically linear, quadratic, periodic, and chaotic, contaminated with artificial noise with varying noise-to-signal ratios. Our results indicate KDEs as the best choice for very short data at relatively high noise-to-signal levels whereas the performance of KNNs is the best for very short data at relatively low noise levels as well as for short data consistently across noise levels. In addition, the optimal smoothing parameter of a Gaussian kernel appears to be the best choice for KDEs while three nearest neighbors appear optimal for KNNs. Thus, in situations where the approximate data sizes are known in advance and exploratory data analysis and/or domain knowledge can be used to provide a priori insights into the noise-to-signal ratios, the results in the paper point to a way forward for automating the process of MI estimation.},
  adsnote   = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl    = {http://adsabs.harvard.edu/abs/2007PhRvE..76b6209K},
  comment   = {Kernel (KDE) and KNN mutual information estimators are better than other measures for capturing dependence structure. KDE best for combo of noise and very small sample sizes (1D) ; otherwise, KNN best.

* compared: KNN, KDE, linear correlation, rank-based correlation (nonlinear) from Kendall's tau
* data length: 50, 100 samples (very short) and 1000 (short)
* apparently one dimensional variables ?
* for k nearest neighbor (KNN), they find that k=3 is best at all noise/ levels for short and very short data (they say Kraskow used k=2..4)
* "Small values of k lead to small bias and large variance whereas large k results in large bias and small variance."
* Also some hints about how to pick KDE MI estimator parameters
* KDE best for very short, very noisy
* KNN best for very short, not noisy and for short, any noise level},
  doi       = {10.1103/PhysRevE.76.026209},
  file      = {Khan07perfMutInfoShortNoise.pdf:Khan07perfMutInfoShortNoise.pdf:PDF;Khan07perfMutInfoShortNoise.pdf:Khan07perfMutInfoShortNoise.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.11.26},
  url       = {http://scitation.aip.org.offcampus.lib.washington.edu/getabs/servlet/GetabsServlet?prog=normal&id=PLEEE8000076000002026209000001&idtype=cvips&gifs=yes},
}

@Article{Powers11evalPrecRecFrocEtc,
  author    = {Powers, David Martin},
  title     = {Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation},
  journal   = {Journal of Machine Learning Technologies},
  year      = {2011},
  volume    = {2},
  number    = {1},
  abstract  = {Commonly used evaluation measures including Recall, Precision, F-Measure and Rand Accuracy are
biased and should not be used without clear understanding of the biases, and corresponding identification of chance
or base case levels of the statistic. Using these measures a system that performs worse in the objective sense of
Informedness, can appear to perform better under any of these commonly used measures. We discuss several
concepts and measures that reflect the probability that prediction is informed versus chance. Informedness and
introduce Markedness as a dual measure for the probability that prediction is marked versus chance. Finally we
demonstrate elegant connections between the concepts of Informedness, Markedness, Correlation and Significance
as well as their intuitive relationships with Recall and Precision, and outline the extension from the dichotomous case
to the general multi-class case.
Keywords ? Recall and Precision, F-Measure, Rand Accuracy, Kappa, Informedness and Markedness, DeltaP,
Correlation, Significance.},
  comment   = {Classifier performance measures.  Includes F-measure and area-under-curve things.

See also: Fawcett06introROCanalysis},
  file      = {Powers11evalPrecRecFrocEtc.pdf:Powers11evalPrecRecFrocEtc.pdf:PDF},
  owner     = {sotterson},
  publisher = {Bioinfo Publications},
  timestamp = {2017.01.16},
  url       = {http://www.bioinfo.in/contents.php?id=51},
}

@InProceedings{Zhang03RgrsnClust,
  author    = {Zhang, B.},
  title     = {Regression clustering},
  booktitle = {Third IEEE International Conference on Data Mining},
  year      = {2003},
  pages     = {451--458},
  abstract  = {Complex distribution in real-world data is often modeled by a mixture of simpler distributions. Clustering is one of the tools to reveal the structure of this mixture. The same is true to the datasets with chosen response variables that people run regression on. Without separating the clusters with very different response properties, the residue error of the regression is large. Input variable selection could also be misguided to a higher complexity by the mixture. In regression clustering (RC), K (>1) regression functions are applied to the dataset simultaneously which guide the clustering of the dataset into K subsets each with a simpler distribution matching its guiding function. Each function is regressed on its own subset of data with a much smaller residue error. Both the regressions and the clustering optimize a common objective function. We present a RC algorithm based on K-harmonic means clustering algorithm and compare it with other existing RC algorithms based on K-means and EM.},
  comment   = {use for local linear neighbhorhood selection, etc.},
  doi       = {10.1109/ICDM.2003.1250952},
  file      = {Zhang03RgrsnClust.pdf:Zhang03RgrsnClust.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.03.31},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250952},
}

@Article{Shampine07AccNumerDiffMatlab,
  author    = {Shampine, Lawrence F},
  title     = {Accurate numerical derivatives in MATLAB},
  journal   = {ACM Transactions on Mathematical Software (TOMS)},
  year      = {2007},
  volume    = {33},
  number    = {4},
  pages     = {26},
  abstract  = {Complex step differentiation (CSD) is a technique for computing very accurate numerical deriva-
tives in languages that support complex arithmetic. We describe here the development of a CSD
package in MATLAB called PMAD. We have extended work done in other languages for scalars to
the arrays that are fundamental to MATLAB. This extension raises questions that we have been
able to resolve in a satisfactory way. Our goal has been to make it as easy as possible to compute ap-
proximate Jacobians in MATLAB that are all but exact. Although PMAD has a fast option for the
expert that implements CSD as in previous work, the default is an object-oriented implementation
that asks very little of the user.
Categories and Subject Descriptors: G.1.4 [Numerical Analysis]: Quadrature and Numerical Dif-
ferentiation?Automatic differentiation
General Terms: Algorithms
Additional Key Words and Phrases: AD, complex step differentiation, MATLAB},
  comment   = {How to do complex step differentiation (CSD), and a description of the Matlab PMAD library that does it. CSD is a simple, accurate way to calculate partial derivatives numerically -- the only catch is that the function the derivative is being calculated on has to work with complex inputs. This is no problem for polynomials functions like splines (if it's even needed). For trickier functions, PMAD has a special library that might be helpful.

Looking at the PMAD code: here are some of the important functions that are different from Matlab's default:
 Cmax(X) = _ complex_ value that has max(real(X))
 Cmin(X) = _ complex_ value that has min(real(X))
 a < b = real(a) < real(b)
 a > b = real(a) > real(b)

The second attached article is by Cleve Moler, one of the inventors of the complex step differentiation idea.

Accuracy shown here: Martins03cmplxStepDerivCSD
Original modern algorithm: Squire98cmplxDerivsCSD

PMAD is available in SendOut.zip, here:
http://faculty.smu.edu/shampine/current.html

Other related matlab (not free): matlabAD library:
http://matlabad.com/

In evernote, see:

https://www.evernote.com/Home.action?csrfBusterToken=U%3D173e13%3AP%3D%2F%3AE%3D1469c2d1bee%3AS%3De3402a635e7ae60712a43adbaf08f7cf#st=p&n=62becee0-ce65-4777-a1b8-506fac155a82

and

https://www.evernote.com/Home.action?csrfBusterToken=U%3D173e13%3AP%3D%2F%3AE%3D1469c2d1bee%3AS%3De3402a635e7ae60712a43adbaf08f7cf#st=p&n=2e9f8e44-edcf-43ba-b23d-29975c9f79ad},
  file      = {The PMAD library:Shampine07AccNumerDiffMatlab.pdf:PDF;Matlab Blog Post\: Cleve Moler (orig author):Shampine07AccNumerDiffMatlab_AuthBlog.pdf:PDF},
  owner     = {sotterson},
  publisher = {ACM},
  timestamp = {2014.06.14},
  url       = {http://dl.acm.org/citation.cfm?id=1268781},
}

@Article{Lizier14JIDTinfoToolkit,
  author   = {Lizier, Joseph Troy},
  title    = {JIDT: An information-theoretic toolkit for studying the dynamics of complex systems},
  journal  = {Frontiers in Robotics and AI},
  year     = {2014},
  volume   = {1},
  number   = {11},
  issn     = {2296-9144},
  abstract = {Complex systems are increasingly being viewed as distributed information processing systems, particularly in the domains of computational neuroscience, bioinformatics, and artificial life. This trend has resulted in a strong uptake in the use of (Shannon) information-theoretic measures to analyze the dynamics of complex systems in these fields. We introduce the Java Information Dynamics Toolkit (JIDT): a Google code project, which provides a standalone (GNU GPL v3 licensed) open-source code implementation for empirical estimation of information-theoretic measures from time-series data. While the toolkit provides classic information-theoretic measures (e.g., entropy, mutual information, and conditional mutual information), it ultimately focuses on implementing higher-level measures for information dynamics. That is, JIDT focuses on quantifying information storage, transfer, and modification, and the dynamics of these operations in space and time. For this purpose, it includes implementations of the transfer entropy and active information storage, their multivariate extensions and local or pointwise variants. JIDT provides implementations for both discrete and continuous-valued data for each measure, including various types of estimator for continuous data (e.g., Gaussian, box-kernel, and Kraskov Stoegbauer Grassberger), which can be swapped at run-time due to Javas object-oriented polymorphism. Furthermore, while written in Java, the toolkit can be used directly in MATLAB, GNU Octave, Python, and other environments. We present the principles behind the code design, and provide several examples to guide users.

Keywords: information transfer, information transfer entropy, Java, MATLAB},
  comment  = {A Java and Matlab libraray with a ton of mutual information functions, including conditional MI and Kraskov.   Also has a great table (Table 1) summarizing all the MI flavors.

Could use for feature selection.  Also the supplemental materials (Lizier14JIDTinfoToolkit_SupMat) contain a good overview of the kinds of MI, including an explanation of how Kraskov04EstMutInfKNN conditional MI approx. was improved for condittional MI in Frenzel07partMutInfo
 (but Frenzel* calls it "partial mutual info," and has a ref. saying PMI and cond. MI are not the same)

* Kraskov KSG (KNN) technique is best for PMI naive alg. here requires O(KN^2) but sophisticated algs does it in O(KNlogN).
* but can get the fast KNN search in hte project's SVN,and it will be in future releases (is Matlab builtin knnsearch good enough?
* have moved to http://jlizier.github.io/jidt/
* Supplementary material (bibtex key: Lizier14JIDTinfoToolkit_SupMat) has more theory, implementation details
* tutorial slides attached.  Also says Kraskov is "best of breed"
* GUI generates Matlab, Octave, Python, R, Julia and Clojure code for you (see slides)
* FaultMap uses JIDT: https://github.com/SimonStreicher/FaultMap
* publication using this toolkit (use to find related work): https://github.com/jlizier/jidt/wiki/PublicationsUsingThisToolkit
* implements Frenzel07partMutInfo
* Mixed discrete/continuous MI/PMI are experimental (need them because of Ross14MutInfoCntnAndDscrt)
* Vlachos10nonUnifStSpcMI may be PMI/CMI methond 2 may be better than Frenzel07: what does this toolkit do?
* my discussion with the author about implementing a stopping criterion
http://tinyurl.com/jy4b6jv

List of other toolkits
* TRENTOOL
* MuTE
* TET
* MILcA
* TIM
* MVGC

Actually, the Kraskov cond, mut. info estimate mentioned might have been:
Kraskov, A. (2004), Synchronization and Interdependence Measures and their Applications to the
427 Electroencephalogram of Epilepsy Patients and Clustering of Data, volume 24 of Publication Series
of the John von Neumann Institute for Computing (John von Neumann Institute for Computing, Julich, Germany)},
  doi      = {10.3389/frobt.2014.00011},
  file     = {Paper:Lizier14JIDTinfoToolkit.pdf:PDF;Tutorial Slides:Lizier14JIDTinfoToolkit_slides.pdf:PDF},
  url      = {http://jlizier.github.io/jidt/},
}

@Misc{Karpathy16convNNcourseNotes,
  author    = {Andrej Karpathy},
  title     = {Course Notes for CS231n: Convolutional Neural Networks for Visual Recognition. Stanford University},
  month     = jan,
  year      = {2016},
  abstract  = {Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka ?deep learning?) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This course is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. During the 10-week course, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision. The final assignment will involve training a multi-million parameter convolutional neural network and applying it on the largest image classification dataset (ImageNet). We will focus on teaching how to set up the problem of image recognition, the learning algorithms (e.g. backpropagation), practical engineering tricks for training and fine-tuning the networks and guide the students through hands-on assignments and a final course project. Much of the background and materials of this course will be drawn from the ImageNet Challenge.},
  file      = {Karpathy16convNNcourseNotes.pdf:Karpathy16convNNcourseNotes.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.01.22},
  url       = {http://cs231n.stanford.edu/},
}

@Article{Geard13synthPopDyn,
  author   = {Geard, Nicholas and McCaw, James M and Dorin, Alan and Korb, Kevin B and McVernon, Jodie},
  title    = {Synthetic Population Dynamics: A Model of Household Demography},
  journal  = {Journal of Artificial Societies and Social Simulation},
  year     = {2013},
  volume   = {16},
  number   = {1},
  pages    = {8},
  issn     = {1460-7425},
  abstract = {Computer-simulated synthetic populations are used by researchers and policy makers to help understand and predict the aggregate behaviour of large numbers of individuals.  Research aims include explaining the structural and dynamic characteristics of populations, and the implications of these characteristics for dynamic processes such as the spread of disease, opinions and social norms.  Policy makers planning for the future economic, healthcare or infrastructure needs of a population want to be able to evaluate the possible effects of their policies.  In both cases, it is desirable that the structure and dynamic behaviour of synthetic populations be statistically congruent to that of real populations.  Here, we present a parsimonious individual-based model for generating synthetic population dynamics that focuses on the effects that demographic change have on the structure and composition of households.},
  comment  = {How to model population changes.  Is cited 7 times, including one (crappy, not bibtexed) paper on renewable energy potential of Russia.  Authors are from Surrey and Monash (usually good stuff).},
  doi      = {10.18564/jasss.2098},
  file     = {:Geard13synthPopDyn.pdf:PDF},
  keywords = {Demography, Synthetic Populations, Household Dynamics, Individual-Based Models},
  url      = {http://jasss.soc.surrey.ac.uk/16/1/8.html},
}

@Article{Gama14conceptDrftSurvey,
  author    = {Gama, Jo{\~a}o and {\v{Z}}liobait{\.e}, Indr{\.e} and Bifet, Albert and Pechenizkiy, Mykola and Bouchachia, Abdelhamid},
  title     = {A survey on concept drift adaptation},
  journal   = {ACM Computing Surveys (CSUR)},
  year      = {2014},
  volume    = {46},
  number    = {4},
  pages     = {44},
  abstract  = {Concept drift primarily refers to an online supervised learning scenario when the relation
put data and the target variable changes over time. Assuming a general knowledge of supervised
in this paper we characterize adaptive learning process, categorize existing strategies for
drift, discuss the most representative, distinct and popular techniques and algorithms,
methodology of adaptive algorithms, and present a set of illustrative applications. This
concept drift adaptation presents the state of the art techniques and a collection of benchmarks
searchers, industry analysts and practitioners. The survey aims at covering the different
drift in an integrated way to reflect on the existing scattered state-of-the-art.
Categories and Subject Descriptors: I.2.6 [Artificial Intelligence]: Learning
General Terms: Design, Algorithms, Performance
Additional Key Words and Phrases: concept drift, change detection, adaptive learning},
  comment   = {Related to anomaly detection},
  file      = {paper:Gama14conceptDrftSurvey.pdf:PDF},
  publisher = {ACM},
}

@TechReport{Zliobaite09lrnCncptDrift,
  author      = {{\v{Z}}liobait{\.e}, I.},
  title       = {Learning under Concept Drift: an Overview},
  institution = {Vilnius University, Faculty of Mathematics and Informatics},
  year        = {2009},
  abstract    = {Concept drift refers to a non stationary learning problem over time. The training and the application data often mismatch in real life problems [61]. In this report we present a context of concept drift problem 1. We focus on the issues relevant to adaptive training set formation. We present the framework and terminology, and formulate a global picture of concept drift learners design. We start with formalizing the framework for the concept drifting data in Section 1. In Section 2 we discuss the adaptivity mechanisms of the concept drift learners. In Section 3 we overview the principle mechanisms of concept drift learners. In this chapter we give a general picture of the available algorithms and categorize them based on their properties. Section 5 discusses the related research elds and Section 5 groups and presents major concept drift applications. This report is intended to give a bird's view of concept drift researcheld, provide a context of the research and position it within broad spectrum of researchelds and applications.},
  comment     = {overview, relevant to adaptivity and streaming feature selection He also has a later report or paper for concept drift w/ ensembles

Might be good for ensemble forecasting with analogs.},
  file        = {Zliobaite09lrnCncptDrift.pdf:Zliobaite09lrnCncptDrift.pdf:PDF},
  groups      = {Ensemble, doReadNonWPV_1},
  owner       = {scot},
  timestamp   = {2010.09.27},
  url         = {http://sites.google.com/site/zliobaite/publications},
}

@Book{Dubitzky07datMinGenBook,
  title     = {Fundamentals of data mining in genomics and proteomics},
  publisher = {Springer},
  year      = {2007},
  author    = {Dubitzky, Werner and Granzow, Martin and Berrar, Daniel P},
  abstract  = {Conclusing paragraphs of Preface:
This volume comprises 12 chapters, which follow a similar structure in
terms of the main sections. The centerpiece of each chapter represents a case
study that demonstrates the use - and misuse - of the presented method or
approach. The first chapter provides a general introduction to the field of data
mining in genomics and proteomics. The remaining chapters are intended to
shed more light on specific methods or approaches.
The second chapter focuses on study design principles and discusses replication,
blocking, and randomization. While these principles are presented in
the context of microarray experiments, they are applicable to many types of
experiments.
Chapter 3 addresses data pre-processing in cDNA and oligonucleotide microarrays.
The methods discussed include background intensity correction,
data normalization and transformation, how to make gene expression levels
comparable across different arrays, and others.
Chapter 4 is also concerned with pre-processing. However, the focus is
placed on high-throughput mass spectrometry data. Key topics include baseline
correction, intensity normalization, signal denoising (e.g., via wavelets),
peak extraction, and spectra alignment.
Data visualization plays an important role in exploratory data analysis.
Generally, it is a good idea to look at the distribution of the data prior
to analysis. Chapter 5 revolves around visualization techniques for highdimensional
data sets, and puts emphasis on multi-dimensional scaling. This
technique is illustrated on mass spectrometry data.
Chapter 6 presents the state of the art of clustering techniques for discovering
groups in high-dimensional data. The methods covered include hierarchical
and fc-means clustering, self-organizing maps, self-organizing tree algorithms,
model-based clustering, and cluster validation strategies, such as functional
interpretation of clustering results in the context of microarray data.
Chapter 7 addresses the important topics of feature selection, feature
weighting, and dimension reduction for high-dimensional data sets in genomics
and proteomics. This chapter also includes statistical tests (parametric or nonparametric)
for assessing the significance of selected features, for example,
based on random permutation testing.
Since data sets in genomics and proteomics are usually relatively small
with respect to the number of samples, predictive models are frequently tested
based on resampled data subsets. Chapter 8 reviews some common data
resampling strategies, including n-fold cross-validation, leave-one-out crossvalidation,
and repeated hold-out method.
Chapter 9 discusses support vector machines for classification tasks, and
illustrates their use in the context of mass spectrometry data.
Chapter 10 presents graphs and networks in genomics and proteomics, such
as biological networks, pathways, topologies, interaction patterns, gene-gene
interactome, and others.
Chapter 11 concentrates on time series analysis in genomics. A methodology
for identifying important predictors of time-varying outcomes is presented.
The methodology is illustrated in a study aimed at finding mutations of the
human immunodeficiency virus that are important predictors of how well a
patient responds to a drug regimen containing two different antiretroviral
drugs.
Automated extraction of information from biological literature promises
to play an increasingly important role in text-based knowledge discovery
processes. This is particularly important for high-throughput approaches such
as microarrays and high-throughput proteomics. Chapter 12 addresses knowledge
extraction via text mining and natural language processing.
Finally, we would like to acknowledge the excellent contributions of the
authors and Alice McQuillan for her help in proofreading.},
  comment   = {Genomics data mining book -- a good place to look for high dimensional techniques.},
  file      = {Dubitzky07datMinGenBook.pdf:Dubitzky07datMinGenBook.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.06.26},
  url       = {http://www.springer.com/life+sciences/systems+biology+and+bioinformatics/book/978-0-387-47508-0},
}

@Article{Lang08neuralCloundMon,
  author    = {Lang, B and Poppe, T and Minin, A and Mokhov, I and Kuperin, Y and Mekler, A and Liapakina, I},
  title     = {Neural clouds for monitoring of complex systems},
  journal   = {Optical memory \& neural networks},
  year      = {2008},
  volume    = {17},
  number    = {3},
  pages     = {183--192},
  abstract  = {Condition monitoring is an important and challenging task actual for many areas of industry,

 

medicine and economics. Nowadays it is necessary to provide on-line monitoring of the complex sys-
tems status, e.g. the steel production, in order to avoid faults, breakdowns or wrong diagnostics. In the
present paper a novel machine learning method for the automated condition monitoring is presented.
Neural Clouds (NC) is a novel data encapsulation method, which provides a confidence measure regard-
ing classification of the complex system conditions. The presented adaptive algorithm requires only the
data which corresponds to the normal system conditions, which is typically available. At the same time
the fault related data acquisition is expensive and fault modeling is not always possible, especially in
case one is dealing with steel production, power stations operation, human health condition or critical
phenomena in financial markets. These real word applications are also presented in the paper.

 

Key words: neural clouds, one side classification, on line monitoring, early fault detection, EEG classi-

 

fication, fault analysis and prevention.},
  comment   = {Related to Brummel (Siemens) patent (Brummel16monEnrgProgSiePtnt).  Was contributed by Peter Deeskow, of STEAG, who must have considered it important.},
  file      = {:Lang08neuralCloundMon.pdf:PDF},
  publisher = {Springer},
}

@TechReport{GE16predixArchSrvcTechNote,
  author      = {GE},
  title       = {Predix Architecture and Services},
  institution = {General Electric Company: GE Digital},
  year        = {2016},
  month       = nov,
  abstract    = {Connected devices and analytics have begun to influence our society 
directly. Digital consumer companies are disrupting the old guard and 
changing the way we live and do business in fundamental ways. 
Companies such as Uber, Airbnb and Zipcar have disrupted the 
traditional businesses of taxis, hotels and car rental companies by 
leveraging software capabilities to create new business models. 
Opportunities in the industrial world are expected 
to outpace consumer business cases substantially. General Electric is 
focused on driving new value for industrial organizations by offering 
them advanced software capabilities. 

At the heart of GE’s software portfolio is the Predix platform. Predix 
leapfrogs traditional enterprise IT solutions with an edge-to-cloud 
architecture that augments industrial operational technologies (OT) for 
both GE and non-GE assets. In essence, the Predix sweet spot is where 
IT and OT converge. 

The Industrial Internet of Things (IoT) differs greatly from the Internet 
of Things touted by mass media. The focus of the IIoT is not on 
connecting coffee pots to alarm clocks, but rather on connecting 
industrial assets, such as turbines, jet engines, and locomotives, to the 
cloud and to each other in meaningful ways. As a leading 
manufacturer of industrial assets, GE is in a unique position to 
leverage its understanding of asset models and industrial operations 
to create new value for industrial customers. The Predix platform 
provides a set of development tools and best practices that rapidly 
enable those customers to bridge the gap between software and 
operations to drive incredible value and innovation. 

To provide a simple yet concrete example that highlights the capabilities 
of the Predix platform, we will look at a wind turbine. The Predix Machine 
device gateway connects such assets to the Predix Cloud, regardless of 
vendor or vintage, enabling operational and historical data to be collected 
and analyzed, thereby improving operational models and potentially 
unlocking transformative business value. 

For example, with respect to predictive maintenance, Predix allows 
customers to leverage all of the components of the Predix platform to 
predict potential problems, conduct preventative maintenance, and 
reduce unplanned downtimes. The Predix Machine component can 
monitor data collected from sensors and, using physics-based analytics, 
detect potential error conditions based on the asset model, and then 
gracefully shut down the asset. In addition to these edge applications, 
Predix Machine can also pass the sensor data to the Predix Cloud, 
where the operational data for all similar machines under management 
can be stored and analyzed. Over time, data scientists can discover 
new patterns and create new and improved physics-based analytical 
models. The new analytic can then be pushed back to all of the assets, 
effectively improving the performance of all assets simultaneously. 

In the following section, this example is discussed in detail as the Predix 
architecture is examined},
  comment     = {GE's predictive maintenance whitepaper.  Relevant to ModernWindABS at least.

Also, a Utility Dive article on it here:  https://tinyurl.com/yclvntxr
},
  file        = {:GE16predixArchSrvcTechNote.pdf:PDF},
}

@Article{Stephen13windSpdDirEMmix,
  author    = {Stephen, B. and Galloway, S. and McMillan, D. and Anderson, L. and Ault, G.},
  title     = {Statistical profiling of site wind resource speed and directional characteristics},
  journal   = {IET Renewable Power Generation},
  year      = {2013},
  volume    = {7},
  number    = {6},
  pages     = {583--592},
  abstract  = {Construction of a wind farm without a reliable plant margin forecast can jeopardise potential returns on investment from the outset. Meteorological and topological factors influence the wind characteristics across any site which in turn affects wind farm output, critical for localised generation, and also the dynamic loading of the turbine structure. The models developed in this study follow the generally advocated use of probability density estimation as a means of representing wind resource characteristics but, owing to differences, in characterisation that may be encountered, do not assume a single distribution form across all sites. A mixture modelling approach is adopted that removes the need for choosing distribution forms on a site by site basis. Advancing previous work constructing statistical distributions over congruent wind speed and direction observations of the wind resource characteristics at a given site, the proposed model, as a consequence of using a mixture distribution, captures both recurring regimes in the site behaviour along with their frequency of occurrence. Preliminary results using data sets from a diverse range of locations in Scotland demonstrate the variation in the forms of model learned; comparisons of the model with current and alternate practices are given through visualisation and resource assessment illustrations.},
  comment   = {An EM mixture for determining wind speed distribution across a collection of sites that learns recurring regimes. There's also a joint speed/dir distribution estimation. This is wind speed, not power, but it seems that the EM algorithm could be somewhat easily modified by a linear transformation via spline projection. Was intended for resource assessment, not forecasting, but still it's maybe relevant for ReWP portfolio design (maybe) and for Stefan Vogt's grid based NWP probabilsitic forecast (non-prob grid: Vogt15HybridPhysMLrgnFrcst)?

This could be a way to learn a power curve.},
  doi       = {10.1049/iet-rpg.2012.0202},
  file      = {Stephen13windSpdDirEMmix.pdf:Stephen13windSpdDirEMmix.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.04.07},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6648796},
}

@Article{Kruppa13cnsmrRiskProbRFnodesize,
  author   = {Jochen Kruppa and Alexandra Schwarz and Gerhard Arminger and Andreas Ziegler},
  title    = {Consumer credit risk: Individual probability estimates using machine learning},
  journal  = {Expert Systems with Applications},
  year     = {2013},
  volume   = {40},
  number   = {13},
  pages    = {5125 - 5131},
  issn     = {0957-4174},
  abstract = {Consumer credit scoring is often considered a classification task where clients receive either a good or a bad credit status. Default probabilities provide more detailed information about the creditworthiness of consumers, and they are usually estimated by logistic regression. Here, we present a general framework for estimating individual consumer credit risks by use of machine learning methods. Since a probability is an expected value, all nonparametric regression approaches which are consistent for the mean are consistent for the probability estimation problem. Among others, random forests (RF), k-nearest neighbors (kNN), and bagged k-nearest neighbors (bNN) belong to this class of consistent nonparametric regression approaches. We apply the machine learning methods and an optimized logistic regression to a large dataset of complete payment histories of short-termed installment credits. We demonstrate probability estimation in Random Jungle, an RF package written in C++ with a generalized framework for fast tree growing, probability estimation, and classification. We also describe an algorithm for tuning the terminal node size for probability estimation. We demonstrate that regression RF outperforms the optimized logistic regression model, kNN, and bNN on the test data of the short-term installment credits.},
  comment  = {Random forest is best probabilistic algorithm for predictin individual consumer risk, beats logistic regression.

Also, Biau16randFrstGuideTour says this can pick random forest nodesize w/ simple algorithm

Biau16randFrstGuideTour also says that this paper explains how to do a kind of CV ("out-of-bag" error estimate) while doing the tree.  No separate CV step, I think.},
  doi      = {https://doi.org/10.1016/j.eswa.2013.03.019},
  file     = {:Kruppa13cnsmrRiskProbRFnodesize.pdf:PDF},
  keywords = {Probability estimation, Random forest, Credit scoring, Probability machines, Logistic regression, Machine learning},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417413001693},
}

@Article{Vrac12CopulaAnalysisMixModels,
  author   = {Vrac, M. and Billard, L. and Diday, E. and Ch{\'e}din, A.},
  title    = {Copula analysis of mixture models},
  journal  = {Computational Statistics},
  year     = {2012},
  volume   = {27},
  number   = {3},
  pages    = {427--457},
  month    = {Sep},
  issn     = {1613-9658},
  abstract = {Contemporary computers collect databases that can be too large for classical methods to handle. The present work takes data whose observations are distribution functions (rather than the single numerical point value of classical data) and presents a computational statistical approach of a new methodology to group the distributions into classes. The clustering method links the searched partition to the decomposition of mixture densities, through the notions of a function of distributions and of multi-dimensional copulas. The new clustering technique is illustrated by ascertaining distinct temperature and humidity regions for a global climate dataset and shows that the results compare favorably with those obtained from the standard EM algorithm method.

Keywords: Classification of distributions Copulas Dynamical clustering Data distributions Estimation Mixture model },
  comment  = {Possibly related to the idea of wind power regime detection, but I'm not quite sure because of some comments in the conclusion.  Anyway, clustering is somehow involved in this method, which sounds like what I want todo with clustering of prevailing wind dependence (correlation matrices).},
  day      = {01},
  doi      = {10.1007/s00180-011-0266-0},
  file     = {:Vrac12CopulaAnalysisMixModels.pdf:PDF},
  keywords = {Classification of distributions Copulas Dynamical clustering Data distributions Estimation Mixture model },
  url      = {https://doi.org/10.1007/s00180-011-0266-0},
}

@Article{Hirth13ctlPowRnwblGerman,
  author    = {Hirth, Lion and Ziegenhagen, Inka},
  title     = {Control power and variable renewables: a glimpse at German data},
  year      = {2013},
  abstract  = {Control power (regulating power, balancing power) is used to quickly restore the supply-demand balance in power systems. Variable renewable energy sources (VRE) such as wind and solar power are often thought to increase the reserve requirement significantly. This paper provides a comprehensive overview of balancing systems in Europe, discusses the role of VRE, and presents empirical market data from Germany. Despite German VRE capacity doubled during the last five years and has surpassed 70\% of peak load, contracted control power decreased by 20\%, and procurement cost fell by 50\%. Today, control power adds only 0.4\% to household electricity prices. Nevertheless, we identify several sources of inefficiency in control power markets and imbalance settlement systems and propose a number of policy changes to stimulate the participation of VRE in control provision and to improve the incentives to forecast accurately.

Keywords: Balancing Power, Control Power, Variable Renewables, Wind Power, Solar Power, Market Design},
  file      = {Hirth13ctlPowRnwblGerman.pdf:Hirth13ctlPowRnwblGerman.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.03.03},
}

@InProceedings{Sogabe16optRESdpLrn,
  author    = {T. Sogabe and H. Ichikawa and T. Sogabe and K. Sakamoto and K. Yamaguchi and M. Sogabe and T. Sato and Y. Suwa},
  title     = {Optimization of decentralized renewable energy system by weather forecasting and deep machine learning techniques},
  booktitle = {Proc. IEEE Innovative Smart Grid Technologies - Asia (ISGT-Asia)},
  year      = {2016},
  pages     = {1014--1018},
  month     = nov,
  abstract  = {Conventional electric energy can be easily adopted to a large scale by providing high quality electricity for wide-area transmissions. However, these energies are usually generated from exhaustible sources such as oil, natural gas, and coal, which are highly expensive in the long run and are the main causes of global warming. Meanwhile a large centralized energy system is more fragile and highly risky in countries like Japan where natural disasters occur frequently. A decentralized renewable energy system containing photovoltaic energy and wind power has been proposed as an alternative energy supply method. Within this system, the photovoltaic energy and wind power are well suited for the {\textquotedblleft}local production and local consumption{\textquotedblright} with domestic energy transmission and are resilient to the unexpected disasters. The challenge of forming an optimal decentralized renewable energy system is to overcome its intrinsic disadvantages such as the instability and the limit of the power output. The research in this regard has drawn a lot of attention for the past twenty years. A decentralized renewable energy optimization problem is in principle categorized as nonlinear mixed integer programing problem(NMIP). Several challenging issues still remained in finding effective solution to NMIP through mathematical optimization. For instance, there is lack of reliable method to predict the energy generation and consumption; the weak scalability to large scale system is also existed due to the limited computing resource and the algorithm which are intrinsically not suitable for high speed computing. In this work, we report on employing the deep learning artificial intelligence techniques to predict the energy consumption and power generation together with the weather forecasting numerical simulation. The prediction and optimization are further examined by a small scale decentralized verification system (i-REMS) constructed inside the University campus. a novel optimization tool platform using Boltzmann machine algorithm for NMIP problem is also proposed for better computing scalable decentralized renewable energy system.},
  comment   = {Sounds like a net demand forecast using deep learning.},
  doi       = {10.1109/ISGT-Asia.2016.7796524},
  file      = {Sogabe16optRESdpLrn.pdf:Sogabe16optRESdpLrn.pdf:PDF},
  keywords  = {Batteries, Optimization, Power generation, Prediction algorithms, Renewable energy sources, Weather forecasting, Boltzmann machine, LSTM, RNN, artificial intelligence, deep learning, optimization, prediction, renewable energy},
  owner     = {sotterson},
  timestamp = {2017.01.16},
}

@InProceedings{Long15fullConvNNsemSeg,
  author    = {J. Long and E. Shelhamer and T. Darrell},
  title     = {Fully convolutional networks for semantic segmentation},
  booktitle = {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},
  year      = {2015},
  pages     = {3431--3440},
  month     = jun,
  abstract  = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build {\textquotedblleft}fully convolutional{\textquotedblright} networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
  comment   = {Source for one ModernWindABS kickoff meeting slide.  Should also probably read, since it has a huge # of citations.},
  doi       = {10.1109/CVPR.2015.7298965},
  file      = {Long15fullConvNNsemSeg.pdf:Long15fullConvNNsemSeg.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {image classification, image segmentation, inference mechanisms, learning (artificial intelligence), NYUDv2, PASCAL VOC, SIFT flow, contemporary classification networks, fully convolutional networks, inference, learning, pixels-to-pixels, semantic segmentation, visual models, Adaptation models, Computer architecture, Convolution, Deconvolution, Image segmentation, Semantics, Training},
  owner     = {sotterson},
  timestamp = {2017.01.27},
}

@Article{Pathak15cnstrnStructCNN,
  author    = {Pathak, Deepak and Kr{\"a}henb{\"u}hl, Philipp and Yu, Stella X and Darrell, Trevor},
  title     = {Constrained structured regression with convolutional neural networks},
  journal   = {arXiv preprint arXiv:1511.07497},
  year      = {2015},
  abstract  = {Convolutional Neural Networks (CNNs) have recently emerged as the dominant
model in computer vision. If provided with enough training data, they predict
almost any visual quantity. In a discrete setting, such as classification, CNNs
are not only able to predict a label but often predict a confidence in the form of
a probability distribution over the output space. In continuous regression tasks,
such a probability estimate is often lacking. We present a regression framework
which models the output distribution of neural networks. This output distribution
allows us to infer the most likely labeling following a set of physical or modeling
constraints. These constraints capture the intricate interplay between different
input and output variables, and complement the output of a CNN. However, they
may not hold everywhere. Our setup further allows to learn a confidence with
which a constraint holds, in the form of a distribution of the constrain satisfaction.
We evaluate our approach on the problem of intrinsic image decomposition, and
show that constrained structured regression significantly increases the state-of-
the-art.},
  comment   = {A convolutional neural net whose output classes obey constraints imposed at test time.  Predicts a probability distribution for each.

Could this be used for distribution or quantile regression (output mononicity constraint) or for predictive maintenance, where the labels are maintenance failures?},
  file      = {Pathak15cnstrnStructCNN.pdf:Pathak15cnstrnStructCNN.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.03.08},
  url       = {https://arxiv.org/abs/1511.07497},
}

@Book{Cherubini04CopulaMethodsInFinance,
  title     = {Copula methods in finance},
  publisher = {John Wiley \& Sons},
  year      = {2004},
  author    = {Cherubini, Umberto and Luciano, Elisa and Vecchiato, Walter},
  abstract  = {Copula Methods in Finance is the first book to address the mathematics of copula functions illustrated with finance applications.  It explains copulas by means of applications to major topics in derivative pricing and credit risk analysis.  Examples include pricing of the main exotic derivatives (barrier, basket, rainbow options) as well as risk management issues.  Particular focus is given to the pricing of asset-backed securities and basket credit derivative products and the evaluation of counterparty risk in derivative transactions.},
}

@Article{Embrechts09copulasPers,
  author    = {Embrechts, Paul},
  title     = {Copulas: A personal view},
  journal   = {Journal of Risk and Insurance},
  year      = {2009},
  volume    = {76},
  number    = {3},
  pages     = {639--650},
  issn      = {1539-6975},
  abstract  = {Copula modeling has taken the world of finance and insurance, and well beyond, by storm. Why is this? In this paper I review the early start of this development, discuss some important current research, mainly from an applications point of view, and comment on potential future developments. An alternative title of the paper would be ?Demystifying the copula craze?. The paper also contains what I would like to call the copula must-reads. Keywords: copula, extreme value theory, Fr?echet?Hoeffding bounds, quantitative risk management, Value?at?Risk},
  comment   = {copula overview},
  file      = {Embrechts09copulasPers.pdf:Embrechts09copulasPers.pdf:PDF},
  owner     = {scot},
  publisher = {John Wiley \& Sons},
  timestamp = {2010.11.24},
}

@InBook{Schmidt07copulasCope,
  chapter   = {Coping with Copulas},
  pages     = {3--34},
  title     = {Copulas: From theory to application in finance},
  publisher = {Haymarket House},
  year      = {2007},
  author    = {Thorsten Schmidt},
  editor    = {Laurie Donaldson},
  abstract  = {Copulas are tools for modelling dependence of several random variables. The term copula was first used in the work of Sklar (1959) and is derived from the latin word copulare, to connect or to join. The main purpose of copulas is to describe the interrelation of several random variables. The outline of this chapter is as follows: ...},
  comment   = {spinning reserves. friendly copula review},
  file      = {Schmidt07copulasCope.pdf:Schmidt07copulasCope.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.12.02},
  url       = {http://www.defaultrisk.com/pp_corr_88.htm},
}

@InProceedings{Strelen09depCopula,
  author    = {Strelen, Johann Christoph},
  title     = {Tools for dependent simulation input with copulas},
  booktitle = {International Conference on Simulation Tools and Techniques},
  year      = {2009},
  series    = {Simutools '09},
  pages     = {301--307},
  address   = {ICST, Brussels, Belgium, Belgium},
  publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
  abstract  = {Copulas encompass the entire dependence structure of multivariate distributions, and not only the correlations. Together with the marginal distributions of the vector elements, they define a multivariate distribution which can be used to generate random vectors with this distribution. A toolbox is presented which implements input models with this method, for random vectors and time series. Time series are modeled with some general autoregressive processes. The copulas are estimated from observed samples of random vectors. The MATLAB tool calculates the copula, generates random vectors and time series, and provides statistics and diagrams which indicate validity and accuracy of the input model. It is fast and allows for random vectors with high dimensions, for example 100. For this efficiency an intricate data structure is essential. The generation algorithm is also implemented with Java methods.},
  acmid     = {1537654},
  articleno = {30},
  comment   = {spinning reserve. nonlinear dependence w/ time dep (I think in marginals) has matlab. This is also a kind of scenario generation.},
  doi       = {10.4108/ICST.SIMUTOOLS2009.5596},
  file      = {Strelen09depCopula.pdf:Strelen09depCopula.pdf:PDF},
  isbn      = {978-963-9799-45-5},
  keywords  = {performance analysis tools, performance modeling, random variate generation, stochastic models, stochastic simulation, workload modeling},
  location  = {Rome, Italy},
  numpages  = {7},
  owner     = {scot},
  timestamp = {2010.11.24},
}

@Article{Lageras10copulaMarkov,
  author    = {Andreas N. Lageras},
  title     = {Copulas for {Markovian} dependence},
  journal   = {Bernoulli},
  year      = {2010},
  volume    = {16},
  number    = {4},
  pages     = {331--342},
  month     = nov,
  abstract  = {Copulas have been popular to model dependence for multivariate distributions, but have not been used much in modelling temporal dependence of univariate time series. This paper demonstrates some difficulties with using copulas even for Markov processes: some tractable copulas such as mixtures between copulas of complete co- and countermonotonicity and independence (Fr?chet copulas) are shown to imply quite a restricted type of Markov process and Archimedean copulas are shown to be incompatible with Markov chains. We also investigate Markov chains that are spreadable or, equivalently, conditionally i.i.d.},
  comment   = {Difficulties w/ using copulas to model Markov temporal dependence. Somehow, discusses scalar variables more than multivariate, as if this was a harder problem.

Use for spinning reserves?},
  doi       = {10.3150/09-BEJ214},
  file      = {Lageras10copulaMarkov.pdf:Lageras10copulaMarkov.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.11.24},
  url       = {http://projecteuclid.org/euclid.bj/1274821073},
}

@Article{Davies00randGenStblCorrMat,
  author   = {Davies, Philip I. and Higham, Nicholas J.},
  title    = {Numerically Stable Generation of Correlation Matrices and Their Factors},
  journal  = {BIT Numerical Mathematics},
  year     = {2000},
  volume   = {40},
  number   = {4},
  pages    = {640--651},
  issn     = {1572-9125},
  abstract = {Correlation matrices---symmetric positive semidefinite matrices with unit diagonal---are important in statistics and in numerical linear algebra. For simulation and testing it is desirable to be able to generate random correlation matrices with specified eigenvalues (which must be nonnegative and sum to the dimension of the matrix). A popular algorithm of Bendel and Mickey takes a matrix having the specified eigenvalues and uses a finite sequence of Givens rotations to introduce 1s on the diagonal. We give improved formulae for computing the rotations and prove that the resulting algorithm is numerically stable. We show by example that the formulae originally proposed, which are used in certain existing Fortran implementations, can lead to serious instability. We also show how to modify the algorithm to generate a rectangular matrix with columns of unit 2-norm. Such a matrix represents a correlation matrix in factored form, which can be preferable to representing the matrix itself, for example when the correlation matrix is nearly singular to working precision.},
  comment  = {Most recent paper for Matlab's gallery function 'randcolu' and 'randcorr' random covariance matrix options.

Compare with: Hardin13genRealisticCorrMat},
  doi      = {10.1023/A:1022384216930},
  file     = {Davies00randGenStblCorrMat.pdf:Davies00randGenStblCorrMat.pdf:PDF},
  url      = {http://dx.doi.org/10.1023/A:1022384216930},
}

@Article{Chernozhukov13infCntrFactDist,
  author    = {Chernozhukov, Victor and Fern{\'a}ndez-Val, Iv{\'a}n and Melly, Blaise},
  title     = {Inference on counterfactual distributions},
  journal   = {Econometrica},
  year      = {2013},
  volume    = {81},
  number    = {6},
  pages     = {2205--2268},
  abstract  = {Counterfactual distributions are important ingredients for policy analysis and de-
composition analysis in empirical economics. In this article we develop modeling and inference
tools for counterfactual distributions based on regression methods. The counterfactual scenarios
that we consider consist of ceteris paribus changes in either the distribution of covariates related
to the outcome of interest or the conditional distribution of the outcome given covariates. For
either of these scenarios we derive joint functional central limit theorems and bootstrap validity
results for regression-based estimators of the status quo and counterfactual outcome distribu-
tions. These results allow us to construct simultaneous confidence sets for function-valued effects
of the counterfactual changes, including the effects on the entire distribution and quantile func-
tions of the outcome as well as on related functionals. These confidence sets can be used to
test functional hypotheses such as no-effect, positive effect, or stochastic dominance. Our theory
applies to general counterfactual changes and covers the main regression methods including clas-
sical, quantile, duration, and distribution regressions. We illustrate the results with an empirical
application to wage decompositions using data for the United States.
As a part of developing the main results, we introduce distribution regression as a comprehen-
sive and flexible tool for modeling and estimating the entire conditional distribution. We show
that distribution regression encompasses the Cox duration regression and represents a useful
alternative to quantile regression. We establish functional central limit theorems and bootstrap
validity results for the empirical distribution regression process and various related functionals.

Key Words: Counterfactual distribution, decomposition analysis, policy analysis, quantile re-
gression, distribution regression, duration/transformation regression, Hadamard differentiability
of the counterfactual operator, exchangeable bootstrap, unconditional quantile and distribution
effects},
  comment   = {The side topic of distribution regression is most interesting, especially that the estimated distributions have no problem with predicting point masses (zero power, max wind power!). But, the main topic, conterfactual distributions, is interesting in itself (see male/female conterfactual salary distribution). Maybe this could be used for predicting new distributions from old data e.g. analog ensemble distributions?

* lists many possible distribution regression link functions
 - log-log, logit, probit, linear, log-log, and Gosset
 - but says choice doesn't matter much as long as input representation, P(X) is sufficiently "rich"
 - really? linear is OK?

In general DR doesn't require a smooth cdf, so:
* DR better than (linear) QR for probability point masses (like ends of wind power curve)
* or discrete valued dependent variable.

Paper is 2013 arXiv version, not the journal paper.

See also: Koenker13DistributionalvsQuantile for an asympotic comparison of DR and QR (e.g. DR may be better at extreme Q's)},
  file      = {arXiv 2013 paper:Chernozhukov13infCntrFactDist.pdf:PDF},
  publisher = {Wiley Online Library},
  url       = {http://arxiv.org/abs/0904.0951},
}

@Article{Lipton15critRevRNNseq,
  author    = {Lipton, Zachary C and Berkowitz, John and Elkan, Charles},
  title     = {A critical review of recurrent neural networks for sequence learning},
  journal   = {arXiv preprint arXiv:1506.00019},
  year      = {2015},
  abstract  = {Countless learning tasks require dealing with sequential data. Image
captioning, speech synthesis, and music generation all require that a model
produce outputs that are sequences. In other domains, such as time series
prediction, video analysis, and musical information retrieval, a model must
learn from inputs that are sequences. Interactive tasks, such as translat-
ing natural language, engaging in dialogue, and controlling a robot, often
demand both capabilities. Recurrent neural networks (RNNs) are connec-
tionist models that capture the dynamics of sequences via cycles in the
network of nodes. Unlike standard feedforward neural networks, recurrent
networks retain a state that can represent information from an arbitrarily
long context window. Although recurrent neural networks have tradition-
ally been difficult to train, and often contain millions of parameters, recent
advances in network architectures, optimization techniques, and paral-
lel computation have enabled successful large-scale learning with them.
In recent years, systems based on long short-term memory (LSTM) and
bidirectional (BRNN) architectures have demonstrated ground-breaking
performance on tasks as varied as image captioning, language translation,
and handwriting recognition. In this survey, we review and synthesize
the research that over the past three decades first yielded and then made
practical these powerful learning models. When appropriate, we reconcile
conflicting notation and nomenclature. Our goal is to provide a self-
contained explication of the state of the art together with a historical
perspective and references to primary research.},
  comment   = {Highly cited.  Still seems to be fairly up to date (in March 2017)},
  file      = {Lipton15critRevRNNseq.pdf:Lipton15critRevRNNseq.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.03.08},
  url       = {https://arxiv.org/abs/1506.00019},
}

@Electronic{Torgersen03geoStatClass,
  author       = {Frede Aakmann T{\o}gersen},
  year         = {2003},
  title        = {Geostatistics and Spatial Modelling},
  howpublished = {Course Slides},
  organization = {Danish Institute of Agricultural Sciences},
  url          = {http://gbi.agrsci.dk/statistics/courses/JBS-Geostatistics-2003/},
  abstract     = {Course Description The following subjects will be covered during the course in greater or lesser details as time permits. See below for a detailed course description. Overview, Course Topics and Case Study: Overview of applications and techniques to be covered in the course: univariate and multivariate statistics, spatial correlation analysis, modelling, estimation, simulation. Exploratory Data Analysis: Statistical analysis, vs. summarization, mapping, representing spatial data, continuous vs. categorical data, histograms and probability distibutions, cross-correlation in multivariate data, data transformations (logarithmic, indicator, normal-score, rank-order), software use and applications Spatial Correlation: Spatial correlation and associated statistical measures, calculation of experimental variograms, fitting model autocorrelation functions, variogram anisotropy and nested structures, indicator variograms, crosscorrelation (spatial co-variability of multiple variables). Spatial Estimation (Kriging): Techniques for spatial estimation, 'best' linear unbiased estimation, the kriging system of equations, use and misuse of kriging variance, sensitivity of kriging estimation to variogram structure, kriging strategy, cross-validation of spatial data, co-kriging. Stochastic Simulation: Simulation vs. kriging, differences, philosophy, applications, adaptation of the kriging system of equations to simulation, theory and application of basic gaussian and indicator simulation algorithms. Change of Support: Impacts of discrepancy between measurement and estimation scales, examples of the effects of scale, accounting for scale discrepancies with analytical techniques, numerical techniques for addressing scaling issues (block kriging, averaging techniques). Geographical Information Systems and Geostatistics: Short introduction to GIS and geostatistics via ArcGIS Geostatistical Analyst. Examples of work cases from the ESRI materials are used. Possibility of exploring own data with the ESRI software.},
  comment      = {Nice slides on kriging},
  file         = {Class slides (final version):Torgersen03geoStatClass.pdf:PDF},
  location     = {Denmark},
  owner        = {scot},
  timestamp    = {2011.04.08},
}

@Unpublished{Schraudolph05GradientMethsMachLrn,
  author    = {Nicol Schraudolph},
  title     = {Gradient Methods for Machine Learning},
  note      = {Machine Learning Summer School: Slides and video lectures},
  year      = {2005},
  abstract  = {Course Overview
1. Mon: Classical Gradient Methods
Direct (gradient-free), Steepest Descent, Newton,
Levenberg-Marquardt, BFGS, Conjugate Gradient
2. Tue: Stochastic Approximation (SA)
Why necessary, why difficult. Step size adaptation.
3. Thu: Stochastic Meta-Descent (SMD)
Advanced stochastic step size adaptation method.
4. Fri: Algorithmic Differentiation (AD)
Forward/reverse mode. Fast Hessian-vector products.},
  comment   = {Nice overview of machine learning optimization, leading from classic methods to stochastic methods: slides and videolecture from the Machine Learning Summer School. Only talks about unconstrained optimization (says this is a superset, constraints can be converted to unconstrained problems).

Corresponding videolecture here:
http://www.quizover.com/oer/course/gradient-methods-for-machine-learning-by-nicol-schraudolph-videolectur

Machine learning summer school might be worth attending!
http://www.mlss.cc/},
  file      = {Schraudolph05GradientMethsMachLrn.pdf:Schraudolph05GradientMethsMachLrn.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.07.11},
  url       = {http://www.schraudolph.org/teach/},
}

@Article{Deng09largeGausCovMarkov,
  author    = {Deng, X. and Yuan, M.},
  title     = {Large {Gauss}ian covariance matrix estimation with {Markov} structures},
  journal   = {Journal of Computational and Graphical Statistics},
  year      = {2009},
  volume    = {18},
  number    = {3},
  pages     = {640--657},
  issn      = {1061-8600},
  abstract  = {Covariance matrix estimation for a large number of Gaussian random variables is a challenging yet increasingly common problem. A fact neglected in practice is that the random variables are frequently observed with certain temporal or spatial structures. Such a problem arises naturally in many practical situations with time series and images as the most popular and important examples. Effectively accounting for such structures not only results in more accurate estimation but also leads to models that are more interpretable. In this article, we propose shrinkage estimators of the covariance matrix specifically to address this issue. The proposed methods exploit sparsity in the inverse covariance matrix in a systematic fashion so that the estimate conforms with models of Markov structure and is amenable for subsequent stochastic modeling. The present approach complements the existing work in this direction that deals exclusively with temporal orders and provides a more general and flexible alternative to explore potential Markov properties. We show that the estimation procedure can be formulated as a semidefinite program and efficiently computed.We illustrate the merits of these methods through simulation and the analysis of a real data example. Matlab implementation of the proposed methods is also available online as supplemental material. Key Words: Conditional independence; GraphGarrote; Markov property.},
  comment   = {cov. shrinkage/feature sel, useful for lagged or 2D spatial cov matrices, better sparseness, has Matlab

Main idea:
* inverse covariance matrices should shrink with "distance"
* "distance" can be temporal or spatial
1.) temporal, as in order of lagged features (would need to be careful w/ diurnal comps)
2.) spatial: example is "city block" distance between 2D image pixels

* "shrink" done a couple ways
1.) icov coeff must be less than the icov coeff which is less "distant"

2.) zero out coeffs that are really distant

Optimization
* done w/ semidefinite programming, in Matlab,
* alg is a standard one, available elsewhere, eg. SDPT3
* shrinkage:
-- sum of shrinkage coeffs is less than constant M, determined by BIC
-- coeff temporal/spatial constraint based on "distance"

Other approaches
* Mentions a Cholesky approach that works for 1D temporal orders but

* forces coeffs to be zero
* not good for 2D, and maybe force to zero isn't good either

RESULTS
* covariance est: better than competitors for p<n and p>n
* regression: better than sample based cov. regression, NOT COMPARED against competing shrinkage methods
* combine w/ partial correlation approach in Schafer05shrinkCov ?

IDEA: use for conditional probabilistic forecast like Pierre's bootstrapper
*no longer "Markov", could have more than one time lag;
* "spatial" closeness
-- could be distance in wind speed or whatever, would be a smoothness constraint).
-- could also be distance from exogenous inputs (this would could be an extension of concept drive ideas as in Zliobaite09lrnCncptDrift and the other "concept drift" papers in this bib file )},
  file      = {Deng09largeGausCovMarkov.pdf:Deng09largeGausCovMarkov.pdf:PDF},
  groups    = {Read, Ensemble, PointDerived, doReadNonWPV_1},
  owner     = {scot},
  publisher = {ASA},
  timestamp = {2010.11.24},
}

@Article{Schefzik13ensCplaCpla,
  author    = {Schefzik, Roman and Thorarinsdottir, Thordis L and Gneiting, Tilmann and others},
  title     = {Uncertainty quantification in complex simulation models using ensemble copula coupling},
  journal   = {Statistical Science},
  year      = {2013},
  volume    = {28},
  number    = {4},
  pages     = {616--640},
  abstract  = {Critical decisions frequently rely on high-dimensional output
from complex computer simulation models that show intricate crossvariable,
spatial and temporal dependence structures, with weather and
climate predictions being key examples. There is a strongly increasing
recognition of the need for uncertainty quantification in such settings,
for which we propose and review a general multi-stage procedure called
ensemble copula coupling (ECC), proceeding as follows:
1. Generate a raw ensemble, consisting of multiple runs of the computer
model that differ in the inputs or model parameters in suitable ways.
2. Apply statistical postprocessing techniques, such as Bayesian model
averaging or nonhomogeneous regression, to correct for systematic errors
in the raw ensemble, to obtain calibrated and sharp predictive distributions
for each univariate output variable individually.
3. Draw a sample from each postprocessed predictive distribution.
4. Rearrange the sampled values in the rank order structure of the raw
ensemble to obtain the ECC postprocessed ensemble.
The use of ensembles and statistical postprocessing have become routine
in weather forecasting over the past decade. We show that seemingly
unrelated, recent advances can be interpreted, fused and consolidated
within the framework of ECC, the common thread being the adoption
of the empirical copula of the raw ensemble. Depending on the use of
Quantiles, Random draws or Transformations at the sampling stage, we
distinguish the ECC-Q, ECC-R and ECC-T variants, respectively. We
also describe relations to the Schaake shuffle and extant copula-based
techniques. In a case study, the ECC approach is applied to predictions
of temperature, pressure, precipitation and wind over Germany, based on
the 50-member European Centre for Medium-Range Weather Forecasts
(ECMWF) ensemble.
Key words and phrases: Bayesian model averaging, empirical copula,
ensemble calibration, nonhomogeneous regression, numerical weather prediction,
probabilistic forecast, Schaake shuffle, Sklar?s theorem.},
  comment   = {Zied (DWD) says this is a way to make scenarios from NWP ensembles.},
  file      = {Schefzik13ensCplaCpla.pdf:Schefzik13ensCplaCpla.pdf:PDF},
  owner     = {sotterson},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2014.04.17},
  url       = {http://projecteuclid.org/euclid.ss/1386078881},
}

@Article{Simpson01estSigXcorrMiss,
  author    = {D. M. Simpson and A. F. C. Infantosi1 and D. A. Botero Rosas},
  title     = {Estimation and significance testing of cross-correlation between cerebral blood flow velocity and background electro-encephalograph activity in signals with missing samples},
  journal   = {Medical \& Biological Engineering \& Computing},
  year      = {2001},
  volume    = {39},
  number    = {4},
  pages     = {428--433},
  abstract  = {Cross-correlation between cerebral blood flow (CBF) and background EEG activity can indicate the integrity of CBF control under changing metabolic demand. The difficulty of obtaining long, continuous recordings of good quality for both EEG and CBF signals in a clinical setting is overcome, in the present work, by an algorithm that allows the cross-correlation function (CCF) to be estimated when the signals are interrupted by segments of missing data. Methods are also presented to test the statistical significance of the CCF obtained in this way and to estimate the power of this test, both based on Monte Carlo simulations. The techniques are applied to the time-series given by the mean CBF velocity (recorded by transcranial Doppler) and the mean power of the EEG signal, obtained in 1 s intervals from nine sleeping neonates. The peak of the CCF is found to be low (?0.35), but reached statistical significance (p0.05) in five of the nine subjects. The CCF further indicates a delay of 4?6s between changes in EEG and CBF velocity. The proposed signal-analysis methods prove effective and convenient and can be of wide use in dealing with the common problem of missing samples in biological signals.},
  comment   = {A way to select xcorr lags? Cross correlation p-value estimated by comparing w/ randomized phase hull hypothesis; missing values just ignored in time domain xcorr calc * null hypothesis: randomizing spectral phase, ifft'ing and histo counting. * test discussion on p. 432 explains when method is vulnerable to false positives},
  doi       = {10.1007/BF02345364},
  file      = {Simpson01estSigXcorrMiss.pdf:Simpson01estSigXcorrMiss.pdf:PDF;Simpson01estSigXcorrMiss.pdf:Simpson01estSigXcorrMiss.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.05.15},
}

@InProceedings{Jin17cmprClustResidEnrgy,
  author       = {Jin, Ling and Lee, Doris and Sim, Alex and Borgeson, Sam and Wu, Kesheng and Spurlock, C. Anna and Todd, Annika},
  title        = {Comparison of Clustering Techniques for Residential Energy Behavior using Smart Meter Data},
  booktitle    = {Workshops at the Thirty-First AAAI Conference on Artificial Intelligence},
  year         = {2017},
  month        = {3},
  abstract     = {Current practice in whole time series clustering of residential meter data focuses on aggregated or subsampled load data at the customer level, which ignores day-to-day differences within customers. This information is critical to determine each customer’s suitability to various demand side management strategies that support intelligent power grids and smart energy management. Clustering daily load shapes provides fine-grained information on customer attributes and sources of variation for subsequent models and customer segmentation. In this paper, we apply 11 clustering methods to daily residential meter data. We evaluate their parameter settings and suitability based on 6 generic performance metrics and post-checking of resulting clusters. Finally, we recommend suitable techniques and parameters based on the goal of discovering diverse daily load patterns among residential customers. To the authors’ knowledge, this paper is the first robust comparative review of clustering techniques applied to daily residential load shape time series in the power systems’ literature.},
  abstractnote = {Current practice in whole time series clustering of residential meter data focuses on aggregated or subsampled load data at the customer level, which ignores day-to-day differences within customers. This information is critical to determine each customer’s suitability to various demand side management strategies that support intelligent power grids and smart energy management. Clustering daily load shapes provides fine-grained information on customer attributes and sources of variation for subsequent models and customer segmentation. In this paper, we apply 11 clustering methods to daily residential meter data. We evaluate their parameter settings and suitability based on 6 generic performance metrics and post-checking of resulting clusters. Finally, we recommend suitable techniques and parameters based on the goal of discovering diverse daily load patterns among residential customers. To the authors’ knowledge, this paper is the first robust comparative review of clustering techniques applied to daily residential load shape time series in the power systems’ literature.},
  comment      = {Rigorous comparison of a lot of load profile clustering algorithms, evaluated several ways.  Probably a good 1st read after Aghabozorgi15tsClustDecRvw},
  file         = {:Jin17cmprClustResidEnrgy.pdf:PDF},
  place        = {United States},
  url          = {https://www.osti.gov/biblio/1398467},
}

@InProceedings{Shieh08iSaxDTW,
  author    = {Shieh, Jin and Keogh, Eamonn},
  title     = {iSAX: indexing and mining terabyte sized time series},
  booktitle = {Proceedings of the 14\textsuperscript{th} ACM SIGKDD international conference on Knowledge discovery and data mining},
  year      = {2008},
  series    = {KDD '08},
  publisher = {ACM},
  location  = {Las Vegas, Nevada, USA},
  isbn      = {978-1-60558-193-4},
  pages     = {623--631},
  doi       = {10.1145/1401890.1401966},
  abstract  = {Current research in indexing and mining time series data has produced many interesting algorithms and representations. However, the algorithms and the size of data considered have generally not been representative of the increasingly massive datasets encountered in science, engineering, and business domains. In this work, we show how a novel multi-resolution symbolic representation can be used to index datasets which are several orders of magnitude larger than anything else considered in the literature. Our approach allows both fast exact search and ultra fast approximate search. We show how to exploit the combination of both types of search as sub-routines in data mining algorithms, allowing for the exact mining of truly massive real world datasets, containing millions of time series.},
  acmid     = {1401966},
  address   = {New York, NY, USA},
  file      = {Shieh08iSaxDTW.pdf:Shieh08iSaxDTW.pdf:PDF},
  keywords  = {data mining, indexing, representations, time series},
  numpages  = {9},
  owner     = {sotterson},
  timestamp = {2013.03.15},
}

@Article{Gutjahr13multiObjStochOptNonScl,
  author    = {Gutjahr, Walter J and Pichler, Alois},
  title     = {Stochastic multi-objective optimization: a survey on non-scalarizing methods},
  journal   = {Annals of Operations Research},
  year      = {2013},
  pages     = {1--25},
  abstract  = {Currently, stochastic optimization on the one hand and multi-objective optimization
on the other hand are rich and well-established special fields of Operations
Research. Much less developed, however, is their intersection: the analysis of decision
problems involving multiple objectives and stochastically represented uncertainty simultaneously.
This is amazing, since in economic and managerial applications, the features
of multiple decision criteria and uncertainty are very frequently co-occurring. Part of the
existing quantitative approaches to deal with problems of this class apply scalarization
techniques in order to reduce a given stochastic multi-objective problem to a stochastic
single-objective one. The present article gives an overview over a second strand of the
recent literature, namely methods that preserve the multi-objective nature of the problem
during the computational analysis. We survey publications assuming a risk-neutral
decision maker, but also articles addressing the situation where the decision maker is
risk-averse. In the second case, modern risk measures play a prominent role, and generalizations
of stochastic orders from the univariate to the multivariate case have recently
turned out as a promising methodological tool. Modeling questions as well as issues of
computational solution are discussed.
Keywords: Stochastic optimization, multi-objective optimization, Pareto optimality, risk
measures, multivariate stochastic dominance.},
  comment   = {Survey paper: multi-objective objective optimization can create a multimdimensional optimal solution surface, which can include multiple time horizons, and levels of risk adversity. May be related to ReWP interday and intraday operations, where the intraday offer is highly risk reverse.

Lots of techniques mentioned but it seems like there is plenty left to do.},
  file      = {Gutjahr13multiObjStochOptNonScl.pdf:Gutjahr13multiObjStochOptNonScl.pdf:PDF},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2015.01.26},
  url       = {http://homepage.univie.ac.at/walter.gutjahr/papers/SurveyArticle.pdf},
}

@Article{Mirowski14loadFrcstFeatAlgAgg,
  author   = {P. Mirowski and S. Chen and T. K. Ho and C. N. Yu},
  title    = {Demand forecasting in smart grids},
  journal  = {Bell Labs Technical Journal},
  year     = {2014},
  volume   = {18},
  number   = {4},
  pages    = {135-158},
  month    = {March},
  issn     = {1089-7089},
  abstract = {Data analytics in smart grids can be leveraged to channel the data downpour from individual meters into knowledge valuable to electric power utilities and end-consumers. Short-term load forecasting (STLF) can address issues vital to a utility but it has traditionally been done mostly at system (city or country) level. In this case study, we exploit rich, multi-year, and high-frequency annotated data collected via a metering infrastructure to perform STLF on aggregates of power meters in a mid-sized city. For smart meter aggregates complemented with geo-specific weather data, we benchmark several state-of-the-art forecasting algorithms, including kernel methods for nonlinear regression, seasonal and temperature-adjusted auto-regressive models, exponential smoothing and state-space models. We show how STLF accuracy improves at larger meter aggregation (at feeder, substation, and system-wide level). We provide an overview of our algorithms for load prediction and discuss system performance issues that impact real time STLF. ?? 2014 Alcatel-Lucent.},
  comment  = {A lot of traditional load forecasts.  Interesting b/c compares
* algorithms
* features
* levels of aggregation},
  doi      = {10.1002/bltj.21650},
  file     = {Mirowski14loadFrcstFeatAlgAgg.pdf:Mirowski14loadFrcstFeatAlgAgg.pdf:PDF},
  keywords = {Autoregressive processes;Data analysis;Electricity supply industry;Forecasting;Meter reading;Power demand;Power grids;Power system measurements;Smart grids;Supply and demand},
}

@Article{Hunt07datAssimSPchaoisLETKF,
  author    = {Hunt, Brian R and Kostelich, Eric J and Szunyogh, Istvan},
  title     = {Efficient data assimilation for spatiotemporal chaos: A local ensemble transform {Kalman} filter},
  journal   = {Physica D: Nonlinear Phenomena},
  year      = {2007},
  volume    = {230},
  number    = {1},
  pages     = {112--126},
  abstract  = {Data assimilation is an iterative approach to the problem of estimating the state of a dynamical system using both current and past observations of the system together with a model for the system?s time evolution. Rather than solving the problem from scratch each time new observations become available, one uses the model to ?forecast? the current state, using a prior state estimate (which incorporates information from past data) as the initial condition, then uses current data to correct the prior forecast to a current state estimate. This Bayesian approach is most effective when the uncertainty in both the observations and in the state estimate, as it evolves over time, are accurately quantified. In this article, we describe a practical method for data assimilation in large, spatiotemporally chaotic systems. The method is a type of ?ensemble Kalman filter?, in which the state estimate and its approximate uncertainty are represented at any given time by an ensemble of system states. We discuss both the mathematical basis of this approach and its implementation; our primary emphasis is on ease of use and computational speed rather than improving accuracy over previously published approaches to ensemble Kalman filtering. We include some numerical results demonstrating the efficiency and accuracy of our implementation for assimilating real atmospheric data with the global forecast model used by the US National Weather Service.

Keywords

 Data assimilation;
 Spatiotemporal chaos;
 State estimation;
 Ensemble Kalman filtering},
  comment   = {DWD's new LETKF ensemble Kalman filter assimilation algorithm (ref from Richard Keane, of DWD).},
  doi       = {10.1016/j.physd.2006.11.008,},
  file      = {paper:Hunt07datAssimSPchaoisLETKF.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.12.08},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167278906004647},
}

@Article{Wikle07TutBayesDatAssum,
  author    = {Christopher K. Wikle and Mark L. Berliner},
  title     = {A {Bayes}ian tutorial for data assimilation},
  journal   = {Physica D: Nonlinear Phenomena},
  year      = {2007},
  volume    = {230},
  number    = {1-2},
  pages     = {1--16},
  month     = jun,
  abstract  = {Data assimilation is the process by which observational data are fused with scientific information. The Bayesian paradigm provides a coherent probabilistic approach for combining information, and thus is an appropriate framework for data assimilation. Viewing data assimilation as a problem in Bayesian statistics is not new. However, the field of Bayesian statistics is rapidly evolving and new approaches for model construction and sampling have been utilized recently in a wide variety of disciplines to combine information. This article includes a brief introduction to Bayesian methods. Paying particular attention to data assimilation, we review linkages to optimal interpolation, kriging, Kalman filtering, smoothing, and variational analysis. Discussion is provided concerning Monte Carlo methods for implementing Bayesian analysis, including importance sampling, particle filtering, ensemble Kalman filtering, and Markov chain Monte Carlo sampling. Finally, hierarchical Bayesian modeling is reviewed. We indicate how this approach can be used to incorporate significant physically based prior information into statistical models, thereby accounting for uncertainty. The approach is illustrated in a simplified advection-diffusion model.},
  booktitle = {Data Assimilation},
  comment   = {Use of offsite observations? Does this handle missing observations?},
  doi       = {10.1016/j.physd.2006.09.017},
  file      = {Wikle07TutBayesDatAssum.pdf:Wikle07TutBayesDatAssum.pdf:PDF;Wikle07TutBayesDatAssum.pdf:Wikle07TutBayesDatAssum.pdf:PDF},
  keywords  = {assimilation bayes model statistics tutorial uncertainty },
  owner     = {sotterson},
  posted-at = {2007-08-29 12:46:03},
  timestamp = {2009.01.22},
}

@InProceedings{Schrems08pseudoCorrVarSel,
  author    = {Andrea Schrems and Kurt Pichler and Konrad Krimpelst?tter and Luigi del Revuelta},
  title     = {Data based multivariate pseudo correlation analysis in steel industry for optimized variable selection},
  booktitle = {Proc. International Federation of Automatic Control},
  year      = {2008},
  month     = jul,
  abstract  = {Data driven variable selection, without including physical knowledge, is an important prerequisite for many applications in the field of data based modeling. This paper deals with a novel approach to optimize the dimension of the input space by a combination of common variable selection methods with multivariate correlation analysis. The results are input structures with revised pseudo correlations between input channels and a physically better interpretable structure. The presented method is successfully applied to measured data from steel industry. Some exemplary results are shown in this paper.},
  comment   = {Remove bogusly correlated indep. vars w/ correlation, partial correlation, part correlation (semipartial). Not clear if this was better combo of corr, partial and part corr: * has decent explanation of the merits of each},
  file      = {Schrems08pseudoCorrVarSel.pdf:Schrems08pseudoCorrVarSel.pdf:PDF;Schrems08pseudoCorrVarSel.pdf:Schrems08pseudoCorrVarSel.pdf:PDF},
  location  = {Seoul, Lorea},
  owner     = {sotterson},
  timestamp = {2009.02.18},
}

@Article{Sugiartawan17waveletLSTMfrcst,
  author    = {Sugiartawan, Putu and Pulungan, Reza and Sari, Anny Kartika},
  title     = {Prediction by a Hybrid of Wavelet Transform and Long-Short-Term-Memory Neural Network},
  journal   = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS},
  year      = {2017},
  volume    = {8},
  number    = {2},
  pages     = {326--332},
  abstract  = {Data originating from some specific fields, for in-
stance tourist arrivals, may exhibit a high degree of fluctuations as
well as non-linear characteristics due to time varying behaviors.
This paper proposes a new hybrid method to perform prediction
for such data. The proposed hybrid model of wavelet transform
and long-short-term memory (LSTM) recurrent neural network
(RNN) is able to capture non-linear attributes in tourist arrival
time series. Firstly, data is decomposed into constitutive series
through wavelet transform. The decomposition is expressed as a
function of a combination of wavelet coefficients, which have
different levels of resolution. Then, LSTM neural network is
used to train and simulate the value at each level to find the
bias vectors and weighting coefficients for the prediction value.
A sliding windows model is employed to capture the time series
nature of the data. An evaluation is conducted to compare the
proposed model with other RNN algorithms, i.e., Elman RNN and
Jordan RNN, as well as the combination of wavelet transform
with each of them. The result shows that the proposed model has
better performance in terms of training time than the original
LSTM RNN, while the accuracy is better than the hybrid of
wavelet-Elman and the hybrid of wavelet-Jordan.
Keywords?Wavelet Transform; Long-Short-Term Memory; Re-
current Neural Network; Time Series Prediction},
  comment   = {Wavelet decomp improves RNN method forecast accuracy, except for LSTM, where it trains about 2X faster but is much less accurate.  I didn't quite understand how the NN inputs are constructed from the wavelet decomposition.},
  file      = {:papers\\Sugiartawan17waveletLSTMfrcst.pdf:PDF},
  owner     = {sotterson},
  publisher = {SCIENCE \& INFORMATION SAI ORGANIZATION LTD 19 BOLLING RD, BRADFORD, WEST YORKSHIRE, 00000, ENGLAND},
  timestamp = {2017.07.04},
  url       = {https://thesai.org/Downloads/Volume8No2/Paper_43-Prediction_by_a_Hybrid_of_Wavelet_Transform.pdf},
}

@Article{May10datSplitNN,
  author    = {May, Robert J and Maier, Holger R and Dandy, Graeme C},
  title     = {Data splitting for artificial neural networks using SOM-based stratified sampling},
  journal   = {Neural Networks},
  year      = {2010},
  volume    = {23},
  number    = {2},
  pages     = {283--294},
  abstract  = {Data splitting is an important consideration during artificial neural network (ANN) development where
hold-out cross-validation is commonly employed to ensure generalization. Even for a moderate sample
size, the sampling methodology used for data splitting can have a significant effect on the quality of the
subsets used for training, testing and validating an ANN. Poor data splitting can result in inaccurate and
highly variable model performance; however, the choice of sampling methodology is rarely given due
consideration by ANN modellers. Increased confidence in the sampling is of paramount importance, since
the hold-out sampling is generally performed only once during ANN development.
This paper considers the variability in the quality of subsets that are obtained using different data
splitting approaches. A novel approach to stratified sampling, based on Neyman sampling of the self-
organizing map (SOM), is developed, with several guidelines identified for setting the SOM size and sam-
ple allocation in order to minimize the bias and variance in the datasets. Using an example ANN function
approximation task, the SOM-based approach is evaluated in comparison to random sampling, DUPLEX,
systematic stratified sampling, and trial-and-error sampling to minimize the statistical differences be-
tween data sets. Of these approaches, DUPLEX is found to provide benchmark performance with good
model performance, with no variability. The results show that the SOM-based approach also reliably gen-
erates high-quality samples and can therefore be used with greater confidence than other approaches,
especially in the case of non-uniform datasets, with the benefit of scalability to perform data splitting on
large datasets.},
  comment   = {How to split inputs from NN train and test sets.  Idea is to not use rotating cross validation but to instead stratify the samples -- using SOM in this paper.  But maybe KNN or kmeans or VQ instead?  Idea is to use as few sets as possible yet avoid variability.},
  file      = {May10datSplitNN.pdf:May10datSplitNN.pdf:PDF},
  publisher = {Elsevier},
}

@Article{McWilliams10sprsPLSonlnVarSel,
  author    = {McWilliams, B. and Montana, G.},
  title     = {Sparse partial least squares regression for on-line variable selection with multivariate data streams},
  journal   = {Statistical Analysis and Data Mining},
  year      = {2010},
  volume    = {3},
  number    = {3},
  pages     = {170193},
  abstract  = {Data streams arise in several domains. For instance, in computational finance, several statistical applications revolve around the real-time discovery of associations between a very large number of co-evolving data feeds representing asset prices. The problem we tackle in this paper consists of learning a linear regression function from multivariate input and output streaming data in an incremental fashion while also performing dimensionality reduction and variable selection. When input and output streams are high-dimensional and correlated, it is plausible to assume the existence of hidden factors that explain a large proportion of the covariance between them. The methods we propose build on recursive partial least squares (PLS) regression. The hidden factors are dynamically inferred and tracked over time and, within each factor, the most important streams are recursively identified by means of sparse matrix decompositions. Moreover, the recursive regression model is able to adapt to sudden changes in the data generating mechanism and also identifies the number of latent factors. Extensive simulation results illustrate how the methods perform and compare with alternative penalized regression models for streaming data. We also apply the algorithm to solve a multivariate version of the enhanced index tracking problem in computational finance.
Keywords: * data streams; * dynamic regression; * partial least squares; * variable selection; * index tracking},
  comment   = {PLS latent factor tracking and featsel
* Compare w/ paper saying that static PLS picked too many graphical model edges
* borrows some matlab for LASSO, so it's probably matlab based},
  doi       = {10.1002/sam.10074},
  file      = {McWilliams10sprsPLSonlnVarSel.pdf:McWilliams10sprsPLSonlnVarSel.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.09.27},
}

@InProceedings{Bhowmik15glmForAggDat,
  author    = {Bhowmik, Avradeep and Ghosh, Joydeep and Koyejo, Oluwasanmi},
  title     = {Generalized linear models for aggregated data},
  booktitle = {Artificial Intelligence and Statistics},
  year      = {2015},
  pages     = {93--101},
  abstract  = {Databases in domains such as healthcare are
routinely released to the public in aggregated
form. Unfortunately, na¨ıve modeling with
aggregated data may significantly diminish
the accuracy of inferences at the individ-
ual level. This paper addresses the scenario
where features are provided at the individual
level, but the target variables are only avail-
able as histogram aggregates or order statis-
tics. We consider a limiting case of gener-
alized linear modeling when the target vari-
ables are only known up to permutation, and
explore how this relates to permutation test-
ing; a standard technique for assessing statis-
tical dependency. Based on this relationship,
we propose a simple algorithm to estimate
the model parameters and individual level in-
ferences via alternating imputation and stan-
dard generalized linear model fitting. Our re-
sults suggest the effectiveness of the proposed
approach when, in the original data, permu-
tation testing accurately ascertains the verac-
ity of the linear relationship. The framework
is extended to general histogram data with
larger bins - with order statistics such as the
median as a limiting case. Our experimen-
tal results on simulated data and aggregated
healthcare data suggest a diminishing returns
property with respect to the granularity of
the histogram - when a linear relationship
holds in the original data, the targets can be
predicted accurately given relatively coarse
histograms.},
  comment   = {Say you have a global histogram of outcomes from some population, and that you have a vector of predictors for each individual in the population.  This paper shows how to use the global histogram and individual predictors to estimate the outcome of each individual, using a GLM.

This has privacy implications because aggregation (as in histograms) is sometimes used to preserve e.g. patient confidentiality.

Uses?
* disaggregation problems e.g. knowing the number of electric car owners on a feeder, predict which electricity customers have an EV.  Or, something like that -- I don't currently have a problem where you actually get a historgram and individual predictors (Bhowmik19estimaggLrnAggDat does it with sums instead of histograms, maybe more useful)
* missing data fill-in?  This could be bent into an imputation problem.},
  file      = {:Bhowmik15glmForAggDat.pdf:PDF},
  url       = {http://proceedings.mlr.press/v38/bhowmik15.pdf},
}

@InProceedings{Kull14datasetDriftPttrn,
  author    = {Kull, Meelis and Flach, Peter},
  title     = {Patterns of dataset shift},
  booktitle = {First International Workshop on Learning over Multiple Contexts (LMCE) at ECML-PKDD},
  year      = {2014},
  abstract  = {Dataset shift is a frequent cause of failure of a predictor. A model
which performs well in several contexts can give bad predictions in other con-
texts where the data are shifted compared to the training context. Earlier work
has revealed many different causes of shift systematised by Storkey [11] and
three important types of effects of shift on probability distributions systematised
by Moreno-Torres et al. [8]: covariate shift (distribution shift in attributes), prior
probability shift (shift in labels) and concept shift (shift in the relationship be-
tween attributes and labels). However, many causes lead to effects not covered
by these three types, and hence are called ‘other types of shift’ by Moreno-Torres
et al. [8]. We propose a formal notation for the effects of shift using graphical
models. We identify 12 patterns of shift (6 of them novel), which cover the ef-
fects of all 6 causes described by Storkey [11]. Furthermore, these patterns can
be combined to describe effects of multiple or more complex reasons of shift.
We see three avenues of work benefitting from our formalism. First, reviewing
shift-adaptive methods regarding their applicability range could be done in our
notation. Second, identifying patterns of shift in a practical task might aid in find-
ing an existing method to solve the task. Finally, our novel patterns and pattern
combinations suggest niches for new shift-adaptive methods.},
  comment   = {The many ways that training data can shift away from test data, breaking ML algorithms.  Here, they're enumerated and explained as graphical models.  Seems like a good paper to read so that can anticipate problems in Wattplan Grid.},
  file      = {:Kull14datasetDriftPttrn.pdf:PDF},
  url       = {http://users.dsic.upv.es/~flip/LMCE2014/Papers/lmce2014_submission_10.pdf},
}

@Article{Wan16WindSpdDAfrcstDeepFeatLrn,
  author   = {Wan, Jie and Liu, Jinfu and Ren, Guorui and Guo, Yufeng and Yu, Daren and Hu, Qinghua},
  title    = {Day-Ahead Prediction of Wind Speed with Deep Feature Learning},
  journal  = {International Journal of Pattern Recognition and Artificial Intelligence},
  year     = {2016},
  volume   = {30},
  number   = {05},
  pages    = {1650011},
  abstract = {Day-ahead prediction of wind speed is a basic and key problem of large-scale wind power penetration. Many current techniques fail to satisfy practical engineering requirements because of wind speed's strong nonlinear features, influenced by many complex factors, and the general model's inability to automatically learn features. It is well recognized that wind speed varies in different patterns. In this paper, we propose a deep feature learning (DFL) approach to wind speed forecasting because of its advantages at both multi-layer feature extraction and unsupervised learning. A deep belief network (DBN) model for regression with an architecture of 144 input and 144 output nodes was constructed using a restricted Boltzmann machine (RBM). Day-ahead prediction experiments were then carried out. By comparing the experimental results, it was found that the prediction errors with respect to both size and stability of a DBN model with only three hidden layers were less than those of the other three typical approaches including support vector regression (SVR), single hidden layer neural networks (SHL-NN), and neural networks with three hidden layers (THL-NN). In addition, the DBN model can learn and obtain complex features of wind speed through its strong nonlinear mapping ability, which effectively improves its prediction precision. In addition, prediction errors are minimized when the number of DBN model's hidden layers reaches a threshold value. Above this number, it is not possible to improve the prediction accuracy by further increasing the number of hidden layers. Thus, the DBN method has a high practical value for wind speed prediction.

Keywords: Wind speed; day-ahead prediction; Boltzmann machine; deep belief network; unsupervised learning; feature extraction; robustness



Read More: http://www.worldscientific.com/doi/abs/10.1142/S0218001416500117?journalCode=ijprai},
  comment  = {I couldn't get the paper...},
  doi      = {10.1142/S0218001416500117},
  eprint   = {http://www.worldscientific.com/doi/pdf/10.1142/S0218001416500117},
  url      = {http://www.worldscientific.com/doi/abs/10.1142/S0218001416500117},
}

@Misc{Agora13costOptResDE,
  author       = {Agora},
  title        = {Cost Optimal Expansion of Renewables in {Germany}: A comparison of strategies for expanding wind and solar power in {Germany}},
  howpublished = {Study},
  month        = aug,
  year         = {2013},
  abstract     = {Dear readers,
Germany?s energy transition has to be affordable. In view of
the continuing expansion of renewables, a number of questions
arise that are crucial for future costs of our power system:
* Where should wind turbines and solar arrays be built ? in
the best sites or close to consumers?
* Does the expansion of renewables have to wait for grid
expansion?
* What will happen if PV arrays with battery storage
breakthrough on the market, and will we still need grids?
Agora Energiewende had Consentec look into these questions
with the support of Fraunhofer IWES.
A number of scenarios were investigated up to 2033 based
on the German Network Agency?s lead scenario for the Grid
Development Plan 2013. In addition to the cost of renewable
energy, the cost of grids, storage, and conventional power
generation was taken into consideration.
The findings are quite interesting and offer some new and
surprising insights even for experts. To facilitate further
discussion, the assumptions used are published on Agora
Energiewende?s website.
I hope you enjoy the read!
Best regards,
Rainer Baake
Director Agora Energiewende},
  comment      = {In Germany, a few years of grid expansion delay don't matter; it doesn't matter (in terms of energy cost by 2030) if put solar and wind close to populations or to good resources. But, by 2050, wind in solar in the best places and with rapid grid buidlout is the cheapest (and will result in lowest emissions, Carsten says). It also looks like solar battery storage is unlikely to obviate the need for wind power and grid expansion. Biggest savings found across the scenarios were from reducing offshore wind.

I used this in my March 2014 GIZ lecuture (Carsten's half)},
  file         = {Executive Summary:Agora13costOptResDE_ExSum.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2014.03.18},
  url          = {http://www.agora-energiewende.de/themen/optimierung/detailansicht/article/cost-optimal-expansion-of-renewables-in-germany/},
}

@Book{Conejo10uncertDecElecMktsBook,
  title     = {Decision making under uncertainty in electricity markets},
  publisher = {Springer},
  year      = {2010},
  author    = {Conejo, Antonio J. and Carri{\'o}n, Miguel and Morales, Juan M.},
  abstract  = {Decision Making Under Uncertainty in Electricity Markets provides models and procedures to be used by electricity market agents to make informed decisions under uncertainty. These procedures rely on well established stochastic programming models, which make them efficient and robust. Particularly, these techniques allow electricity producers to derive offering strategies for the pool and contracting decisions in the futures market. Retailers use these techniques to derive selling prices to clients and energy procurement strategies through the pool, the futures market and bilateral contracting.},
  booktitle = {International series in operations research \& management science},
  comment   = {Juan Miguel's book. I've asked Malte if we can get a copy. It's probably good but I won't plan on seeing a copy anytime soon.},
  groups    = {Use, CitaviImport1, doReadNonWPV_2},
  keywords  = {Electric utilities, Energy industries, Decision making, Environmental Studies, BUSINESS \& ECONOMICS},
  location  = {New York},
  owner     = {sotterson},
  timestamp = {2013.09.26},
  url       = {http://www.springer.com/business+%26+management/operations+research/book/978-1-4419-7420-4},
}

@Article{LeCun15deepLearnRev,
  author    = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  title     = {Deep learning},
  journal   = {Nature},
  year      = {2015},
  volume    = {521},
  number    = {7553},
  pages     = {436--444},
  abstract  = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of
data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech rec-
ognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep
learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine
should change its internal parameters that are used to compute the representation in each layer from the representation in
the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and
audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  comment   = {Tutorial papers by leaders of Deep Learning.},
  file      = {LeCun15deepLearnRev.pdf:LeCun15deepLearnRev.pdf:PDF},
  owner     = {sotterson},
  publisher = {Nature Research},
  timestamp = {2017.01.22},
  url       = {http://www.nature.com/nature/journal/v521/n7553/abs/nature14539.html},
}

@InProceedings{Bengio14scaleUpDeepLrn,
  author       = {Bengio, Yoshua},
  title        = {Scaling up deep learning},
  booktitle    = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
  year         = {2014},
  pages        = {1966--1966},
  organization = {ACM},
  abstract     = {Deep learning has rapidly moved from a marginal approach in the machine learning community less than ten years ago to one that has strong industrial impact, in particular for high-dimensional perceptual data such as speech and images, but also natural language. The demand for experts in deep learning is growing very fast (faster than we can graduate PhDs), thereby considerably increasing their market value. Deep learning is based on the idea of learning multiple levels of representation, with higher levels computed as a function of lower levels, and corresponding to more abstract concepts automatically discovered by the learner. Deep learning arose out of research on artificial neural networks and graphical models and the literature on that subject has considerably grown in recent years, culminating in the creation of a dedicated conference (ICLR). The tutorial will introduce some of the basic algorithms, both on the supervised and unsupervised sides, as well as discuss some of the guidelines for successfully using them in practice. Finally, it will introduce current research questions regarding the challenge of scaling up deep learning to much larger models that can successfully extract information from huge datasets.},
  comment      = {Has speech recog. WER graph.  I only have slides, could not find the paper.},
  doi          = {10.1145/2623330.2630802},
  file         = {Slides:Bengio14scaleUpDeepLrn_Slides.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2017.01.23},
  url          = {https://dl.acm.org/purchase.cfm?id=2630802&CFID=719808056&CFTOKEN=92826381},
}

@Article{Ambati15deepLrnGuidePract,
  author    = {SriSatish Ambati},
  title     = {Deep learning: A brief guide for practical problem solvers},
  journal   = {InfoWorld},
  year      = {2015},
  month     = nov,
  abstract  = {Deep learning has the potential to solve most of the problems in the field of machine learning and artificial intelligence, can be used to solve problems such as speech recognition, three-dimensional object recognition and natural language processing and other fields. This paper introduces the advantages and disadvantages of deep learning in solving practical problems.},
  comment   = {Nice quick summary covering what Deep Learning is good for, and its problems.  

Examples show how:
	1. GLMs single draw straight lines through data
	2. Random and Bosted Forests draw many straight lines (better)
	3. Deep neural net captures non-straight lines through the data -- good for complex interactions, I guess.

Also, Deep learning is good for classifiers with a high number of classes (high cardinality, high dimension output).

Applications
* Payment systems providers use deep learning to identify suspicious transactions in real time.
* Organizations with large data centers and computer networks use deep learning to mine log files and detect threats.
* Vehicle manufacturers and fleet operators use deep learning to mine sensor data to predict part and vehicle failure.
* Deep learning helps companies with large and complex supply chains predict delays and bottlenecks in production.

Advantages over other methods
1. Its ability to detect complex interactions among features
2. Its ability to learn low-level features from minimally processed raw data
3. Its ability to work with high-cardinality class memberships
4. Its ability to work with unlabeled data (but see Karpathy14featLrnEscpdsBlog)

Deep Learning disadvantages
* Hard to interpret
* Cause/effect hard to know
* But can rank inputs based on importance
* Lots of data and CPU power
* Overkill for simpler problems},
  file      = {Ambati15deepLrnGuidePract.pdf:Ambati15deepLrnGuidePract.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.01.25},
  url       = {http://www.infoworld.com/article/3003315/big-data/deep-learning-a-brief-guide-for-practical-problem-solvers.html},
}

@InProceedings{Sergio15DeepLrnWindFrcst,
  author    = {A. T. Sergio and T. B. Ludermir},
  title     = {Deep Learning for Wind Speed Forecasting in Northeastern Region of {Brazil}},
  booktitle = {Proc. Brazilian Conf. Intelligent Systems (BRACIS)},
  year      = {2015},
  pages     = {322--327},
  month     = nov,
  abstract  = {Deep Learning is one of the latest approaches in the field of artificial neural networks. Since they were first proposed in mid-2006, Deep Learning models have obtained state-of-art results in some problems with classification and pattern recognition. However, such models have been little used in time series forecasting. This work aims to investigate the use of some of these architectures in this kind of problem, specifically in predicting the hourly average speed of winds in the Northeastern region of Brazil. The results showed that Deep Learning offers a good alternative for performing this task, overcoming some results of previous works.},
  comment   = {One step ahead autoregressive scalar wind speed prediction using a Deep Belief Network (DBN), which has a generative probabilsitic model.  Doesn't work as well as an MLP.  But could use for scenario generation somehow?

The author himself (here: https://www.quora.com/How-does-one-apply-deep-learning-to-time-series-forecasting) says about this paper:

The last work is mine, and I could say that in most cases, the gain in using Deep Learning in time series forecasting is lower than in pattern recognition. Maybe a greater cpu consumption does not justify its use. As you can see in these works, some of them pretrain the layers in an unsupervised way, others consider deep learning the use of more than one hidden layer.

Talking about a specific model, Deep Belief Networks, there is a version of Restricted Bolztmann Machines that deals with continuous data (CRBM - Continuous Restricted Bolztmann Machines). Also, there is a version of RBM that is more focused on dynamic data, Conditional RBM.},
  doi       = {10.1109/BRACIS.2015.40},
  keywords  = {learning (artificial intelligence), neural nets, power engineering computing, wind power plants, Brazil, Northeastern Region, artificial neural networks, deep learning, time series forecasting, wind speed forecasting, Biological neural networks, Forecasting, Machine learning, Neurons, Predictive models, Time series analysis, Training, deep learning, neural networks, time series forecasting, wind forecasting},
  owner     = {sotterson},
  timestamp = {2017.03.08},
}

@Article{DiazVico17deepNNwindSolEnrgyPred,
  author   = {D{\'i}az--Vico, David and Torres--Barr{\'a}n, Alberto and Omari, Adil and Dorronsoro, Jos{\'e} R.},
  title    = {Deep Neural Networks for Wind and Solar Energy Prediction},
  journal  = {Neural Processing Letters},
  year     = {2017},
  month    = {Apr},
  issn     = {1573-773X},
  abstract = {Deep Learning models are recently receiving a large attention because of their very powerful modeling abilities, particularly on inputs that have a intrinsic one- or two-dimensional structure that can be captured and exploited by convolutional layers. In this work we will apply Deep Neural Networks (DNNs) in two problems, wind energy and daily solar radiation prediction, whose inputs, derived from Numerical Weather Prediction systems, have a clear spatial structure. As we shall see, the predictions of single deep models and, more so, of DNN ensembles can improve on those of Support Vector Regression, a Machine Learning method that can be considered the state of the art for regression.

Keywords Deep learning · Convolutional neural network · Wind energy · Solar energy},
  comment  = {Also see: Diaz15DeepNNwindEnrgyPred

},
  day      = {01},
  doi      = {10.1007/s11063-017-9613-7},
  file     = {:DiazVico17deepNNwindSolEnrgyPred.pdf:PDF},
  url      = {https://doi.org/10.1007/s11063-017-9613-7},
}

@InProceedings{Gal16nnDropoutBayesApprox,
  author    = {Yarin Gal and Zoubin Ghahramani},
  title     = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  year      = {2016},
  editor    = {Maria Florina Balcan and Kilian Q. Weinberger},
  volume    = {48},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1050--1059},
  address   = {New York, New York, USA},
  month     = {20--22 Jun},
  publisher = {PMLR},
  abstract  = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs --  extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout;s uncertainty in deep reinforcement learning.},
  comment   = {Deep Learning dropout regularization has some relationship to Bayesian regularization and Gaussian Processes.},
  file      = {Paper:Gal16nnDropoutBayesApprox.pdf:PDF;Appendix:Gal16nnDropoutBayesApprox_Appendix.pdf:PDF},
  url       = {http://proceedings.mlr.press/v48/gal16.html},
}

@Article{Simon07hiDimDelSelMutInfDistDiag,
  author    = {Simon, Geoffroy and Verleysen, Michel},
  title     = {High-dimensional delay selection for regression models with mutual information and distance-to-diagonal criteria},
  journal   = {Neurocomputing},
  year      = {2007},
  volume    = {70},
  number    = {7-9},
  pages     = {1265--1275},
  issn      = {0925-2312},
  abstract  = {Delay selection for time series phase space reconstruction may be performed using a mutual information (MI) criterion. However, the delay selection is in that case limited to the estimation of a single delay using MI between two variables only. A high-dimensional estimator of the MI may be used to select more than one delay between more than two variables but this approach is rather time consuming. In this paper, an alternative fast criterion is proposed to optimize all delays for a high-dimensional phase space reconstruction: the distance-to-diagonal (DD) criterion, based on a geometrical heuristic. The use of the distance to diagonal criterion is illustrated and compared to MI on artificial and benchmark time series.},
  comment   = {Select phase space samples w/ min mutual info or equivalently performing but cheap distance to diagonal. Decent chaotic system description.

* for chaotic system, if know dimension and delay of phase space, then know everything about dyanamics
* embedding gets 1-1 relationship between tine series and please space. Don't have to be chaotic all systems show phase space structure.

* how to estimate the dimension not covered here (but refs); just talk about delays
* here "delay" doesn't have to be equally spaced. Could select inputs at unequally spaced time intervals.
* want to select inputs that are minally related to each other (capture most info in fixed dim phase space)
* multi-D mut info better than correlation; distance-to-diagonal (DD) works about as well as multi-MI but is comutationally cheap
* DD is the sum of distances from a diagonal (correlated line) to the delay-sampled inputs
* either way, there's still a brute force search of lags in this algorithm

* use RBFN neural net to predict one step ahead
* DD seems to work about as well as multi-MI Implications for wind foreacasting?

This is a system model where you're predicting the future purely from past outputs (like an AR model) so, it's hard to know what to do w/ exogenous, related variables. Anyway, since there's a NN in there and since the delays are selected arbitrarily, it's like a feature selection algorithm which selects not based on correlation with output, but w/ anti-corrleation w/ input. The other difference is that the number of NN inputs is set by the phase space dimension estimator -- there's no BIC or anything like that. Not sure what to do with this...},
  doi       = {10.1016/j.neucom.2006.10.150},
  file      = {Simon07hiDimDelSelMutInfDistDiag.pdf:Simon07hiDimDelSelMutInfDistDiag.pdf:PDF;Simon07hiDimDelSelMutInfDistDiag.pdf:Simon07hiDimDelSelMutInfDistDiag.pdf:PDF},
  groups    = {Read},
  location  = {Amsterdam, The Netherlands, The Netherlands},
  owner     = {sotterson},
  publisher = {Elsevier Science Publishers B. V.},
  timestamp = {2009.02.09},
}

@Article{Busseti12deepLrnTimeSer,
  author    = {Enzo Busseti and Ian Osband and Scott Wong},
  title     = {Deep Learning for Time Series Modeling},
  journal   = {CS 229 Final Project Report. Stanford University},
  year      = {2012},
  abstract  = {Demand forecasting is crucial to electricity providers because
their ability to produce energy exceeds their ability to store it.
Excess demand can cause ?brown outs,? while excess supply
ends in waste. In an industry worth over \$1 trillion in the U.S.
alone [1], almost 9\% of GDP [2], even marginal improvements
can have a huge impact. Any plan toward energy efficiency
should include enhanced utilization of existing production.
Energy loads provide an interesting topic for Machine
Learning techniques due to the availability of large datasets
that exhibit fundamental nonlinear patterns. Using data from
the Kaggle competition Global Energy Forecasting Compe-
tition 2012 - Load Forecasting? [3] we sought to use deep
learning architectures to predict energy loads across different
network grid areas, using only time and temperature data.
Data included hourly demand for four and a half years from
20 different geographic regions, and similar hourly temper-
ature readings from 11 zones. For most of our analysis we
focused on short term load forecasting because this will aide
online operational scheduling.},
  comment   = {Demand learning with three kinds of NN's some deep learning.  Deep Recurrent NN's work the best (I think, it's hard to read) but it's only 1-step ahead forecasting.},
  file      = {Busseti12deepLrnTimeSer.pdf:Busseti12deepLrnTimeSer.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.03.08},
  url       = {http://cs229.stanford.edu/proj2012/BussetiOsbandWong-DeepLearningForTimeSeriesModeling.pdf},
}

@InProceedings{Sharman05whyWindDK,
  author    = {Hugh Sharman},
  title     = {Why wind power works for {Denmark}},
  booktitle = {Proceedings of ICE, Civil Engineering 158},
  year      = {2005},
  volume    = {158},
  number    = {13663},
  pages     = {66--72},
  month     = may,
  abstract  = {Denmark generates more wind power per head of population than any other country in the world. Its 5500 wind turbines, including the world?s two largest offshore wind farms, generate 16\% of national demand. With increasing concerns over fossil fuels, the country is now being closely monitored by energy planners and funders worldwide. However, as this paper reveals, Denmark is exporting most of its wildly fluctuating wind power to larger neighbours while finding other solutions for supply and demand at home. As an ?island? grid based on slow-reacting thermal power stations, Britain may find its comparable wind-power aspirations more difficult to achieve.},
  comment   = {Denmark's advantages for wind power},
  file      = {Sharman05whyWindDK.pdf:Sharman05whyWindDK.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2012.04.10},
}

@Article{Tran06knnDensClustHiDim,
  author    = {Tran, Thanh N and Wehrens, Ron and Buydens, Lutgarde},
  title     = {KNN-kernel density-based clustering for high-dimensional multivariate data},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {2006},
  volume    = {51},
  number    = {2},
  pages     = {513--525},
  abstract  = {Density-based clustering algorithms for multivariate data often have difficulties with high-dimensional data and clusters of very different densities. A new density-based clustering algorithm, called KNNCLUST, is presented in this paper that is able to tackle these situations. It is based on the combination of nonparametric k-nearest-neighbor (KNN) and kernel (KNN-kernel) density estimation. The KNN-kernel density estimation technique makes it possible to model clusters of different densities in high-dimensional data sets. Moreover, the number of clusters is identified automatically by the algorithm. KNNCLUST is tested using simulated data and applied to a multispectral compact airborne spectrographic imager (CASI)_image of a floodplain in the Netherlands to illustrate the characteristics of the method.
Keywords
 Multivariate data;
 Classification;
 Clustering},
  comment   = {Agglom clust w/ a KNN kernel density estimate and Bayes rule. Triangular or Gaussian kernel prevents KNN from following weird shapes, which is I think why you would use KNN. Baseline comparisons are a bit weak, and the claimed high dimensional ability comes from PCA. But has some advantages with mixed sparse and dense classes, and Matlab is available so easy to try.

Advantages
* not sensitive to initial cluster choice b/c it's agglomerative, w/ each point in its own nbhd
* "good for high dim" if use PCA
* good for differing densities: maybe a real advantage for high, low and middle parts of wind power curve (low density at the top)
* figures out the best number of clusters.
* not too sensitve to the value of k, which is the only parameter required (beside the kernel function)
* ==> could be a first stage locLin cluster alg, maybe better than kmeans, with later followup

Disadvantages
* estimated density is really noisy, so not good for locLin kernels
* kernels blur the good "wierd neighborhood" properties of KNN
* performance against modern methods? Baselines were kinda weak.


An update of Tran03knnDensClust, has matlab

Matlab: http://www.cac.science.ru.nl/research/software/tnthanh/knnclust.m
(code comments say this if for Tran03knnDensClust but it looks like it really is for this paper)\
why KNN is not a good density estimator: GutierrezOsuna11kdnnLecNotes},
  file      = {Tran06knnDensClustHiDim.pdf:Tran06knnDensClustHiDim.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.04.21},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167947305002537},
}

@Article{Joe14DepModelCopulasBk,
  author   = {Joe, H},
  title    = {Dependence modeling with Copulas, Chapman Hall, CRC},
  journal  = {Published June/July},
  year     = {2014},
  abstract = {Dependence Modeling with Copulas covers the substantial advances that have taken place in the field during the last 15 years, including vine copula modeling of high-dimensional data. Vine copula models are constructed from a sequence of bivariate copulas. The book develops generalizations of vine copula models, including common and structured factor models that extend from the Gaussian assumption to copulas. It also discusses other multivariate constructions and parametric copula families that have different tail properties and presents extensive material on dependence and tail properties to assist in copula model selection.

The author shows how numerical methods and algorithms for inference and simulation are important in high-dimensional copula applications. He presents the algorithms as pseudocode, illustrating their implementation for high-dimensional copula models. He also incorporates results to determine dependence and tail properties of multivariate distributions for future constructions of copula models},
}

@TechReport{Horowitz19ovrvwDERinterConnNREL,
  author      = {Kelsey Horowitz and Zac Peterson and Michael Coddington and Fei Ding and Ben Sigrin and Danish Saleem and Sara E. Baldwin and Brian Lydic and Sky C. Stanfield and Nadav Enbar and Steven Coley and Aditya Sundararajan and Chris Schroeder},
  title       = {An Overview of Distributed Energy Resource (DER) Interconnection: Current Practices and Emerging Solutions},
  institution = {National Renewable Energy Laboratory (NREL)},
  year        = {2019},
  number      = {NREL/TP-6A20-72102},
  month       = apr,
  abstract    = {Deployment of distributed energy resources (DERs), in particular distributed photovoltaics 
(DPV), has increased in recent years and is anticipated to continue increasing in the future (GTM 
2017, Labastida 2017). The increase has been particularly significant on certain systems. Figure 
1 shows an example of a rapid rise in the number and capacity of DER with net-energy metering 
(NEM) for two different systems in Missouri and South Carolina. 

 
Figure 1. Examples of rapidly accelerating DPV deployment on some U.S. systems: Missouri’s 
Empire District Electric Company and South Carolina Electric & Gas Company (EIA 2017) 
As DER deployment grows, there is a need for utilities and regulators to understand 
considerations for interconnecting these resources to their systems as well as different solutions 
that may be suitable given their DER penetration levels, system characteristics, capabilities, and 
organizational structures. 
This report from the Distributed Generation Interconnection Collaborative (DGIC) was 
commissioned based on the need—identified through DGIC—for a central document 
summarizing considerations, practices, and emerging solutions across a broad set of topics 
related to DER interconnection. The report is targeted at a high-level, strategic-planning 
audience at utilities who are seeking an overview of DER interconnection issues and approaches 
and looking to understand how these may relate to their own situations. The audience includes a 
broad set of utilities and situations, including investor-owned utilities (IOUs), municipal utilities 
(munis), and cooperatives (co-ops) with a range of current DER penetration levels. 
This report complements existing resources, including more detailed research reports on specific 
interconnection-related topics (e.g., Coddington et al. 2012b; Parks, Woerner, and Ardani 2014), 
in-depth handbooks or reports on a specific interconnection-related topic (Seguin et al. 2016), as 
well as the recently published “New Approaches to Distributed PV Interconnection: 
Implementation Considerations for Addressing Emerging Issues” (McAllister forthcoming), 
which is geared more at a policymaker audience and provides a more detailed review of 
interconnection practices at utilities and states. It also provides a broader perspective and some 
forward-looking information not contained in interconnection handbooks or guidebooks 
provided by some utilities (e.g., PG&E 2017b), which provide details of current interconnection 
policies and procedures of individual utilities relevant to interconnection applicants.  

Although some areas of interconnection have established standards, many are still nascent with 
no clear or accepted best practice. Additionally, the practice most suitable for a given situation 
will vary depending on the level of DER penetration; the utility, customer, and developer 
characteristics and preferences; the attributes of the electrical power system; and other factors. 
This report does not seek to recommend or dictate practices in any of these areas, but rather 
provides an overview of the status of different aspects of interconnection, existing standards, and 
emerging solutions currently being explored that can inform utility planning and decisions. Some 
of this information may also be useful to regulators, policymakers, and DER developers seeking 
to understand barriers to interconnection, potential solutions currently being explored, and ways 
to work with utilities on interconnection policies. },
  comment     = {Ch. 7, pp. 44-46 is a nice overview of DER adoption techniques

Main Methods
* Top-down: estimate whole regions, not individuals
   - time-series: extrapolate historical, cyclic data (has a 2017 reference)
   - econometric: techniques intended to explain adoption.  Less accurate but can be trained
   - Bass diffusion: most frequently used, assumes S-shaped curve
   - said to be good for low DER penetration (because errors are less costly?  Because not enough dat for Bottom up?)
* Bottom-up: regional estimate starts w/ individual a.k.a. "agent" predictions   
   - increasingly popular
   - data driven, ML like
   - said to be more accurate than theory-driven top down
   - bad for small sample sizes   
* Hybrid to-down/bottom-up
   - top down forecast with disaggregation to smaller regions.  
   - Not sure how disaggregation would work, though.

Uncertainty
* Should acknowledge it
* Uncertainty types
  - Economic: unknown techno-economic factors
  - Modeling: "relates to the modeling choices made"
* Says should run multiple scenarios, consider uncertainty, consider "interactive effects"

Importance of DER adoption forecasting
* bad DER adoption forecast could cast util as much as \$/MWh of served load (1/4 of current wholesale electricity price)
* adoption forecast should be considered a strategic asset.
},
  file        = {:Horowitz19ovrvwDERinterConnNREL.pdf:PDF},
  url         = {https://www.nrel.gov/dgic/},
}

@MastersThesis{Wildenhues13OptAlocSizDynStatcom,
  author      = {Sebastian Wildenhues},
  title       = {Optimal Allocation And Sizing Of Statcom For Power System Dynamic Performance Enhancement},
  year        = {2013},
  abstract    = {Deregulation and liberalization of the electricity markets, fundamentally changing patterns in production, as well as an ever-increasing power demand and pronounced environmental awareness lead to significant challenges for today?s utilities. Studies of recent blackouts revealed an intricate relationship existing between insufficient reactive power support and unreliable system operation. Besides, the disappearance of large central generating units in some countries introduces a strong demand for strategic implementation of devices providing enhanced control actions concerning the dynamic security and performance of power systems during emergencies. The question, however, is not only to determine, if a compliant system state can be reached in such frameworks, but rather to find out, how this can be achieved in a cost-optimal fashion. This study is therefore devoted to the problem of joint optimal determination of location and size of multiple dynamic VAr sources.
It develops an integrated multi-contingency approach using STATCOM that rigorously exploits technical requirements while ensuring optimality of investments. First, a set of credible disturbances is filtered out through an alternative formulation of contingency severity index (CSI). Recent heuristic Mean-Variance Mapping optimization (MVMO) algorithm is then applied in combination with an intervention scheme ensuring efficient utilization of computational resources, which is essential due to consideration of the full dynamic system model. A so called Trajectory Violation Integral (TVI) is introduced to determine solution?s feasibility and continuously control the evolutionary search process. The methodology is applied to the IEEE New England 39-Bus test system while PowerFactory and Matlab software packages are used. Fast and robust communication is realized by an user-written C++-interface. The results show sensitivities of STATCOM allocation with respect to three major factors, including: fault clearing time, transient voltage requirements and dynamic load portion. Finally, cost versus dynamic performance benefit is further improved by introducing a method that explicitly makes use of economies of scale: large devices are preferred over smaller ones attained from the optimization by proper adjustment of voltage control droop characteristics. An adaptive search space modification ensures that the iterative concentration proceeds in a controlled and reasonable way.},
  comment     = {Master's thesis of the new IWES guy who wants to collaborate on dynamic redispatch.},
  file        = {Wildenhues13OptAlocSizDynStatcom.pdf:Wildenhues13OptAlocSizDynStatcom.pdf:PDF;Wildenhues13OptAlocSizDynStatcom.pdf:Wildenhues13OptAlocSizDynStatcom.pdf:PDF},
  institution = {University of Duisburg-Essen},
  keywords    = {fred},
  owner       = {sotterson},
  timestamp   = {2014.07.09},
  url         = {http://duepublico.uni-duisburg-essen.de/servlets/DocumentServlet?id=31859},
}

@InCollection{Elidan13copulasMachLrn,
  author    = {Elidan, Gal},
  title     = {Copulas in machine learning},
  booktitle = {Copulae in mathematical and quantitative finance},
  publisher = {Springer},
  year      = {2013},
  pages     = {39--60},
  abstract  = {Despite overlapping goals of multivariate modeling and dependence identification, until recently the fields of machine learning in general and probabilistic graphical models in particular have been ignorant of the framework of copulas. At the same time, the complementing strengths of the two fields suggests the great fruitfulness of a synergy. The purpose of this paper is to survey recent copula-based constructions in the field of machine learning, so as to provide a stepping stone for those interested in further exploring this emerging symbiotic research.},
  comment   = {Covers vine copulas, copula networks, and copula processes.

Related slides: Elidan12copulaMachLrnTut

Extension to multimodalit, sensor networkd: Liu17multDatFusCplaProc},
  file      = {Elidan13copulasMachLrn.pdf:Elidan13copulasMachLrn.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.01.26},
  url       = {http://link.springer.com/chapter/10.1007/978-3-642-35407-6_3},
}

@Article{Xu16adaptStrmAnomAnlysTsubmod,
  author    = {Xu, Zhao and von Ritter, Lorenzo and Kersting, Kristian},
  title     = {Adaptive Streaming Anomaly Analysis},
  journal   = {NIPS},
  year      = {2016},
  month     = dec,
  abstract  = {Detecting anomalous activities from time series data is critical for enhancing avail-
ability and security of systems in many domains. Streaming data usually contains
complex dynamic patterns, which complicates the learning process. In this paper,
we present a nonparametric Bayesian method AOTS to help automating the model
learning for anomaly detection in streaming time series. The method learns the dy-
namics of anomaly-contaminated time series with submodular optimization based
kernel selection to effectively adapt to the data and identify potential anomalous
events. Experiments on real data show encouraging results.},
  comment   = {Student T process anomaly detection with submodular optimization kernel selection.},
  file      = {:Xu16adaptStrmAnomAnlysTsubmod.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.20},
  url       = {http://workshops.inf.ed.ac.uk/nips2016-ai4datasci/submission.html},
}

@Article{Blythe12FeatExtrctChngPtSubSpc,
  author    = {D. A. J. Blythe and P. von Bunau and F. C. Meinecke and K. R. Muller},
  title     = {Feature Extraction for Change-Point Detection Using Stationary Subspace Analysis},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  year      = {2012},
  volume    = {23},
  number    = {4},
  pages     = {631--643},
  month     = apr,
  issn      = {2162-237X},
  abstract  = {Detecting changes in high-dimensional time series is difficult because it involves the comparison of probability densities that need to be estimated from finite samples. In this paper, we present the first feature extraction method tailored to change-point detection, which is based on an extended version of stationary subspace analysis. We reduce the dimensionality of the data to the most nonstationary directions, which are most informative for detecting state changes in the time series. In extensive simulations on synthetic data, we show that the accuracy of three change-point detection algorithms is significantly increased by a prior feature extraction step. These findings are confirmed in an application to industrial fault monitoring.},
  comment   = {subspace dimension analysis for changepoint detect.  Good for modernwindabs.

Belloni13progEvalHiDim says featsel (reduction too?) is hard when also doing inference},
  doi       = {10.1109/TNNLS.2012.2185811},
  file      = {Blythe12FeatExtrctChngPtSubSpc.pdf:Blythe12FeatExtrctChngPtSubSpc.pdf:PDF},
  keywords  = {condition monitoring, feature extraction, object detection, production engineering computing, statistical analysis, time series, change-point detection, feature extraction, high-dimensional time series, industrial fault monitoring, nonstationary direction, probability density, state change detection, stationary subspace analysis, synthetic data, Accuracy, Covariance matrix, Data models, Detection algorithms, Feature extraction, Monitoring, Time series analysis, Change-point detection, feature extraction, high-dimensional data, segmentation, stationarity, time-series analysis},
  owner     = {sotterson},
  timestamp = {2016.12.19},
}

@Unpublished{Asghar07perfLagLenSelCrit,
  author    = {Zahid Asghar and , Irum Abid},
  title     = {Performance Of Lag Length Selection Criteria In Three Different Situations},
  note      = {Self published, I thnk: http://interstat.statjournals.net/},
  month     = apr,
  year      = {2007},
  abstract  = {Determination of the lag length of an autoregressive process is one of the most difficult parts of ARIMA modeling. Various lag length selection criteria (Akaike Information Criterion, Schwarz Information Criterion, Hannan-Quinn Criterion, Final Prediction Error, Corrected version of AIC) have been proposed in the literature to overcome this difficulty. We have compared these criteria for lag length selection for three different cases that is under normal errors, under non-normal errors and under structural break by using Monte Carlo simulation. It has been found that SIC is the best for large samples and no criteria is not useful for selecting true lag length in presence of regime shifts or shocks to the system.},
  comment   = {A change in AR input noise mean screws up AR order estimation; SIC is best method but it's not good * forecasting w/ AR and estimting lags (kinda like AR order) will be a problem under non-stationarity * "structural break" is change in mean of AR input noise. Don't say how big the break is, though -- this had caused bad errors in AR modeling on some other work the authors did * SIC picks order the best but it's right only 10 pct of the time AR order estimating methods tested (equations given, so maybe this will be useful somewhere else?) 1. Akaike's informatino criterion: AIC 2. Schwarz information criterion: SIC 3. Hannan-Quinn criterion: HQC 4. Final prediction error: FPE 5. Corrected version of AIC: AICC},
  file      = {Asghar07perfLagLenSelCrit.pdf:Asghar07perfLagLenSelCrit.pdf:PDF;Asghar07perfLagLenSelCrit.pdf:Asghar07perfLagLenSelCrit.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.02.09},
  url       = {http://interstat.statjournals.net/YEAR/2007/abstracts/0704001.php},
}

@Article{Carroll16failRateRepairOffshrWind,
  author   = {Carroll, James and McDonald, Alasdair and McMillan, David},
  title    = {Failure rate, repair time and unscheduled O\&M cost analysis of offshore wind turbines},
  journal  = {Wind Energy},
  year     = {2016},
  volume   = {19},
  number   = {6},
  pages    = {1107--1119},
  issn     = {1099-1824},
  abstract = {Determining and understanding offshore wind turbine failure rates and resource requirement for repair are vital for modelling and reducing O&M costs and in turn reducing the cost of energy. While few offshore failure rates have been published in the past even less details on resource requirement for repair exist in the public domain. Based on ~350 offshore wind turbines throughout Europe this paper provides failure rates for the overall wind turbine and its sub-assemblies. It also provides failure rates by year of operation, cost category and failure modes for the components/sub-assemblies that are the highest contributor to the overall failure rate. Repair times, average repair costs and average number of technicians required for repair are also detailed in this paper. An onshore to offshore failure rate comparison is carried out for generators and converters based on this analysis and an analysis carried out in a past publication. The results of this paper will contribute to offshore wind O&M cost and resource modelling and aid in better decision making for O&M planners and managers. },
  comment  = {What breaks when in offshore wind turbines.  Read for ModernWindABS, and give to Marek},
  doi      = {10.1002/we.1887},
  file     = {:Carroll16failRateRepairOffshrWind.pdf:PDF},
  keywords = {failure mode, failure rate, offshore wind turbine, reliability},
  url      = {http://dx.doi.org/10.1002/we.1887},
}

@Misc{Altman09glmNonlinMdlCourseNts,
  author       = {Rachel MacKay Altman},
  title        = {Generalized Linear and Nonlinear Modelling},
  howpublished = {Course Notes: Statistics 402, Simon Fraser University, Canada},
  year         = {2009},
  abstract     = {Deviance is an important idea associated with a fitted GLM. It can be used to test the fit of
the link function and linear predictor to the data, or to test the significance of a particular
predictor variable (or variables) in the model. The following discussion applies to GLMs
where the random component is in the 1-parameter exponential family (e.g., Poisson and
binomial).},
  comment      = {Statistics Class Course notes.  Pretty good.

Lecture 11: Deviance performance metric for a generalized linear model.

Lecture 32: Survival and Hazard Functions
* Definitions 
   - survival function: probabilty of an event not having happened yet (1 - the cdf)
   - hazard function: probability of an event happening in some time range
      - it's stated that this isn't a probablity but it is, as Grace-Martin19hazardFunc makes clear
      - so see Grace-Martin19hazardFunc for a more intuitive but less mathy definition.
   - baseline hazard function (below)
   - proportional hazards model
      - can build one with a GLM
      - here, it's a Weibull distribution with fixed lambda, and theta varying as a linear function of input variables, x
      - baseline hazard: hazard predicted when all of x are zero
      - maybe there are other kinds of proportional hazard models
* How to determine if an exponential is a good fit using a Weibull model.},
  file         = {:Altman09glmNonlinMdlCourseNts.pdf:PDF},
  url          = {http://people.stat.sfu.ca/~raltman/stat402.html},
}

@Article{Radovanovic10knnHubsSpc,
  author    = {Radovanovi{\'c}, Milo{\v{s}} and Nanopoulos, Alexandros and Ivanovi{\'c}, Mirjana},
  title     = {Hubs in space: Popular nearest neighbors in high-dimensional data},
  journal   = {The Journal of Machine Learning Research},
  year      = {2010},
  volume    = {9999},
  pages     = {2487--2531},
  abstract  = {Different aspects of the curse of dimensionality are known to present serious challenges to various machine-learning methods and tasks. This paper explores a new aspect of the dimensionality curse, referred to as hubness, that affects the distribution of k-occurrences: the number of times a point appears among the k nearest neighbors of other points in a data set. Through theoretical and empirical analysis involving synthetic and real data sets we show that under commonly used assumptions this distribution becomes considerably skewed as dimensionality increases, causing the emergence of hubs, that is, points with very high k-occurrences which effectively represent "popular" nearest neighbors. We examine the origins of this phenomenon, showing that it is an inherent property of data distributions in high-dimensional vector space, discuss its interaction with dimensionality reduction, and explore its influence on a wide range of machine-learning tasks directly or indirectly based on measuring distances, belonging to supervised, semi-supervised, and unsupervised learning families.},
  comment   = {Another KNN high dim hubs paper, explaining how plain nearest neighbhor distance has problems in high dims. The 2009 paper by these authors seems to be one of the orignal explanations of the idea. Is this one better?},
  file      = {Radovanovic10knnHubsSpc.pdf:Radovanovic10knnHubsSpc.pdf:PDF},
  owner     = {sotterson},
  publisher = {JMLR. org},
  timestamp = {2014.04.04},
  url       = {http://dl.acm.org/citation.cfm?id=1953015},
}

@Article{Kusiak09powerMonModel,
  author    = {Kusiak, Andrew and Zheng, Haiyang and Song, Zhe},
  title     = {Models for monitoring wind farm power},
  journal   = {Renewable Energy},
  year      = {2009},
  volume    = {34},
  number    = {3},
  pages     = {583--590},
  abstract  = {Different models for monitoring wind farm power output are considered. Data mining and evolutionary computation are integrated for building the models for prediction and monitoring. Different models using wind speed as input to predict the total power output of a wind farm are compared and analyzed. The k-nearest neighbor model, combined with the principal component analysis approach, outperforms other models studied in this research. However, this model performs poorly when the conditions of the wind farm are abnormal. The latter implies that the original data contains many noisy points that need to be ?ltered. An evolutionary computation algorithm is used to build a nonlinear parametric model to monitor the wind farm performance. This model ?lters the outliers according to the residual approach and control charts. The k-nearest neighbor model produces good performance for the wind farm operating in normal conditions. Keywords: Wind farm Data mining Power prediction Monitoring Evolutionary computation Control chart},
  comment   = {Power curve learning, curtailment detection},
  file      = {Kusiak09powerMonModel.pdf:Kusiak09powerMonModel.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2013.02.21},
  url       = {http://www.engineering.uiowa.edu/~ankusiak/journal-papers/Renewable_Energy_1.pdf},
}

@Article{Duggan17bassMetaSpace,
  author   = {Duggan, Jim},
  title    = {Implementing a Metapopulation Bass Diffusion Model using the R Package deSolve.},
  journal  = {R Journal},
  year     = {2017},
  volume   = {9},
  number   = {1},
  abstract = {Diffusion is a fundamental process in physical, biological, social and economic settings.
Consumer products often go viral, with sales driven by the word of mouth effect, as their adoption
spreads through a population. The classic diffusion model used for product adoption is the Bass
diffusion model, and this divides a population into two groups of people: potential adopters who
are likely to adopt a product, and adopters who have purchased the product, and influence others
to adopt. The Bass diffusion model is normally captured in an aggregate form, where no significant
consumer differences are modeled. This paper extends the Bass model to capture a spatial perspective,
using metapopulation equations from the field of infectious disease modeling. The paper’s focus is on
simulation of deterministic models by solving ordinary differential equations, and does not encompass
parameter estimation. The metapopulation model in implemented in R using the deSolve package,
and shows the potential of using the R framework to implement large-scale integral equation models,
with applications in the field of marketing and consumer behaviour.},
  comment  = {A spatial Bass diffusion model.

Could be used in something like e.g. Dong17resPVdeployFrcst},
  file     = {:Duggan17bassMetaSpace.pdf:PDF},
  url      = {https://journal.r-project.org/archive/2017/RJ-2017-006/RJ-2017-006.pdf},
}

@Article{Adragni09suffDimRed,
  author    = {Adragni, Kofi P. and Cook, R. Dennis},
  title     = {Sufficient dimension reduction and prediction in regression},
  journal   = {Philosophical Transactions of the Royal Society A: Mathematical,Physical and Engineering Sciences},
  year      = {2009},
  volume    = {367},
  number    = {1906},
  pages     = {4385--4405},
  abstract  = {Dimension reduction for regression is a prominent issue today because technological advances now allow scientists to routinely formulate regressions in which the number of predictors is considerably larger than in the past. While several methods have been proposed to deal with such regressions, principal components (PCs) still seem to be the most widely used across the applied sciences. We give a broad overview of ideas underlying a particular class of methods for dimension reduction that includes PCs, along with an introduction to the corresponding methodology. New methods are proposed for prediction in regressions with many predictors.},
  comment   = {Dimension reduction while preserving regression performance. Use for pruning down a lot of lagged wind velocity basis coefficients One implementation in Tomassi11suffDimRedPkg ?},
  doi       = {10.1098/rsta.2009.0110},
  eprint    = {http://rsta.royalsocietypublishing.org/content/367/1906/4385.full.pdf+html},
  file      = {Adragni09suffDimRed.pdf:Adragni09suffDimRed.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.08.31},
  url       = {http://rsta.royalsocietypublishing.org/content/367/1906/4385.abstract},
}

@Article{Berens09circStatMatlab,
  author    = {Philipp Berens},
  title     = {CircStat: A MATLAB Toolbox for Circular Statistics},
  year      = {2009},
  volume    = {31},
  number    = {10},
  month     = sep,
  pages     = {1--21},
  issn      = {1548-7660},
  url       = {http://www.jstatsoft.org/v31/i10},
  abstract  = {Directional data is ubiquitious in science. Due to its circular nature such data cannot be analyzed with commonly used statistical techniques. Despite the rapid development of specialized methods for directional statistics over the last fty years, there is only little software available that makes such methods easy to use for practioners. Most importantly, one of the most commonly used programming languages in biosciences, MATLAB, is currently not supporting directional statistics. To remedy this situation, we have implemented the CircStat toolbox for MATLAB which provides methods for the descriptive and inferential statistical analysis of directional data. We cover the statistical background of the available methods and describe how to apply them to data. Finally, we analyze a dataset from neurophysiology to demonstrate the capabilities of the CircStat toolbox.},
  accepted  = {2009-08-26},
  bibdate   = {2009-08-26},
  coden     = {JSSOBK},
  day       = {23},
  file      = {Berens09circStatMatlab.pdf:Berens09circStatMatlab.pdf:PDF},
  journal   = {Journal of Statistical Software},
  owner     = {scot},
  submitted = {2009-07-27},
  timestamp = {2010.08.05},
}

@Article{Zhang16copulaConvPowSys,
  author   = {N. Zhang and C. Kang and C. Singh and Q. Xia},
  title    = {Copula Based Dependent Discrete Convolution for Power System Uncertainty Analysis},
  journal  = {IEEE Transactions on Power Systems},
  year     = {2016},
  volume   = {31},
  number   = {6},
  pages    = {5204-5205},
  month    = {Nov},
  issn     = {0885-8950},
  abstract = {Discrete convolution (DC) is a generally accepted approach for the probabilistic analysis such as reliability assessment and probabilistic load flow. However, it has a strong precondition that the stochastic variables being convolved must be independent, which may not be fully satisfied in all cases. Using copula functions, this letter derives the formulation of DC for dependent variables. The performance of the proposed dependent discrete convolution (DDC) is illustrated using reliability assessment involving wind power. The result shows that the DDC inherits the efficient and reliable performance of DC, indicating a promising potential for practical applications.},
  comment  = {This is a way to convolve dependent quantiles!  Would be an improvement over the way Germany does it as of 2017: Maurer09dimSTctlRsrvProb, which convolves just assuming independence},
  doi      = {10.1109/TPWRS.2016.2521328},
  file     = {Zhang16copulaConvPowSys.pdf:Zhang16copulaConvPowSys.pdf:PDF},
  keywords = {convolution;load flow;power system reliability;probability;DDC;copula based dependent discrete convolution;power system uncertainty analysis;probabilistic load flow analysis;reliability assessment;Convolution;Mathematical model;Power system reliability;Probabilistic logic;Reliability;Wind farms;Convolution;copula;dependent;reliability;wind power},
}

@Article{Blinn93dealDCT,
  author    = {Blinn, J.F.},
  title     = {What's that deal with the {DCT}?},
  journal   = {Computer Graphics and Applications, IEEE},
  year      = {1993},
  volume    = {13},
  number    = {4},
  pages     = {78--83},
  month     = jul,
  issn      = {0272-1716},
  abstract  = {Discrete cosine transforms (DCTs) and discrete Fourier transforms (DFTs) are reviewed in order to determine why DCTs are more popular for image compression than the easier-to-compute DFTs. DCT-based image compression takes advantage of the fact that most images do not have much energy in the high-frequency coefficients. It is suggested that DCTs are more popular because fewer DCT coefficients than DFT coefficients are needed to get a good approximation to a typical signal, since the higher-frequency coefficients are small in magnitude and can be more crudely quantized than the low-frequency coefficients.},
  comment   = {Why use DCT instead of DFT * periodic extension at edges is mirrored, making it smoother * reconstruction is better, less ringy, therefore * can use fewer coeffs for low freq images (are wind signals low freq too?) Can check this. * good for real signals},
  doi       = {10.1109/38.219457},
  file      = {Blinn93dealDCT.pdf:Blinn93dealDCT.pdf:PDF},
  keywords  = {DCT-based image compression;discrete Fourier transforms;discrete cosine transforms;higher-frequency coefficients;Fourier transforms;data compression;discrete cosine transforms;image processing;},
  owner     = {sotterson},
  timestamp = {2012.12.06},
}

@Article{Golub99tikhRegTotLSQ,
  author    = {Golub, Gene H and Hansen, Per Christian and O'Leary, Dianne P},
  title     = {Tikhonov regularization and total least squares},
  journal   = {SIAM Journal on Matrix Analysis and Applications},
  year      = {1999},
  volume    = {21},
  number    = {1},
  pages     = {185--194},
  abstract  = {Discretizations of inverse problems lead to systems of linear equations with a highly
ill-conditioned coefficient matrix, and in order to compute stable solutions to these systems it is
necessary to apply regularization methods. We show how Tikhonov?s regularization method, which
in its original formulation involves a least squares problem, can be recast in a total least squares
formulation suited for problems in which both the coefficient matrix and the right-hand side are
known only approximately. We analyze the regularizing properties of this method and demonstrate
by a numerical example that, in certain cases with large perturbations, the new method is superior
to standard regularization methods.

Key words. total least squares, discrete ill-posed problems, regularization, bidiagonalization},
  comment   = {How to regularize like with ridge regression but with an arbitrary penalty matrix.  Can be solved with a least-squares approach.  Related to pspline, and maybe good for multivariate pspline or maybe even regularized quantile regression.},
  file      = {Golub99tikhRegTotLSQ.pdf:Golub99tikhRegTotLSQ.pdf:PDF},
  publisher = {SIAM},
}

@Article{Szekely09brownDistCov,
  author    = {Sz{\'e}kely, G{\'a}bor J and Rizzo, Maria L and others},
  title     = {Brownian distance covariance},
  journal   = {The annals of applied statistics},
  year      = {2009},
  volume    = {3},
  number    = {4},
  pages     = {1236--1265},
  abstract  = {Distance correlation is a new class of multivariate dependence coefficients applicable to random vectors of arbitrary and not necessarily equal dimension. Distance covariance and distance correlation are analogous to product-moment covariance and correlation, but generalize and extend these classical bivariate measures of dependence. Distance correlation characterizes independence: it is zero if and only if the random vectors are independent. The notion of covariance with respect to a stochastic process is introduced, and it is shown that population distance covariance coincides with the covariance with respect to Brownian motion; thus, both can be called Brownian distance covariance. In the bivariate case, Brownian covariance is the natural extension of product-moment covariance, as we obtain Pearson product-moment covariance by replacing the Brownian motion in the definition with identity. The corresponding statistic has an elegantly simple computing formula. Advantages of applying Brownian covariance and correlation vs the classical Pearson covariance and correlation are discussed and illustrated.},
  comment   = {A  nonlinear, multivariate dependence test that is fast to compute, perhaps a faster, cheaper version of mutual information, possibly without dimension bias?

LNEG used this to select principal components for NWP undertainty features in IRPWIND WP82.3.

Szekely12distCovUniq shows that it's unique.},
  file      = {:Szekely09brownDistCov.pdf:PDF},
  publisher = {Institute of Mathematical Statistics},
  url       = {https://projecteuclid.org/euclid.aoas/1267453933},
}

@Article{Szekely12distCovUniq,
  author    = {Sz{\'e}kely, G{\'a}bor J and Rizzo, Maria L},
  title     = {On the uniqueness of distance covariance},
  journal   = {Statistics \& Probability Letters},
  year      = {2012},
  volume    = {82},
  number    = {12},
  pages     = {2278--2282},
  abstract  = {Distance covariance and distance correlation are non-negative real numbers that characterize the independence of random vectors in arbitrary dimensions. In this work we prove that distance covariance is unique, starting from a definition of a covariance as a weighted L2 norm that measures the distance between the joint characteristic function of two random vectors and the product of their marginal characteristic functions. Rigid motion invariance and scale equivariance of these weighted L2 norms imply that the weight function of distance covariance is unique.},
  comment   = {See also Szekely09brownDistCov},
  file      = {:Szekely12distCovUniq.pdf:PDF},
  publisher = {Elsevier},
  url       = {https://www.sciencedirect.com/science/article/pii/S0167715212003124},
}

@Article{Polasek90vecDistLagSmooth,
  author    = {Wolfgang Polasek},
  title     = {Vector distributed lag models with smoothness priors},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {1990},
  volume    = {10},
  number    = {2},
  pages     = {133--141},
  issn      = {0167-9473},
  abstract  = {Distributed lag models with smoothness priors have been found useful in econometrics since they were introduced by Shiller [12] but the estimation of the smoothness parameters is still an open problem. The assumption of smooth distributed lag or regression coefficients requires the estimation of variance (hyper-) parameters and the order of the lag distribution. Akaike [1] suggested for univariate distributed lag models an estimation procedure which maximizes the conditional likelihood function of the hyperparameters. This empirical Bayes method is extended to the multivariate case for models with simple covariance structures. For complicated models a fully Bayesian approach based on the posterior-imputation (PI) algorithm of Tanner and Wong [14] is suggested.},
  comment   = {Multivariate input AND output distributed lag model; blown out lags. These methods smooth directly on the linear coefficients without imposing a function on them. The coefficients are constrained by coefficient derivative limits (I think).


Bayesianly smoothed Distributed lags aren't the result of spline or polynomial interpolation, like other "distributed" lag papers but a Bayesian smoothness prior does smooth them in several different arrangements. Since lags are fully blown out, there are a lot of parameters and a big computational load. Seems to me that spline smoothing constraint is better unless you have a ton of data to make this highlly parameterized model work (although maybe the Bayesian stuff is good-enough at regularizing it).

My take on the comparison w/ spline/basis polynomial functions: can have lots of parameters and regularize them (here) or have only a few parameters which obey a restricted relationship. Anyway, in case it wasn't clear before, eq (1.1) tells you that you can separately expand lags (either by literally blowing them out or by using a basis like a spline or polynomial) and then do linear regression.

Three kinds of lagged regression models:
1.) bivariate: dim(out, in) = [1,1]
2.) multivariate: dim(out,in) = [1,N]
3.) vector DL: dim(out,in) = [M,N]

Three kinds of parameter estimation:

1.) BASIC
* Brute force blow out of all lags, do linear regression on that, but with constrained coefficients.
* Split into same-variable logged column so can enforce a temporal smoothness constraint on each variable's lag coefficients.
* Smoothing on one variable is independent from the next.
* No interaction between output dimensions

2.) GENERAL
* Hierarchical: two normal dist's assumed, one in each "stage"
-- output error
-- departure from perfect "smoothness"
* can allow different smoothing difference cost functions
* but the joint smoothing error dist. can make smoothing dependent across variables (maybe you would care about this for spatially ordered variables, or something, where you know there's a relationship)
* no relationship between output dimensions
* no joint cost function between smoothing and output error (but you could say "total likelihood is the product of the to Normals or something)

3.) VECTOR
* do Bayesian stuff on the ratio of variances of each stage: makes them connected
* this is getting too complex so I didn't figure out why this is better...},
  doi       = {DOI: 10.1016/0167-9473(90)90058-P},
  file      = {Polasek90vecDistLagSmooth.pdf:Polasek90vecDistLagSmooth.pdf:PDF},
  groups    = {Read},
  keywords  = {Smoothness priors},
  owner     = {scot},
  timestamp = {2010.08.05},
  url       = {http://www.sciencedirect.com/science/article/B6V8V-45D9T59-V/2/fe61140be78607c03ac9d23ca4e0b4b6},
}

@TechReport{Kwasnik19adoptDERagentRsltn,
  author      = {Kwasnik, Ted and Sigrin, Benjamin O and Bielen, David A},
  title       = {Quantifying Resolution Implications for Agent-based Distributed Energy Resource Customer Adoption Models},
  institution = {National Renewable Energy Lab.(NREL), Golden, CO (United States)},
  year        = {2019},
  abstract    = {Distributed photovoltaics (DPV) are a growing source of electricity generation in the United 
States, and with adoption driven by customer behavior and localized economics, projecting the 
deployment of this technology is a challenging analytical problem. Moreover, understanding the 
sources of uncertainty in customer adoption models and how they can be reduced is important to 
a range of stakeholders that use their outputs, including grid planners, regulators, and industry. 
Most prior studies have used top-down methods, such as the use of population central tendencies 
to project aggregate adoption. In contrast, a growing field of work seeks to use bottom-up 
methods (i.e., individual-level decision-making).  
We explore trade-offs of top-down and bottom-up methods in their precision and computational 
burden using the National Renewable Energy Laboratory’s (NREL’s) Distributed Generation 
Market Demand (dGen) model, an agent-based model of residential and nonresidential 
distributed PV adoption. In particular, we assess the role of agent resolution in instantiating 
statistically-representative populations in the model—and the resulting variance of model 
projections at the state, sector, and county levels. At low sampling rates, the model resembles a 
top-down model, whereas as sampling rates increase dGen converges to a bottom-up structure 
by simulating more unique customer types. Though sampling-based models such as dGen can 
be operated with many agents to ensure accuracy, doing so greatly increases the computational 
burden of the simulation. This report lends insight into whether high-resolution results can be 
approximated sufficiently well using fewer computational resources. 
At the state and sector levels, we find systematic differences in mean projected DPV capacity 
adopted across different levels of agent resolution, namely that cumulative DPV adoption 
estimates decrease with greater agent resolution. Following this, we conclude that sets of low 
agent runs in dGen cannot be directly substituted for high-resolution runs. Moreover, we find 
that the average size of installed DPV explains decreases in system sizes by agent resolution, and 
we attribute this trend to a system-sizing mechanism that biases smaller sizes when agent annual 
load and roof area suitable to PV are not positively correlated. Thus, associating suitable roof 
area with annual load on an agent by agent basis in a manner more consistent with the 
relationship inherent in input data sets is expected to improve the interpretability of results. 
At the county level, we generally see similar trends to the state and sector levels; however, 
we also note the rates at which DPV estimates actually increase with agent resolution.  
We also find that the variance in projected DPV capacity adoption decreases substantially as 
agent resolution increases. This trend holds at the state and sector levels and for most cases at the 
county level. We find that precision improvements in both sectors are greatest at low levels of 
agent resolution, which matches our intuition that estimates will differ less across model runs as 
underlying samples increasingly resemble the full population. We also find reason to limit agent 
resolution in that each additional agent adds about 3 minutes to a minimum solve time of 18 
minutes. Also, significantly fewer than 100 iterations may be sufficient to achieve acceptable 
levels of precision; state-level variance varies by less than 5% beyond about 50 runs relative to 
the average of 100 runs. At the county level, we can explain most of the variance in the 
nonresidential sector, yet this scale reveals trends that diverge from those observed at the state 
and sector levels that are not fully explained. Therefore, we encourage further research into 
sources of variance in both sectors at this more-refined resolution.  },
  file        = {:Kwasnik19adoptDERagentRsltn.pdf:PDF},
  url         = {https://www.nrel.gov/analysis/dgen/publications.html},
}

@TechReport{Sigrin18uncertMktMdlPVmultiMdl,
  author      = {Sigrin, Benjamin O and Kwasnik, Ted and Spitsen, Paul and Fickling, Meera and Jarzomski, Kevin and Boedecker, Erin},
  title       = {Market and Modeling Uncertainty in Distributed Solar Deployment Projections: A Multi-Model Comparison},
  institution = {National Renewable Energy Lab.(NREL), Golden, CO (United States)},
  year        = {2018},
  abstract    = {Distributed solar photovoltaic (DPV) systems that generate energy in behind-the-meter 
applications for residential, commercial, and other end-use sectors are a growing—and 
potentially disruptive—development in the U.S. power system. While less than 1% of all 
electricity generation in the United States came from DPV systems in 2016 (EIA 20171), this 
technology has experienced rapid growth in recent years and, as of the end of 2017, over 1.6 
million DPV systems had been installed (GTM 2017). Given this, projecting distributed solar 
deployment is increasingly pertinent—yet remains highly uncertain. 
The traditional U.S. power system has historically consisted of large, centralized baseload 
generators connected to demand centers via a hub-and-spoke network of synchronized voltage 
transmission and distribution lines. Utilities have also historically financed most infrastructure 
with charges apportioned among customers on a volumetric basis. DPV systems disrupt both 
prior foundations of the U.S. electric power system. Not only are DPV systems typically low 
capacity, but their generation directly offsets on-site electricity consumption or is exported to the 
grid in the case of excess, thereby reducing electricity sales. Recent declines in DPV costs has 
prompted questions regarding the conditions in which consumers might find electricity sourced 
from DPV more economically compelling than grid-sourced electricity. Sometimes termed “grid 
parity”, this phenomenon could accelerate DPV adoption. Clarity about whether DPV might 
achieve grid parity, the timing and extent of this transition, and future levels of DPV deployment 
are important factors to consider for power system operational conditions, engineering and 
financial risk, and long-term planning of the U.S. electric grid.  
To address these issues, we project trends in the net present value of residential-scale solar 
systems on a county-level basis from 2017 through 2050. Next, we combine the long-term 
projections of three national DPV adoption models to examine their short and long-term outlooks 
and sensitivities to future macroeconomic conditions. Finally, we also address the inherent 
uncertainty in projections from two distinct perspectives: (1) economic uncertainty, or the set of 
techno-economic factors that significantly impact the real world as well as our model 
projections, and (2) modeling uncertainty, or the differing implications of choices made 
regarding modeling methodology in computational simulations.  
The analysis conducted for this report is relatively static and does not capture the full range of 
factors influencing electricity markets. Nevertheless, it is intended to be instructive of the range 
of potential DPV deployment should grid conditions remain similar to today’s. Future work 
could consider other potentially disruptive factors, such as influence of electric vehicles, energy 
storage, competition between utility-scale, community-scale and distributed-scale solar systems, 
and impacts of integrating high levels of distributed solar into the electrical grid. For instance, 
distribution grid integration limits could limit the actual deployable potential or introduce new 
system integration costs not considered in this analysis. Perhaps most significant are 
“endogenous” changes; that is, evolution in retail rate design, policy, and market structure as a 
response to the expansion of DPV to better reflect the value of distributed generation.  },
  file        = {:Sigrin18uncertMktMdlPVmultiMdl.pdf:PDF},
  url         = {https://www.nrel.gov/analysis/dgen/publications.html},
}

@Article{Verkuilen12mixRgrsBoundBeta,
  author    = {Verkuilen, Jay and Smithson, Michael},
  title     = {Mixed and mixture regression models for continuous bounded responses using the beta distribution},
  journal   = {Journal of Educational and Behavioral Statistics},
  year      = {2012},
  volume    = {37},
  number    = {1},
  pages     = {82--113},
  abstract  = {Doubly bounded continuous data are common in the social and behavioral sciences. Examples include judged probabilities, confidence ratings, derived proportions such as percent time on task, and bounded scale scores. Dependent variables of this kind are often difficult to analyze using normal theory models because their distributions may be quite poorly modeled by the normal distribution. The authors extend the beta-distributed generalized linear model (GLM) proposed in Smithson and Verkuilen (2006) to discrete and continuous mixtures of beta distributions, which enables modeling dependent data structures commonly found in real settings. The authors discuss estimation using both deterministic marginal maximum likelihood and stochastic Markov chain Monte Carlo (MCMC) methods. The results are illustrated using three data sets from cognitive psychology experiments.
Keywords: beta distribution; general linear model; mixed model; mixture model},
  comment   = {Multivariate regression for bounded output, done as a mixture. MCMC based.},
  file      = {Verkuilen12mixRgrsBoundBeta.pdf:Verkuilen12mixRgrsBoundBeta.pdf:PDF},
  owner     = {sotterson},
  publisher = {SAGE Publications},
  timestamp = {2014.07.18},
  url       = {http://jeb.sagepub.com/content/37/1/82.short},
}

@Article{Lowery12frcstErrImpactUC,
  author    = {Lowery, C. and O'Malley, M.},
  title     = {Impact of Wind Forecast Error Statistics Upon Unit Commitment},
  journal   = {Sustainable Energy, IEEE Transactions on},
  year      = {2012},
  volume    = {3},
  number    = {4},
  pages     = {760--768},
  issn      = {1949-3029},
  abstract  = {Driven by a trend towards renewable forms of generation, in particular wind, the nature of power system operation is changing. Systems with high wind penetrations should be capable of managing the uncertainty contained within the wind power forecasts. Stochastic unit commitment with rolling planning and input scenarios, based on wind forecasts, is one way of achieving this. Here, a scenario tree tool is developed which allows forecast error statistics to be altered and facilitates the study of how these statistics impact on unit commitment and system operation. It is shown that the largest individual impact on system operation is from the inclusion of variance and that variance, kurtosis, and skewness together produced the error information with the lowest system cost. Similar impacts for inaccurate error statistics are observed but generalization of these results will need more studies on a range of test systems.},
  comment   = {I think this shows the value of characterizing the error distribution correctly e.g. a probabilistic forecast.  What kinds of wind forecast errors are bad for stoch. unit commitment; explanation of WILMAR and scenario tree reduction. Related presentation slides attached too.

Slides came from:
WORKSHOP ON VARIABLE GENERATIION FORECASTIING APPLIICATIIONS TO UTIILIITY PLANNIING AND OPERATIIONS
February 23-24, 2011
Crown Plaza Hotel
Albany, NY},
  doi       = {10.1109/TSTE.2012.2210150},
  file      = {Journal Paper:Lowery12frcstErrImpactUC.pdf:PDF;Slides:Lowery12frcstErrImpactUC_slides.pdf:PDF},
  groups    = {Test, Use, doReadWPV_2},
  keywords  = {error statistics;load forecasting;power generation scheduling;stochastic processes;wind power;error information;kurtosis;power system operation;renewable energy sources;rolling planning;stochastic unit commitment;wind forecast error statistics;wind penetrations;Optimization;Power engineering and energy;Power generation;Stochastic systems;Time series analysis;Wind forecasting;Wind power generation;Power engineering and energy;power generation;stochastic systems;wind power generation},
  owner     = {sotterson},
  timestamp = {2013.10.01},
}

@InProceedings{Bruninx13costWndFrcstErrBE,
  author    = {Bruninx, Kenneth and Delarue, Erik and D?haeseleer, William},
  title     = {The cost of wind power forecast errors in the Belgian power system},
  booktitle = {2\textsuperscript{nd} BAEE Research Workshop,(Leuven, Belgium)},
  year      = {2013},
  pages     = {1--20},
  abstract  = {Driven by growing concerns over the impact of greenhouse gas emissions on climate change, elec-
tricity generation from renewable energy sources (RES) has risen considerably over the last decade.
Due to their location-specific, variable and limitedly predictable character, they affect the power sys-
tem in terms of generation and grid adequacy, as well as the need of short term balancing services.
In this paper, we will focus on the cost associated with balancing -- i.e. maintaining the balance be-
tween supply and demand on the short term -- resulting from a specific source of uncertainty, namely
the error on wind power forecasts. We study the impact of this forecast error on the operational
power system costs and carbon emissions in the Belgian power system. Employing a state-of-the-art
stochastic modeling framework, we were able to estimate a lower bound on the operational impact of
wind power forecast errors. Compared to the literature, this approach is richer in the detail it entails
in modeling this uncertainty. Results show that the lower bound on balancing costs varies between
4.3 and 6.7 Euro per MWh of wind energy for wind power penetrations between 5\% and 30\% of the
annual energy demand. If one excludes the cost of a reduced reliability, this balancing cost becomes
relatively constant, ranging from 3.6 to 4 Euro per MWh of wind energy. This is the result of the
fact that the same units, with similar operational costs, are used to mitigate wind power forecast
errors. Carbon emissions are shown to rise 7.4 to 12.7\% at a 30\% wind energy penetration compared
to the case where no uncertainty exists on the wind power forecast. Comparison with a deterministic
optimization, considering a conservative reserve requirement, reveals that sub-optimal scheduling of
reserves can lead to strongly increased balancing costs, up to 8.3 Euro per MWh wind energy. These
results show that utilities, power system operators and policy makers should aim to improve forecast
methodologies and employ state-of-the-art reserve sizing methods to mitigate the resulting balancing
costs of the remaining uncertainty. Results furthermore indicate that the gas-fired units which are
used to provide the needed balancing services will have less and less full load operating hours as the
share of wind energy increases. Future work may investigate whether this is sufficient for operators
to recover their fixed costs, in order to assess the need for capacity mechanisms. A solution to this
weakness may be to include investments and disinvestments in the model. In addition, the obtained
balancing costs should be compared to and validated against market data, such as imbalance volumes
and prices, as well as (activated) reserve volumes.},
  comment   = {Euro, security and C02 costs of Belgian wind power forecast erros. Could be a forecast algorithm metric.},
  file      = {Bruninx13costWndFrcstErrBE.pdf:Bruninx13costWndFrcstErrBE.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.18},
  url       = {http://www.usaee.org/usaee2014/submissions/OnlineProceedings/3872-CostWPFE3.pdf},
}

@InProceedings{Muhr05SsagAgedLines,
  author    = {M. Muhr and S. Pack and S. Jaufer},
  title     = {Sag Calculation of Aged Overhead Lines},
  booktitle = {International Symposium on High Voltage Engineering},
  year      = {2005},
  abstract  = {Due to the changing operation of the transmission network in Europe caused by the liberalisation of the energy market condition based diagnostics get more importance. Based on the large expansion of the network and the changed stress of the line components a considerable effort for condition evaluation is necessary. The Austrian transmission line system at the level 230kV and 400kV has an important role for the energy exchange in the centre of Europe and was built up in 50?s to the70?s of the last century, therefore parts of the system are older than 40 years. Based on this situation this paper focuses on the operation behaviour of ageing overhead lines with special regard to the sag calculation of span sections. In general, the conductor elongates by the permanent mechanical forces (everyday stress) during its live time. Additionally to this irreversible elongation, the sag of a transmission line will increase by the conductor temperature caused by the electrical load and specific environmental conditions or other rope forces (e.g. ice load). Key Words?Overhead line, Aging, Sag, Calculation},
  comment   = {Simulation of sagging due to current power load and aging * simulations of sagging over time * elongation of the conductor caused by thermal or aging effects has different effects on the sags of a line section. * aluminum creeps at normal temps, so its sag exists w/o temp heating},
  file      = {Muhr05SsagAgedLines.pdf:Muhr05SsagAgedLines.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.02.23},
}

@Article{Pinson07tradeWindProbFrcst,
  author    = {Pinson, P. and Chevallier, C. and Kariniotakis, G.N.},
  title     = {Trading Wind Generation From Short-Term Probabilistic Forecasts of Wind Power},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2007},
  volume    = {22},
  number    = {3},
  pages     = {1148--1156},
  month     = aug,
  issn      = {0885-8950},
  abstract  = {Due to the fluctuating nature of the wind resource, a wind power producer participating in a liberalized electricity market is subject to penalties related to regulation costs. Accurate forecasts of wind generation are therefore paramount for reducing such penalties and thus maximizing revenue. Despite the fact that increasing accuracy in spot forecasts may reduce penalties, this paper shows that, if such forecasts are accompanied with information on their uncertainty, i.e., in the form of predictive distributions, then this can be the basis for defining advanced strategies for market participation. Such strategies permit to further increase revenues and thus enhance competitiveness of wind generation compared to other forms of dispatchable generation. This paper formulates a general methodology for deriving optimal bidding strategies based on probabilistic forecasts of wind generation, as well as on modeling of the sensitivity a wind power producer may have to regulation costs. The benefits resulting from the application of these strategies are clearly demonstrated on the test case of the participation of a multi-MW wind farm in the Dutch electricity market over a year.},
  comment   = {Quantile forecasts used to optimally offer day ahead wind power so that regulation costs are minimized. A min/max risk averse method is also mentioned but not developed or tested. Finds that optimal bidding doesn't decrease totla balancing energy but reduces cost. Paper argues this is OK b/c asymmetric balancing costs reflect true costs (but later, I think Pierre said that this would increase GHG's ?)

Offering schemes
* quantile is picked by simple ratio of regulation prices (eq. 8 of Bremnes04windLocQR)
 -- ratio --> tau --> pick power at closest tau (or interpolation across quantiles)
 -- crossover could be a problem, but not as bad if needed density calculated from derivative of a (crossed) quantile forecast.
* can also do a min/max risk averse offer but not evaluated
 -- like TSOs?
 -- e.g. minimize so don't ever exceed the fraction of reserve allocated to EEG?
* also has a scheme for offering when have some storage. Could be useful for IWES virtual power plant?

Market model
* Nordpool model showed lots of wind depressed spot prices but but from 2002-3, didn't see it. Note that I found that Forecast errors do affect market prices in DE.
* Assume bidder can't affect spot or balance prices!
* says Unknown reg. cost could be frcsted e.g. from mkt. close price frcst?
 - In DE, would this also work?
 - unlikely given comments in Moehrlen12tradeStrtgyRESeeg2012
 - Here, assume annual, quarterly forecast is possible (but don't actually test this?)
* but don't do that: instead cheat using anticausal yearly or quarterly averages

Pierre says this suggests that poor market design can cause bad bidding that increases GHG's

Is generalized in Zugno13tradeWindGenMrktQs, where stochastic market prices are used.
Tryggvi had a 2013 price forecasting paper (Jonsson13elecPriceQR). Did it use this idea?
* Marco had a 2010 paper modeling market feedback (Giabardo10feedbackElecMkt). Would that help this paper

RESULTS
* 39\% cost reg cost reduction w/ quarterly avg. price forecast.
* increase in balance energy, which paper says is OK},
  doi       = {10.1109/TPWRS.2007.901117},
  file      = {Pinson07tradeWindProbFrcst.pdf:Pinson07tradeWindProbFrcst.pdf:PDF;Pinson07tradeWindProbFrcst.pdf:Pinson07tradeWindProbFrcst.pdf:PDF},
  groups    = {DOE-PNL09, Use, doReadWPV_2},
  keywords  = {costing, load forecasting, power generation economics, power markets, wind power plantsDutch electricity market, liberalized electricity market, optimal bidding strategies, regulation costs, short-term probabilistic forecasts, wind farm, wind generation trading},
  ncite     = {190},
  owner     = {sotterson},
  timestamp = {2009.03.03},
}

@Article{Bremnes06compQuantileWind,
  author    = {John Bj{\o}rnar Bremnes},
  title     = {A comparison of a few statistical models for making quantile wind power forecasts},
  journal   = {Wind Energy},
  year      = {2006},
  volume    = {9},
  number    = {1-2},
  pages     = {3--11},
  abstract  = {During the last few years, probabilistic wind power forecasts have received increasing attention because of their assumed value in decision-making processes. In the current article, three statistical methods are described and several models based on these are compared. The statistical methods are local quantile regression, a local Gaussian model and the Nadaraya-Watson estimator for conditional cumulative distribution functions. The focus is on quantile forecasts, since these often provide the required type of information to make optimal economic decisions and are ideal for visualizing uncertainty. The statistical methods are applied to data from a wind farm in Norway and results are compared using appropriate measures for assessment of quantile forecasts and in terms of a simple model for economic value.},
  comment   = {Compares local QR, local Gaussian, and Nadaraya-Watson product kernel for probabilistic forecasting. Performance is about the same (on incomplete tests), prefers NW b/c it's simple. Good ideas on performance metrics, expected wind variance, and output boundedness. Bremness won a contest using NW in 2013 (Alessandrini13wireCostFrcstResults)

Bounded output power transformation
* logistic (really, logit) transform (they seem to have done tests showing this is best)
- what the call "logistic" is really logit(). See: https://personality-project.org/r/psych/help/logistic.html
* said they could have used logistic, probit, or arcsin xform
* did use arcsin() in Bremnes04
* NOTE: Must logit() pow meas (output) and pow forecast (if input)!
* If speed is an input, how handle that? Some kinda log-log (Simonoff09transfRegrsn)
* How handle dir?

METHODS COMPARED

* Local quantile regression (LQR)
- Quantiles come from Gaussian distribution of transformed inputs
- logistic, probit or arcsin transforms make the Gaussian quantiles match bounded power output.
- Gaussian params estimated by local linear regression (LSQ in neigbhorhoods with dis-to-training pt. wgts)

* Local Gaussian (LG)
- ordinary linear regression but with distance-weighting
- variance estimate is locally weighted too (they use "local likelihood"; other option is totally unclear)
- is a kind of Gaussian assumption which can be made better by bounded output xform, which they did
- produced the least sharp forecasts!
- not able to produce bimodal distributions
- yet performs same as others in terms of optimal bidding (b/c quanitles good near optimal bid, maybe)

* Nadaraya-Watson (NW)
- Just "samples less than quantile" bin counts weighted by distance to training point
- quantiles are interpolated between training points, and extrapolated for out-of-bound inputs
- could also replace counts by "a known CDF" but I didn't understand what that meant
- three versions:
--- NW: speed --> untransformed power dist
--- NW:T: speed --> transformed power dist
--- NW:TE: point power forecast error from LG --> power error dist
- NW also called GRNN in Ahmed10empCompForecast

Forecast Quality Metrics
* reliabiilty/calibration (same thing):
- proportion of measurements below forecasted quantile
- have Chi-Square significance test for either individual or all quantiles at once (Bremnes04 only had all)
* sharpness
- mean or median distance between quantiles (he calls it "length")
- not good for multi-modal distributions
* refinement
- standard deviation or range of inter-quantile distances
- idea is to measure if things change, but does not measure if they chance correctly
- I think this is usuall called "resolution"
- INTERESTING: says that wind forecast should be most uncertain around 10m/s "by nature"!
* income based on optimal bidding from Bremnes04
* CRPS not mentioned: maybe should redo using this metric since the others (except reliability) are not so great, in my opinion.

Distance weighting
* used for all methods, making them "local"
* is a product of distances in each feature dimension
* i.e. a product kernel, like is assumed in likelihood calculations in a diagonal Gaussian.

Input Features
* NWP spd, dir, hourly increase in speed (all at 10m)
* no attempt to make dir circular w/ a spline or cos/sin comps
* data quality must have been terrible b/c had only 300 points available over a year
* feature selection involved cross validation but was kinda handwavy

Test Case
* forecast a 2.2 MW, five turbine farm
* horizons: 24, 36, 47h
* semi-test-on-train: had a cross validation test on same data used to train, but w/ different data partition

RESULTS
* difference between the difference models were minor
* but all beat climatology
* says NW kernel might then be a good choice due to its simpicity

The 1\textsuperscript{st} paper I can find on wind farm quantile regression is Bremnes04windLocQR

Apparently, Bremnes did move on to NW kernels w/ direction and wind speed and copulae: Alessandrini13wireCostFrcstResults},
  file      = {Bremnes06compQuantileWind.pdf:Bremnes06compQuantileWind.pdf:PDF;Bremnes06compQuantileWind.pdf:Bremnes06compQuantileWind.pdf:PDF},
  groups    = {Read, DOE-PNL09, Ensemble, PointDerived, Test, doReadWPV_1},
  owner     = {sotterson},
  timestamp = {2009.03.03},
}

@Book{Chapelle06SemiSupLrnBook,
  title     = {Semi-supervised learning},
  publisher = {MIT press, Cambridge},
  year      = {2006},
  author    = {Chapelle, Olivier and Sch{\"o}lkopf, Bernhard and Zien, Alexander and others},
  volume    = {2},
  abstract  = {During the last years, semi-supervised learning has emerged as an exciting new
direction in machine learning reseach. It is closely related to profound issues of how
to do inference from data, as witnessed by its overlap with transductive inference
(the distinctions are yet to be made precise).
At the same time, dealing with the situation where relatively few labeled training
points are available, but a large number of unlabeled points are given, it is directly
relevant to a multitude of practical problems where is it relatively expensive to
produce labeled data, e.g., the automatic classification of web pages. As a field,
semi-supervised learning uses a diverse set of tools and illustrates, on a small scale,
the sophisticated machinery developed in various branches of machine learning such
as kernel methods or Bayesian techniques.
As we work on semi-supervised learning, we have been aware of the lack of
an authoritative overview of the existing approaches. In a perfect world, such an
overview should help both the practitioner and the researcher who wants to enter
this area. A well researched monograph could ideally fill such a gap; however, the
field of semi-supervised learning is arguably not yet sufficiently mature for this.
Rather than writing a book which would come out in three years, we thus decided
instead to provide an up-to-date edited volume, where we invited contributions by
many of the leading proponents of the field. To make it more than a mere collection
of articles, we have attempted to ensure that the chapters form a coherent whole
and use consistent notation. Moreover, we have written a short introduction, a
dialogue illustrating some of the ongoing debates in the underlying philosophy of
the field, and we have organized and summarized a comprehensive benchmark of
semi-supervised learning.
Benchmarks are helpful for the practitioner to decide which algorithm should be
chosen for a given application. At the same time, they are useful for researchers
to choose issues to study and further develop. By evaluating and comparing the
performance of many of the presented methods on a set of eight benchmark
problems, this book aims at providing guidance in this respect. The problems are
designed to reflect and probe the different assumptions that the algorithms build
on. All data sets can be downloaded from the book web page, which can be found
at http://www.kyb.tuebingen.mpg.de/ssl-book/.
Finally, we would like to give thanks to everybody who contributed towards the
success of this book project, in particular to Karin Bierig, Sabrina Nielebock, Bob
Prior, to all chapter authors, and to the chapter reviewers.},
  comment   = {Book on semi-supervised learning. This might be good for learning relationship of wind with power when don't have power measurements for every farm.},
  file      = {Chapelle06SemiSupLrnBook.pdf:Chapelle06SemiSupLrnBook.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.06.27},
  url       = {http://www.acad.bg/ebook/ml/MITPress-%20SemiSupervised%20Learning.pdf},
}

@Article{Matus12spanDynLinRat,
  author    = {Matus, M. and Saez, D. and Favley, M. and Suazo-Martinez, C. and Moya, J. and Jimenez-Estevez, G. and Palma-Behnke, R. and Olguin, G. and Jorquera, P.},
  title     = {Identification of Critical Spans for Monitoring Systems in Dynamic Thermal Rating},
  journal   = {Power Delivery, IEEE Transactions on},
  year      = {2012},
  volume    = {27},
  number    = {2},
  pages     = {1002--1009},
  month     = apr,
  issn      = {0885-8977},
  abstract  = {Dynamic thermal rating (DTR) has been seen as an important tool for planning and operation of power systems, and recently, for smart-grid applications. To implement an effective DTR system, it is necessary to install monitoring stations along the studied lines, with a tradeoff between accurate estimations and equipment investments. In this paper, a novel heuristic is developed for identifying the number and locations of critical monitoring spans for the implementation of DTR. The heuristic is based on the use of historical-simulated weather data, obtained from a Mesoscale Weather Model, and the statistical analysis of the thermal capacities computed in each span along the line. The heuristic is applied to a line that is 325 km long in North Chile. Optimal monitoring sets, including the number and location of required monitoring stations, are determined for different confidence levels in all line segments. The results are compared to an equidistant monitoring strategy. The proposed heuristic shows robustness since it outperforms the equidistant monitoring strategy in all of the analyzed cases, especially for the longer line segments, which are subject to more complex weather patterns.},
  comment   = {Simple approach for placing meterological stations needed to ID critical transmission line spans for dynamic line rating. Poorly tested, and probably, poorly designed.

* Main idea is adding a span monitoring station by greedy search.
* cost function is correlation!
 -- not a "confidence," but the call it that
 -- can miss bias errors
 -- not a probability, which is what TSO's want
 -- could correlation-derived distribution (Gaussian something) produce an OK probabilistic estimate?
* appears to be test-on-train
* no test of the algorithm w/ real meteo data
 -- closest is w/ surface stations that are within 20km
 -- but lines are at 16m and stations are at 10m
 -- time of mean wind speed comparison doesn't even line up!
* no test of actual sag or temperature.
* calc temperature using IEEE temperature algorithm
* could use cigre02thermRateStd for the algorithm (different from IEEE?)
* assumption is that you have to have a sensor on the hottest segment
* aren't modelling the distribution of the max -- seems that this would be better

RESULTS
* greedy search correlation better (w/ fewer stations) than linear station spacing
* greedy search eliminates ugly non-montonicity in cost function
* problem is easiest on short coastal stations; hardest on long, inland lines.

IDEAS
* really need a probabilistic forecast w/ proper statio-temporal correlations
* do better optimization, maybe submodular, since this is a sensor placing problem?
* maybe don't need station directly on worst case span. Predict it instead, from distributed measurements.},
  doi       = {10.1109/TPWRD.2012.2185254},
  file      = {Matus12spanDynLinRat.pdf:Matus12spanDynLinRat.pdf:PDF},
  groups    = {Read},
  keywords  = {power cables;power overhead lines;power system identification;power system measurement;smart power grids;statistical analysis;DTR system;complex weather patterns;critical spans identification;dynamic thermal rating;equidistant monitoring strategy;equipment investments;historical-simulated weather data;mesoscale weather model;monitoring station;monitoring system;optimal monitoring sets;power system operation;power system planning;smart grid application;statistical analysis;thermal capacity;Atmospheric modeling;Computational modeling;Correlation;Data models;Monitoring;Wind speed;Critical spans and hot-spot identification;dynamic thermal rating (DTR);smart grids;weather model applications in power systems},
  owner     = {sotterson},
  timestamp = {2014.03.18},
}

@InProceedings{Hung07saxPiecewise,
  author    = {Nguyen Quoc Viet Hung and Duong Tuan Anh},
  title     = {Combining SAX and Piecewise Linear Approximation to Improve Similarity Search on Financial Time Series},
  booktitle = {International Symposium on Information Technology Convergence},
  year      = {2007},
  pages     = {58--62},
  month     = nov,
  abstract  = {Efficient and accurate similarity searching on a large time series data set is an important but non- trivial problem. In this work, we propose a new approach to improve the quality of similarity search on time series data by combining symbolic aggregate approximation (SAX) and piecewise linear approximation. The approach consists of three steps: transforming real valued time series sequences to symbolic strings via SAX, pattern matching on the symbolic strings and a post-processing via Piecewise Linear Approximation.},
  comment   = {For slope hinting while training, ramp clustering, or ramp forecasting; use for similarity seach as in: Chen10simDayLdFrcst},
  doi       = {10.1109/ISITC.2007.24},
  file      = {Hung07saxPiecewise.pdf:Hung07saxPiecewise.pdf:PDF},
  keywords  = {approximation theory;financial data processing;pattern matching;piecewise linear techniques;statistical databases;temporal databases;very large databases;financial time series;large time series data set;piecewise linear approximation;similarity search;symbolic aggregate approximation;Aggregates;Data engineering;Databases;Discrete Fourier transforms;Discrete wavelet transforms;Information technology;Pattern matching;Piecewise linear approximation;Programmable logic arrays;Shape},
  owner     = {sotterson},
  timestamp = {2013.03.15},
}

@PhdThesis{Bacher12solarIntegPhd,
  author      = {Bacher, Peder},
  title       = {Models for efficient integration of solar energy},
  year        = {2012},
  month       = nov,
  abstract    = {Efficient operation of energy systems with substantial amount of renewable energy production is becoming increasingly important. Renewables are dependent on the weather conditions and are therefore by nature volatile and uncontrollable, opposed to traditional energy production based on combustion. The ''smart grid'' is a broad term for the technology for addressing the challenge of operating the grid with a large share of renewables. The {sup s}mart{sup }part is formed by technologies, which models the properties of the systems and efficiently adapt the load to the volatile energy production, by using the available flexibility in the system. In the present thesis methods related to operation of solar energy systems and for optimal energy use in buildings are presented. Two approaches for forecasting of solar power based on numerical weather predictions (NWPs) are presented, they are applied to forecast the power output from PV and solar thermal collector systems. The first approach is based on a developed statistical clear-sky model, which is used for estimating the clear-sky output solely based on observations of the output. This enables local effects such as shading from trees to be taken into account. The second approach to solar power forecasting is based on conditional parametric modelling. It is well suited for forecasting of solar thermal power, since it can be make non-linear in the inputs. The approach is also extended to a probabilistic solar power forecasting model. The statistical clear-sky model is furthermore used as basis for a method for correction of global radiation observations. This method can used for correction of typical errors, for example from shading trees or buildings. Two methods for efficient energy use in buildings are presented in the last part of the thesis. First a method for forecasting of the heat load in single-family houses based on weather forecasts is presented. A model is idented, which works well when applied to forecast the heat load for sixteen single-family houses. The model adapts to the individual houses and needs no specific information about the buildings. Finally a procedure for identification of a suitable model for the heat dynamics of a building is presented. The applied models are greybox model based on stochastic differential equations and the identification is carried out with likelihood ratio tests. The models can be used for providing detailed information of the thermal characteristics of buildings and as basis for optimal control for flexible heating of buildings. (Author)

Subject:14 SOLAR ENERGY; 32 ENERGY CONSERVATION, CONSUMPTION, AND UTILIZATION; SOLAR ENERGY; RESIDENTIAL BUILDINGS; POWER SYSTEMS; SOLAR CELLS; SOLAR COLLECTORS; MATHEMATICAL MODELS; FORECASTING; NUMERICAL SOLUTION; INSOLATION; ENERGY CONSUMPTION; SPACE HEATING; ENERGY SYSTEMS; MATHEMATICAL SOLUTIONS; BUILDINGS; ENERGY; DIRECT ENERGY CONVERTERS; PHOTOVOLTAIC CELLS; RENEWABLE ENERGY SOURCES; HEATING; EQUIPMENT; PHOTOELECTRIC CELLS; ENERGY SOURCES; SOLAR EQUIPMENT},
  comment     = {Peder Bacher's PhD thesis (DTU). Solar forecating w/ quantile regression. Some discussion of modeling dynamics with stochastic differential equations.},
  file        = {Bacher12solarIntegPhd.pdf:Bacher12solarIntegPhd.pdf:PDF},
  groups      = {PointDerived, doReadWPV_1},
  institution = {Technical Univ. of Denmark. DTU Informatics, Kgs. Lyngby (Denmark)},
  location    = {Lyngby, Denmark},
  owner       = {sotterson},
  timestamp   = {2014.02.04},
  url         = {http://orbit.dtu.dk/en/publications/models-for-efficient-integration-of-solar-energy%28a23ccb36-41d9-4736-93dd-14c131788c32%29.html},
}

@InProceedings{Wilamowski99redMemLevMarq1hidLayer,
  author    = {B. M. Wilamowski and Yixin Chen and A. Malinowski},
  title     = {Efficient algorithm for training neural networks with one hidden layer},
  booktitle = {Proc. Int. Joint Conf. Neural Networks IJCNN '99},
  year      = {1999},
  volume    = {3},
  pages     = {1725--1728 vol.3},
  abstract  = {Efficient second order algorithm for training feedforward neural networks is presented. The algorithm has a similar convergence rate as the Lavenberg-Marquardt (LM) method and it is less computationally intensive and requires less memory. This is especially important for large neural networks where the LM algorithm becomes impractical. Algorithm was verified with several examples},
  comment   = {How to reduce memory for Levenburg Marquart algorithm when have one hidden layer NN.  Is this the algorithm used by Matlab?  See:  Mathworks05redMemLevMarq},
  doi       = {10.1109/IJCNN.1999.832636},
  file      = {:Wilamowski99redMemLevMarq1hidLayer.pdf:PDF},
  issn      = {1098-7576},
  keywords  = {computational complexity, convergence, feedforward neural nets, learning (artificial intelligence), multilayer perceptrons, computational complexity, convergence rate, efficient second-order algorithm, feedforward neural network training, hidden neural layer, modified Lavenberg-Marquardt method, modified Levenberg-Marquardt method, Backpropagation algorithms, Convergence, Equations, Feedforward neural networks, Jacobian matrices, Neural networks, Neurons, Performance analysis, Stability, Stochastic processes},
}

@InProceedings{Pinson06ProbFrcstPropsEval,
  author    = {Pinson, P. and Kariniotakis, G. and Nielsen, H. A. and Nielsen, T. S. and Madsen, H.},
  title     = {Properties of quantile and interval forecasts of wind generation and their evaluation},
  booktitle = {Proceedings of the European Wind Energy Conference \& Exhibition},
  year      = {2006},
  abstract  = {Either for managing or trading wind power generation, it is
recognized today that forecasting is a cornerstone. Traditionally,
methods that are developed and implemented are
point forecasting methods, i.e. they provide a single estimated
value for a given horizon. As errors are unavoidable,
several research teams have recently proposed uncertainty estimation
methods in order to optimize the decision-making
process (reserve quantification, bidding strategy definition,
etc.). Here, focus is given to methods that quote quantiles
or intervals from predictive distributions of wind generation.
The paper describes what the required properties of appropriate
uncertainty estimation methods are and how they can be
evaluated. Finally, it is shown how the introduced evaluation
criteria may be used for highlighting or optimizing the performance
of current probabilistic forecasting methodologies.
Keywords: Wind power, short-term forecasting, uncertainty
estimation, probabilistic forecasting, evaluation methods},
  comment   = {Not super highly cited, but maybe tells you how to make cost functions for tuning forecast methods. Talks about unique skill score.

Maybe Brocker07properScore is more thorough?},
  file      = {Pinson06ProbFrcstPropsEval.pdf:Pinson06ProbFrcstPropsEval.pdf:PDF},
  groups    = {Ensemble, PointDerived, Test, CitaviImport1, doReadWPV_2},
  keywords  = {probabilistic forecast, unique skill score},
  location  = {Athens},
  ncite     = {19},
  owner     = {sotterson},
  timestamp = {2013.09.26},
}

@InProceedings{Zheng17loadFrcstRNNlstm,
  author    = {Jian Zheng and Cencen Xu amd Ziang Zhang and Xiaohua Li},
  title     = {Electric Load Forecasting in Smart Grid Using Long-Short-Term-Memory based Recurrent Neural Network},
  booktitle = {51st Annual Conference on Information Systems and Sciences Proceedings},
  year      = {2017},
  address   = {Baltimore},
  abstract  = {Electric load forecasting plays a vital role in smart
grid. Short term electric load forecasting forecasts the load that is
several hours to several weeks ahead. Due to the nonlinear, non-
stationary and nonseasonal nature of the electric load time series,
accurate forecasting is challenging. This paper explores Long-
Short-Term-Memory (LSTM) based Recurrent Neural Network
(RNN) to deal with this challenge. LSTM-based RNN is able
to exploit the long term dependencies in the electric load time
series for more accurate forecasting. Experiments are conducted
to demonstrate that LSTM-based RNN is capable of forecasting
accurately the complex electric load time series with a long
forecasting horizon. Its performance compares favorably to many
other forecasting methods.
Index Terms?Electric load forecasting, univariate time series,
smart grid, recurrent neural network (RNN), long-short-term-
memory (LSTM)},
  comment   = {Load forecast: RNN and LSTM.  I'm not sure if it was better, have to read it to know.},
  file      = {Zheng17loadFrcstRNNlstm.pdf:Zheng17loadFrcstRNNlstm.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.03.08},
  url       = {https://www.researchgate.net/publication/312593525_Electric_Load_Forecasting_in_Smart_Grid_Using_Long-Short-Term-Memory_based_Recurrent_Neural_Network},
}

@TechReport{Cabrera14genQuantFrcstDemandFuncD,
  author      = {Cabrera, Brenda L{\'o}pez and Schulz, Franziska and others},
  title       = {Forecasting Generalized Quantiles of Electricity Demand: A Functional Data Approach},
  institution = {Humboldt-Universit{\"a}t zu Berlin, Wirtschaftswissenschaftliche Fakult{\"a}t},
  year        = {2014},
  address     = {Sonderforschungsbereich 649, Humboldt University, Berlin, Germany},
  abstract    = {Electricity load forecasts are an integral part of many decision-making pro-
cesses in the electricity market. However, most literature on electricity load
forecasting concentrates on deterministic forecasts, neglecting possibly impor?
tant information about uncertainty. A more complete picture of future demand
can be obtained by using distributional forecasts, allowing for a more efficient
decision?making. A predictive density can be fully characterized by tail mea-
sures such as quantiles and expectiles. Furthermore, interest often lies in the
accurate estimation of tail events rather than in the mean or median. We pro-
pose a new methodology to obtain probabilistic forecasts of electricity load,
that is based on functional data analysis of generalized quantile curves. The
core of the methodology is dimension reduction based on functional principal
components of tail curves with dependence structure. The approach has sev?
eral advantages, such as flexible inclusion of explanatory variables including
meteorological forecasts and no distributional assumptions. The methodol?
ogy is applied to load data from a transmission system operator (T80) and
a balancing unit in Germany. Our forecast method is evaluated against other
models including the T80 forecast model. It outperforms them in terms of
mean absolute percentage error (MAPE) and achieves a MAPE of 2.7% for
the TSO.
Keywords: Electricity, Load forecasting, FPCA},
  comment     = {Demand forecast for Amprion and the BU Stadtwerke Saarbriicken balancing authority.  Says the demand measurements are on the web..

They also have an adaptive price forecast for the German spot market: Cabrera16adaptProbPriceFrcst Maybe that's joint?},
  file        = {Cabrera14genQuantFrcstDemandFuncD.pdf:Cabrera14genQuantFrcstDemandFuncD.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2016.12.21},
  url         = {http://econpapers.repec.org/paper/humwpaper/sfb649dp2014-030.htm},
}

@TechReport{Palsson11stOartLdFrcst,
  author      = {{\'O}lafur P{\'e}tur P{\'a}lsson and Einar Geirsson and Einar {\'O}li Gu{\dh}mundsson},
  title       = {State-of-the-Art Report Electricity Load Forecasting},
  institution = {Faculty of Industrial Engineering, Mechanical Engineering and Computer Science. University of Iceland},
  year        = {2011},
  abstract    = {Electricity load forecasts are indispensable to the operation and planning of power companies.
Load forecasts help power companies in important decision making, including decisions on purchasing
electric power, infrastructure development and load switching (Feinberg and Genethliou,
2005). Accurate models for electric load forecasting are becoming more important than before in
the new world of deregulated electricity markets. The forecasts are crucial for energy suppliers,
financial institutions, independent system operators (ISOs) and other players in the electricity
market.
Control and scheduling of power systems require online electricity demand forecasts with lead
times from a minute-ahead up to a day-ahead. Predictions with lead times from 10 - 30 minutes
have been defined as very short-term forecasts while short-term forecasts are defined as
predictions with a half-hour up to a day-ahead of lead time (Taylor, 2008).
Medium- and long-term forecasts deal with greater lead times and often use other types of models
than short-term forecasts. Literature is not in agreement about the definition of medium- and
long-term forecasts. Predictions with lead times of approximately a week to a year are usually
defined as medium-term forecasts and predictions with lead times of several years are long-term
forecasts (Feinberg and Genethliou, 2005) (Badran et al., 2008).
Models of electricity demand can either be univariate, with past demand series as an input, or
multivariate with more than one input like weather forecast data (Taylor, 2003). For short lead
times multivariate modeling is usually considered impractical since the weather forecast input
would require default procedures in order to ensure robustness (Bunn, 1982). Univariate methods
are considered adequate for short lead times since the demand series tend to capture the change
of the weather variables, which change in a regular manner over the time span of the short lead
time (Taylor, 2003).},
  comment     = {From the OSR Nordic spinning reserves project I worked on at DTU. May explain a little about how the Nordpool market works too, but I haven't read it.},
  file        = {Palsson11stOartLdFrcst.pdf:Palsson11stOartLdFrcst.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2013.10.11},
}

@InProceedings{Kahn09frcst1dayWvlts,
  author    = {Khan, A. A. and Shahidehpour, M.},
  title     = {One day ahead wind speed forecasting using wavelets},
  booktitle = {Power Systems Conference and Exposition},
  year      = {2009},
  pages     = {1--5},
  month     = mar,
  abstract  = {Electricity markets are evolving throughout the world, accommodating the penetration of alternative energy sources, in particular wind. As the wind energy penetration is increasing, there is a need for accurate and efficient wind forecasting technique. If the wind speed can be reliably forecasted up to several hours, the generating schedule can efficiently accommodate wind power generation, leading to higher profits for wind plant developers and lower integration costs for utilities. This paper proposes a new application of wavelets in the field of wind speed forecasting. A new wavelet coefficient predictor technique is developed for forecasting wind speed up to 24 hours. This technique is successfully applied on the wind speed data obtained from Colorado public utility sites. The results obtained are compared with the common benchmark persistence method.},
  comment   = {The only paper I've found using wavelets for 1 day ahead wind (speed) forecasting.},
  doi       = {10.1109/PSCE.2009.4840129},
  file      = {Kahn09frcst1dayWvlts.pdf:Kahn09frcst1dayWvlts.pdf:PDF},
  keywords  = {load forecasting;power markets;wavelet transforms;wind power plants;Colorado public utility sites;benchmark persistence method;electricity markets;wavelet coefficient predictor technique;wind energy penetration;wind plant developers;wind power generation;wind speed data;wind speed forecasting;Costs;Economic forecasting;Electricity supply industry;Power generation;Wavelet coefficients;Wind energy;Wind energy generation;Wind forecasting;Wind power generation;Wind speed;Forecasting;Wavelets;Wind},
  owner     = {sotterson},
  timestamp = {2013.03.15},
}

@TechReport{Tastu13scenSpatTimeCpla,
  author      = {Julija Tastu and Pierre Pinson and Henrik Madsen},
  title       = {Space-time scenarios of wind power generation produced using a {Gauss}ian copula with parametrized precision matrix},
  institution = {Technical University of Denmark},
  year        = {2013},
  type        = {DTU Compute-Technical Report-2013},
  number      = {14},
  abstract    = {Emphasis is placed on generating space-time trajectories of wind power
generation, consisting of paths sampled from high-dimensional joint predictive densities,
describing wind power generation at a number of contiguous locations and
successive lead times. A modelling approach taking advantage of the sparsity of precision
matrices is introduced for the description of the underlying space-time dependence
structure. The proposed parametrization of the dependence structure accounts
for important process characteristics such as lead-time-dependent conditional precisions
and direction-dependent cross-correlations. Estimation is performed in a maximum
likelihood framework. Based on a test case application in Denmark, with spatial
dependencies over 15 areas and temporal ones for 43 hourly lead times (hence,
for a dimension of n = 645), it is shown that accounting for space-time effects is
crucial for generating skilful trajectories.},
  comment     = {The space-time scenario forecasting study that Henrik Madsen presented at the EERA forecasting workshop, Roskilde, 2014. Attached is the revised 2013 version.

Updated: Tastu15spcTimeTrajGaussCpla},
  file        = {Tastu13scenSpatTimeCpla.pdf:Tastu13scenSpatTimeCpla.pdf:PDF},
  groups      = {PointDerived, doReadWPV_1},
  owner       = {sotterson},
  publisher   = {Technical University of Denmark},
  series      = {DTU Compute-Technical Report-2013},
  timestamp   = {2014.03.25},
  url         = {http://orbit.dtu.dk/en/publications/spacetime-scenarios-of-wind-power-generation-produced-using-a-gaussian-copula-with-parametrized-precision-matrix%287f73ed19-ed61-465a-a39b-6065e7595bbc%29.html},
}

@InBook{Tastu15spcTimeTrajGaussCpla,
  pages     = {267--296},
  title     = {Space-Time Trajectories of Wind Power Generation: Parametrized Precision Matrices Under a Gaussian Copula Approach},
  publisher = {Springer International Publishing},
  year      = {2015},
  author    = {Tastu, Julija and Pinson, Pierre and Madsen, Henrik},
  editor    = {Antoniadis, Anestis and Poggi, Jean-Michel and Brossat, Xavier},
  address   = {Cham},
  isbn      = {978-3-319-18732-7},
  abstract  = {Emphasis is placed on generating space-time trajectories of wind power
generation, consisting of paths sampled from high-dimensional joint predictive densities, describing wind power generation at a number of contiguous locations and successive lead times. A modelling approach taking advantage of the sparsity of precision matrices is introduced for the description of the underlying space-time dependence structure. The proposed parametrization of the dependence structure accounts for important process characteristics such as lead-time-dependent conditional precisions and direction-dependent cross-correlations. Estimation is performed in a maximum likelihood framework. Based on a test case application in Denmark, with spatial dependencies over 15 areas and temporal ones for 43 hourly lead times (hence, for a dimension of n = 645), it is shown that accounting for space-time effects is crucial for generating skilful trajectories.},
  booktitle = {Modeling and Stochastic Learning for Forecasting in High Dimensions},
  comment   = {A Gaussian copula scenario wind power forecast like in Pinson09probFrcstStatScenWind but expanded to include spatial covariances (across 16 grid zones across DK).  This is for lead times 1:43 hours ahead, in one hour steps.  There is no sensitivity to wind direction.

COVARIANCE REGULARIZATION / CONSTRAINTS
* done by hand-coding separable sparsity constraints for the precision matrix
* Spatial covarance
   - limit spatial precision matrix  to NWSE neighbors, everything else is zero
   - same  neighbor coeffs for all farms (seems like more flexibility is better outside of flat DK)
   - this doesn't make covariance zero everywhere but NWSE
   - instead, it is saying that "only the NWSE neighbors have non-zero partial correlation"
      i.e. they have unique information that no other farms have, as seen by linear correlation
   - covariance matrix (inverse of precision) will have way more non-zero values
   - but wouldn't it have been equally good to drive some covariances to zero?
      (maybe Simpson12spatStatForgetCov explains why)
* Temporal covariance
   - is parameterized, with fixed-shape decreasing-with-lag autocorrelation
      -- decreasing autocorrelation is  INCREASING precision matrix values (eq. 2 in Rue02fitGausMarkovRndFlds)
      -- kind of like Stephan Vogt's "making up extra" samples idea for solar
      -- same autocorr profile for all farms b/c they couldn't find something that would predict the shape
      -- could there be a more flexible parameterization?  See [[Distributed lag]] in energytop.org
   - There are also special boundary points at min,max horizon (I don't understand why).
* Result is solved using maximum likelihood.

RESULTS
* Huge reduction in the number of paramenters
   - 2007690 params for a full sample covariance  --> 10 for the constrained precision matrix.
   - compare with matrix normal decomposition in Owen13monteCarloBook
   - also compare with mixture comp reduction in Anderlucci15covPatMix
   - and compare with other stuff in energytop.org <<Covariance sparseness>>
* Accuracy
   - not quite as good as just using the sample covariance trained over a year's worth of data.
   - as measured by Logarithmic Score (mean likelihood of observations)
     -- sensitive to outliers (but problem not seen in this data)
     -- is sharpness rewarded, since it only looks at Gaussian likelihood, not marginals?
     - could also have used the Energy Score but it's computationally expensive, and is consistent w/ L. Score

CONCLUSIONS
* need better scoring
* sparse precision matrix may have future advantages
   - adding direction senstivity (switching models with less data, ref [24] recommended)
   - clustering
   - adaptive approaces
* SDE might be the way to go with some kind of precision matrix
   (I think they mean, as in Simpson12spatStatForgetCov but they also link to Lindgren11linkGaussMMRFandSDE)

RELATED
* Bessa16gaussCplaDcsns:  Gaussian copula better than D-vine, temp. constraints better than emp. cov.
   -  Bessa: cov mat with exponential  autocorr restriction much better than empirical cov.
   - Tastu: empirical cov. mat is  better
      - problems w/ spatial part of Tastu's cov. mat constraints?
      - is Tastu's temporal constraint different than the one here?
      - different data?
       - is spatio-temporal different enough from single farm that a different model is better?
* linear correlations, need for lags justified in Tastu10spatTempErr
* compare with Hagspiel12copulaWindPowEur},
  doi       = {10.1007/978-3-319-18732-7_14},
  file      = {Tastu15spcTimeTrajGaussCpla.pdf:Tastu15spcTimeTrajGaussCpla.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.07},
  url       = {http://dx.doi.org/10.1007/978-3-319-18732-7_14},
}

@Article{North82sampErrEstOrthogPCAnumCmps,
  author    = {Gerald R. North and Thomas L. Bell and Robert F. Cahalan and Fanthune J. Moeng},
  title     = {Sampling Errors in the Estimation of Empirical Orthogonal Functions},
  journal   = {Monthly Weather Review},
  year      = {1982},
  volume    = {110},
  number    = {7},
  pages     = {699--706},
  month     = {jul},
  abstract  = {Empirical Orthogonal Functions (EOF's), eigenvectors of the spatial cross-covariance matrix of a meteorological field, are reviewed with special attention given to the necessary weighting factors for gridded data and the sampling errors incurred when too small a sample is available. The geographical shape of an EOF shows large intersample variability when its associated eigenvalue is “close” to a neighboring one. A rule of thumb indicating when an EOF is likely to be subject to large sampling fluctuations is presented. An explicit example, based on the statistics of the 500 mb geopotential height field, displays large intersample variability in the EOF's for sample sizes of a few hundred independent realizations, a size seldom exceeded by meteorological data sets.},
  comment   = {The number of principal components test apparently used by LNEG for its weather classification work in IRPWIND WP82.3.  Is it somehow more appropriate for temporal-spatio covarariance matrices?},
  doi       = {10.1175/1520-0493(1982)110<0699:SEITEO>2.0.CO;2},
  file      = {:North82sampErrEstOrthogPCAnumCmps.pdf:PDF},
  publisher = {American Meteorological Society},
}

@TechReport{Wu11curtailStochOpt,
  author      = {Wu, Owen and Kapuscinski, Roman},
  title       = {Curtailing intermittent generation in electrical systems},
  institution = {Stephen M. Ross School of Business, University of Michigan},
  year        = {2011},
  type        = {Working Paper},
  number      = {1170},
  month       = mar,
  abstract    = {Energy generation from intermittent renewable sources introduces additional variability into electrical systems, resulting in a higher cost of balancing against the increased variabilities. Ways to balance demand and supply for electricity include using flexible generation resources, storage operations, and curtailing intermittent generation. This paper focuses on the operational and environmental impact of curtailing intermittent generation. We construct a stochastic dynamic optimization model that captures the critical components of the system operating cost and analyze how various generation resources should operate with and without curtailing intermittent generation. We find that the system cost reduction per unit of curtailed energy is consistently significant, and the presence of storage may increase the cost saving per unit of curtailed energy. We also find that curtailing intermittent generation often leads to system emission reductions.

Keywords: intermittent generation, wind power, curtailment, stochastic dynamic programming},
  comment     = {Optimal 15 minute ahead wind power curtailment decisions, made with stochastic optimization.

15 mins ahead, so assume forecasts are perfect! Where is the "stochastic"?

Could this be extended to scenarios, assuming non-zero error (would certainly have to for longer lookaheads)},
  file        = {Wu11curtailStochOpt.pdf:Wu11curtailStochOpt.pdf:PDF},
  groups      = {Use, doReadWPV_2},
  journal     = {Ross School of Business Paper},
  location    = {Michigan, USA},
  owner       = {sotterson},
  school      = {Oregon State University,},
  timestamp   = {2013.10.16},
  url         = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2012428},
}

@Article{Lien13unlckWeibullAnlys,
  author    = {Paul Lien and David Nicholls},
  title     = {Unlocking Weibull analysis},
  journal   = {Machine Design},
  year      = {2013},
  abstract  = {Engineers can learn to use this valuable tool for predicting product life and other product qualities, without having to learn all the daunting statistics.

When products start failing, management wants answers. Are they failing because of manufacturing problems? Or is the design to blame?

One of the most widely regarded methods for ferreting out the reason behind failures, as well as accurately predicting operational life, warranty claims and other product qualities is statistical analysis of a component?s or device?s failure data. Though there are many statistical distributions that could be used, including the exponential and lognormal, the Weibull distribution is particularly useful because it can characterize a wide range of data trends, including increasing, constant, and decreasing failure rates, a task its counterparts cannot handle. This characteristic also lets Weibull distributions mimic other statistical distributions, which is why it is often an engineer?s first approximation for analyzing failure data.},
  comment   = {How  Weibull diistribution analysis is used to do failure root cause analysis.

Weilbull is an exponential family member,:
http://www.math.uah.edu/stat/special/GeneralExponential.html
so lots of ways to estimate it

See also: Abernethy06Chapter1Overview},
  file      = {Lien13unlckWeibullAnlys13unlckWeibullAnlys.pdf:Lien13unlckWeibullAnlys:PDF},
  owner     = {sotterson},
  timestamp = {2017.01.02},
  url       = {http://machinedesign.com/technologies/unlocking-weibull-analysis},
}

@Article{Keune14multivarProbEnsFrcst,
  author    = {Keune, Jessica and Ohlwein, Christian and Hense, Andreas},
  title     = {Multivariate probabilistic analysis and predictability of medium-range ensemble weather forecasts},
  journal   = {Monthly Weather Review},
  year      = {2014},
  volume    = {142},
  number    = {11},
  pages     = {4074--4090},
  abstract  = {Ensemble weather forecasting has been operational for two decades now. However, the related uncertainty analysis in terms of probabilistic postprocessing still focuses on single variables, grid points, or stations. Inevitable dependencies in space and time and between variables are often ignored. To address this problem, two probabilistic postprocessing methods are presented, which are multivariate versions of Gaussian fit and kernel dressing, respectively. The multivariate case requires the estimation of a full rank, invertible covariance matrix. For this purpose, a Graphical Least Absolute Shrinkage and Selection Operators (GLASSO) estimator has been employed that is based on sparse undirected graphical models regularized by an L1 penalty term in order to parameterize the full rank inverse covariance. In all cases, the result is a multidimensional probability density. The forecasts used to test the approach are station forecasts of 2-m temperature and surface pressure from four main global ensemble prediction systems (EPS) with medium-range weather forecasts: the NCEP Global Ensemble Forecast System (GEFS), the Met Office Global and Regional Ensemble Prediction System (MOGREPS), the Canadian Meteorological Centre (CMC) Global Ensemble Prediction System (GEPS), and the ECMWF EPS. To evaluate the multivariate probabilistic postprocessing, especially the uncertainty estimates, common verification methods such as the analysis rank histogram and the continuous ranked probability score (CRPS) are applied. Furthermore, a multivariate extension of the CRPS, the energy score, allows for the verification of a complete medium-range forecast as well as for determining its predictability. It is shown that the predictability is similar for all of the examined ensemble prediction systems, whereas the GLASSO proved to be a useful tool for calibrating the commonly observed underdispersion of ensemble forecasts during the first few lead days by using information from the full covariance matrix.

Keywords: Bayesian methods, Statistical techniques, Ensembles, Forecast verification/skill, Probability forecasts/models/distribution, Model output statistics},
  comment   = {Spatio temporal ensemble calibration suggested and Nov. 2014 Eweline IFP mtng. But seems to require consistent ensemble ordering.},
  doi       = {10.1175/MWR-D-14-00015.1},
  file      = {Keune14multivarProbEnsFrcst.pdf:Keune14multivarProbEnsFrcst.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.12.09},
}

@InProceedings{Marino16bldngLdFrcstDeepNN,
  author       = {Marino, Daniel L and Amarasinghe, Kasun and Manic, Milos},
  title        = {Building energy load forecasting using deep neural networks},
  booktitle    = {Industrial Electronics Society, IECON 2016-42nd Annual Conference of the IEEE},
  year         = {2016},
  pages        = {7046--7051},
  organization = {IEEE},
  abstract     = {Ensuring sustainability demands more efficient
energy management with minimized energy wastage. Therefore,
the power grid of the future should provide an unprecedented level
of flexibility in energy management. To that end, intelligent
decision making requires accurate predictions of future energy
demand/load, both at aggregate and individual site level. Thus,
energy load forecasting have received increased attention in the
recent past, however has proven to be a difficult problem. This
paper presents a novel energy load forecasting methodology based
on Deep Neural Networks, specifically Long Short Term Memory
(LSTM) algorithms. The presented work investigates two variants
of the LSTM: 1) standard LSTM and 2) LSTM-based Sequence to
Sequence (S2S) architecture. Both methods were implemented on
a benchmark data set of electricity consumption data from one
residential customer. Both architectures where trained and tested
on one hour and one-minute time-step resolution datasets.
Experimental results showed that the standard LSTM failed at
one-minute resolution data while performing well in one-hour
resolution data. It was shown that S2S architecture performed
well on both datasets. Further, it was shown that the presented
methods produced comparable results with the other deep
learning methods for energy forecasting in literature.
Keywords?Deep Learning; Deep Neural Networks; Long-
Short-Term memory; LSTM; Energy; Building Energy; Energy
Load forecasting},
  comment      = {Uses RNN LSTM.  Talks about aggregation in abstract, anyway.},
  file         = {Marino16bldngLdFrcstDeepNN.pdf:Marino16bldngLdFrcstDeepNN.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2017.03.11},
  url          = {Ensuring sustainability demands more efficient
energy management with minimized energy wastage. Therefore,
the power grid of the future should provide an unprecedented level
of flexibility in energy management. To that end, intelligent
decision making requires accurate predictions of future energy
demand/load, both at aggregate and individual site level. Thus,
energy load forecasting have received increased attention in the
recent past, however has proven to be a difficult problem. This
paper presents a novel energy load forecasting methodology based
on Deep Neural Networks, specifically Long Short Term Memory
(LSTM) algorithms. The presented work investigates two variants
of the LSTM: 1) standard LSTM and 2) LSTM-based Sequence to
Sequence (S2S) architecture. Both methods were implemented on
a benchmark data set of electricity consumption data from one
residential customer. Both architectures where trained and tested
on one hour and one-minute time-step resolution datasets.
Experimental results showed that the standard LSTM failed at
one-minute resolution data while performing well in one-hour
resolution data. It was shown that S2S architecture performed
well on both datasets. Further, it was shown that the presented
methods produced comparable results with the other deep
learning methods for energy forecasting in literature.
Keywords?Deep Learning; Deep Neural Networks; Long-
Short-Term memory; LSTM; Energy; Building Energy; Energy
Ensuring sustainability demands more efficient
energy management with minimized energy wastage. Therefore,
the power grid of the future should provide an unprecedented level
of flexibility in energy management. To that end, intelligent
decision making requires accurate predictions of future energy
demand/load, both at aggregate and individual site level. Thus,
energy load forecasting have received increased attention in the
recent past, however has proven to be a difficult problem. This
paper presents a novel energy load forecasting methodology based
on Deep Neural Networks, specifically Long Short Term Memory
(LSTM) algorithms. The presented work investigates two variants
of the LSTM: 1) standard LSTM and 2) LSTM-based Sequence to
Sequence (S2S) architecture. Both methods were implemented on
a benchmark data set of electricity consumption data from one
residential customer. Both architectures where trained and tested
on one hour and one-minute time-step resolution datasets.
Experimental results showed that the standard LSTM failed at
one-minute resolution data while performing well in one-hour
resolution data. It was shown that S2S architecture performed
well on both datasets. Further, it was shown that the presented
methods produced comparable results with the other deep
learning methods for energy forecasting in literature.
Keywords?Deep Learning; Deep Neural Networks; Long-
Short-Term memory; LSTM; Energy; Building Energy; Energy
http://ieeexplore.ieee.org/abstract/document/7793413/?reload=true},
}

@Article{Ahmed89entropyMultiVar,
  author    = {Ahmed, N.A. and Gokhale, D.V.},
  title     = {Entropy expressions and their estimators for multivariate distributions},
  journal   = {Information Theory, IEEE Transactions on},
  year      = {1989},
  volume    = {35},
  number    = {3},
  pages     = {688--692},
  month     = may,
  issn      = {0018-9448},
  abstract  = {Entropy expressions for several continuous multivariate distributions are derived. Point estimation of entropy for the multinormal distribution and for the distribution of order statistics from D.G. Weinman's (Ph.D dissertation, Ariz. State Univ., Tempe, AZ, 1966) exponential distribution is considered. The asymptotic distribution of the uniformly minimum variance unbiased estimator for multinormal entropy is obtained. Simulation results on convergence of the means and variances of these estimators are provided.},
  comment   = {Differential Entropy equation for e.g. Normal dist, and the estimators from data could maybe use to see if MI is comparable across variables w/ different dimensions. it defines the entropy found here as differential: http://en.wikipedia.org/wiki/Multivariate_normal_distribution\#Entropy and even cites Ahmed as the source.},
  doi       = {10.1109/18.30996},
  file      = {Ahmed89entropyMultiVar.pdf:Ahmed89entropyMultiVar.pdf:PDF},
  groups    = {Read},
  keywords  = {asymptotic distribution;continuous multivariate distributions;convergence;exponential distribution;multinormal distribution;multinormal entropy;order statistics distribution;point estimation;statistical inference;uniformly minimum variance unbiased estimator;entropy;information theory;parameter estimation;statistical analysis;},
  owner     = {scot},
  timestamp = {2011.05.19},
  url       = {http://ieeexplore.ieee.org.globalproxy.cvt.dk/xpls/abs_all.jsp?arnumber=30996&tag=1},
}

@Article{Cook14envlpMatlab,
  author    = {Cook, Dennis and Su, Zhihua and Yang, Yi},
  title     = {envlp: A MATLAB Toolbox for Computing Envelope Estimators in Multivariate Analysis},
  journal   = {Journal of Statistical Software (submitted 2013)},
  year      = {2014},
  abstract  = {Envelope models and methods represent new constructions that can lead to substantial
increases in estimation efficiency in multivariate analyses. The envlp toolbox implements
a variety of envelope estimators under the framework of multivariate linear regression,
including the envelope model, partial envelope model, heteroscedastic envelope model,
inner envelope model, scaled envelope model, and envelope model in the predictor space.
The toolbox also implements the envelope model for estimating a multivariate mean.
The capabilities of this toolbox include estimation of the model parameters, as well as
performing standard multivariate inference in the context of envelope models; for example,
prediction and prediction errors, F test for two nested models, the standard errors for
contrasts or linear combinations of coefficients, and more. Examples and datasets are
contained in the toolbox to illustrate the use of each model. All functions and datasets
are documented.
Keywords: multivariate linear regression, envelope models, dimension reduction, Grassmann
manifold, MATLAB.},
  comment   = {Matlab code is here:
https://code.google.com/p/envlp/

Submitted and accepted in 2013,
http://users.stat.umn.edu/~yiyang/research/
but not published yet in Oct 2014

General paper: Cook13EnvlpPLS
Fast initialization: Cook14envlpEstAlgs},
  file      = {Cook14envlpMatlab.pdf:Cook14envlpMatlab.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.10.28},
  url       = {http://users.stat.umn.edu/~yiyang/resources/papers/envlp.pdf},
}

@Article{Cook14envlpEstAlgs,
  author    = {Cook, R Dennis and Zhang, Xin},
  title     = {Algorithms for envelope estimation},
  journal   = {arXiv preprint},
  year      = {2014},
  abstract  = {Envelopes were recently proposed as methods for reducing estimative variation in multivariate linear regression. Estimation of an envelope usually involves optimization over Grassmann manifolds. We propose a fast and widely applicable one-dimensional (1D) algorithm for estimating an envelope in general. We reveal an important structural property of envelopes that facilitates our algorithm, and we prove both Fisher consistency and root-n-consistency of the algorithm.
KeyWords: Envelopes; Grassmann manifold; reducing subspaces.},
  comment   = {Either a fast way to initialize PLSR-style envelope regression, or to do the regression itself. May have Matlab.

Matlab envelope toolbox: Cook14envlpMatlab
General paper: Cook13EnvlpPLS},
  file      = {Cook14envlpEstAlgs.pdf:Cook14envlpEstAlgs.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.10.28},
  url       = {http://arxiv.org/abs/1403.4138},
}

@Article{Kankanala14adaBoostPlus,
  author    = {Kankanala, P. and Das, S. and Pahwa, A.},
  title     = {AdaBoost$^{+}$: An Ensemble Learning Approach for Estimating Weather-Related Outages in Distribution Systems},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2014},
  volume    = {29},
  number    = {1},
  pages     = {359--367},
  issn      = {0885-8950},
  abstract  = {Environmental factors, such as weather, trees, and animals, are major causes of power outages in electric utility distribution systems. Of these factors, wind and lightning have the most significant impacts. The objective of this paper is to investigate models to estimate wind and lighting related outages. Such estimation models hold the potential for lowering operational costs and reducing customer downtime. This paper proposes an ensemble learning approach based on a boosting algorithm, AdaBoost+, for estimation of weather-caused power outages. Effectiveness of the model is evaluated using actual data, which comprised of weather data and recorded outages for four cities of different sizes in Kansas. The proposed ensemble model is compared with previously presented regression, neural network, and mixture of experts models. The results clearly show that AdaBoost+ estimates outages with greater accuracy than the other models for all four data sets.},
  comment   = {AdaBoost ensemble learning for continuous regression. NN input weighting is built into the algorithm.

Is compared to Solomatine04adaBoostContin (AdaBoost.Rt)},
  doi       = {10.1109/TPWRS.2013.2281137},
  file      = {Kankanala14adaBoostPlus.pdf:Kankanala14adaBoostPlus.pdf:PDF},
  keywords  = {lightning;power distribution economics;power engineering computing;wind;AdaBoost;Kansas;customer downtime reduction;distribution systems;electric utility distribution systems;ensemble learning approach;environmental factors;expert model;lightning-related outage estimation;neural network;operational costs;power outages;weather-related outage estimation;wind-related outage estimation;Biological neural networks;Cities and towns;Lightning;Training;Vegetation;Wind;Artificial intelligence;ensemble learning;environmental factors;power distribution systems;power system reliability},
  owner     = {sotterson},
  timestamp = {2014.01.21},
}

@Article{Gasparrini10distLagNonLin,
  author    = {Gasparrini, A and Armstrong, B and Kenward, MG},
  title     = {Distributed lag non-linear models},
  journal   = {Statistics in Medicine},
  year      = {2010},
  note      = {Epub ahead of print May 7. DOI: 10.1002/sim.3940},
  abstract  = {Environmental stressors often show effects that are delayed in time, requiring the use of statistical models that are flexible enough to describe the additional time dimension of the exposure-response relationship. Here we develop the family of distributed lag non-linear models (DLNM), a modelling framework that can simultaneously represent non-linear exposure-response dependencies and delayed effects. This methodology is based on the definition of a lsquocross-basisrsquo, a bi-dimensional space of functions that describes simultaneously the shape of the relationship along both the space of the predictor and the lag dimension of its occurrence. In this way the approach provides a unified framework for a range of models that have previously been used in this setting, and new more flexible variants. This family of models is implemented in the package dlnm within the statistical environment R. To illustrate the methodology we use examples of DLNMs to represent the relationship between temperature and mortality, using data from the National Morbidity, Mortality, and Air Pollution Study (NMMAPS) for New York during the period 1987-2000.},
  comment   = {R library dlnm (Gasparrini10dlnmManual)},
  doi       = {10.1002/sim.3940},
  file      = {Gasparrini10distLagNonLin.pdf:Gasparrini10distLagNonLin.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.08.05},
}

@Article{Adams13glblWindPowOvrEst,
  author    = {Adams, Amanda S and Keith, David W},
  title     = {Are global wind power resource estimates overstated?},
  journal   = {Environmental Research Letters},
  year      = {2013},
  volume    = {8},
  number    = {1},
  pages     = {015021},
  abstract  = {Estimates of the global wind power resource over land range from 56 to 400 TW. Most estimates have implicitly assumed that extraction of wind energy does not alter large-scale winds enough to significantly limit wind power production. Estimates that ignore the effect of wind turbine drag on local winds have assumed that wind power production of 2?4 W m?2 can be sustained over large areas. New results from a mesoscale model suggest that wind power production is limited to about 1 W m?2 at wind farm scales larger than about 100 km2. We find that the mesoscale model results are quantitatively consistent with results from global models that simulated the climate response to much larger wind power capacities. Wind resource estimates that ignore the effect of wind turbines in slowing large-scale winds may therefore substantially overestimate the wind power resource.},
  comment   = {Yes wind power slows down wind, accoding to this simulation
* global wind power estimated at 56-400TW W 2-4 W/msq
* but can only get 1 W/msq in farms > 100 kmsq
* good pictures of effect of giant wind far.  I included it in GIZ Colombia
.

But also see rebuttals (kind of):
*  https://www.evernote.com/shard/s13/nl/1523219/0f9b2965-6edc-4b0b-9db1-6afd9ac9eb4f
*  https://www.evernote.com/shard/s13/nl/1523219/15d711f3-2f6f-4f81-884e-ac5a6b864b27

For global impact, see Prinn09climImpctlargeWindFrmsTechRep

Charts I grabbed for GIZ

Figure 3: Figure 3. Atmospheric response for two wind farm configurations with similar total capacities but differing capacity density. The left-hand column shows results from a 100 ? 300 km wind farm with an installed CD of 4 W m?2, while the right-hand column is for a 300 ? 900 km farm with a CD of 0.5 W m?2. The top row shows fractional difference in wind speed cubed between control and wind farms while the bottom two panels show average potential temperature difference (K) at hub height (perturbed-control). Simulations initialized on 5 January 2006 and integrated for 10 days

Total power
300*900*0.5 = 135K
100*300*4    =  120K},
  file      = {Adams13glblWindPowOvrEst.pdf:Adams13glblWindPowOvrEst.pdf:PDF},
  owner     = {sotterson},
  publisher = {IOP Publishing},
  timestamp = {2017.04.26},
  url       = {http://iopscience.iop.org/article/10.1088/1748-9326/8/1/015021/meta;jsessionid=123139FA78A4FDA4F003430002F6171A.ip-10-40-1-105},
}

@Article{Atiya99neuralForecastRiver,
  author    = {Amir F. Atiya and Senior Member and Suzan M. El-shoura and Samir I. Shaheen and Mohamed S. El-sherif},
  title     = {A comparison between neural-network forecasting techniques case study: River flow forecasting},
  journal   = {Neural Networks, IEEE Transactions on},
  year      = {1999},
  volume    = {10},
  pages     = {402--409},
  abstract  = {Estimating the flows of rivers can have significant economic impact, as this can help in agricultural water management and in protection from water shortages and possible flood damage. The first goal of this paper is to apply neural networks to the problem of forecasting the flow of the River Nile in Egypt. The second goal of the paper is to utilize the time series as a benchmark to compare between several neural-network forecasting methods.We compare between four different methods to preprocess the inputs and outputs, including a novel method proposed here based on the discrete Fourier series. We also compare between three different methods for the multistep ahead forecast problem: the direct method, the recursive method, and the recursive method trained using a backpropagation through time scheme. We also include a theoretical comparison between these three methods. The final comparison is between different methods to perform longer horizon forecast, and that includes ways to partition the problem into the several subproblems of forecasting K steps ahead. Index Terms? Backpropagation, Fourier series, multistep ahead prediction, neural networks, Nile River, river flow forecasting, seasonal time series, time series prediction.},
  comment   = {Multi-step problem for Neural Nets and rivers: explicit period model fails, direct method is best; backprop through time is best.},
  file      = {Atiya99neuralForecastRiver.pdf:Atiya99neuralForecastRiver.pdf:PDF;Atiya99neuralForecastRiver.pdf:Atiya99neuralForecastRiver.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.09.29},
  url       = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.620},
}

@Article{Schneider01missImputMeanCov,
  author    = {Schneider, Tapio},
  title     = {Analysis of Incomplete Climate Data: Estimation of Mean Values and Covariance Matrices and Imputation of Missing Values},
  journal   = {Journal of Climate},
  year      = {2001},
  volume    = {14},
  number    = {5},
  pages     = {853--871},
  month     = mar,
  issn      = {0894-8755},
  abstract  = {Estimating the mean and the covariance matrix of an incomplete dataset and filling in missing values with imputed values is generally a nonlinear problem, which must be solved iteratively. The expectation maximization (EM) algorithm for Gaussian data, an iterative method both for the estimation of mean values and covariance matrices from incomplete datasets and for the imputation of missing values, is taken as the point of departure for the development of a regularized EM algorithm. In contrast to the conventional EM algorithm, the regularized EM algorithm is applicable to sets of climate data, in which the number of variables typically exceeds the sample size. The regularized EM algorithm is based on iterated analyses of linear regressions of variables with missing values on variables with available values, with regression coef?cients estimated by ridge regression, a regularized regression method in which a continuous regularization parameter controls the filtering of the noise in the data. The regularization parameter is determined by generalized cross-validation, such as to minimize, approximately, the expected mean-squared error of the imputed values. The regularized EM algorithm can estimate, and exploit for the imputation of missing values, both synchronic and diachronic covariance matrices, which may contain information on spatial covariability, stationary temporal covariability, or cyclostationary temporal covariability. A test of the regularized EM algorithm with simulated surface temperature data demonstrates that the algorithm is applicable to typical sets of climate data and that it leads to more accurate estimates of the missing values than a conventional noniterative imputation technique.},
  booktitle = {Journal of Climate},
  comment   = {doi: 10.1175/1520-0442(2001)014<0853:AOICDE>2.0.CO;2
Review:
Estimating missing data using normal assumption, regularized EM. Has Matlab. Assumes a normal distribution like many other methods, but EM is normalized for stability (unlike Amelia II, Honaker10missValTseries) and the regularization is done during regression (unlike BPCAfill, Oba03pcaImp, which regularizes by selecting the number of PCA comps) Could also bootstrap, like Amelia II to get better error bounds, and perhaps a better mean value estimate for the missing values. Matlab (regEM) is available here: http://climate-dynamics.org/software/num.regem (Matlab is frequently updated, at least up to 2012)},
  doi       = {10.1175/1520-0442(2001)014<0853:AOICDE>2.0.CO;2},
  file      = {Schneider01missImputMeanCov.pdf:Schneider01missImputMeanCov.pdf:PDF},
  owner     = {sotterson},
  publisher = {American Meteorological Society},
  timestamp = {2012.07.25},
}

@Article{Wang12hiCondQuantHeavyTail,
  author    = {Wang, Huixia Judy and Li, Deyuan and He, Xuming},
  title     = {Estimation of high conditional quantiles for heavy-tailed distributions},
  journal   = {Journal of the American Statistical Association},
  year      = {2012},
  volume    = {107},
  number    = {500},
  pages     = {1453--1464},
  abstract  = {Estimation of conditional quantiles at very high or low tails is of interest in numerous applications. Quantile regression provides a convenient
and natural way of quantifying the impact of covariates at different quantiles of a response distribution. However, high tails are often associated
with data sparsity, so quantile regression estimation can suffer from high variability at tails especially for heavy-tailed distributions. In this
article, we develop new estimation methods for high conditional quantiles by first estimating the intermediate conditional quantiles in a
conventional quantile regression framework and then extrapolating these estimates to the high tails based on reasonable assumptions on tail
behaviors. We establish the asymptotic properties of the proposed estimators and demonstrate through simulation studies that the proposed
methods enjoy higher accuracy than the conventional quantile regression estimates. In a real application involving statistical downscaling of
daily precipitation in the Chicago area, the proposed methods provide more stable results quantifying the chance of heavy precipitation in
the area. Supplementary materials for this article are available online.
KEY WORDS: Downscaling; Extrapolation; Extreme value; High quantile; Quantile regression.},
  comment   = {Axel's extreme value paper idea.},
  doi       = {10.1080/01621459.2012.716382},
  file      = {Wang12hiCondQuantHeavyTail.pdf:Wang12hiCondQuantHeavyTail.pdf:PDF},
  publisher = {Taylor \& Francis},
}

@Article{Meinshausen10Stabilityselection,
  author    = {Meinshausen, Nicolai and B{\"u}hlmann, Peter},
  title     = {Stability selection},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year      = {2010},
  volume    = {72},
  number    = {4},
  pages     = {417--473},
  abstract  = {Estimation of structure, such as in variable selection, graphical modelling or cluster analysis, is notoriously difficult, especially for high dimensional data. We introduce stability selection. It is based on subsampling in combination with (high dimensional) selection algorithms. As such, the method is extremely general and has a very wide range of applicability. Stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularization for structure estimation. Variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. We prove for the randomized lasso that stability selection will be variable selection consistent even if the necessary conditions for consistency of the original lasso method are violated. We demonstrate stability selection for variable selection and Gaussian graphical modelling, using real and simulated data.},
  comment   = {The feature selection algorithm used in Fenske11idRiskBoostAddQR

Compare with Nogueira18featSelStbl},
  doi       = {10.1111/j.1467-9868.2010.00740.x/full},
  file      = {Meinshausen10Stabilityselection.pdf:Meinshausen10Stabilityselection.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.07.25},
}

@TechReport{Pape15fndmntlPriceVarDEpowMkt,
  author      = {Pape, Christian and Weber, Christoph and others},
  title       = {Are Fundamentals Enough? Explaining Price Variations in the German Day-Ahead and Intraday Power Market},
  institution = {University of Duisburg-Essen, Chair for Management Science and Energy Economics},
  year        = {2015},
  abstract    = {European electricity market participants are encouraged to balance intraday deviations from
their day-ahead schedules via trades in the intraday market. Together with the increasing produc-
tion of variable renewable energy sources, the intraday market is gaining importance. We inves-
tigate the explanatory power of a fundamental modeling approach explicitly accounting for must-
run operations of combined heat and power plants (CHP) and intraday peculiarities such as a
shortened intraday supply stack. The fundamental equilibria between every hour?s supply stack
and aggregated demand in 2012 and 2013 are modeled to yield hourly price estimates. The major
benefits of a fundamental modeling approach are the ability to account for non-linearities in the
supply stack and the ability to combine time-varying information consistently. The empirical re-
sults show that fundamental modeling explains a considerable share of spot price variance. How-
ever, differences between the fundamental and actual prices persist and are explored using re-
gression models. The main differences can be attributed to (avoided) start up-costs, market states and trading behavior.

Keywords: Intraday market for electricity, fundamental price modeling},
  comment     = {Useful for DE intraday prob. frcst experiments?},
  file        = {Pape15fndmntlPriceVarDEpowMkt.pdf:Pape15fndmntlPriceVarDEpowMkt.pdf:PDF},
  url         = {http://www.econstor.eu/bitstream/10419/113275/1/822013509.pdf},
}

@PhdThesis{Scolnik16binPredAccIncAndOpt_PhD,
  author        = {Scolnik, Ryan},
  title         = {Predictive Accuracy Measures for Binary Outcomes: Impact of Incidence Rate and Optimization Techniques},
  school        = {Florida State University},
  year          = {2016},
  __markedentry = {[Scott:1]},
  abstract      = {Evaluating the performance of models predicting a binary outcome can be done using a variety of
measures. While some measures intend to describe the model’s overall fit, others more accurately
describe the model’s ability to discriminate between the two outcomes. If a model fits well but
doesn’t discriminate well, what does that tell us? Given two models, if one discriminates well
but has poor fit while the other fits well but discriminates poorly, which of the two should we
choose? The measures of interest for our research include the area under the ROC curve, Brier
Score, discrimination slope, Log-Loss, R2 and Fbeta. To examine the underlying relationships among
all of the measures, real data and simulation studies are used.
The real data comes from multiple cardiovascular research studies and the simulation studies
are run under general conditions and also for incidence rates ranging from 2% to 50%. The results
of these analyses provide insight into the relationships among the measures and raise concern for
scenarios when the measures may yield different conclusions. The impact of incidence rate on the
relationships provides a basis for exploring alternative maximization routines to logistic regression.
While most of the measures are easily optimized using the Newton-Raphson algorithm, the max-
imization of the area under the ROC curve requires optimization of a non-linear, non-differentiable
function. Usage of the Nelder-Mead simplex algorithm and close connections to economics research
yield unique parameter estimates and general asymptotic conditions. Using real and simulated
data to compare optimizing the area under the ROC curve to logistic regression further reveals the
impact of incidence rate on the relationships, significant increases in achievable areas under the
ROC curve and differences in conclusions about including a variable in a model.},
  comment       = {Shows how incidence (rate of true positives) affects Brier score (see bookmarks, Section 3.4) and other metrics for probabilistic classification.  Also interesting in that it shows how many performance metrics are related, sometimes with equations.  .

* "incidence" means number of positive cases

* Rsquared vs. BS and incidence (Brier Score, eq. 3.6):  
  - there's a simple equation
  - incidence important in denominator
* AUC vs. logloss and incidence (section 3.3.4)
  - relationship is kinda linear
  - AUC is lower than linear when there is low indidence or low logloss
* BS and incidence (section 3.4.2)
   - BS isn't independent of incidence: is biased high by high incidence

AUC (area under ROC) vs. incidence
* AUC doesn't have a unique solution
* using for optimization can't rely on derivatives
* when logistic regression fit with log lik and evaluated w/ AUC overall fit (must be AUC) and accuracy don't line up (p 81)
* for low incidence cases, apparently suggest Nelder-Mead Simplex for logistic regression opt. or the method proposed in this thesis, I think AUCRanks


},
  file          = {:Scolnik16binPredAccIncAndOpt_PhD.pdf:PDF},
  publisher     = {USA: Florida State University},
  url           = {https://fsu.digital.flvc.org/islandora/object/fsu%3A360437},
}

@InProceedings{Habte14calMeasUncertRadiometric,
  author    = {Aron Habte and Manajit Sengupta and Ibrahim Reda and A Neuber Andreas and Joep Konings},
  title     = {Calibration and Measurement Uncertainty Estimation of Radiometric Data},
  booktitle = {Solar},
  year      = {2014},
  address   = {Sand Fransisco, CA},
  month     = jul,
  abstract  = {Evaluating the performance of photovoltaic cells, modules, and 
arrays that form large solar deployments relies on accurate 
measurements of the available solar resource. Therefore, 
determining the accuracy of these solar radiation measurements 
provides a better understanding of investment risks. This 
becomes especially important as deployment size and 
investment costs grow to hundreds of millions of dollars. The 
accuracy of measurements is also important for acceptance 
testing and operations. 
Radiometers such as pyranometers are used to measure global 
horizontal irradiance or plane of array irradiance, whereas 
pyrheliometers are used to measure direct normal irradiance. 
Currently, most radiometric data users rely on manufacturers’ 
specifications of calibration uncertainty to quantify the 
uncertainty of measurements. However, the accuracy of solar 
radiation measured by radiometers depends not only on the 
specifications of the instrument but also on the (a) calibration 
procedure, (b) measurement conditions and maintenance, and 
(c) environmental conditions. Therefore, statements about the 
overall measurement uncertainty can be made only on an 
individual basis, taking all relevant factors into account. This 
paper provides guidelines and recommended procedures for 
estimating the uncertainty in calibrations and measurements by 
radiometers using methods that follow the Guide to the 
Expression of Uncertainty (GUM). Standardized analysis based 
on these procedures ensures that the uncertainty quoted is well 
documented. 
Keywords: Global Horizontal Irradiance; Direct Normal 
Irradiance; Type A; Type B; Uncertainty},
  comment   = {Maybe this is what CPR uses to compute satellite irradiance uncertainty.  It's referenced by the slides in Habte15radiometUncertGUM

Method: JCGM08guideUncertMeas},
  file      = {:Habte14calMeasUncertRadiometric.pdf:PDF},
  url       = {https://www.osti.gov/servlets/purl/1164884},
}

@TechReport{Dubois15deepMineCopulaHyper,
  author      = {S{\'e}bastien Dubois},
  title       = {Deep Mining : Copula-based Hyper-Parameter Optimization for Machine Learning Pipelines},
  institution = {{'\E}cole polytechnique and Massachusetts Institute of Technology},
  year        = {2015},
  abstract    = {Every machine learning model has several hyper-parameters that need to be carefully chosen for
they can hugely impact its quality. While grid search was the first automatic approach to tackle
this problem, random search has proved to be much faster for such tasks. Recently some sequential
models have been proposed in order to leverage the information acquired during the search process.
We build our work upon such a technique which models the performances yielded by the hyper-
parameters with a Gaussian Process (GP).
First, we present a novel non-parametric approach for Gaussian Copula Processes (nGCP), and use
it for hyper-parameter optimization. In addition, we present a framework to auto-tune machine
learning pipelines (MLP). Specifically, we consider noisy performance evaluations which speeds up
the search and paves the way for parallel designs.
We finally demonstrate that nGCP outperforms GP for regression. We also test nGCP-based
hyper-parameter optimization techniques on two classic problems involving text data and images.
We show that even with noisy estimations our techniques outperform random search, and that
nGCP-based methods are a bit faster than GP-based ones.},
  comment     = {Gaussian Copula Processes for big, costly hyperparameter tuning iterations, has python code.

Is similar to Bayesian learning with Gaussian Processes, but says that Gaussian Copula Processes are better models.

Python: https://github.com/samandarr/DeepMini

Is this the same Copula Processes as in: Wilson10copulaProc ?
Gaussian Copula Processes can handle missing data: Wilson10copulaProc},
  file        = {Thesis:Dubois15deepMineCopulaHyper:PDF},
  owner       = {sotterson},
  timestamp   = {2017.03.17},
  url         = {http://groups.csail.mit.edu/EVO-DesignOpt/groupWebSite/uploads/Site/DuboisThesis},
}

@TechReport{Mills14integPVbalRsrv,
  author      = {A. Mills and A. Botterud and J. Zhou and B-M. Heaney},
  title       = {Integrating Solar {PV} in Utility System Operations},
  institution = {Argonne National Laboratory},
  year        = {2014},
  type        = {Technical Report},
  number      = {ANL/DIS-13/18},
  abstract    = {Executive Summary

Deployment of solar photovoltaic (PV) power generation is growing rapidly in the United States.
Utilities and system operators are increasingly conducting studies of the impact of PV on
operations, including assessments of short-term variability and uncertainty. Consideration of the
complex issues surrounding sub-hourly variability and forecasting of PV power output has still
been somewhat limited because of the difficulty of creating realistic sub-hourly PV datasets and
forecast errors for future scenarios with increased PV production. How utility operations should
be changed to more economically integrate large amounts of solar PV power is an open question
currently being considered by many utilities. This study develops a systematic framework for
estimating the increase in operating costs due to uncertainty and variability in renewable
resources, uses the framework to quantify the integration costs associated with sub-hourly solar
power variability and uncertainty, and shows how changes in system operations may affect these
costs. Toward this end, we present a statistical method for estimating the required balancing
reserves to maintain system reliability along with a model for commitment and dispatch of the
portfolio of thermal and renewable resources at different stages of system operations. We
estimate the costs of sub-hourly solar variability, short-term forecast errors, and day-ahead (DA)
forecast errors as the difference in production costs between a case with ?realistic? PV (i.e., sub-
hourly solar variability and uncertainty are fully included in the modeling) and a case with ?well
behaved? PV (i.e., PV is assumed to have no sub-hourly variability and can be perfectly
forecasted). In addition, we highlight current practices that allow utilities to compensate for the
issues encountered at the sub-hourly time frame with increased levels of PV penetration.

In this analysis we use the analytical framework to simulate utility operations with increasing
deployment of PV in a case study of Arizona Public Service Company (APS), a utility in the
southwestern United States. In our analysis, we focus on three processes that are important in
understanding the management of PV variability and uncertainty in power system operations.
First, we represent the decisions made the day before the operating day through a DA
commitment model that relies on imperfect DA forecasts of load and wind as well as PV
generation. Second, we represent the decisions made by schedulers in the operating day through
hour-ahead (HA) scheduling. Peaking units can be committed or decommitted in the HA
schedules and online units can be redispatched using forecasts that are improved relative to DA
forecasts, but still imperfect. Finally, we represent decisions within the operating hour by
schedulers and transmission system operators as real-time (RT) balancing. We simulate the DA
and HA scheduling processes with a detailed unit-commitment (UC) and economic dispatch (ED)
optimization model. This model creates a least-cost dispatch and commitment plan for the
conventional generating units using forecasts and reserve requirements as inputs. We consider
only the generation units and load of the utility in this analysis; we do not consider opportunities
to trade power with neighboring utilities. We also do not consider provision of reserves from
renewables or from demand-side options.

We estimate dynamic reserve requirements in order to meet reliability requirements in the RT
operations, considering the uncertainty and variability in load, solar PV, and wind resources.
Balancing reserve requirements are based on the 2.5
th
 and 97.5
th
 percentile of 1-min deviations
from the HA schedule in a previous year. We then simulate RT deployment of balancing reserves using a separate minute-by-minute simulation of deviations from the HA schedules in the
operating year. In the simulations we assume that balancing reserves can be fully deployed in
10 min. The minute-by-minute deviations account for HA forecasting errors and the actual
variability of the load, wind, and solar generation. Using these minute-by-minute deviations and
deployment of balancing reserves, we evaluate the impact of PV on system reliability through
the calculation of the standard reliability metric called Control Performance Standard 2 (CPS2).
Broadly speaking, the CPS2 score measures the percentage of 10-min periods in which a
balancing area is able to balance supply and demand within a specific threshold. Compliance
with the North American Electric Reliability Corporation (NERC) reliability standards requires
that the CPS2 score must exceed 90% (i.e., the balancing area must maintain adequate balance
for 90% of the 10-min periods). The combination of representing DA forecast errors in the DA
commitments, using 1-min PV data to simulate RT balancing, and estimates of reliability
performance through the CPS2 metric, all factors that are important to operating systems with
increasing amounts of PV, makes this study unique in its scope.

Results

We analyze the impact of distributed and utility-scale PV on the APS system based on projected
conventional generation, load, and wind and PV resources in 2027. Two PV deployment levels
are considered: low PV is based on the PV that APS includes in its 2012 Integrated Resource
Plan (IRP) base case, and high PV is based on the PV penetration that APS includes in the
expanded renewables case of the IRP. The low-PV case includes sufficient PV to meet 8.8% of
the annual energy, and the high-PV case includes enough PV to meet 17.0% of the annual energy
(prior to any curtailment of renewables). Both cases also consider wind penetration of 4.9% of
annual energy. Based on existing practices at APS five of the eight coal plants are treated as
must-run units that can dispatch between minimum and maximum generation, but they cannot be
turned off. Similarly, nuclear units are always operated at full nameplate capacity. We find that
the combination of must-run generation, inflexible nuclear operations, and large amounts of solar
in the high-PV case leads to severe operational challenges during low-load and high solar periods
under the assumption that the utility cannot trade power with neighboring utilities. For a high-PV
case to be practical, some solution to these challenges will be necessary. We included a ?flexible
nuclear? case as one option for introducing flexibility during low-load and high solar periods.
The impacts of this level of PV deployment under the assumption of constant nuclear operation
in the low-PV and high-PV cases and the alternative flexible nuclear operation in the high-PV
case are summarized in Table ES-1.

The assumption of flexible nuclear operation in the high-PV flexible nuclear case (where all the
nuclear units can operate below maximum output and can provide reserves) decreases the
integration cost and greatly reduces the need to curtail renewables from almost 18% down to
3.4% of available renewables},
  comment     = {Estimation of RES integration costs -- for solar most costly are in the hour-ahead range -- including an estimate of needed balancing reserves (relevant to ReWP). Inflexible nuclear doesn't play well with solar PV and wind.},
  file        = {Mills14integPVbalRsrv.pdf:Mills14integPVbalRsrv.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2014.12.20},
}

@Misc{Jornsten07btstrpClassNotes,
  author    = {Rebecka Jornsten},
  title     = {"Bootstrap methods" Class notes Stat 565, Rutgers University.},
  year      = {2007},
  abstract  = {Exerpt: Bootstrapping in the frequency domain has an appeal in that the Fourier transform acts as a decorrelating transform.},
  comment   = {General bootstrapping class notes, esp. bootstrapping in frequency domain, has R code. Maybe useful for missing feature fill-in. p. 6, Section 4: Frequency domain methods Models random variance by sampling (what amounts to?) linear prediction error. Calls it a "frequency domain jacknife": (like "Residual-based bootstrap in Kirch11timeFreqBtStrp, which recommends instead a "local bootstrap" based on the same idea) X --> DFT --> Whiten --> IFT --> random sample --> DFT --> unwhiten --> IFT Gives example where frequency domain jackknife works nearly as well as isolating a significant lag 2 AR relationship. But QUESTION: If not as good, then why use it? Because it can be used for more than ccf CI construction? Link to this pdf: http://www.stat.rutgers.edu/home/rebecka/Stat565/bootstrap.pdf R code on class page (this entry's main URL)},
  file      = {Jornsten07btstrpClassNotes.pdf:Jornsten07btstrpClassNotes.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2012.12.03},
  url       = {http://www.stat.rutgers.edu/home/rebecka/Stat565/},
}

@Article{Wood08FastSmthGAM,
  author    = {Wood, Simon N},
  title     = {Fast stable direct fitting and smoothness selection for generalized additive models},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year      = {2008},
  volume    = {70},
  number    = {3},
  pages     = {495--518},
  abstract  = {Existing computationally efficient methods for penalized likelihood generalized additive model fitting employ iterative smoothness selection on working linear models (or working mixed models). Such schemes fail to converge for a non-negligible proportion of models, with failure being particularly frequent in the presence of concurvity. If smoothness selection is performed by optimizing ?whole model? criteria these problems disappear, but until now attempts to do this have employed finite-difference-based optimization schemes which are computationally inefficient and can suffer from false convergence. The paper develops the first computationally efficient method for direct generalized additive model smoothness selection. It is highly stable, but by careful structuring achieves a computational efficiency that leads, in simulations, to lower mean computation times than the schemes that are based on working model smoothness selection. The method also offers a reliable way of fitting generalized additive mixed models.

Keywords:

 Akaike's information criterion;
 Generalized additive mixed models;
 Generalized additive models;
 Generalized approximate cross-validation;
 Generalized cross-validation;
 Penalized likelihood;
 Penalized regression splines;
 Stable computation},
  comment   = {See slides attached. Has R

Appears to have been used in load forecasting in Wood14GnrlzdGAMlrgDat

Intro in: Wood03ThinPltRgrsSpln and Wood06LowRnkTensProSmthGAM

I think many of these algorithms are implemented in R mgcv toolbox:
http://people.bath.ac.uk/sw283/mgcv/

The toolbox vignettes are worth looking at too e.g. "A toolbox of smooths" here:
http://people.bath.ac.uk/sw283/mgcv/smooth-toolbox.pdf},
  doi       = {10.1111/j.1467-9868.2007.00646.x/full},
  file      = {paper:Wood08FastSmthGAM.pdf:PDF;slides:Wood08FastSmthGAM_slides.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.07.25},
}

@Article{Li07sprsSuffDimRed,
  author    = {Li, Lexin},
  title     = {Sparse sufficient dimension reduction},
  journal   = {Biometrika},
  year      = {2007},
  volume    = {94},
  number    = {3},
  pages     = {603--613},
  abstract  = {Existing sufficient dimension reduction methods suffer from the fact that each dimension reduction component is a linear combination of all the original predictors, so that it is difficult to interpret the resulting estimates. We propose a unified estimation strategy, which combines a regression-type formulation of sufficient dimension reduction methods and shrinkage estimation, to produce sparse and accurate solutions. The method can be applied to most existing sufficient dimension reduction methods such as sliced inverse regression, sliced average variance estimation and principal Hessian directions. We demonstrate the effectiveness of the proposed method by both simulations and real data analysis.},
  comment   = {For lagged velocity basis function regression. Does LASSO, shrinkage, etc.},
  doi       = {10.1093/biomet/asm044},
  eprint    = {http://biomet.oxfordjournals.org/cgi/reprint/94/3/603.pdf},
  file      = {Li07sprsSuffDimRed.pdf:Li07sprsSuffDimRed.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.08.31},
  url       = {http://biomet.oxfordjournals.org/cgi/content/abstract/94/3/603},
}

@Article{Flynn06johnsonSBextrm,
  author    = {Flynn, Michael R},
  title     = {Fitting human exposure data with the Johnson SB distribution},
  journal   = {Journal of Exposure Science and Environmental Epidemiology},
  year      = {2006},
  volume    = {16},
  number    = {1},
  pages     = {56--62},
  abstract  = {Exposure evaluations for epidemiological investigations and risk assessments may require estimates of background concentrations and peak exposures, as well as the population mean and variance. The SB distribution is a theoretically appealing probability function for characterizing ratios, and random variables bound by extremes, such as human exposures and environmental concentrations. However, fitting the parameters of this distribution with maximum likelihood methods is often problematic, and some alternative methods are examined here. Two methods based on percentiles, a quantile estimator, and a method-of-moments fitting procedure are explored. The quantile and method-of-moments procedures are based on new explicit expressions for the first four moments of this distribution. The fitting procedures are compared by simulation, and with actual data sets consisting of measurements of human exposure to airborne contaminants.
Keywords:

exposure modeling, Johnson SB distribution, 4-parameter lognormal distribution, parameter estimation, exposure assessment},
  comment   = {Extreme values with bounded Johnson distribution. Could use like Axel's alpha-stable idea (not bounded like wind) for extreme values. Maybe these are tighter.},
  file      = {Flynn06johnsonSBextrm.pdf:Flynn06johnsonSBextrm.pdf:PDF},
  owner     = {sotterson},
  publisher = {Nature Publishing Group},
  timestamp = {2015.11.22},
  url       = {http://www.nature.com/jes/journal/v16/n1/full/7500437a.html},
}

@Article{BenBourllegue13ensCalLgstIntrct,
  author    = {Ben Bouall{\`e}gue, Zied},
  title     = {Calibrated short-range ensemble precipitation forecasts using extended logistic regression with interaction terms},
  journal   = {Weather and Forecasting},
  year      = {2013},
  volume    = {28},
  number    = {2},
  pages     = {515--524},
  abstract  = {Extended logistic regression has been shown to be a method well suited to calibrating precipitation forecasts from medium-range ensemble prediction systems. The extension of the logistic regression unifies the separate predictive equations for each threshold, introducing the predictive threshold as part of the predictors. Mutually consistent probabilities and a reduction in the total number of regression parameters to be evaluated are part of the benefits of the extended approach. However, considering the predictive threshold as the only ???nification? predictor constrains the regression parameters associated with the primary predictors to be constant with the threshold. To alleviate the rigidity of the extended scheme, interaction terms are introduced in the unified predictive equation. Within the framework of the convection-permitting German-focused Consortium for Small-Scale Modeling ensemble prediction system (COSMO-DE-EPS), it is shown that extended logistic regression, applied to short-range precipitation forecasts with the ensemble mean as the primary predictor, improves the performance of the system. Interaction effects are first illustrated through the analysis of regression parameters and then the positive impact on the calibrated forecasts of the new extended logistic regression scheme, including interaction terms, is shown using quantitative and qualitative measures of reliability and sharpness.

Keywords: Statistical techniques, Ensembles, Forecast verification/skill, Short-range prediction},
  comment   = {Ensemble calibration with interaction terms.

preliminary version is also attached in case scan is too hard to read.},
  doi       = {10.1175/WAF-D-12-00062.1},
  file      = {Published Journal Paper:BenBourllegue13ensCalLgstIntrct.pdf:PDF;Preliminary 2012 version:BenBourllegue13ensCalLgstIntrct_prelim.pdf:PDF},
  groups    = {Ensemble, CitaviImport1, doReadNonWPV_1},
  ncite     = {3},
  owner     = {sotterson},
  timestamp = {2013.09.26},
}

@Article{Ho10condEntropErr,
  author    = {Siu-Wai Ho and Verdu? and, S.},
  title     = {On the Interplay Between Conditional Entropy and Error Probability},
  journal   = {Information Theory, IEEE Transactions on},
  year      = {2010},
  volume    = {56},
  number    = {12},
  pages     = {5930--5942},
  month     = dec,
  issn      = {0018-9448},
  abstract  = {Fano's inequality relates the error probability of guessing a finitely-valued random variable X given another random variable Y and the conditional entropy of X given Y. It is not necessarily tight when the marginal distribution of X is fixed. This paper gives a tight upper bound on the conditional entropy of X given Y in terms of the error probability and the marginal distribution of X. A new lower bound on the conditional entropy for countably infinite alphabets is also found. The relationship between the reliability criteria of vanishing error probability and vanishing conditional entropy is also discussed. A strengthened form of the Schur-concavity of entropy which holds for finite or countably infinite random variables is given.},
  comment   = {tighter bounds on error prob than Fano's inequality cond. entropy. Also, I think generalizations for infinite variables. Does this explain anything in Kraskov03hierClustMutInf, where can't divide by entropy for tiny delta, and therefore dimension scaling can't be "divde by entropy" but just by dims? This is for conditional entropies, here, but is there a relation?},
  doi       = {10.1109/TIT.2010.2080891},
  file      = {Ho10condEntropErr.pdf:Ho10condEntropErr.pdf:PDF},
  keywords  = {Fano inequality;Schur-concavity;finitely-valued random variable;infinite random variables;vanishing conditional entropy;vanishing error probability;entropy;error statistics;},
  owner     = {scotto},
  timestamp = {2011.05.20},
}

@InProceedings{Laskey14fastFrugalTreesVsBayesNetsRisk,
  author    = {Laskey, K and Martignon, Laura},
  title     = {Comparing fast and frugal trees and Bayesian networks for risk assessment},
  booktitle = {Proc. 9th International Conference on Teaching Statistics},
  year      = {2014},
  address   = {Flagstaff, Arizona, USA},
  abstract  = {Fast and frugal trees have been proposed as efficient heuristics for decision under risk. We 
describe the construction of fast and frugal trees and compare their robustness for prediction 
under risk with that of Bayesian networks. In particular we analyze situations of risky decisions in 
the medical domain. We show that the performance of fast and frugal trees does not fall too far 
behind that of the more complex Bayesian networks.},
  comment   = {A "frugal learning" approach, I think.},
  file      = {:Laskey14fastFrugalTreesVsBayesNetsRisk.pdf:PDF},
}

@InProceedings{Brown09newInfoFeatSel,
  author       = {Brown, G.},
  title        = {A new perspective for information theoretic feature selection},
  booktitle    = {Artificial Intelligence and Statistics},
  year         = {2009},
  volume       = {5},
  pages        = {49--56},
  organization = {Citeseer},
  abstract     = {Feature Filters are among the simplest and fastest approaches to feature selection. A filter defines a statistical criterion, used to rank features on how useful they are expected to be for classification. The highest ranking features are retained, and the lowest ranking can be discarded. A common approach is to use the Mutual Information between the feature and class label. This area has seen a recent flurry of activity, resulting in a confusing variety of heuristic criteria all based on mutual information, and a lack of a principled way to understand or relate them. The contribution of this paper is a unifying theoretical understanding of such filters. In contrast to current methods which manually construct filter criteria with particular properties, we show how to naturally derive a space of possible ranking criteria. We will show that several recent contributions in the feature selection literature are points within this continuous space, and that there exist many points that have never been explored.},
  comment      = {Mutual info bounds regression error; generalization of all pairwise MI featsel techniques, suggesting new parameteriztions could be better; good multi-Dim MI info

* explains why MI maximization good for featsel for /any/ regression function (from Fano's inequality)
 -- This is OK for discrete output (classifier output) where the code size is defined, as needed for standard Fano's
 -- but does this work across dimensions? ie. if X and Y have different dims, or if you're comparing g(x) for X's of different dims?
* pairwise MI feature ranking is optimal for Naive Bayes (if true) for maximizing log likelihood of data. Sub optimal for interdependent data.
* MI decomposed into (slightly) simpler interaction terms, w/o the grouping
* All greedy one-step algs in effect assume only pairwise interactions !!
* General equation pairwise approx is given
 -- shows that 12 published MI featsel algs. actually use this
 -- truncating the general equation at two assumes that no higher order interactions exist (beyond pairwise)
 ---- this is what peng05featSelMutInfo and others do....
 ---- Me: so, it would miss MI from an output which is the sum of three variables!
 ---- I can see why this could fail
 ---- Also, since interaction information can be pos. or neg. the truncation might increase or decrease MI.
 -- Could tune the two params of the general form to get new methods not yet explored
 ---- (and these have better perf than any existing method)
* comparison of the MI featsel algs, but only w/ a 1-NN classifier; not sure how good this is.
* papers that cite it are interesting! E.g. concept drift... See google scholar
* future research is to include higher order terms beyond just pairwise Multivariate mutual information (clarified in Appendix A)
* two main types
1. Shannon Mutual Information
 -- the usual one, what's in pairwise MI
 -- expresses how much info is gained about a variable given some others
 -- always non-neg? Not said here, but Kraskov08MIChierClustMutInf info said to be non-neg: is it Shannon?
2. McGill Interaction Information
 -- information among all variables
 -- is used in this paper to derive the general MI relationship
 -- can be neg
* Eq. 3 is the relation between interaction and shannon information (proof included)
* multi-dim MI can be negative!
 -- example explains why.
 -- This was McGill interaction info, not Shannon, I think
 -- Srinivasa05mutInfoReview agrees
 -- Kraskov08MIChierClustMutInf disagrees, for binned MI},
  file         = {Brown09newInfoFeatSel.pdf:Brown09newInfoFeatSel.pdf:PDF},
  groups       = {Read},
  owner        = {scotto},
  timestamp    = {2011.05.15},
  url          = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.158.7880&rep=rep1&type=pdf},
}

@InProceedings{Jovic15revFeatSel,
  author    = {A. Jovi{\'{c}} and K. Brki{\'{c}} and N. Bogunovi{\'{c}}},
  title     = {A review of feature selection methods with applications},
  booktitle = {Proc. 38th Int Information and Communication Technology, Electronics and Microelectronics Convention},
  year      = {2015},
  month     = may,
  pages     = {1200--1205},
  doi       = {10.1109/MIPRO.2015.7160458},
  abstract  = {Feature selection (FS) methods can be used in data pre-processing to achieve efficient data reduction. This is useful for finding accurate data models. Since exhaustive search for optimal feature subset is infeasible in most cases, many search strategies have been proposed in literature. The usual applications of FS are in classification, clustering, and regression tasks. This review considers most of the commonly used FS techniques. Particular emphasis is on the application aspects. In addition to standard filter, wrapper, and embedded methods, we also provide insight into FS for recent hybrid approaches and other advanced topics.},
  file      = {paper:Jovic15revFeatSel.pdf:PDF},
  keywords  = {data reduction, embedded systems, feature selection, pattern clustering, regression analysis, search problems, FS methods, application aspects, classification, clustering, data preprocessing, data reduction, embedded methods, feature selection methods, hybrid approaches, optimal feature subset, regression tasks, search strategies, standard filter, wrapper, Accuracy, Classification algorithms, Clustering algorithms, Filtering algorithms, Information filters, Search problems},
  owner     = {sotterson},
  timestamp = {2016.06.18},
}

@InCollection{BolonCanedo15CritRevFeatSel,
  author    = {Bol{\'o}n-Canedo, Ver{\'o}nica and S{\'a}nchez-Maro{\~n}o, Noelia and Alonso-Betanzos, Amparo},
  title     = {A Critical Review of Feature Selection Methods},
  booktitle = {Feature Selection for High-Dimensional Data},
  year      = {2015},
  publisher = {Springer},
  pages     = {29--60},
  doi       = {10.1007/978-3-319-21858-8_3},
  abstract  = {Feature selection has been a fruitful field of research and it is undoubtedly important. However, a statement like ?the best feature selection method? simply does not exist in general, making it difficult for users to select one method over another. For this reason, the objective of this chapter is to perform a critical review of state-of-the-art feature selection methods. The chapter starts with the description of the existing reviews (Section 3.1). Then, Section 3.2 depicts the methods and data involved in the experiments of this chapter and Section 3.3 shows the results obtained. In Section 3.4 we present several cases of study, aiming at deciding between methods that showed similar behaviors. Finally, Section 3.5 analyzes and discusses the findings of the experimental study and Section 3.6 summarizes this chapter},
  owner     = {sotterson},
  timestamp = {2016.06.18},
}

@Article{Zheng11featSelHimDimMI,
  author    = {Zheng, Yun and Kwoh, Chee Keong},
  title     = {A Feature Subset Selection Method Based On High-Dimensional Mutual Information},
  journal   = {Entropy},
  year      = {2011},
  volume    = {13},
  number    = {4},
  pages     = {860--901},
  issn      = {1099-4300},
  abstract  = {Feature selection is an important step in building accurate classifiers and provides better understanding of the data sets. In this paper, we propose a feature subset selection method based on high-dimensional mutual information. We also propose to use the entropy of the class attribute as a criterion to determine the appropriate subset of features when building classi?ers. We prove that if the mutual information between a feature set X and the class attribute Y equals to the entropy of Y, then X is a Markov Blanket of Y . We show that in some cases, it is infeasible to approximate the high-dimensional mutual information with algebraic combinations of pairwise mutual information in any forms. In addition, the exhaustive searches of all combinations of features are prerequisite for finding the optimal feature subsets for classifying these kinds of data sets. We show that our approach outperforms existing filter feature subset selection methods for most of the 24 selected benchmark data sets. Keywords: feature selection; mutual information; Entropy; information theory; Markov blanket; classi?cation},
  comment   = {how to pick num. of clusters/features (MI==Entropy) and how to partition},
  doi       = {10.3390/e13040860},
  file      = {Zheng11featSelHimDimMI.pdf:Zheng11featSelHimDimMI.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2011.12.09},
  url       = {http://www.mdpi.com/1099-4300/13/4/860/},
}

@InProceedings{Doquire11mutInfMissDat,
  author    = {Doquire, G. and Verleysen, M.},
  title     = {Mutual information for feature selection with missing data},
  booktitle = {European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, Bruges (Belgium)},
  year      = {2011},
  pages     = {27--29},
  abstract  = {Feature selection is an important task for many machine learning applications; moreover missing data are encoutered very often in practice. This paper proposes to adapt a nearest neighbors based mutual information estimator to handle missing data and to use it to achieve feature selection. Results on artifcial and real world datasets show that the method is able to select important features without the need for any imputation algorithm. Moreover, experiments also indicate that selecting the features before imputing the data generally increases the precision of the prediction models},
  comment   = {Simple mod of Kraskov MI to handle missing features. No imputation. Kraskov: Kraskov04EstMutInfKNN},
  file      = {Doquire11mutInfMissDat.pdf:Doquire11mutInfMissDat.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2012.03.25},
  url       = {http://www.i6doc.com/en/livre/?GCOI=28001100817300},
}

@Article{Nogueira18featSelStbl,
  author   = {Nogueira, Sarah and Sechidis, Konstantinos and Brown, Gavin},
  title    = {On the Stability of Feature Selection Algorithms.},
  journal  = {Journal of Machine Learning Research},
  year     = {2018},
  volume   = {18(174)},
  pages    = {1-54},
  abstract = {Feature Selection is central to modern data science, from exploratory data analysis to
predictive model-building. The “stability” of a feature selection algorithm refers to the
robustness of its feature preferences, with respect to data sampling and to its stochastic
nature. An algorithm is ‘unstable’ if a small change in data leads to large changes in
the chosen feature subset. Whilst the idea is simple, quantifying this has proven more
challenging—we note numerous proposals in the literature, each with different motivation
and justification. We present a rigorous statistical treatment for this issue. In particular,
with this work we consolidate the literature and provide (1) a deeper understanding of
existing work based on a small set of properties, and (2) a clearly justified statistical
approach with several novel benefits. This approach serves to identify a stability measure
obeying all desirable properties, and (for the first time in the literature) allowing confidence
intervals and hypothesis tests on the stability, enabling rigorous experimental comparison
of feature selection algorithms.
Keywords: stability, feature selection},
  comment  = {This paper develops an easy-to-comprehend metric for feature selection stability (same features selected regardless of small data perturbations).  It can also be used improve the choice of  lasso or elastic net lambda (L1 penalty) so that the true features are selected more often, at little or no cost to classification error (synthetic data).  For pure LASSO there is zero false positive feature selection but a decent chance of missing true features (very little cost to classification error); elastic net is much better at stable feature selection, with no missed features, very few false postive features, and zero sacrifice in clasification error (with slighly worse classification error than LASSO on a highly redundant feature set). 

The paper also develops rigorous hypothesis tests for comparing the stability of different feature selection algorithms.

HAS CODE: Matlab, Python and R
                    (sklearn implments some versionm of it for some models:  Saabas15featSelStabilRFE)

HOWEVER
* The choice of lambda doesn't seem to be totally databased for elastic net, although I can imagine a hack that would make it fully automatic, and perhaps not much worse than in Figure 11.
* The really good results were on elastic net, which had slightly worse classification error than LASSO on a redundant dataset where, perhaps, it should have had an advantage (since it can also do ridge).
* How would this work on non-Guassian data?
* How would this work for more complex algorithms -- and how would you search their many more parameters?
* You have to run quite a few bootstraps in order to do the stability estimation:  100 repeated train/testings for 1000 data samples of 100 features, 50 of which are relevant.

INTERESTING
* Elastic net choice of "alpha," the L2 penalty in this paper didn't seem to matter
* On this data, Elastic Net is MUCH better than LASSO at not excluding true features.  
  -  (at least when features are linearly correlated)
  - would make it a good preprocessing featsel step for fancier algorithm?
  - correct feature selection is either perfect or almost perfect at not excluding true features, 
  - and if you select for stability, is almost as good at not selecting bad features as is LASSO (compare with figure 9).  
* Would elastic net then be a good way to ID features for a more complex regression algorithm?
Python, Matlab and R
https://github.com/nogueirs/JMLR2018

Video and Matlab (video proposes linear correlation but this paper is a bit fancier than that):
http://www.cs.man.ac.uk/~nogueirs/publications.html

Demo
http://www.cs.man.ac.uk/~nogueirs/stabilityDemo.html

COMPARE TO:  Meinshausen08pValHiDim, Meinshausen10Stabilityselection},
  file     = {:Nogueira18featSelStbl.pdf:PDF},
  url      = {http://jmlr.org/papers/v18/17-514.html},
}

@Article{Yang12nbrCompFeatSel,
  author   = {Yang, Wei and Wang, Kuanquan and Zuo, Wangmeng},
  title    = {Neighborhood Component Feature Selection for High-Dimensional Data},
  journal  = {Journal of Computers},
  year     = {2012},
  volume   = {7},
  number   = {1},
  issn     = {1796-203X},
  abstract = {Feature selection is of considerable importance in data mining and machine learning, especially for high dimensional data. In this paper, we propose a novel nearest neighbor-based feature weighting algorithm, which learns a feature weighting vector by maximizing the expected leave-one-out classification accuracy with a regularization term. The algorithm makes no parametric assumptions about the distribution of the data and scales naturally to multiclass problems. Experiments conducted on artificial and real data sets demonstrate that the proposed algorithm is largely insensitive to the increase in the number of irrelevant features and performs better than the state-ofthe- art methods in most cases.

Index Terms?Feature selection, feature weighting, nearest neighbor.},
  comment  = {The NCA features selection algorithm in Matlab statistics toolbox, R2016b (classification and regression functions fscnca  and fsrnca)
.

I've marked the paper but need to put some notes in here to make sure the ideas are firm.  Some day....

Idea:
 write what the basic idea is.

Original idea may have come from: Goldberger04nbrhdCompAnal},
  date     = {2012-01-01},
  doi      = {10.4304/jcp.7.1.161-168},
  file     = {Yang12nbrCompFeatSel.pdf:Yang12nbrCompFeatSel.pdf:PDF},
  url      = {http://ojs.academypublisher.com/index.php/jcp/article/view/5076},
  urldate  = {2016-10-29},
}

@Article{Saeys07reviewFeatSelBioInf,
  author    = {Saeys, Yvan and Inza, I{\~n}aki and Larra{\~n}aga, Pedro},
  title     = {A review of feature selection techniques in bioinformatics},
  journal   = {Bioinformatics},
  year      = {2007},
  volume    = {23},
  number    = {19},
  pages     = {2507--2517},
  abstract  = {Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques.In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.Contact: yvan.saeys\@psb.ugent.beSupplementary information: http://bioinformatics.psb.ugent.be/supplementary_data/yvsae/fsreview},
  comment   = {Review: read to get back up to date},
  doi       = {10.1093/bioinformatics/btm344},
  eprint    = {http://bioinformatics.oxfordjournals.org/content/23/19/2507.full.pdf+html},
  file      = {Saeys07reviewFeatSelBioInf.pdf:Saeys07reviewFeatSelBioInf.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2016.06.18},
}

@Article{Guo09gaitMutInfoSubsetSel,
  author    = {Baofeng Guo and Nixon, M.S.},
  title     = {Gait Feature Subset Selection by Mutual Information},
  journal   = {Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on},
  year      = {2009},
  volume    = {39},
  number    = {1},
  pages     = {36--46},
  month     = jan,
  issn      = {1083-4427},
  abstract  = {Feature subset selection is an important preprocessing step for pattern recognition, to discard irrelevant and redundant information, as well as to identify the most important attributes. In this paper, we investigate a computationally efficient solution to select the most important features for gait recognition. The specific technique applied is based on mutual information (MI), which evaluates the statistical dependence between two random variables and has an established relation with the Bayes classification error. Extending our earlier research, we show that a sequential selection method based on MI can provide an effective solution for high-dimensional human gait data. To assess the performance of the approach, experiments are carried out based on a 73-dimensional model-based gait features set and on a 64 by 64 pixels model-free gait symmetry map on the Southampton HiD Gait database. The experimental results confirm the effectiveness of the method, removing about 50\% of the model-based features and 95\% of the symmetry map's pixels without significant loss in recognition capability, which outperforms correlation and analysis-of-variance-based methods.},
  comment   = {selection using real multivariate MI, but only works for categorical predictand. * unlike Peng mRMR uses true joint mutual information, but uses MI conditioned on discrete predictand * not sure how this would work for continuous predictand * anyway it's better than ANOVA * improvement could be to select num. of selected inputs w/ one of the MI permutation tests.},
  doi       = {10.1109/TSMCA.2008.2007977},
  file      = {Guo09gaitMutInfoSubsetSel.pdf:Guo09gaitMutInfoSubsetSel.pdf:PDF},
  keywords  = {Bayes classification error;feature subset selection;gait recognition;mutual information;pattern recognition;random variable;sequential selection method;statistical analysis;Bayes methods;error statistics;feature extraction;image classification;image motion analysis;random processes;statistical analysis;},
  owner     = {scot},
  timestamp = {2011.02.24},
}

@Book{Lu13MultilinSubspaceLearnBook,
  title     = {Multilinear Subspace Learning: Dimensionality Reduction of Multidimensional Data},
  publisher = {CRC press},
  year      = {2013},
  author    = {Lu, Haiping and Plataniotis, Konstantinos N and Venetsanopoulos, Anastasios},
  abstract  = {Features

 Introduces both MSL theories and practical considerations, including multilinear algebra fundamentals, multilinear projections, framework formulation, optimality criterion construction, and implementation tips
 Provides a strong foundation for developing new MSL algorithms and exploring new MSL applications
 Presents pseudocode for algorithms in a unifying format, with MATLAB code available on a supporting website
 Offers examples of real-world applications in video surveillance, biometrics, and object recognition
 Includes numerous figures that clarify and link concepts, enabling readers to easily grasp and visualize the main ideas
 Covers mathematical background, data preprocessing, and software tools in the appendices

Summary

Due to advances in sensor, storage, and networking technologies, data is being generated on a daily basis at an ever-increasing pace in a wide range of applications, including cloud computing, mobile Internet, and medical imaging. This large multidimensional data requires more efficient dimensionality reduction schemes than the traditional techniques. Addressing this need, multilinear subspace learning (MSL) reduces the dimensionality of big data directly from its natural multidimensional representation, a tensor.

Multilinear Subspace Learning: Dimensionality Reduction of Multidimensional Data gives a comprehensive introduction to both theoretical and practical aspects of MSL for the dimensionality reduction of multidimensional data based on tensors. It covers the fundamentals, algorithms, and applications of MSL.

Emphasizing essential concepts and system-level perspectives, the authors provide a foundation for solving many of today?s most interesting and challenging problems in big multidimensional data processing. They trace the history of MSL, detail recent advances, and explore future developments and emerging applications.

The book follows a unifying MSL framework formulation to systematically derive representative MSL algorithms. It describes various applications of the algorithms, along with their pseudocode. Implementation tips help practitioners in further development, evaluation, and application. The book also provides researchers with useful theoretical information on big multidimensional data in machine learning and pattern recognition. MATLAB? source code, data, and other materials are available at www.comp.hkbu.edu.hk/~haiping/MSL.html
Share this Title},
  comment   = {Practical book (with Matlab, and expensive ebook) explaining tensor learning and some basic stuff. Is a companion to Lu09UncorrMultiLinPCA.},
  owner     = {sotterson},
  timestamp = {2015.02.09},
  url       = {http://www.crcpress.com/product/isbn/9781439857243#googlePreviewContainer},
}

@Article{Razavi11newFormNN,
  author    = {Razavi, S. and Tolson, B.A.},
  title     = {A New Formulation for Feedforward Neural Networks},
  journal   = {Neural Networks, IEEE Transactions on},
  year      = {2011},
  volume    = {22},
  number    = {10},
  pages     = {1588--1598},
  month     = oct,
  issn      = {1045-9227},
  abstract  = {Feedforward neural network is one of the most commonly used function approximation techniques and has been applied to a wide variety of problems arising from various disciplines. However, neural networks are black-box models having multiple challenges/difficulties associated with training and generalization. This paper initially looks into the internal behavior of neural networks and develops a detailed interpretation of the neural network functional geometry. Based on this geometrical interpretation, a new set of variables describing neural networks is proposed as a more effective and geometrically interpretable alternative to the traditional set of network weights and biases. Then, this paper develops a new formulation for neural networks with respect to the newly defined variables; this reformulated neural network (ReNN) is equivalent to the common feedforward neural network but has a less complex error response surface. To demonstrate the learning ability of ReNN, in this paper, two training methods involving a derivative-based (a variation of backpropagation) and a derivative-free optimization algorithms are employed. Moreover, a new measure of regularization on the basis of the developed geometrical interpretation is proposed to evaluate and improve the generalization ability of neural networks. The value of the proposed geometrical interpretation, the ReNN approach, and the new regularization measure are demonstrated across multiple test problems. Results show that ReNN can be trained more effectively and efficiently compared to the common neural networks and the proposed regularization measure is an effective indicator of how a network would perform in terms of generalization.},
  comment   = {NN equation manipulations provide new regularization criteria and improve training (it seems).

Has Matlab. Basic ideas:
* Rewrite standard signmoid NN equations so that hidden node response slope is an explicit optimization parameter, instead of being implied by the usual NN parameters
* This improves training, apparently because derivative training algorithms then produce more shallow slopes, leading to less generalization error
* New regularization metric is based on average squared slope instead of on sparseness (as is traditional) or upon Baysian regularization.


Results:
* NN's trained w/ new params usually converge faster
* biggest improvement w/ deriv. based method, even though paper says that many studies show that non-deriv methods work best w/ traditional ANN forumulation. Note that, for non-deriv, they use DDS. One of the authors, Tolson, has a paper on this, so it seems to be his baby.

* Faster training methods like Levenberg-Marquart are aren't implemented, although the algorithm is supposed to be compatible with them.
* New slope-centric regularization method (computed on the training set) is clearly better-associated with test set generalization success than is the standard sparsity measure (as long as the training performance is fairly good).
* Speculate that it could guide training (they don't do that here)

* Speculate that it could help Bayesian regulation (don't do that either)},
  doi       = {10.1109/TNN.2011.2163169},
  file      = {Razavi11newFormNN.pdf:Razavi11newFormNN.pdf:PDF},
  keywords  = {ReNN approach;black box model;derivative free optimization algorithm;error response surface;feedforward neural network;function approximation techniques;generalization ability;geometrical interpretation;learning ability;neural network functional geometry;reformulated neural network;training method;feedforward neural nets;function approximation;generalisation (artificial intelligence);learning (artificial intelligence);optimisation;},
  owner     = {sotterson},
  timestamp = {2012.04.25},
}

@InCollection{Lucinska12specClustKNN,
  author    = {Luci{\'n}ska, Ma{\l}gorzata and Wierzcho{\'n}, S{\l}awomir T},
  title     = {Spectral clustering based on k-nearest neighbor graph},
  booktitle = {Computer Information Systems and Industrial Management},
  publisher = {Springer},
  year      = {2012},
  pages     = {254--265},
  abstract  = {Finding clusters in data is a challenging task when the clusters differ widely in shapes, sizes, and densities. We present a novel spectral algorithm Speclus with a similarity measure based on modified mutual nearest neighbor graph. The resulting affinity matrix reflex the true structure of data. Its eigenvectors, that do not change their sign, are used for clustering data. The algorithm requires only one parameter ? a number of nearest neighbors, which can be quite easily established. Its performance on both artificial and real data sets is competitive to other solutions.

Keywords: spectral clustering, nearest neighbor graph, signless Laplacian},
  comment   = {Uses some kind of KNN adjacency matrix in spectral clustering (I think) and also uses unsigned Laplacian. Good as a preprocessing step for clustered local quantile regression?},
  doi       = {10.1007/978-3-642-33260-9_22},
  file      = {Lucinska12specClustKNN.pdf:Lucinska12specClustKNN.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.04.03},
}

@Book{Dunning14practMachLrnAnom,
  title     = {Practical Machine Learning: A New Look at Anomaly Detection},
  publisher = {O'Reilly Media, Inc, USA},
  year      = {2014},
  author    = {Ted Dunning and Ellen Friedman},
  isbn      = {1491911603},
  abstract  = {Finding Data Anomalies You Didn't Know to Look For

Anomaly detection is the detective work of machine learning: finding the unusual, catching the fraud, discovering strange activity in large and complex datasets. But, unlike Sherlock Holmes, you may not know what the puzzle is, much less what ?suspects? you?re looking for. This O?Reilly report uses practical examples to explain how the underlying concepts of anomaly detection work.

From banking security to natural sciences, medicine, and marketing, anomaly detection has many useful applications in this age of big data. And the search for anomalies will intensify once the Internet of Things spawns even more new types of data. The concepts described in this report will help you tackle anomaly detection in your own project.

* Use probabilistic models to predict what?s normal and contrast that to what you observe
* Set an adaptive threshold to determine which data falls outside of the normal range, using the t-digest algorithm
* Establish normal fluctuations in complex systems and signals (such as an EKG) with a more adaptive probablistic model
* Use historical data to discover anomalies in sporadic event streams, such as web traffic
* Learn how to use deviations in expected behavior to trigger fraud alerts},
  comment   = {Available as pdf and ebook as well as paper

Recommended here: https://www.mapr.com/blog/n-novelty-detection-and-news-few-my-favorite-data-science-things},
  date      = {2014-08-22},
  ean       = {9781491911600},
  pagetotal = {66 Seiten},
  url       = {http://www.ebook.de/de/product/22664248/ted_dunning_ellen_m_d_friedman_practical_machine_learning_a_new_look_at_anomaly_detection.html},
}

@Article{GomezHerrero15cplDynTSens,
  author    = {G{\'o}mez-Herrero, Germ{\'a}n and Wu, Wei and Rutanen, Kalle and Soriano, Miguel C and Pipa, Gordon and Vicente, Raul},
  title     = {Assessing coupling dynamics from an ensemble of time series},
  journal   = {Entropy},
  year      = {2015},
  volume    = {17},
  number    = {4},
  pages     = {1958--1970},
  abstract  = {Finding interdependency relations between time series provides valuable knowledge about the processes that generated the signals. Information theory sets a natural framework for important classes of statistical dependencies. However, a reliable estimation from information-theoretic functionals is hampered when the dependency to be assessed is brief or evolves in time. Here, we show that these limitations can be partly alleviated when we have access to an ensemble of independent repetitions of the time series. In particular, we gear a data-efficient estimator of probability densities to make use of the full structure of trial-based measures. By doing so, we can obtain time-resolved estimates for a family of entropy combinations (including mutual information, transfer entropy and their conditional counterparts), which are more accurate than the simple average of individual estimates over trials. We show with simulated and real data generated by coupled electronic circuits that the proposed approach allows one to recover the time-resolved dynamics of the coupling between different subsystems.
Keywords: entropy; transfer entropy; estimator; ensemble; trial; time series},
  comment   = {Another paper showing derivations of conditional mutual information feature selection.  Has derivation of KSG (Kraskov) MI equations.  This one was also recommended as a refrence to the MI change stopping point.by Lizier.

maybe an alternative to Frenzel07partMutInfo},
  file      = {GomezHerrero15cplDynTSens.pdf:GomezHerrero15cplDynTSens.pdf:PDF},
  publisher = {Multidisciplinary Digital Publishing Institute},
  url       = {http://www.mdpi.com/1099-4300/17/4/1958/htm},
}

@Article{George11estJohnsonDistBnd,
  author   = {George, Florence and Ramachandran, KM},
  title    = {Estimation of Parameters of Johnson?s System of Distributions},
  journal  = {Journal of Modern Applied Statistical Methods},
  year     = {2011},
  volume   = {10},
  number   = {2},
  pages    = {9},
  abstract = {Fitting distributions to data has a long history and many different procedures have been advocated.
Although models like normal, log-normal and gamma lead to a wide variety of distribution shapes, they
do not provide the degree of generality that is frequently desirable (Hahn & Shapiro, 1967). To formally
represent a set of data by an empirical distribution, Johnson (1949) derived a system of curves with the
flexibility to cover a wide variety of shapes. Methods available to estimate the parameters of the Johnson
distribution are discussed, and a new approach to estimate the four parameters of the Johnson family is
proposed. The estimate makes use of both the maximum likelihood procedure and least square theory.
The new MLE-Least Square approach is compared with other two commonly used methods. A simulation
study shows that the MLE-Least square approach provides better results for S B , SU and S L families.



Key words: Johnson distribution, unbouded, bounded, lognormal, estimation.},
  comment  = {An MLE-Least Square approach to estimating the 4 parameters of a Johnson distribution, which can be bounded. A possible alternative to Cauchy (Cook10distFromQuant), Alpha Stable (Veillette10AlphaStableDistMatlab), or censoring approaches? Good for wind power error, for which Cauchy is a good fit (Hodge11frcstErrDistTime)? Has Matlab.

Johnson is unimodal, though (see other paper) so can it handle probability masses at edges?
MATLAB:
review:
 http://de.mathworks.com/help/stats/generating-data-using-flexible-families-of-distributions.html\#br5k833-2
johnsrnd() generates random numbers given a set of quantiles. Can also return the coeffs. Not clear if it allows you to declare that it's bounded; instead it seems to guess.
 http://de.mathworks.com/help/stats/johnsrnd.html
* another kind of distribution that cna be estimated from quantiles: Keelin11QuantParamDist
Matlab Central Johnson toolbox, looks more complete and flexible than Matlab's stats toolbox johnsrnd():
http://www.mathworks.com/matlabcentral/fileexchange/46123-johnson-curve-toolbox
* copula-like multivariate distribution estimation using Johnson (handy analytical expressions too: Xiao14genCorrRVjohnson
* parameters estimated from quantiles, then some kind of stoch. opt.: Torrez93JohnsonDistFitNeuron},
  file     = {George11estJohnsonDistBnd.pdf:George11estJohnsonDistBnd.pdf:PDF},
  url      = {http://digitalcommons.wayne.edu/cgi/viewcontent.cgi?article=1326&context=jmasm},
}

@Article{Rowe07fMRIangularRgrsn,
  author    = {Daniel B. Rowe and Christopher P. Meller and Raymond G. Hoffmann},
  title     = {Characterizing phase-only fMRI data with an angular regression model},
  year      = {2007},
  volume    = {161},
  number    = {2},
  pages     = {331--341},
  issn      = {0165-0270},
  doi       = {DOI: 10.1016/j.jneumeth.2006.10.024},
  url       = {http://www.sciencedirect.com/science/article/B6T04-4MHPHD6-2/2/245b7f3951fba0e3408a70c2403a97ba},
  abstract  = {FMRI voxel time series are complex-valued with real and imaginary parts that are usually converted to magnitude-phase polar coordinates. Magnitude-only data models that discard the phase portion of the data have dominated fMRI analysis. However, when such analyses are performed, the data that is discarded may contain valuable biologic information that is not in the magnitude data. This biologic information from BOLD fMRI data may be vascular [Menon RS. Postacquisition suppression of large-vessel BOLD signals in high-resolution fMRI. Magn Reson Med 2002;47(1):1?9] or neuronal [Bodurka J, Jesmanowicz A, Hyde JS, Xu H, Estowski L, Li S-J. Current-induced magnetic resonance phase imaging. J Magn Reson 1999;137(1):265?71] in origin. When phase-only time series that discard the magnitude portion of the data have been analyzed, ordinary least squares (OLS) regression has been the technique of choice. However, OLS models may fit poorly when phase-wrap or low signal-to-noise ratio (SNR) is present. We have explored alternatives to the OLS model which will account for the angular response of the phase while also allowing us the flexibility to develop similar hypothesis tests. We adopt an angular regression model by Fisher and Lee [Fisher NI, Lee AJ. Regression models for an angular response. Biometrics 1992;48:665?77] for our analysis and show its improvement over the OLS model at low SNR in terms of both parameter estimation and inferences. We found an improvement in parameter estimation along with modeling for the Fisher and Lee method in simulated data while detailing potential benefits when used with experimentally acquired data. Finally, we look at a map of the statistics testing the association of the observed voxel phase time course},
  file      = {Rowe07fMRIangularRgrsn.pdf:Rowe07fMRIangularRgrsn.pdf:PDF;Rowe07fMRIangularRgrsn.pdf:Rowe07fMRIangularRgrsn.pdf:PDF},
  journal   = {Journal of Neuroscience Methods},
  keywords  = {Angular regression},
  owner     = {sotterson},
  timestamp = {2009.05.08},
}

@Conference{Kurt11frcstNewOffshore,
  author    = {Melih Kurt and Jan Dobschinski and Bernhard Lange and Arne Wessel},
  title     = {Development of wind power forecast quality for new offshore wind farms},
  booktitle = {EWEA Offshore},
  year      = {2011},
  month     = nov,
  abstract  = {Focus of the study is to analyze development of the offshore wind power forecast quality from beginning of operation up to one year of operational experience. This study represents a case study using data of the first German offshore wind farm ?alpha ventus?. The work was carried out with measured data from meteorological measurement mast Fino1, measured power from ?alpha ventus? and numerical weather prediction (NWP) from the German Weather Service. This study facilitates to decide the length of needed time series and selection of forecast method to get a reliable wind power forecast on a weekly time axis. In this work, the weekly development of wind power forecast quality for day-ahead and shortest-term wind power forecast via different models is presented. The models are physical model; physical model extended with a statistical correction (MOS) and artificial neural network (ANN) as a pure statistical model. And also a feasibility study is done for shortest-term wind power forecasting, which improves forecast quality of physical models with taking the measured power into account.},
  comment   = {Melih's EWEA poster on wake-corrected NWP w/ static power curve vs. ANN during farm burn-in.},
  file      = {Kurt11frcstNewOffshore.pdf:Kurt11frcstNewOffshore.pdf:PDF},
  groups    = {Read},
  location  = {Amsterdam, The Netherlands},
  owner     = {sotterson},
  timestamp = {2011.11.25},
}

@Article{Kim08estMarkovRegimeEndogSw,
  author    = {Kim, Chang-Jin and Piger, Jeremy and Startz, Richard},
  title     = {Estimation of {Markov} regime-switching regression models with endogenous switching},
  year      = {2008},
  volume    = {143},
  number    = {2},
  month     = apr,
  pages     = {263--273},
  url       = {http://ideas.repec.org/a/eee/econom/v143y2008i2p263-273.html},
  abstract  = {Following Hamilton [1989. A new approach to the economic analysis of nonstationary time series and the business cycle. Econometrica 57, 357?384], estimation of Markov regime-switching regressions typically relies on the assumption that the latent state variable controlling regime change is exogenous. We relax this assumption and develop a parsimonious model of endogenous Markov regime-switching. Inference via maximum likelihood estimation is possible with relatively minor modifications to existing recursive filters. The model nests the exogenous switching model, yielding straightforward tests for endogeneity. In Monte Carlo experiments, maximum likelihood estimates of the endogenous switching model parameters were quite accurate, even in the presence of certain model misspecifications. As an application, we extend the volatility feedback model of equity returns given in Turner et al. [1989. A Markov model of heteroskedasticity, risk, and learning in the stock market. Journal of Financial Economics 25, 3?22] to allow for endogenous switching. r 2007 Elsevier B.V. All rights reserved.},
  file      = {Kim08estMarkovRegimeEndogSw.pdf:Kim08estMarkovRegimeEndogSw.pdf:PDF;Kim08estMarkovRegimeEndogSw.pdf:Kim08estMarkovRegimeEndogSw.pdf:PDF},
  journal   = {Journal of Econometrics},
  owner     = {sotterson},
  timestamp = {2008.12.12},
}

@Article{Schmidt09lawFromDat,
  author    = {Michael Schmidt and Hod Lipson},
  title     = {Distilling Free-Form Natural Laws from Experimental Data},
  journal   = {Science},
  year      = {2009},
  volume    = {324},
  number    = {5923,},
  pages     = {81--85},
  month     = apr,
  abstract  = {For centuries, scientists have attempted to identify and document analytical laws that underlie physical phenomena in nature. Despite the prevalence of computing power, the process of finding natural laws and their corresponding equations has resisted automation. A key challenge to finding analytic relations automatically is defining algorithmically what makes a correlation in observed data important and insightful. We propose a principle for the identification of nontriviality. We demonstrated this approach by automatically searching motion-tracking data captured from various physical systems, ranging from simple harmonic oscillators to chaotic double-pendula. Without any prior knowledge about physics, kinematics, or geometry, the algorithm discovered Hamiltonians, Lagrangians, and other laws of geometric and momentum conservation. The discovery rate accelerated as laws found for simpler systems were used to bootstrap explanations for more complex systems, gradually uncovering the "alphabet" used to describe those systems.},
  comment   = {Extracting analytical expression for dynamical system from raw data * useful for regime detection, causal discovery, feature selection, or MOS? * I think I saw something like this a few years ago but maybe it didn't capture dynamics, like this one does},
  doi       = {10.1126/science.1165893},
  file      = {Schmidt09lawFromDat.pdf:Schmidt09lawFromDat.pdf:PDF;Schmidt09lawFromDat.pdf:Schmidt09lawFromDat.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.04.03},
}

@TECHREPORT{Zhao14neISOrobustUC,
  Author                   = {Zhao, J and Litvinov, E and Zheng, T and Rajan, D and Bulaevskaya, V and Lamont, A and Min, L},
  Title                    = {A Comprehensive Evaluation of Robust Unit Commitment},
  Institution              = {Lawrence Livermore National Laboratory},
  Year                     = {2014},
  Type                     = {Tech. Report},
  Number                   = {LLNL-TR-650700},
  Abstract                 = {For day-ahead scheduling of electric grid operations, system operators forecast
the system condition for the following day, and determine which power generators will
be used through a process called unit commitment (UC). New England?s policymakers
are seeking to increase the amount of renewable resources available to meet
consumers? energy needs. These resources offer low-emission energy, but their
variability poses a unique challenge for the reliable scheduling and operation of the
region?s power system. The existing deterministic UC model is not equipped with any
effective uncertainty management strategy. As a result, the cost of the real-time system
operation can be increased, and the reliability of the system can be jeopardized when
the amount of stochastic elements (variability) associated with renewable resources is
significant. For example, if the amount of wind energy is heavily over-forecasted, the
uncommitted units with long lead time many not be able to start up quickly enough to
make up for the difference between forecast and actual wind energy production. ISO
New England (NE) enlisted Lawrence Livermore National Laboratory (LLNL)?s help in
determining whether a new methodology, known as robust UC (Bertsimas et.al. (2013)),
would improve system reliability while keeping the operation cost relatively low in the
presence of renewable variability. The robust approach can produce commitment
schedules that accommodate a forecasted generation range of variable resources as
shown in Figure 1, rather than only the forecasted level as in the deterministic
approach.
Figure 1: The black line shows expected wind generation level over a 24-hour
period. The blue line represents the range of uncertain wind generation the UC model
must accommodate between projected and actual wind output.
Over the course of one and half year, ISO NE used LLNL?s high performance
computing capabilities to run a large number of simulation to analyze and optimize
robust UC solutions that would mitigate the impact of variability and uncertainty on the
power system. During the first phase of the collaboration, because of the number of
inputs, including wind penetration levels, weather conditions, and conservatism levels
(or the forecast range), up to 2,500 robust UC and 1,250 deterministic UC
configurations were analyzed ? with each configuration taking an average of 30 minutes
to compute. By parallelizing the software used to solve the problems, the team reduced
Wind
output
(MW)
Time (hour)
Lawrence Livermore National Laboratory iv
the computation time from 1800 hours to 90 minutes. During the second phase of the
collaboration, the team ran simulations and identified the impact of various factors ?
such as the conservatism levels on the economic and operational benefits of robust UC
over deterministic UC. The simulations used Monte Carlo sampling based on historical
load and wind generation data combined with a Livermore-developed statistical model.
One thousand dispatch problems, each taking 15 seconds, must be answered for every
UC configuration. By efficiently parallelizing the dispatch simulation in batches, the total
simulation time was reduced from 1.8 years to 4.8 hours.
Based on the simulation results, the optimal conservatism level that led to the
best trade-off between the system cost and reliability under the robust approach was
identified. The robust approach demonstrated the economic and operational
advantages over the deterministic approach in terms of lowering the total costs and
increasing the system reliability. From the generators? perspective, compared to the
deterministic approach, the robust approach reduced the profit largely due to the
depressed energy prices under the latter. The robust approach also exhibited stable
performance when large forecast error occurred. The advantages of the robust
approach are magnified when there are more renewable resources integrated into the
system.
Processing thousands of simulations simultaneously increased the productivity,
provided ISO NE with a more comprehensive evaluation of the robust commitment
schedule, and allowed ISO NE to assess the effectiveness of the robust unit
commitment approach under a broad range of energy use, wind and power generation
scenarios.},
  File                     = {Zhao14neISOrobustUC.pdf:Zhao14neISOrobustUC.pdf:PDF},
  Owner                    = {sotterson},
  Timestamp                = {2014.12.08},
  URL                      = {https://e-reports-ext.llnl.gov/pdf/770804.pdf}
}

@Article{Bernards18,
  author   = {R. {Bernards} and J. {Morren} and H. {Slootweg}},
  title    = {Development and Implementation of Statistical Models for Estimating Diversified Adoption of Energy Transition Technologies},
  journal  = {IEEE Transactions on Sustainable Energy},
  year     = {2018},
  volume   = {9},
  number   = {4},
  pages    = {1540--1554},
  month    = oct,
  issn     = {1949-3029},
  abstract = {For efficient network investments, insight in the expected spatial spread of new load and generation units is of prime importance. This paper presents and applies a method to determine key factors for adoption of photovoltaics, electric vehicles, and heat pumps. Using a logistic regression analysis, the impact of geographical and socio-economic factors on adoption probabilities of these new energy technologies is quantified. Income level, average age, and household composition are shown to be important factors. Additionally, for photovoltaics, peer effects were also shown to significantly influence the likelihood of adoption. The implementation of the developed models and the achievable improvement in prediction accuracy is demonstrated by application to a scenario study based on historical data. The models can be incorporated in future energy scenarios to provide a probabilistic spatial forecast of future penetration levels of the mentioned technologies and identify key areas of interest.},
  comment  = {Multi DER probabilistic forecast with peer effects for PV.},
  doi      = {10.1109/TSTE.2018.2794579},
  keywords = {electric vehicles, heat pumps, investment, photovoltaic power systems, power engineering computing, power system planning, probability, regression analysis, social aspects of automation, socio-economic effects, statistical models, energy transition technologies, expected spatial spread, load, generation units, photovoltaics, electric vehicles, heat pumps, logistic regression analysis, socio-economic factors, adoption probabilities, energy technologies, income level, household composition, peer effects, developed models, prediction accuracy, probabilistic spatial forecast, network investments, penetration levels, Predictive models, Forecast uncertainty, Probabilistic logic, Statistical learning, Power system planning, Forecast uncertainty, power system planning, statistical learning, technology adoption},
}

@Article{Bickel08choiceOfMsubsampBtstrp,
  author    = {Peter J. Bickel and Anat Sakov},
  title     = {On the choice of m in the m out of n bootstrap and confidence bounds for extrema},
  journal   = {Statistica Sinica},
  year      = {2008},
  volume    = {18},
  pages     = {967--985},
  abstract  = {For i.i.d. samples of size , the ordinary bootstrap (Efron (1979)) is known to be consistent in many situations, but it may fail in important examples (Bickel, Gotze and van Zwet (1997)). Using bootstrap samples of size , where and , typically resolves the problem (Bickel et al. (1997), Politis and Romano (1994)). The choice of is a key issue. In this paper, we consider an adaptive rule, proposed by Bickel, Gotze, and van Zwet (personal communication), to pick . We give general sufficient conditions for first order validity of the rule, and consider its higher order behavior when the ordinary bootstrap fails, and when it works. We then examine the behavior of the rule in the context of setting confidence bounds on high percentiles, such as the asymptotic expected maximum.},
  comment   = {Another way to pick m in m outof n subsampling bootstrap? 

I don't understand if this is appropriate for subsampling (see my notes on p. 969 of paper) 

Mentioned in Simar10mofnBtstrp but doesn't seem to be used. 
* could possibly use for: Geyer06subSampBootStrap 
* seems to be an alternative to: Politis01asympSubsamp 
* Politis later used this appoach (with KS distance) in Jach11subSampInfTS so maybe this one is better than Politis01asympSubsamp 
* here, they seem most focused on finding confidence intervals for the distribution maximum value estimate 
* another example of when can't bootstrap w/ replacement is K-NN MI, which Francois06permTestMutInf has found is biased by this (I have found the same thing in my ramp experiments). 
* test subsample sizes are logarithmic, like Geyer06... 
* for exponential increase, they used q=0.75, but choice wasn't sensitive 

Assumes known rate but there's a workaround: Use Politis method to determine convergence rate before using this paper's method to determine the subsample size. This is OK since, for the purposes of rate estimation, Politis uses several subsample sizes; it doesn't chose a particular one.},
  file      = {Bickel08choiceOfMsubsampBtstrp.pdf:Bickel08choiceOfMsubsampBtstrp.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  timestamp = {2011.06.28},
  url       = {http://www3.stat.sinica.edu.tw/statistica/j18n3/j18n38/j18n38.html},
}

@Article{Pinson09ensembProbFrcst,
  author    = {Pierre Pinson and Henrik Madsen},
  title     = {Ensemble-based Probabilistic Forecasting at Horns Rev},
  journal   = {Wind Energy},
  year      = {2009},
  volume    = {12},
  number    = {2},
  pages     = {137--155},
  abstract  = {For management and trading purposes, information on short-term wind generation (from a few hours to a few days ahead) is crucial at large offshore wind farms, since they concentrate a large capacity at a single location. The most complete information that can be provided today consists of probabilistic forecasts, the resolution of which may be maximized by using meteorological ensemble predictions as input. The paper concentrates on the test case of the Horns Rev wind farm over a period of approximately 1 year, in order to describe, apply and discuss a complete ensemble-based probabilistic forecasting methodology. In a first stage, ensemble forecasts of meteorological variables are converted to power through a suitable power curve model. This model employs local polynomial regression, and is adaptively estimated with an orthogonal fitting method. The obtained ensemble forecasts of wind power are then converted into predictive distributions with an original adaptive kernel dressing method. The shape of the kernels is driven by a mean-variance model, the parameters of which are recursively estimated in order to maximize the overall skill of obtained predictive distributions. Such a methodology has the benefit of yielding predictive distributions that are of increased reliability (in a probabilistic sense) in comparison with the raw ensemble forecasts, at the same time taking advantage of their high resolution.},
  comment   = {NWP ensembles --> wind power quantiles. Malte said the orthogonal interpolation used here might be good for either point or ensemble forecasts.},
  doi       = {10.1002/we.309},
  file      = {Pinson09ensembProbFrcst.pdf:Pinson09ensembProbFrcst.pdf:PDF},
  groups    = {DOE-PNL09, Ensemble, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2009.03.04},
}

@Article{Mittelmeier17powStdRatioWake,
  author   = {Mittelmeier, N. and Allin, J. and Blodau, T. and Trabucchi, D. and Steinfeld, G. and Rott, A. and K{\"u}hn, M.},
  title    = {An analysis of offshore wind farm SCADA measurements to identify key parameters influencing the magnitude of wake effects},
  journal  = {Wind Energy Science},
  year     = {2017},
  volume   = {2},
  number   = {2},
  pages    = {477--490},
  abstract = {For offshore wind farms, wake effects are among the largest sources of losses in energy production. At the same time, wake modelling is still associated with very high uncertainties. Therefore current research focusses on improving wake model predictions. It is known that atmospheric conditions, especially atmospheric stability, crucially influence the magnitude of those wake effects. The classification of atmospheric stability is usually based on measurements from met masts, buoys or lidar (light detection and ranging). In offshore conditions these measurements are expensive and scarce. However, every wind farm permanently produces SCADA (supervisory control and data acquisition) measurements. The objective of this study is to establish a classification for the magnitude of wake effects based on SCADA data. This delivers a basis to fit engineering wake models better to the ambient conditions in an offshore wind farm. The method is established with data from two offshore wind farms which each have a met mast nearby. A correlation is established between the stability classification from the met mast and signals within the SCADA data from the wind farm. The significance of these new signals on power production is demonstrated with data from two wind farms with met mast and long-range lidar measurements. Additionally, the method is validated with data from another wind farm without a met mast. The proposed signal consists of a good correlation between the standard deviation of active power divided by the average power of wind turbines in free flow with the ambient turbulence intensity (TI) when the wind turbines were operating in partial load. It allows us to distinguish between conditions with different magnitudes of wake effects. The proposed signal is very sensitive to increased turbulence induced by neighbouring turbines and wind farms, even at a distance of more than 38 rotor diameters.},
  comment  = {std(power)/mean(power) is related to wake effect.  Maybe it's a good input into a single turbine neural net power curve.  For ModernWindABS, could be a way to get around our inability to measure wake effects.},
  doi      = {10.5194/wes-2-477-2017},
  file     = {:Mittelmeier17powStdRatioWake.pdf:PDF},
  url      = {https://www.wind-energ-sci.net/2/477/2017/},
}

@Article{Nielsen06quantRegr,
  author    = {Henrik Aalborg Nielsen and Henrik Madsen and Torben Skov Nielsen},
  title     = {Using quantile regression to extend an existing wind power forecasting system with probabilistic forecasts},
  journal   = {Wind Energy},
  year      = {2006},
  volume    = {9},
  number    = {1-2},
  pages     = {95--108},
  abstract  = {For operational planning it is important to provide information about the situation-dependent uncertainty of a wind power forecast. Factors which influence the uncertainty of a wind power forecast include the predictability of the actual meteorological situation, the level of the predicted wind speed (due to the non-linearity of the power curve) and the forecast horizon. With respect to the predictability of the actual meteorological situation a number of explanatory variables are considered, some inspired by the literature.The article contains an overview of related work within the field. An existing wind power forecasting system (Zephyr/WPPT) is considered and it is shown how analysis of the forecast error can be used to build a model of the quantiles of the forecast error. Only explanatory variables or indices which are predictable are considered,whereby the model obtained can be used for providing situation-dependent information regarding the uncertainty. Finally, the article contains directions enabling the reader to replicate the methods and thereby extend other forecast systems with situation-dependent information on uncertainty.},
  comment   = {A "summed" effect spline QR model (periodic and natural splines), considering typical NWP features + a time-lagged ensemble "risk" index."(ad-hoc featsel). Has R code -- the algorithm used in Mendes11statWindFrcst.

MODEL
* Additive effects model
- each quantile regressor is the sum of marginal (one input variable) functions
 --- makes it computationally tractable (says local regression fails for higher dims)
 --- (but consider high dim tricks, like in Taylor12locRgrsnStrat)
 --- allows for human interpretablity (if slope of influnce graph is zero, then useless)
* functions are splines, w/ restrictions on edges and on sums, so problem is unique, somehow
- dir is periodic spline, integrates to zero (why?)
- others are natural splines, w/ zero-valued knots to handle low limit
* seems to be forecasting the error instead of the total power
- seems to cause out-of-bound problems
- tentatively recommends local regression like in Bremnes04/06 instead
---- local regression reduces out of boundeness, but increases curse of dimensionality for multi-inputs
---- allows bounded outputs
----- but skepticism b/c of curse of dim. for local nhbds. Solution in Taylor12locRgrsnStrat ?
* crossover not handled, and it happens a little

INPUTS CONSIDERED
* forecasted power
* horizon
* density
* friction velocity
* spd (10m)
* dir (10m)
* Pierre Pinson "risk index"
- time-lagged ensemble idea: sumSqDiff w/ prev NWP run

FEATURE SELECTION
* a little handwavy
* not based on CRPS
* sharpness (inter-quantile range)
* amount the feature makes the marginal function move
* Selected features seem to be: forecasted power, horizon, wind dir

DATA
* 10 0.5 MW turbines (5 MW total)
* horizons tested: 18036 hours
* HIRLAM model runs
* Only a year of data, split into discrete test and train
* but NWP model change in the middle of it seems to screw things up},
  doi       = {10.1002/we.180},
  file      = {Nielsen06quantRegr.pdf:Nielsen06quantRegr.pdf:PDF},
  groups    = {Read, DOE-PNL09, PointDerived, ErrDistProps, doReadWPV_1, doReadNonWPV_1},
  keywords  = {wind power forecasting, uncertainty, quantile regression, additive model},
  owner     = {sotterson},
  timestamp = {2009.03.03},
}

@InProceedings{Fan12spinRsrvVaR,
  author    = {Wenshuai Fan and Renjun Zhou and Hao Tang and Xiaohong Ran},
  title     = {Conditional risk constraint model of spinning reserve in wind power integrated system},
  booktitle = {Innovative Smart Grid Technologies - Asia (ISGT Asia), 2012 IEEE},
  year      = {2012},
  pages     = {1--6},
  abstract  = {For the influence from the randomness of wind power output to the system operation, a model, which achieves the spinning reserve in wind power integrated system, is studied. With the modeling idea about combining power generation plan and reserve plan, minimizing the total cost of power production and spinning reserve as an objective, conditional value-at-risk is presented in this paper. Under regarding the conditional value at risk of up/down spinning reserve as security constraints, the impact from the randomness of wind power to the system can be quantitated effectively. The CVaR value of up/down spinning reserve in the system is calculated by Monte Carlo simulation and analysis method, a measurement analysis aimed at economic cost and the reserve capacity under different confidence levels and wind power output is carried out. The simulation results verify the feasibility of the proposed method and provide a new reference model for the calculation of spinning reserve capacity in wind power integrated system.},
  comment   = {Spinning reserves choice by value at risk},
  doi       = {10.1109/ISGT-Asia.2012.6303350},
  file      = {Fan12spinRsrvVaR.pdf:Fan12spinRsrvVaR.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.12.04},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6303350},
}

@InProceedings{Bentzien14quantScrDecomp,
  author    = {Bentzien, Sabrina and Friederichs, Petra},
  title     = {The quantile score and its decomposition},
  booktitle = {EGU General Assembly Conference Abstracts},
  year      = {2014},
  volume    = {16},
  pages     = {15691},
  abstract  = {Forecast verification for probabilistic weather and climate predictions gain more and more importance due to
the increasing number of ensemble prediction systems. The predictive performance of probabilistic forecasts is
generally assessed using proper score functions, which are applied to a set of forecast-observation pairs. The
propriety of a score guarantees honesty and prevents hedging. A variety of proper scores exist for different types
of probabilistic forecasts. Moreover, proper scoring functions can be decomposed into the three parts reliability,
resolution, and uncertainty, which describe main characteristics of a forecasting scheme. This decomposition is
well known for the Brier score and the continuous ranked probability score.

This study expands the pool of verification methods for probabilistic forecasts by a decomposition of the
quantile score (QS). Quantiles are suitable probabilistic measures especially for extreme forecast events, since
they do not depend on an apriori defined threshold. The QS is a weighted absolute error between quantile forecasts
and observations. We derive a decomposition of the QS in reliability, resolution, and uncertainty, and give a
brief description of potential biases. A quantile reliability plot is presented. The quantile verification within
this framework is illustrated on precipitation forecasts derived from the mesoscale ensemble prediction system
COSMO-DE-EPS of the German Meteorological Service.

(From: http://adsabs.harvard.edu/abs/2014EGUGA..1615691B#)},
  comment   = {Poster showoing that quantile score is the integral of the check loss function.  Can be estimated as the average of the check loss functions  over a sample.

Can be decomposed into reliability and resolution, (as can CRPS, I guess)

Quantile reliability plots (like DWD does).

See also Bentzien13qntScrDecomp

I think that Malte wrote Eweline's QuantileScoreDecom.m based on this.},
  file      = {Bentzien14quantScrDecomp.pdf:Bentzien14quantScrDecomp.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.03.15},
  url       = {http://www2.meteo.uni-bonn.de/staff/wahl/files/bentzien_poster_egu2014_qs.pdf},
}

@InBook{Aler15machLrnDAsolarFrcstNWP,
  pages     = {269--278},
  title     = {A Study of Machine Learning Techniques for Daily Solar Energy Forecasting Using Numerical Weather Models},
  publisher = {Springer International Publishing},
  year      = {2015},
  author    = {Aler, Ricardo and Mart{\'i}n, Ricardo and Valls, Jos{\'e} M. and Galv{\'a}n, In{\'e}s M.},
  editor    = {Camacho, David and Braubach, Lars and Venticinque, Salvatore and Badica, Costin},
  address   = {Cham},
  isbn      = {978-3-319-10422-5},
  abstract  = {Forecasting solar energy is becoming an important issue in the context of renewable energy sources and Machine Learning Algorithms play an important rule in this field. The prediction of solar energy can be addressed as a time series prediction problem using historical data. Also, solar energy forecasting can be derived from numerical weather prediction models (NWP). Our interest is focused on the latter approach.We focus on the problem of predicting solar energy from NWP computed from GEFS, the Global Ensemble Forecast System, which predicts meteorological variables for points in a grid. In this context, it can be useful to know how prediction accuracy improves depending on the number of grid nodes used as input for the machine learning techniques. However, using the variables from a large number of grid nodes can result in many attributes which might degrade the generalization performance of the learning algorithms. In this paper both issues are studied using data supplied by Kaggle for the State of Oklahoma comparing Support Vector Machines and Gradient Boosted Regression. Also, three different feature selection methods have been tested: Linear Correlation, the ReliefF algorithm and, a new method based on local information analysis.},
  booktitle = {Intelligent Distributed Computing VIII},
  doi       = {10.1007/978-3-319-10422-5_29},
  file      = {Aler15machLrnDAsolarFrcstNWP.pdf:Aler15machLrnDAsolarFrcstNWP.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.26},
  url       = {http://dx.doi.org/10.1007/978-3-319-10422-5_29},
}

@Article{Nagy13basPredTechProg,
  author    = {Nagy, B{\'e}la and Farmer, J Doyne and Bui, Quan M and Trancik, Jessika E},
  title     = {Statistical basis for predicting technological progress},
  journal   = {PloS one},
  year      = {2013},
  volume    = {8},
  number    = {2},
  pages     = {e52669},
  abstract  = {Forecasting technological progress is of great interest to engineers, policy makers, and private investors. Several models have been proposed for predicting technological improvement, but how well do these models perform? An early hypothesis made by Theodore Wright in 1936 is that cost decreases as a power law of cumulative production. An alternative hypothesis is Moore's law, which can be generalized to say that technologies improve exponentially with time. Other alternatives were proposed by Goddard, Sinclair et al., and Nordhaus. These hypotheses have not previously been rigorously tested. Using a new database on the cost and production of 62 different technologies, which is the most expansive of its kind, we test the ability of six different postulated laws to predict future costs. Our approach involves hindcasting and developing a statistical model to rank the performance of the postulated laws. Wright's law produces the best forecasts, but Moore's law is not far behind. We discover a previously unobserved regularity that production tends to increase exponentially. A combination of an exponential decrease in cost and an exponential increase in production would make Moore's law and Wright's law indistinguishable, as originally pointed out by Sahal. We show for the first time that these regularities are observed in data to such a degree that the performance of these two laws is nearly the same. Our results show that technological progress is forecastable, with the square root of the logarithmic error growing linearly with the forecasting horizon at a typical rate of 2.5% per year. These results have implications for theories of technological change, and assessments of candidate technologies and policies for climate change mitigation.},
  comment   = {Many tech improvement rates can be predicted with exponential laws, and some other laws are sometimes equivalent to the exponential law.  

Big comparison of various tech. improvement algs, and also gives their error bars.

I've also evernoted this.

Also
* https://www.evernote.com/shard/s13/nl/1523219/628b6c62-2f3e-43e3-9181-fe4157603e49/
* Trancik2015imprvRinfSolWind},
  file      = {:Nagy13basPredTechProg.pdf:PDF},
  publisher = {Public Library of Science},
  url       = {https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0052669},
}

@Article{Garnier14blncErrIntraday,
  author    = {Garnier, Ernesto and Madlener, Reinhard},
  title     = {Balancing forecast errors in continuous-trade intraday markets},
  year      = {2014},
  abstract  = {Forecasting the production of photovoltaic (PV) and wind power systems inevitably implies inaccuracies. Therefore, sales made based on forecasts almost always require the vendor to make balancing efforts. In the absence of resources available within their own portfolios, operators can turn towards the intraday market in order to avoid an engagement in the imbalance market with the resulting surcharges and regulatory penalties. In this paper, we combine a novel trade value concept with options valuation and dynamic programming to optimize volume and timing decisions of an individual operator without market power when compensating PV or wind power forecast errors in the market. The model employs a multi-dimensional binomial lattice, with trade value maximized at every node to help formulating bids in view of correlated, uncertain production forecast and price patterns. Inspired by the German electricity market's characteristics, we test the sensitivity of the model's output ? namely trade timing and trade volume ? to changing uncertainty and transaction cost parameters in 50 different setups. It shows that the model effectively outbalances price against volumetric risks. Trades are executed early and with large batch sizes in the case of price volatility. In contrast, increasing forecast error uncertainty leads to trade delays. High transaction costs trigger batch size reductions and ultimately further trade delays. Running 10,000 simulations across ten scenarios, we find that the model translates its flexible trade execution into a competitive advantage vis-?-vis static bidding strategy alternatives.

Keywords: Bidding strategy, Production forecast, Renewable energy, Options, Intraday market},
  comment   = {Dynamic programming in intraday trading. Good for Eweline/IRPWIND "use of forecast" work, as it is taylored to German market. Also considers error correlations.

* German laws don't allow speculating against balancing power.  Recently, enforcement of this rule has gotten stricter
* Double trading undesirable because:
   1. transaction cost
   2. "bid ask spread" for small volumes (people charge more for less?)
   3. may buy haigh and sell low
   4. risk of not finding a trade.},
  file      = {Garnier14blncErrIntraday.pdf:Garnier14blncErrIntraday.pdf:PDF},
  owner     = {sotterson},
  publisher = {FCN Working Paper},
  timestamp = {2015.03.31},
  url       = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2463199},
}

@Article{Messner13probFrcstInvCensor,
  author    = {Messner, Jakob W. and Zeileis, Achim and Broecker, Jochen and Mayr, Georg J.},
  title     = {Probabilistic wind power forecasts with an inverse power curve transformation and censored regression},
  journal   = {Wind Energy},
  year      = {2013},
  issn      = {1099-1824},
  abstract  = {Forecasting wind power is an important part of a successful integration of wind power into the power grid. Forecasts with lead times longer than 6?h are generally made by using statistical methods to post-process forecasts from numerical weather prediction systems. Two major problems that complicate this approach are the non-linear relationship between wind speed and power production and the limited range of power production between zero and nominal power of the turbine. In practice, these problems are often tackled by using non-linear non-parametric regression models. However, such an approach ignores valuable and readily available information: the power curve of the turbine's manufacturer. Much of the non-linearity can be directly accounted for by transforming the observed power production into wind speed via the inverse power curve so that simpler linear regression models can be used. Furthermore, the fact that the transformed power production has a limited range can be taken care of by employing censored regression models.In this study, we evaluate quantile forecasts from a range of methods: (i)?using parametric and non-parametric models, (ii)?with and without the proposed inverse power curve transformation and (iii)?with and without censoring. The results show that with our inverse (power-to-wind) transformation, simpler linear regression models with censoring perform equally or better than non-linear models with or without the frequently used wind-to-power transformation.Copyright ? 2013 John Wiley \& Sons, Ltd.},
  comment   = {Haven't read it yet, but seems to do prob. forecasts w/ inverse power curve and other stuff.},
  doi       = {10.1002/we.1666},
  file      = {Messner13probFrcstInvCensor.pdf:Messner13probFrcstInvCensor.pdf:PDF},
  groups    = {PointDerived, doReadWPV_2},
  keywords  = {wind power, probabilistic forecasting, power curve transformation, censored regression, quantile regression},
  owner     = {sotterson},
  timestamp = {2013.10.01},
}

@Book{Jolliffe03FrcstVerificationBook,
  title     = {Forecast verification: a practitioner's guide in atmospheric science},
  publisher = {John Wiley \& Sons},
  year      = {2003},
  author    = {Jolliffe, Ian T and Stephenson, David B},
  abstract  = {Forecasts are made in many disciplines, the best known of which are economic
forecasts and weather forecasts. Other situations include medical diagnostic tests,
prediction of the size of an oil field, and any sporting occasion where bets are placed
on the outcome. It is very often useful to have some measure of the skill or value of
a forecast or forecasting procedure. Definitions of ?skill? and ?value? will be deferred
until later in the book, but in some circumstances financial considerations are
important (economic forecasting, betting, oil field size), whilst in others a correct
or incorrect forecast (medical diagnosis, extreme weather events) can mean the
difference between life and death.
Often the ?skill? or ?value? of a forecast is judged in relative terms. Is forecast
provider A doing better than B? Is a newly developed forecasting procedure an
improvement on current practice? Sometimes, however, there is a desire to measure
absolute, rather than relative, skill. Forecast verification, the subject of this book, is
concerned with judging how good is a forecasting system or single forecast.
Although the phrase ?forecast verification? is generally used in atmospheric science,
and hence adopted here, it is rarely used outside the discipline. For example, a
survey of keywords from articles in the International Journal of Forecasting between
1996 and 2002 has no instances of ?verification?. This journal attracts authors from a
variety of disciplines, though economic forecasting is prominent. The most frequent
alternative terminology in the journal?s keywords is ?forecast evaluation?, although
validation and accuracy also occur. Evaluation and validation also occur in other
subject areas, but the latter is often used to denote a wider range of activities than
simply judging skill or value ? see, for example, Altman and Royston (2000).
Many disciplines make use of forecast verification, but it is probably fair to say
that a large proportion of the ideas and methodology have been developed in the
context of weather and climate forecasting, and this book is firmly rooted in that
area. It will therefore be of greatest interest to forecasters, researchers and students
in atmospheric science. It is written at a level that is accessible to students and to
operational forecasters, but it also contains coverage of recent developments in the
area. The authors of each chapter are experts in their fields and are well aware of the
needs and constraints of operational forecasting, as well as being involved in
research into new and improved methods of verification. The audience for the
book is not restricted to atmospheric scientists ? there is discussion in several
chapters of similar ideas in other disciplines. For example, ROC curves (Chapter
3) are widely used in medical applications, and the ideas of Chapter 8 are particularly
relevant to finance and economics.
To our knowledge there is currently no other book that gives a comprehensive
and up-to-date coverage of forecast verification. For many years, the WMO
publication by Stanski et al. (1989), and its earlier versions, was the standard
reference for atmospheric scientists, though largely unknown in other disciplines.
Its drawback is that it is somewhat limited in scope and is now rather out-of-date.
Wilks (1995, Chapter 7) and von Storch and Zweirs (1999, Chapter 18) are more
recent but, inevitably as each comprises only one chapter in a book, are far from
comprehensive. Katz and Murphy (1997a) discuss forecast verification in some
detail, but mainly from the limited perspective of economic value. The current
book provides a broad coverage, although it does not attempt to be encyclopaedic,
leaving the reader to look in the references for more technical material.
Chapters 1 and 2 of the book are both introductory. Chapter 1 gives a brief
review of the history and current practice in forecast verification, gives some
definitions of basic concepts such as skill and value, and discusses the benefits and
practical considerations associated with forecast verification. Chapter 2 describes a
number of informal descriptive ways, both graphical and numerical, of comparing
forecasts and corresponding observed data. It then establishes some theoretical
groundwork that is used in later chapters, by defining and discussing the joint
probability distribution of the forecasts and observed data. Consideration of this
joint distribution and its decomposition into conditional and marginal distributions
leads to a number of fundamental properties of forecasts. These are defined, as are
the ideas of accuracy, association and skill.
Both Chapters 1 and 2 discuss the different types of data that may be forecast,
and each of the next five chapters then concentrates on just one type. The subject of
Chapter 3 is binary data in which the variable to be forecast has only two values, for
example, {Rain, No Rain}, {Frost, No Frost}. Although this is apparently the
simplest type of forecast, there have been many suggestions of how to assess them,
in particular many different verification measures have been proposed. These are
fully discussed, along with their properties. One particularly promising approach is
based on signal detection theory and the ROC curve.
For binary data one of two categories is forecast. Chapter 4 deals with the case in
which the data are again categorical, but where there are more than two categories.
A number of skill scores for such data are described, their properties are discussed,
and recommendations are made.
Chapter 5 is concerned with forecasts of continuous variables such as temperature.
Mean square error and correlation are the best-known verification measures
for such variables, but other measures are also discussed including some based on
comparing probability distributions.
Atmospheric data often consist of spatial fields of some meteorological variable
observed across some geographical region. Chapter 6 deals with verification for
such spatial data. Many of the verification measures described in Chapter 5 are
also used in the spatial context, but the correlation due to spatial proximity causes
complications. Some of these complications, together with verification measures
that have been developed with spatial correlation in mind, are discussed in
Chapter 6.
Probability plays a key role in Chapter 7, which covers two topics. The first is
forecasts that are actually probabilities. For example, instead of a deterministic
forecast of ?Rain? or ?No Rain?, the event ?Rain? may be forecast to occur with
probability 0.2. One way in which such probabilities can be produced is to generate
an ensemble of forecasts, rather than a single forecast. The continuing increase of
computing power has made larger ensembles of forecasts feasible, and ensembles of
weather and climate forecasts are now routinely produced. Both ensemble and
probability forecasts have their own peculiarities that necessitate different, but
linked, approaches to verification. Chapter 7 describes these approaches.
The discussion of verification for different types of data in Chapters 3?7 is largely
in terms of mathematical and statistical properties, albeit properties that are defined
with important practical considerations in mind. There is little mention of cost or
value ? this is the topic ofChapter 8.Much of the chapter is concerned with the simple
cost-loss model, which is relevant for binary forecasts. These forecasts may be either
deterministic as in Chapter 3, or probabilistic as in Chapter 7. Chapter 8 explains
some of the interesting relationships between economic value and skill scores.
The final chapter (9) reviews some of the key concepts that arise elsewhere in the
book. It also summarizes those aspects of forecast verification that have received
most attention in other disciplines, including Statistics, Finance and Economics,
Medicine, and areas of Environmental and Earth Science other than Meteorology
and Climatology. Finally, the chapter discusses some of the most important topics
in the field that are the subject of current research or that would benefit from future
research.
This book has benefited from discussions and help from many people. In particular,
as well as our authors, we would like to thank the following colleagues for
their particularly helpful comments and contributions: Harold Brook, Barbara
Casati, Martin Goeber, Mike Harrison, Rick Katz, Simon Mason, Buruhani
Nyenzi and Dan Wilks. Some of the earlier work on this book was carried while
one of us (I.T. Jolliffe) was on research leave at the Bureau of Meteorology
Research Centre (BMRC) in Melbourne. He is grateful to BMRC and its staff,
especially Neville Nicholls, for the supportive environment and useful discussions;
to the Leverhulme Trust for funding the visit under a Study Abroad Fellowship;
and to the University of Aberdeen for granting the leave.
Looking to the future, we would be delighted to receive any feedback comments
from you, the reader, concerning material in this book, in order that improvements
can be made in future editions (see www.met.rdg.ac.uk/cag/forecasting).},
  comment   = {Rafael found this...},
  file      = {Jolliffe03FrcstVerificationBook.pdf:Jolliffe03FrcstVerificationBook.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.05.29},
}

@Misc{Jolliffe12frcstVerif,
  author    = {Jolliffe, Ian T. and Stephenson, David B.},
  title     = {Forecast verification: a practitioner's guide in atmospheric science},
  year      = {2012},
  abstract  = {Forecasts are made in many disciplines, the best known of which are economic
forecasts and weather forecasts. Other situations include medical diagnostic tests,
prediction of the size of an oil field, and any sporting occasion where bets are placed
on the outcome. It is very often useful to have some measure of the skill or value of
a forecast or forecasting procedure. Definitions of skill and value will be deferred
until later in the book, but in some circumstances financial considerations are
important (economic forecasting, betting, oil field size), whilst in others a correct
or incorrect forecast (medical diagnosis, extreme weather events) can mean the
difference between life and death.
Often the skill or value of a forecast is judged in relative terms. Is forecast
provider A doing better than B? Is a newly developed forecasting procedure an
improvement on current practice? Sometimes, however, there is a desire to measure
absolute, rather than relative, skill. Forecast verification, the subject of this book, is
concerned with judging how good is a forecasting system or single forecast.
Although the phrase forecast verification is generally used in atmospheric science,
and hence adopted here, it is rarely used outside the discipline. For example, a
survey of keywords from articles in the International Journal of Forecasting between
1996 and 2002 has no instances of verification. This journal attracts authors from a
variety of disciplines, though economic forecasting is prominent. The most frequent
alternative terminology in the journals keywords is forecast evaluation, although
validation and accuracy also occur. Evaluation and validation also occur in other
subject areas, but the latter is often used to denote a wider range of activities than
simply judging skill or value see, for example, Altman and Royston (2000).

...},
  comment   = {Verification book, has cost functions e.g. economic value of skill.  New addition came out in 2012.  Is ther pdf version here just the 2013 version?

Older version: Jolliffe03FrcstVerificationBook},
  file      = {Jolliffe12frcstVerif.pdf:Jolliffe12frcstVerif.pdf:PDF},
  owner     = {sotterson},
  publisher = {John Wiley \& Sons},
  timestamp = {2014.01.13},
  url       = {http://www.atmosfera.unam.mx/jzavala/TemSelModNum/Libros/Jolliffe,%20Stephenson%20(eds.).%20Forecast%20Verification..%20A%20Pract.pdf},
}

@TechReport{Vlasova07windShortSpatioTemp,
  author      = {J. Vlasova and E. Kotwa and H. A. Nielsen and H. Madsen},
  title       = {Spatio-temporal modelling of short-term wind power prediction errors (02004/FU5766 - Improved wind power prediction)},
  institution = {Informatics and Mathematical Modelling, Technical University of Denmark, {DTU}},
  year        = {2007},
  abstract    = {Forecasts of wind power generation are more and more frequently used in various management tasks related to integration of wind power generation in power systems. The quality of the forecast is very important, and a reliable estimate of the uncertainty of the forecast is known to be essential. Today the forecasts of wind power generation are provided without a proper consideration to the spatio-temporal dependencies observed in the wind power generation field. State-of-art prediction systems, like Wind Power Prediction Tool ({WPPT} 1 ), typically provide forecast for a single wind farm. Predictions for a larger region with wind farms are then obtained by an upscale of the individual farm predictions. This means that the spatio-temporal relations are not adequately considered. The aim of this work is to investigate weather it is possible to improve the wind power forecasting system {WPPT,} developed in Denmark, by examining the spatio-temporal correlation of the prediction errors. The paper is organized in the following way: Section 2 presents the data set used in the work. Then in Section 3 a pre-modelling analysis of the data structure is provided. The results of the correlation study are presented and discussed. Further, the modelling step is performed: three types of the models for {WPPT} error analysis are discussed: {ARX,} Threshold and Conditional Parametric Models. This takes place in Section 4. The description of each model consists of 4 parts Modeling, Estimation, Application and Results. The first two are theoretical: Modeling is a general description of the model; Estimation deals with how the parameters can be estimated. The last two parts show respectively how the model was applied to the data and the results obtained. Section 5 describes validation methods used in this study for checking the adequacy of the performance of the fitted models. The paper concludes in Section 6 with a small discussion on the general results and possible future work.},
  comment     = {Can predict hour ahead forecasting errors by looking at past errors and wind speed/dir-dependent errors from other projects
* predicting ERROR of wind POWER forecasting, not wind.
* could use, I suppose, if used last hour's error to predict this hour's error but that's not done here
* error is assumed to propagate; I think this means that WPPT forecaster errors make same errors everywhere: what is propagated is weather they can't model. WPPT is a mixed statistical and NWP forecasting system
* they expect 3TIER switching regime approach to fail in more complex wind patterns () Clustering
* wind farms in denmark are clustered manually; analysis is done between 5 groups of wind farms
* groups are similar to algorithmic clustering done in Siebert08regPowForecast, I think

Lag selection
* based on linear correlation magnitude
* not explained in this paper, can't find reference
* talk about other ways to measure nonlinear dependence
* correlations used to pick lags assume stationarity but shouldn't these be regime switching too?
* speculates that correlations at 5 hour ahead lags are due to NWP timing errors (I think) Error prediction models
* ARX: autoregressive with inputs from other farms at selected lags
-- error autoregression indicates that errors can be modeled w/ ARMA
-- but ARX skips the MA part so did they miss something?
* TARX: switching ARX based on wind direction
-- four direction regimes -- different model, depending upon direction
-- "direction" is directly averaged; not vector average which would weight by speed
-- wind direction (and speed) are forecasts
-- switching thresholds chosen manually
* CP-TARX: ARX switches on dir; params interpolated from wind speed
-- they're using Nielsen00timeTrackCoeffs models: could have smooth coeffs for both dir and speed
-- some rationale on why mix of switch and tune on p. 21 but I don't get it.
-- don't know why they didn't do that
-- NN's could do that but they don't have the autoregressive part
-- model structure comes from TARX; only param tuning based on speed for CP-TARX
-- "speed" is a weighted average across groups, weights derived from a linear regression Results
* CP-TARX > TARX > ARX
* but most of error prediction power comes from simplest ARX
* but maybe didn't have enough data (< 7 mos.) to avoid overtraining complex CP-TARX
* did cross validation on param training but not model structure (so partly test on train) Residual analysis
* independent errors (of error predictor) OK
* but tails longer than Gaussian and skewed: suggest using skew-t},
  file        = {Vlasova07windShortSpatioTemp.pdf:Vlasova07windShortSpatioTemp.pdf:PDF;Vlasova07windShortSpatioTemp.pdf:Vlasova07windShortSpatioTemp.pdf:PDF},
  groups      = {Read},
  location    = {Richard Petersens Plads, Building 321, {DK-}2800 Kgs. Lyngby},
  owner       = {sotterson},
  series      = {IMM-Technical Report-2007-18},
  timestamp   = {2009.01.07},
  url         = {http://www2.imm.dtu.dk/pubdb/p.php?5552},
}

@Article{Tastu10spatTempErr,
  author    = {Julija Tastu and Pierre Pinson and Ewelina Kotwa and Henrik Madsen and Henrik Aa. Nielsen},
  title     = {Spatio-temporal analysis and modeling of short-term wind power forecast errors},
  journal   = {Wind Energy},
  year      = {2010},
  issn      = {1099-1824},
  abstract  = {Forecasts of wind power production are increasingly being used in various management tasks. So far, such forecasts and
related uncertainty information have usually been generated individually for a given site of interest (either a wind farm
or a group of wind farms), without properly accounting for the spatio-temporal dependencies observed in the wind generation
fi eld. However, it is intuitively expected that, owing to the inertia of meteorological forecasting systems, a forecast
error made at a given point in space and time will be related to forecast errors at other points in space in the following
period. The existence of such underlying correlation patterns is demonstrated and analyzed in this paper, considering
the case-study of western Denmark. The effects of prevailing wind speed and direction on autocorrelation and crosscorrelation
patterns are thoroughly described. For a fl at terrain region of small size like western Denmark, signifi cant
correlation between the various zones is observed for time delays up to 5 h. Wind direction is shown to play a crucial
role, while the effect of wind speed is more complex. Nonlinear models permitting capture of the interdependence structure
of wind power forecast errors are proposed, and their ability to mimic this structure is discussed. The best performing
model is shown to explain 54\% of the variations of the forecast errors observed for the individual forecasts used
today. Even though focus is on 1-h-ahead forecast errors and on western Denmark only, the methodology proposed may
be similarly tested on the cases of further look-ahead times, larger areas, or more complex topographies. Such generalization
may not be straightforward. While the results presented here comprise a fi rst step only, the revealed error propagation
principles may be seen as a basis for future related work.

KEYWORDS
wind power prediction; forecast errors; correlation analysis; spatio-temporal modeling; non-linear regime-switching modeling},
  comment   = {Predict future forecast errors w/ old errors at other farms; switch by wind dir, interp by wind speed. (Henrik likes this work, says maybe s/ use it for NORSEWInD)
* they don't expect 3TIER (Klarson) switching to work when no channels (why not?)
* DK is flat, has prevailing winds
   ==> they expect switching to succeed

This work corrects errors on WPPT-based forecasts
* HIRLAM met model from Danish Met. Inst.
* hour ahead foreacasts, sample rate is one hour, so one step ahead
* forecasts include diurnal and also adapt
* does POWER forecasts, so they're correcting power Groups
* forecasts for 15 regions lumped into 5 for this experiment * not sure why they lumped
* within group, wind speed and dir are separately averaged (not vectoral). WHY?

Prelim study
* see linear error dependency across space
* dependence is mostly in wind direction: "errors propagate downind"
* CCF and ACF waveforms show ARMA pattern

Wind dir/speed dependence
* clear downwind dependence
* high speed ==> high corr of errors, up to 5 lags
* says phase ships more prominent in high speeds (no cite)
* dependence is nonlinear

Three models
* simple ARX error, no switching
* TARX model: ARX w/ threshold-based wind dir switching
 -- dir thresholds are chosen heuristically
 -- Switching training described in Pinson08regimeSwitchWind
* CP-TARX model: TARX w/ linear coeffs changing smoothly w/ wind speed
 -- wind speed input is a linear combo of all speeds; kind of a hack
 -- trained CP coeffs sho increased long lag dependence w/ increasing wind speed
 -- also, why didn't they use the forecasted speed?

Results
* most improvement from ARX
* TARX, CP-TARX better than ARX (test on train)
* cross validation: CP-TARX not so good, probably overtrained

My Comments
* study probably good enough to show that spatio-temporal dep. works
* scarce data probably makes speed and dir improvement numbers questionable.
* test case is only DK, which is probably too simple a system to make general conclusions about the approach
* I'm not sure that cross-validation totally works here: only 7 months, split into 3 sets
* lags picked contiguously; by AIC I think. Better to have non-contiguous
* only using wind; not pressure, temp, etc.  Optimal?
* I don't like the ad-hoc direction partitioning
* They want to add new farms w/o damaging old coeffs; an interesting problem

The Tastu moved on to copulas:
- Tastu13scenSpatTimeCpla
-Tastu15spcTimeTrajGaussCpla},
  doi       = {10.1002/we.401},
  file      = {Tastu10spatTempErr.pdf:Tastu10spatTempErr.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  timestamp = {2010.05.21},
  url       = {http://www3.interscience.wiley.com.globalproxy.cvt.dk/journal/123334586/abstract?CRETRY=1&SRETRY=0},
}

@InProceedings{Tastu10mVarCondParam,
  author    = {Julija Tastu and Pierre Pinson and Henrik Madsen},
  title     = {Multivariate conditional parametric models for a spatio-temporal analysis of short-term wind power forecast errors},
  booktitle = {European Wind Energy Conference and Exhibition (EWEC)},
  year      = {2010},
  abstract  = {Forecasts of wind power production are increasingly being used in various management tasks. So far, such forecasts and related uncertainty information have usually been generated individually for a given site of interest (either a wind farm or a group of wind farms), without properly accounting for the spatio-temporal dependencies observed in the wind generation field. However, it is intuitively expected that, owing to the inertia of meteorological forecasting systems, a forecast error made at a given point in space and time will be related to forecast errors at other points in space in the following period. The existence of such underlying patterns is demonstrated and analyzed in this paper, considering the case-study of western Denmark. Adaptive Multivariate Conditional Parametric models are proposed for capturing the interdependence structure of wind power forecast errors and reveal the effects of meteorological forecasts on this dependence. The results show that by considering spatiotemporal dependencies, the wind power forecast errors might be significally reduced. Even though focus is on one-hour-ahead forecast errors and on western Denmark only, the methodology proposed may be similarly applied to the case of further look-ahead times and larger areas.},
  comment   = {Linear regression of power prediction errors, conditioned on wind spd, dir.

Paper is really vague. Tastu10spatTempErr explains this work better that this paper

Idea
* lump NWP errors for 15 DK regions into 8 groups
* predict errors for each, using past errors from the others
* prediction is linear regression
-- output is error at group
-- input is error at all groups, possibly lagged
* but the coeffs depend "smoothly" upon wind dir and speed (how is not explained)

Results:
* VAR w/o varying coeefs shows good forecast improvement (after add errors back on)
* CP-VAR: coeffs conditioned on wind spd/dir improve even more
* results reported only for forecasted wind direction conditioning

* coeffs show that downwind sites depend upon upwind sites (coeff sensitivity to wind dor)},
  file      = {Tastu10mVarCondParam.pdf:Tastu10mVarCondParam.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  timestamp = {2010.05.21},
  url       = {http://www.imm.dtu.dk/English/Research/Mathematical_Statistics/People.aspx?lg=showcommon&id=257115},
}

@Article{Jouan87fourDescCardiac,
  author    = {A. Jouan},
  title     = {Analysis of Sequences of Cardiac Contours by {Fourier} Descriptors for Plane Closed Curves},
  journal   = {Medical Imaging, IEEE Transactions on},
  year      = {1987},
  volume    = {6},
  number    = {2},
  pages     = {176--180},
  month     = jun,
  issn      = {0278-0062},
  abstract  = {Fourier descriptors (FD's) for plane closed curves allow a quantitative analysis of the shape of cardiac cavities whose contours are extracted from sequences of scintigraphic images. The application of the FD method leads to the characterization and the recognition of each contour in the sequence as well as the analysis of the temporal evolution of the contours (in relation to the cardiac contraction). As a result, automatic procedure of shape analysis can be designed from the use of FD's.},
  comment   = {Good examples of Fourier Descriptors - maybe a better explanation than Zahn72fourDesc},
  doi       = {10.1109/TMI.1987.4307820},
  file      = {Jouan87fourDescCardiac.pdf:Jouan87fourDescCardiac.pdf:PDF},
  owner     = {scotto},
  timestamp = {2010.09.06},
}

@Conference{Alessandrini13wireCostFrcstResults,
  author    = {Stefano Alessandrini and Simone Sperati and George Kariniotakis and Pierre Pinson},
  title     = {WIRE COST Action, benchmark results},
  booktitle = {EWEA Forecasting Workshop},
  year      = {2013},
  month     = dec,
  abstract  = {Fraunhofer researchers win wind forecasting competition
02/07/2014

In the EU research program WIRE: "Weather Intelligence for Renewable Energies" of European cooperation in Science and Technology (COST) a forecasting benchmark test was conducted in which the Fraunhofer IWES from Kassel created the best wind farm forecasts.


 IWES group leader and award winner Jan Dobschinski with his colleagues Thomas Kanefendt and Scott Otterson in the interpretation of weather and production forecasting
Fraunhofer IWES

Eleven different participants from Europe, Japan, India, USA and Australia participated in the benchmark test, the WIRE COST Action initiated in 2013. Dr. Kurt Rohrig, Deputy Director of the IWES and developer of the first prediction system of IWES was not surprised, but was pleased that the scientists of its area occupied the first place.

The focus of the evaluation, the quality of the individual prediction algorithms that were used for the transformation of the meteorological input variables in the expected power generated by wind farms was. The participants received the same records of historical local weather forecasts and performance measurements from two wind farms. However, only the weather forecasts were provided for the evaluation period. The task was to predict the performance of two wind farms for that period and they are ready to ask the organizers for the evaluation anonymously.

In order to also make statements about the applicability of prediction models for different orographic conditions, two wind farms were especially extreme local properties selected. Both wind farms are located in Italy, one of which has an installed nominal capacity of 104 MW and is located in complex terrain. The other includes an investment portfolio of 21 MW and is located in the lowlands.

To model the relationship between the meteorological input variables from the weather forecast model and the expected power generated by wind farms, researchers at the Fraunhofer IWES used Artificial Neural Networks (ANN), whose topology and learning methods have been adapted specifically for this task. "The use of ANN for wind power prediction was implemented by Fraunhofer IWES back in 2001 and then ushered in a new era in this field," says Dr. Rohrig. A particular focus is on the preprocessing of the available data. This included a comprehensive analysis of existing historical performance measurements and an optimized treatment and selection of model input variables.

The performance predictions of all participating institutions have been evaluated by the COST Action organizing committee for all forecast horizons of the underlying weather forecast. These include lead times from 1 to 48 or to 72 hours depending on the weather model used.

The wind farm forecasting model of the Fraunhofer IWES had the smallest error for both wind farms on average. For the wind farm in the lowlands, the prognosis of the Fraunhofer IWES had a mean absolute error (MAE) of 9.45\% of the installed capacity. This corresponds to an average deviation of about 2 MW. An even lower mean absolute error of 9.04\% of the total installed capacity showed the power prediction of IWES for the wind farm in complex terrain, corresponding to an average error of about 9.4 MW.

The result of this benchmark reflects the competence of the IWES for performance predictions and statistically based modeling and makes it for many years a reliable partner for network operators and energy traders.},
  comment   = {IWES (Jan?) wins EWEA determinstic forecasting contest. Best prob. forecast algorithm was kernel/copula, from Bremnes, in Norway.

The winning prob. alorithm was:
* Conditional kernel density estimation with a quantile-copula estimator.
* Inputs:
 - forecasted wind speed (power not an input)
 - and direction (level 30),
 - hour of the day and
 - lead-time (so must have trained a single algorithm across all lookahead times)
* 5\% and 95\% quantiles were computed from the forecasted PDF using numerical integration.
* I'm not sure of the lookahead time but the graph suggests 0-72 hours

IWES press release is here (pasted in as an abstract):
http://www.iwes.fraunhofer.de/de/Presse-Medien/Pressemitteilungen/2014/forscher-des-fraunhofer-iwes-gewinnen-prognosewettbewerb.html

I've also evernoted the press release and a Franfurt newspaper article about it.},
  file      = {Alessandrini13wireCostFrcstResults.pdf:Alessandrini13wireCostFrcstResults.pdf:PDF},
  location  = {Rotterdam},
  owner     = {sotterson},
  timestamp = {2014.02.11},
  url       = {http://www.ewea.org/events/workshops/wp-content/uploads/2013/12/EWEA-Forecasting-Workshop-2013-3-1-Stefano-Alessandrini-RSE.pdf},
}

@InProceedings{Jost15dynDimFrqRsrvQR,
  author       = {Jost, Dominik and Braun, Axel and Fritz, Rafael},
  title        = {Dynamic dimensioning of frequency restoration reserve capacity based on quantile regression},
  booktitle    = {European Energy Market (EEM), 2015 12th International Conference on the},
  year         = {2015},
  pages        = {1--5},
  organization = {IEEE},
  abstract     = {Frequency restoration reserve capacity is 
traditionally dimensioned with the help of deterministic criteria 
or by using probabilistic approaches that determine the capacity 
for a long period (several months). These static approaches work 
out quite well with traditional power systems. But increasing 
shares of intermittent generation introduce higher volatility to 
today’s and future power systems which leads to a more volatile 
need for balancing. In this paper the main influences on the 
occurrence of imbalances are identified. Subsequently a new 
method for the dimensioning of reserve capacities is presented. 
This method uses quantile regression based on artificial neural 
networks to forecast the reserve capacities to meet the striven 
security level. Subsequently the method is tested for the day-
ahead dimensioning of frequency restoration reserve capacities 
in Germany.  
Index Terms—ancillary services, balancing capacity, capacity 
dimensioning, frequency restoration reserve capacity, quantile 
regression},
  comment      = {Dominik's first paper (I think) about using QR for frequency reserve allocation.  Newer version in  Jost16dynSizFreqResLen},
  file         = {:Jost15dynDimFrqRsrvQR.pdf:PDF},
  groups       = {[sotterson:]},
}

@Article{Genccay01xtrmValMatlabEVIM,
  author    = {Gen{\c{c}}ay, Ramazan and Sel{\c{c}}uk, Faruk and Ulug{\"u}lyagci, Abdurrahman},
  title     = {{EV}IM: a software package for extreme value analysis in Matlab},
  journal   = {Studies in Nonlinear Dynamics and Econometrics},
  year      = {2001},
  volume    = {5},
  number    = {3},
  pages     = {213--239},
  abstract  = {From the practitioners' point of view, one of the most interesting questions that tail studies can answer is what are the extreme movements that can be expected in financial markets? Have we already seen the largest ones or are we going to experience even larger movements? Are there theoretical processes that can model the type of fat tails that come out of our empirical analysis? Answers to such questions are essential for sound risk management of financial exposures. It turns out that we can answer these questions within the framework of the extreme value theory. This paper provides a step-by-step guideline for extreme value analysis in the MATLAB environment with several examples.},
  comment   = {A very readable intro to extreme values, and specifically, to a Matlab package that computes them.

Good background for probabilistic forecasts, since the extreme values are what set the limits on reserve power.},
  file      = {Genccay01xtrmValMatlabEVIM.pdf:Genccay01xtrmValMatlabEVIM.pdf:PDF},
  groups    = {Ensemble, PointDerived, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2013.10.14},
}

@Article{Moncrieff07rsrchIntrWeathClim,
  author    = {Moncrieff, Mitchell W and Shapiro, Melvyn A and Slingo, Julia M and Molteni, Franco},
  title     = {Collaborative research at the intersection of weather and climate},
  journal   = {Bulletin of the World Meteorological Organization},
  year      = {2007},
  volume    = {56},
  number    = {3},
  pages     = {204--211},
  abstract  = {Fundamental barriers to advancing
weather and climate diagnosis and
prediction on time-scales from days
to years are partly attributable
to gaps in knowledge and the
limited capability of contemporary
operational and research numerical
prediction systems to represent
precipitating convection and its multi-
scale organization, particularly in the
tropics. In this regard improvements
in convective parameterization have

not kept pace with improvements
in knowledge gained from process
studies of convective organization.
As convective organization is not
represented by contemporar y
parameterizations, the large-scale
effects of convective organization
have, therefore, yet to be properly
assessed. Examples of tropical
phenomena in which the multi-scale
organization of convection is a key
process are:},
  comment   = {Nice graph of timescales of tropical weather and climate

I copied it for the GIZ colombia cours from this URL},
  file      = {Moncrieff07rsrchIntrWeathClim.pdf:Moncrieff07rsrchIntrWeathClim.pdf:PDF},
  publisher = {Geneva, Switzerland: The Organization, 1990-},
  url       = {https://public.wmo.int/en/bulletin/collaborative-research-intersection-weather-and-climate},
}

@Article{Soares17distGridMgmtPowFlw,
  author   = {T. Soares and R. J. Bessa and P. Pinson and H. Morais},
  title    = {Active Distribution Grid Management based on Robust AC Optimal Power Flow},
  journal  = {IEEE Transactions on Smart Grid},
  year     = {2017},
  volume   = {PP},
  number   = {99},
  pages    = {1-1},
  issn     = {1949-3053},
  abstract = {Further integration of distributed renewable energy sources in distribution systems requires a paradigm change in grid management by the distribution system operators (DSO). DSOs are currently moving to an operational planning approach based on activating flexibility from distributed energy resources in day/hour-ahead stages. This paper follows the DSO trends by proposing a methodology for active grid management by which robust optimization is applied to accommodate spatial-temporal uncertainty. The proposed method entails the use of a multi-period AC-OPF, ensuring a reliable solution for the DSO. Wind and PV uncertainty is modeled based on spatial-temporal trajectories, while a convex hull technique to define uncertainty sets for the model is used. A case study based on real generation data allows illustration and discussion of the properties of the model. An important conclusion is that the method allows the DSO to increase system reliability in the real-time operation. However, the computational effort grows with increases in system robustness.},
  comment  = {Use of scenarios for DSO power flow optimization.  Related to net Demand forecast scenarios, since net demand is what flows.

Uses Scenario Generator in:  Pinson09probFrcstStatScenWind},
  doi      = {10.1109/TSG.2017.2707065},
  file     = {Soares17distGridMgmtPowFlw.pdf:Soares17distGridMgmtPowFlw.pdf:PDF},
  keywords = {Capacitors;Optimization;Real-time systems;Robustness;Uncertainty;Wind power generation;Decision-making;distribution system operator;robust optimization;solar power;uncertainty;wind power},
}

@Article{Jensen17euDatSimRE,
  author    = {Jensen, Tue V and Pinson, Pierre},
  title     = {RE-Europe, a large-scale dataset for modeling a highly renewable European electricity system},
  journal   = {Scientific data},
  year      = {2017},
  volume    = {4},
  pages     = {170175},
  abstract  = {Future highly renewable energy systems will couple to complex weather and climate dynamics. This coupling is generally not captured in detail by the open models developed in the power and energy system communities, where such open models exist. To enable modeling such a future energy system, we describe a dedicated large-scale dataset for a renewable electric power system. The dataset combines a transmission network model, as well as information for generation and demand. Generation includes conventional generators with their technical and economic characteristics, as well as weather-driven forecasts and corresponding realizations for renewable energy generation for a period of 3 years. These may be scaled according to the envisioned degrees of renewable penetration in a future European energy system. The spatial coverage, completeness and resolution of this dataset, open the door to the evaluation, scaling analysis and replicability check of a wealth of proposals in, e.g., market design, network actor coordination and forecasting of renewable power generation.},
  comment   = {European simulated wind/solar/demand measurements and forecasts.  Used by Peder Bacher for WP82.3 of IRPWIND for his stochastic differential equation studies.  There are some problems with this data, some of which he mentioned during our lunch at that Roskilde Cafe -- maybe I wrote them down in OneNote.  The main one is that it's all based on weather model analysis/synthesis, and then power curves, etc. are simulated.  There was also some problem with the first X hours of a run being garbage, but I don't remember the issue right now.},
  file      = {:Jensen17euDatSimRE.pdf:PDF},
  groups    = {[Scott Otterson:]},
  publisher = {Nature Publishing Group},
  url       = {https://www.nature.com/articles/sdata2017175},
}

@InProceedings{Shaaban11Riskbasedsecurity,
  author    = {Shaaban, M.},
  title     = {Risk-based security assessment in smart power grids},
  booktitle = {Innovative Smart Grid Technologies - Middle East},
  year      = {2011},
  pages     = {1--5},
  abstract  = {Future power grids are increasingly complex as they encompass elements such as renewables, storage, consumer options, and smart appliances. While these are likely to exacerbate uncertainties in both supply and demand, the overture of smart grid sensor and communication technologies promises, on the other hand, enhanced visibility and refined controls. Nonetheless, an evolutionary change in the tools used in system analysis is necessary to realize the full potential of the smart grid. This paper outlines a framework for assessing security in the operation planning horizon based on a quantitative risk measure. Value at Risk (VaR) is proposed as a measure of system security and is applied to an illustrative six-bus test system.},
  comment   = {Uses value at risk to assess security of a simulated power system.},
  doi       = {10.1109/ISGT-MidEast.2011.6220792},
  owner     = {sotterson},
  timestamp = {2014.12.04},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6220792},
}

@INPROCEEDINGS{Palm05multiStepGaussProcFuzzy,
  Author                   = {Palm, R.},
  Title                    = {Multi-Step-Ahead Prediction with {Gauss}ian Processes and TS-Fuzzy Models},
  Booktitle                = {Fuzzy Systems (FUZZ)},
  Year                     = {2005},
  Pages                    = {945--950},
  Month                    = may,
  Abstract                 = {Fuzzy multiple modeling is a method that combines analytical and black-box modeling in an optimal way. On the other hand, modeling with Gaussian processes is a probabilistic and non-parametric method which allows the prediction of the uncertainty of the model. This is of advantage for one-step ahead or even multi-step ahead predictions with noisy time series and disturbed closed loop control systems. The multi-step ahead prediction assumes the previous outputs and control values to be known as well as the future control values. A "naive" multi-step ahead prediction is a consecutive one-step ahead prediction whereas the outputs in each consecutive step are considered as inputs for the next step of prediction. Usually for closed loop control systems the nominal output trajectory is known in advance. However, because of the uncertainties and disturbances in the control loop the resulting control trajectory is only known up to the present time step but not for the future steps. To obtain the future control inputs for the multi-step ahead prediction the system is modeled by a multiple TS fuzzy model which was trained in advance to generate a nominal control trajectory in closed loop for a given nominal output trajectory. Simulations of nonlinear systems with built-in uncertainties illustrate the good performance of the multi-step ahead prediction with the combination of TS fuzzy models and Gaussian process models},
  DOI                      = {10.1109/FUZZY.2005.1452521},
  File                     = {Palm05multiStepGaussProcFuzzy.pdf:Palm05multiStepGaussProcFuzzy.pdf:PDF;Palm05multiStepGaussProcFuzzy.pdf:Palm05multiStepGaussProcFuzzy.pdf:PDF},
  Keywords                 = {Gaussian processes, closed loop systems, control system synthesis, fuzzy set theory, fuzzy systems, nonlinear systems, nonparametric statistics, prediction theory, predictive control, probabilityGaussian process, TS-fuzzy models, Takagi-Sugeno fuzzy models, black-box modeling, disturbed closed loop control systems, model uncertainty prediction, multistep-ahead prediction, noisy time series, nominal output trajectory, nonparametric method, probabilistic method},
  Owner                    = {sotterson},
  Timestamp                = {2008.10.03},
  URL                      = {http://ieeexplore.ieee.org.offcampus.lib.washington.edu/search/srchabstract.jsp?arnumber=1452521&isnumber=31200&punumber=9859&k2dockey=1452521@ieeecnfs&query=((multi-step-ahead+prediction+with+gaussian+processes+and+ts-fuzzy+models)%3Cin%3Emetadata)&pos=0&access=no}
}

@Article{Fouque06copulaPertGauss,
  author    = {Fouque, J.P. and Zhou, X.},
  title     = {Perturbed gaussian copula},
  journal   = {Advances in Econometrics},
  year      = {2006},
  abstract  = {{Gauss}ian copula is by far the most popular copula used in the financial industry in default dependency modeling. However, it has a major drawback -- it does not exhibit tail dependence, a very important property for copula. The essence of tail dependence is the interdependence when extreme events occur, say, defaults of corporate bonds. In this paper we show that some tail dependence can be restored by introducing stochastic volatility on a Gaussian copula. Using perturbation methods we then derive an approximate copula -- called perturbed Gaussian copula in this paper.},
  comment   = {Tail dependence of gaussian copula created w/ random pertubations use for spinning reserves},
  file      = {Fouque06copulaPertGauss.pdf:Fouque06copulaPertGauss.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.12.13},
}

@InProceedings{Francois05locKernHiDim,
  author    = {Fran{\cc}ois, Damien and Wertz, Vincent and Verleysen, Michel and others},
  title     = {About the locality of kernels in high-dimensional spaces},
  booktitle = {International Symposium on Applied Stochastic Models and Data Analysis},
  year      = {2005},
  pages     = {238--245},
  abstract  = {Gaussian kernels are widely used in many data analysis tools such as
Radial-Basis Function networks, Support Vector Machines and many others. Gaus-
sian kernels are most often deemed to provide a local measure of similarity between
vectors. In this paper, we show that Gaussian kernels are adequate measures of
similarity when the representation dimension of the space remains small, but that
they fail to reach their goal in high-dimensional spaces. We suggest the use of p-
Gaussian kernels that include a supplementary degree of freedom in order to adapt
to the distribution of data in high-dimensional problems. The use of such more
flexible kernel may greatly improve the numerical stability of algorithms, and also
the discriminative power of distance- and neighbor-based data analysis methods.
Keywords: High dimensional spaces, Local Models, Gaussian Kernels.},
  comment   = {Gaussian kernel distance not good in high dimensions; the proposed p-Gaussian kernel is argued to be better (other papers have experiments that show it works), and is super easy to train. Maybe useful for neighborhood weighting in local linear quantile regression, especially since it's kind of self-tuning, maybe eliminating the cross-validation step? Is used in an R library (which seems to work, while the formula in this paper seems to have a bug).

Gaussian Kernel Good:
1. sum of indep vars is Gaussian (CLT)
2. only need 2 moments to describe
3. local in low dims

Gaussian Kernel Bad:
1. is not local in high dims
2. cannot be made local by scaling (kernel bandwidth, I think he means)

Locality matters because
1. more interpretable (knowing what training points were close to test point increases interpretability)
2. non-local kernels produce ill-conditioned kernel matrices (matters e.g. when optimizing weighting of linear RBF coeffs

Why a Gaussian kernel is non-local in high dims:
* Ideal Kernel: should be able to find in acceptable proportions both similar and non similar vectors to a query point
* but can't tune Gaussian kernel bandwidth in high dim spaces so that distance slope occurs where the data points are (slope means they would be distinguishable).

p-Guassian Kernel
 -- idea is to adjust kernel curve so that it has significant slope at distances where distribution samples are
 -- adds one extra param to normal Gaussian kernel (so now there are a total of two)
 -- tuning is super-easy, based on percentiles of input point distance distribution
 -- IDEA: p-Gaussian uses Euclidean; I wonder if Mahanlonibus would be better??

Related paper has good explanations of high dim craziness: Verleysen05cursDimMinTser

Book says p-kernels generalize well in high dims but are computationally expensive (really?) and calls them exotic! Ullrich09frcstHdgMkt

Is used in R library: computeKernel {semisupKernelPCA}
http://www.inside-r.org/packages/cran/semisupKernelPCA/docs/computeKernel

Compare with high dim knn hubness e.g. Schnitzer12Localandglobal},
  doi       = {http://www.inma.ucl.ac.be/publi/289808.pdf},
  file      = {Francois05locKernHiDim.pdf:Francois05locKernHiDim.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2014.03.27},
}

@Article{Simpson12spatStatForgetCov,
  author    = {Simpson, Daniel and Lindgren, Finn and Rue, H{\aa}vard},
  title     = {In order to make spatial statistics computationally feasible, we need to forget about the covariance function},
  journal   = {Environmetrics},
  year      = {2012},
  volume    = {23},
  number    = {1},
  pages     = {65--74},
  issn      = {1099-095X},
  abstract  = {Gaussian random fields (GRFs) are the most common way of modeling structured spatial random effects in spatial statistics. Unfortunately, their high computational cost renders the direct use of GRFs impractical for large problems and approximations are commonly used. In this paper, we compare two approximations to GRFs with Mat??rn covariance functions: the kernel convolution approximation and the Gaussian Markov random field representation of an associated stochastic partial differential equation. We show that the second approach is a natural way to tackle the problem and is better than methods based on approximating the kernel convolution. Furthermore, we show that kernel methods, as described in the literature, do not work when the random field is not smooth. Copyright ?? 2011 John Wiley & Sons, Ltd.},
  comment   = {Says to model spatial random fields with Gaussian Markov Random Fields and stochastic partial differential equations (SPDEs).  Looks well written.  Has Matlab and R.

Referenced by Tastu15spcTimeTrajGaussCpla},
  doi       = {10.1002/env.1137},
  file      = {Simpson12spatStatForgetCov.pdf:Simpson12spatStatForgetCov.pdf:PDF},
  keywords  = {SPDE, GMRF, kernel methods, covariance functions},
  owner     = {sotterson},
  publisher = {John Wiley \& Sons, Ltd},
  timestamp = {2017.06.08},
  url       = {http://dx.doi.org/10.1002/env.1137},
}

@TechReport{Wang06imageProcE161crsNts,
  author       = {Ruye Wang},
  title        = {Computer Image Processing and Analysis (E161) Course Notes},
  institution  = {Harvey Mudd College},
  year         = {2006},
  address      = {Claremont, CA USA},
  abstract     = {Gaussiaon Process
Linar Regression
Nonlinar Regression
Gaussian process
Appendix A: Conditional and Marginal of Multivariate Gaussian
Inverse and determinant of partitioned symmetric matrix
Marginal and conditional distributions of multivariate normal distribution
Appendix B: Kernels and Mercer's Theorem

About this document ...},
  comment      = {Derivation of conditional and marginal multivariate random normal distributions.

If model data with a big MVN, then, for x = [x1 x2]; (x1 and x2 are vectors)

* Conditional is the probability of a seeing x2 when you know x1 is: f(x2|x1)

* Marginal could be used to assess the probability of some data, x2, when you know nothing about x1:  f(x1)

Example:  reference farm uscaling:  x = a set of power values, either reference farms measuremets or estimated measurements give NWP analysis data and/or forecasts.

See also: p. 200 of Rasmussen06GaussProcBook},
  file         = {Wang06imageProcE161crsNts.pdf:Wang06imageProcE161crsNts_GaussProc.pdf:PDF},
  organization = {Harvey Mudd College},
  owner        = {sotterson},
  timestamp    = {2017.03.16},
  url          = {http://fourier.eng.hmc.edu/e161/},
}

@Article{Qin07rgrsnClustVSkmeans,
  author    = {Qin, Li-Xuan and Self, Steven G},
  title     = {On comparing the clustering of regression models method with K-means clustering},
  year      = {2007},
  abstract  = {Gene clustering is a common question addressed with microarray data. Previous methods, such as K-means clustering and hierarchical clustering, base gene clustering directly on the observed measurements. A new model-based clustering method, the clustering of regression models (CORM) method, bases the clustering of genes on their relationship to covariates. It explicitly models different sources of variations and bases gene clustering solely on the systematic variation. Both being partitional clustering, CORM is closely related to K-means clustering. In this paper, we discuss the relationship between the two clustering methods in terms of both model formulation and implications on other important aspects of cluster analysis. We show that the two methods can both be considered as solutions to a least squares problem with missing data but they each concern a different type of least squares. We also show that CORM tends to provide stable clusters across samples and is particularly useful if the cluster averages are used as predictors for sample classification. Finally we illustrate the application of CORM to a set of time course data measured on four yeast samples, which has a complicated experimental design and is difficult for K-means to handle.
Disciplines
Microarrays},
  comment   = {Clusters genes based on their responses to multivariate input . Can handle missing data. Algorithm is implemented in the R CORM package. Useful for turbine/farm forecast aggregation, regime learning

Specifically, clusters their spline regression coefficients using an EM algorithm.

Result: Algorithm is able to ID genes that have similarly shaped expression over time
* this is apparenlty only 1D input and 1D output
* makes it easy to do usual spline basis w/ linear function
* but instead of spline, model could in principal be multilinear regression, thin plate spline, or whatever.
* since spline coeffs are unique, it's possible to cluster their coeffs (for other functions, they wouldn't be unique, e.g. cos())

USES
* cluster turbines for a farm forecast
 - those with the same power curve are forecasted jointly b/c their noise would cancel in their sums
 - ability to handle missing data (EM) is useful!
 - would want at least 3D spline of some sort so get directional dependence
 - my lagged wind velocity basis, again
* Regional forecast: same is idea as w/ turbines in a farm
* regime learning: the spline coeffs are unique for a regression relationship, so can cluster regression coeffs from different time periods to find regimes

Also see supplementary graphs pdf (attached)

See also: Qin06clustRgrsnGene
See newer version: Qin14geneRgrsnClustTime (whcih was cited by: Kun14windIntrvlClustSOM (irrelevantly?))

For large scale problems (lots of wind turbines) could run the EM algorithm with stochastic MM: Mairal13stochMMlargeOpt (has C++)

R CORM package: http://cran.r-project.org/web/packages/CORM/index.html
Many types of regression models compared w/ kmeans. Functions implemented in the R package, CORM. Probably high dim since it's genetics. Especially interesting because they can cluster with a B-spline basis.},
  file      = {paper:Qin07rgrsnClustVSkmeans.pdf:PDF;Supplementary Materials:Qin07rgrsnClustVSkmeans_supp.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {bepress},
  timestamp = {2014.04.01},
  url       = {http://biostats.bepress.com/mskccbiostat/paper14/},
}

@Article{Mayr12genAddModelsHiDimBoost,
  author    = {Mayr, Andreas and Fenske, Nora and Hofner, Benjamin and Kneib, Thomas and Schmid, Matthias},
  title     = {Generalized additive models for location, scale and shape for high dimensional data: flexible approach based on boosting},
  journal   = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  year      = {2012},
  volume    = {61},
  number    = {3},
  pages     = {403--427},
  abstract  = {Generalized additive models for location, scale and shape (GAMLSSs) are a popular semiparametric modelling approach that, in contrast with conventional generalized additive models, regress not only the expected mean but also every distribution parameter (e.g. location, scale and shape) to a set of covariates. Current fitting procedures for GAMLSSs are infeasible for high dimensional data set-ups and require variable selection based on (potentially problematic) information criteria. The present work describes a boosting algorithm for high dimensional GAMLSSs that was developed to overcome these limitations. Specifically, the new algorithm was designed to allow the simultaneous estimation of predictor effects and variable selection. The algorithm proposed was applied to Munich rental guide data, which are used by landlords and tenants as a reference for the average rent of a flat depending on its characteristics and spatial features. The net rent predictions that resulted from the high dimensional GAMLSSs were found to be highly competitive and covariate-specific prediction intervals showed a major improvement over classical generalized additive models.

Keywords:

 Generalized additive models for location, scale and shape;
 Gradient boosting;
 High dimensional data;
 Prediction inference;
 Spatial information;
 Variable selection},
  comment   = {Direct distribution estimate instead of quantiles. Maybe good for wind power upscaled quantile regression: up to 6 distribution parameters are estimated as a function of a potentially high dimensional feature vector. Boosting does feature selection and regularization, and gamlss.dist has a beta distribution (which allows 0 and 1 for min/max wind power saturation) as an output.

Is used by Schmid13betaBoostRgrssn for boosted beta regression.

I think...
The advantage is that crossover is impossible (it's estimating a distribution) and the disadvantage could be that the quantiles all must have the same parameter choice, since the whole distribution is being estimated. Can this be somehow melded with the per-quantile method of Fenske11idRiskBoostAddQR ?

See Hothorn14condXfrmModlQR for what might be another way.


Has R package.
Sides attached},
  doi       = {10.1111/j.1467-9876.2011.01033.x/full},
  file      = {Paper:Mayr12genAddModelsHiDimBoost.pdf:PDF;Slides:Mayr12genAddModelsHiDimBoost_slides.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.07.28},
}

@InProceedings{AbdulRahman12flexRampCAISO,
  author    = {Abdul-Rahman, K.H. and Alarian, H. and Rothleder, M. and Ristanovic, P. and Vesovic, B. and Bo Lu},
  title     = {Enhanced system reliability using flexible ramp constraint in CAISO market},
  booktitle = {Power and Energy Society General Meeting, 2012 IEEE},
  year      = {2012},
  pages     = {1--6},
  abstract  = {Generation fleet flexibility has been identified in all renewable integration studies performed by CAISO as one of the major factors required to enable high levels of renewable integration. The quantity of flexible capacity will be negatively impacted as a result of retirement or repowering of flexible once-through-cooling resources. This paper addresses management of the available CAISO's fleet ramping flexibility and offers a mathematical formulation for the solution of this issue. Operational issues associated with shortage of system ramping capability, which impact both grid and market operations, are presented in this paper. CAISO's implementation of the flexible ramp constraint as one mechanism to drive towards improved system reliability is discussed. The mathematical formulation and methodology to incorporate flexible ramp constraint into the market optimization application is given. Market results are included to demonstrate the benefits of the flexible ramp constraint.},
  comment   = {CAISO's kind-of probabilistic treatments of ramps.

system overview slides here: AbdulRahman13hybrdUncertCAISO
detailed proposal here: Xu12flexRamp2ndFprpsl},
  doi       = {10.1109/PESGM.2012.6345371},
  file      = {AbdulRahman12flexRampCAISO.pdf:AbdulRahman12flexRampCAISO.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  issn      = {1944-9925},
  keywords  = {optimisation;power generation scheduling;power grids;power markets;power system reliability;CAISO market;California;flexible ramp constraint;generation fleet flexibility;grid operations;market optimization;power system reliability;renewable integration;renewable portfolio standard;system ramping capability;Mathematical model;Optimization;Probabilistic logic;Real-time systems;Reliability;Supply and demand;Uncertainty;Day-Ahead Market;Flexible Ramp Constraint;Hour-Ahead Scheduling Process;Imbalance Energy;Load Following;Real-Time Market;Regulation Service;Renewable Portfolio Standard;System Flexibility;Variable Energy Resource},
  owner     = {sotterson},
  timestamp = {2013.10.11},
}

@TechReport{Koenker13DistributionalvsQuantile,
  author    = {Koenker, Roger and Leorato, Samantha and Peracchi, Franco},
  title     = {Distributional vs. Quantile Regression},
  year      = {2013},
  type      = {CEIS Working Paper No. 300. Available at SSRN: http://ssrn.com/abstract=2368737 or http://dx.doi.org/10.2139/ssrn.2368737},
  abstract  = {Given a scalar random variable Y and a random vector X defined on the same probability space,
the conditional distribution of Y given X can be represented by either the conditional distribution
function or the conditional quantile function. To these equivalent representations correspond two
alternative approaches to estimation. One approach, distributional regression (DR), is based on
direct estimation of the conditional distribution function; the other approach, quantile regression
(QR), is instead based on direct estimation of the conditional quantile function. Indirect estimates
of the conditional quantile function and the conditional distribution function may then be obtained
by inverting the direct estimates obtained from either approach. Despite the growing attention to
the DR approach, and the vast literature on the QR approach, the link between the two approaches
has not been explored in detail. The aim of this paper is to fill-in this gap by providing a better
understanding of the relative performance of the two approaches, both asymptotically and in finite
samples, under the linear location model and certain types of heteroskedastic location-scale models.

KEYWORDS. Quantile regression, distributional regression, functional Delta-method, asymptotic
relative efficiency, linear location model, location-scale models},
  comment   = {Can regress to estimate either quantiles (thresholds at given exceedance probabilities) or distributions (probabilities at given exceedance thresholds). The methods are mostly equivalent and can be interchanged. However, direct DISTRIBUTIONAL regression seems better at extreme quantiles.

* original paper (clearer than this one): Foresi95condDistRetDR
* Chernozhukov13infCntrFactDist also compares distribution regression vs. quantile regression
* distribution regression can easily model point masses (ends of wind power curve. See
 Chernozhukov13infCntrFactDist)
.  I think this is because CDF can have sharp derivatives, making point masses in a density.

Implementation advantage: standard LSQ algorithms, high dims (I say)
* DR predicts a binary threshold indicator so (I say) it can use the huge library of standard LSQ classification algorithms e.g. an NN.
* this would also make it more suitable than splineQR or linear QR for high dimensional inputs
* use invertible NN instead of logit?
 - e.g. Baird05oneStepInvrtNN or Rippel13highDimProbDeepNN
 - an NN w/ LSQ cost function estimates the mean of a binary value, which the logit used here does and which is required:
https://books.google.de/books?id=U_b2BwAAQBAJ&pg=PA5&lpg=PA5&dq=neural+network+for+estimating+conditional+mean&source=bl&ots=hLZiI0M6Tp&sig=hljm-efcyj4eiRoXF9KVHzsgZP0&hl=en&sa=X&redir_esc=y\#v=onepage&q=neural%20network%20for%20estimating%20conditional%20mean&f=false

From this paper:
* DR is better for censored distributions (p. 2)
 - but what does that mean? Does it mean "outlies were removed" or that the distribution is bounded (like wind power)
 - If each quantile is separately estimated, can't it be as jaggy as needed?

Estimating distribution of a vector random variable
* DR can estimate a random vector
* no obvious way for QR to estimate quantiles for non-scalar dependent variables
* good for grid node probabilistic forecast, where vector elements are grid nodes?

For linear location model
* DR is better near (0,1) quantiles
* for low R^2, QR same as DR but QR is "more efficient"
* for high R^2, QR is better than DR

Logistic regression for threshold probs.
* what they use in this paper
* I -think- they used Poisson link function (Eilers15twentyYrsPspline says CLM is better).

See also: Chernozhukov13infCntrFactDist for more on DR},
  doi       = {10.2139/ssrn.2368737},
  file      = {Koenker13DistributionalvsQuantile.pdf:Koenker13DistributionalvsQuantile.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {CEIS Working Paper},
  timestamp = {2015.07.03},
  url       = {http://ssrn.com/abstract=2368737},
}

@Article{Higham02nearestCorrMat,
  author    = {Higham, Nicholas J.},
  title     = {Computing the nearest correlation matrix -- a problem from finance},
  journal   = {IMA Journal of Numerical Analysis},
  year      = {2002},
  volume    = {22},
  number    = {3},
  pages     = {329--343},
  abstract  = {Given a symmetric matrix, what is the nearest correlation matrix -- that is, the nearest symmetric positive semidefinite matrix with unit diagonal? This problem arises in the finance industry, where the correlations are between stocks. For distance measured in two weighted Frobenius norms we characterize the solution using convex analysis. We show how the modified alternating projections method can be used to compute the solution for the more commonly used of the weighted Frobenius norms. In the finance application the original matrix has many zero or negative eigenvalues; we show that for a certain class of weights the nearest correlation matrix has correspondingly many zero eigenvalues and that this fact can be exploited in the computation.},
  comment   = {Given a symmetric matrix, how to compute the nearest valid correlation matrix. Has Matlab.

Useful for:
* empirical correlations of large, low rank matrices.
* simulating random Gaussian systems, where correlations are random realizations that must be fixed up to work
* possibly for agglomeratively weather regimes based on sliding window correlation matrices.
* possibly for spatio-temporal analog ensemble selection (but may want to do something w/ PLS or equivalent first, so there's a relationship w/ the predictand).

Matlab: http://www.mathworks.com/matlabcentral/fileexchange/21486-finding-a-similar-valid-correlation-matrix
https://nickhigham.wordpress.com/2013/02/13/the-nearest-correlation-matrix/},
  doi       = {10.1093/imanum/22.3.329},
  eprint    = {http://imajna.oxfordjournals.org/content/22/3/329.full.pdf+html},
  file      = {Higham02nearestCorrMat.pdf:Higham02nearestCorrMat.pdf:PDF},
  groups    = {Ensemble, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2012.03.09},
  url       = {http://imajna.oxfordjournals.org/content/22/3/329.abstract},
}

@Article{Li11trendSVD,
  author    = {Li, Gen and Ren, Baohua and Zheng, Jianqiu and Yang, Chengyun},
  title     = {Trend singular value decomposition analysis and its application to the global ocean surface latent heat flux and SST anomalies},
  journal   = {Journal of Climate},
  year      = {2011},
  volume    = {24},
  number    = {12},
  pages     = {2931--2948},
  abstract  = {Given the complexity of trends in the actual climate system, distinguishing between different trends and
different trend modes is important for climate research. This study introduces a new method called ??trend
singular value decomposition (TSVD) analysis,?? which is designed for systematically extracting coupled trend
modes, albeit small, by performing an eigenanalysis of the inverse-rank covariance matrix between two fields.
Applications to simple time series models and annual mean surface latent heat flux (LHF) and SST data for
1958?2006 are presented and discussed. Results show that the TSVD analysis can capture different coherent
trends into different leading modes. The first TSVD mode between the global LHF and SST anomalies,
similar to the first conventional SVD mode, generally represents a large-scale increasing LHF trend induced
by a warming SST trend; whereas, interestingly, unlike the second SVD mode that is mainly associated with
the familiar ENSO, the second TSVD mode is mainly associated with the Pacific decadal oscillation (PDO).
TSVD analysis casts the (global) long-term and (Pacific) decadal trends into the leading two modes, re-
spectively. Compared to SVD analysis, the advantages of the TSVD analysis in detecting coupled low-fre-
quency modes are even more evident in the tropical Pacific (TP), where the coherent trend signals (i.e., the
long-term trends and the decadal trends) are smaller than the ENSO-related signals. Thus, TSVD analysis
performs better than SVD analysis when focusing on trends rather than on maximum covariance patterns,
particularly on relatively small coherent trend patterns, such as the coupled long-term trends and decadal
trends in the TP.},
  comment   = {Use to identify variables that trend together, as opposed to have maxium covariance together (this is because the rank transform enforces montonicity).  Maybe good for ModernWindABS (Dienst16AnomOffshrWnd), where the idea was to detect trends between measurements of different components.  This is also somehow related to a copula....

Good idea to check articles that cite this one, since it's kinda old.},
  file      = {Li11trendSVD.pdf:Li11trendSVD.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2016.12.09},
  url       = {http://journals.ametsoc.org/doi/full/10.1175/2010JCLI3743.1},
}

@Article{Maurer09dimSTctlRsrvProb,
  author    = {Maurer, Christoph and Krahl, Simon and Weber, Holger},
  title     = {Dimensioning of secondary and tertiary control reserve by probabilistic methods},
  journal   = {European Transactions on Electrical Power},
  year      = {2009},
  volume    = {19},
  number    = {4},
  pages     = {544--552},
  month     = {may},
  abstract  = {Given the rising share of intermittent generation out of renewable energy sources on the one hand and the increased regulatory efforts to lower transmission costs and tariffs on the other hand, the optimal dimensioning of necessary control reserve has gained additional importance during the last years. Grid codes like the UCTE Operation Handbook do not provide definitive and unambiguous methods for dimensioning of secondary and tertiary control reserves. This paper therefore presents a method which calculates the necessary control reserve considering all important drivers for power imbalances like power plant outages, load variations and forecast error. For dimensioning, a probabilistic criterion, the accepted probability of insufficient control reserve, is used. Probability density functions of control area imbalances are calculated using a convolution algorithm. This paper provides analyses for a stylised example system to demonstrate the capabilities of the method. In a sensitivity analysis the impact of drivers like plant failures and forecast errors of load and generation is shown. The presented method is used by transmission system operators and regulatory authorities to determine and substantiate the necessary amount of control reserve.},
  comment   = {How Germany allocates control reserve, according to Dominik J.

Assumes error distributions are independent and then convolves them.  And improvement would be dependent convolution, as in: Zhang16copulaConvPowSys},
  doi       = {10.1002/etep.326},
  file      = {Maurer09dimSTctlRsrvProb.pdf:Maurer09dimSTctlRsrvProb.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2017.01.17},
}

@Article{Westacott16gisPVdeployUK,
  author         = {Westacott, Paul and Candelise, Chiara},
  title          = {A Novel Geographical Information Systems Framework to Characterize Photovoltaic Deployment in the UK: Initial Evidence},
  journal        = {Energies},
  year           = {2016},
  volume         = {9},
  number         = {1},
  issn           = {1996-1073},
  abstract       = {Globally, deployment of grid-connected photovoltaics (PV) has increased dramatically in recent years. The UK has seen rapid uptake reaching over 500,000 installations totalling 2.8 GWp by 2013. PV can be installed in different market segments (domestic rooftop, non-domestic rooftop and ground-mounted “solar-farms”) covering a broad range of system sizes in a high number of locations. It is important to gain detailed understanding of what grid-connected PV deployment looks like (e.g., how it deployed across different geographic areas and market segments), and identify the major drivers behind it. This paper answers these questions by developing a novel geographical information systems (GIS)-framework—the United Kingdom Photovoltaics Database (UKPVD)—to analyze temporal and spatial PV deployment trends at high resolution across all market segments. Results show how PV deployment changed over time with the evolution of governmental PV policy support. Then spatial trends as function of local irradiation, rurality (as a proxy of building and population density) and building footprint (as a proxy for roof-area) are analyzed. We find in all market segments, PV deployment is strongly correlated with the level of policy support. Furthermore, all markets show a preference to deploy in rural areas and those with higher irradiation. Finally, local clustering of PV in all market segments was observed, revealing that PV is not spread evenly across areas. This work reveals the complex nature of PV deployment, both spatially and by market segment, reinforcing the need capture this through mapping.},
  article-number = {26},
  doi            = {10.3390/en9010026},
  file           = {:Westacott16gisPVdeployUK.pdf:PDF},
  url            = {http://www.mdpi.com/1996-1073/9/1/26},
}

@TechReport{Crawford16loadFrcstERCOT,
  author      = {Todd Crawford},
  title       = {ERCOT Load Forecasting},
  institution = {The Weather Company},
  year        = {2016},
  abstract    = {Good load forecasts are strongly dependent upon good weather forecasts, and The Weather Company?s weather forecasting engine is unsurpassed in that regard. Learn more in this white paper.

When a company?s profitability is dependent on weather, accuracy and insight can be
critical to success. The Weather Company, an IBM Business (Weather) has recently
made significant investments in both (a) an improved weather forecasting system and (b)
data science capabilities. The former allows for the most accurate, timely, and spatially
resolute weather forecasts in the industry, while expertise in the latter allows us to convert
these accurate weather forecasts into user-friendly products for our clients in the utility
and energy trading businesses. One of these exciting new products is a load forecasting
module for the ERCOT region, with new forecasts produced each hour, at hourly resolution
out to 15 days.
The Weather Company load forecasting algorithm exhibits errors (expressed with the
MAPE metric) of generally less than 2% (for the aggregate ERCOT region) during the
first three days of the forecast, rising to 3-4% by day 6 and 4-5% by day 9. A comparison
with archived ERCOT-produced load forecasts out to 180 hours indicates that The
Weather Company forecasts had lower errors for 98% of the forecast hours, with relative
improvements ranging from 5-20%.},
  comment     = {IBM's new load forecast for Texas ERCOT uses NWP grid and 2600 NN's.  Seems like a CNN and maybe RNN could be better?  Goes from 1 hour (?)  to 15 days in 1 hour increments.  I suppose the huge # of NN's is partly b/c they have a separate NN per horizon.

There does not seem to be a spatial component in the NN's so would miss fronts, etc.  But I haven't read this whitepaper yet.

Wang16loadFrcstRcncyBigDat thinks that separate models may not be better than single model w/ interactions.},
  file        = {Crawford16loadFrcstERCOT.pdf:Crawford16loadFrcstERCOT.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2017.03.13},
  url         = {https://business.weather.com/resource/ercot-load-forecasting-white-paper},
}

@Article{Murgia17googleDeepMindUKgrid,
  author    = {Madhumita Murgia and Nathalie Thomas},
  title     = {DeepMind and National Grid in AI talks to balance energy supply},
  journal   = {Financial Times},
  year      = {2017},
  month     = mar,
  abstract  = {Google?s machine learning technology likely to better predict UK electricity demand},
  comment   = {Google is in talks with the UK's National Grid (UK's TSO?) about estimating demand.  They want to predict peaks in demand and supply, so this could be a net demand forecast, or at least a joint demand and RES forecast.

Google wants to use DeepMind, which has saved 40\% cooling costs (15\% electricity savings) in Google's data centers.  This might be 100's of millions of dollars in savings  for Google over several years.  DeepMind is what won the AlphaGo tournament (ERCIMnews16spclIssueMachLrn) and that was deep reinforcement learning (Burger16googleDeepMindAphaGoReinfLrn)

Google Exec says there's no reason you can't treat a whole country the same way.},
  file      = {Murgia17googleDeepMindUKgrid.pdf:Murgia17googleDeepMindUKgrid.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.26},
  url       = {https://www.ft.com/content/27c8aea0-06a9-11e7-97d1-5e720a26771b},
}

@PhdThesis{Maier10clustNhbds,
  author    = {Maier, Markus Martin},
  title     = {Clustering with neighborhood graphs},
  year      = {2010},
  abstract  = {Graph clustering methods are defined for general weighted graphs. If data is given in the form of points and distances between them, a neighborhood graph, such as the r-graph or kNN-graphs, is constructed and graph clustering is applied to this graph. We investigate the influence of the type and parameter of the neighborhood graph on the clustering results, when n sample points are drawn independently from a density in Euclidean space. In Chapter 2 we study "cluster identification';: the true clusters are the connected components of density level sets and a cluster is identified if its points are a connected component of the graph. We compare (modifications of) the mutual and the symmetric kNN-graph. They behave differently if the goal is to identify the "most significant'; clusters, whereas there is no difference if the goal is to identify all clusters. We give the range of k for which the clusters are identified in the graphs and derive the optimal choice of k, which, surprisingly, is linear in n. In Chapter 3 we study the convergence of the normalized cut (Ncut) and the ratio cut as n -> for cuts in the kNN- and the r-graph induced by a hyperplane. The limits differ; consequently Ncut on a kNN-graph does something systematically different than Ncut on an r-graph! This can be experimentally observed on toy and real data sets. Therefore, graph clustering criteria cannot be studied independently of the type of graph to which they are applied.},
  comment   = {Spectral clustering review, how to pick k in k-nearest neighbhor (KNN) graphs. Also r-graphs, which seem to be something like knn graphs. Guy's advisor was Bernhard Schoelkopf, the SVN researcher.

Could be useful for either Kraskov knn mutual information estimator, or for clustering regression.},
  file      = {Maier10clustNhbds.pdf:Maier10clustNhbds.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.04.04},
  url       = {http://scidok.sulb.uni-saarland.de/volltexte/2010/2966/},
}

@Article{Marttinen09graphVARlrnUlag,
  author    = {Marttinen, Pekka and Corander, Jukka},
  title     = {{Bayes}ian learning of graphical vector autoregressions with unequal lag-lengths},
  journal   = {Machine Learning},
  year      = {2009},
  volume    = {75},
  number    = {2},
  pages     = {217--243},
  issn      = {0885-6125},
  abstract  = {Graphical modelling strategies have been recently discovered as a versatile tool for analyzing multivariate stochastic processes. Vector autoregressive processes can be structurally represented by mixed graphs having both directed and undirected edges between the variables representing process components. To allow for more expressive vector autoregressive structures, we consider models with separate time dynamics for each directed edge and non-decomposable graph topologies for the undirected part of the mixed graph. Contrary to static graphical models, the number of possible mixed graphs is extremely large even for small systems, and consequently, standard Bayesian computation based on Markov chain Monte Carlo is not in practice a feasible alternative for model learning. To obtain a numerically efficient approach we utilize a recent Bayesian information theoretic criterion for model learning, which has attractive properties when the potential model complexity is large relative to the size of the observed data set. The performance of our method is illustrated by analyzing both simulated and real data sets. Our simulation experiments demonstrate the gains in predictive accuracy which can obtained by considering structural learning of vector autoregressive processes instead of unstructured models. The analysis of the real data also shows that the understanding of the dynamics of a multivariate process can be improved significantly by considering more flexible model classes.},
  comment   = {Builds graph of multivariate time series process w/ some causal edges, loops, variable num. of lags

Performance
-- improves performance over unstructured models when have less data

-- fMRI lagged graph learning probably is too high dim. for this technique

-- one hour on 2.2G workstation (20 variables, in MATLAB)
-- automatically picked up a 7 day lag in pollution data (matching traffic patterns)

Edge building
-- based on linear dependence
-- use partial correlation Directed edges
-- direction from granger causality (timing of dependence effects)

-- are the only ones with lags
---- but lags must be contiguous; not isolated
-- can be inferred w/ high accuracy on small data sets
-- WAY more directed graph options than undirected for same num. of vertices

-- are between "components" (cliques of some kind?)

Undirected edges
-- "contemporaneous" partial correlations (instantaneous)
-- are only within "components"
-- non-causal
-- harder to learn

Fit/complexity tradeoff: optimize BEC, (Bayesian Entropy Critereon)

-- a new (2006) kind-of BIC
-- "takes the curvature of the log likelihood into account"
-- nonlinear w.r.t., unlike BIC, AIC, etc which makes it better for small num. of obs.
--on tests, BEC is always better than BIC AIC, etc.
-- Should I try to borrow this for other problems?

Graph search
-- subtracts or adds edges w/ alternating approaches
---- greedy: optimize BEC by considering each possible edge once per iter.
---- stochastic (MCMC): values of edges are drawn from a distribution

-- converges rapidly},
  doi       = {10.1007/s10994-009-5101-2},
  file      = {Marttinen09graphVARlrnUlag.pdf:Marttinen09graphVARlrnUlag.pdf:PDF},
  groups    = {Read},
  location  = {Hingham, MA, USA},
  owner     = {scot},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2010.07.30},
}

@TechReport{Kubiniec19solarSatelliteTuneGrndTR,
  author      = {Alex Kubiniec and Julie Chard and Patrick Keelin and Richard Perez},
  title       = {Site-Specific Solar Resource Assessment: SolarAnywhere Satellite Data tuned to GroundWork Data},
  institution = {Clean Power Research},
  year        = {2019},
  abstract    = {GroundWork Renewables, Inc. and Clean Power Research are pleased to provide this resource assessment for the 
Athos solar monitoring station (SMS), in Imperial County, CA. This resource assessment reports overall quality of 
the ground-based measurements, and pairs the ground-based irradiance measurements with historical satellite 
data. Tuning of the historical data to the ground-based data results in a Typical Global Horizontal Irradiance Year 
(TGY) dataset with reduced overall uncertainty relative to ground-based or satellite measurements alone. },
  comment     = {One of 3 papers describing CPR's solar assessment tuning.  Sent by Patrick.

Uncertainty described in:
Habte15radiometUncertGUM
Perez02newMdllSatelliteIrrad (I think this is reference 2 in this paper)
Perez04satelliteIrradTerrain (ref. 3)
},
  file        = {:Kubiniec19solarSatelliteTuneGrndTR.pdf:PDF},
}

@InProceedings{Choo12HeteroDatFuseMDS,
  author       = {Choo, Jaegul and Bohn, Shawn and Nakamura, Grant and White, Amanda M and Park, Haesun},
  title        = {Heterogeneous Data Fusion via Space Alignment Using Nonmetric Multidimensional Scaling.},
  booktitle    = {SDM},
  year         = {2012},
  pages        = {177--188},
  organization = {SIAM},
  abstract     = {Heterogeneous data sets are typically represented in different
feature spaces, making it difficult to analyze relationships
spanning different data sets even when they are semantically
related. Data fusion via space alignment can
remedy this task by integrating multiple data sets lying in
different spaces into one common space. Given a set of reference
correspondence data that share the same semantic
meaning across different spaces, space alignment attempts
to place the corresponding reference data as close together
as possible, and accordingly, the entire data are aligned in
a common space. Space alignment involves optimizing two
potentially conflicting criteria: minimum deformation of the
original relationships and maximum alignment between the
different spaces. To solve this problem, we provide a novel
graph embedding framework for space alignment, which converts
each data set into a graph and assigns zero distance
between reference correspondence pairs resulting in a single
graph. We propose a graph embedding method for fusion
based on nonmetric multidimensional scaling (MDS).
Its criteria using the rank order rather than the distance allows
nonmetric MDS to effectively handle both deformation
and alignment. Experiments using parallel data sets demonstrate
that our approach works well in comparison to existing
methods such as constrained Laplacian eigenmaps, Procrustes
analysis, and tensor decomposition. We also present
standard cross-domain information retrieval tests as well as
interesting visualization examples using space alignment.},
  comment      = {Non-classical MDS is quite good at aligning speech features with the corresponding text features. Might be a good way to handle missing data (convert missing and non-missing to same space?) . Or maybe weather measurements from some set of masts with power production from some set of nearby farms. Could also be a way to do feature dimension reduction.},
  doi          = {10.1137/1.9781611972825.16},
  file         = {Choo12HeteroDatFuseMDS.pdf:Choo12HeteroDatFuseMDS.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2014.06.26},
}

@InProceedings{Radovanovic09knnHubEmrg,
  author       = {Radovanovi{\'c}, Milo{\v{s}} and Nanopoulos, Alexandros and Ivanovi{\'c}, Mirjana},
  title        = {Nearest neighbors in high-dimensional data: The emergence and influence of hubs},
  booktitle    = {Proceedings of the 26\textsuperscript{th} Annual International Conference on Machine Learning},
  year         = {2009},
  pages        = {865--872},
  organization = {ACM},
  abstract     = {High dimensionality can pose severe difficulties, widely recognized as different aspects of the curse of dimensionality. In this paper we study a new aspect of the curse pertaining to the distribution of k-occurrences, i.e., the number of times a point appears among the k nearest neighbors of other points in a data set. We show that, as dimensionality increases, this distribution becomes considerably skewed and hub points emerge (points with very high k-occurrences). We examine the origin of this phenomenon, showing that it is an inherent property of high-dimensional vector space, and explore its influence on applications based on measuring distances in vector spaces, notably classification, clustering, and information retrieval.},
  comment      = {Maybe first explanation of the high dim hub errors in knn space},
  file         = {Radovanovic09knnHubEmrg.pdf:Radovanovic09knnHubEmrg.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2014.04.04},
  url          = {http://dl.acm.org/citation.cfm?id=1553485},
}

@InProceedings{Alfi16uncertSatteliteGrnd,
  author    = {J. {Alfi} and A. {Kubiniec} and G. {Mani} and J. {Christopherson} and Y. {He} and J. {Bosch}},
  title     = {Importance of input data and uncertainty associated with tuning satellite to ground solar irradiation},
  booktitle = {Proc. IEEE 43rd Photovoltaic Specialists Conf. (PVSC)},
  year      = {2016},
  pages     = {0301--0305},
  month     = jun,
  abstract  = {High quality satellite solar irradiation data is used throughout the solar industry to perform energy estimates. The uncertainty of the raw satellite data has been shown to be low. Ground data is often used to correct satellite data but determining the uncertainty of the final dataset could be challenging since the traditional statistical uncertainty and error calculation methods have proven to be unrepresentative. In this paper the limitations of traditional statistical methods are explored along with alternative approaches to calculate a more representative uncertainty value for a long term dataset resulting from ground corrected satellite data.},
  comment   = {
CPR method (which Patrick later said is not actually used)
* Inputs
  - SURFRAD and ISIS groun dstations
  - 10+ years
  - good sensors, geographically dispersed
* Data Preprocessing
  - decimation: 1min --> 1hr
  - remove data that's bad (doesn't agree w/ solar anywhere, in some sense)
  - remove night
* Sliding window calibration
  - follows Kankiewicz14solarUncertRsrcTune
  - a binned bias and histogram (I think) correction
  - cloudy periods removed (I think they could stay, if a nonlinear calibration was used)
* Accuracy
  - I'm not clear here on in the test and what the training data was.  May be test on train?
  - 11-12 months of ground data was the max length needed for something

New Uncertainty Calculation (Proposed methodology)
(seems to be a bootstrap estimate of P50 (median).  Should be subsampled bootstrap.  
It is also unclear what the test and train data were)
* "data pairs" chose at random (input/output pairs?)  
   - If so, temporal autocorr will be bad
   - should have done, at least, subsampling bootstrap
* smaller sample sizes are given "linear weighting" whatever that means
* groud corrections done on random samples
  - correction done with OLS
  - so have they abandoned Kankiewicz14solarUncertRsrcTune ?
* correction done on "long term satellite data," which is not defined, but I suppose this means "backwards in time"
* long term mean of annual P50 (median) was generated.
* do this 50K times or until convergence
* Error is diff from annual PV across "entire dataset"
  - is expresses as relative standard error, RSE
  - is standard deviation / mean

Addition of unknown instrument error
* is estimated by EDF Renewable Energy (company)
* assumes normal distribution and independence
* normal distribution might be OK but could have used a normal copula instead
* normal error variance is sum of ground and satellite error variance
* which is not correct since a ground instrument error will produce a satellite error, since the ground value is used to correct the satellite.

This was one of 3 papers describing CPR's solar assessment tuning.  Sent by Patrick, who later said he's not sure if CPR actually does this.


},
  doi       = {10.1109/PVSC.2016.7749598},
  file      = {:Alfi16uncertSatteliteGrnd.pdf:PDF},
  keywords  = {solar power satellites, solar power stations, statistical analysis, sunlight, solar industry, energy estimation, raw satellite data uncertainty, statistical uncertainty method, error calculation method, ground corrected satellite solar irradiation data, satellite tuning, Uncertainty, Tuning, Standards, Satellites, Meteorology, Correlation, Rocks},
}

@InProceedings{Tran03knnDensClust,
  author    = {Tran, T.N. and Wehrens, R. and Buydens, L.M.C.},
  title     = {Knn density-based clustering for high dimensional multispectral images},
  booktitle = {, 2\textsuperscript{nd} GRSS/ISPRS Joint Workshop on Remote Sensing and Data Fusion over Urban Areas},
  year      = {2003},
  pages     = {147--151},
  abstract  = {High resolution and high dimension satellite images cause problems for clustering methods due to clusters of different sizes, shapes and densities. The most common clustering methods, e.g. K-means and ISODATA, do not work well for such kinds of datasets. In this work, density estimation techniques and density-based clustering methods are exploited. Density-based clustering is well known in data mining to classify a data set based on its density parameters, where lower density areas separate high-density areas, although it can only work with a simple data set in which cluster densities are not very different. Out contribution is to propose the k nearest neighbor (knn) density-based rule for high dimensional dataset and to develop a new knn density-based clustering (KNNCLUST) for such complex dataset. KNNCLUST is stable, clear and easy to understand and implement. The number of clusters is automatically determined. These properties are illustrated by the segmentation of a multispectral image of a floodplain in the Netherlands.},
  comment   = {Simple KNN clustering with Matlab. High dim performance claimed but not demonstrated. Maybe probabilities produced would be good for local linear neighborhood weights? Has Matlab?

Updated in: Tran06knnDensClustHiDim
Matlab: http://www.cac.science.ru.nl/research/software/tnthanh/knnclust.m
(code comments say it's for this paper but it really looks like it's for Tran06knnDensClustHiDim)

PROBLEMS WITH THE ALTERNATIVES
* kmeans and ISODATA are noise/outlier sensitive, unstable
* agglom clust more stable but too much CPU
* single link (whatever that is) is unstable, bad with weird cluster shapes
* The usual density methods: OK w/ weird shapes but must be simple, similar. Also has probs w/ high dim.

ALGORITHM for KNN density-based clustering (KNNCLUST)
 foreach point
 - find the k nearest neighbors (K is a tweak param)
 - assign it to the cluster that contains the most of those neighbors (with a tie breaker rule)
 Repeat until reassignment stops (or gets small)

GENERAL PROPERTIES of KNNCLUST
* merging done w/ KNN rule, which is related to density estimation
* can merge or reassign each point on each iteration (not clear how, but matlab is given to look at)
* seems like merging this way would over-favor clusters with a lot of points?
 -- should include distance somehow
 -- maybe merge based on average distance of the k nearest points in each cluster?
 -- but maybe that would make things spherical, like kmeans??
* high dim performance not demonstrated: PCA down to 4 dims
* is a super CPU hog (they're working on a faster spatial algorithm)
* not auto-k: you must define that yourself
* KNN not a good density estimator (GutierrezOsuna11kdnnLecNotes) but Trans says OK for clustering?
RESULTS
* works better than Kmeans on satellite image classification problem
* result is data order dependent but permutation tests show it's not too bad},
  doi       = {10.1109/DFUA.2003.1219976},
  file      = {Tran03knnDensClust.pdf:Tran03knnDensClust.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2014.04.04},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5731018},
}

@Article{Opgen-Rhein07rankShrink,
  author    = {Rainer Opgen-Rhein and Korbinian Strimmer},
  title     = {Accurate Ranking of Differentially Expressed Genes by a Distribution-Free Shrinkage Approach},
  journal   = {Statistical Applications in Genetics and Molecular Biology},
  year      = {2007},
  volume    = {6},
  number    = {1},
  abstract  = {High-dimensional case-control analysis is encountered in many different settings in genomics. In order to rank genes accordingly, many different scores have been proposed, ranging from ad hoc modifications of the ordinary t statistic to complicated hierarchical Bayesian models. Here, we introduce the ?shrinkage t? statistic that is based on a novel and model-free shrinkage estimate of the variance vector across genes. This is derived in a quasi-empirical Bayes setting. The new rank score is fully automatic and requires no specification of parameters or distributions. It is computationally inexpensive and can be written analytically in closed form. Using a series of synthetic and three real expression data we studied the quality of gene rankings produced by the ?shrinkage t? statistic. The new score consistently leads to highly accurate rankings for the complete range of investigated data sets and all considered scenarios for acrossgene variance structures. KEYWORDS: high-dimensional case-control data, James-Stein shrinkage, limited-translation, quasi-empirical Bayes, regularized t statistic, variance shrinkage},
  comment   = {The method implemented in the R package, corpcor * On average, it might be better than the lasso aproaches in Kramer09gaussGraphModelsLasso * Explains how to write the regression coefficient directly in terms of partial correlations},
  file      = {Opgen-Rhein07rankShrink.pdf:Opgen-Rhein07rankShrink.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.08.02},
  url       = {http://ideas.repec.org/a/bpj/sagmbi/v6y2007i1n9.html},
}

@Article{Allen13regPLSnmr,
  author    = {Allen, Genevera I and Peterson, Christine and Vannucci, Marina and Maleti{\'c}-Savati{\'c}, Mirjana},
  title     = {Regularized partial least squares with an application to NMR spectroscopy},
  journal   = {Statistical analysis and data mining},
  year      = {2013},
  volume    = {6},
  number    = {4},
  pages     = {302--314},
  abstract  = {High-dimensional data common in genomics, proteomics, and chemometrics often
contains complicated correlation structures. Recently, partial least squares (PLS) and
Sparse PLS methods have gained attention in these areas as dimension reduction techniques
in the context of supervised data analysis. We introduce a framework for
Regularized PLS by solving a relaxation of the SIMPLS optimization problem with
penalties on the PLS loadings vectors. Our approach enjoys many advantages including

flexibility, general penalties, easy interpretation of results, and fast computation
in high-dimensional settings. We also outline extensions of our methods leading to
novel methods for Non-negative PLS and Generalized PLS, an adaption of PLS for
structured data. We demonstrate the utility of our methods through simulations and
a case study on proton Nuclear Magnetic Resonance (NMR) spectroscopy data.
Keywords: sparse PLS, sparse PCA, NMR spectroscopy, generalized PCA, non-negative
PLS, generalized PLS},
  comment   = {Abstract and paper is from 2012 arXiv submission. Abstract looks the same as journal web page.},
  doi       = {10.1002/sam.11169/full},
  file      = {Allen13regPLSnmr.pdf:Allen13regPLSnmr.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.10.17},
}

@InProceedings{Wu16probSTwindPowRNNlstm,
  author    = {W. Wu and K. Chen and Y. Qiao and Z. Lu},
  title     = {Probabilistic short-term wind power forecasting based on deep neural networks},
  booktitle = {Proc. Int. Conf. Probabilistic Methods Applied to Power Systems (PMAPS)},
  year      = {2016},
  pages     = {1--8},
  month     = oct,
  abstract  = {High-precision wind power forecasting is an essential operation issue of power systems integrated with large numbers of wind farms. In addition to traditional forecasting methods, probabilistic forecasting is recognized as an optimal forecasting solution since it provides a wealth of valuable uncertainty information of wind power. In this paper, a novel approach based on deep neural networks (DNNs) for the deterministic short-term wind power forecasting of wind farms is proposed. DNN models including long short-term memory (LSTM) recurrent neural networks (RNNs) have achieved better results compared with traditional methods. Further, probabilistic forecasting based on conditional error analysis is also implemented. Favorable results of probabilistic forecasting are achieved owing to elaborate division of the conditions set based on cluster analysis. The performance of the proposed method is tested on a dataset of several wind farms in north-east China. Forecasting results are evaluated using different indices, which proves the effectiveness of the proposed method.},
  comment   = {Has some kind of regime learning too, I think.},
  doi       = {10.1109/PMAPS.2016.7764155},
  file      = {Wu16probSTwindPowRNNlstm.pdf:Wu16probSTwindPowRNNlstm.pdf:PDF},
  keywords  = {error analysis, load forecasting, pattern clustering, power engineering computing, recurrent neural nets, wind power plants, DNN model, LSTM RNN, cluster analysis, conditional error analysis, deep neural network, deterministic short-term wind power forecasting, long short-term memory recurrent neural network, north-east China, probabilistic forecasting, probabilistic short-term wind power forecasting, wind farm, Forecasting, Neural networks, Predictive models, Probabilistic logic, Random variables, Wind forecasting, Wind power generation, Deep neural networks, LSTM RNN, cluster analysis, conditional error analysis, probabilistic forecasts, wind power},
  owner     = {sotterson},
  timestamp = {2017.03.08},
}

@Article{Jose14TrimOpinPoolCal,
  author    = {Jose, Victor Richmond R and Grushka-Cockayne, Yael and Lichtendahl, Jr, Kenneth C},
  title     = {Trimmed Opinion Pools and the Crowd's Calibration Problem},
  journal   = {Management Science},
  year      = {2014},
  volume    = {60},
  number    = {2},
  pages     = {463--475},
  month     = feb,
  abstract  = {How can the accuracy of opinion-based probability forecasts be improved? A problem with using traditional opinion pools is that they can be poorly calibrated, tending toward underconfidence as the crowd's diversity increases. The authors propose the ?exterior-trimmed? opinion pool, in which the outliers are ignored, which decreases the pool's variance and improves its calibration. On the other hand, a linear opinion pool will remain overconfident when individuals are overconfident and not very diverse. For these situations, the authors suggest ?interior trimming? of forecasts with moderate means to reduce overconfidence. The insight for management: Disregarding some poll information can reduce bias and improve accuracy.

Keywords: trimming; probability forecasts; expert combination; linear opinion pool; underconfidence;
overconfidence; scoring rules; wisdom of crowds; diversity},
  comment   = {A kind of feature selection for summed probabilistic forecasts. When averaging probability forecasts, strategically eliminating outlier forecasts increases sharpness and eliminating some forecasts in the middle reduces overconfidence.

Possible uses:
* probabilistic multi-model forecasts
* ensemble forecast where each member has its own probabilistic forecast
* aggregated probability forecasts (forecasting the sum of of something from individual forecasts)
* upscaling

Related to: Lichtendahl13probOrQuantAvg},
  file      = {Jose14TrimOpinPoolCal.pdf:Jose14TrimOpinPoolCal.pdf:PDF},
  groups    = {Ensemble, Upscaling (prob), doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2014.07.15},
  url       = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2175093},
}

@InProceedings{Tomasev11knnHubsHighDim,
  author    = {Tomasev, N. and Mladenic, D.},
  title     = {Nearest Neighbor Voting in High-Dimensional Data: Learning from Past Occurrences},
  booktitle = {Data Mining Workshops (ICDMW), 2011 IEEE 11\textsuperscript{th} International Conference on},
  year      = {2011},
  pages     = {1215--1218},
  abstract  = {Hub ness is a recently described aspect of the curse of dimensionality inherent to nearest-neighbor methods. In this paper we present a new approach for exploiting the hub ness phenomenon in k-nearest neighbor classification. We argue that some of the neighbor occurrences carry more information than others, by the virtue of being less frequent events. This observation is related to the hub ness phenomenon and we explore how it affects high-dimensional k-nearest neighbor classification. We propose a new algorithm, Hub ness Information k-Nearest Neighbor (HIKNN), which introduces the k-occurrence informativeness into the hub ness-aware k-nearest neighbor voting framework. Our evaluation on high-dimensional data shows significant improvements over both the basic k-nearest neighbor approach and all previously used hub ness-aware approaches.},
  comment   = {Parameter free tunning of high dim knn helps with rare neighbhor classification. Seems better than previous approaches. Criticizes his own method: Tomasev11probKNNhubNaiv},
  doi       = {10.1109/ICDMW.2011.127},
  file      = {Tomasev11knnHubsHighDim.pdf:Tomasev11knnHubsHighDim.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.04.04},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137521},
}

@Article{Perea12hydroSchedIberdrola,
  author    = {Perea, A and Bellido, R and Sanz, P and L{\'o}pez, E and Latorre, JM and Ramos, A and Cerisola, S},
  title     = {Medium-term Hydro-scheduling in Iberdrola},
  year      = {2012},
  abstract  = {Hydro power is a renewable source of energy that plays a key role in electric power systems, especially due to its flexibility and to its ability to allow the integration of other intermittent renewable sources. The medium-term hydro-scheduling is a very complex task that involves a great variety of processes and variables, some of which are considered stochastic, because of their uncertainty in the medium term. Two combined methods are used by Iberdrola in medium-term hydro-scheduling: stochastic optimization and simulation. This paper briefly describes these processes and shows two examples of applications to hydro management. One is referred to the continuous task of scheduling, and the other is an example of maintenance planning. Nevertheless these models have also been used for designing and upgrading of hydropower plants, international river agreements, analysis of ecological flows, etc.},
  comment   = {Iberdrola uses stochastic optimization to schedule its hydro.},
  file      = {Perea12hydroSchedIberdrola.pdf:Perea12hydroSchedIberdrola.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.02},
  url       = {http://www.iit.upcomillas.es/aramos/Ramos_CV.htm},
}

@Article{Brochero11ensSelGreedy,
  author    = {Brochero, D. and Anctil, F. and Gagn\'e, C.},
  title     = {Simplifying a hydrological ensemble prediction system with a backward greedy selection of members: Part 1: Optimization criteria},
  journal   = {Hydrology and Earth System Sciences},
  year      = {2011},
  volume    = {15},
  number    = {11},
  pages     = {3307--3325},
  abstract  = {Hydrological Ensemble Prediction Systems (HEPS), obtained by forcing rainfall-runoff models with Meteorological Ensemble Prediction Systems (MEPS), have been recognized as useful approaches to quantify uncertainties of hydrological forecasting systems. This task is complex both in terms of the coupling of information and computational time, which may create an operational barrier. The main objective of the current work is to assess the degree of simplification (reduction of the number of hydrological members) that can be achieved with a HEPS configured using 16 lumped hydrological models driven by the 50 weather ensemble forecasts from the European Centre for Medium-range Weather Forecasts (ECMWF). Here, Backward Greedy Selection (BGS) is proposed to assess the weight that each model must represent within a subset that offers similar or better performance than a reference set of 800 hydrological members. These hydrological models' weights represent the participation of each hydrological model within a simplified HEPS which would issue real-time forecasts in a relatively short computational time. The methodology uses a variation of the k-fold cross-validation, allowing an optimal use of the information, and employs a multi-criterion framework that represents the combination of resolution, reliability, consistency, and diversity. Results show that the degree of reduction of members can be established in terms of maximum number of members required (complexity of the HEPS) or the maximization of the relationship between the different scores (performance).},
  comment   = {Backwards ensemble selection and weighting w/ multi-function criteria (resolution, ...). Criteria assumes another giant ensemble set that is being matched.

Improved genetic algorithm in: Brochero13ensSelEvoMultipOpt (maybe read that first)

Maybe useful for adaboost stuff.

Evaluation is here: Simplifying a hydrological ensemble prediction system with a backward greedy selection of members ? Part 2: Generalization in time and space
http://www.hydrol-earth-syst-sci.net/15/3327/2011/hess-15-3327-2011.html},
  doi       = {10.5194/hess-15-3307-2011},
  file      = {Brochero11ensSelGreedy.pdf:Brochero11ensSelGreedy.pdf:PDF},
  groups    = {Ensemble, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2014.01.29},
  url       = {http://www.hydrol-earth-syst-sci.net/15/3307/2011/},
}

@Article{Kalteh09imputMissPrecip,
  author    = {Aman Mohammad Kalteh and Peder Hjorth},
  title     = {Imputation of missing values in a precipitation runoff process database},
  journal   = {Hydrology Research},
  year      = {2009},
  volume    = {40},
  number    = {4},
  pages     = {420--432},
  abstract  = {Hydrologists are often faced with the problem of missing values in a precipitation?runoff process database to construct runoff prediction models. They tend to use simple and naive methods to deal with the problem of missing data. Thus far, the common practice has been to discard observations with missing values. In this paper, we present some statistically principled methods for gap filling and discuss the pros and cons of these methods. We employ and discuss imputations of missing values by means of self-organizing map (SOM), multilayer perceptron (MLP), multivariate nearest-neighbor (MNN), regularized expectation?maximization algorithm (REGEM) and multiple imputation (MI) in the context of a precipitation?runoff process database in northern Iran in order to construct a serially complete database for analyses such as runoff prediction. In our case, the SOM and MNN tend to give similar and robust results. REGEM and MI build on the assumption of multivariate normal data, which we don?t seem to have in one of our cases. MLP tends to produce inferior results because it fragments the data into 68 different models. Therefore, we conclude that it makes most sense to use either the computationally simple MNN method or the more demanding SOM},
  comment   = {Compares several imputation methods, pointers to matlab source too. Mark Stoelinga is experimenting with this method for missing data fill-in: http://www.gps.caltech.edu/~tapio/imputation/},
  doi       = {10.2166/nh.2009.001},
  file      = {Kalteh09imputMissPrecip.pdf:Kalteh09imputMissPrecip.pdf:PDF},
  keywords  = {data fill in, imputation methods: SOM, MLP, MNN, REGEM, MI, missing values, serially complete data},
  owner     = {sotterson},
  timestamp = {2009.09.11},
}

@Article{Wood03ThinPltRgrsSpln,
  author    = {Wood, Simon N},
  title     = {Thin plate regression splines},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year      = {2003},
  volume    = {65},
  number    = {1},
  pages     = {95--114},
  abstract  = {I discuss the production of low rank smoothers for d >= 1 dimensional data, which
can be fitted by regression or penalized regression methods. The smoothers are constructed
by a simple transformation and truncation of the basis that arises from the solution of the thin
plate spline smoothing problem and are optimal in the sense that the truncation is designed to
result in the minimum possible perturbation of the thin plate spline smoothing problem given the
dimension of the basis used to construct the smoother. By making use of Lanczos iteration the
basis change and truncation are computationally efficient. The smoothers allow the use of approximate
thin plate spline models with large data sets, avoid the problems that are associated
with ?knot placement? that usually complicate modelling with regression splines or penalized
regression splines, provide a sensible way of modelling interaction terms in generalized additive
models, provide low rank approximations to generalized smoothing spline models, appropriate
for use with large data sets, provide ameans for incorporating smooth functions of more than one
variable into non-linear models and improve the computational efficiency of penalized likelihood
models incorporating thin plate splines. Given that the approach produces spline-like models
with a sparse basis, it also provides a natural way of incorporating unpenalized spline-like terms
in linear and generalized linear models, and these can be treated just like any other model terms
from the point of view of model selection, inference and diagnostics.
Keywords: Generalized additive model; Regression spline; Thin plate spline},
  comment   = {Multidimdimensional regression splines with a rank-based way of both smoothing and doing the regression. Tons of cites, maybe some improvements made.

Updated in: Wood06LowRnkTensProSmthGAM
Computation updated in: Wood08FastSmthGAM},
  doi       = {10.1111/1467-9868.00374/full},
  file      = {Wood03ThinPltRgrsSpln.pdf:Wood03ThinPltRgrsSpln.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.07.25},
}

@Article{Charpentier15copulaFinTimSerFreak,
  author    = {Arthur Charpenteir},
  title     = {Copulas and Financial Time Series},
  journal   = {Freakonometrics Blog},
  year      = {2015},
  month     = dec,
  abstract  = {I was recently asked to write a survey on copulas for financial time series. The paper is, so far, unfortunately, in French, and is available on https://hal.archives-ouvertes.fr/. There is a description of various models, including some graphs and statistical outputs, obtained from read data.},
  comment   = {Nice plots showing how the bivariate dependencies that can be represented with copulas.  Has R code too.},
  file      = {Charpentier15copulaFinTimSerFreak.pdf:Charpentier15copulaFinTimSerFreak.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.01.22},
  url       = {https://freakonometrics.hypotheses.org/tag/student},
}

@Article{Vu15solarFrcstIBMmachLrn,
  author    = {Christine Vu},
  title     = {Machine learning helps IBM boost accuracy of US Department of Energy solar forecasts by up to 30 percent},
  journal   = {Phys.org},
  year      = {2015},
  month     = jul,
  abstract  = {IBM Research today revealed that solar and wind forecasts it is producing using machine learning and other cognitive computing technologies are proving to be as much as 30 percent more accurate than ones created using conventional approaches.  Part of a research program funded the by the U.S. Department of Energy's SunShot Initiative, the breakthrough results suggest new ways to optimize solar resources as they are increasingly integrated into the nation's energy systems.
IBM also announced that for a limited time it will provide foundational solar forecasts at five-kilometer spatial resolution to help government agencies and other organizations in the lower 48 states best evaluate their impact on supply and demand as well as operations.},
  comment   = {What IBM is doing in RES forecasting, which is "30%" more accurate than conventional approaches.

Forecasts they're providing
* Solar
* Wind
* Hydro
* I think demand because they mention invisible PV "behind the meter" effect on demand curve.
* horizon: minutes to weeks ahead
* 5 km res for US lower 48, I think for free for some gov't agencies

Techniques:
* SMT: Self-learning weather Model and renewable forecasting Technology
* machine learning
* big data analytics
* condintously analyses (adaptive?)
* Deep Learning
  - blends "domain" data (From video, he means NWP)
  - sensor networks (thousands of weather stations and realtime measurements)
  - sky cameras
  - satellites
  - multiple weather models -- a "large number"
  - "first time a broad range of forecasting methods have been integrated into a single scalable platform"
  - say the include a "broad range" of forecasting variables, unlike anybody else

Customers: ISOs and Utilities.  Odd that markets aren't mentioned

From Video here: https://youtu.be/cj2RXjvRKOA
Henrik Hamann, of the project, says
* domain: NWP
* combine other stuff  with NWP
* thinks 50\% improvements
* ramp forecasting is a focus
(video was linked to by this article: http://www.computerworld.com/article/2948987/sustainable-it/ibms-machine-learning-crystal-ball-can-foresee-renewable-energy-availability.html)},
  file      = {:Vu15solarFrcstIBMmachLrn.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.25},
  url       = {https://phys.org/news/2015-07-machine-ibm-boost-accuracy-department.html},
}

@Article{Hennig00clustLinRgrsnID,
  author    = {Hennig, Christian},
  title     = {Identifiablity of models for clusterwise linear regression},
  journal   = {Journal of Classification},
  year      = {2000},
  volume    = {17},
  number    = {2},
  pages     = {273--296},
  abstract  = {Identifiability of the parameters is a necessary condition for the existence of
consistent estimators. In this paper the identifiability of the parameters of models for
data generated by different linear regression distributions with Gaussian errors is investigated.
It turns out that such models cause other identifiability problems than do simple
Gaussian mixtures. This problem was heretofore ignored; thus there are no satisfying
consistency proofs in this area. Three different models are treated: Finite mixture models
with random and fixed covariates and a fixed partition model. Counterexamples and
sufficient conditions for identifiability are given, including an example for nonidentifiable
parameters with an invertible information matrix.
The model choice and the interpretation of the parameters are discussed as well as the
use of the identifiability concept for fixed partition models. The concept is generalized
to "partial identifiability".
Keywords: Partial identifiability; Switching regression; Mixture model; Fixed partition
model; Change point problem; Gaussian mixtures with covariates},
  comment   = {Clusterwise regression is hard, or was hard in 2000.},
  file      = {Hennig00clustLinRgrsnID.pdf:Hennig00clustLinRgrsnID.pdf:PDF},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2014.04.01},
  url       = {http://www.springerlink.com/index/UV5MEHYR0MQCG7GD.pdf},
}

@Article{Qin06clustRgrsnGene,
  author    = {Qin, Li-Xuan and Self, Steven G},
  title     = {The clustering of regression models method with applications in gene expression data},
  journal   = {Biometrics},
  year      = {2006},
  volume    = {62},
  number    = {2},
  pages     = {526--533},
  abstract  = {Identification of differentially expressed genes and clustering of genes are two important and complementary objectives addressed with gene expression data. For the differential expression question, many "per-gene" analytic methods have been proposed. These methods can generally be characterized as using a regression function to independently model the observations for each gene; various adjustments for multiplicity are then used to interpret the statistical significance of these per-gene regression models over the collection of genes analyzed. Motivated by this common structure of per-gene models, we proposed a new model-based clustering method--the clustering of regression models method, which groups genes that share a similar relationship to the covariate(s). This method provides a unified approach for a family of clustering procedures and can be applied for data collected with various experimental designs. In addition, when combined with per-gene methods for assessing differential expression that employ the same regression modeling structure, an integrated framework for the analysis of microarray data is obtained. The proposed methodology was applied to two microarray data sets, one from a breast cancer study and the other from a yeast cell cycle study.},
  comment   = {Clustering of regression coeffs algorithm used in the R package RegClust. Probably high dim since it's genetics.

PDF is subito

See also: Qin07rgrsnClustVSkmeans

RegClust
http://cran.r-project.org/web/packages/RegClust/index.html},
  doi       = {10.1111/j.1541-0420.2005.00498.x/full},
  file      = {Qin06clustRgrsnGene.pdf:Qin06clustRgrsnGene.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.04.01},
}

@Article{Reshef11maxInfCoef,
  author    = {Reshef, David N. and Reshef, Yakir A. and Finucane, Hilary K. and Grossman, Sharon R. and McVean, Gilean and Turnbaugh, Peter J. and Lander, Eric S. and Mitzenmacher, Michael and Sabeti, Pardis C.},
  title     = {Detecting Novel Associations in Large Data Sets},
  journal   = {Science},
  year      = {2011},
  volume    = {334},
  number    = {6062},
  pages     = {1518--1524},
  abstract  = {Identifying interesting relationships between pairs of variables in large data sets is increasingly important. Here, we present a measure of dependence for two-variable relationships: the maximal information coefficient (MIC). MIC captures a wide range of associations both functional and not, and for functional relationships provides a score that roughly equals the coefficient of determination (R2) of the data relative to the regression function. MIC belongs to a larger class of maximal information-based nonparametric exploration (MINE) statistics for identifying and classifying relationships. We apply MIC and MINE to data sets in global health, gene expression, major-league baseball, and the human gut microbiota and identify known and novel relationships.},
  comment   = {Maximal Information Coefficient: Like linear correlation R^2 but works for nonlinear relationships. But has weak power?. Criticisms here: http://www-stat.stanford.edu/~tibs/reshef/comment.pdf R implementation from the criticizers: http://www-stat.stanford.edu/~tibs/reshef/script.R},
  doi       = {10.1126/science.1205438},
  eprint    = {http://www.sciencemag.org/content/334/6062/1518.full.pdf},
  file      = {Reshef11maxInfCoef.pdf:Reshef11maxInfCoef.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2012.03.25},
  url       = {http://www.sciencemag.org/content/334/6062/1518.abstract},
}

@InProceedings{VanderVeen10imbalSettleDEvsNL,
  author    = {Van der Veen, Reinier AC and Abbasy, Alireza and Hakvoort, RA},
  title     = {A comparison of imbalance settlement designs and results of {Germany} and the {Netherlands}},
  booktitle = {Young Energy Engineers \& Economists Seminar (YEEES), 8-9 April 2010, Cambridge, UK},
  year      = {2010},
  abstract  = {Imbalance settlement is a vital part of the balancing market, i.e. the institutional arrangement that establishes market-
based balance management in liberalized electricity markets. We investigate the impact of the imbalance settlement design on the
behaviour of Balance Responsible Parties and thereby on balancing market performance by means of a comparison of the German and
Dutch imbalance settlement designs and balancing market results for the period May-December 2009. It is found that Germany has
much higher activated balancing energy volumes, imbalance prices and actual BRP cost levels than the Netherlands, but these
differences are perhaps rather caused by balancing energy market design differences and differences in intermittent generation shares
than by imbalance settlement design differences. The real-time publication of balance regulation in the Netherlands enables internal
balancing by BRPs, which may reduce the size of system imbalances. Generally, BRPs will over-contract a little, because of the lower
risk of having a negative individual imbalance and because of the evening out of imbalance costs over a longer time period.



Index Terms?balancing market, imbalance settlement, Balance Responsible Parties (BRPs)},
  comment   = {Smallish difference in German and Dutch imbalance settlement results in diffferent market behavior.

Reasons why the Bremnes algorith implmented by DTU (in IRPWIND)
* German settlement calculation is done weeks or months after the imbalance; Dutch is a little bit quicker.
* German prices are symmetric; Dutch are asymmetric.},
  file      = {VanderVeen10imbalSettleDEvsND.pdf:VanderVeen10imbalSettleDEvsNL.pdf:PDF},
  url       = {https://www.sintef.no/globalassets/project/balance-management/paper/comparison-imbalance-settlement-germany-and-netherlands_van-der-veen_2010.pdf},
}

@Article{Lemaitre17imbalanced-learnPyLib,
  author    = {Lema{\^\i}tre, Guillaume and Nogueira, Fernando and Aridas, Christos K},
  title     = {Imbalanced-learn: A python toolbox to tackle the curse of imbalanced datasets in machine learning},
  journal   = {The Journal of Machine Learning Research},
  year      = {2017},
  volume    = {18},
  number    = {1},
  pages     = {559--563},
  abstract  = {imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of over and under-sampling, and (iv) ensemble learning methods. The proposed toolbox depends only on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. Source code, binaries, and documentation can be downloaded from https://github.com/scikit-learn-contrib/imbalanced-learn.
Keywords: Imbalanced Dataset, Over-Sampling, Under-Sampling, Ensemble Learning, Machine Learning, Python.},
  comment   = {Python library relevant to rare event prediction e.g. DER adoption forecasting.  

Has 18 methods for binary and/or multi-class classification (see table).  Types are: 
1. Undersampling: of most common class; 
    a. fixed: get a certain class sample ratio
    b. cleaning: clean based on empirical criteria (?)
2. Oversampling: of least common class
3. Combination: avoids overfitting due to oversampling
    - Somehow it does this with cleaning undersampling (?)
4. Ensemble: avoids loss of samples due to undersampling
    - makes ensembles of balanced classifiers
    - most samples should show up in some ensemble

Can fit into a scikit learn pipeline
- has methods: fit, sample, fit_sample
- Pipeline class inherited from scikit-learn combines samplers, transformers, estimators.

How to use the library
https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8

On GitHub:
https://github.com/scikit-learn-contrib/imbalanced-learn/blob/master/README.rst


Nice graphs of the many methods and apparently case studies
https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/index.html},
  file      = {:Lemaitre17imbalanced-learnPyLib.pdf:PDF},
  publisher = {JMLR. org},
  url       = {http://jmlr.org/papers/volume18/16-365/16-365.pdf},
}

@TechReport{Lee12probCongestFrcst,
  author      = {Steven Lee and Liang Min},
  title       = {Probabilistic Transmission Congestion Forecasting},
  institution = {California Energy Commission, Electric Power Research Institute},
  year        = {2012},
  number      = {CE C-500-2013-120},
  month       = dec,
  abstract    = {Improved forecasting of transmission congestion in both the short term and the long term can 
increase the efficiency and reliability of the California electricity system. Improved forecasting 
requires an approach that accounts for significant uncertainty caused by load and generation 
forecasts as well as random unplanned equipment outages. California’s Renewable Portfolio 
Standard requires that 20 percent of the state’s power be generated by renewable energy by 
2020, which will increase the volatility of congestion due to wind generation. Annual congestion 
costs on the California-Oregon Intertie path increased to $12 million in 2006 compared to $6.7 
million in 2005, so improvements in congestion management could yield significant cost 
savings and help achieve California’s energy goals. 
This report introduced two probabilistic methods for long-term and short-term transmission 
congestion forecasting that were developed by the Electric Power Research Institute. The 
proposed method applied sequential Monte Carlo simulation in a probabilistic load flow as the 
conceptual framework, added all the significant uncertainties and their probability distributions 
that needed to be modeled, developed the models, and specified how to accurately model the 
key input assumptions in order to derive valid confidence levels of the forecasted congestion 
variables. The developed probabilistic method was successfully applied to the four-area 
Western Electricity Coordinating Council equivalent system. The focus was on the confidence 
levels of making such forecasts so that a window of forecastability was defined beyond which 
any forecast would be considered to contain little actionable information. The probabilistic 
forecasts of congestion would provide confidence limits and information for ranking the 
potential benefits of alleviating congestion at the various transmission bottlenecks within the 
window of forecastability. 

 

Keywords: Transmission congestion forecasting, Monte Carlo Simulation (MCS), probability 
distribution, probabilistic load flow, transmission bottlenecks, Western Electricity Coordinating 
Council (WECC). },
  comment     = {Related to net demand forecasting, and it's probabilstic.},
  file        = {:Lee12probCongestFrcst.pdf:PDF},
  url         = {http://www.energy.ca.gov/2013publications/CEC-500-2013-120/CEC-500-2013-120.pdf},
}

@Article{Marvuglia11lrnFarmPowCrvGMR,
  author    = {Marvuglia, Antonino and Messineo, Antonio},
  title     = {Learning a wind farm power curve with a data-driven approach},
  journal   = {Volume 15 Wind Energy Applications},
  year      = {2011},
  pages     = {4217},
  abstract  = {Improving the performance of prediction algorithms is one of the priorities in the wind energy research agenda of the scientific community. In a very simplistic approach, short-term predictions of wind power production at a given site could be generated by passing forecasts of meteorological variables (namely wind speed) through the so-called wind farm power curve, which links the wind speed to the power that is produced by the whole wind farm. However, the estimation of this conversion function is indeed a challenging task, because it is nonlinear and bounded, in addition to being non-stationary due for example to changes in the site environment and seasonality. Even for a single wind turbine the measured power at different wind speeds is generally different to the rated power, since the operating conditions on site are generally different to the conditions under which the turbine was calibrated (the wind speed on site is not uniform horizontally across the face of the turbine; the vertical wind profile and the air density are different than during the calibration; the wind data available on site are not always measured at the height of the turbine?s hub).
The recent developments in data mining and evolutionary computation (EC) offer promising approaches to modelling the power curves of turbines. In this paper we use a self-supervised neural network called GMR (Generalized Mapping Regressor) to learn the relationship between the wind speed and the generated power in a whole wind farm. GMR is an incremental self-supervised neural network which can approximate every multidimensional function or relation presenting any kind of discontinuity. The approach used is a data driven one, in the sense that the relationship is learned directly from the data, without using any explicit physical or mathematical relationship between input and output space. The model is potentially applicable to any site, provided that a statistically consistent amount of wind and power data is available. The methodology allows the creation of a non-parametric model of the power curve that can be used as a reference profile for on-line monitoring of the power generation process, as well as for power forecasts.
The results obtained with the proposed approach are compared with another state-of-the-art data mining algorithm (namely, a feedforward Multi Layer Perceptron) showing that the algorithm provides fair performances if a suitable pre-processing of the input data is accomplished.
Keywords: Wind farm, Power curve, Data-driven, Neural network, Machine learning},
  comment   = {Learns whole-farm power curve with a self-organizing GMR instead of a neural net (MLP). The GMR is better and can help with detecting farm performance anomalies.},
  file      = {Marvuglia11lrnFarmPowCrvGMR.pdf:Marvuglia11lrnFarmPowCrvGMR.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.08},
  url       = {http://www.ep.liu.se/ecp/057/vol15/022/ecp57vol15_022.pdf},
}

@Article{Magnussen14locLinKNN,
  author    = {Magnussen, Steen and Tomppo, Erkki},
  title     = {The k-nearest neighbor technique with local linear regression},
  journal   = {Scandinavian Journal of Forest Research},
  year      = {2014},
  number    = {ahead-of-print},
  pages     = {1--12},
  abstract  = {In a standard k-nearest neighbor (kNN) technique, imputations of unit-level values in the variables of interest (Y) are based on the k-nearest neighbors in a set of reference units. Nearest is defined with respect to a distance metric in the space of auxiliary variables (X). This study evaluates kNN imputations of Y with a selection, by the same distance metric, of k-nearest locally weighted regression models. Imputations are obtained as predictions using the X values of the k-nearest neighbors in the population. In simulated random sampling from three artificial multivariate populations and two actual univariate populations and sampling units composed of a single population element or a cluster of four elements, the new kNN technique: (1) improved the correlation between an imputation and its actual value; (2) lowered the root mean square error (RMSE) of imputations; (3) increased the slope in regressions of actual y values regressed against their imputed values; (4) performed relatively best with k values of 4 and sample sizes of 200 or greater; (5) compared favorably with a recently proposed kNN calibration procedure; and (6) had a higher (15?28\%) RMSE than with a simple local linear regression. Distribution matching had a consistent negative effect (+10\%) on RMSE.},
  comment   = {Almost the only paper I've found that exactly does loca linear regression with KNN neighborhoods. But I haven't found the pdf!},
  doi       = {10.1080/02827581.2013.878744},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2014.04.21},
}

@Article{Schmidt13noCrossQRspacing,
  author   = {Schmidt, Lawrence DW},
  title    = {Quantile spacings: a simple method for the joint estimation of multiple quantiles},
  journal  = {Available at SSRN 2220901},
  year     = {2013},
  abstract = {In a variety of economic applications, we would like learn about aspects of a conditional distribution which are not well described by conditional means and/or variances. One simple
econometric approach is to model a representative number of conditional quantiles. However,
many existing methods suffer from the well-known quantile crossing problem, namely that the
estimated quantile functions do not satisfy a basic monotonicity requirement that every quan-
tile function must satisfy. We propose a simple but ?exible parametric model for conditional
quantiles. These quantiles will satisfy the monotonicity requirement by construction, so they
are not susceptible to the quantile crossing problem. Rather than directly modeling the level
of each individual quantile, we begin with a single quantile (usually the median), and then add
or subtract nonnegative functions (quantile spacings) obtain the other quantiles. We propose
a simple interpolation method which generates a mapping from a ?nite number of quantiles to
a probability density function. Two estimation methods are discussed in detail, and we char-
acterize the limiting behavior of each. We identify a number of potential applications, and we
highlight an application of the method by Schmidt, Timmermann, and Wermers (2013) to the
study of a run on money market mutual funds in September 2008},
  comment  = {Avoid QR crossing by starting with one quantile, e.g. the median, and then building new quantiles based on "spacing." A kind-of greedy approach that cannot cross by construction. Could also be used in DR (Foresi95condDistRetDR)?

Seems like it would also be possible to have different features for different quantiles -- a probable advantage for extreme quantiles. Have to read more to know if it can be parallelized.

Not published, apparently, but the author (now an assistant professor at U of Chicago) used the techique again in 2014, here:

http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1784445},
  file     = {Schmidt13noCrossQRspacing.pdf:Schmidt13noCrossQRspacing.pdf:PDF},
  url      = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2220901},
}

@InProceedings{Bristol09swingDoorTrend,
  author    = {Bristol, EH},
  title     = {Swinging door trending: Adaptive trend recording},
  booktitle = {ISA National Conference Proceedings},
  year      = {1990},
  volume    = {45},
  pages     = {749--753},
  abstract  = {In a world of Smart Measurements, Adaptive Control, and Expert System Alarm Managers, one might wonder if there is a place for traditional, stodgy, process control. Surely, Trend Recording will resist change. Not so: new data compression techniques permit computer trending to effectively store and analyze practically unlimited amounts of process history as Trend Records for later evaluation. They compress stored data by orders of magnitude, representing the data as sequences of predefined shapes. The Swinging Door Algorithm is a very simple, effective example of these techniques. It reduces data naturally, as it occurs, to a sequence of optimally chosen straight lines, with a core algorithm that takes, at most, 3 additions, 2 divisions, and 3 comparisons, per sample time, per trended variable. Not only does this allow reduced memory but, in the long run, it allows the computer to make helpful pattern inferences. When combined with the evermore unlimited capabilities of mass memory hardware, it will not only allow the storage of a complete plant history, but it will provide the ability to search that history for relevant phenomena.},
  comment   = {Original reference for swinging door algorithm, used for unit commitment ramp constraints
  * picture explanations are fairly nice
  * used in Makarov09OperWindCA and Lu10unitCommCnstrnt, kind of in Crampes18flexElecMktsRamp},
  file      = {Bristol09swingDoorTrend.pdf:Bristol09swingDoorTrend.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  timestamp = {2011.06.20},
  url       = {http://homepage.mac.com/ebristol/PDF/SwDr.pdf},
}

@Unpublished{BPA07inHourBalanceRqt,
  author    = {BPA},
  title     = {In-Hour Balancing Requirements for Wind: 2009~{W}ind Integration Rate Case Conference Call},
  year      = {2007},
  abstract  = {In all studies to this point, we have found that the 733 MW of wind capacity in the BPA Balancing Authority Area (BAA) did not affect the amount of in-hour balancing requirements needed to meet North American Electric Reliability Corporation (NERC) CPS1 and CPS2 requirements. However, we saw that the amount of wind that was planned on being installed could easily cause issues within two years. We decided to analyze the data to see how much extra in-hour balancing will be required with the build-up over this time period.},
  comment   = {Has BPA inter-site lag times
* lags (delays) depend upon wind speed and pressure difference
==> delta pressure as a feature would help NN pick the right delay in a delay window},
  file      = {BPA07inHourBalanceRqt.pdf:BPA07inHourBalanceRqt.pdf:PDF;BPA07inHourBalanceRqt.pdf:BPA07inHourBalanceRqt.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.02.19},
  url       = {http://www.transmission.bpa.gov/business/rates_and_tariff/2009wrc/11_27_07_method_for_In_HR_Bal_Req.pdf},
}

@InProceedings{Puma-Villanueva07predWrapSelClust,
  author    = {Puma-Villanueva, W.J. and dos Santos, E.P. and Von Zuben, F.J.},
  title     = {Long-term time series prediction using wrappers for variable selection and clustering for data partition},
  booktitle = {International Joint Conference on Neural Networks (IJCNN)},
  year      = {2007},
  pages     = {3068--3073},
  month     = aug,
  abstract  = {In an attempt to implement long-term time series prediction based on the recursive application of a one-step-ahead multilayer neural network predictor, we have considered the eleven short time series provided by the organizers of the Special Session NN3 Neural Network Forecasting Competition, and have proposed a joint application of a variable selection technique and a clustering procedure. The purpose was to define unbiased partition subsets and predictors with high generalization capability, based on a wrapper methodology. The proposed approach overcomes the performance of the predictor that considers all the lags in the regression vector. After obtaining the eleven long-term predictors, we conclude the paper presenting the eighteen multi-step predictions for each time series, as requested in the competition.},
  comment   = {This does it all: multi-sep forecasting, feature and lag selection, clustering by regime (lags), ensembles
* As of 7/2010, no cites on google scholar},
  doi       = {10.1109/IJCNN.2007.4371450},
  file      = {Puma-Villanueva07predWrapSelClust.pdf:Puma-Villanueva07predWrapSelClust.pdf:PDF;Puma-Villanueva07predWrapSelClust.pdf:Puma-Villanueva07predWrapSelClust.pdf:PDF},
  issn      = {1098-7576},
  journal   = {International Joint Conference on Neural Networks (IJCNN)},
  keywords  = {neural nets, pattern clustering, time seriesdata partition clustering, generalization capability, multilayer neural network predictor, multistep predictions, recursive application, special session NN3 neural network forecasting competition, time series prediction, variable selection technique, wrapper methodology},
  owner     = {sotterson},
  timestamp = {2009.03.13},
}

@Article{Steinbrecher08QuantileMech,
  author    = {Steinbrecher, Gy{\"o}rgy and Shaw, William T},
  title     = {Quantile mechanics},
  journal   = {European Journal of Applied Mathematics},
  year      = {2008},
  volume    = {19},
  number    = {02},
  pages     = {87--112},
  abstract  = {In both modern stochastic analysis and more traditional probability and statistics, one way of charac-
terizing a static or dynamic probability distribution is through its quantile function. This paper is focused
on obtaining a direct understanding of this function via the classical approach of establishing and then
solving differential equations for the function. We establish ODEs and power series for the quantile func-
tions of several common distributions. We then develop the PDE for the evolution of the quantile function
associated with the solution of a class of SDEs, by a transformation of the Fokker-Planck equation. We
are able to utilize the static formulation to provide elementary time-dependent and equilibrium solutions.
Such a direct understanding is important because quantile functions find important uses in the simu-
lation of physical and financial systems. The simplest way of simulating any non-uniform random variable
is by applying its quantile function to uniform deviates. Modern methods of Monte-Carlo simulation,
techniques based on low-discrepancy sequences and copula methods all call for the use of quantile func-
tions of marginal distributions. We provide web resources for prototype implementations in computer
code. These implementations may variously be used directly in live sampling models or in a high-precision
benchmarking mode for developing fast rational approximations also for use in simulation.
Keywords: Inverse CDF, quantile function, normal, Student, beta, T-distribution, simulation, Monte Carlo, in-
verse cumulative distribution function, non linear ordinary differential equations, recurrence relations, Fokker-
Planck equation, stochastic differential equation, partial differential equation.},
  comment   = {Used in market simuation, same as SDE?  Is related to bivariate copula.  Look into it.

https://scholar.google.com/scholar?cites=22943422861875729&as_sdt=2005&sciodt=0,5&hl=en},
  doi       = {10.1017/S0956792508007341},
  file      = {Steinbrecher08QuantileMech.pdf:Steinbrecher08QuantileMech.pdf:PDF},
  publisher = {Cambridge Univ Press},
  url       = {http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=1813152&fileId=S0956792508007341},
}

@Article{Yamada06permTstCCA,
  author    = {Yamada, Tomoya and Sugiyama, Takakazu},
  title     = {On the permutation test in canonical correlation analysis},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {2006},
  volume    = {50},
  number    = {8},
  pages     = {2111--2123},
  month     = apr,
  abstract  = {In canonical correlation analysis, we are interested in testing whether the th canonical correlation coefficient is some number, especially the first canonical correlation coefficient. In this paper, we try the permutation test in canonical correlation analysis and suggest some test statistics.},
  comment   = {Use for regime clustering stopping criteria somehow?},
  doi       = {10.1016/j.csda.2005.03.006},
  file      = {Yamada06permTstCCA.pdf:Yamada06permTstCCA.pdf:PDF;Yamada06permTstCCA.pdf:Yamada06permTstCCA.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.03.09},
  url       = {http://ideas.repec.org/a/eee/csdana/v50y2006i8p2111-2123.html},
}

@Article{Nielsen01genClassicTimeSer,
  author    = {H. A. Nielsen and H. Madsen},
  title     = {A Generalization of Some Classical Time Series Tools},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {2001},
  volume    = {37},
  number    = {1},
  pages     = {13--31},
  abstract  = {In classical time series analysis the sample autocorrelation function (SACF) and the sample partial autocorrelation function (SPACF) has gained wide application for structural identification of linear time series models. We suggest generalizations, founded on smoothing techniques, applicable for structural identification of non-linear time series models. A similar generalization of the sample cross correlation function is discussed. Furthermore, a measure of the departure from linearity is suggested. It is shown how bootstrapping can be applied to construct confidence intervals under independence or linearity. The generalizations do not prescribe a particular smoothing technique. In fact, when the smoother is replaced by a linear regression the generalizations reduce to close approximations of {SACF} and {SPACF}. For this reason a smooth transition from the linear to the non-linear case can be obtained by varying the bandwidth of a local linear smoother. By adjusting the flexibility of the smoother the power of the tests for independence and linearity against specific alternatives can be adjusted. The generalizations allow for graphical presentations, very similar to those used for {SACF} and {SPACF}. In this paper the generalizations are applied to some simulated data sets and to the Canadian lynx data. The generalizations seem to perform well and the measure of the departure from linearity proves to be an important additional tool.},
  comment   = {How do extend linear dependence (correlation) to nonlinear Make correlations multivariate with Pena03descSctrLinDep ?},
  file      = {Nielsen01genClassicTimeSer.pdf:Nielsen01genClassicTimeSer.pdf:PDF;Nielsen01genClassicTimeSer.pdf:Nielsen01genClassicTimeSer.pdf:PDF},
  keywords  = {Lagged scatter plot; {R-}squared; Non-linear time series; Smoothing; Non-parametric; Independence; Bootstrap.},
  owner     = {sotterson},
  publisher = {Elsevier Science B.V.},
  timestamp = {2009.01.22},
  url       = {http://www2.imm.dtu.dk/pubdb/p.php?493},
}

@Article{Volkovich11resampClustModSel,
  author    = {Volkovich, Zeev and Barzily, Zeev and Weber, G-W and Toledano-Kitai, Dvora and Avros, Renata},
  title     = {Resampling approach for cluster model selection},
  journal   = {Machine learning},
  year      = {2011},
  volume    = {85},
  number    = {1-2},
  pages     = {209--248},
  abstract  = {In cluster analysis, selecting the number of clusters is an ?ill-posed? problem of crucial importance. In this paper we propose a re-sampling method for assessing cluster stability. Our model suggests that samples? occurrences in clusters can be considered as realizations of the same random variable in the case of the ?true? number of clusters. Thus, similarity between different cluster solutions is measured by means of compound and simple probability metrics. Compound criteria result in validation rules employing the stability content of clusters. Simple probability metrics, in particular those based on kernels, provide more flexible geometrical criteria. We analyze several applications of probability metrics combined with methods intended to simulate cluster occurrences. Numerical experiments are provided to demonstrate and compare the different metrics and simulation approaches.},
  comment   = {Selecting the number of clusters, etc, one referenced paper of author's does adjacency matrix stuff ("On an Adjacency Cluster Merit"), and I would guess this paper does too.

PDF is write protected, can find ugly, non-write protected version on authors researchgate page. Or, could convert w/ pdfcreator on SP3, but would then get bitmaps},
  doi       = {10.1007/s10994-011-5236-9},
  file      = {Volkovich11resampClustModSel.pdf:Volkovich11resampClustModSel.pdf:PDF},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2014.11.24},
}

@Article{Chen10simDayLdFrcst,
  author    = {Ying Chen and Luh, P.B. and Che Guan and Yige Zhao and Michel, L.D. and Coolbeth, M.A. and Friedland, P.B. and Rourke, S.J.},
  title     = {Short-Term Load Forecasting: Similar Day-Based Wavelet Neural Networks},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2010},
  volume    = {25},
  number    = {1},
  pages     = {322--330},
  issn      = {0885-8950},
  abstract  = {In deregulated electricity markets, short-term load forecasting is important for reliable power system operation, and also significantly affects markets and their participants. Effective forecasting, however, is difficult in view of the complicated effects on load by a variety of factors. This paper presents a similar day-based wavelet neural network method to forecast tomorrow's load. The idea is to select similar day load as the input load based on correlation analysis, and use wavelet decomposition and separate neural networks to capture the features of load at low and high frequencies. Despite of its "noisy" nature, high frequency load is well predicted by including precipitation and high frequency component of similar day load as inputs. Numerical testing shows that this method provides accurate predictions.},
  comment   = {Maybe useful for regime detection? Maybe a way to handle wavelet edge effects? A way to do analog ensemble forecasts?

* algorithm has "similar" days as forecast inputs
* Padding to avoid Daubechies (DB4) wavelet edge effects: UNCLEAR!
-- How use today's predicted load as in input to prediction?
-- Just copy today's predicted load at 1-3 to next day's 1-3? How does this ever change?
-- some people say these DB wavelets aren't appropriate for forecasting b/c of the edge effects

Referenced in Guan09loadFrcstNNmultiWvlt.

Could similarity search be done by one of the many SAX algorithms e.g.
Hung07saxPiecewise ?},
  doi       = {10.1109/TPWRS.2009.2030426},
  file      = {Chen10simDayLdFrcst.pdf:Chen10simDayLdFrcst.pdf:PDF},
  groups    = {Ensemble, doReadNonWPV_1},
  keywords  = {load forecasting;neural nets;power engineering computing;power markets;correlation analysis;day-based wavelet neural networks;deregulated electricity markets;power system operation;short-term load forecasting;wavelet decomposition;Neural network;short-term load forecasting;similar day;wavelet},
  owner     = {sotterson},
  timestamp = {2013.04.09},
}

@InProceedings{Tang12threshMonDistProb,
  author    = {Mingwang Tang and Feifei Li and Phillips, J.M. and Jestes, J.},
  title     = {Efficient Threshold Monitoring for Distributed Probabilistic Data},
  booktitle = {Data Engineering (ICDE), 2012 IEEE 28\textsuperscript{th} International Conference on},
  year      = {2012},
  pages     = {1120--1131},
  month     = apr,
  abstract  = {In distributed data management, a primary concern is monitoring the distributed data and generating an alarm when a user specified constraint is violated. A particular useful instance is the threshold based constraint, which is commonly known as the distributed threshold monitoring problem [4], [16], [19], [29]. This work extends this useful and fundamental study to distributed probabilistic data that emerge in a lot of applications, where uncertainty naturally exists when massive amounts of data are produced at multiple sources in distributed, networked locations. Examples include distributed observing stations, large sensor fields, geographically separate scientific institutes/units and many more. When dealing with probabilistic data, there are two thresholds involved, the score and the probability thresholds. One must monitor both simultaneously, as such, techniques developed for deterministic data are no longer directly applicable. This work presents a comprehensive study to this problem. Our algorithms have significantly outperformed the baseline method in terms of both the communication cost (number of messages and bytes) and the running time, as shown by an extensive experimental evaluation using several, real large datasets.},
  comment   = {Computationally feasible ways of computing aggregate statistics of probabilistic variables. One of them is (I think), whether or not a sum exceeds a threshold (almost a quantile).

Use for probabilistic upscaling.},
  doi       = {10.1109/ICDE.2012.34},
  file      = {Tang12threshMonDistProb.pdf:Tang12threshMonDistProb.pdf:PDF},
  groups    = {Upscaling (prob), doReadNonWPV_1},
  issn      = {1063-6382},
  keywords  = {data integration;data mining;probability;deterministic data;distributed data management;distributed probabilistic data;distributed threshold monitoring problem;probability threshold;score threshold;threshold based constraint;user specified constraint;Distributed databases;Marine vehicles;Markov processes;Monitoring;Poles and towers;Probabilistic logic;Uncertainty},
  owner     = {sotterson},
  timestamp = {2014.03.21},
  url       = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6228161},
}

@Article{Frunt10classQuantRsrvBal,
  author               = {Frunt, J. and Kling, W. L. and van den Bosch, P. P. J.},
  title                = {Classification and quantification of reserve requirements for balancing},
  journal              = {Electric Power Systems Research},
  year                 = {2010},
  volume               = {In Press, Corrected Proof},
  abstract             = {In electrical power systems there must always be a balance between supply and demand of power. Any imbalance will result in a frequency deviation. To reduce the imbalance to zero, ancillary services for balance management are in use. Ancillary services for balance management are characterized by their deployment time and their capacity. These ancillary services as well as the requirements for balance management can be characterized in the frequency domain. Actors, that are responsible for maintaining the balance in the system, can use the characterization in the frequency domain to define their needs for reserve capacity. This paper describes a method for spectral analysis of the required and available reserve capacity.},
  citeulike-article-id = {7560977},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.epsr.2010.06.018},
  citeulike-linkout-1  = {http://www.sciencedirect.com/science/article/B6V30-50NGNVD-1/2/3359c4131ff2eae928347bd7ebfafa21},
  comment              = {Markus Speckman's recommendation for German Primary/Secondary/Tertiary res. rqts.},
  doi                  = {10.1016/j.epsr.2010.06.018},
  file                 = {Frunt10classQuantRsrvBal.pdf:Frunt10classQuantRsrvBal.pdf:PDF},
  groups               = {Use, doReadWPV_2},
  keywords             = {ancillary\_services, reserve\_operation},
  owner                = {sotterson},
  posted-at            = {2010-08-02 23:17:32},
  timestamp            = {2012.02.09},
  url                  = {http://www.sciencedirect.com/science/article/B6V30-50NGNVD-1/2/3359c4131ff2eae928347bd7ebfafa21},
}

@InProceedings{Ryu18loadProfClustDpConvAutoEnc,
  author    = {S. {Ryu} and H. {Choi} and H. {Lee} and H. {Kim} and V. W. S. {Wong}},
  title     = {Residential Load Profile Clustering via Deep Convolutional Autoencoder},
  booktitle = {Proc. and Computing Technologies for Smart Grids (SmartGridComm) 2018 IEEE Int. Conf. Communications, Control},
  year      = {2018},
  pages     = {1--6},
  month     = oct,
  abstract  = {In energy data analytics, load profile clustering is essential for various smart grid applications such as demand response, load forecasting, and tariff design. Most of the conventional clustering techniques are based on a representative time domain load profile within a certain period, and the daily and seasonal variations are not well captured. In this paper, we propose a deep learning based customer load profile clustering framework that jointly captures daily and seasonal variations. By leveraging convolutional autoencoder (CAE), the yearly load profile in the time domain is converted into a representative vector in the smaller dimensional encoded space. The clusters are then determined based on the vectors encoded by the CAE. We apply the proposed framework to 1,405 households' yearly load profiles and verify that the trained CAE can encode those load profiles into approximately 100 times smaller dimensional space. The encoded load profiles can be decoded by the CAE with a negligible loss between 1-3\%. The clustered load images can visualize both daily and seasonal variations, and clustering in the encoded space speeds up the clustering process by almost three orders of magnitude.},
  comment   = {Compresses entire year of load profiles (dim=8640),  capturing seasonal (and day of week?) dependence, then does k-means clustering in compressed space (dim=100).  Compare w/ AEC in Madiraju18deepUnsupTSclust, where clustering is built-in.

Anyway, could make it probabilistic with the approach in Li16loadProfileClustMultiRes},
  doi       = {10.1109/SmartGridComm.2018.8587454},
  file      = {:Ryu18loadProfClustDpConvAutoEnc.pdf:PDF},
  keywords  = {convolutional neural nets, data analysis, demand side management, learning (artificial intelligence), load forecasting, pattern clustering, power engineering computing, smart power grids, vectors, deep learning based customer load profile clustering, time domain load profile, convolutional autoencoder, energy data analytics, deep convolutional autoencoder, residential load profile clustering, representative vector, smart grid applications, Image reconstruction, Smart meters, Smart grids, Complexity theory, Load management, Encoding},
}

@Article{Straube14infreqPerfEstImbal,
  author    = {Straube, Sirko and Krell, Mario M},
  title     = {How to evaluate an agent's behavior to infrequent events??Reliable performance estimation insensitive to class distribution},
  journal   = {Frontiers in computational neuroscience},
  year      = {2014},
  volume    = {8},
  abstract  = {In everyday life, humans and animals often have to base decisions on infrequent relevant stimuli with respect to frequent irrelevant ones. When research in neuroscience mimics this situation, the effect of this imbalance in stimulus classes on performance evaluation has to be considered. This is most obvious for the often used overall accuracy, because the proportion of correct responses is governed by the more frequent class. This imbalance problem has been widely debated across disciplines and out of the discussed treatments this review focusses on performance estimation. For this, a more universal view is taken: an agent performing a classification task. Commonly used performance measures are characterized when used with imbalanced classes. Metrics like Accuracy, F-Measure, Matthews Correlation Coefficient, and Mutual Information are affected by imbalance, while other metrics do not have this drawback, like AUC, d-prime, Balanced Accuracy, Weighted Accuracy and G-Mean. It is pointed out that one is not restricted to this group of metrics, but the sensitivity to the class ratio has to be kept in mind for a proper choice. Selecting an appropriate metric is critical to avoid drawing misled conclusions.

Keywords: metrics, decision making, confusion matrix, oddball, imbalance, performance evaluation, classification},
  comment   = {Commonly used performance measures are characterized when used with imbalanced classes. A nice review paper.

Metrics like Accuracy, F-Measure, Matthews Correlation Coefficient, and Mutual Information are affected by imbalance, while other metrics do not have this drawback, like AUC, d-prime, Balanced Accuracy, Weighted Accuracy and G-Mean.

Mutual Information and Imbalance
* I(X;Y) = H(X) - H(X|Y)
* if X is a binary class prediction, then H(X) is soley determined by the prior class distribution
* so you'll get a different # for different class imbalance
* but if you're just doing featsel using MI, then all features will have the same H(X) and (I think) it still makes sense to pick features w/ max MI, right?},
  doi       = {10.3389/fncom.2014.00043},
  file      = {Straube14infreqPerfEstImbal.pdf:Straube14infreqPerfEstImbal.pdf:PDF},
  owner     = {sotterson},
  publisher = {Frontiers Media SA},
  timestamp = {2017.01.17},
  url       = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3989732/},
}

@Article{Langousis16threshPickGPD,
  author    = {Langousis, Andreas and Mamalakis, Antonios and Puliga, Michelangelo and Deidda, Roberto},
  title     = {Threshold detection for the generalized Pareto distribution: Review of representative methods and application to the NOAA NCDC daily rainfall database},
  journal   = {Water Resources Research},
  year      = {2016},
  volume    = {52},
  number    = {4},
  pages     = {2659--2681},
  abstract  = {In extreme excess modeling, one fits a generalized Pareto (GP) distribution to rainfall excesses
above a properly selected threshold u. The latter is generally determined using various approaches, such as
nonparametric methods that are intended to locate the changing point between extreme and nonextreme
regions of the data, graphical methods where one studies the dependence of GP-related metrics on the
threshold level u, and Goodness-of-Fit (GoF) metrics that, for a certain level of significance, locate the lowest
threshold u that a GP distribution model is applicable. Here we review representative methods for GP
threshold detection, discuss fundamental differences in their theoretical bases, and apply them to 1714
overcentennial daily rainfall records from the NOAA-NCDC database. We find that nonparametric methods
are generally not reliable, while methods that are based on GP asymptotic properties lead to unrealistically
high threshold and shape parameter estimates. The latter is justified by theoretical arguments, and it is
especially the case in rainfall applications, where the shape parameter of the GP distribution is low; i.e., on
the order of 0.1–0.2. Better performance is demonstrated by graphical methods and GoF metrics that rely
on preasymptotic properties of the GP distribution. For daily rainfall, we find that GP threshold estimates
range between 2 and 12 mm/d with a mean value of 6.5 mm/d, while the existence of quantization in the
empirical records, as well as variations in their size, constitute the two most important factors that may
significantly affect the accuracy of the obtained results.},
  comment   = {Reviews methods for picking threshold for the Generalized Pareto Distribution (GPD), which is used to predict the tails of a distribution.

Idea is to pick the lowest threshold that satisfies some criterion.  Options are:
* nonparametric change point between extreme and nonextreme regions of the data,
* graphical methods where one studies the dependence of GP-related metrics on threshold
* Goodness-of-Fit (GoF): for a certain level of significance, find lowest thresh where GPD fits},
  file      = {:Langousis16threshPickGPD.pdf:PDF},
  publisher = {Wiley Online Library},
}

@Article{Tille12histoLorenzGini,
  author    = {Yves Tillé and Matti Langel},
  title     = {Histogram-Based Interpolation of the Lorenz Curve and Gini Index for Grouped Data},
  journal   = {The American Statistician},
  year      = {2012},
  volume    = {66},
  number    = {4},
  pages     = {225--231},
  issn      = {00031305},
  abstract  = {In grouped data, the estimation of the Lorenz curve without taking into account the within-class variability leads to an overestimation of the curve and an underestimation of the Gini index. We propose a new strictly convex estimator of the Lorenz curve derived from a linear interpolation-based approximation of the cumulative distribution function. Integrating the Lorenz curve, a correction can be derived for the Gini index that takes the intraclass variability into account.},
  comment   = {Calculating GINI coefficients from anonymized summary stats, like quantiles, interval means, etc.   Uses only histograms.

See Fancier: Prendergast16quantLorenzGINI, Lyon16advtgsGrpMnLorenzGINI},
  file      = {:Tille12histoLorenzGini.pdf:PDF},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  url       = {http://www.jstor.org/stable/23339499},
}

@Misc{Wikipedia15cholesky,
  author       = {Wikipedia},
  title        = {Cholesky decomposition},
  howpublished = {Web Page},
  month        = dec,
  year         = {2015},
  abstract     = {In linear algebra, the Cholesky decomposition or Cholesky factorization is a decomposition of a Hermitian,
positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose, which is useful
e.g. for efficient numerical solutions and Monte Carlo simulations. It was discovered by Andr?-Louis Cholesky
for real matrices. When it is applicable, the Cholesky decomposition is roughly twice as efficient as the LU
decomposition for solving systems of linear equations.},
  comment      = {Cholesky decomp. is 2X more efficient than other techniques for solving systems of equations, while a variant, LDL a.k.a LDLT may be preferred b/c it does not use square roots.  Many uses (below), can be done online.  Detailed computation algorithm explained.

* Applications:
4.1 Linear least squares (in the Normal Equation)
4.2 Non-linear optimization
4.3 Monte Carlo simulation
4.4 Kalman filters
4.5 Matrix inversion

* Cholesky can be done online w/o full recomputation.
* Ill-conditioned, collinear things cause square root problem (Gale08cholesky); can LDL(T) replace Cholesky in those applications?
* lots of details on how to compute it, and generalizations},
  file         = {Wikipedia15cholesky.pdf:Wikipedia15cholesky.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2015.12.22},
  url          = {https://en.wikipedia.org/wiki/Cholesky_decomposition},
}

@Article{Krause08robustSubModObsSel,
  author    = {Andreas Krause and H. Brendan McMahan and Carlos Guestrin and Anupam Gupta},
  title     = {Robust Submodular Observation Selection},
  journal   = {Journal of Machine Learning Research},
  year      = {2008},
  volume    = {9},
  pages     = {2761--2801},
  month     = dec,
  abstract  = {In many applications, one has to actively select among a set of expensive observations before making an informed decision. For example, in environmental monitoring, we want to select locations to measure in order to most effectively predict spatial phenomena. Often, we want to select observations which are robust against a number of possible objective functions. Examples include minimizing the maximum posterior variance in Gaussian Process regression, robust experimental design, and sensor placement for outbreak detection. In this paper, we present the Submodular Saturation algorithm, a simple and efficient algorithm with strong theoretical approximation guarantees for cases where the possible objective functions exhibit submodularity, an intuitive diminishing returns property. Moreover, we prove that better approximation algorithms do not exist unless NP-complete problems admit efficient algorithms. We show how our algorithm can be extended to handle complex cost functions (incorporating non-unit observation cost or communication and path costs). We also show how the algorithm can be used to near-optimally trade off expected-case (e.g., the Mean Square Prediction Error in Gaussian Process regression) and worst-case (e.g., maximum predictive variance) performance. We show that many important machine learning problems fit our robust submodular observation selection formalism, and provide extensive empirical evaluation on several real-world problems. For Gaussian Process regression, our algorithm compares favorably with state-of-the-art heuristics described in the geostatistics literature, while being simpler, faster and providing theoretical guarantees. For robust experimental design, our algorithm performs favorably compared to SDP-based algorithms.},
  comment   = {Combinatorial feature selection focused on Gaussian Process Regression; missing features too see tutorial video and matlab: http://www.select.cs.cmu.edu/tutorials/icml08submodularity.html Related to Jeff Bilmes talk at ASRU 2008 on combinatorial optimization},
  file      = {Krause08robustSubModObsSel.pdf:Krause08robustSubModObsSel.pdf:PDF;Krause08robustSubModObsSel.pdf:Krause08robustSubModObsSel.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.02.20},
  url       = {http://jmlr.csail.mit.edu/papers/v9/krause08b.html},
}

@TechReport{Hyndman07optCombHierFrcst,
  author      = {Hyndman, Rob J and Ahmed, Roman A and Athanasopoulos, George},
  title       = {Optimal combination forecasts for hierarchical time series},
  institution = {Monash University, Department of Econometrics and Business Statistics},
  year        = {2007},
  type        = {Monash Econometrics and Business Statistics Working Papers},
  number      = {9/07},
  abstract    = {In many applications, there are multiple time series that are hierarchically organized and can be aggregated at several different levels in groups based on products, geography or some other features. We call these "hierarchical time series". They are commonly forecast using either a "bottom-up" or a "top-down" method. In this paper we propose a new approach to hierarchical forecasting which provides optimal forecasts that are better than forecasts produced by either a top-down or a bottom-up approach. Our method is based on independently forecasting all series at all levels of the hierarchy and then using a regression model to optimally combine and reconcile these forecasts. The resulting revised forecasts add up appropriately across the hierarchy, are unbiased and have minimum variance amongst all combination forecasts under some simple assumptions. We show in a simulation study that our method performs well compared to the top-down approach and the bottom-up method. It also allows us to construct prediction intervals for the resultant forecasts. Finally, we apply the method to forecasting Australian tourism demand where the data are disaggregated by purpose of visit and geographical region.},
  comment     = {Forecast a sum of time series from individual time series forecasts Tech note for: Athanasopoulos09hierFrcs},
  file        = {Hyndman07optCombHierFrcst.pdf:Hyndman07optCombHierFrcst.pdf:PDF},
  keywords    = {Bottom-up forecasting; combining forecasts; GLS regression; hierarchical forecasting; Moore-Penrose inverse; reconciling forecasts; top-down forecasting.},
  owner       = {scot},
  timestamp   = {2010.07.01},
  url         = {http://econpapers.repec.org/RePEc:msh:ebswps:2007-9},
}

@Article{ElGohary07causWhiteCorr,
  author    = {El-Gohary, M. and McNames, J.},
  title     = {Establishing Causality With Whitened Cross-Correlation Analysis},
  journal   = {Biomedical Engineering, IEEE Transactions on},
  year      = {2007},
  volume    = {54},
  number    = {12},
  pages     = {2214--2222},
  month     = dec,
  issn      = {0018-9294},
  abstract  = {In many biomedical applications, it is important to determine whether two recorded signals have a causal relationship and, if so, what the nature of that relationship is. Many advanced techniques have been proposed to characterize this relationship, but in practice simple techniques such as cross-correlation analysis are used. Unfortunately, the traditional cross-correlation analysis is influenced by the autocorrelation of the signals as much as it is by the relationship between the signals. Practically, this results in the cross correlation suggesting that the signals are correlated over a broad range of lags. Prewhitening the signals overcomes this limitation and reveals the essentially causal relationship between the signals. This is a simple method that can also easily generalize cross-correlation analysis to nonstationary signals, which are frequently encountered in biomedical applications. This technique has been used in other fields, but remains mostly undiscovered in biomedical research. In the case of a purely causal relationship, we show that whitened cross-correlation analysis is equivalent to directly estimating the all-pass component of the transfer function that relates the signals. We give examples of this type of analysis applied to several biomedical applications to demonstrate some of the new insights and information that can be produced by this type of analysis.},
  comment   = {Whitening removes autocorrelation smearing; can see if A happened before B. Sometimes, this is just the old GCC-PHAT Noise-driven LTI filter signal model * many signals can be modeled this way (AR, MA, ARMA, which also fit wind) * use this to derive a min phase model w/ the pure delay * in one case, this just turns out to be GCC-PHAT! (eq 17) * max correlated freq limits xcorr time resolution but there's something confused about the limits and it seems to me like what they're really worried about is not noise pumping so they avoid dividing by zero in low amplitude chunks of the spectrum. Whitening methods * any linear prediction filter residual can be the whitener * but they use Kalman filter * why not use GCC-PHAT? Window too wide? Causality and Statistical Significance * A "causes" B is xcorr has stat. sig. coeffs at neg lags * Significace from uncorrelated null hypothesis, which is Gaussian w/ var 1/N; N is num. samples in xcorr calc? -- insead do permutation test like in Francois07resampParamFreeFeatSel?},
  doi       = {10.1109/TBME.2007.906519},
  file      = {ElGohary07causWhiteCorr.pdf:ElGohary07causWhiteCorr.pdf:PDF;ElGohary07causWhiteCorr.pdf:ElGohary07causWhiteCorr.pdf:PDF},
  groups    = {Read},
  keywords  = {adaptive Kalman filters, causality, delays, medical signal processing, stochastic processesautocorrelation signal, causal relationship, causality, cross-correlation analysis, signal whitening, transfer function},
  owner     = {sotterson},
  timestamp = {2009.02.10},
}

@TechReport{Grothe12directMktSpatDE,
  author      = {Oliver Grothe and Felix M{\"u}sgens},
  title       = {The Influence of Spatial Effects on Wind Power Revenues under Direct Marketing Rules},
  institution = {EWI},
  year        = {2012},
  type        = {EWI Working Paper},
  number      = {No 12/07},
  abstract    = {In many countries worldwide, investment in renewable technologies has been accelerated by the introduction of ?xed feed-in tariffs for electricity from renewable energy sources (RES). While ?xed tariffs accomplish this purpose, they lack incentives to align the RES production with price signals. Today, due to a growing proportion of renewable electricity, the intermittency of most RES increases the volatility of electricity prices and might even prevent market clearing. Therefore, support schemes for RES have to be modi?ed. Recently, Germany launched a market premium model which gives wind power operators the monthly choice to either receive a ?xed feed-in tariff or to risk a - subsided - access to the wholesale electricity market. This paper quanti?es the revenues of wind turbines under this new model and, in particular, analyzes whether, when and where producers may pro?t. We ?nd that the position of the wind turbine within the country significantly in?uences revenues. The results are of interest and importance for wind farm operators deciding whether electricity should be sold in the ?xed tariff or in the wholesale market. Key words: Wind Power, Market Premium Model, Optimal Areas of Production},
  comment     = {Seasonal price differences due to Germany's new direct wind market; not strong enough to affect build locations.
* effect is 5-6 EUR / MWh
* premium summer prices, some locations; penalty in winter (same locations? Not clear)
* but no locations dominate for the whole year, so won't change locational allocation
* could be used to guide future build policy
--- increase in efficiency for new turbines: build aligned w/ demand},
  file        = {Grothe12directMktSpatDE.pdf:Grothe12directMktSpatDE.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2012.05.28},
  url         = {http://www.ewi.uni-koeln.de/fileadmin/user_upload/Publikationen/Working_Paper/EWI_WP_12-07_Influence_spatial_effects.pdf},
}

@InCollection{Zhang11graphFeatSel,
  author      = {Zhang, Zhihong and Hancock, Edwin},
  title       = {A Graph-Based Approach to Feature Selection},
  booktitle   = {Graph-Based Representations in Pattern Recognition},
  publisher   = {Springer Berlin / Heidelberg},
  year        = {2011},
  editor      = {Jiang, Xiaoyi and Ferrer, Miquel and Torsello, Andrea},
  volume      = {6658},
  series      = {Lecture Notes in Computer Science},
  pages       = {205--214},
  isbn        = {978-3-642-20843-0},
  abstract    = {In many data analysis tasks, one is often confronted with very high dimensional data. The feature selection problem is essentially a combinatorial optimization problem which is computationally expensive. To overcome this problem it is frequently assumed either that features independently influence the class variable or do so only involving pairwise feature interaction. To tackle this problem, we propose an algorithm consisting of three phases, namely, i) it first constructs a graph in which each node corresponds to each feature, and each edge has a weight corresponding to mutual information (MI) between features connected by that edge, ii) then perform dominant set clustering to select a highly coherent set of features, iii) further selects features based on a new measure called multidimensional interaction information (MII). The advantage of MII is that it can consider third or higher order feature interaction. By the help of dominant set clustering, which separates features into clusters in advance, thereby allows us to limit the search space for higher order interactions. Experimental results demonstrate the effectiveness of our feature selection method on a number of standard data-sets.},
  affiliation = {Department of Computer Science, University of York, UK},
  comment     = {information theory-based dominant clique graph clustering for featsel; better than Peng mRMR; num. features not selected. * Dominant clique clustering might be a way to partition before doing Kraskov04 MI featsel or MI est. * Seems likey they're confusing "interaction information" with multimensional Shannon information?},
  doi         = {10.1007/978-3-642-20844-7_21},
  file        = {Paper:Zhang11graphFeatSel.pdf:PDF;Slides:Zhang11graphFeatSel_Slides.pdf:PDF},
  keyword     = {Computer Science},
  owner       = {sotterson},
  timestamp   = {2011.12.09},
}

@Article{Bollaerts06psplineSimpMultiShape,
  author    = {Bollaerts, Kaatje and Eilers, Paul HC and Mechelen, Iven},
  title     = {Simple and multiple P-splines regression with shape constraints},
  journal   = {British Journal of Mathematical and Statistical Psychology},
  year      = {2006},
  volume    = {59},
  number    = {2},
  pages     = {451--469},
  abstract  = {In many research areas, especially within social and behavioural sciences, the
relationship between predictor and criterion variables is often assumed to have a
particular shape, such as monotone, single-peaked or U-shaped. Such assumptions can
be transformed into (local or global) constraints on the sign of the nth-order derivative
of the functional form. To check for such assumptions, we present a non-parametric
regression method, P-splines regression, with additional asymmetric discrete penalties
enforcing the constraints.We show that the corresponding loss function is convex and
present a Newton-Raphson algorithm to optimize. Constrained P-splines are
illustrated with an application on monotonicity-constrained regression with both one
and two predictor variables using data from research on cognitive development of
children.},
  comment   = {1D and multivariate regression psplines, also with monotonicity, etc. constraints. Explanations are pretty clear too.


* joint solution of penalty and constraint is done w/ Newton Raphson iteration
* Roughness penalty choice
  - uses AIC instead of k-fold CV
  - I suppose this is k fewer computation iterations
  - effective dimensions of a pspline w/ a ton of knots is found using the trace of the Mexican hat.

Is referenced by Jonathan14nonStatCondXtrmPenSpln

* says Matlab is available on request.  I emailed a (probably dead) web page of Bollaerts},
  doi       = {10.1348/000711005X84293/full},
  file      = {Bollaerts06psplineSimpMultiShape.pdf:Bollaerts06psplineSimpMultiShape.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.10.30},
}

@InProceedings{Kramer11dimRedKNN,
  author       = {Kramer, Oliver},
  title        = {Dimensionality reduction by unsupervised k-nearest neighbor regression},
  booktitle    = {Machine Learning and Applications and Workshops (ICMLA), 2011 10\textsuperscript{th} International Conference on},
  year         = {2011},
  volume       = {1},
  pages        = {275--278},
  organization = {IEEE},
  abstract     = {In many scientific disciplines structures in high-dimensional
data have to be found, e.g., in stellar spectra, in
genome data, or in face recognition tasks. In this work we
present a novel approach to non-linear dimensionality reduction.
It is based on fitting K-nearest neighbor regression to the unsupervised
regression framework for learning of low-dimensional
manifolds. Similar to related approaches that are mostly based on
kernel methods, unsupervised K-nearest neighbor (UNN) regression
optimizes latent variables w.r.t. the data space reconstruction
error employing the K-nearest neighbor heuristic. The problem
of optimizing latent neighborhoods is difficult to solve, but the
UNN formulation allows the design of efficient strategies that
iteratively embed latent points to fixed neighborhood topologies.
UNN is well appropriate for sorting of high-dimensional data.
The iterative variants are analyzed experimentally.},
  comment      = {Map high dim points onto a low dimensional manifold in such a way that the indices of the k nearest neighbors in low dimensional space are also the indices of the k nearest neighbors in high dim space (or "close" in L2 norm difference of the high dim points). The idea is to make the low dim space as invertible as possible. Work is done only on a 1dim manifold but future work will include multidim.

This could be another way of reducing dimensionality during local linear quantile regression (a 1D manifold would be easy to do spline QR on). Maybe like Giglio13riskPartialQR (which also goes to 1D) but this one would only reduce the input space, like PCA does, and would not be aware of the regression target, like PLSR is. What trick does PLS use to be aware of both input and output space errors?

Anyway, I think this would do it for the whole quantile instead of the local model (which could have lasso feature selection).

I also wonder how this would compare the multidimensional scaling (MDS)
Also, wasn't there a KNN-lasso algorithm somewhere?

Idea w/ missing feats and noise margin in Kramer13SortHiDimNN (but it's still 1D)

Author also has a book:
http://link.springer.com/book/10.1007/978-3-642-38652-7},
  file         = {Kramer11dimRedKNN.pdf:Kramer11dimRedKNN.pdf:PDF},
  groups       = {Read},
  owner        = {sotterson},
  timestamp    = {2014.04.04},
  url          = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6146983},
}

@Article{Pinson07nonParamProbFrcsEval,
  author    = {Pinson, Pierre and Nielsen, Henrik Aa and M{\o}ller, Jan K and Madsen, Henrik and Kariniotakis, George N},
  title     = {Non-parametric probabilistic forecasts of wind power: required properties and evaluation},
  journal   = {Wind Energy},
  year      = {2007},
  volume    = {10},
  number    = {6},
  pages     = {497--516},
  abstract  = {In market conditions where program responsible parties are
penalized for deviations from proposed bids, energy storage can be used
for compensating the energy imbalances induced by limited predictability
of wind power. The energy storage capacity necessary for performing this
task will differ between delivery periods, according to the magnitude and
the evolution of forecast errors in each delivery period. A methodology is
presented for the assessment of the necessary storage capacity for each
delivery period, based on the degree of risk that the power producer
accepts to be exposed to. This approach leads to a dynamic assessment
of the energy storage capacity for different delivery periods. In such
a context, energy storage is used as a means of risk hedging against
penalties from the regulation market. The application of the algorithm
on real data (both measurements and forecasts) of the yearly output of
a wind farm shows that the application of a dynamic daily sizing of the
necessary storage leads to a significant reduction of the storage capacity
used, without affecting the producer?s profit significantly. The method
proposed here may provide the basis for the introduction of storage
as an independent market entity, where each producer may rent the
necessary daily storage capacity for hedging the risk of the wind power
limited predictability.
Index Terms?wind power, storage, electricity markets, forecast, un-
certainty, scenarios},
  comment   = {Evaluation of the exponential tails model used in Pinson09dynStorSiz (I don't see this on 1\textsuperscript{st} reading).

But says that adaptive quantile regression may be better at extreme values than adaptive resampling.},
  doi       = {10.1002/we.230/abstract},
  file      = {Pinson07nonParamProbFrcsEval.pdf:Pinson07nonParamProbFrcsEval.pdf:PDF},
  publisher = {Wiley Online Library},
}

@InProceedings{Pinson09dynStorSiz,
  author    = {Pinson, P. and Papaefthymiou, G. and Klockl, B. and Verboomen, J.},
  title     = {Dynamic sizing of energy storage for hedging wind power forecast uncertainty},
  booktitle = {IEEE Power \& Energy Society (PES)},
  year      = {2009},
  pages     = {1--8},
  abstract  = {In market conditions where program responsible parties are penalized for deviations from proposed bids, energy storage can be used for compensating the energy imbalances induced by limited predictability of wind power. The energy storage capacity necessary for performing this task will differ between delivery periods, according to the magnitude and the evolution of forecast errors in each delivery period. A methodology is presented for the assessment of the necessary storage capacity for each delivery period, based on the degree of risk that the power producer accepts to be exposed to. This approach leads to a dynamic assessment of the energy storage capacity for different delivery periods. In such a context, energy storage is used as a means of risk hedging against penalties from the regulation market. The application of the algorithm on real data (both measurements and forecasts) of the yearly output of a wind farm shows that the application of a dynamic daily sizing of the necessary storage leads to a significant reduction of the storage capacity used, without affecting the producer's profit significantly. The method proposed here may provide the basis for the introduction of storage as an independent market entity, where each producer may rent the necessary daily storage capacity for hedging the risk of the wind power limited predictability.},
  comment   = {Minimize up-regulation risk w/ prob. forecast scenarios
* Use quantile regression based scenarios from Pinson09probFrcstStatScenWind

* trajectory of demand/windpower discrepancy smoothed by simulated hydro storage
* can have v. different storage rqts. dep. on trajectory

Simple test
* just shoot for fixed risk of unmet demand
* pdf for risk comes from histogram of 120 forecast scenario
* bidding once per day for next 24 hours (I don't quite get this)

* extreme events modeled w/ exponential tail, not sure how well this works (w/ Gaussian copula used here0
 (the method is evaluated in Pinson07nonParamProbFrcsEval)
* test system is 2 hydro resevoirs w/ costs, max ramp rates, and losses

* but just see what the cost turns out to be w/ risk limit; don't optimize on price
* NOT CLEAR where energy demand signal comes from

Conclusion
* can make almost as much profit w/ small risk as w/ infinite storage.
-- I'm not sure how surprised I am by this
-- seems like low risk means EES almost as big as energy stored w/ infinite storage?
* anyway, see some increased operating profit w/ storage
-- w/o get 0.85 of that possible w/ max storage
-- cost of reservoirs not compared to increased profit, though},
  doi       = {10.1109/PES.2009.5275816},
  file      = {Pinson09dynStorSiz.pdf:Pinson09dynStorSiz.pdf:PDF},
  groups    = {Read, Use, doReadNonWPV_1},
  issn      = {1944-9925},
  keywords  = {energy storage, wind power plants, delivery periods, dynamic sizing, energy imbalances, energy storage, hedging wind power forecast uncertainty},
  owner     = {scotto},
  timestamp = {2010.12.12},
}

@TechReport{cigre02thermRateStd,
  author      = {cigr{\'e}},
  title       = {Thermal Behaviour of overhead conductors},
  institution = {cigr{\'e}},
  year        = {2002},
  type        = {Report of Working Group 22.12},
  number      = {203, Brochure 207},
  month       = aug,
  abstract    = {In most countries the demand for electric power is constantly increasing, and there is a corresponding requirement to increase the power transferred by transmission and distribution lines. A solution would be
to build new lines, but this may not be feasible on account of.economic or enviromnental consideration.
Hence, there may be pressure to increase the load transfer capacity ofboth new and old lines.
The maximum load capacity of a long line is usually dictated by consideration of system stability,
permissible voltage regulation or the cost of energy losses. The capacity of a shorter line may be
determined by the maximum permissible operating temperature of the conductors, assuming that the
joints and clamps are in good condition and are not a constraint in operation. The maximum permissible
temperature is that which results in the greatest pennissible sag (allowing for creep), or that which results
in the maximum allowable loss of tensile strength by annealing throughout the life of the conductor.},
  comment     = {Transmission line thermal rating standard probably used in Europe? Could be used in: Matus12spanDynLinRat},
  file        = {cigre02thermRateStd.pdf:cigre02thermRateStd.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2014.03.18},
  url         = {http://c1.cigre.org/Media/SC/B2/Cigre-Technical-Brochure-207-August-2002},
}

@Article{Ozturk00spline4D,
  author    = {Ozturk, C and McVeigh, ER},
  title     = {Four-dimensional B-spline based motion analysis of tagged MR images: introduction and in vivo validation},
  journal   = {Physics in Medicine and Biology},
  year      = {2000},
  volume    = {45},
  number    = {6},
  pages     = {1683--1702},
  month     = jun,
  issn      = {0031-9155},
  abstract  = {In MRI tagging, magnetic tags-spatially encoded magnetic saturation planes-are created within tissues acting as temporary markers. Their deformation pattern provides useful qualitative and quantitative information about the functional properties of underlying tissue and allows non-invasive analysis of mechanical function. The measured displacement at a given tag point contains only unidirectional information; in order to track the full 3D motion, these data have to be combined with information from other orthogonal tag sets over all time frames. Here, we provide a method to describe the motion of the heart using a four-dimensional tensor product of B-splines. In vivo validation of this tracking algorithm is performed using different tagging sets on the same heart. Using the validation results, the appropriate control point density was determined for normal cardiac motion tracking. Since our motion fields are parametric and based on an image plane based Cartesian coordinate system, trajectories or other derived values (velocity, acceleration, strains...) can be calculated for any desired point within the volume spanned by the control points. This method does not rely on specific chamber geometry, so the motion of any tagged structure can be tracked. Examples of displacement and strain analysis for both ventricles are also presented.},
  comment   = {B-spline of 3dims, w/ time as 4\textsuperscript{th} dim. Use for AR model of lagged velocity? maybe also interesting b/c this is modeling _displacements_ which is kind of like derivatives that I found to be useful for prediction! an improvement may be the 3D cylindrical coordinate spline in Deng04cylindSpln3D Jian08volCylBspln may show how to turn this into matrix form},
  file      = {Ozturk00spline4D.pdf:Ozturk00spline4D.pdf:PDF},
  owner     = {scotto},
  timestamp = {2010.08.20},
  unique-id = {ISI:000087802000020},
}

@TechReport{IWES14totProjDescReWPR,
  author      = {IWES},
  title       = {Gesamtvorhabensbeschreibung: ReWP -- Regelleistung durch Wind- und Photovoltaikparks},
  institution = {Fraunhofer IWES},
  year        = {2014},
  abstract    = {In order to ensure security of electricity supply, set the transmission system operator (TSO) is a
socalled
system services. One of these system services is the controlling power. The systems
that provide control power, balance out the imbalance between production and consumption,
which in the nominal grid frequency can be kept. The control power is available in three different
grades, which differ mainly in terms of their rate of activation. The primary control power must be
able to be fully activated within 30 seconds. In the secondary control is 5 minutes away and the
minute reserve 15 minutes.
Due to the expansion of renewable energies, particularly fluctuating generator, two challenges
arise in relation to the control performance. Firstly, the demand for balancing power increases
(CONSENTEC2010) due to the increasing with the expansion of wind energy and photovoltaic
forecasts sefehlers. On the other hand, reduces the number of conventional power plants, which
have been largely provided control power. Therefore, it is important that also provide wind and
photovoltaic parks control power (DENA2010), so that these challenges can be met. Currently
provide wind and photovoltaic parks in Germany ready no control power. Furthermore it can be
avoided by the control power deployment by wind and photo voltaikparks the expensive high or
shutdown of power plants and the construction of storage capacities (DENA2010). Furthermore,
it increases competition increased on the control performance market (BM2010), which in turn
should reduce costs.
In the BMUfunded
projects balancing energy through wind turbines and combined cycle power
plant 2 has already been working on the control power deployment by wind and photovoltaic
parks. To this end, tools and methods have been developed such as the new detection methods
possible feed which has also been demonstrated in a field test. However, the developed in this
projects
procedures and tools have two drawbacks. For one, they did not meet all the
requirements and secondly, they were based largely on the existing regulations, mainly because
the feasibility should be shown. Therefore, it is important to develop tools and methods that allow
an optimum of total},
  comment     = {Total project description: ReWP -- Control power by wind and photovoltaic parks

Predecessor project was ReW: Brauns14RegelWindLeist},
  file        = {English:IWES14totProjDescReWPR_trans.pdf:PDF;German:IWES14totProjDescReWPR.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2014.11.05},
}

@InProceedings{Bofinger02windPowFrcstBeta,
  author    = {Bofinger, S and Luig, A and Beyer, HG},
  title     = {Qualification of wind power forecasts},
  booktitle = {CD-Proceedings of the Global Wind Power Conference},
  year      = {2002},
  abstract  = {In order to manage the remarkable share of wind power as present in several utilities forecast information is needed. In the last years several respective procedures have been developed and put into operation. Up to now the outcome of the forecast tools is mainly restricted to the forecasted value itself and general information of the overall error of the procedure as e.g. the standard deviation. However, for the handling of these forecast information in the framework of e.g. power station dispatch schemes, more detailed information of the structure of the expected errors - beyond the expected standard deviation - seem to be desirable. Due to the fact that the output of wind turbine systems is limited between zero and the maximum power, the error statistics cannot follow a normal distribution. Thus, for the assessment of the probability of occurrence of a certain forecast error a model for the distribution function of the errors has to be set up. We have analysed the errors of the forecast model PREVIENTO as applied for an ensemble of installations representing the lumped power output of the turbines within a given region. Given bias free forecasts, the applicability of various models for the distribution function of the set of errors has been tested. It turned out, that the use of a beta function is justifiable for this task with respect to chi-squared tests. Together with an empirically derived parametric model for the expected standard deviation of the ensemble forecast, the knowledge of the respective distribution function allows for the assignment of risk figures to any decision taken based on the wind power forecast.},
  comment   = {Stefan Bofinger's paper recommending beta distribution for characterizing wind power forecast error.

For BMA, or ensemble dressing, could be a better ensemble distribution choice than a Gaussian. This must be more EM'able than a NN or whatever.},
  file      = {Bofinger02windPowFrcstBeta.pdf:Bofinger02windPowFrcstBeta.pdf:PDF},
  groups    = {ErrDistProps, doReadNonWPV_1, doReadNonWPV_2},
  owner     = {sotterson},
  timestamp = {2013.02.18},
  url       = {https://www.hs-magdeburg.de/fachbereiche/f-iwid/ET/Personen/prof.-beyer/Qualification%20of%20wind%20power%20forecasts%20GWP02.pdf},
}

@InProceedings{Lange03uncertFrcstMetSit,
  author    = {Matthias Lange and Detlev Heinemann},
  title     = {Relating the uncertainty of short-term wind speed predictions to meteorological situations},
  booktitle = {Proceedings of the European Wind Energy Conference EWEC},
  year      = {2003},
  abstract  = {In order to provide an uncertainty estimate for short-term wind power predictions the accuracy of the underlying wind speed prediction is assessed quantitatively for different meteorological situations. With methods from synoptic climatology an automatic classification scheme is implemented using measurements of wind speed, wind direction and pressure at mean sea level to characterize the local weather conditions at a site. The classification procedure involves principal component analysis
to efficiently reduce the data to the most relevant patterns. Cluster analysis is used to group days with similar meteorological conditions into common classes. A comparison of these clusters with weather maps shows that typical weather patterns are successfully captured by the classification scheme. The mean forecast error of the wind speed prediction of the German Weather Service is calculated for each of the clusters. It is found that different meteorological situations have indeed significant differences in the prediction error where the highest rmse can be by a factor 1.3 to 1.6 larger than the smallest
rmse. Typically, high uncertainties in the forecast have to be expected in situations where low pressure systems quickly pass north of the site while stationary high pressure situations have smaller forecast errors.

Keywords: short-term prediction, wind power forecast, uncertainty, meteorology},
  comment   = {PCA of met measurements yield weather classes which are consistently assiciated with more or less forecast error (max error is up to 1.6 x min). This is YET ANOTHER REASON for getting 10m weather station data...},
  file      = {Lange03uncertFrcstMetSit.pdf:Lange03uncertFrcstMetSit.pdf:PDF},
  groups    = {PointDerived, doReadWPV_1},
  location  = {Madrid},
  owner     = {sotterson},
  timestamp = {2014.02.05},
  url       = {http://www.energymeteo.com/en/downloads/downloads.php},
}

@Article{Xu07optWgtPLS,
  author    = {Xu, Lu and Jiang, Jian-Hui and Lin, Wei-Qi and Zhou, Yan-Ping and Wu, Hai-Long and Shen, Guo-Li and Yu, Ru-Qin},
  title     = {Optimized sample-weighted partial least squares},
  journal   = {Talanta},
  year      = {2007},
  volume    = {71},
  number    = {2},
  pages     = {561--566},
  abstract  = {In ordinary multivariate calibration methods, when the calibration set is determined to build the model describing the relationship between the dependent variables and the predictor variables, each sample in the calibration set makes the same contribution to the model, where the difference of representativeness between the samples is ignored. In this paper, by introducing the concept of weighted sampling into partial least squares (PLS), a new multivariate regression method, optimized sample-weighted PLS (OSWPLS) is proposed. OSWPLS differs from PLS in that it builds a new calibration set, where each sample in the original calibration set is weighted differently to account for its representativeness to improve the prediction ability of the algorithm. A recently suggested global optimization algorithm, particle swarm optimization (PSO) algorithm is used to search for the best sample weights to optimize the calibration of the original training set and the prediction of an independent validation set. The proposed method is applied to two real data sets and compared with the results of PLS, the most significant improvement is obtained for the meat data, where the root mean squared error of prediction (RMSEP) is reduced from 3.03 to 2.35. For the fuel data, OSWPLS can also perform slightly better or no worse than PLS for the prediction of the four analytes. The stability and efficiency of OSWPLS is also studied, the results demonstrate that the proposed method can obtain desirable results within moderate PSO cycles.
Keywords

 Partial least squares (PLS);
 Optimized sample-weighted PLS (OSWPLS);
 Particle swarm optimization (PSO) algorithm;
 Weighted samples;
 Sampling theory},
  comment   = {weighted pls alg. for local learning, maybe even a way to do the clustering, given that there's an optimization alg in there.},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.04.27},
  url       = {http://www.sciencedirect.com/science/article/pii/S0039914006003080},
}

@InProceedings{Yang07distMetricLrnOvrvw,
  author    = {Yang, Liu},
  title     = {An overview of distance metric learning},
  booktitle = {Proceedings of the Computer Vision and Pattern Recognition Conference},
  year      = {2007},
  url       = {https://www.cs.cmu.edu/~liuy/dist_overview.pdf},
  abstract  = {In our previous comprehensive survey [41], we have categorized the disparate is-
sues in distance metric learning. Within each of the four categories, we have summa-
rized existing work, disclosed their essential connections, strengths and weaknesses.
The first category is supervised distance metric learning, which contains supervised
global distance metric learning, local adaptive supervised distance metric learning,
Neighborhood Component Analysis (NCA) [13], and Relevant Components Analy-
sis (RCA) [1]. The second category is unsupervised distance metric learning, cov-
ering linear (Principal Component Analysis (PCA) [14], Multidimensional Scaling
(MDS) [5]) and nonlinear embedding methods (ISOMAP [35], Locally Linear Em-
bedding (LLE) [30], and Laplacian Eigenamp (LE) [2]). We further unify these al-
gorithms into a common framework based on the embedding computation. The third
category, which is maximum margin based distance metric learning approaches, in-
cludes the large margin nearest neighbor based distance metric learning methods and
semi-definite Programming (SDP) methods to solve the kernelized margin maximiza-
tion problem. And the fourth category discussing kernel methods towards learning
distance metrics, covers kernel alignment [28] and its SDP approaches [26], and also
the extension work of learning the idealized kernel [25].},
  file      = {paper:Yang07distMetricLrnOvrvw.pdf:PDF},
}

@Article{Bo09probLMPfrcstLdUncrt,
  author    = {Rui Bo and Fangxing Li},
  title     = {Probabilistic {LMP} Forecasting Considering Load Uncertainty},
  journal   = {{IEEE} Transactions on Power Systems},
  year      = {2009},
  volume    = {24},
  number    = {3},
  pages     = {1279--1289},
  month     = {aug},
  abstract  = {In power market studies, the forecast of locational
marginal price (LMP) relies on the load forecasting results from
the viewpoint of planning. It is well known that short-term load
forecasting results always carry certain degree of errors mainly
due to the random nature of the load. At the same time, LMP step
changes occur at critical load levels (CLLs). Therefore, it is inter-
esting to investigate the impact of load forecasting uncertainty on
LMP. With the assumption of a certain probability distribution of
the actual load, this paper proposes the concept of probabilistic
LMP and formulates the probability mass function of this random
variable. The expected value of probabilistic LMP is then derived,
as well as the lower and upper bound of its sensitivity. In addition,
two useful curves, alignment probability of deterministic LMP
versus forecasted load and expected value of probabilistic LMP
versus forecasted load, are presented. The first curve is designed
to identify the probability that the forecasted price in a determin-
istic LMP matches the actual price at the forecasted load level.
The second curve is demonstrated to be smooth and therefore
eliminates the step changes in deterministic LMP forecasting.
This helps planners avoid the possible sharp changes during
decision-making process. The proposed concept and method are
illustrated with a modified PJM five-bus system and the IEEE
118-bus system.
Index Terms—Critical load level, energy markets, load fore-
casting, locational marginal pricing (LMP), normal distribution,
optimal power flow (OPF), power markets, probabilistic LMP
forecasting, uncertainty.},
  doi       = {10.1109/tpwrs.2009.2023268},
  file      = {:Bo09probLMPfrcstLdUncrt.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Matos11rsrvProbFrcst,
  author    = {Matos, M.A. and Bessa, R.J.},
  title     = {Setting the Operating Reserve Using Probabilistic Wind Power Forecasts},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2011},
  volume    = {26},
  number    = {2},
  pages     = {594--603},
  month     = may,
  issn      = {0885-8950},
  abstract  = {In power systems with a large integration of wind power, setting the adequate operating reserve levels is one of the main concerns of system operators (SO). The integration of large shares of wind generation in power systems led to the development of new forecasting methodologies, including probabilistic forecasting tools, but management tools able to use those forecasts to help making operational decisions are still needed. In this paper, a risk evaluation perspective is used, showing that it is possible to describe the consequences of each possible reserve level through a set of risk indices useful for decision making. The new reserve management tool (RMT) described in the paper is intended to support the SO in defining the operating reserve needs for the daily and intraday markets. Decision strategies like setting an acceptable risk level or finding a compromise between economic issues and the risk of loss of load are explored. An illustrative example based on the Portuguese power system demonstrates the usefulness and efficiency of the tool.},
  comment   = {Markus Speckman' papers on German Primary/Secondary/Tertiary res. rqts.},
  doi       = {10.1109/TPWRS.2010.2065818},
  file      = {Matos11rsrvProbFrcst.pdf:Matos11rsrvProbFrcst.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  keywords  = {daily market;decision making;intraday market;management tools;operating reserve levels;operational decisions;probabilistic wind power forecasts;reserve management tool;risk evaluation;system operators;wind generation;decision making;load forecasting;power generation economics;power markets;wind power plants;},
  owner     = {sotterson},
  timestamp = {2012.02.09},
}

@Article{Gneiting13CombinPredDist,
  author    = {Gneiting, Tilmann and Ranjan, Roopesh and others},
  title     = {Combining predictive distributions},
  journal   = {Electronic Journal of Statistics},
  year      = {2013},
  volume    = {7},
  pages     = {1747--1782},
  abstract  = {In probabilistic forecasting combination formulas for the ag-
gregation of predictive distributions need to be estimated based on past
experience and training data. We study combination formulas and aggrega-
tion methods for predictive cumulative distribution functions from the per-
spectives of calibration and dispersion, taking an original prediction space
approach that applies to discrete, mixed discrete-continuous and continu-
ous predictive distributions alike. The key idea is that aggregation methods
ought to be parsimonious, yet sufficiently flexible to accommodate any type
of dispersion in the component distributions. Both linear and non-linear ag-
gregation methods are investigated, including generalized, spread-adjusted
and beta-transformed linear pools. The effects and techniques are demon-
strated theoretically, in simulation examples, and in case studies, where we
fit combination formulas for density forecasts of S&P 500 returns and daily
maximum temperature at Seattle-Tacoma Airport.
AMS 2000 subject classifications: Primary 62; secondary 91B06.
Keywords and phrases: Beta transform, conditional calibration, density
forecast, flexibly dispersive, forecast aggregation, linear pool, probability
integral transform, probabilistic calibration.},
  comment   = {How to combine density forecasts. Useful for probabilistic upscaling or RES pooling? Also possibly for the probabilistic adaboost ideas (which could combine density forecasts).},
  file      = {Gneiting13CombinPredDist.pdf:Gneiting13CombinPredDist.pdf:PDF},
  groups    = {Upscaling (prob), doReadNonWPV_1},
  owner     = {sotterson},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2014.05.26},
  url       = {http://projecteuclid.org/euclid.ejs/1372861687},
}

@InCollection{Zalewski12saxSlope,
  author    = {Zalewski, Willian and Silva, Fabiano and Wu, Feng Chung and Lee, Huei Diana and Maletzke, Andr {\'e} Gustavo},
  title     = {A Symbolic Representation Method to Preserve the Characteristic Slope of Time Series},
  booktitle = {Advances in Artificial Intelligence - SBIA 2012},
  publisher = {Springer Berlin Heidelberg},
  year      = {2012},
  editor    = {Barros, LelianeN. and Finger, Marcelo and Pozo, AuroraT. and Gimen??nez-Lugo, GustavoA. and Castilho, Marcos},
  series    = {Lecture Notes in Computer Science},
  pages     = {132--141},
  isbn      = {978-3-642-34458-9},
  abstract  = {In recent years many studies have been proposed for knowledge discovery in time series. Most methods use some technique to transform raw data into another representation. Symbolic representations approaches have shown effectiveness in speedup processing and noise removal. The current most commonly used algorithm is the Symbolic Aggregate Approximation (SAX). However, SAX doesn't preserve the slope information of the time series segments because it uses only the Piecewise Aggregate Approximation for dimensionality reduction. In this paper, we present a symbolic representation method to dimensionality reduction and discretization that preserves the behavior of slope characteristics of the time series segments. The proposed method was compared with the SAX algorithm using artificial and real datasets with 1-nearest-neighbor classification. Experimental results demonstrate the method effectiveness to reduce the error rates of time series classification and to keep the slope information in the symbolic representation.},
  comment   = {A Sax technique that preserves slope. Use to cluster wind ramps, ether has a hint for training/boosting, or as a way to improve ramp forecasts?

Could be another analog esemble technique.},
  doi       = {10.1007/978-3-642-34459-6_14},
  file      = {Zalewski12saxSlope.pdf:Zalewski12saxSlope.pdf:PDF},
  groups    = {Ensemble, doReadNonWPV_1},
  keywords  = {Time Series; Knowledge Discovery; Symbolic Representation; Classification; Dimensionality Reduction},
  owner     = {sotterson},
  timestamp = {2013.03.15},
}

@Article{Dalto14NshrtWindFrcstNNpmi,
  author    = {{\DH}alto, Mladen and Va{\v{s}}ak, Mario and Baotic, Mato and Matu{\v{s}}ko, Jadranko and Horvath, Kristian},
  title     = {Neural-network-based ultra-short-term wind forecasting},
  journal   = {European Wind Energy Association 2014 Annual Event (EWEA 2014)},
  year      = {2014},
  abstract  = {In recent years rapid growth of wind power generation
in many countries around the world has highlighted
the importance of wind prediction. In this work neural
networks are used for ultra-short-term wind prediction.
In many instances reported in the literature neural
network exhibit poor performance - very often because
no complexity reduction methods were considered. To
that end, in this paper two input variable selection algorithms
based on partial mutual information are compared
for further use with nonlinear models such as neural
networks. Performance improvements of the proposed
prediction system are compared to neural networks without
input variable selection, and validated for locations
near Split, Croatia. The use of neural network drastically
outperforms simple persistence estimator on 3 hour
horizon.},
  comment   = {NN wind power forecast down to 10 minute horizon. Uses KDE partial mutual information to select features. KDE PMI is touted as having a good stopping criteria, but I think Fig. 2 suggests that the KNN PMI in Frenzel07partMutInfo is more senstive.

KDE/KNN efficiency opinion here:
http://stats.stackexchange.com/questions/35089/feature-selection-using-mutual-information-in-matlab
Kernel based PMI: (+) has a stopping criteria (Akaike Information Criteria) (-) higher complexity
kNN based PMI: (-) does not have a stopping criteria (+) lower complexity

* has a good explanation of PMI

PMI is also used this way in May08partMutInfoWaterFrcst, Dalto14NshrtWindFrcstNNpmi and others},
  file      = {Dalto14NshrtWindFrcstNNpmi.pdf:Dalto14NshrtWindFrcstNNpmi.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.01.30},
  url       = {https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjZxJXE9PTWAhVlDJoKHZwyCKYQFggpMAA&url=https%3A%2F%2Fbib.irb.hr%2Fdatoteka%2F692827.EWEA_ultra_short.pdf&usg=AOvVaw3fNDNdya5Blukrn50EJpEJ},
}

@Article{Nielsen98newWindRef,
  author    = {Nielsen, T. S},
  title     = {A New Reference for Wind Power Forecasting},
  journal   = {Wind Energy},
  year      = {1998},
  volume    = {1},
  pages     = {29--34},
  abstract  = {In recent years some research towards developing forecasting models for wind power or energy has been carried out. In order to evaluate the prediction ability of these models, the forecasts are usually compared with those of the persistence forecast model. As shown in this article, however, it is not reasonable to use the persistence model when the forecast length is more than a few hours. Instead, a new statistical reference for predicting wind power, which basically is a weighting between the persistence and the mean of the power, is proposed. This reference forecast model is adequate for all forecast lengths and, like the persistence model, requires only measured time series as input.},
  comment   = {Kristin recommends using this to evaluate forecasting algorithms. A combo of climatology and persistence},
  file      = {Nielsen98newWindRef.pdf:Nielsen98newWindRef.pdf:PDF;Nielsen98newWindRef.pdf:Nielsen98newWindRef.pdf:PDF},
  owner     = {scotto},
  timestamp = {2008.12.27},
}

@Article{Sadegh13ABChyrdro,
  author    = {Sadegh, M and Vrugt, JA},
  title     = {Approximate {Bayes}ian Computation in hydrologic modeling: equifinality of formal and informal approaches},
  journal   = {Hydrology and Earth System Sciences Discussions},
  year      = {2013},
  volume    = {10},
  number    = {4},
  pages     = {4739--4797},
  abstract  = {In recent years, a strong debate has emerged in the hydrologic literature how to properly
treat non-traditional error residual distributions and quantify parameter and predictive
uncertainty. Particularly, there is strong disagreement whether such uncertainty
framework 5 should have its roots within a proper statistical (Bayesian) context using
Markov chain Monte Carlo (MCMC) simulation techniques, or whether such a framework
should be based on a quite different philosophy and implement informal likelihood
functions and simplistic search methods to summarize parameter and predictive
distributions. In this paper we introduce an alternative framework, called Approximate
Bayesian Computation (ABC) that summarizes the differing viewpoints of formal and
informal Bayesian approaches. This methodology has recently emerged in the fields of
biology and population genetics and relaxes the need for an explicit likelihood function
in favor of one or multiple different summary statistics that measure the distance of
each model simulation to the data. This paper is a follow up of the recent publication
15 of Nott et al. (2012) and further studies the theoretical and numerical equivalence of
formal (DREAM) and informal (GLUE) Bayesian approaches using data from different
watersheds in the United States. We demonstrate that the limits of acceptability approach
of GLUE is a special variant of ABC in which each discharge observation of the
calibration data set is used as a summary diagnostic.},
  comment   = {An example of ABC use in forecasting (hydro flood). Also, pointers to other techniques in the hydro literature where distribution simplifications are used.

POSSIBLE USES: any of the probabilistic forecast ideas. Note that there is an R implementation of a kind of ABC in one of the other ABC articles I have bibtexed

Note reference to "simplistic search methods" which might be kinda like analog ensemble forecasts},
  file      = {Sadegh13ABChyrdro.pdf:Sadegh13ABChyrdro.pdf:PDF},
  groups    = {Ensemble, PointDerived, Test, doReadNonWPV_1},
  owner     = {sotterson},
  publisher = {Copernicus GmbH},
  timestamp = {2013.10.04},
}

@TechReport{Maaten09DimRedCmprTechNote,
  author      = {Laurens van der Maaten and Eric Postma and Jaap van den Herik},
  title       = {Dimensionality Reduction: A Comparative Review},
  institution = {TiCC, Tilburg University},
  year        = {2009},
  type        = {Tech. Report},
  number      = {TiCC TR 2009?005},
  month       = oct,
  abstract    = {In recent years, a variety of nonlinear dimensionality reduction techniques have been
proposed that aim to address the limitations of traditional techniques such as PCA
and classical scaling. The paper presents a review and systematic comparison of
these techniques. The performances of the nonlinear techniques are investigated on
artificial and natural tasks. The results of the experiments reveal that nonlinear techniques
perform well on selected artificial tasks, but that this strong performance does
not necessarily extend to real-world tasks. The paper explains these results by identifying
weaknesses of current nonlinear techniques, and suggests how the performance
of nonlinear dimensionality reduction techniques may be improved},
  comment     = {Tech note for Matlab Toolbox for Dimensionality Reduction

* Links between MDS and PCA, and refs to more (p. 4, but it's not totally clear to me).
* out-of-sample extension for MDS is described here: Bengio04OutOfSmplExt

Corresponding, shorter paper: Maaten09DimRedCmpr

Matlab Toolbox for Dimensionality Reduction (v0.8.1 - March 2013)
http://homepage.tudelft.nl/19j49/Matlab_Toolbox_for_Dimensionality_Reduction.html},
  file        = {Maaten09DimRedCmprTechNote.pdf:Maaten09DimRedCmprTechNote.pdf:PDF},
  location    = {Tilburg, The Netherlands},
  owner       = {sotterson},
  timestamp   = {2014.06.26},
  url         = {http://homepage.tudelft.nl/19j49/Publications.html},
}

@Article{Maaten09DimRedCmpr,
  author    = {van der Maaten, Laurens JP and Postma, Eric O and van den Herik, H Jaap},
  title     = {Dimensionality reduction: A comparative review},
  journal   = {Journal of Machine Learning Research},
  year      = {2009},
  volume    = {10},
  number    = {1-41},
  pages     = {66--71},
  abstract  = {In recent years, a variety of nonlinear dimensionality reduction techniques have been proposed that aim to address the limitations
of traditional techniques such as PCA. The paper presents a review and systematic comparison of these techniques. The performances of the nonlinear techniques are investigated on artificial and natural tasks. The results of the experiments reveal that nonlinear techniques perform well on selected artificial tasks, but do not outperform the traditional PCA on real-world tasks.
The paper explains these results by identifying weaknesses of current nonlinear techniques, and suggests how the performance of nonlinear dimensionality reduction techniques may be improved.
Key words: Dimensionality reduction, manifold learning, feature extraction.},
  comment   = {Highly cited dimensionality reduction review. Includes refs to intrinsic dimension estimation. Many techniques have matlab here:

Matlab Toolbox for Dimensionality Reduction (v0.8.1 - March 2013)
http://homepage.tudelft.nl/19j49/Matlab_Toolbox_for_Dimensionality_Reduction.html

Technote has much more discussion. See: Maaten09DimRedCmprTechNote},
  file      = {Maaten09DimRedCmpr.pdf:Maaten09DimRedCmpr.pdf:PDF},
  owner     = {sotterson},
  publisher = {Citeseer},
  timestamp = {2014.05.12},
  url       = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.125.6716&rep=rep1&type=pdf},
}

@Article{Schmidhuber15deepLrnOvrvw,
  author    = {J{\"{u}}rgen Schmidhuber},
  title     = {Deep learning in neural networks: An overview},
  journal   = {Neural Networks},
  year      = {2015},
  volume    = {61},
  pages     = {85--117},
  abstract  = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in
pattern recognition and machine learning. This historical survey compactly summarizes relevant work,
much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth
of their credit assignment paths, which are chains of possibly learnable, causal links between actions
and effects. I review deep supervised learning (also recapitulating the history of backpropagation),
unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short
programs encoding deep and large networks.},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/nn/Schmidhuber15},
  comment   = {Another good overview: Bengio13repLrnOvrvw},
  doi       = {10.1016/j.neunet.2014.09.003},
  file      = {:Schmidhuber15deepLrnOvrvw.pdf:PDF},
}

@Article{VonLuxburg07specClustTut,
  author    = {Ulrike von Luxburg},
  title     = {A tutorial on spectral clustering},
  journal   = {Statistics and computing},
  year      = {2007},
  volume    = {17},
  number    = {4},
  pages     = {395--416},
  abstract  = {In recent years, spectral clustering has become one of the most popular modern clustering
algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software,
and very often outperforms traditional clustering algorithms such as the k-means algorithm. On
the frst glance spectral clustering appears slightly mysterious, and it is not obvious to see why
it works at all and what it really does. The goal of this tutorial is to give some intuition on
those questions. We describe diffierent graph Laplacians and their basic properties, present the
most common spectral clustering algorithms, and derive those algorithms from scratch by several
different approaches. Advantages and disadvantages of the different spectral clustering algorithms
are discussed.
Keywords: spectral clustering; graph Laplacian},
  comment   = {Tutorial says asymmetric Lapace is best, other fundamentals on spectral clustering. Same other has a newer paper on KNN clustering w/ arbitrary functions (Bubeck09knnClustArbitObj)},
  doi       = {10.1007/s11222-007-9033-z},
  file      = {VonLuxburg07specClustTut.pdf:VonLuxburg07specClustTut.pdf:PDF},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2014.04.23},
}

@InCollection{Aggarwal01SrprsDistMetHighDim,
  author    = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
  title     = {On the Surprising Behavior of Distance Metrics in High Dimensional Space},
  booktitle = {Database Theory {ICDT} 2001},
  publisher = {Springer Berlin Heidelberg},
  year      = {2001},
  editor    = {Bussche, Jan Van den and Vianu, Victor},
  number    = {1973},
  series    = {Lecture Notes in Computer Science},
  pages     = {420--434},
  note      = {{DOI}: 10.1007/3-540-44503-X\_27},
  abstract  = {In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a effciency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used L k norm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric L(1 norm) is consistently more preferable than the Euclidean distance metric L(2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the L k norm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.},
  comment   = {Likes Manhattan distance & fractional norms (Lf norms) for high dimensional classification.  Fractional norms may not satisfy triangle inequalites (that KD trees need, I think)},
  file      = {Aggarwal01SrprsDistMetHighDim.pdf:Aggarwal01SrprsDistMetHighDim.pdf:PDF},
  keywords  = {Algorithm Analysis and Problem Complexity, Database Management, Data Structures, Cryptology and Information Theory, Information Storage and Retrieval, Information Systems Applications (incl. Internet), Mathematical Logic and Formal Languages},
  langid    = {english},
  rights    = {??2001 Springer-Verlag Berlin Heidelberg},
  url       = {http://link.springer.com/chapter/10.1007/3-540-44503-X_27},
  urldate   = {2016-10-31},
}

@InBook{Giebel13possiblePowOfshrWind,
  title     = {POSSPOW: Possible Power of Offshore Wind Power Plants},
  publisher = {European Wind Energy Association (EWEA)},
  year      = {2013},
  author    = {Gregor Giebel and Tuhfe G??men and S?rensen, {Poul Ejnar} and Poulsen, {Niels Kj?lstad} and {Runge Kristoffersen}, Jesper},
  abstract  = {In recent years, the large offshore wind farms were designed as wind power plants, including possibilities to contribute to the stability of the grid by offering grid services (also called ancillary services). One of those services is reserve power, which is achieved by down-regulating the wind farm from its maximum possible power in order to quickly ramp production up. Due to the nature of the turbines the power can be ramped up quite quickly, within a few seconds. Already during the planning phase of the Nysted and Horns Rev offshore wind farms, the developers worked closely together with turbine manufacturers to get an extra signal in the SCADA data feed from the individual turbines called possible power. In normal operation, this would be the actual power, but during down-regulation it would give the possible power given the current wind regime. While in the Horns Rev controller it was called possible power, in the grid codes it is usually referred to as available power, therefore this nomenclature should be used further. This signal is quite reliable ? the grid code of the System Operator of Northern Ireland SONI requires an accuracy of 5%, but in a down-regulated wind farm, the sum of the possible and actual power from a down-regulated wind farm is not the same as the regulation power reserve in that wind farm since turbines downwind of down-regulated turbines see more wind that would be there without the regulation. In order to be able to offer reserve power in the market, the System Operators need to be able to trust the level of reserves. However, currently Energinet.dk, UK National Grid and other Transmission System Operators (TSOs) have no real way to determine exactly the possible power of a whole wind farm which is down-regulated.},
  booktitle = {Proceedings of EWEA 2013},
  comment   = {DTU's latest possible power results. Good for ReWP},
  file      = {Giebel13possiblePowOfshrWind.pdf:Giebel13possiblePowOfshrWind.pdf:PDF},
  keywords  = {Electricity markets, System management, Integration strategies & policies, Balancing},
  owner     = {sotterson},
  timestamp = {2014.12.05},
  url       = {http://orbit.dtu.dk/en/publications/posspow-possible-power-of-offshore-wind-power-plants%283b02adc5-25f2-4b99-ab28-111190ef1048%29.html},
}

@Article{Lemke10metaFrcstCombo,
  author    = {Christiane Lemke and Bogdan Gabrys},
  title     = {Meta-learning for time series forecasting and forecast combination},
  journal   = {Neurocomputing},
  year      = {2010},
  volume    = {73},
  number    = {10-12},
  pages     = {2006--2016},
  issn      = {0925-2312},
  note      = {Subspace Learning / Selected papers from the European Symposium on Time Series Prediction},
  abstract  = {In research of time series forecasting, a lot of uncertainty is still related to the task of selecting an appropriate forecasting method for a problem. It is not only the individual algorithms that are available in great quantities; combination approaches have been equally popular in the last decades. Alone the question of whether to choose the most promising individual method or a combination is not straightforward to answer. Usually, expert knowledge is needed to make an informed decision, however, in many cases this is not feasible due to lack of resources like time, money and manpower. This work identifies an extensive feature set describing both the time series and the pool of individual forecasting methods. The applicability of different meta-learning approaches are investigated, first to gain knowledge on which model works best in which situation, later to improve forecasting performance. Results show the superiority of a ranking-based combination of methods over simple model selection approaches.},
  comment   = {Comparison of combination or machine learning selection for the best forecast; simple combo is best? forecast picking features were derived, which as also interesting.

Related to ensemble picking, BMA, etc.},
  doi       = {DOI: 10.1016/j.neucom.2009.09.020},
  file      = {Lemke10metaFrcstCombo.pdf:Lemke10metaFrcstCombo.pdf:PDF},
  groups    = {Ensemble, doReadWPV_2},
  keywords  = {Forecasting},
  owner     = {scot},
  timestamp = {2010.07.01},
  url       = {http://www.sciencedirect.com/science/article/B6V10-4YJCKS5-3/2/712234f17cbba7970e06c9c3f23ecab8},
}

@Article{Guastaroba09scenarioEffect,
  author    = {Guastaroba, Gianfranco and Mansini, Renata and Speranza, M. Grazia},
  title     = {On the effectiveness of scenario generation techniques in single-period portfolio optimization},
  journal   = {European Journal of Operational Research},
  year      = {2009},
  volume    = {192},
  number    = {2},
  pages     = {500--511},
  month     = jan,
  issn      = {0377-2217},
  abstract  = {In single-period portfolio selection problems the expected value of both the risk measure and the portfolio return have to be estimated. Historical data realizations, used as equally probable scenarios, are frequently used to this aim. Several other parametric and non-parametric methods can be applied. When dealing with scenario generation techniques practitioners are mainly concerned on how reliable and effective such methods are when embedded into portfolio selection models. In this paper we survey different techniques to generate scenarios for the rates of return. We also compare the techniques by providing in-sample and out-of-sample analysis of the portfolios obtained by using these techniques to generate the rates of return. Evidence on the computational burden required by the different techniques is also provided. As reference model we use the Worst Conditional Expectation model with transaction costs. Extensive computational results based on different historical data sets from London Stock Exchange Market (FTSE) are presented and some interesting financial conclusions are drawn.},
  comment   = {spinning reserves},
  file      = {Guastaroba09scenarioEffect.pdf:Guastaroba09scenarioEffect.pdf:PDF},
  keywords  = {Risk management, Conditional value at risk, Portfolio optimization, Scenario generation, Mixed integer linear programming},
  owner     = {scot},
  timestamp = {2010.11.24},
  url       = {http://www.sciencedirect.com/science/article/B6VCT-4PWKSMF-2/2/66c6508a10bdc31551f5e0473c225f5c},
}

@Article{Trentin01mixRecurrNeuralSpkrNorm,
  author    = {Edmondo Trentin and Diego Giuliani},
  title     = {A Mixture of Recurrent Neural Networks for Speaker Normalisation},
  journal   = {Neural Computing \& Applications},
  year      = {2001},
  volume    = {10},
  number    = {2},
  pages     = {120--135},
  abstract  = {In spite of recent advances in automatic speech
recognition, the performance of state-of-the-art
speech recognisers fluctuates depending on the
speaker. Speaker normalisation aims at the
reduction of differences between the acoustic space
of a new speaker and the training acoustic space of
a given speech recogniser, improving performance.
Normalisation is based on an acoustic feature transformation,
to be estimated from a small amount of
speech signal. This paper introduces a mixture of
recurrent neural networks as an effective regression
technique to approach the problem. A suitable Viterbi-
based time alignment procedure is proposed for
generating the adaptation set. The mixture is compared
with linear regression and single-model connectionist
approaches. Speaker-dependent and
speaker-independent continuous speech recognition
experiments with a large vocabulary, using Hidden
Markov Models, are presented. Results show that
the mixture improves recognition performance,
yielding a 21\% relative reduction of the word error
rate, i.e. comparable with that obtained with modeladaptation
approaches.
Keywords: Mixture of neural networks; Multivariate
regression; Recurrent neural network; Speaker adaptation;
Speaker normalisation; Speech recognition},
  comment   = {Could use this to model temporal correlations (scenarios) in probabilistic forecast (errors). Works on short speech segs so could be a fast adaptation approach.

For BMA ensemble forecasting (this NN approach is a mixture), could also be a way of flexibly modeling the power curve in each ensemble member (avoiding censored distribution problems) and including temporal dependence},
  file      = {Trentin01mixRecurrNeuralSpkrNorm.pdf:Trentin01mixRecurrNeuralSpkrNorm.pdf:PDF},
  groups    = {ErrDistProps, doReadWPV_2, doReadNonWPV_2},
  owner     = {scotto},
  timestamp = {2008.07.07},
  url       = {http://www.springerlink.com.offcampus.lib.washington.edu/content/h1b5hfl7el0wvp9q/?p=a43e9bde57ba4c278edd5f442e492ad8&pi=3},
}

@Article{Wikipedia17matrixNormDist,
  author    = {Wikipedia},
  title     = {Matrix normal distribution},
  journal   = {Wikipedia},
  year      = {2017},
  abstract  = {In statistics, the matrix normal distribution is a
probability distribution that is a generalization of
the multivariate normal distribution to matrix-
valued random variables.},
  comment   = {This is a multivariate normal distribution parameterization in which the random variable is treated as a matrix and the covariance matrix is restricted to be an element-wise product of matrices separately modelling row-wise (U) and  column-wise (V)covariance.

 It's useful for parsimoniously  modeling temporal covariance with mixtures (Anderlucci15covPatMix).

Factoids in this article
* expected values
* covariances
* linear transforms of the RV and the effect on the means and covariances
* MLE estimatoin
  - mean is just the average
  - covariance can be solved iteratively but is not unique
    (Anderlucci15covPatMix and Owen13monteCarloBook) force it somehow
* Sampling: pretty simple.  Fanciest thing is a Cholesky (or other) decomposition.},
  file      = {:papers\\Wikipedia17matrixNormDist.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.07.08},
  url       = {https://en.wikipedia.org/wiki/Matrix_normal_distribution},
}

@Misc{Wikpedia16degOfFrdm,
  author       = {Wikipedia},
  title        = {Degrees of Freedom},
  year         = {2016},
  howpublished = {Web page},
  month        = jan,
  url          = {https://en.wikipedia.org/wiki/Degrees_of_freedom_%28statistics%29},
  abstract     = {In statistics, the number of degrees of freedom is the number of values in the final calculation of a statistic that are free to vary.[1]

The number of independent ways by which a dynamic system can move, without violating any constraint imposed on it, is called number of degrees of freedom. In other words, the number of degrees of freedom can be defined as the minimum number of independent coordinates that can specify the position of the system completely.

Estimates of statistical parameters can be based upon different amounts of information or data. The number of independent pieces of information that go into the estimate of a parameter are called the degrees of freedom. In general, the degrees of freedom of an estimate of a parameter are equal to the number of independent scores that go into the estimate minus the number of parameters used as intermediate steps in the estimation of the parameter itself (i.e. the sample variance has N-1 degrees of freedom, since it is computed from N random scores minus the only 1 parameter estimated as intermediate step, which is the sample mean).[2]

Mathematically, degrees of freedom is the number of dimensions of the domain of a random vector, or essentially the number of "free" components (how many components need to be known before the vector is fully determined).

The term is most often used in the context of linear models (linear regression, analysis of variance), where certain random vectors are constrained to lie in linear subspaces, and the number of degrees of freedom is the dimension of the subspace. The degrees of freedom are also commonly associated with the squared lengths (or "sum of squares" of the coordinates) of such vectors, and the parameters of chi-squared and other distributions that arise in associated statistical testing problems.

While introductory textbooks may introduce degrees of freedom as distribution parameters or through hypothesis testing, it is the underlying geometry that defines degrees of freedom, and is critical to a proper understanding of the concept. Walker (1940)[3] has stated this succinctly as "the number of observations minus the number of necessary relations among these observations."},
  file         = {Wikpedia16degOfFrdm.pdf:Wikpedia16degOfFrdm.pdf:PDF},
}

@Article{Wikipedia17WishartDist,
  author    = {Wikipedia},
  title     = {Wishart distribution},
  journal   = {Wikipedia},
  year      = {2017},
  abstract  = {In statistics, the Wishart distribution is a generalization to multiple
dimensions of the chi-squared distribution, or, in the case of non-integer
degrees of freedom, of the gamma distribution. It is named in honor of
John Wishart, who first formulated the distribution in 1928.[1]
It is a family of probability distributions defined over symmetric,
nonnegative-definite matrix-valued random variables (?random
matrices?). These distributions are of great importance in the estimation
of covariance matrices in multivariate statistics. In Bayesian statistics, the
Wishart distribution is the conjugate prior of the inverse covariance-
matrix of a multivariate-normal random-vector.},
  comment   = {The distribution of the MLE est of the multivariate normal dist is Wishart.  It's good for other stuff too, and is related to the matrix normal distribution.},
  file      = {:papers\\Wikipedia17WishartDist.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.07.08},
  url       = {https://en.wikipedia.org/wiki/Wishart_distribution},
}

@Article{Zhou06streamFeatSel,
  author    = {Zhou,, Jing and Foster,, Dean P. and Stine,, Robert A. and Ungar,, Lyle H.},
  title     = {Streamwise Feature Selection},
  journal   = {Journal of Machine Learning Research},
  year      = {2006},
  volume    = {7},
  number    = {12},
  pages     = {1861--1885},
  issn      = {1533-7928},
  abstract  = {In streamwise feature selection, new features are sequentially considered for addition to a predictive model. When the space of potential features is large, streamwise feature selection offers many advantages over traditional feature selection methods, which assume that all features are known in advance. Features can be generated dynamically, focusing the search for new features on promising subspaces, and overfitting can be controlled by dynamically adjusting the threshold for adding features to the model. In contrast to traditional forward feature selection algorithms such as stepwise regression in which at each step all possible features are evaluated and the best one is selected, streamwise feature selection only evaluates each feature once when it is generated. We describe information-investing and ?-investing, two adaptive complexity penalty methods for streamwise feature selection which dynamically adjust the threshold on the error reduction required for adding a new feature. These two methods give false discovery rate style guarantees against overfitting. They differ from standard penalty methods such as AIC, BIC and RIC, which always drastically over- or under-fit in the limit of infinite numbers of non-predictive features. Empirical results show that streamwise regression is competitive with (on small data sets) and superior to (on large data sets) much more compute-intensive feature selection methods such as stepwise regression, and allows feature selection on problems with millions of potential features.},
  comment   = {See also: Keller15EstMutualInfStrm},
  file      = {Zhou06streamFeatSel.pdf:Zhou06streamFeatSel.pdf:PDF;Zhou06streamFeatSel.pdf:Zhou06streamFeatSel.pdf:PDF},
  location  = {Cambridge, MA, USA},
  owner     = {sotterson},
  publisher = {MIT Press},
  timestamp = {2009.03.06},
}

@Article{Andreas95gateExpTSregime,
  author    = {Andreas, S Weigend and Mangeas, Morgan and Ashok, N Srivastava},
  title     = {Nonlinear gated experts for time series: Discovering regimes and avoiding overfitting},
  journal   = {International Journal of Neural Systems},
  year      = {1995},
  volume    = {6},
  number    = {04},
  pages     = {373--399},
  abstract  = {In the analysis and predictionof real-worldsystems, two of the key problems are nonstationarity(often in the form of switching between regimes), and overfitting (particularly serious for noisy processes). This article addresses these problems using gated experts, consisting of a (nonlinear) gating network, and several (also nonlinear) competing experts. Each expert learns to predict the conditional mean, and each expert adapts its width to match the noise level in its regime. The gating network learns to predict the probability of each expert, given the input. This article focuses on the case where the gating network bases its decision on information from the inputs. This can be contrasted to hidden Markov models where the decision is based on the previous state(s) (i.e., on the output of the gating network at the previous time step), as well as to averaging over several predictors. In contrast, gated experts soft-partition the input space. This article discusses the underlying statistical assumptions, derives the weight update rules, and compares the performance of gated experts to standard methods on three time series: (1) a computergenerated series, obtained by randomly switching between two nonlinear processes, (2) a time series from the Santa Fe Time Series Competition (the light intensity of a laser in chaotic state), and (3) the daily electricity demand of France, a real-world multivariate problem with structure on several time scales. The main results are (1) the gating network correctly discovers the different regimes of the process, (2) the widths associated with each expert are important for the segmentation task (and they can be used to characterize the sub-processes), and (3) there is less overfitting compared to single networks (homogeneous multi-layer perceptrons), since the experts learn to match their variances to the (local) noise levels. This can be viewed as matching the local complexity of the model to the local complexity of the data.},
  comment   = {Maybe a simple way to learn regimes?},
  doi       = {10.1142/S0129065795000251},
  file      = {Andreas95gateExpTSregime.pdf:Andreas95gateExpTSregime.pdf:PDF},
  owner     = {sotterson},
  publisher = {World Scientific},
  timestamp = {2013.03.13},
}

@Article{Cole18impactSunShot2030,
  author   = {Wesley Cole and Bethany Frew and Pieter Gagnon and Andrew Reimers and Jarett Zuboy and Robert Margolis},
  title    = {Envisioning a low-cost solar future: Exploring the potential impact of Achieving the SunShot 2030 targets for photovoltaics},
  journal  = {Energy},
  year     = {2018},
  volume   = {155},
  pages    = {690 - 704},
  issn     = {0360-5442},
  abstract = {In the context of recent dramatic solar energy cost reductions, the U.S. Department of Energy set new levelized cost of energy goals for photovoltaics (PV) to achieve by 2030 to enable significantly greater PV adoption: \$0.03/kWh for utility-scale, \$0.04/kWh for commercial, and \$0.05/kWh for residential PV systems. We analyze the potential impacts of achieving these “SunShot 2030” cost targets for the contiguous United States using the Regional Energy Deployment System (ReEDS) and Distributed Generation (dGen) capacity expansion models. We consider the impacts under a wide range of future conditions. We find that PV could provide 13\%–18\% of U.S. electricity demand in 2030 and 28\%–64\% of demand if the SunShot 2030 goals are achieved, with PV deployment increasing in every state. The availability of low-cost storage has the largest impact on projected deployment, followed by natural gas prices and electricity demand. For comparison, PV deployed under a business-as-usual scenario could provide only 5\% of generation in 2030 and 17\% in 2050. We find that the high levels of PV deployment explored here lead to lower electricity prices and system costs, lower carbon dioxide emissions, lower water consumption, increased renewable energy curtailment, and increased storage deployment compared with the business-as-usual scenario.},
  comment  = {A user of NREL's generic adoption forecast, maybe the thing I'll want to do for MN SP, or the WattPlan Grid generic adopttion forecast.

NREL generic adoption forecast: Sigrin17mktDmdDgenDoc},
  doi      = {https://doi.org/10.1016/j.energy.2018.04.166},
  file     = {:Cole18impactSunShot2030.pdf:PDF},
  keywords = {SunShot, PV, ReEDS, Capacity expansion, Renewable energy, Energy storage},
  url      = {http://www.sciencedirect.com/science/article/pii/S0360544218307898},
}

@Article{Hayn14loadPrfofilesHshldSeg,
  author   = {Marian Hayn and Valentin Bertsch and Wolf Fichtner},
  title    = {Electricity load profiles in Europe: The importance of household segmentation},
  journal  = {Energy Research \& Social Science},
  year     = {2014},
  volume   = {3},
  pages    = {30 - 45},
  issn     = {2214-6296},
  abstract = {In the current market design, the increasing use of renewable energy sources for electricity generation leads to new challenges in balancing supply and demand. While households are responsible for 29% of total electricity demand in Europe, a good understanding of their consumption and load profiles is missing. Similar to existing clustering methodologies from marketing science, this paper proposes an approach for the segmentation of households. The approach particularly focusses on the impact of socio-demographic factors and the equipment with electric appliances as well as new technologies for electricity and heat supply on residential load profiles. In addition to these three factors themselves, the dependencies between them are identified as crucial. Therefore, in order to adequately assess the future development of residential load profiles, on the one hand, a qualitative analysis of socio-demographic factors is carried out and, on the other hand, the influence of selected technologies is quantitatively modeled. Beyond the mere impact on households’ annual energy demand, in focus of most existing research in the field, particular emphasis will be given to the peak load development, which is considered increasingly relevant for balancing supply and demand and maintaining security of supply.},
  comment  = {Factors found in several studies to affect total energy consumption of the shape of load profiles.

Main factors are 
1. household size
2. net income
3. age of "reference" person
4. employment status
5 other stuff},
  doi      = {https://doi.org/10.1016/j.erss.2014.07.002},
  file     = {:Hayn14loadPrfofilesHshldSeg.pdf:PDF},
  keywords = {Household segmentation, Residential electricity demand and load profiles, Lifestyles and socio-demographic factors, Electric appliances and technologies for electricity and heat supply},
  url      = {http://www.sciencedirect.com/science/article/pii/S2214629614000802},
}

@Article{Zheng12qboost,
  author    = {Songfeng Zheng},
  title     = {QBoost: Predicting quantiles with boosting for regression and binary classification},
  journal   = {Expert Systems with Applications},
  year      = {2012},
  volume    = {39},
  number    = {2},
  pages     = {1687--1697},
  issn      = {0957-4174},
  abstract  = {In the framework of functional gradient descent/ascent, this paper proposes Quantile Boost (QBoost) algorithms which predict quantiles of the interested response for regression and binary classification. Quantile Boost Regression performs gradient descent in functional space to minimize the objective function used by quantile regression (QReg). In the classification scenario, the class label is defined via a hidden variable, and the quantiles of the class label are estimated by fitting the corresponding quantiles of the hidden variable. An equivalent form of the definition of quantile is introduced, whose smoothed version is employed as the objective function, and then maximized by functional gradient ascent to obtain the Quantile Boost Classification algorithm. Extensive experimentation and detailed analysis show that QBoost performs better than the original QReg and other alternatives for regression and binary classification. Furthermore, QBoost is capable of solving problems in high dimensional space and is more robust to noisy predictors.},
  comment   = {A quantile classifier but can do regression good for high dimensions. Maybe good for extreme events or regime detection too.

Maybe related to the adaboost ideas too.

Maybe dynamic line rating too, since it would be high dim, and are estimating the exceedance of a high quantile.},
  doi       = {10.1016/j.eswa.2011.06.060},
  groups    = {PointDerived, doReadNonWPV_1},
  keywords  = {Quantile regression},
  owner     = {sotterson},
  timestamp = {2014.03.30},
  url       = {http://www.sciencedirect.com/science/article/pii/S0957417411009602},
}

@Article{Witten13ClusterElasticNet,
  author    = {Witten, Daniela M and Shojaie, Ali and Zhang, Fan},
  title     = {The Cluster Elastic Net for High-Dimensional Regression With Unknown Variable Grouping},
  journal   = {Technometrics},
  year      = {2013},
  number    = {just-accepted},
  abstract  = {In the high-dimensional regression setting, the elastic net produces a parsimonious model by shrinking all coefficients toward the origin. However, in certain settings, this behavior might not be desirable: if some features are highly correlated with each other and associated with the response, then we might wish to perform less shrinkage on the coefficients corresponding to that subset of features. We propose the cluster elastic net, which selectively shrinks the coefficients for such variables toward each other, rather than toward the origin. Instead of assuming that the clusters are known a priori, the cluster elastic net infers clusters of features from the data, on the basis of correlation among the variables as well as association with the response. These clusters are then used to more accurately perform regression. We demonstrate the theoretical advantages of our proposed approach, and explore its performance in a simulation study, and in an application to HIV drug resistance data. Supplementary materials are available online.

Keywords: correlated variables, feature selection, feature clustering, structured sparsity,
lasso, ridge, p >> n},
  comment   = {Super-high dimensional regression w/ something like group lasso clustering (but the correlated variable groups are found automatically). I guess this is elastic net clustering.

Note that this wouldn't work for cos/sin wind direction input (they are uncorrelated), but would it work for circular spline basis of wind direction?

Anyway could use for local linear quantile regression.},
  doi       = {10.1080/00401706.2013.810174},
  file      = {Witten13ClusterElasticNet.pdf:Witten13ClusterElasticNet.pdf:PDF},
  owner     = {sotterson},
  publisher = {Taylor \& Francis Group},
  timestamp = {2014.04.04},
}

@Article{Ehm16quantileExpectileScrFrcst,
  author   = {Ehm, Werner and Gneiting, Tilmann and Jordan, Alexander and Krüger, Fabian},
  title    = {Of quantiles and expectiles: consistent scoring functions, Choquet representations and forecast rankings},
  journal  = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year     = {2016},
  volume   = {78},
  number   = {3},
  pages    = {505--562},
  issn     = {1467-9868},
  abstract = {In the practice of point prediction, it is desirable that forecasters receive a directive in the form of a statistical functional. For example, forecasters might be asked to report the mean or a quantile of their predictive distributions. When evaluating and comparing competing forecasts, it is then critical that the scoring function used for these purposes be consistent for the functional at hand, in the sense that the expected score is minimized when following the directive. We show that any scoring function that is consistent for a quantile or an expectile functional can be represented as a mixture of elementary or extremal scoring functions that form a linearly parameterized family. Scoring functions for the mean value and probability forecasts of binary events constitute important examples. The extremal scoring functions admit appealing economic interpretations of quantiles and expectiles in the context of betting and investment problems. The Choquet-type mixture representations give rise to simple checks of whether a forecast dominates another in the sense that it is preferable under any consistent scoring function. In empirical settings it suffices to compare the average scores for only a finite number of extremal elements. Plots of the average scores with respect to the extremal scoring functions, which we call Murphy diagrams, permit detailed comparisons of the relative merits of competing forecasts.},
  comment  = {Compares quantile score loss function with expectile loss function},
  doi      = {10.1111/rssb.12154},
  file     = {:Ehm16quantileExpectileScrFrcst.pdf:PDF},
  keywords = {Choquet representation, Consistent scoring function, Decision theory, Economic utility, Elicitable, Expectile, Forecast ranking, Order sensitivity, Point forecast, Probability forecast, Quantile},
  url      = {http://dx.doi.org/10.1111/rssb.12154},
}

@Article{Gupta14OutlierDetTempSurv,
  author   = {M. Gupta and J. Gao and C. C. Aggarwal and J. Han},
  title    = {Outlier Detection for Temporal Data: A Survey},
  journal  = IEEE_J_KDE,
  year     = {2014},
  volume   = {26},
  number   = {9},
  pages    = {2250--2267},
  month    = sep,
  issn     = {1041-4347},
  abstract = {In the statistics community, outlier detection for time series data has been studied for decades. Recently, with advances in hardware and software technology, there has been a large body of work on temporal outlier detection from a computational perspective within the computer science community. In particular, advances in hardware technology have enabled the availability of various forms of temporal data collection mechanisms, and advances in software technology have enabled a variety of data management mechanisms. This has fueled the growth of different kinds of data sets such as data streams, spatio-temporal data, distributed streams, temporal networks, and time series data, generated by a multitude of applications. There arises a need for an organized and detailed study of the work done in the area of outlier detection with respect to such temporal datasets. In this survey, we provide a comprehensive and structured overview of a large set of interesting outlier definitions for various forms of temporal data, novel techniques, and application scenarios in which specific definitions and techniques have been widely used.},
  comment  = {Also has a book:
https://www.amazon.com/Detection-Temporal-Synthesis-Knowledge-Discovery/dp/1627053751},
  doi      = {10.1109/TKDE.2013.184},
  file     = {Gupta14OutlierDetTempSurv.pdf:Gupta14OutlierDetTempSurv.pdf:PDF},
  keywords = {data handling, time series, computer science community, data management mechanisms, data streams, distributed streams, hardware technology, software technology, spatio-temporal data, statistics community, temporal data collection mechanisms, temporal datasets, temporal networks, temporal outlier detection, time series data, Computational modeling, Distributed databases, Hidden Markov models, Pattern matching, Predictive models, Time series analysis, Data mining, Mining methods and algorithms, Temporal outlier detection, applications of temporal outlier detection, data streams, distributed data streams, network outliers, spatio-temporal outliers, temporal networks, time series data},
}

@Article{Lee14PowerCurveEstimation,
  author    = {Lee, Giwhyun and Ding, Yu and Genton, Marc G and Xie, Le},
  title     = {Power Curve Estimation with Multivariate Environmental Factors for Inland and Offshore Wind Farms},
  journal   = {Journal of the American Statistical Association},
  year      = {2014},
  number    = {just-accepted},
  pages     = {00--00},
  abstract  = {In the wind industry, a power curve refers to the functional relationship between the power
output generated by a wind turbine and the wind speed at the time of power generation.
Power curves are used in practice for a number of important tasks including predicting wind
power production and assessing a turbine's energy production efficiency. Nevertheless, actual
wind power data indicate that the power output is affected by more than just wind speed.
Several other environmental factors, such as wind direction, air density, humidity, turbulence
intensity, and wind shears, have potential impact. Yet, in industry practice, as well as in
the literature, current power curve models primarily consider wind speed and, sometimes,
wind speed and direction. We propose an additive multivariate kernel method that can
include the aforementioned environmental factors as a new power curve model. Our model
provides, conditional on a given environmental condition, both the point estimation and
density estimation of power output. It is able to capture the nonlinear relationships between
environmental factors and the wind power output, as well as the high-order interaction
effects among some of the environmental factors. Using operational data associated with four
turbines in an inland wind farm and two turbines in an offshore wind farm, we demonstrate
the improvement achieved by our kernel method.
Keywords: Additive multivariate kernel regression, Nonparametric estimation, Turbine
performance assessment, Wind power forecast},
  comment   = {Physical power curve equation used to justify a multiplicative power curve Kernel function. But only generally: the things that are most important, the general nonlinear, multiplicative relationship.... ==> interaction terms are important.


* says ordinary GAM's are unlikely to work well b/c true physical law as multiplicative relationship
 - so needs interaction terms
* so, possibilities are
 - Bayesian additive regression trees
 - smoothing spline anova
 - kernel based methods
* They like conditional kernel density (CKD) estimators
 - this leads to a product kernel, which I guess is the multiplicative relationship they're after
* 3 most important terms are
 - wind speed
 - direction
 - air density (explanation for conversion to it from pressure and temp is on p. 11)
* 3 possible inputs listed on p. 16 (other than wind speed, direction). Also in Fig 3.
 - air density
 - turbulence intensity
 - wind shear},
  doi       = {10.1080/01621459.2014.977385},
  file      = {Lee14PowerCurveEstimation.pdf:Lee14PowerCurveEstimation.pdf:PDF},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2014.11.14},
}

@Article{Crane08timeSpaceCopula,
  author    = {Crane, Glenis},
  title     = {Time and Space Varying Copulas},
  journal   = {arXiv preprint arXiv:0812.3208},
  year      = {2008},
  abstract  = {In this article we review existing literature on dynamic copulas and then propose an n-copula which varies in time and space. Our approach makes use of stochastic differential equations, and gives rise to a dynamic copula which is able to capture the dependence between multiple Markov diffusion processes. This model is suitable for pricing basket derivatives in finance and may also be applicable to other areas such as bioinformatics and environmental science.},
  comment   = {spatio-temporal varying copula w/ stochastic differential equations

Such copulas are also implemented with vine copulas (time-varying? w/ SDE or not?) here: Graler12timeSpaceVnCopula},
  file      = {Crane08timeSpaceCopula.pdf:Crane08timeSpaceCopula.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_2},
  owner     = {sotterson},
  timestamp = {2014.02.07},
  url       = {https://www.researchgate.net/profile/Glenis_Crane/publications/},
}

@InProceedings{Perez17grndCalDetSatIrrad,
  author    = {R. {Perez} and J. {Schlemmer} and A. {Kankiewicz} and J. {Dise} and A. {Tadese} and T. {Hoff}},
  title     = {Detecting Calibration Drift at Ground Truth Stations A Demonstration of Satellite Irradiance Models' Accuracy},
  booktitle = {Proc. IEEE 44th Photovoltaic Specialist Conf. (PVSC)},
  year      = {2017},
  pages     = {1104--1109},
  month     = jun,
  abstract  = {In this article we show how a state-of-the-art satellite 
irradiance model – SolarAnywhere – is capable of undetected 
calibration issues at a trusted reference ground truth irradiance 
measurement station. This evidence suggests that the best satellite 
models have now achieved a degree of accuracy and versatility 
that makes them an acceptable, if not a preferred choice, for solar 
energy engineering applications ranging from long-term site 
characterization and system monitoring.  
Index Terms —solar resource, satellite, modeling, irradiance, 
benchmarking. },
  comment   = {CPR SolarAnywhere Satellite irradiance meas catches ground station instrument calibration drift.

How ground station calibration drift was detected
* year 2015 solaranywhere GHI was higher than ground station; OK fo 2013, though
* apparently, GHI, DNI and DIS can be separate instruments but calculated GHI is often used instead (corroborated by Jaus19navSolarIradMeasIss).
* the bias detected in thispaper must have been in the direct GHI sensor, since they later use DNI and DIF to compute GHI and check against it.

Two lines of evidence pointed to a ground sensor error
1.  If SA was underestimating turbidity (so too-high GHI), then DNI s/b even higher relative to the ground instrument, since DNI is direct radiation, which would have been scattered less.  But DNI matched ground station.
2. Indepdendent DIF data and DNI ground meas were used to calculate GHI; this GHI matched SA.

Conclusions
1. SA is accurate enough to ID calibration errors in one of US's most trusted ground instruments
2. It's accepted that indirect GHI meas (from DNI and DIF) is better than pyranometric GHI for ground truthing and climate.  Yet pyranometric might be more accurate b/c less calibrational uncertainty.  YET WASN'T and error in a direct measuring GHI pyroanometer what was detected here?  This statement seems illogical (See Jaus19navSolarIradMeasIss for a drawing of a pyrometer).
3. SA can be used to sort out which ground instrument is bad -- it's a trusty common denominator
4. satellite-tuning is only as good as the base station instruments used to do the tuning.  Need to cal. and maintain the ground stuff.

},
  doi       = {10.1109/PVSC.2017.8366469},
  file      = {:Perez17grndCalDetSatIrrad.pdf:PDF},
  keywords  = {atmospheric radiation, atmospheric techniques, calibration, solar power, satellite irradiance models, state-of-the-art satellite irradiance model, undetected calibration issues, trusted reference ground truth, measurement station, satellite models, calibration drift, ground truth stations, SolarAnywhere, Indexes, solar resource, satellite, modeling, irradiance, benchmarking},
  url       = {https://www.cleanpower.com/wp-content/uploads/manuscript-85-perez-format-ok.pdf},
}

@Article{Killick12OptDetChngPtsLinCompMatlab,
  author    = {R. Killick and P. Fearnhead and I. A. Eckley},
  title     = {Optimal Detection of Changepoints With a Linear Computational Cost},
  journal   = {Journal of the American Statistical Association},
  year      = {2012},
  volume    = {107},
  number    = {500},
  pages     = {1590-1598},
  abstract  = {In this article, we consider the problem of detecting multiple changepoints in large datasets. Our focus is on applications where the number of changepoints will increase as we collect more data: for example, in genetics as we analyze larger regions of the genome, or in finance as we observe time series over longer periods. We consider the common approach of detecting changepoints through minimizing a cost function over possible numbers and locations of changepoints. This includes several established procedures for detecting changing points, such as penalized likelihood and minimum description length. We introduce a new method for finding the minimum of such cost functions and hence the optimal number and location of changepoints that has a computational cost, which, under mild conditions, is linear in the number of observations. This compares favorably with existing methods for the same problem whose computational cost can be quadratic or even cubic. In simulation studies, we show that our new method can be orders of magnitude faster than these alternative exact methods. We also compare with the binary segmentation algorithm for identifying changepoints, showing that the exactness of our approach can lead to substantial improvements in the accuracy of the inferred segmentation of the data. This article has supplementary materials available online.

Keywords: Dynamic programming, PELT, Segmentation, Structural change},
  comment   = {The algorithms used in Matlab's ischange function.

Supplemental materials pasted on to the end of the main article pdf.},
  doi       = {10.1080/01621459.2012.737745},
  eprint    = {http://dx.doi.org/10.1080/01621459.2012.737745},
  file      = {:Killick12OptDetChngPtsLinCompMatlab.pdf:PDF},
  publisher = {Taylor \& Francis},
  url       = { 
        http://dx.doi.org/10.1080/01621459.2012.737745
    
},
}

@Article{Peng09parcorJointSprs,
  author    = {Peng, Jie and Wang, Pei and Zhou, Nengfeng and Zhu, Ji},
  title     = {Partial Correlation Estimation by Joint Sparse Regression Models},
  journal   = {Journal of the American Statistical Association},
  year      = {2009},
  volume    = {104},
  number    = {486},
  pages     = {735--746},
  abstract  = {In this article, we propose a computationally efficient approach?space (Sparse PArtial Correlation Estimation)?for selecting nonzero partial correlations under the high-dimension-low-sample-size setting. This method assumes the overall sparsity of the partial correlation matrix and employs sparse regression techniques for model fitting. We illustrate the performance of space by extensive simulation studies. It is shown that space performs well in both nonzero partial correlation selection and the identification of hub variables, and also outperforms two existing methods. We then apply space to a microarray breast cancer dataset and identify a set of hub genes that may provide important insights on genetic regulatory networks. Finally, we prove that, under a set of suitable assumptions, the proposed procedure is asymptotically consistent in terms of model selection and parameter estimation.},
  comment   = {High dim sparse correlation estimation; is R "space" package, I think There's an older, 2007 paper on the same topic, and referenced in the R package I couldn't get a pdf of this paper but was able to find a tech report posted in 2008 on arXiv, which refers to the 2009 paper in the Acknowledgements (and is attached)},
  file      = {Technical report (2008):Peng09parcorJointSprs_TechRep.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.09.02},
  url       = {http://econpapers.repec.org/RePEc:bes:jnlasa:v:104:i:486:y:2009:p:735-746},
}

@Article{Jach11subSampInfTS,
  author    = {Jach, Agnieszka and McElroy, Tucker and Politis, Dimitris N.},
  title     = {Subsampling inference for the mean of heavy-tailed long-memory time series},
  journal   = {Journal of Time Series Analysis},
  year      = {2011},
  issn      = {1467-9892},
  abstract  = {In this article, we revisit a time series model introduced by MCElroy and Politis (2007a) and generalize it in several ways to encompass a wider class of stationary, nonlinear, heavy-tailed time series with long memory. The joint asymptotic distribution for the sample mean and sample variance under the extended model is derived; the associated convergence rates are found to depend crucially on the tail thickness and long memory parameter. A self-normalized sample mean that concurrently captures the tail and memory behaviour, is defined. Its asymptotic distribution is approximated by subsampling without the knowledge of tail or/and memory parameters; a result of independent interest regarding subsampling consistency for certain long-range dependent processes is provided. The subsampling-based confidence intervals for the process mean are shown to have good empirical coverage rates in a simulation study. The influence of block size on the coverage and the performance of a data-driven rule for block size selection are assessed. The methodology is further applied to the series of packet-counts from ethernet traffic traces.},
  comment   = {among other things, tests subsampling bootstrap size picker in Bickel08choiceOfMsubsampBtstrp


Subsample size choice (p. 7)
* uses KS distance instead of whatever Bickel08choiceOfMsubsampBtstrp used
* says automatic choice of b was adequate, although somewhat undercovered at 0.95 conf ints.
* since didn't use method in Politis01asympSubsamp, then maybe this method (and Bickel's) is better?

How test b's were chosen
* about 10 were tested, covering between about 3 and 42\% of full sample.

* they're logarithmically spaced, as in Geyer
* q=0.75 and jmin=3 and jmax = (log func of n) but I don't fully understand it.},
  doi       = {10.1111/j.1467-9892.2011.00742.x},
  file      = {Jach11subSampInfTS.pdf:Jach11subSampInfTS.pdf:PDF},
  groups    = {Read},
  keywords  = {Infinite variance, self-normalization, subsampling, weak dependence, adaptive block size},
  owner     = {sotterson},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2011.11.24},
}

@Article{Pena07dimMultiVarDep,
  author    = {Pena, D. and van der Linde, A.},
  title     = {Dimensionless measures of variability and dependence for multivariate continuous distributions},
  journal   = {Communications in Statistics-Theory and Methods},
  year      = {2007},
  volume    = {36},
  number    = {10},
  pages     = {1845--1854},
  abstract  = {In this article, we suggest dimensionless descriptive measures of multivariate variability and dependence which can be used in comparisons of random vectors of different dimensions. Our work generalizes the measures of scatter and linear dependence proposed by Pe?a and Rodr?guez (2003). The measure of variability we introduce is the rth root of the (transformed) entropy and the measure of dependence is based on the mutual information between the components of an r-dimensional random vector, capturing general stochastic dependence instead of merely linear dependence. We further investigate decompositions of the measure of variability into a measure of scale and a measure of stochastic dependence. The decomposition resulting from independent components provides a representation of variability by the scale of independent components and thus generalizes the explanation of covariance by principal components in classical multivariate analysis. We illustrate our ideas for examples of non Gaussian random vectors. Keywords Entropy; Independent components; Mutual information},
  comment   = {Dependency measure that works for vectors of different dimensions but it's total dependence (like interactive info), not pairwise, like Shannon. Still, see p. 10 and 17},
  file      = {Pena07dimMultiVarDep.pdf:Pena07dimMultiVarDep.pdf:PDF},
  owner     = {scot},
  publisher = {Taylor \& Francis},
  timestamp = {2011.05.19},
  url       = {http://www.math.uni-bremen.de/~avdl/download/papers/avdlpenacorr.pdf},
}

@InCollection{Abdollahi16grphMdlsFaultDiag,
  author    = {Abdollahi, Ali and Pattipati, Krishna R and Kodali, Anuradha and Singh, Satnam and Zhang, Shigang and Luh, Peter B},
  title     = {Probabilistic Graphical Models for Fault Diagnosis in Complex Systems},
  booktitle = {Principles of Performance and Reliability Modeling and Evaluation},
  publisher = {Springer},
  year      = {2016},
  pages     = {109--139},
  abstract  = {In this chapter, we discuss the problem of fault diagnosis for complex systems in two different contexts: static and dynamic probabilistic graphical models of systems. The fault diagnosis problem is represented using a tripartite probabilistic graphical model. The first layer of this tripartite graph is composed of components of the system, which are the potential sources of failures. The condition of each component is represented by a binary state variable which is zero if the component is healthy and one otherwise. The second layer is composed of tests with binary outcomes (pass or fail) and the third layer is the noisy observations associated with the test outcomes. The cause?effect relations between the states of components and the observed test outcomes can be compactly modeled in terms of detection and false alarm probabilities. For a failure source and an observed test outcome, the probability of fault detection is defined as the probability that the observed test outcome is a fail given that the component is faulty, and the probability of false alarm is defined as the probability that the observed test outcome is a fail given that the component is healthy. When the probability of fault detection is one and the probability of false alarm is zero, the test is termed perfect; otherwise, it is deemed imperfect. In static models, the diagnosis problem is formulated as one of maximizing the posterior probability of component states given the observed fail or pass outcomes of tests. Since the solution to this problem is known to be NP-hard, to find near-optimal diagnostic solutions, we use a Lagrangian (dual) relaxation technique, which has the desirable property of providing a measure of suboptimality in terms of the approximate duality gap. Indeed, the solution would be optimal if the approximate duality gap is zero. The static problem is discussed in detail and some interesting properties, such as the reduction of the problem to a set covering problem in the case of perfect tests, are discussed. We also visualize the dual function graphically and introduce some insights into the static fault diagnosis problem. In the context of dynamic probabilistic graphical models, it is assumed that the states of components evolve as independent Markov chains and that, at each time epoch, we have access to some of the observed test outcomes. Given the observed test outcomes at different time epochs, the goal is to determine the most likely evolution of the states of components over time. The application of dual relaxation techniques results in significant reduction in the computational burden as it transforms the original coupled problem into separable subproblems, one for each component, which are solved using a Viterbi decoding algorithm. The problems, as stated above, can be regarded as passive monitoring, which relies on synchronous or asynchronous availability of sensor results to infer the most likely state evolution of component states. When information is sequentially acquired to isolate the faults in minimum time, cost, or other economic factors, the problem of fault diagnosis can be viewed as active probing (also termed sequential testing or troubleshooting). We discuss the solution of active probing problems using the information heuristic and rollout strategies of dynamic programming. The practical applications of passive monitoring and active probing to fault diagnosis problems in automotive, aerospace, power, and medical systems are briefly mentioned.},
  comment   = {How to track down the most probable cause of a detected fault using graphical models.  Looks hard.  I think it's a book chapter.  Couldn't find a pdf.},
  owner     = {sotterson},
  timestamp = {2016.12.19},
  url       = {http://link.springer.com/chapter/10.1007%2F978-3-319-30599-8_5},
}

@Misc{Lyche05SplineMethodsLecNts,
  author       = {Tom Lyche and Knut M{\o}rken},
  title        = {Spline Methods -- INF-MAT5340 - Spring 2005},
  howpublished = {Lecture Notes},
  year         = {2005},
  note         = {University of Oslo},
  abstract     = {In this first chapter, we consider the following fundamental problem: Given a set of points
in the plane, determine a smooth curve that approximates the points. The algorithm
for determining the curve from the points should be well suited for implementation on a
computer. That is, it should be efficient and it should not be overly sensitive to round-off
errors in the computations. We only consider methods that
involve a relatively small number of elementary arithmetic operations; this ensures that
the methods are efficient. The sensitivity of the methods to round-off errors is controlled
by insisting that all
the operations involved should amount to forming weighted averages of the given
points. This has the added advantage that the constructions are geometrical in nature
and easy to visualise.
In Section 1.1, we discuss affine and convex combinations and the convex hull of a set
of points, and relate these concepts to numerical stability (sensitivity to rounding errors),
while in
Section 1.2 we give a brief and very informal introduction to parametric curves. The
first method for curve construction, namely polynomial interpolation, is introduced in
Section 1.3. In Section 1.4 we show how to construct Bezier curves, and in Section 1.5
we generalise this construction to spline curves. At the outset, our construction of spline
curves is geometrical in nature, but in Section 1.6 we show that spline curves can be
written conveniently in terms of certain basis functions, namely B-splines. In the final
section, we relate the material in this chapter to the rest of the book.},
  comment      = {Spline methods lecture notes: b-splines, tensor splines, smoothing splines, shape conserving (monotonic, boundedness,...) splines},
  file         = {Lyche05SplineMethodsLecNts.pdf:Lyche05SplineMethodsLecNts.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2014.10.30},
  url          = {http://www.uio.no/studier/emner/matnat/ifi/INF-MAT5340/v05/undervisningsmateriale/komp.html},
}

@Article{Deng12deepLnr3arch,
  author   = {Deng, Li},
  title    = {Three classes of deep learning architectures and their applications: a tutorial survey},
  journal  = {APSIPA transactions on signal and information processing},
  year     = {2012},
  abstract = {In this invited paper, my overview material on the
same topic as presented in the plenary overview session of
APSIPA-2011 and the tutorial material presented in the same
conference (Deng, 2011) are expanded and updated to include
more recent developments in deep learning. The previous and
the updated materials cover both theory and applications, and
analyze its future directions. The goal of this tutorial survey is to
introduce the emerging area of deep learning or hierarchical
learning to the APSIPA community. Deep learning refers to a
class of machine learning techniques, developed largely since
2006, where many stages of nonlinear information processing in
hierarchical architectures are exploited for pattern classification
and for feature learning.  In the more recent literature, it is also
connected to representation learning, which involves a hierarchy
of features or concepts where higher-level concepts are defined
from lower-level ones and where the same lower-level concepts
help to define higher-level ones. In this tutorial, a brief history of
deep learning research is discussed first. Then, a classificatory
scheme is developed to analyze and summarize major work
reported in the deep learning literature. Using this scheme, I
provide a taxonomy-oriented survey on the existing deep
architectures and algorithms in the literature, and categorize
them into three classes: generative, discriminative, and hybrid.
Three representative deep architectures --- deep auto-encoder,
deep stacking network, and deep neural network (pre-trained
with deep belief network) --- one in each of the three classes, are
presented in more detail. Next, selected applications of deep
learning are reviewed in broad areas of signal and information
processing including audio/speech, image/vision, multimodality,
language modeling, natural language processing, and
information retrieval. Finally, future directions of deep learning
are discussed and analyzed.},
  comment  = {A recommended good review of autoencoders, Boltzmann nets, and CNN's.},
  file     = {:Deng12deepLnr3arch.pdf:PDF},
  url      = {http://ai2-s2-pdfs.s3.amazonaws.com/5bd4/177440c17dad736f1e0d2227694d612f5a59.pdf},
}

@InProceedings{Sedighizadeh04pidWaveletWindCtl,
  author    = {Sedighizadeh, M. and Arzaghi-Harris, D. and Kalantar, M.},
  title     = {Adaptive {PID} control of wind energy conversion systems using RASP1~{m}other wavelet basis function networks},
  booktitle = {TENCON},
  year      = {2004},
  volume    = {3},
  pages     = {524--527},
  month     = nov,
  abstract  = {In this paper a PID control strategy using neural network adaptive RASP1 wavelet for WECS's control is proposed. It is based on single layer feedforward neural networks with hidden nodes of adaptive RASP1 wavelet functions controller and an infinite impulse response (IIR) recurrent structure. The IIR is combined by cascading to the network to provide double local structure resulting in improving speed of learning. This particular neuro PID controller assumes a certain model structure to approximately identify the system dynamics of the unknown plant (WECS's) and generate the control signal. The results are applied to a typical turbine/generator pair, showing the feasibility of the proposed solutions.},
  comment   = {Adaptive wind generator plant estimator. Neural nets w/ wavelet nodes? * explanation of some of the dynamics * use for wind-to-power estimation? * This version from 2008 has a few more pictures: http://www.waset.org/pwaset/v27/v27-47.pdf},
  doi       = {10.1109/TENCON.2004.1414823},
  file      = {Sedighizadeh04pidWaveletWindCtl.pdf:Sedighizadeh04pidWaveletWindCtl.pdf:PDF;Sedighizadeh04pidWaveletWindCtl.pdf:Sedighizadeh04pidWaveletWindCtl.pdf:PDF},
  journal   = {TENCON 2004. 2004 IEEE Region 10 Conference},
  keywords  = {adaptive control, neurocontrollers, power generation control, radial basis function networks, three-term control, wavelet transforms, wind power RASP1, adaptive PID control, infinite impulse response recurrent structure, mother wavelet basis function networks, neural network adaptive wavelet, neuro PID controller, single layer feedforward neural networks, wind energy conversion systems},
  owner     = {sotterson},
  timestamp = {2008.08.21},
  url       = {http://ieeexplore.ieee.org.offcampus.lib.washington.edu/search/srchabstract.jsp?arnumber=1414823&isnumber=30648&punumber=9709&k2dockey=1414823@ieeecnfs&query=((adaptive+pid+control+of+wind+energy+conversion+systems+using+rasp1+mother+wavelet+basis+function+networks)%3Cin%3Emetadata)&pos=0&access=no},
}

@Article{Stanisic15gitOrgWorkflow,
  author    = {Stanisic, Luka and Legrand, Arnaud and Danjean, Vincent},
  title     = {An effective git and org-mode based workflow for reproducible research},
  journal   = {ACM SIGOPS Operating Systems Review},
  year      = {2015},
  volume    = {49},
  number    = {1},
  pages     = {61--70},
  abstract  = {In this paper we address the question of developing a
lightweight and effective workflow for conducting experimen-
tal research on modern parallel computer systems in a repro-
ducible way. Our approach builds on two well-known tools
(Git and Org-mode) and enables to address, at least par-
tially, issues such as running experiments, provenance track-
ing, experimental setup reconstruction or replicable analysis.
We have been using such a methodology for two years now
and it enabled us to recently publish a fully reproducible
article [12]. To fully demonstrate the effectiveness of our
proposal, we have opened our two year laboratory notebook
with all the attached experimental data. This notebook and
the underlying Git revision control system enable to illus-
trate and to better understand the workflow we used.},
  comment   = {Maybe I should start using this to version my code and data.  Uses git, emacs and org-mode.  Not sure I want the "literate programming" stuff, but I should read the paper and consider it.},
  file      = {:Stanisic15gitOrgWorkflow.pdf:PDF},
  groups    = {sotterson:6},
  publisher = {ACM},
  url       = {https://hal.inria.fr/hal-01112795},
}

@InProceedings{Jonk95axiomLineClust,
  author    = {Jonk, A. and Smeulders, A.W.M.},
  title     = {An axiomatic approach to clustering line-segments},
  booktitle = {Document Analysis and Recognition, 1995., Proceedings of the Third International Conference on},
  year      = {1995},
  volume    = {1},
  pages     = {386--389},
  abstract  = {In this paper we consider the problem of clustering line-segments into new ones. The clustering-hierarchy gives an answer to the question what original line segments are combined into larger ones. Such a clustering is defined as a hierarchical ordering of a set of line-segments. Criteria on a clustering-method are presented. The difference between edges and lines in relation to scale-invariant clustering is demonstrated. Existing approaches are evaluated using the presented criteria. It is shown that these approaches do not meet desirable criteria such as scale-invariance. A new method is described that adheres the formulated criteria. Finally an experiment is presented that illustrates the usefulness of the new method},
  comment   = {A rotation and offset approach to line segment clustering. Is agglomerative. Could use for local inear regression neighbhorhood stuff.},
  doi       = {10.1109/ICDAR.1995.599019},
  file      = {Jonk95axiomLineClust.pdf:Jonk95axiomLineClust.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.03.31},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599019},
}

@Article{Foresi95condDistRetDR,
  author    = {Foresi, Silverio and Peracchi, Franco},
  title     = {The conditional distribution of excess returns: An empirical analysis},
  journal   = {Journal of the American Statistical Association},
  year      = {1995},
  volume    = {90},
  number    = {430},
  pages     = {451--466},
  abstract  = {In this paper we describe the cumulative distribution function of excess returns conditional on a broad set of predictors that summarize the state of the economy. We do so by estimating a sequence of conditional logit models over a grid of values of the response variable. Our method uncovers higher-order multidimensional structure that cannot be found by modeling only the first two moments of the distribution. We compare two approaches to modeling: one based on a conventional linear logit model, the other an additive logit. The second approach avoids the ??Scurse of dimensionality??? problem of fully nonparametric methods while retaining both interpretability and the ability to let the data determine the shape of the relationship between the response variable and the predictors. We find that additive logit fits better and reveals aspects of the data that remain undetected by the linear logit. The additive model retains its superiority even in out-of-sample prediction and portfolio selection performance, suggesting that this model captures genuine features of the data which seem to be important to guide investors" optimal portfolio choices.

Keywords: Asset pricing, generalized additive models, nonparametric methods},
  comment   = {Distribution regression predicts cdf probabilities instead of quantile dependent variable levels. Has same crossing problems as quantile regression, but may be better for extreme quantiles (if somehow coverted to them). This is the original distribution regression paper. As far as I can tell, it's clearer than the later papers.

For ReWP, and other post-bidding operations like redispatch, where a fixed level of power has already been promised, Distributional regression could be more appropriate than quantile regression: you pick a level, and DR tells you the probability that it will be exceeded. This could be used for intraday correction of DA forecasts, or reserve activation.

Prediction is done best w/ an additive logistic regression model (on trading test), but it's not clear that logistic regression is necessarily the best method. Anyway crossover is an issue w/ this method, just as w/ quantile regression.

DR vs. QR
* quantile regression: given a c.d.f. probability, predict the value of the dependent variable
* distribution regression: given a value of a dependent variable, predict the c.d.f. probability
* Performance (from Koenker13DistributionalvsQuantile)
 - DR perf. ~ QR perf, but DR might be better in extremes:
 - both QR and DR have crossover problems (they give up on it in this paper)

Predicting the c.d.f. probabilities
* given a level threshold, create a binary indicator vector w/ a 1 where the dep. var. is <= the threshold
* try to predict the indicator using the independent variables, x
* prediction done w/ variants of logistic regression
 - predicts the mean value of indicator
 - the mean value prediction, not the distribution assumed by logistic regression, is the goal
* use many thresholds
* Result: for every test, x, you get a set of predicted cdf probs at a constant set of levels
* somehow build an ordered c.d.f. out of this (can have crossover)
 - think you could turn probs into quantiles byr training a model that uses an (x,prob) input to predict the levels
* Simple logistic regression details can be found in, for example: Shalizi15datAnalElemViewBk
* but I don't think that logistic regression matters: could be an NN if it's using RMSE training
* it is not clear to me how the cdf probs are used in their studies, however!!

Logistic regression models: 3 types tested
1. linear logistic regression
 - best on squared prediction error metrics
 - not best on trading simulation b/c of transaction costs from frequent trading
2. additive logistic ref. model, which can accommodate splines
 - 2\textsuperscript{nd} best on pred. error
 - clearly the best in trading experiment
3. semi-additive L.R.
 - generally doesn't work

Crossover avoidance approaches
 (in cdf invversion)

1. Do nothing
 - montonicity not guaranteed
 - max quantile prob <= 1
2. Ordered logit
 - monotonicity guaranteed
 - 0 <= quantile prob <=1 guaranteed
 - too restrictive, only models location shift (but see location+shift model in Koenker13DistributionalvsQuantile ?)
3. Probability spacing
 - monotonicity guaranteed
 - max prob <=1 not guaranteed
 -- Schmidt13noCrossQRspacing does spacing but this works b/c they're spacing quantiles, which aren't bounded
4. Survivor recursion
 - montonicity guaranteed
 - 0 <= p <=1 guaranteed
 - not interpretable (and hard to estimate?).
 - but maybe this is no less interpretable than a NN?

Their log-odds ratio model
* could use projection-persuit logistic regression
* but they use a semi-additive model (could be spline smoothing based)
* estimated w/ some kind of local scoring

Feature selection
* heuristic
* have 7 of them

MY NOTES:

Regression function
* xx says the threshold error is gaussian?
* an NN with an LSQ cost function predicts the conditional mean e.g.
https://books.google.de/books?id=U_b2BwAAQBAJ&pg=PA5&lpg=PA5&dq=neural+network+for+estimating+conditional+mean&source=bl&ots=hLZiI0M6Tp&sig=hljm-efcyj4eiRoXF9KVHzsgZP0&hl=en&sa=X&redir_esc=y\#v=onepage&q=neural%20network%20for%20estimating%20conditional%20mean&f=false},
  file      = {Foresi95condDistRetDR.pdf:Foresi95condDistRetDR.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2015.07.11},
  url       = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1299462},
}

@Article{Athanasopoulos09hierFrcst,
  author    = {George Athanasopoulos and Roman A. Ahmed and Rob J. Hyndman},
  title     = {Hierarchical forecasts for {Australia}n domestic tourism},
  journal   = {International Journal of Forecasting},
  year      = {2009},
  volume    = {25},
  number    = {1},
  pages     = {146--166},
  issn      = {0169-2070},
  abstract  = {In this paper we explore the hierarchical nature of tourism demand time series and produce short-term forecasts for Australian domestic tourism. The data and forecasts are organized in a hierarchy based on disaggregating the data according to geographical regions and purposes of travel. We consider five approaches to hierarchical forecasting: two variations of the top-down approach, the bottom-up method, a newly proposed top-down approach where top-level forecasts are disaggregated according to the forecasted proportions of lower level series, and a recently proposed optimal combination approach. Our forecast performance evaluation shows that the top-down approach based on forecast proportions and the optimal combination method perform best for the tourism hierarchies we consider. By applying these methods, we produce detailed forecasts of the Australian domestic tourism market.},
  comment   = {Forecast a sum of time series from individual time series forecasts. Useful for upscaling.
* use this for regional power forecast using individual plant forecasts?

* modify to account for spatio-temporal lags?
* R package, hts, here: http://cran.r-project.org/web/packages/hts/index.html (which also does a multi-line plot of time series, I think)
* tech note: Hyndman07optCombHierFrcst},
  doi       = {DOI: 10.1016/j.ijforecast.2008.07.004},
  file      = {Athanasopoulos09hierFrcst.pdf:Athanasopoulos09hierFrcst.pdf:PDF},
  keywords  = {Australia},
  owner     = {scot},
  timestamp = {2010.07.01},
  url       = {http://www.sciencedirect.com/science/article/B6V92-4TPF8WD-1/2/cd935d9529968e15142a3ab5cce8ddb6},
}

@Conference{Botterud09windFrcstElecMktOp,
  author    = {Botterud, A. and Wang, J. and Monteiro, C. and Miranda, V.},
  title     = {Wind power forecasting and electricity market operations},
  booktitle = {International Association of Energy Economics (IAEE)},
  year      = {2009},
  abstract  = {In this paper we give a brief overview of wind power forecasting models and how they are used in power system and electricity market operations. We focus on the organized electricity markets in the United States, where several independent system operators (ISOs/RTOs) have recently introduced wind power forecasting systems as part of their operations. We find that wind power forecasting is already used for a number of important applications. However, as the amount of wind power capacity is rapidly increasing, there is a need to better integrate wind power forecasting into different parts of power system operations, from determination of operating reserve requirements to unit commitment and dispatch decisions. It is also important that wind power forecast providers tailor their products to meet the specific needs of the system operators.},
  comment   = {Overview of forecasting in N. American wind markets and operations. Has comprehensive table of them
* operating reserves regulated by NERC, limits out-of-balancing scheduling errors

UNIT COMMITMENT
2 main markets, apparently
* Day ahead market is a mixed integer programming problem
-- intermittent resources currently don't bid on DA!
-- they just take what the price is (so no motive for better DA forecasting)

* SCED follows, allocates transmission and is linear (not clear how two are related)
* wind also exempt from realtime deviation penalties (again, no motive for better forecasts)
-- but this is being introduced
-- NERC 2009 report said to emphasize need for revision of mkts. But I looked at it and didn't find much

Uncertainty
* ERCOT alreadying considering forecasting uncertainty
-- somehow using 80 pct exceedance in DA mkts (thought this wasn't done?)
-- but must look at refs to find out how
* dont' seem to trust stochastic Unit Committment yet

DISPATCH
* CAISO has wind power bids in hour ahead market, but deviation charges are somehow limited

Nice table of market operation and wind power forecasting in the Appendix},
  file      = {Botterud09windFrcstElecMktOp.pdf:Botterud09windFrcstElecMktOp.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  timestamp = {2011.04.18},
}

@Article{Jones04newToolsNLpred,
  author    = {Antonia Jones},
  title     = {New tools in non-linear modelling and prediction},
  journal   = {Computational Management Science},
  year      = {2004},
  volume    = {1},
  number    = {2},
  pages     = {109--149},
  month     = jul,
  abstract  = {In this paper we give an account of a new change of perspective in nonlinear modelling and prediction as applied to smooth systems. The core element of these developments is the Gamma test a non-linear modelling and analysis tool which allows us to examine the nature of a hypothetical input/output relationship in a numerical data-set. In essence, the Gamma test allows us to efficiently calculate that part of the variance of the output which cannot be accounted for by the existence of any smooth model based on the inputs, even though this model be unknown. A key aspect of this tool is its speed: theGammatest has time complexity O(M logM), where M is the number of data-points. For data-sets consisting of a few thousand points and a reasonable number of attributes, a single run of theGammatest typically takes a fewseconds.Around this essentially simple procedure a newset of analytical tools has evolved which allow us to model smooth non-linear systems directly from the data with a precision and confidence that hitherto was inaccessible. In this paper we briefly describe the Gamma test, its benefits in model identification and model building, and then in more detail explain and motivate the procedures which facilitate a Gamma analysis.We briefly report on a case study applying these ideas to the practical problem of predicting level and flow rates in the Thames valley river basin. Finally we speculate on the future development and enhancement of these techniques into areas such as datamining and the production of complex nonlinear models directly from data via graphical representations of process charts and automated Gamma analysis of each input-output node.},
  comment   = {An alternative to mutual information for relevance determination, feature selection, model tuning/regression and selection Quicker than MI, I think; insert into: Francois07resampParamFreeFeatSel},
  file      = {Jones04newToolsNLpred.pdf:Jones04newToolsNLpred.pdf:PDF;Jones04newToolsNLpred.pdf:Jones04newToolsNLpred.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.02.11},
  url       = {http://ideas.repec.org/a/spr/comgts/v1y2004i2p109-149.html},
}

@Article{Bonilla08MultiTaskGaussProc,
  author      = {Bonilla, Edwin V. and Chai, Kian Ming and Williams, Christopher},
  title       = {Multi-task {Gauss}ian Process Prediction},
  journal     = {Advances in Neural Information Processing Systems (NIPS)},
  year        = {2008},
  abstract    = {In this paper we investigate multi-task learning in the context of Gaussian Processes (GP). We propose a model that learns a shared covariance function on input-dependent features and a ``free-form'' covariance matrix over tasks. This allows for good flexibility when modelling inter-task dependencies while avoiding the need for large amounts of data for training. We show that under the assumption of noise-free observations and a block design, predictions for a given task only depend on its target values and therefore a cancellation of inter-task transfer occurs. We evaluate the benefits of our model on two practical applications: a compiler performance prediction problem and an exam score prediction task. Additionally, we make use of GP approximations and properties of our model in order to provide scalability to large data sets.},
  comment     = {Use to model a large scale "wind" and separately model turbine outputs},
  file        = {Bonilla08MultiTaskGaussProc.pdf:Bonilla08MultiTaskGaussProc.pdf:PDF;Bonilla08MultiTaskGaussProc.pdf:Bonilla08MultiTaskGaussProc.pdf:PDF},
  institution = {PASCAL EPrints [http://eprints.pascal-network.org/perl/oai2] (United Kingdom)},
  keywords    = {Learning/Statistics \& Optimisation},
  location    = {http://www.scientificcommons.org/26947628},
  owner       = {scotto},
  timestamp   = {2008.10.04},
  url         = {http://eprints.pascal-network.org/archive/00003442/},
}

@Article{Gorriz05preProcForecastRN,
  author    = {Juan Manuel G{\'o}rriz and J. C. Segura-Luna and Carlos Garc\'{\i}a Puntonet and Mois{\'e}s Salmer{\'o}n},
  title     = {A Survey of Forecasting Preprocessing Techniques using RNs},
  journal   = {Informatica (Slovenia)},
  year      = {2005},
  volume    = {29},
  number    = {1},
  pages     = {13--32},
  abstract  = {In this paper we make a survey of various preprocessing techniques including the statistical method for volatile time series forecasting using Regularization Networks (RNs). These methods improve the performance of Regularization Networks i.e. using Independent Component Analysis (ICA) algorithms and filtering as preprocessing tools. The preprocessed data is introduced into a Regularized Artificial Neural Network (ANN) based on radial basis functions (RBFs) and the prediction results are compared with the ones we get without these preprocessing tools, with the high computational effort method based on multidimensional regularization networks (MRN) and with the Principal Component Analysis (PCA) technique},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comment   = {preprocessing w/ pca, ica and regularization networks. Whitening Lin07frcstLimICA may have improvements to ICA},
  ee        = {http://www.informatica.si/PDF/29-1/02_Gorriz-A\%20Survey\%20of\%20Forecasting...pdf},
  file      = {Gorriz05preProcForecastRN.pdf:Gorriz05preProcForecastRN.pdf:PDF;Gorriz05preProcForecastRN.pdf:Gorriz05preProcForecastRN.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.02.10},
  url       = {http://www.informatik.uni-trier.de/~ley/db/journals/informaticaSI/informaticaSI29.html},
}

@InProceedings{Bernard08forestRKmtry,
  author    = {Simon {Bernard} and Laurent {Heutte} and Sébastien {Adam}},
  title     = {Forest-RK: A New Random Forest Induction Method},
  booktitle = {ICIC '08 Proceedings of the 4th international conference on Intelligent Computing: Advanced Intelligent Computing Theories and Applications - with Aspects of Artificial Intelligence},
  year      = {2008},
  pages     = {430--437},
  abstract  = {In this paper we present our work on the parametrization of Random Forests (RF), and more particularly on the number K of features randomly selected at each node during the tree induction process. It has been shown that this hyperparameter can play a significant role on performance. However, the choice of the value of K is usually made either by a greedy search that tests every possible value to choose the optimal one, either by choosing a priori one of the three arbitrary values commonly used in the literature. With this work we show that none of those three values is always better than the others. We thus propose an alternative to those arbitrary choices of K with a new “push-button” RF induction method, called Forest-RK, for which K is not an hyperparameter anymore. Our experimentations show that this new method is at least as statistically accurate as the original RF method with a default K setting.
Keywords
Classification Classifier Ensemble Classifier Combination Random Forests Decision Trees Bagging },
  comment   = {Biau16randFrstGuideTour says this can pick mtry, but what it actually does is randomizes it, thus avoiding the choice.

This has been improved in: Bernard12dynRandForest

Slides for this algorithm and the improvent are also attached.  From the slides it seems like there's a proof that this method works, and that the improvement, Dynamic Random Forests, is not (yet?) proven, but works better empirically.},
  file      = {:Bernard08forestRKmtry.pdf:PDF;:Bernard12dynRandForest_slides.pdf:PDF},
  url       = {https://link.springer.com/chapter/10.1007/978-3-540-85984-0_52},
}

@InProceedings{Mcwilliams09predOnlineVarSel,
  author       = {{M}cwilliams, {B}rian and {M}ontana, {G}iovanni},
  title        = {{P}redictive modeling with high-dimensional data streams: an on-line variable selection approach},
  booktitle    = {Signal Processing with Adaptive Sparse Structured Representations (SPARS)},
  year         = {2009},
  editor       = {{R}{\'e}mi {G}ribonval},
  organization = {{I}nria {R}ennes - {B}retagne {A}tlantique},
  abstract     = {{I}n this paper we propose a computationally efficient algorithm for on-line variable selection in multivariate regression problems involving high dimensional data streams. {T}he algorithm recursively extracts all the latent factors of a partial least squares solution and selects the most important variables for each factor. {T}his is achieved by means of only one sparse singular value decomposition which can be efficiently updated on-line and in an adaptive fashion. {S}imulation results based on artificial data streams demonstrate that the algorithm is able to select important variables in dynamic settings where the correlation structure among the observed streams is governed by a few hidden components and the importance of each variable changes over time. {W}e also report on an application of our algorithm to a multivariate version of the ?enhanced index tracking? problem using financial data streams. {T}he application consists of performing on-line asset allocation with the objective of overperforming two benchmark indices simultaneously},
  affiliation  = {{D}epartment of {M}athematics - {I}mperial {C}ollege {L}ondon - {I}mperial {C}ollege {L}ondon },
  audience     = {internationale },
  comment      = {Partial least squares (PLS), svd and time varying cov. struct

Could be good for 1K ensembles},
  file         = {Mcwilliams09predOnlineVarSel.pdf:Mcwilliams09predOnlineVarSel.pdf:PDF},
  groups       = {Ensemble, doReadNonWPV_1},
  hal_id       = {inria-00369564},
  language     = {{A}nglais},
  location     = {{S}aint {M}alo {F}rance},
  owner        = {scot},
  timestamp    = {2010.09.27},
  url          = {http://hal.inria.fr/inria-00369564/en/},
}

@InProceedings{Goldberger04nbrhdCompAnal,
  author    = {Goldberger, Jacob and Hinton, Geoffrey E and Roweis, Sam T and Salakhutdinov, Ruslan},
  title     = {Neighbourhood components analysis},
  booktitle = {Advances in neural information processing systems},
  year      = {2004},
  pages     = {513--520},
  abstract  = {In this paper we propose a novel method for learning a Mahalanobis
distance measure to be used in the KNN classification algorithm. The
algorithm directly maximizes a stochastic variant of the leave-one-out
KNN score on the training set. It can also learn a low-dimensional lin-
ear embedding of labeled data that can be used for data visualization
and fast classification. Unlike other methods, our classification model
is non-parametric, making no assumptions about the shape of the class
distributions or the boundaries between them. The performance of the
method is demonstrated on several data sets, both for metric learning and
linear dimensionality reduction.},
  comment   = {It's in Matlab R2016b stats toolbox

Ref. in Matlab help is: Yang12nbrCompFeatSel},
  file      = {Goldberger04nbrhdCompAnal.pdf:Goldberger04nbrhdCompAnal.pdf:PDF},
  url       = {http://www.cs.nyu.edu/~roweis/publications.html},
}

@Article{Pena03descSctrLinDep,
  author    = {Pe{\~n}a, D. and Rodr{\'\i}guez, J.},
  title     = {Descriptive measures of multivariate scatter and linear dependence},
  journal   = {Journal of Multivariate Analysis},
  year      = {2003},
  volume    = {85},
  number    = {2},
  pages     = {361--374},
  abstract  = {In this paper we propose two new descriptive measures for multivariate data: the effective variance and the effective dependence. These measures have a direct geometric and statistical interpretation and can be used to compare groups with different number of variables. The contribution of these measures to understanding multivariate data is illustrated by several examples},
  comment   = {Describes a scalar measure of multivariate linear dependence. Description of generalized variance. Also a way to compare clusters of different dimensions.

Scalar measures of coherence
* total variance: sum of eigenvectors
* generalied variance
-- determinant of correlation matrix
-- product of eigenvectors, only for same-dim comparisons
-- need to standardize first, or is that fixed by eigenvecs?
* effective variance: genVar^(1/p) (p=dimension)
-- can compare across different dims
* effective standard deviation: genVar^(1/(2p))
* effective dependence: function of the determinant of the CORR. matrix
-- strongly related to num. of PCA comps needed to explain something
-- can be used to est. num. PCA comps needed to explain 90\% of var.
-- see also: Minka00AutoPCAdim
-- why not just use num. of PCA comps instead?
* Is this related to Henriks generalized correlation idea in Nielsen01genClassicTimeSer ?},
  doi       = {10.1016/S0047-259X(02)00061-1},
  file      = {Pena03descSctrLinDep.pdf:Pena03descSctrLinDep.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2012.03.09},
  url       = {http://www.sciencedirect.com/science/article/pii/S0047259X02000611},
}

@InProceedings{Balkanski18adaptMaxSubMod,
  author       = {Balkanski, Eric and Singer, Yaron},
  title        = {The adaptive complexity of maximizing a submodular function},
  booktitle    = {Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing},
  year         = {2018},
  pages        = {1138--1151},
  organization = {ACM},
  abstract     = {In this paper we study the adaptive complexity of submodular optimization. Informally, the
adaptive complexity of a problem is the minimal number of sequential rounds required to achieve
a constant factor approximation when polynomially-many queries can be executed in parallel at
each round. Adaptivity is a fundamental concept that is heavily studied in computer science,
largely due to the need for parallelizing computation. Somewhat surprisingly, very little is
known about adaptivity in submodular optimization. For the canonical problem of maximizing
a monotone submodular function under a cardinality constraint, to the best of our knowledge,
all that is known to date is that the adaptive complexity is between 1 and Ω(n).
Our main result in this paper is a tight characterization showing that the adaptive complexity
of maximizing a monotone submodular function under a cardinality constraint is

˜

Θ(log n):
• We describe an algorithm which requires O(log n) sequential rounds and achieves an ap-
proximation that is arbitrarily close to 1/3;
• We show that no algorithm can achieve an approximation better than O(log1 n) with fewer
than O( log n

log log n) rounds.
Thus, when allowing for parallelization, our algorithm achieves a constant factor approximation
exponentially faster than any known existing algorithm for submodular maximization.
Importantly, the approximation algorithm is achieved via adaptive sampling and comple-
ments a recent line of work on optimization of functions learned from data. In many cases
we do not know the functions we optimize and learn them from labeled samples. Recent
results show that no algorithm can obtain a constant factor approximation guarantee using
polynomially-many labeled samples as in the PAC and PMAC models, drawn from any distribu-
tion [BRS17, BS17a]. Since learning with non-adaptive samples over any distribution results
in a sharp impossibility, we consider learning with adaptive samples where the learner obtains
poly(n) samples drawn from a distribution of her choice in every round. Our result implies that
in the realizable case, where there is a true underlying function generating the data,

˜

Θ(log n)
batches of adaptive samples are necessary and sufficient to approximately “learn to optimize”
a monotone submodular function under a cardinality constraint.},
  comment      = {A new method of submodular optimization.  Good for e.g. taxi routing and recommendation engines.  Is more than an order of mag. faster than existing methods (July 2018), "exponentially" according to quotes in this IEEE Spectrum article:

https://spectrum.ieee.org/tech-talk/computing/software/new-optimization-algorithm-exponentially-speeds-computation?utm_source=techalert&utm_campaign=techalert-07-12-18&utm_medium=email},
  file         = {:Balkanski18adaptMaxSubMod.pdf:PDF},
  groups       = {Scott Otterson:6},
  url          = {https://scholar.harvard.edu/files/ericbalkanski/files/the-adaptive-complexity-of-maximizing-a-submodular-function.pdf},
}

@Article{Chen84cmplxHarmSline,
  author              = {Chen, Han-Lin and Tron Hvaring},
  title               = {Approximation of Complex Harmonic Functions by Complex Harmonic Splines},
  journal             = {Mathematics of Computation},
  year                = {1984},
  volume              = {42},
  number              = {165},
  pages               = {151--164},
  issn                = {0025-5718},
  abstract            = {In this paper, a class of complex harmonic spline functions (C.H.S.) are defined on the unit disc $U$. We use the C.H.S. to approximate the complex harmonic function on $U$, showing that C.H.S. may be represented by elementary functions. If the maximum step tends to zero and the mesh ratio is bounded, then C.H.S. converge uniformly to the interpolated function $F$ on the closed disc $\bar U$. If the interpolated function $F$ is a conformal mapping, then the C.H.S. is a quasi-conformal mapping.},
  comment             = {Complex-valued splines interpolate harmonics; for lagged wind vel. basis?

Quick read: I'm not sure if this is relevant, but maybe could use something about complex valued splines if I read it more carefully. I thought this might provide a hint towards deriving a 3D spline basis that has wind velocity as a periodic function. The other 2 dims aren't periodic, which I believe means that the solution can be a linear sum of cylindrical harmonics -- useful for joint regression and approximation like you can do with a 1D distributed lag basis. There is a solution of a linear set of equations in eq. (61) requiring equality of the function to be approximated, F(Z), with the approximation at N points. Equality probably OK b/c it's approximating known harmonic functions.},
  copyright           = {Copyright ?? 1984 American Mathematical Society},
  file                = {Chen84cmplxHarmSline.pdf:Chen84cmplxHarmSline.pdf:PDF},
  jstor_articletype   = {primary_article},
  jstor_formatteddate = {Jan., 1984},
  owner               = {scotto},
  publisher           = {American Mathematical Society},
  timestamp           = {2010.08.20},
  url                 = {http://www.jstor.org/stable/2007565},
}

@InProceedings{Jian08volCylBspln,
  author    = {{Ting-ting} Jiang and Chen, S.Y. and Yiqiang Xu},
  title     = {{3-D} Representation and Volumetric Measurement of Human Heart from a Cylindrical B-Spline Surface Model},
  booktitle = {BioMedical Engineering and Informatics (BEMI)},
  year      = {2008},
  volume    = {1},
  pages     = {765--769},
  month     = may,
  abstract  = {In this paper, a cylindrical 3-D B-Spline model of heart's surfaces is proposed. Since volume is a basic parameter in cardiac image analysis, an effectively integral algorithm for volumetric measurement of the B-Spline surface model represented by matrix forms is presented in this article as well. This method makes the volumetric measurement convenient and its result is more accurate than the results counted by the traditional methods. In order to validate B-Spine surface fitting and advantage of our method for volumetric measurement, several experiments were done. Finally, the results show that the cylindrical B- Spline with a given number of control points can well fit the surfaces of the heart and left ventricle. Experiments also show the algorithm can get satisfactory measurement accuracy and efficiency.},
  comment   = {matrix rep of 3D cylindrical spline. Looks all linear.
* could almost use for lagged wind velocity basis
-- estimates f(t,w)
-- could be regressionInput(spd,dir)
-- lags still not in there
* nice that it goes into a matrix (looks all linear)
* is poorly written: hard to understand equation notation
-- are refs 10, 11 or Deng04cylindSpln3D more readable?
* adopt eigen structure used in Tewfik87eigCylindHarm for matrix in here?
* add lag w/ 4D technique?
-- use this matrix technique for 4D?
-- refs 10 or 11?
-- 4D splines in Ozturk00spline4D},
  doi       = {10.1109/BMEI.2008.182},
  file      = {Jian08volCylBspln.pdf:Jian08volCylBspln.pdf:PDF},
  keywords  = {3D heart representation;cardiac image analysis;cylindrical B-spline surface model;human heart;volumetric measurement;cardiology;splines (mathematics);volume measurement;},
  owner     = {scot},
  timestamp = {2010.08.30},
}

@Article{Damousis04fuzzWindSpatCorr,
  author    = {Damousis, I.G. and Alexiadis, M.C. and Theocharis, J.B. and Dokopoulos, P.S.},
  title     = {A fuzzy model for wind speed prediction and power generation in wind parks using spatial correlation},
  journal   = {Energy conversion, IEEE Transactions on},
  year      = {2004},
  volume    = {19},
  number    = {2},
  pages     = {352--361},
  month     = jun,
  issn      = {0885-8969},
  abstract  = {In this paper, a fuzzy model is suggested for the prediction of wind speed and the produced electrical power at a wind park. The model is trained using a genetic algorithm-based learning scheme. The training set includes wind speed and direction data, measured at neighboring sites up to 30 km away from the wind turbine clusters. Extensive simulation results are shown for two application cases, providing wind speed forecasts from 30 min to 2 h ahead. It is demonstrated that the suggested model achieves an adequate understanding of the problem while it exhibits significant improvement compared to the persistent method.},
  comment   = {cites Schueter},
  doi       = {10.1109/TEC.2003.821865},
  file      = {Damousis04fuzzWindSpatCorr.pdf:Damousis04fuzzWindSpatCorr.pdf:PDF;Damousis04fuzzWindSpatCorr.pdf:Damousis04fuzzWindSpatCorr.pdf:PDF},
  keywords  = {fuzzy logic, genetic algorithms, load forecasting, power engineering computing, wind power 30 km, 30 min to 2 hour, fuzzy logic, genetic algorithm, power generation, spatial correlation, wind forecasting, wind speed prediction, wind turbine clusters},
  owner     = {sotterson},
  timestamp = {2009.02.20},
}

@Conference{Xiang02shortTgaussianization,
  author       = {Xiang, B. and Chaudhari, U.V. and Navratil, J. and Ramaswamy, G.N. and Gopinath, R.A.},
  title        = {Short-time {Gauss}ianization for robust speaker verification},
  booktitle    = {Acoustics, Speech and Signal Processing (ICASSP)},
  year         = {2002},
  volume       = {1},
  organization = {Citeseer},
  abstract     = {In this paper, a novel approach for robust speaker verification, namely short-time Gaussianization, is proposed. Shorttime Gaussianization is initiated by a global linear transformation of the features, followed by a short-time windowed cumulative distribution function(CDF) matching. First, the linear transformation in the feature space leads to local independence or decorrelation. Then the CDF matching is applied to segments of speech localized in time and tries to warp a given feature so that its CDF matches normal distribution. It is shown that one of the recent techniques used for speaker recognition, feature warping[1] can be formulated within the framework of Gaussianization. Compared to the baseline system with cepstral mean subtraction( CMS), around  relative improvement in both equal error rate(EER) and minimum detection cost function(DCF) is obtained on NIST 2001 cellular phone data evaluation.},
  comment      = {spinning reserves, an alternative to copulae?},
  file         = {Xiang02shortTgaussianization.pdf:Xiang02shortTgaussianization.pdf:PDF},
  issn         = {0749-8411},
  owner        = {scot},
  timestamp    = {2010.12.07},
}

@InProceedings{Luna06partMutInfoNNfeatsel,
  author    = {Luna, I and Soares, S and Ballini, R},
  title     = {Partial mutual information criterion for modelling time series via neural networks},
  booktitle = {Proceedings of the 11th information processing and management of uncertainty international conference},
  year      = {2006},
  volume    = {1},
  pages     = {2012--2019},
  abstract  = {In this paper, a strategy for mod-
elling temporal time series is pro-
posed. This approach is based on
the Partial Mutual Information Cri-
terion, which is evaluated for select-
ing relevant inputs for a time se-
ries model. This criterion does not
only consider input-output relations
but also stored information each in-
put provides. The methodology is
applied to identify a linear time se-
ries model and a non-linear model
based on artificial neural networks,
which is used for modelling Brazil-
ian monthly streamflow series. Sim-
ulation results show the usefulness of
the presented method.
Keywords: Partial mutual infor-
mation, input selection, time series,
streamflow forecasting, FIR neural
network.},
  comment   = {Partial mutual information used to incrementally select neural net forecasting features (lags).  The PMI calc requires forwards and backwards regression but they don't use a NN to do that.  Is it better to just work on NN residuals?},
  file      = {Luna06partMutInfoNNfeatsel.pdf:Luna06partMutInfoNNfeatsel.pdf:PDF},
  url       = {https://www.researchgate.net/profile/Ivette_Luna2/publication/228408686_Partial_Mutual_Information_Criterion_For_Modelling_Time_Series_Via_Neural_Networks/links/09e4150ae0a91f3b3f000000.pdf},
}

@InProceedings{Song16elecCnsmptFrcstELM,
  author    = {H. Song and A. K. Qin and F. D. Salim},
  title     = {Multivariate electricity consumption prediction with Extreme Learning Machine},
  booktitle = {Proc. Int. Joint Conf. Neural Networks (IJCNN)},
  year      = {2016},
  pages     = {2313--2320},
  month     = jul,
  abstract  = {In this paper, Extreme Learning Machine (ELM) is demonstrated to be a powerful tool for electricity consumption prediction based on its competitive prediction accuracy and superior computational speed compared to Support Vector Machine (SVM). Moreover, ELM is utilized to investigate the potentials of using auxiliary information such as electricity-related factors and environmental factors to augment the prediction accuracy obtained by purely using the electricity consumption factors. Furthermore, we formulate a combinatorial optimization problem of seeking an optimal subset of auxiliary factors and their corresponding optimal window sizes using the most suitable ELM structure, and propose a Discrete Dynamic Multi-Swarm Particle Swarm Optimization (DDMS-PSO) to address this problem. Experimental studies on a real-world building dataset demonstrate that electricity-related factors improve accuracy while environmental factors further boost accuracy. By using DDMSPSO, we find a subset of electricity-related and environmental factors, their respective window sizes, and the number of hidden neurons in ELM which leads to the best prediction accuracy.},
  comment   = {Extreme learning machine for electricty consumption.  Also does feature selection using particle swarm.},
  doi       = {10.1109/IJCNN.2016.7727486},
  file      = {Song16elecCnsmptFrcstELM.pdf:Song16elecCnsmptFrcstELM.pdf:PDF},
  keywords  = {combinatorial mathematics, learning (artificial intelligence), particle swarm optimisation, power consumption, power engineering computing, DDMS-PSO, ELM, SVM, auxiliary factors, auxiliary information, building dataset, combinatorial optimization problem, computational speed, discrete dynamic multiswarm particle swarm optimization, electricity consumption factors, electricity-related factors, environmental factors, extreme learning machine, multivariate electricity consumption prediction, optimal subset seeking, optimal window sizes, prediction accuracy, support vector machine, Buildings, Energy consumption, Environmental factors, Neurons, Optimization, Support vector machines, Time series analysis},
  owner     = {sotterson},
  timestamp = {2017.01.31},
}

@TechReport{Torrez93JohnsonDistFitNeuron,
  author      = {Torrez, WC and Durham, JT},
  title       = {Johnson Distributions for Fitting Weighted Sums of Sigmoided Neuron Outputs},
  institution = {DTIC Document},
  year        = {1993},
  abstract    = {In this paper, it is shown that a continuum of distributions best characterizes the hidden
iii-7 layer outputs of a multilayer perceptron when trained as a 0-1 classifier and tested with a range
of signal-to-noise ratio (SNR) input distributions. A four parameter system of transformed
normal distributions, known as the Johnson system of distributions,is utilized to illustrate the
shape of output distributions as a function of input SNR. levels.},
  comment     = {How to fit Johnson distribution given quantiles. Somehow this is used for neural net stuff but I didn't bother to understand it. But then they recommend somd kind of stochastic optimization for fitting. Could this be used for wind power upscaling?

Upscaling use?
* sigmoids look like wind power curves.
* sums of weighted sigmoids looks like capacity weighted power production giving wind speed input
* could this be like Stefan's "PGM"? Vogt15HybridPhysMLrgnFrcst

* main Johnson Dist paper: George11estJohnsonDistBnd},
  file        = {Torrez93JohnsonDistFitNeuron.pdf:Torrez93JohnsonDistFitNeuron.pdf:PDF},
  url         = {http://www.researchgate.net/publication/235145247_Johnson_Distributions_for_Fitting_Weighted_Sums_of_Sigmoided_Neuron_Outputs},
}

@Article{Zheng09rampDataMine,
  author    = {Haiyang Zheng and Andrew Kusiak},
  title     = {Prediction of Wind Farm Power Ramp Rates: A Data-Mining Approach},
  journal   = {Journal of Solar Energy Engineering},
  year      = {2009},
  volume    = {131},
  number    = {3},
  pages     = {031011},
  abstract  = {In this paper, multivariate time series models were built to predict the power ramp rates of a wind farm. The power changes were predicted at 10 min intervals. Multivariate time series models were built with data-mining algorithms. Five different data-mining algorithms were tested using data collected at a wind farm. The support vector machine regression algorithm performed best out of the five algorithms studied in this research. It provided predictions of the power ramp rate for a time horizon of 10-60 min. The boosting tree algorithm selects parameters for enhancement of the prediction accuracy of the power ramp rate. The data used in this research originated at a wind farm of 100 turbines. The test results of multivariate time series models were presented in this paper. Suggestions for future research were provided. DOI: 10.1115/1.3142727 Keywords: power ramp rate prediction, wind farm, data-mining algorithms, multivariate time series model, parameter selection},
  comment   = {Autoregressive ramp forecast for (10,50 minutes ahead)
* pretty easy to ready; easy to implement
* Refs 2, 3 and 4 supposedly use ramp rates during integration
-- but I don't see a ramp rate constraint in [4] (Potter06vShortWindFrcstTas) Forecast inputs
* lagged versions past power stats, like mean/std of turbine wind speed, past power ramp rates, etc.
* starts w/ about 30 (6 vars, 5 lags); selects 9 for 10 min ahead, and fewer for longer horizons
* selection w/ a boosting tree, different for each lookahead
-- is a decision tree really a good way to select features for a continuous predictand?
* selection threshold was heurstically set, rather than cross-validated or whatever Ramp definition:
* Diff of powers on either end of a 10 minute interval
* seems very noise succeptable
* predicting ramp rate; not binary ramp if greater than a threshold


Forecast algorithm
* support vector regression is best
* random forest and others not so good.
* works for up to 40 minutes ahead; says need NWP for longer lookaheads

* says that ramp forecasts for less than 10 minutes are also demanded by operators},
  doi       = {10.1115/1.3142727},
  eid       = {031011},
  file      = {Paper:Zheng09rampDataMine.pdf:PDF;Slides:Zheng09rampDataMineSlides.pdf:PDF},
  groups    = {Read},
  keywords  = {data mining; power engineering computing; regression analysis; support vector machines; time series; wind power plants; wind turbines},
  numpages  = {8},
  publisher = {ASME},
  timestamp = {2011.04.27},
  url       = {http://link.aip.org/link/?SLE/131/031011/1},
}

@Article{Abbott11nukeGlbScl,
  author    = {Abbott, D.},
  title     = {Is Nuclear Power Globally Scalable? [Point of View]},
  journal   = {Proceedings of the IEEE},
  year      = {2011},
  volume    = {99},
  number    = {10},
  pages     = {1611--1617},
  month     = oct,
  issn      = {0018-9219},
  abstract  = {In this paper, nuclear power utility as the main energy source is discussed.},
  comment   = {Powering even 1/15\textsuperscript{th} of the world w/ uranium nuclear fission (even fast breeders) will be almost impossible. Fusion has similar problems. Related article on Phys.org is in Evernote: evernote:///view/1523219/s13/26f535ce-7042-4ab1-a0b2-cda7da89e2ea/26f535ce-7042-4ab1-a0b2-cda7da89e2ea/},
  doi       = {10.1109/JPROC.2011.2161806},
  file      = {Abbott11nukeGlbScl.pdf:Abbott11nukeGlbScl.pdf:PDF},
  keywords  = {energy source;nuclear power;nuclear power;},
  owner     = {sotterson},
  timestamp = {2012.03.10},
}

@InProceedings{Goebel05apprxMIdist,
  author    = {Goebel, B. and Dawy, Z. and Hagenauer, J. and Mueller, J.C.},
  title     = {An approximation to the distribution of finite sample size mutual information estimates},
  booktitle = {2005 IEEE International Conference on Communications},
  year      = {2005},
  volume    = {2},
  pages     = {1102--1106},
  month     = may,
  abstract  = {In this paper, the distribution of mutual information between two discrete random variables is approximated by means of a second-order Taylor series expansion. Approximative expressions for the distribution of mutual information (MI) between independent random variables, conditional MI between conditionally independent variables, and MI between (weakly) dependent random variables are derived. These distributions are functions of the available sample size and the number of realisations of the random variables only; knowledge of the variables' PMF is not required. The results are verified numerically for various cases. Exemplary application ideas in statistics and communications engineering are proposed.},
  comment   = {distribution of MI estimates with finite sample sizes; useful for ramp MI confidence interval boosting? * could just mention it and its restrictions to justify bootstrapping MI * for discrete, 2 variable problems, though. * but I think it's cited later by people who did continuous and maybe multivariate stuff},
  doi       = {10.1109/ICC.2005.1494518},
  file      = {Goebel05apprxMIdist.pdf:Goebel05apprxMIdist.pdf:PDF},
  keywords  = { communications engineering; discrete random variable; finite sample size mutual information estimation; second-order Taylor series expansion; statistical analysis; approximation theory; information theory; statistical analysis;},
  owner     = {sotterson},
  timestamp = {2011.11.30},
}

@Article{Botterud13dispUCprobWndFrcst,
  author    = {Botterud, A. and Zhi Zhou and Jianhui Wang and Sumaili, J. and Keko, H. and Mendes, J. and Bessa, R.J. and Miranda, V.},
  title     = {Demand Dispatch and Probabilistic Wind Power Forecasting in Unit Commitment and Economic Dispatch: A Case Study of Illinois},
  journal   = {Sustainable Energy, IEEE Transactions on},
  year      = {2013},
  volume    = {4},
  number    = {1},
  pages     = {250--261},
  issn      = {1949-3029},
  abstract  = {In this paper, we analyze how demand dispatch combined with the use of probabilistic wind power forecasting can help accommodate large shares of wind power in electricity market operations. We model the operation of day-ahead and real-time electricity markets, which the system operator clears by centralized unit commitment and economic dispatch. We use probabilistic wind power forecasting to estimate dynamic operating reserve requirements, based on the level of uncertainty in the forecast. At the same time, we represent price responsive demand as a dispatchable resource, which adds flexibility in the system operation. In a case study of the power system in Illinois, we find that both demand dispatch and probabilistic wind power forecasting can contribute to efficient operation of electricity markets with large shares of wind power.},
  comment   = {How stoch optimization and prob forecasts would benefit Illinois
Very long tech note: Botterud11windPowFrcstOpUse},
  doi       = {10.1109/TSTE.2012.2215631},
  file      = {Botterud13dispUCprobWndFrcst.pdf:Botterud13dispUCprobWndFrcst.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  keywords  = {load forecasting;power generation dispatch;power generation economics;power generation scheduling;power markets;wind power plants;Illinois;centralized unit commitment;demand dispatch;dispatchable resource;dynamic operating reserve requirements estimation;economic dispatch;price responsive demand;probabilistic wind power forecasting;real-time electricity markets;Electricity supply industry;Forecasting;Load management;Probabilistic logic;Uncertainty;Wind forecasting;Wind power generation;Demand dispatch;dynamic operating reserves;economic dispatch;electricity markets;probabilistic forecasts;unit commitment;wind power},
  owner     = {sotterson},
  timestamp = {2013.10.01},
}

@Article{Schaffernicht11wgtMutInfoFeatSel,
  author    = {Schaffernicht, E. and Gross, H.M.},
  title     = {Weighted mutual information for feature selection},
  journal   = {Artificial Neural Networks and Machine Learning--ICANN 2011},
  year      = {2011},
  pages     = {181--188},
  abstract  = {In this paper, we apply weighted Mutual Information for ef- fective feature selection. The presented hybrid filter wrapper approach resembles the well known AdaBoost algorithm by focusing on those samples that are not classifed or approximated correctly using the selected features. Redundancies and bias of the employed learning machine are handled implicitly by our approach. In experiments, we compare the weighted Mutual Information algorithm with other basic approaches for feature subset selection that use similar selection criteria. The efficiency and effectiveness of our method are demonstrated by the obtained results.},
  comment   = {Hybrid filter/wrapper feature sel., MI samples weighted by error w/ prev. featsel. No redundancy check. Algorithm is a lot like AdaBoost

* calculate prediction error-weighted MI of output each unselected feature
* select max wMI feature
* retrain predictor (classifier, here)
* if error reduction more than epsilon, continue. Else stop

Comments
* no redundancy penalty but technique elegantlly avoids redundancy since it's looking at residuals
-- as claimed in Schaffernicht09residMutInfoFeatSel
* weighted MI is pairwise, just candidate feature against residual
* these two things mean it's missing higher order reactions (see Brown09newInfoFeatSel)

MI est
* histogram or kernel density
* hard to adapt Kraskov04EstMutInfKNN to this

Immediate applications for this algorithm
* tested on classification
* works best when classifier output is "soft," not hard binary
-- seems like it would work OK on continuous prediction too
* doesn't work so well on KNN or "local methods" but does work on global ones, like MLP's

Possible Inspiration for Regime detection
* somehow build a tree of predictions, greedily adding features
* somehow weight MI to allow it to branch differently, for different input patterns (wind directions)
* would have to be accompanied by some kinda weighted prediction algorithm


Tests:
* on real and synthetic data
* no test against Peng mRMR, even though it was mentioned
* essentially no improvement for KNN but on MLP's, seems comparable to RMI

Other interesting methods mentioned:
* RMI (residual mutual information): computes MI between _target_ and residual -- normal MI calc
-- residual is at output
-- seems like a similar idea, and performance seems similar, although authors of this paper seem to claim that it's not quite as good?
-- paper: Schaffernicht09residMutInfoFeatSel
* Torkkola Quadratic MI feature transformation
-- MI est between output and target, used to adjust a gradient descent transformation.
-- I don't quite get it, but the fact that it's a transformation is interesting
* Chow-Liu Trees
-- filterlike construction of trees in forward selection
-- I'll have to look this up, but maybe interesting for regime detection?},
  file      = {Schaffernicht11wgtMutInfoFeatSel.pdf:Schaffernicht11wgtMutInfoFeatSel.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2012.02.17},
}

@Article{Bianchi15ShortTermElectric,
  author    = {F. M. Bianchi and E. De Santis and A. Rizzi and A. Sadeghian},
  title     = {Short-Term Electric Load Forecasting Using Echo State Networks and {PCA} Decomposition},
  journal   = {IEEE Access},
  year      = {2015},
  volume    = {3},
  pages     = {1931--1943},
  issn      = {2169-3536},
  abstract  = {In this paper, we approach the problem of forecasting a time series (TS) of an electrical load measured on the Azienda Comunale Energia e Ambiente (ACEA) power grid, the company managing the electricity distribution in Rome, Italy, with an echo state network (ESN) considering two different leading times of 10 min and 1 day. We use a standard approach for predicting the load in the next 10 min, while, for a forecast horizon of one day, we represent the data with a high-dimensional multi-variate TS, where the number of variables is equivalent to the quantity of measurements registered in a day. Through the orthogonal transformation returned by PCA decomposition, we reduce the dimensionality of the TS to a lower number k of distinct variables; this allows us to cast the original prediction problem in k different one-step ahead predictions. The overall forecast can be effectively managed by k distinct prediction models, whose outputs are combined together to obtain the final result. We employ a genetic algorithm for tuning the parameters of the ESN and compare its prediction accuracy with a standard autoregressive integrated moving average model.},
  comment   = {Echo state networks, separate modeles/horizon, PCA.  I haven't read it...},
  doi       = {10.1109/ACCESS.2015.2485943},
  file      = {Bianchi15ShortTermElectric.pdf:Bianchi15ShortTermElectric.pdf:PDF},
  keywords  = {distribution networks, load forecasting, power grids, principal component analysis, time series, ACEA power grid, Azienda Comunale Energia e Ambiente, ESN, Italy, PCA decomposition, Rome, autoregressive integrated moving average model, echo state networks, electric load forecasting, electricity distribution, time series, Forecasting, Genetic algorithms, Load management, Predictive models, Smart grids, Time series analysis, Dimensionality Reduction, Echo State Network, Electric Load Prediction, Forecasting, Genetic Algorithm, PCA, Smart Grid, Time-Series, Time-series, dimensionality reduction, echo state network, electric load prediction, forecasting, genetic algorithm, smart grid},
  owner     = {sotterson},
  timestamp = {2017.03.08},
}

@InProceedings{Iyengar10copulaEEGsynch,
  author    = {Iyengar, S.G. and Dauwels, J. and Varshney, P.K. and Cichocki, A.},
  title     = {Quantifying {EEG} synchrony using copulas},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2010},
  pages     = {505--508},
  abstract  = {In this paper, we consider the problem of quantifying synchrony between multiple simultaneously recorded electroencephalographic signals. These signals exhibit nonlinear dependencies and non-Gaussian statistics. A copula based approach is presented to model the joint statistics. We then consider the application of copula derived synchrony measures for early diagnosis of Alzheimer's disease. Results on real data are presented.},
  comment   = {spinning reserves scenario generation quality measure or just the generator? Interesting b/c doesn't pick Gaussian copula; finds that t-copula does better job on tails.},
  doi       = {10.1109/ICASSP.2010.5495664},
  file      = {Iyengar10copulaEEGsynch.pdf:Iyengar10copulaEEGsynch.pdf:PDF},
  issn      = {1520-6149},
  keywords  = {diseases, electroencephalography, medical signal processing, neurophysiology, Alzheimer disease, EEG synchrony, copulas, electroencephalographic signals, nonGaussian statistics, nonlinear dependencies},
  owner     = {scot},
  timestamp = {2010.12.02},
}

@InProceedings{Osborne08realtimeGausProc,
  author    = {Osborne, M.A. and Roberts, S.J. and Rogers, A. and Ramchurn, S.D. and Jennings, N.R.},
  title     = {Towards Real-Time Information Processing of Sensor Network Data Using Computationally Efficient Multi-output {Gauss}ian Processes},
  booktitle = {Information Processing in Sensor Networks (IPSN)},
  year      = {2008},
  pages     = {109--120},
  month     = apr,
  abstract  = {In this paper, we describe a novel, computationally efficient algorithm that facilitates the autonomous acquisition of readings from sensor networks (deciding when and which sensor to acquire readings from at any time), and which can, with minimal domain knowledge, perform a range of information processing tasks including modelling the accuracy of the sensor readings, predicting the value of missing sensor readings, and predicting how the monitored environmental variables will evolve into the future. Our motivating scenario is the need to provide situational awareness support to first responders at the scene of a large scale incident, and to this end, we describe a novel iterative formulation of a multi-output Gaussian process that can build and exploit a probabilistic model of the environmental variables being measured (including the correlations and delays that exist between them). We validate our approach using data collected from a network of weather sensors located on the south coast of England.},
  comment   = {realtime switching features, missing feature handling, could be used for targeted observations. Also handles delays
* they're using weather sensors!
* online learning
* does multiple output: wind U and V?
* assumes that sensor readings have a cost and optimizes for that
-- not ideal?},
  doi       = {10.1109/IPSN.2008.25},
  file      = {Osborne08realtimeGausProc.pdf:Osborne08realtimeGausProc.pdf:PDF;Osborne08realtimeGausProc.pdf:Osborne08realtimeGausProc.pdf:PDF},
  keywords  = {Gaussian processes, distributed processing, sensor fusionmultioutput Gaussian processes, real-time information processing, sensor network data, sensor networks, sensor readings, weather sensors},
  owner     = {sotterson},
  timestamp = {2009.03.02},
}

@Article{Lobel11cnsmrChoiceDmdFrcstSlr,
  author    = {Lobel, Ruben and Perakis, Georgia},
  title     = {Consumer choice model for forecasting demand and designing incentives for solar technology},
  year      = {2011},
  abstract  = {In this paper, we develop a model for the adoption of solar photovoltaic technology by residential consumers. In particular,
we assume consumers purchase these solar panels according to a discrete choice model. The technology adoption process
is reinforced by network externalities such as imitating customer behavior and cost improvements through learning-by-
doing. Using this model, we develop a framework for policy makers to find optimal subsidies in order to achieve a desired
adoption target with minimum cost for the system. We discuss the structure of the optimal subsidy policy and how the
overall system cost changes with the adoption target. Furthermore, we validate the model through an empirical study of
the German solar market, where we estimate the model parameters, generate adoption forecasts and demonstrate how to
solve the policy design problem. We use this framework to show that the current policies in Germany are not efficient.
In particular, our study suggests that their subsidies should be higher in the near future and the gradual phase-out of the
subsidies should occur faster.},
  comment   = {A study of forecasted German PV adoption rates concludes that DE policies aren't efficient, and that they shoud have, in 2011, increased their PV incentives.

Seems very close to what MN SP wants to do.  60 cites.},
  file      = {:Lobel11cnsmrChoiceDmdFrcstSlr.pdf:PDF},
  publisher = {MIT Sloan Research Paper},
  url       = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1748424},
}

@InProceedings{Kanter15deepFeatSyntCpla,
  author    = {J. M. Kanter and K. Veeramachaneni},
  title     = {Deep feature synthesis: Towards automating data science endeavors},
  booktitle = {2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)},
  year      = {2015},
  pages     = {1-10},
  month     = {Oct},
  abstract  = {In this paper, we develop the Data Science Machine, which is able to derive predictive models from raw data automatically. To achieve this automation, we first propose and develop the Deep Feature Synthesis algorithm for automatically generating features for relational datasets. The algorithm follows relationships in the data to a base field, and then sequentially applies mathematical functions along that path to create the final feature. Second, we implement a generalizable machine learning pipeline and tune it using a novel Gaussian Copula process based approach. We entered the Data Science Machine in 3 data science competitions that featured 906 other data science teams. Our approach beats 615 teams in these data science competitions. In 2 of the 3 competitions we beat a majority of competitors, and in the third, we achieved 94% of the best competitor's score. In the best case, with an ongoing competition, we beat 85.6% of the teams and achieved 95.7% of the top submissions score.},
  comment   = {Uses a Gaussian Copula to synthesize features.  Idea is to automatically generate features, do dim reduction, and prediction.},
  doi       = {10.1109/DSAA.2015.7344858},
  file      = {Kanter15deepFeatSyntCpla.pdf:Kanter15deepFeatSyntCpla.pdf:PDF},
  keywords  = {Gaussian processes;data analysis;learning (artificial intelligence);Gaussian copula process;data science endeavor automation;data science machine;deep feature synthesis;feature generation;generalizable machine learning pipeline;mathematical functions;relational datasets;Algorithm design and analysis;Data mining;Data models;Feature extraction;Machine learning algorithms;Prediction algorithms;Predictive models},
}

@TechReport{DeSilva04sparseMDS,
  author      = {De Silva, Vin and Tenenbaum, Joshua B},
  title       = {Sparse multidimensional scaling using landmark points},
  institution = {Technical report, Stanford University},
  year        = {2004},
  abstract    = {In this paper, we discuss a computationally efficient approximation to the classi-
cal multidimensional scaling (MDS) algorithm, called Landmark MDS (LMDS), for use when the
number of data points is very large. The ?rst step of the algorithm is to run classical MDS to em-
bed a chosen subset of the data, referred to as the ?landmark points?, in a low-dimensional space.
Each remaining data point can be located within this space given knowledge of its distances to the
landmark points. We give an elementary and explicit theoretical analysis of this procedure, and
demonstrate with examples that LMDS is effiective in practical use.
Keywords: visualization, embedding, online algorithms, feature discovery, unsupervised learning},
  comment     = {Do out of sample classical MDS by doing MDS on a subset of "landmark" points, and then fitting the remaining out of sample points making use of landmark point distances (I think this was the idea). Is this what they do in isomap out of sample functions in the matlab dimnesion reduction toolbox (Maaten09DimRedCmprTechNote)?},
  file        = {DeSilva04sparseMDS.pdf:DeSilva04sparseMDS.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2014.06.27},
  url         = {http://window.stanford.edu/courses/cs468-05-winter/Papers/Landmarks/Silva_landmarks5.pdf},
}

@Article{Kaut03evalScenarioGen,
  author    = {Kaut, M. and Wallace, S.W.},
  title     = {Evaluation of scenario-generation methods for stochastic programming},
  journal   = {Stochastic Programming E-Print Series},
  year      = {2003},
  volume    = {14},
  number    = {1},
  abstract  = {In this paper, we discuss the evaluation of quality/suitability of scenario-generation methods for a given stochastic programming model. We formulate minimal requirements that should be imposed on a scenario-generation method before it can be used for solving the stochastic programming model. We also show how the requirements can be tested. The procedure of testing a scenario-generation method is illustrated on a case from portfolio management. In addition, we provide a short overview of the most common scenario-generation methods. Keywords: stochastic programming, scenario tree, scenario generation},
  comment   = {Scenario forecast quality metrics.},
  file      = {Kaut03evalScenarioGen.pdf:Kaut03evalScenarioGen.pdf:PDF},
  groups    = {Test, doReadNonWPV_2},
  owner     = {scot},
  publisher = {Citeseer},
  timestamp = {2010.11.23},
}

@InProceedings{Waldl10rulezRampAlarm,
  author    = {Waldl, H.P.I. and Brandt, P.},
  title     = {Anemos. Rulez: Extreme and ramp event alarming to support stability of energy grids},
  booktitle = {German Wind Energy Conference (DEWEK)},
  year      = {2010},
  abstract  = {In this paper, we discuss the need to predict and alarm upcoming extreme events, e.g. like a fast increase of the wind power production or cut-off events, as a complement to daily operational wind power predictions. As there is no universal definition for extreme events, we describe important parameters for their definition and factors that influence these parameters. A tool for extreme event predictions, Anemos.Rulez, will be presented, including evaluation results from an application test case.},
  comment   = {Ramp alarms based on derivatives. Very wide timing collar, no mathematical way to use.
* says extreme event alarms can be included in control room routines but doesn't say how.
* how derivatives are calcuated is not described.
* no explanation of thresholds chosen in Table 1
* a "Hit" is declared if within 12 hours of a measured ramp
-- 12 hours!
-- on a 12-36 hour ahead horizon},
  file      = {Waldl11rulezRampAlarm.pdf:Waldl11rulezRampAlarm.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  owner     = {scot},
  timestamp = {2011.05.02},
  url       = {http://www.overspeed.de/2011-03-29_DEWEK2010.session20.Anemos.Rulez.Waldi.final.pdf},
}

@Article{Bernard12dynRandForest,
  author   = {Simon Bernard and Sébastien Adam and Laurent Heutte},
  title    = {Dynamic Random Forests},
  journal  = {Pattern Recognition Letters},
  year     = {2012},
  volume   = {33},
  number   = {12},
  pages    = {1580 - 1586},
  issn     = {0167-8655},
  abstract = {In this paper, we introduce a new Random Forest (RF) induction algorithm called Dynamic Random Forest (DRF) which is based on an adaptative tree induction procedure. The main idea is to guide the tree induction so that each tree will complement as much as possible the existing trees in the ensemble. This is done here through a resampling of the training data, inspired by boosting algorithms, and combined with other randomization processes used in traditional RF methods. The DRF algorithm shows a significant improvement in terms of accuracy compared to the standard static RF induction algorithm.},
  comment  = {Incrmentally builds trees in a kind of boosting, but instead of predicting error, it weights training samples that were bad on previous collection of trees.  Python scikit learn has sample-weighting, so this could be easily implemented.

An improvement on Bernard08forestRKmtry

Slides attached.

Was used in Nami18cardFraudDynRandFrstKnn},
  doi      = {https://doi.org/10.1016/j.patrec.2012.04.003},
  file     = {:Bernard12dynRandForest.pdf:PDF;:Bernard12dynRandForest_slides.pdf:PDF},
  keywords = {Random forests, Ensemble of classifiers, Random feature selection, Dynamic induction},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167865512001274},
}

@InProceedings{Rendle10factrMachns,
  author    = {S. Rendle},
  title     = {Factorization Machines},
  booktitle = {Proc. IEEE Int. Conf. Data Mining},
  year      = {2010},
  pages     = {995--1000},
  month     = dec,
  abstract  = {In this paper, we introduce Factorization Machines (FM) which are a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models. Like SVMs, FMs are a general predictor working with any real valued feature vector. In contrast to SVMs, FMs model all interactions between variables using factorized parameters. Thus they are able to estimate interactions even in problems with huge sparsity (like recommender systems) where SVMs fail. We show that the model equation of FMs can be calculated in linear time and thus FMs can be optimized directly. So unlike nonlinear SVMs, a transformation in the dual form is not necessary and the model parameters can be estimated directly without the need of any support vector in the solution. We show the relationship to SVMs and the advantages of FMs for parameter estimation in sparse settings. On the other hand there are many different factorization models like matrix factorization, parallel factor analysis or specialized models like SVD++, PITF or FPMC. The drawback of these models is that they are not applicable for general prediction tasks but work only with special input data. Furthermore their model equations and optimization algorithms are derived individually for each task. We show that FMs can mimic these models just by specifying the input data (i.e. the feature vectors). This makes FMs easily applicable even for users without expert knowledge in factorization models.},
  comment   = {Powerful ML agorithm in H2O},
  doi       = {10.1109/ICDM.2010.127},
  file      = {:Rendle10factrMachns.pdf:PDF},
  issn      = {2374-8486},
  keywords  = {data mining, matrix decomposition, support vector machines, factorization machine, support vector machine, SVM, model parameters, parameter estimation, feature vector, sparse data, tensor factorization, Mathematical model, Support vector machines, Frequency modulation, Predictive models, Data models, Equations, Computational modeling, factorization machine, sparse data, tensor factorization, support vector machine},
}

@Article{Wang11windFrcstUncertUC,
  author    = {J. Wang and A. Botterud and R. Bessa and H. Keko and L. Carvalho and D. Issicaba and J. Sumaili and V. Miranda},
  title     = {Wind power forecasting uncertainty and unit commitment},
  journal   = {Applied Energy},
  year      = {2011},
  volume    = {88},
  number    = {11},
  pages     = {4014--4023},
  issn      = {0306-2619},
  abstract  = {In this paper, we investigate the representation of wind power forecasting (WPF) uncertainty in the unit commitment (UC) problem. While deterministic approaches use a point forecast of wind power output, \{WPF\} uncertainty in the stochastic \{UC\} alternative is captured by a number of scenarios that include cross-temporal dependency. A comparison among a diversity of \{UC\} strategies (based on a set of realistic experiments) is presented. The results indicate that representing \{WPF\} uncertainty with wind power scenarios that rely on stochastic \{UC\} has advantages over deterministic approaches that mimic the classical models. Moreover, the stochastic model provides a rational and adaptive way to provide adequate spinning reserves at every hour, as opposed to increasing reserves to predefined, fixed margins that cannot account either for the system?s costs or its assumed risks.},
  comment   = {Advocate representing wind power uncertainty as scenarios. Scenarios have advantages for stochastic UC, and using them for hourly spinning reserveis better than fixed allocations.

The pdf is from Subito -- need the subito openFile plugin installed in Acrobat in order to read it.

But the same info is probably in this tech note: Botterud11windPowFrcstOpUse},
  doi       = {10.1016/j.apenergy.2011.04.011},
  file      = {Wang11windFrcstUncertUC.pdf:Wang11windFrcstUncertUC.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  keywords  = {Electricity markets},
  ncite     = {29},
  owner     = {sotterson},
  timestamp = {2013.10.01},
  url       = {http://www.sciencedirect.com/science/article/pii/S0306261911002339},
}

@Article{Tewfik87eigCylindHarm,
  author    = {Ahmed H. Tewfik and Bernard C. Levy and Alan S. Willsky},
  title     = {An eigenstructure approach for the retrieval of cylindrical harmonics},
  journal   = {Signal Processing},
  year      = {1987},
  volume    = {13},
  number    = {2},
  pages     = {121--139},
  issn      = {0165-1684},
  abstract  = {In this paper, we present a high resolution spectral estimation method for 2-D isotropic random fields with covariance functions equal to weighted sums of cylindrical harmonics. Such fields are often used to model some types of background noises in geophysics and in ocean acoustics. The approach that we present differs from previous 2-D spectral estimation techniques by the fact that we take maximal advantage of the symmetries implied by both the isotropy and the special covariance structure of these fields. Note that isotropy is the natural generalization to several dimensions of the 1-D notion of stationarity. Our approach is similar in spirit to 1-D harmonic retrieval techniques, such as the MUSIC method, which rely on an eigenanalysis of the covariance matrix. In the 2-D isotropic context, we begin with a Fourier series representation of an isotropic field with respect to the angle [theta] in a polar coordinate representation of the underlying 2-D space. We then obtain a spectral estimate by performing an eigenanalysis of the covariance matrix of samples of the zeroth-order Fourier coefficient process in order to extract the cylindrical harmonics. We also discuss the estimation of this covariance matrix and present examples to illustrate the high resolution and robustness properties of our procedure.},
  comment   = {efficient way to compute cyclindrical basis for lagged wind velocity? makes use of known symmetries so would be more efficient than brute force inclusion of basis-projected variables in a linear regression input could be an interesting paper? is this 2D or 3D? is Salehin09eigLungCirc better? adaptable to 3D harmonics in Sernelius10laplacePoisson},
  doi       = {DOI: 10.1016/0165-1684(87)90043-0},
  file      = {Tewfik87eigCylindHarm.pdf:Tewfik87eigCylindHarm.pdf:PDF},
  keywords  = {Isotropic random field},
  owner     = {scotto},
  timestamp = {2010.08.19},
  url       = {http://www.sciencedirect.com/science/article/B6V18-49990NJ-5B/2/078661905f55fa98f1fb3e26de90d59b},
}

@Article{Deng04cylindSpln3D,
  author    = {Xiang Deng and Denney, T.S., Jr.},
  title     = {Three-dimensional myocardial strain reconstruction from tagged {MRI} using a cylindrical B-spline model},
  journal   = {Medical Imaging, IEEE Transactions on},
  year      = {2004},
  volume    = {23},
  number    = {7},
  pages     = {861--867},
  month     = jul,
  issn      = {0278-0062},
  abstract  = {In this paper, we present a new method for reconstructing three-dimensional (3-D) left ventricular myocardial strain from tagged magnetic resonance (MR) image data with a 3-D B-spline deformation model. The B-spline model is based on a cylindrical coordinate system that more closely fits the morphology of the myocardium than previously proposed Cartesian B-spline models and does not require explicit regularization. Our reconstruction method first fits a spatial coordinate B-spline displacement field to the tag line data. This displacement field maps each tag line point in the deformed myocardium back to its reference position (end-diastole). The spatial coordinate displacement field is then converted to material coordinates with another B-spline fit. Finally, strain is computed by analytically differentiating the material coordinate B-spline displacement field with respect to space. We tested our method with strains reconstructed from an analytically defined mathematical left ventricular deformation model and ten human imaging studies. Our results demonstrate that a quadratic cylindrical B-spline with a fixed number of control points can accurately fit a physiologically realistic range of deformations. The average 3-D reconstruction computation time is 20 seconds per time frame on a 450 MHz Sun Ultra80 workstation.},
  comment   = {models heart w/ cylindrical 3D spline. use for lagged wind velocity model? 4D spline in Ozturk00spline4D adds time; use for autoregressive lagged velocity model? Jian08volCylBspln shows how to put this into matrix form},
  doi       = {10.1109/TMI.2004.827961},
  file      = {Deng04cylindSpln3D.pdf:Deng04cylindSpln3D.pdf:PDF},
  keywords  = {20 s;3-D B-spline deformation model;450 MHz;Sun Ultra workstation;end-diastole;left ventricle;magnetic resonance imaging;quadratic cylindrical B-spline model;spatial coordinate B-spline displacement field;tagged MRI;three-dimensional myocardial strain reconstruction;biomechanics;biomedical MRI;cardiology;deformation;image reconstruction;medical image processing;physiological models;splines (mathematics);Algorithms;Heart Ventricles;Humans;Image Enhancement;Image Processing, Computer-Assisted;Imaging, Three-Dimensional;Magnetic Resonance Imaging;Models, Cardiovascular;Models, Statistical;Myocardium;},
  owner     = {scotto},
  timestamp = {2010.08.20},
}

@InCollection{Wang02ddrCylindHarm,
  author      = {Wang, Fei and Davis, Thomas and Vemuri, Baba},
  title       = {Real-Time DRR Generation Using Cylindrical Harmonics},
  booktitle   = {Medical Image Computing and Computer-Assisted Intervention},
  publisher   = {Springer Berlin / Heidelberg},
  year        = {2002},
  editor      = {Dohi, Takeyoshi and Kikinis, Ron},
  volume      = {2489},
  series      = {Lecture Notes in Computer Science},
  pages       = {671--678},
  abstract    = {In this paper, we present a very fast algorithm for generating Digitally Reconstructed Radiographs(DRRs) using cylindrical harmonics. Real-time generation of DRRs is crucial in intra-operative applications requiring matching of pre-operative 3D data to 2D X-ray images acquired intra-operatively. Our algorithm involves representing the preoperative 3D data set in a cylindrical harmonic representation and then projecting each of these harmonics from the chosen projection point to construct a set of 2D projections whose superposition is the DRR of the data set in its reference orientation. The key advantage of our algorithm over existing algorithms such as the ray-casting or the voxel projection or the hybrid schemes is that in our method, once the projection set is generated from an arbitrarily chosen point of projection, DRRs of the underlying object at arbitrary rotations are simply obtained via a complete exponentially weighted superposition of the set. This leads to tremendous computational savings over and above the basic computational advantages of the algorithm involving the use of truncated cylindrical harmonic representation of the data. We present examples of DRR synthesis with fanbeam projection geometry for synthetic and real data. As an indicator of the speed of computation of one DRR from an arbitrary projection point, only 2?3 CPU seconds are required on a DELL Precision420 using MATLAB as the program development environment.},
  affiliation = {University of Florida Department of Computer Science and Engineering Gainesville FL 32611},
  comment     = {Cylindrical harmonics used for CT scan (I think). Works as a basis here, could it handle sharp discontinuities you'd see in a wind spd/dir/lag basis function (down lag dir)},
  doi         = {10.1007/3-540-45787-9_84},
  file        = {Wang02ddrCylindHarm.pdf:Wang02ddrCylindHarm.pdf:PDF},
  owner       = {scotto},
  timestamp   = {2010.08.19},
}

@InProceedings{Schaffernicht09residMutInfoFeatSel,
  author    = {Schaffernicht, E. and M{\"o}ller, C. and Debes, K. and Gross, H.M.},
  title     = {Forward Feature Selection Using Residual Mutual Information},
  booktitle = {17\textsuperscript{th} European Symposium on Artificial Neural Networks, ESANN},
  year      = {2009},
  pages     = {583--588},
  abstract  = {In this paper, we propose a hybrid filter/wrapper approach for fast feature selection using the Residual Mutual Information (RMI) between the function approximator output and the remaining features as selection criterion. This approach can handle redundancies in the data as well as the bias of the employed learning machine while keeping the number of required training and evaluation procedures low. In classification experiments, we compare the Residual Mutual Information algorithm with other basic approaches for feature subset selection that use similar selection criteria. The e?ciency and e?ectiveness of our method are demonstrated by the obtained results on UCI datasets.},
  comment   = {Hybrid filter/wrapper: featsel based on mutInfo of predictor error and candidate input. Maybe an elegant redundancy solution.

A way to avoid mRMR hack of subtracting redundancy (mRMR: peng05featSelMutInfo) but it's not compared to Peng in the end, so I don't know if this is really true. Otherwise it seems to work. A (debatably) better method by same authors is in: Schaffernicht11wgtMutInfoFeatSel But nice thing about this method vs. Schaffernicht11wgtMutInfoFeatSel is that Kraskov04EstMutInfKNN KNN MI method works easily.

Other comments:
* only train the NN (num. features selected +1) so a lot faster than the usual NN wrapper (and results seem about the same, if not a little better).
* stopping criteria is unclear
* not clear if they're reporting test on train
* can expect zero MI if have gotten all important features
-- great for Kraskov MI b/c it's unbiased when there's zero MI
-- but does this help because you still must avoid overtraining somehow
-- rely on num. of hidden nodes selection somewhere?
* is there a Vapnik information capacity way to figure out when to stop?
* Kraskov multi-dim MI could allow you to make selections by peaking ahead by n-dim -1 steps, which would be closer to a true combinatoric search. Can test this. Could also test to see if that would make convergence faster, as the paper claims that this algorithm already provides the fastest possible convergence.},
  file      = {Schaffernicht09residMutInfoFeatSel.pdf:Schaffernicht09residMutInfoFeatSel.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2012.02.17},
  url       = {http://www.tu-ilmenau.de/fileadmin/media/neurob/publications/conferences_int/2009/Schaffernicht-ESANN-2009.pdf},
}

@Article{Rao11indepGenCorr,
  author    = {Rao, Murali and Seth, Sohan and Xu, Jianwu and Chen, Yunmei and Tagare, Hemant and Principe, Jose C.},
  title     = {A test of independence based on a generalized correlation function},
  journal   = {Signal Processing},
  year      = {2011},
  volume    = {91},
  number    = {1},
  pages     = {15--27},
  issn      = {0165-1684},
  abstract  = {In this paper, we propose a novel test of independence based on the concept of correntropy. We explore correntropy from a statistical perspective and discuss its properties in the context of testing independence. We introduce the novel concept of parametric correntropy and design a test of independence based on it. We further discuss how the proposed test relaxes the assumption of Gaussianity. Finally, we discuss some computational issues related to the proposed method and compare it with state-of-the-art techniques.},
  comment   = {like mutual information but more estimatable? scenario similarity measures, feature selection,....},
  file      = {Rao11indepGenCorr.pdf:Rao11indepGenCorr.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.12.02},
}

@InProceedings{Agarwal10UnivMDS,
  author       = {Agarwal, Arvind and Phillips, Jeff M and Venkatasubramanian, Suresh},
  title        = {Universal multi-dimensional scaling},
  booktitle    = {Proceedings of the 16\textsuperscript{th} ACM SIGKDD international conference on Knowledge discovery and data mining},
  year         = {2010},
  pages        = {1149--1158},
  organization = {ACM},
  abstract     = {In this paper, we propose a unified algorithmic framework for
solving many known variants of MDS. Our algorithm is a simple
iterative scheme with guaranteed convergence, and is modular;
by changing the internals of a single subroutine in the algorithm,
we can switch cost functions and target spaces easily. In addition
to the formal guarantees of convergence, our algorithms are
accurate; in most cases, they converge to better quality solutions
than existing methods in comparable time. Moreover, they have
a small memory footprint and scale effectively for large data
sets. We expect that this framework will be useful for a number
of MDS variants that have not yet been studied.
Our framework extends to embedding high-dimensional points
lying on a sphere to points on a lower dimensional sphere, preserving
geodesic distances. As a complement to this result, we
also extend the Johnson-Lindenstrauss Lemma to this spherical
setting, by showing that projecting to a random O((1="2) log n)-
dimensional sphere causes only an "-distortion in the geodesic
distances.
Categories and Subject Descriptors
H.2.8 [Database applications]: Data mining; F.2.2 [Non-numerical
algorithms and problems]: Geometrical algorithms
Keywords
Multi-dimensional scaling, dimensionality reduction.},
  comment      = {Low memory (in compute space, not in input space) iterative MDS for non-metric MDS. Said to be accurate. Could be good for MDS for huge dimensions, when you need a lot of training points ergo, high compute memory with conventional methods). I believe that Agarwal10IncrMDSoos used something like this in that out of sample paper (but the title of the referenced paper doesn't match this one).},
  file         = {Agarwal10UnivMDS.pdf:Agarwal10UnivMDS.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2014.06.27},
  url          = {http://dl.acm.org/citation.cfm?id=1835948},
}

@InProceedings{Xu16bndLogRgrssnImbal,
  author    = {G. Xu and B. G. Hu and J. C. Principe},
  title     = {Robust bounded logistic regression in the class imbalance problem},
  booktitle = {Proc. Int. Joint Conf. Neural Networks (IJCNN)},
  year      = {2016},
  pages     = {1434--1441},
  month     = jul,
  abstract  = {In this paper, we propose to deal with the problems of logistic regression with outliers and class imbalance, which are common in a wide range of practical applications. The robust bounded logistic regression with different error costs is developed to reduce the combined influence of outliers and class imbalance. First, inspired by the Correntropy induced loss function, we develop the bounded logistic loss function which is a monotonic, bounded and nonconvex loss and thus robust to outliers. With the bounded logistic loss, we construct a new robust logistic regression. Second, under the principle of cost-sensitive learning, we assign different error costs for different classes in order to reduce the sensitiveness of the new robust logistic regression to class imbalance. Using the half-quadratic optimization method, it is easy to optimize the proposed logistic regression model. Experimental results demonstrate that our proposed method improves the performance of logistic regression on the datasets with outliers and class imbalance.},
  comment   = {Handles class imbalance and cost-sensitive learning, which is what you want for fault detection problems.  Good for ModernWindABS?},
  doi       = {10.1109/IJCNN.2016.7727367},
  file      = {Xu16bndLogRgrssnImbal.pdf:Xu16bndLogRgrssnImbal.pdf:PDF},
  keywords  = {pattern classification, quadratic programming, regression analysis, Correntropy induced loss function, bounded logistic loss function, class imbalance problem, cost-sensitive learning, error costs, half-quadratic optimization, monotonic bounded nonconvex loss, robust bounded logistic regression, Blogs, Data preprocessing, Electronic mail, Logistics, Optimization methods, Robustness, Training},
  owner     = {sotterson},
  timestamp = {2017.01.17},
}

@Article{Riedel09poolMultiFrcstCombo,
  author    = {Riedel, S. and Gabrys, B.},
  title     = {Pooling for Combination of Multilevel Forecasts},
  journal   = {Knowledge and Data Engineering, IEEE Transactions on},
  year      = {2009},
  volume    = {21},
  number    = {12},
  pages     = {1753--1766},
  month     = dec,
  issn      = {1041-4347},
  abstract  = {In this paper, we provide a theoretical analysis of effects of applying different forecast diversification methods on the structure of the forecast error covariance matrices and decomposed forecast error components based on the bias-variance-Bayes error decomposition of James and Hastie. We express the "diversityrdquo of different forecasts in relation to different error components and propose a measure in order to quantify it. We illustrate and discuss typical inhomogeneities frequently occurring in the forecast error covariance matrices and show that previously proposed pooling based only on error variances cannot fully exploit the complementary information present in a set of diverse forecasts to be combined. If covariance values could be reliably calculated, they could be taken into account during the pooling process. We study the difficult case in which covariance information cannot be measured properly and propose a novel simplified representation of the covariance matrix, which is only based on knowledge about the forecast generation process. Finally, we propose a new pooling approach that avoids inhomogeneities in the forecast error covariance matrix by considering the information contained in the simplified covariance representation and compare it with the error-variance-based pooling approach introduced by Aiolfi and Timmermann. Applying our approach more than once leads to the generation of multistep and multilevel forecast combination structures, which have generated significantly improved forecasts in our previous extensive experimental work; the summary of which is also provided.},
  comment   = {Theoretical treatment for best forecast combo; simplified covariance matrix helps
* either ensemble learning or forecast aggregation applied in: Lemke09dynFrcstComboDiv},
  doi       = {10.1109/TKDE.2009.18},
  file      = {Riedel09poolMultiFrcstCombo.pdf:Riedel09poolMultiFrcstCombo.pdf:PDF},
  groups    = {Ensemble, Test, doReadNonWPV_1},
  keywords  = {bias-variance-Bayes error decomposition;forecast diversification methods;forecast error components;forecast error covariance matrices;multilevel forecasts;pooling process;Bayes methods;covariance matrices;forecasting theory;},
  owner     = {scot},
  timestamp = {2010.07.01},
}

@Article{Belloni13progEvalHiDim,
  author    = {Belloni, Alexandre and Chernozhukov, Victor and Fern{\'a}ndez-Val, Ivan and Hansen, Chris},
  title     = {Program evaluation with high-dimensional data},
  journal   = {arXiv preprint arXiv:1311.2645v6},
  year      = {2013},
  abstract  = {In this paper, we provide efficient estimators and honest confidence bands for a variety
of treatment effects including local average (LATE) and local quantile treatment effects (LQTE)
in data-rich environments. We can handle very many control variables, endogenous receipt of
treatment, heterogeneous treatment effects, and function-valued outcomes. Our framework covers
the special case of exogenous receipt of treatment, either conditional on controls or unconditionally
as in randomized control trials. In the latter case, our approach produces efficient estimators and
honest bands for (functional) average treatment effects (ATE) and quantile treatment effects (QTE).
To make informative inference possible, we assume that key reduced form predictive relationships
are approximately sparse. This assumption allows the use of regularization and selection methods to
estimate those relations, and we provide methods for post-regularization and post-selection inference
that are uniformly valid (honest) across a wide-range of models. We show that a key ingredient
enabling honest inference is the use of orthogonal or doubly robust moment conditions in estimating
certain reduced form functional parameters. We illustrate the use of the proposed methods with an
application to estimating the effect of 401(k) eligibility and participation on accumulated assets.
The results on program evaluation are obtained as a consequence of more general results on
honest inference in a general moment condition framework, which arises from structural equation
models in econometrics. Here too the crucial ingredient is the use of orthogonal moment conditions,
which can be constructed from the initial moment conditions. We provide results on honest inference
for (function-valued) parameters within this general framework where any high-quality, modern
machine learning methods can be used to learn the nonparametric/high-dimensional components
of the model. These include a number of supporting auxilliary results that are of major independent
interest: namely, we (1) prove uniform validity of a multiplier bootstrap, (2) offer a uniformly valid
functional delta method, and (3) provide results for sparsity-based estimation of regression functions
for function-valued outcomes.},
  comment   = {A high dimensional distribution paper (I think, it predicts a binary outcome, which is, for example 1 when income is less that the value u).  Does LASSO feature selection (use for ModernWindABS?) but warns against naiive treatment.  Does bootstrap or boost or something.  Handles more inut dims than has points, I think.

One problem is that it concerns the effect of a binary treatment; for ModernWindABS and other things, the the treatments are continous.  I'm not sure if this is easier or harder than binary....

Has both slides and paper.

I've got version 6 of this paper, from 2016.

See Cameron13instrVarEconCrsNotes for defintion of terms like "instrument"

FEATSEL AND INFERENCE
* One interesting point, though, is that he says it's very hard to do inference after have done feature selection (p. 3, how to do that is handled in this paper).
* But Belloni11sparseQRl1 doees it.
* So ModernWindABS LASSO could be tricky, as could my distribution regression feature selection.
* Ide09ProxAnomDetSprs also says that LASSO is tricky for fault diag.
},
  file      = {Paper, v6, 2016:Belloni13progEvalHiDim.pdf:PDF;Slides 2015:Belloni13progEvalHiDim_slides.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2016.12.17},
  url       = {https://arxiv.org/abs/1311.2645},
}

@InProceedings{Zhao10wvltUncTseries,
  author    = {Zhao, Yuchen and Aggarwal, Charu and Yu, Philip},
  title     = {On Wavelet Decomposition of Uncertain Time Series Data Sets},
  booktitle = {Proceedings of the 19\textsuperscript{th} ACM International Conference on Information and Knowledge Management},
  year      = {2010},
  series    = {CIKM '10},
  pages     = {129--138},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {In this paper, we will explore the construction of wavelet decompositions of uncertain data. Uncertain representations of data sets require significantly more space, and it is therefore even more important to construct compressed representations for such cases. We will use a hierarchical optimization technique in order to construct the most effective partitioning for our wavelet representation. We explore two different schemes which optimize the uncertainty in the resulting representation. We will show that the incorporation of uncertainty into the design of the wavelet representations significantly improves the compression rate of the representation. We present experimental results illustrating the effectiveness of our approach.
Categories and Subject Descriptors
H.2.8 [Information Systems]: Database Applications},
  acmid     = {1871458},
  comment   = {Use for probabilistic upscaling? How does this relate to the LNEG wavelet baed upscaling? TODO get LNEG paper.

See also: Lave13solarVarWvlt and Cormode10histoWvltProb},
  doi       = {10.1145/1871437.1871458},
  file      = {Zhao10wvltUncTseries.pdf:Zhao10wvltUncTseries.pdf:PDF},
  groups    = {Upscaling (prob), doReadNonWPV_1},
  isbn      = {978-1-4503-0099-5},
  keywords  = {time series, uncertain data, wavelets},
  location  = {Toronto, ON, Canada},
  numpages  = {10},
  owner     = {sotterson},
  timestamp = {2014.03.21},
}

@Article{Venkatasubramanian03rvwProcFaultDet_IIsearch,
  author    = {Venkat Venkatasubramanian and Raghunathan Rengaswamy and Surya N Kavuri},
  title     = {A review of process fault detection and diagnosis: Part II: Qualitative models and search strategies},
  journal   = {Computers \& Chemical Engineering},
  year      = {2003},
  volume    = {27},
  number    = {3},
  pages     = {313 - 326},
  issn      = {0098-1354},
  abstract  = {In this part of the paper, we review qualitative model representations and search strategies used in fault diagnostic systems. Qualitative models are usually developed based on some fundamental understanding of the physics and chemistry of the process. Various forms of qualitative models such as causal models and abstraction hierarchies are discussed. The relative advantages and disadvantages of these representations are highlighted. In terms of search strategies, we broadly classify them as topographic and symptomatic search techniques. Topographic searches perform malfunction analysis using a template of normal operation, whereas, symptomatic searches look for symptoms to direct the search to the fault location. Various forms of topographic and symptomatic search strategies are discussed.},
  doi       = {http://dx.doi.org/10.1016/S0098-1354(02)00161-8},
  keywords  = {Symptomatic search, Topographic search},
  owner     = {sotterson},
  timestamp = {2017.06.17},
  url       = {http://www.sciencedirect.com/science/article/pii/S0098135402001618},
}

@TechReport{Botterud11windPowFrcstOpUse,
  author      = {Audun Botterud and Zhi Zhou and Jianhui Wang and Ricardo J. Bessa and Hnroje Keko and Joana Mendes and Jean Sumaili and Vladimiro Miranda},
  title       = {Use of Wind Power Forecasting in Operational Decisions},
  institution = {Argonne National Laboratories},
  year        = {2011},
  number      = {ANL/DIS-11-8},
  month       = sep,
  abstract    = {In this project, we have also developed and successfully tested several methodologies and modeling tools for the use of wind power forecasting in operational decisions, from the perspectives of the system operator as well as the wind power producers. We have investigated how the different objectives of system operators and market participants may lead to different opinions on what constitutes a good wind power forecast, which in turn may influence the training criteria used for wind power point forecasting. We have also focused on the use probabilistic wind power forecasts in electricity markets. We have investigated the representation of wind power forecasting uncertainty in the unit commitment problem. Traditional deterministic unit commitment models use a point forecast for wind power output. In contrast, we have developed a stochastic alternative that represents forecast uncertainty by using scenarios that capture cross-temporal dependencies in the predicted wind power. Furthermore, we have investigated the use of probabilistic wind power forecasts to estimate dynamic operating reserve requirements. We have tested the new algorithms on several case studies on a small-scale hypothetical power system as well as on realistic data for the power system of Illinois. A comparison of a diversity of unit commitment and operating reserve strategies illustrate the potential advantages of using probabilistic forecasts in the scheduling of energy and operating reserves compared to the traditional deterministic approach. We have also developed a model for optimal trading of wind power under uncertainty in wind power and prices. The model has been tested on several case studies on both hypothetical and real-world data. The results show that the model can control the trade-offs between risk and return for wind farm owners depending on their risk preferences. We have also used the model to investigate how market design, in the form of potential deviation penalties, influences optimal bidding decisions, system imbalances, and wind power?s profitability.},
  comment     = {Illinois test case (simulation w/ real data inputs) show benefits of stochastic optimization

Journal paper: Botterud13dispUCprobWndFrcst

Another Journal paper may be: Wang11windFrcstUncertUC},
  file        = {Botterud11windPowFrcstOpUse.pdf:Botterud11windPowFrcstOpUse.pdf:PDF},
  groups      = {Use, doReadWPV_2},
  location    = {Oak Ridge, TN, USA},
  owner       = {sotterson},
  timestamp   = {2013.10.01},
  url         = {http://www.dis.anl.gov/projects/windpowerforecasting.html},
}

@TechReport{Ahmad05multiWvltFrcst,
  author      = {Ahmad, Saif and Popoola, Ademola and Ahmad, Khurshid},
  title       = {Wavelet-based multiresolution forecasting},
  institution = {Department of Computing, University of Surrey},
  year        = {2005},
  month       = jun,
  abstract    = {In this report, we discuss results of modelling and forecasting nonstationary financial time series using a combination of the maximal overlap discreet wavelet transform (MODWT) and fuzzy logic. A financial time series is decomposed into an over complete, shift invariant wavelet representation. A fuzzy-rule base is created for each individual wavelet sub-series to predict future values. To form the aggregate forecast, the individual wavelet sub-series forecasts are recombined utilizing the linear reconstruction property of the wavelet multiresolution analysis (MRA). Results are presented for IBM, NASDAQ and S\&P 500 daily (adjusted) close values.},
  comment     = {Friendly review explaining MODWT and "a trous" processing needed for forecasting. Seems to be largely a copy of: Zhang01multiWvltFrcst},
  file        = {Ahmad05multiWvltFrcst.pdf:Ahmad05multiWvltFrcst.pdf:PDF},
  journal     = {University of Surrey, Technical Report},
  owner       = {sotterson},
  timestamp   = {2013.03.15},
  url         = {http://saifahmad.com/unis_report.pdf},
}

@Article{LazaroGredilla12ovMixGPassoc,
  author    = {Miguel L{\'a}zaro-Gredilla and Steven Van Vaerenbergh and Neil D. Lawrence},
  title     = {Overlapping Mixtures of {Gauss}ian Processes for the data association problem},
  journal   = {Pattern Recognition},
  year      = {2012},
  volume    = {45},
  number    = {4},
  pages     = {1386--1395},
  issn      = {0031-3203},
  abstract  = {In this work we introduce a mixture of \{GPs\} to address the data association problem, i.e., to label a group of observations according to the sources that generated them. Unlike several previously proposed \{GP\} mixtures, the novel mixture has the distinct characteristic of using no gating function to determine the association of samples and mixture components. Instead, all the \{GPs\} in the mixture are global and samples are clustered following ?trajectories? across input space. We use a non-standard variational Bayesian algorithm to efficiently recover sample labels and learn the hyperparameters. We show how multi-object tracking problems can be disambiguated and also explore the characteristics of the model in traditional regression settings.},
  comment   = {Kind of like scenario tree reduction, in that a combinatorial explosion of trajectories is avoided.},
  doi       = {10.1016/j.patcog.2011.10.004},
  file      = {LazaroGredilla12ovMixGPassoc.pdf:LazaroGredilla12ovMixGPassoc.pdf:PDF},
  groups    = {doReadNonWPV_2},
  keywords  = {Gaussian Processes},
  owner     = {sotterson},
  timestamp = {2013.10.08},
}

@Article{Ahmed10empCompForecast,
  author    = {Ahmed, N.K. and Atiya, A.F. and El Gayar, N. and El-Shishiny, H.},
  title     = {An empirical comparison of machine learning models for time series forecasting},
  journal   = {Econometric Reviews},
  year      = {2010},
  volume    = {29},
  number    = {5},
  pages     = {594--621},
  abstract  = {In this work we present a large scale comparison study for the major machine
learning models for time series forecasting. Specifically, we apply the models on
the monthly M3 time series competition data (around a thousand time series).
There have been very few, if any, large scale comparison studies for machine
learning models for the regression or the time series forecasting problems, so we
hope this study would fill this gap. The models considered are multilayer per-
ceptron, Bayesian neural networks, radial basis functions, generalized regression
neural networks (also called kernel regression), K-nearest neighbor regression,
CART regression trees, support vector regression, and Gaussian processes. The
study reveals significant differences between the different methods. The best
two methods turned out to be the multilayer perceptron and the Gaussian pro-
cess regression. In addition to model comparisons, we have tested different
preprocessing methods and have shown that they have different impacts on the
performance.},
  comment   = {MLP's and Gaussian processes come out on top for business forecasting

* This review of mine comes from a 2007 draft (I think) of this paper, which was listed as submitted to Economics Review.
* considers only one-step ahead forecast
* cautions that may not work as well for different series e.g. from physics
* MLP superiority may be from ability to become a linear regressor

* Gaussians processes have real probabilistic model ==> better for classifier combo?
* Result of paper: these guys got 6\textsuperscript{th} place out of 26 and 5 out of 44 in the NN3 competition

Feature Transforms (for all regressor types)
* Preprocessing -- just lagged inputs (window width)
-- 1\textsuperscript{st} difference (diff is only input)
---- they say that diff isn't good for non-stationary processes; good for stationary ones
---- HUH, Hsu08SlassoVARsubset uses 1\textsuperscript{st} difference to remove stationarity, seems to like it
---- but Bisgaard papers say to do it
-- moving average (window width, etc. chosen by cross-validation)

* transforms (applied to each of the preprocessing sets above)
-- logged data
-- deseasonalization when test detects seasonality (look up ref's)
-- scale to +-1
* note other possible transforms in:
Granger76frcstXfrmSeries
Fink09xformFAQs
Bremnes06compQuantileWind
Simonoff09transfRegrsn

* Generalized Regression Neural Network (GRNN) is the Nadaraya-Watson used (I think) for quantile regression, and found to be good, in Bremnes06compQuantileWind.},
  doi       = {10.1080/07474938.2010.481556},
  file      = {2007 Draft (the one I reviewed):Ahmed07empCompForecast.pdf:PDF;2010 final version:Ahmed10empCompForecast.pdf:PDF},
  owner     = {scotto},
  publisher = {Citeseer},
  timestamp = {2011.11.07},
}

@InBook{Diaz15DeepNNwindEnrgyPred,
  pages     = {430--443},
  title     = {Deep Neural Networks for Wind Energy Prediction},
  publisher = {Springer International Publishing},
  year      = {2015},
  author    = {D{\'i}az, David and Torres, Alberto and Dorronsoro, Jos{\'e} R.},
  editor    = {Rojas, Ignacio and Joya, Gonzalo and Catala, Andreu},
  address   = {Cham},
  isbn      = {978-3-319-19258-1},
  abstract  = {In this work we will apply some of the Deep Learning models that are currently obtaining state of the art results in several machine learning problems to the prediction of wind energy production. In particular, we will consider both deep, fully connected multilayer perceptrons with appropriate weight initialization, and also convolutional neural networks that can take advantage of the spatial and feature structure of the numerical weather prediction patterns. We will also explore the effects of regularization techniques such as dropout or weight decay and consider how to select the final predictive deep models after analyzing their training evolution.},
  booktitle = {Advances in Computational Intelligence: 13th International Work-Conference on Artificial Neural Networks, IWANN 2015, Palma de Mallorca, Spain, June 10-12, 2015. Proceedings, Part I},
  comment   = {I couldn't get paper but could get the paper for DiazVico17deepNNwindSolEnrgyPred

Also see: DiazVico17deepNNwindSolEnrgyPred},
  doi       = {10.1007/978-3-319-19258-1_36},
  url       = {https://doi.org/10.1007/978-3-319-19258-1_36},
}

@Article{Vergara14revFeatSelMutInfo,
  author    = {Vergara, Jorge R and Est{\'e}vez, Pablo A},
  title     = {A review of feature selection methods based on mutual information},
  journal   = {Neural Computing and Applications},
  year      = {2014},
  volume    = {24},
  number    = {1},
  pages     = {175--186},
  abstract  = {In this work, we present a review of the state of the art of information-theoretic feature selection methods. The concepts of feature relevance, redundance, and complementarity (synergy) are clearly defined, as well as Markov blanket. The problem of optimal feature selection is defined. A unifying theoretical framework is described, which can retrofit successful heuristic criteria, indicating the approximations made by each method. A number of open problems in the field are presented.
Keywords
Feature selection Mutual information Relevance Redundancy Complementarity Sinergy Markov blanket},
  comment   = {Covers a few more one-to-one relationship MI feat sel algs (in addition to Brown12condLikInfoFeatSel) but doesn't cover much about high dim featsel.  Has recommendatinos at end for what needs to be improved.

* May11revVarSelNN also metions partial info featsel
* says that Brown12condLikInfoFeatSel didn't explain CMI and others (but this CMI is not the high dim MI in Frenzel07partMutInfo
* has only one or two high dim MI techniques.  Doesn't have e.g. Frenzel07partMutInfo and followers.},
  date      = {January},
  doi       = {10.1007/s00521-013-1368-0},
  file      = {paper:Vergara14revFeatSelMutInfo.pdf:PDF},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2016.06.18},
}

@InProceedings{Darudi13partMutInfoFeatSel,
  author    = {A. Darudi and S. Rezaeifar and Mohammd Hossein Javidi Dasht Bayaz},
  title     = {Partial mutual information based algorithm for input variable selection for time series forecasting},
  booktitle = {Proc. 13th Int Environment and Electrical Engineering (EEEIC) Conf},
  year      = {2013},
  pages     = {313--318},
  month     = nov,
  abstract  = {In time series forecasting, it is a crucial step to identify proper set of variables as the inputs to the model. Many input variable selection (IVS) techniques fail to perform suitably due to inherent assumption of linearity or rich redundancy between variables. The motivation behind this research is to propose an input variable selection algorithm which not only can handle nonlinear problems and redundant data, but also is straightforward and easy-to-implement. In the field of information theory, partial mutual information is a reliable measure to evaluate linear/nonlinear dependency and redundancy among variables. In this paper, we propose an IVS algorithm based on partial mutual information. The algorithm is tested on three time series with known dependence attributes. Results confirm credibility of the proposed method to capture linear/non-linear dependence and redundancy between variables.},
  comment   = {A hacked version of PMI kind of like mRMR in Peng* which allows bivariate pairwise MI/PMI estimators -- calls it PMI-mRMR.  May11revVarSelNN also does PMI w/ bivariate MI estimator.  Looks like a similar algorithm but maybe with a hack. I am not sure if this is a great paper.

Fernando05inputSelNNpmi may have a similar hack.},
  doi       = {10.1109/EEEIC-2.2013.6737928},
  file      = {Darudi13partMutInfoFeatSel.pdf:Darudi13partMutInfoFeatSel.pdf:PDF},
  keywords  = {forecasting theory, probability, redundancy, time series, IVS algorithm, credibility, information theory, input variable selection algorithm, nonlinear dependency evaluation, nonlinear problem handling, partial mutual information based algorithm, redundancy, time series forecasting, Algorithm design and analysis, Computational modeling, Forecasting, Input variables, Mutual information, Redundancy, Time series analysis, information theory, input variable selection, partial mutual information, time series forecasting},
}

@Article{Moons15selfBrandPersEV,
  author   = {Moons, Ingrid and De Pelsmacker, Patrick},
  title    = {Self-Brand Personality Differences and Attitudes towards Electric Cars},
  journal  = {Sustainability},
  year     = {2015},
  volume   = {7},
  number   = {9},
  pages    = {12322--12339},
  issn     = {2071-1050},
  abstract = {In two representative Belgian samples, by means of an online survey, we investigate the effect of self-brand personality differences on car brand evaluation, the evaluation of an eco-friendly branded electric car extension and the evaluation of car brands after electric extension. We show that self-brand personality differences influence the attitude towards car brands. The relative importance of personality dimensions that drive extension judgment and parent brand attitudes after electric extension is different from that of brand evaluation without extension. More particularly, perceptions of a brand being more responsible than one’s self is a much more important driver of brand evaluation after electric extension than without extension. Car personality characteristics, such as activity and sophistication, drive brand evaluations before, as well as after electric extension.  These effects are moderated by brand ownership in that the relative importance of brand personality dimensions is different for brand owners than for consumers who do not own a specific brand. Car manufacturers can fine-tune their marketing approach when launching eco-friendly extensions, taking into account that, in this context, partly different  self-brand personality fit considerations are used by consumers than for car brands without electric extension.},
  doi      = {10.3390/su70912322},
  file     = {:Moons15selfBrandPersEV.pdf:PDF},
  url      = {http://www.mdpi.com/2071-1050/7/9/12322},
}

@Article{Lerch13CmprProbWndFrcstNonHomogGR,
  author     = {Lerch, Sebastian and Thorarinsdottir, Thordis L.},
  title      = {Comparison of nonhomogeneous regression models for probabilistic wind speed forecasting},
  journal    = {Tellus A},
  volume     = {65},
  number     = {0},
  abstract   = {In weather forecasting, nonhomogeneous regression is used to statistically postprocess forecast ensembles in order to obtain calibrated predictive distributions. For wind speed forecasts, the regression model is given by a truncated normal distribution where location and spread are derived from the ensemble. This paper proposes two alternative approaches which utilize the generalized extreme value ({GEV}) distribution. A direct alternative to the truncated normal regression is to apply a predictive distribution from the {GEV} family, while a regime switching approach based on the median of the forecast ensemble incorporates both distributions. In a case study on daily maximum wind speed over Germany with the forecast ensemble from the European Centre for Medium-Range Weather Forecasts, all three approaches provide calibrated and sharp predictive distributions with the regime switching approach showing the highest skill in the upper tail.},
  comment    = {extreme value, switching regression ensembles.  Example of non-homogenous Gaussian Regression.},
  date       = {2013-11-14},
  doi        = {10.3402/tellusa.v65i0.21206},
  eprint     = {1305.2026},
  eprinttype = {arxiv},
  file       = {Lerch13CmprProbWndFrcstNonHomogGR.pdf:Lerch13CmprProbWndFrcstNonHomogGR.pdf:PDF},
  keywords   = {Statistics - Applications},
  owner      = {sotterson},
  timestamp  = {2016.11.10},
  url        = {http://arxiv.org/abs/1305.2026},
  urldate    = {2016-11-10},
}

@InProceedings{Kemper10modFrcstErr,
  author    = {Kemper, Jason J and Bielecki, Mark F and Acker, Thomas L},
  title     = {Modeling of Wind Power Production Forecast Errors for Wind Integration Studies},
  booktitle = {Proc. Of ASME 2010 4\textsuperscript{th} International Conference on Energy Sustainability},
  year      = {2010},
  abstract  = {In wind integration studies, accurate representations of the wind power output from potential wind power plants and corresponding representations of wind power forecasts are needed, and typically used in a production cost simulation. Two methods for generating 'synthetic' wind power forecasts that capture the statistical trends and characteristics found in commercial forecasting techniques are presented. These two methods are based on auto-regressive moving average (ARMA) models and the Markov random walk method. Statistical criteria are suggested for evaluation of wind power forecast performance, and both synthetic forecast methods proposed are evaluated quantitatively and qualitatively. The forecast performance is then compared with a commercial forecast used for an operational wind power plant in the Northwestern United States evaluated using the same statistical performance measures. These quantitative evaluation parameters are monitored during specific months of the year, during rapid ramping events, and at all times. The best ARMA based models failed to replicate the auto-regressive decay of forecast errors associated with commercial forecasts. A modification to the Markov method, consisting of adding a dimension to the state transition array, allowed the forecast time series to depend on multiple inputs. This improvement lowered the artificial variability in the original time series. The overall performance of this method was better than for the ARMA based models, and provides a suitable technique for use in creating a synthetic wind forecast for a wind integration study.},
  comment   = {Has ramp identification. Time dependence of markov forecast error modeled by adding a dimension to the state transition matrix.},
  file      = {Kemper10modFrcstErr.pdf:Kemper10modFrcstErr.pdf:PDF},
  groups    = {ErrDistProps, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2013.02.18},
  url       = {http://nau.edu/uploadedFiles/Academic/CEFNS/Centers-Institutes/Folder_Templates/_Media/Modeling-Of-Wind-Power-Production-Forecast-Errors-For-Wind-Integration-Studies.pdf},
}

@Article{Keane07connPolicyDistGen,
  author    = {Keane, A. and Denny, E. and O'Malley, M.},
  title     = {Quantifying the Impact of Connection Policy on Distributed Generation},
  journal   = {Energy Conversion, IEEE Transactions on},
  year      = {2007},
  volume    = {22},
  number    = {1},
  pages     = {189--196},
  month     = mar,
  issn      = {0885-8969},
  abstract  = {Increasing connections for distributed generation (DG), and in particular, for wind generation, are being sought in power systems across the world. These increased applications present a significant challenge to the existing connection policies of distribution network operators. In particular, nonfirm access to the network has been proposed as a method to increase the penetration of DG. The impact of the connection policies arising from nonfirm access are investigated in detail here. The Irish system is used as a case study, and with reference to the available energy resource and network parameters, the costs and benefits of DG are determined under a number of planning policies. The costs and benefits assessed include connection and cycling costs along with emissions, capacity value, and fuel bill saving. It is shown that a significant increase in the net benefits of DG is gained if the appropriate connection policy is utilized from the outset, and conversely, significant costs are incurred if ad hoc policies are employed. Furthermore, it is shown that nonfirm access has the scope to facilitate a significant extra amount of DG capacity},
  comment   = {Greater benefit to wind farms if buy non-firm transmission (says Lloyd Cibulka)
* sides also in files},
  doi       = {10.1109/TEC.2006.889618},
  file      = {Keane07connPolicyDistGen.pdf:Keane07connPolicyDistGen.pdf:PDF;:Keane07connPolicyDistGen-Slides.pdf:PDF;Keane07connPolicyDistGen.pdf:Keane07connPolicyDistGen.pdf:PDF},
  keywords  = {distributed power generation, power generation planning, wind power plantsIrish system, ad hoc policy, cycling costs, distributed generation connection policy, fuel bill saving, planning policy, wind generation},
  owner     = {sotterson},
  timestamp = {2009.05.27},
}

@Article{Lu11multiLinSubspaceLrnSurvey,
  author    = {Lu, Haiping and Plataniotis, Konstantinos N and Venetsanopoulos, Anastasios N},
  title     = {A survey of multilinear subspace learning for tensor data},
  journal   = {Pattern Recognition},
  year      = {2011},
  volume    = {44},
  number    = {7},
  pages     = {1540--1551},
  abstract  = {Increasingly large amount of multidimensional data are being generated on a daily
basis in many applications. This leads to a strong demand for learning algorithms
to extract useful information from these massive data. This paper surveys the eld
of multilinear subspace learning (MSL) for dimensionality reduction of multidimensional
data directly from their tensorial representations. It discusses the central
issues of MSL, including establishing the foundations of the eld via multilinear
projections, formulating a unifying MSL framework for systematic treatment of the
problem, examining the algorithmic aspects of typical MSL solutions, and categorizing
both unsupervised and supervised MSL algorithms into taxonomies. Lastly,
the paper summarizes a wide range of MSL applications and concludes with perspectives
on future research directions.
Key words: Subspace learning, dimensionality reduction, feature extraction,
multidimensional data, tensor, multilinear, survey, taxonomy},
  comment   = {Maybe another quick survey of tensor learning. Author has a book (Lu13MultilinSubspaceLearnBook) and a couple papers too on multilinear PCA.

Multilinear pca is sped up here: Althoff12OnlnTnsrFactPCA},
  file      = {Lu11multiLinSubspaceLrnSurvey.pdf:Lu11multiLinSubspaceLrnSurvey.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2015.02.09},
  url       = {http://www.sciencedirect.com/science/article/pii/S0031320311000136},
}

@Article{Luo08mixLasso,
  author    = {Luo, Ronghua and Tsai , Chih-Ling and Wang, Hansheng},
  title     = {On Mixture Regression Shrinkage and Selection Via the {MR-Lasso}},
  journal   = {International Journal of Pure and Applied Mathematics},
  year      = {2008},
  volume    = {46},
  pages     = {403--414},
  abstract  = {Infnite mixture regression models, we generalize the application of the least absolute shrinkage and selection operator (LASSO) to obtain MR-Lasso, which incorporates both mixture and regression penalties. Because MR-Lasso jointly penalizes both regression coeffcients and mixture components, it enables simul-taneous identifcation of signifcant variables and determination of important mixture components. Simulation studies indicate that MR-Lasso outperforms LASSO. Extensions to mixture non-Gaussian and mixture time series models are briefly described.},
  comment   = {Combined feature and mixture estimation (lags too, I think). Use for regime clustering?},
  file      = {Luo08mixLasso.pdf:Luo08mixLasso.pdf:PDF;Luo08mixLasso.pdf:Luo08mixLasso.pdf:PDF},
  keywords  = {finite mixture model, LASSO, mixture penalty},
  language  = {English},
  location  = {http://ssrn.com/paper=1308327},
  owner     = {sotterson},
  publisher = {SSRN},
  timestamp = {2009.08.17},
  type      = {Accepted Paper Series},
}

@Article{Rossetti17frcstAdoptEarlyData,
  author    = {Rossetti, Giulio and Milli, Letizia and Giannotti, Fosca and Pedreschi, Dino},
  title     = {Forecasting success via early adoptions analysis: A data-driven study},
  journal   = {PloS one},
  year      = {2017},
  volume    = {12},
  number    = {12},
  pages     = {e0189096},
  abstract  = {Innovations are continuously launched over markets, such as new products over the retail market or new artists over the music scene. Some innovations become a success; others don’t. Forecasting which innovations will succeed at the beginning of their lifecycle is hard. In this paper, we provide a data-driven, large-scale account of the existence of a special niche among early adopters, individuals that consistently tend to adopt successful innovations before they reach success: we will call them Hit-Savvy. Hit-Savvy can be discovered in very different markets and retain over time their ability to anticipate the success of innovations. As our second contribution, we devise a predictive analytical process, exploiting Hit-Savvy as signals, which achieves high accuracy in the early-stage prediction of successful innovations, far beyond the reach of state-of-the-art time series forecasting models. Indeed, our findings and predictive model can be fruitfully used to support marketing strategies and product placement.},
  comment   = {Seems like WattPlan Grid in that it examines how good these forecasts are using early adoption data.  DAta is distributed with the paper on the paper homepage URL.},
  doi       = {0.1371/journal.pone.0189096},
  file      = {:Rossetti17frcstAdoptEarlyData.pdf:PDF},
  publisher = {Public Library of Science},
  url       = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5720712/},
}

@PhdThesis{Nassaj10modelLinNonLinPhD,
  author      = {Feras Nassaj},
  title       = {Modeling Linearly and non-Linearly Dependent Simulation Input Data},
  year        = {2010},
  abstract    = {Input modeling software tries to fit standard probability distributions to data assuming that the data are independent. However, the input environment can generate correlated data. Ignoring the correlations might lead to serious inaccuracies in the performance measures. In the past few years, several dependence modeling packages with different properties have been developed. In our dissertation, we explain how to fit non-Gaussian autoregressive models to correlated data and compare our approach with similar dependence modeling approaches that already exist. Moreover, we extend the Yule-Walker method so as to fit non-linear models to data samples using this method. We use in our dissertation also copulas for the purpose of fitting models to data samples. Copulas are used in finance and insurance for modeling stochastic dependency. Copulas comprehend the entire dependence structure, not only the linear correlations. In our dissertation, copulas serve the purpose to analyze measured samples of random vectors and time series, to estimate a multivariate distribution for them, and to generate random vectors with this distribution.},
  comment     = {spinning reserves, overview of scenarios},
  file        = {Nassaj10modelLinNonLinPhD.pdf:Nassaj10modelLinNonLinPhD.pdf:PDF},
  institution = {Universitat Bonn},
  owner       = {scot},
  timestamp   = {2010.11.24},
  url         = {http://hss.ulb.uni-bonn.de/2010/2233/2233.htm},
}

@Article{Nassaj05nonGausARgenetic,
  author    = {Nassaj, F. and Strelen, J.C.},
  title     = {Dependence input modeling with the help of non-{Gauss}ian {AR} models and genetic algorithms},
  journal   = {Modelling and Simulation},
  year      = {2005},
  pages     = {146--153},
  abstract  = {Input modeling software tries to fit standard probability distributions to data assuming that the data are independent. However, the input environment can generate correlated data. Ignoring the correlations might lead to serious inaccuracy in the performance measures. In the past few years, several dependence modeling packages with different properties have been developed. In this paper, we explain how to fit non-Gaussian autoregressive models to correlated data and compare our approach with similar dependence modeling approaches that already exist.},
  comment   = {spinning reserves. Nonlinear varta, w/ genetic algorithm fitting.
Do I care? DO I have to pick the polynomial oders, etc.?},
  file      = {Nassaj05nonGausARgenetic.pdf:Nassaj05nonGausARgenetic.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.11.24},
}

@Article{Huang06inputFinSimBtstrp,
  author    = {Huang, H and Willemain, T R},
  title     = {Input modelling for financial simulations using the bootstrap},
  journal   = {Journal of Simulation},
  year      = {2006},
  volume    = {1},
  pages     = {39--52},
  abstract  = {Inputs are the fuel that powers simulations. If one is designing a new system or product, inputs are scenarios representing the conditions with which the new system must cope. The need for large numbers of realistic scenarios applies to every domain in which simulation is used as part of the system design process. To be useful, scenarios should mimic the underlying data generating process by reflecting the auto- and cross-correlations of the historical data. We describe a new scenario generation procedure based on the nearest-neighbour bootstrap. We also propose a new performance evaluation criterion for multivariate time-series scenario generators based on the distribution of a composite correlation discrepancy measure. We illustrate the new method and measure by generating simulated scenarios for the US Treasury yield curve.},
  comment   = {spinning reserves, scenarios. I think this generates multivariate time series by bootstrapping. Also, there's a new figure of merit for cross-dim scenario evaluation.},
  doi       = {10.1057/palgrave.jos.4250007},
  groups    = {Test, doReadNonWPV_2},
  owner     = {scot},
  timestamp = {2010.12.01},
}

@Article{Wibral15BitsBrainsTE,
  author   = {Wibral, Michael and Lizier, Joseph T. and Priesemann, Viola},
  title    = {Bits from Brains for Biologically-Inspired Computing},
  journal  = {Frontiers in Robotics and AI},
  year     = {2015},
  volume   = {2},
  number   = {5},
  issn     = {2296-9144},
  abstract = {Inspiration for artificial biologically inspired computing is often drawn from neural systems. This article shows how to analyze neural systems using information theory with the aim of obtaining constraints that help to identify the algorithms run by neural systems and the information they represent. Algorithms and representations identified this way may then guide the design of biologically inspired computing systems. The material covered includes the necessary introduction to information theory and to the estimation of information-theoretic quantities from neural recordings. We then show how to analyze the information encoded in a system about its environment, and also discuss recent methodological developments on the question of how much information each agent carries about the environment either uniquely or redundantly or synergistically together with others. Last, we introduce the framework of local information dynamics, where information processing is partitioned into component processes of information storage, transfer, and modification ??? locally in space and time. We close by discussing example applications of these measures to neural data and other complex systems.},
  comment  = {Intro to Transfer Entropy, could be useful for picking autoregressive NN features and their lags, all at the same time.

Transfer Entropy
It's like R^2 for an information theoretic ARX filter: It's information about the output contained in its history and the  history of other external conditioning variables.

Recommended by the author here, as an intro to Transfer Entropy
https://groups.google.com/forum/\#!topic/jidt-discuss/PU3cZsqkP2U

Also said that Lizier12compNetsTransEn had more details
* explains shortcomings of greedy search, has some (obvious) fixes},
  doi      = {10.3389/frobt.2015.00005},
  file     = {Wibral15BitsBrainsTE.pdf:Wibral15BitsBrainsTE.pdf:PDF},
}

@InCollection{AlStouhi11AdaptBoostXfrLrn,
  author    = {Al-Stouhi, Samir and Reddy, Chandan K},
  title     = {Adaptive boosting for transfer learning using dynamic updates},
  booktitle = {Machine Learning and Knowledge Discovery in Databases},
  publisher = {Springer},
  year      = {2011},
  pages     = {60--75},
  abstract  = {Instance-based transfer learning methods utilize labeled ex-
amples from one domain to improve learning performance in another
domain via knowledge transfer. Boosting-based transfer learning algo-
rithms are a subset of such methods and have been applied successfully
within the transfer learning community. In this paper, we address some of
the weaknesses of such algorithms and extend the most popular transfer
boosting algorithm, TrAdaBoost. We incorporate a dynamic factor into
TrAdaBoost to make it meet its intended design of incorporating the ad-
vantages of both AdaBoost and the \Weighted Majority Algorithm". We
theoretically and empirically analyze the eect of this important factor
on the boosting performance of TrAdaBoost and we apply it as a \cor-
rection factor" that signicantly improves the classication performance.
Our experimental results on several real-world datasets demonstrate the
eectiveness of our framework in obtaining better classication results.
Keywords: Transfer learning, AdaBoost, TrAdaBoost, Weighted Ma-
jority Algorithm},
  comment   = {General adaptation, or a way to handle changing NWP models.},
  doi       = {10.1007/978-3-642-23780-5_14},
  file      = {AlStouhi11AdaptBoostXfrLrn.pdf:AlStouhi11AdaptBoostXfrLrn.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.07.24},
}

@InCollection{Lieu11ensembClassLowDimDist,
  author      = {Lieu, Linh and Saito, Naoki},
  title       = {Signal Ensemble Classification Using Low-Dimensional Embeddings and Earth Mover's Distance},
  booktitle   = {Wavelets and Multiscale Analysis},
  publisher   = {Birkh{\"a}user Boston},
  year        = {2011},
  editor      = {Cohen, Jonathan and Zayed, Ahmed I.},
  series      = {Applied and Numerical Harmonic Analysis},
  pages       = {227--256},
  isbn        = {978-0-8176-8095-4},
  abstract    = {Instead of classifying individual signals, we address classification of objects characterized by signal ensembles (i.e., collections of signals). Such necessity arises frequently in real situations: e.g., classification of video clips or object classification using acoustic scattering experiments to name a few. In particular, we propose an algorithm for classifying signal ensembles by bringing together well-known techniques from various disciplines in a novel way. Our algorithm first performs the dimensionality reduction on training ensembles using either the linear embeddings (e.g., Principal Component Analysis (PCA), Multidimensional Scaling (MDS)) or the nonlinear embeddings (e.g., the Laplacian eigenmap (LE), the diffusion map (DM)). After embedding training ensembles into a lower-dimensional space, our algorithm extends a given test ensemble into the trained embedding space, and then measures the distance between the test ensemble and each training ensemble in that space, and classify it using the nearest neighbor method. It turns out that the choice of this ensemble distance measure is critical, and our algorithm adopts the so-called Earth Mover's Distance (EMD), a robust distance measure successfully used in image retrieval and image registration. We will demonstrate the performance of our algorithm using two real examples: classification of underwater objects using multiple sonar waveforms; and classification of video clips of digit-speaking lips. This article also provides a concise review on the several key concepts in statistical learning such as PCA, MDS, LE, DM, and EMD as well as the practical issues including how to tune parameters, which will be useful for the readers interested in numerical experiments.},
  affiliation = {Department of Mathematics, University of California, One Shields Avenue, Davis, CA 95616, USA},
  comment     = {Use for regime clustering and dimension reduction. Could use for analog ensemble system},
  doi         = {10.1007/978-0-8176-8095-4_11},
  file        = {Lieu11ensembClassLowDimDist.pdf:Lieu11ensembClassLowDimDist.pdf:PDF},
  groups      = {Ensemble, doReadNonWPV_1},
  keyword     = {Mathematics},
  owner       = {sotterson},
  timestamp   = {2011.11.11},
}

@INPROCEEDINGS{Zhang13RobustoptPowFlwVaR,
  Author                   = {Yu Zhang and Giannakis, G.B.},
  Title                    = {Robust optimal power flow with wind integration using conditional value-at-risk},
  Booktitle                = {Smart Grid Communications (SmartGridComm), 2013 IEEE International Conference on},
  Year                     = {2013},
  Pages                    = {654--659},
  Abstract                 = {Integrating renewable energy into the power grid requires intelligent risk-aware dispatch accounting for the stochastic availability of renewables. Toward achieving this goal, a robust DC optimal flow problem is developed in the present paper for power systems with a high penetration of wind energy. The optimal dispatch is obtained as the solution to a convex program with a suitable regularizer, which is able to mitigate the potentially high risk of inadequate wind power. The regularizer is constructed based on the energy transaction cost using conditional value-at-risk (CVaR). Bypassing the prohibitive high-dimensional integral, the distribution-free sample average approximation method is efficiently utilized for solving the resulting optimization problem. Case studies are reported to corroborate the efficacy of the novel model and approach tested on the IEEE 30-bus benchmark system with real operation data from seven wind farms.},
  DOI                      = {10.1109/SmartGridComm.2013.6688033},
  Owner                    = {sotterson},
  Timestamp                = {2014.12.04},
  URL                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688033}
}

@Misc{Ableson12classNotesMonoSpline,
  author       = {Alan Ableson},
  title        = {Applications of Numerical Methods, Lecture 2: MATLAB Splines, Monotone Interpolants},
  howpublished = {Math 272 Course Notes},
  month        = apr,
  year         = {2012},
  abstract     = {Intro page of lecture notes

Interpolation
? Interpolation is the generation of a function that goes exactly through a set of n
user-provided control points, (x1, y1), (x2, y2), . . . , (xn, yn).
? We saw last class that for a set of n points, a unique degree n ? 1 polynomial can
be found that interpolates the points
? Polynomials had some undesirable properties as interpolants, so we next looked at
piece-wise functions, specifically splines.
Splines
Based on desired smoothness properties, we defined spline interpolants, which are piecewise
cubic polynomials. Without any restrictions, there are many possibilities for a cubic
polynomial on each interval. To narrow our choices, we can make the spline interpolant
unique by requiring
? Continuity at the control points
? Continuous derivatives at the control points
? Continuous second derivatives at the control points
? User-selected properties at the end points (e.g. end slopes)
Splines in MATLAB
Exercise: starting with the script L2 1.m, use MATLAB to generate the spline interpolant
? through (0, 0), (2, 0.2), (4, 2), and (6, 1.5), and
? with end slopes P?(0) = 0.5, and P?(6) = ?1},
  comment      = {Explains how to do interpolation, assuming montonicity in Matlab Splines vs PCHIP Comparison
* Splines are smoother than pchip interpolants (pchip has discontinuous second derivatives at the data points).
* Splines generally fit underlying smooth functions better than pchip.

* pchip will guarantee that if your data is monotone increasing or decreasing, the interpolation function will be too.
* Splines are much more commonly used "in the wild".
* Choice ultimately depends on application.},
  file         = {Ableson12classNotesMonoSpline.pdf:Ableson12classNotesMonoSpline.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2012.07.16},
  url          = {http://www.mast.queensu.ca/~math272/index.shtml},
}

@Article{Taylor07frcstSprMktPriceQR,
  author    = {Taylor, James W},
  title     = {Forecasting daily supermarket sales using exponentially weighted quantile regression},
  journal   = {European Journal of Operational Research},
  year      = {2007},
  volume    = {178},
  number    = {1},
  pages     = {154--167},
  abstract  = {Inventory control systems typically require the frequent updating of forecasts for many different products. In addition to point predictions, interval forecasts are needed to set appropriate levels of safety stock. The series considered in this paper are characterised by high volatility and skewness, which are both time-varying. These features motivate the consideration of forecasting methods that are robust with regard to distributional assumptions. The widespread use of exponential smoothing for point forecasting in inventory control motivates the development of the approach for interval forecasting. In this paper, we construct interval forecasts from quantile predictions generated using exponentially weighted quantile regression. The approach amounts to exponential smoothing of the cumulative distribution function, and can be viewed as an extension of generalised exponential smoothing to quantile forecasting. Empirical results are encouraging, with improvements over traditional methods being particularly apparent when the approach is used as the basis for robust point forecasting.

Keywords
Forecasting; Exponential smoothing; Quantile regression; Interval forecasting; Robust point forecasting},
  comment   = {Adaptive, exponentially weighted quantile regression forecasting for price forecasting (actually QR is used to make better price forecasts). Skimming it, this seems to be a clear paper. The adaptation is brute fore retraining of QR at each step (with exponential weights). The resulting point forecasts are compared to Holt-Winters.


Holt-Winters orig paper: Winters60exponMvAvgSalesFrcst
Tryggvi's QR forecasting using Holt-Winters: Jonsson13spotFrcstWind},
  file      = {Taylor07frcstSprMktPriceQR.pdf:Taylor07frcstSprMktPriceQR.pdf:PDF},
  publisher = {Elsevier},
  url       = {http://www.sciencedirect.com/science/article/pii/S0377221706000737#},
}

@Misc{Sweeney12cosmoPostProc,
  author       = {Sweeney, Conor and Lynch, P and Nolan, P and Courtney, J},
  title        = {Post-processing COSMO output for improved wind forecasts},
  howpublished = {COSMO Newsletter No. 12},
  month        = apr,
  year         = {2012},
  abstract     = {Ireland has a large number of wind farms, which have supplied an average of 15\% of system demand over the last year, and have peaked to supply up to 50\% of demand (EirGrid), as shown in figure 1. The amount of electricity delivered by wind farms is due to increase, with more wind farms under construction. The government has set an ambitious target of 40\% of electricity to be supplied from renewables by 2020. The majority of this is due to come from wind energy. Although wind energy has the benefit of being environmentally friendly, it has the disadvantage of being difficult to integrate efficiently into the national electricity grid. Unlike traditional generators, the wind can not be turned on or off at will. To help with this potentially costly problem of managing a large amount of wind energy on the grid, there is a keen interest in developing methods to accurately predict when the wind will blow, and how much electricity will be generated. It is common practice for a forecast office to issue a forecast based,amongst other things, on the output of a numerical weather prediction model (like COSMO!). Even the best model,however, cannot produce a perfect forecast. Some of these forecast errors are difficult to overcome, but others may be due to some systematic process. It is these systematic errors that we hope to reduce by applying post-processing methods to COSMO.},
  comment      = {Just a newsletter reporting results in: Sweeney11frcstCombo},
  file         = {Sweeney12cosmoPostProc.pdf:Sweeney12cosmoPostProc.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2013.03.13},
  url          = {http://cosmo-model.cscs.ch/content/model/documentation/newsLetters/newsLetter12/default.htm},
}

@Misc{EPRI09incrTransWeathForecast,
  author       = {EPRI},
  title        = {Increased Transmission Capacity - DTCR Technologies - Project 38.002},
  howpublished = {http://tinyurl.com/dh2ss4},
  year         = {2009},
  abstract     = {Issue The demand for electric power over transmission circuits is increasing at a faster rate than the construction of new transmission facilities. This trend has pushed the capacity of many existing transmission circuits to their design limits. In addition, much of the grid has already aged beyond its original design specifications, resulting in an increasing number of bottlenecks, brownouts, and other severe reliability issues. However, because original design specifications were conservative, most transmission assets have significantly greater power capacity than previously assumed. With the proper technology, greater power capacities can be reliably and safely realized. Description This project continues to develop software and methodologies for optimizing the power ratings of transmission circuits. A new software product for dynamic thermal circuit rating (DTCR), the DTCR Data Analysis Program (DAP), will have its first version released in 2008, and a second version in 2009. Also, EPRI?s well-known DTCR program will be updated to version 5.0 in 2009. These software products can provide real-time ratings or be used with innovative methods for quasi-dynamic, probabilistic, and predictive ratings. The project also explores these innovative methodologies for optimizing transmission circuit ratings. One focus for 2009 will be to use existing weather prediction technologies for accurate forecasting of transmission circuit ratings. Value * Increase and optimize power flow through entire transmission circuits (including lines, cables, transformers, and substation equipment) * Defer capital expenditures * Operate transmission circuits reliably and safely * Meet new mandatory FERC requirements for circuit ratings * Optimize energy transactions through rating forecasts How to Apply Results Transmission planners, designers, operators, and engineers will use this project?s results of to gather the necessary data to enable the software products to provide real-time ratings of circuit components or entire circuits. Further, the same data and rating simulations can be analyzed to provide optimal static rating assignments through the quasi-dynamic rating process. In addition, these products provide an ideal way to fulfill the new mandatory FERC reliability requirements regarding the determination and documentation of ratings.},
  comment      = {EPRI project that will forecast thermal limits based on weather forecasting},
  owner        = {sotterson},
  timestamp    = {2009.02.24},
  url          = {http://portfolio.epri.com/ProgramTab.aspx?sId=PDU&rId=120&pId=4163&pjId=4166},
}

@Article{Girard10trajEvalWind,
  author    = {Robin Girard and Pierre Pinson},
  title     = {Evaluation of time trajectories - Application to wind power forecasting},
  journal   = {International Journal of Forecasting},
  year      = {2010},
  month     = sep,
  abstract  = {Issuing multivariate probabilistic forecasts simultaneously covering several lead times may be intractable. In contrast samples of these probabilistic forecasts in the form of time trajectories are routinely produced, though mainly for weather-related processes. So far the verification of these time trajectories is almost always focused on their marginal distributions for each lead time only, thus overlooking their temporal interdependence structure. A proposal for their evaluation is developed, inspired by functional data analysis and event-based verification. Its application to the evaluation of two sets of time trajectories of wind power generation, generated using wind power ensemble forecasts and a Gaussian copula approach, demonstrates it as a powerful discrimination tool. Keywords: Uncertainty, Scenarios, Error measures, Evaluating forecasts, Energy forecasting, Density forecasts},
  comment   = {spinning reserves scenario evaluation (preprint from Pierre)

Preprint submitted to International Journal of Forecasting
* uses Gaussian copula to model interdependence
* Gaussian copula seems to not be as good as NWP ensembles (p. 28)

* uses functionals to detect events (I tried to use the same idea for phase error detection)
* compares NWP ensembles against statistical ensembles in Pinson09probFrcstStatScenWind

* related: Pinson12scenQualWind},
  file      = {Preprint:Girard10trajEvalWind_preprint.pdf:PDF},
  groups    = {Test, doReadWPV_2},
  owner     = {scot},
  timestamp = {2010.11.24},
}

@Article{Brownlee18whenRocPrecRec,
  author        = {Jason Brownlee},
  title         = {How and When to Use ROC Curves and Precision-Recall Curves for Classification in Python},
  journal       = {Machine Learning Mastery Blog},
  year          = {2018},
  month         = aug,
  __markedentry = {[Scott:1]},
  abstract      = {It can be more flexible to predict probabilities of an observation belonging to each class in a classification problem rather than predicting classes directly.

This flexibility comes from the way that probabilities may be interpreted using different thresholds that allow the operator of the model to trade-off concerns in the errors made by the model, such as the number of false positives compared to the number of false negatives. This is required when using models where the cost of one error outweighs the cost of other types of errors.

Two diagnostic tools that help in the interpretation of probabilistic forecast for binary (two-class) classification predictive modeling problems are ROC Curves and Precision-Recall curves.

In this tutorial, you will discover ROC Curves, Precision-Recall Curves, and when to use each to interpret the prediction of probabilities for binary classification problems.},
  comment       = {A good reference for Precision Recall, ROC, and those terms I always confuse like "sentivity"},
  file          = {:Brownlee18whenRocPrecRec.pdf:PDF},
  url           = {https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/},
}

@Article{Bai16condCplaWindPowCrvFeat,
  author    = {Bai, Guanghan and Fleck, Brian and Zuo, Ming J.},
  title     = {A stochastic power curve for wind turbines with reduced variability using conditional copula},
  journal   = {Wind Energy},
  year      = {2016},
  volume    = {19},
  number    = {8},
  pages     = {1519--1534},
  issn      = {1099-1824},
  note      = {WE-15-0003.R1},
  abstract  = {It has been observed that a large variability exists between wind speed and wind power in real metrological conditions. To reduce this substantial variability, this study developed a stochastic wind turbine power curve by incorporating various exogenous factors. Four measurements, namely, wind azimuth, wind elevation, air density and solar radiation are chosen as exogenous influence factors. A recursive formula based on conditional copulas is used to capture the complex dependency structure between wind speed and wind power with reduced variability. A procedure of selecting a proper form for each factor and its corresponding copula models is given. Through a case study on the small wind turbine located in southeast of Edmonton, Alberta, Canada, we demonstrate that the variability can be reduced significantly by incorporating these influence factors. Wind turbine operators can apply the method reported in this study to construct a stochastic power curve for local wind farms and use it to achieve more accurate power forecasting and health condition monitoring of the turbine. Copyright ?? 2015 John Wiley & Sons, Ltd.},
  comment   = {wind power curve modeled by set of multivariate copulas.  Different forms of input variables, e.g. pressure, are selected amongst using feature selection.  Result is narrower power curve uncertainty band.

Also, as table of various forms of conditional copulas.},
  doi       = {10.1002/we.1934},
  file      = {Bai16condCplaWindPowCrvFeat.pdf:Bai16condCplaWindPowCrvFeat.pdf:PDF},
  keywords  = {stochastic power curve, wind turbine, conditional copula, wind azimuth, wind elevation},
  owner     = {sotterson},
  timestamp = {2017.05.23},
  url       = {http://dx.doi.org/10.1002/we.1934},
}

@Article{He98bivarQsmSplines,
  author    = {He, Xuming and Ng, Pin and Portnoy, Stephen},
  title     = {Bivariate quantile smoothing splines},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year      = {1998},
  volume    = {60},
  number    = {3},
  pages     = {537--550},
  abstract  = {It has long been recognized that the mean provides an inadequate summary whereas the set of quantiles can supply a more complete description of a sample. We introduce bivariate quantile smoothing splines, which belong to the space of bilinear tensor product splines, as nonparametric estimators for the conditional quantile functions in a two-dimensional design space. The estimators can be computed by using standard linear programming techniques and can further be used as building-blocks for conditional quantile estimations in higher dimensions. For moderately large data sets, we recommend penalized bivariate B-splines as approximate solutions. We use real and simulated data to illustrate the methodology proposed.


Keywords:

 Conditional quantile;
 Linear program;
 Nonparametric regression;
 Robust regression;
 Schwarz information criterion;
 Tensor product spline},
  comment   = {Shows that binlinear tensor quantile splines are optimal for a bounded 2D region. Much better than thin plate splines, I think he says.},
  doi       = {10.1111/1467-9868.00138/abstract},
  file      = {He98bivarQsmSplines.pdf:He98bivarQsmSplines.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.11.07},
}

@Article{Wright99bayesNN,
  author    = {Wright, W. A.},
  title     = {{Bayes}ian approach to neural-network modeling with input uncertainty},
  journal   = {Neural Networks, IEEE Transactions on},
  year      = {1999},
  volume    = {10},
  number    = {6},
  pages     = {1261--1270},
  issn      = {1045-9227},
  abstract  = {It is generally assumed when using Bayesian inference methods for neural networks that the input data contains no noise or corruption. For real-world (errors in variable) problems this is clearly an unsafe assumption. This paper presents a Bayesian neural-network framework which allows for input noise provided that some model of the noise process exists. In the limit where the noise process is small and symmetric it is shown, using the Laplace approximation, that this method gives an additional term to the usual Bayesian error bar which depends on the variance of the input noise process. Further, by treating the true (noiseless) input as a hidden variable and sampling this jointly with the network weights using a Markov chain Monte Carlo method, it is demonstrated that it is possible to infer the regression over the noiseless input},
  comment   = {Maybe good for Bayesian Model Averaging of an ensemble of point forecasts (coming from NN).

Referenced in Guan12hybKalmanLdFrcst-Rev3, where it is said that this paper:

Starts with a prior distribution of the NN's weights, and then optimized weights are determined by maximizing the posterior distribution based on historical data. Through Taylor series expansion, the prediction distribution conditioned on a new input and weights was derived and approximated as a Gaussian distribution.

Zhang03bayesNNnewt is same idea but more computationally efficient via Quasi-Newton methods},
  doi       = {10.1109/72.809073},
  file      = {Wright99bayesNN.pdf:Wright99bayesNN.pdf:PDF},
  groups    = {Ensemble, doReadNonWPV_1},
  keywords  = {Bayes methods;Markov processes;Monte Carlo methods;approximation theory;error statistics;estimation theory;inference mechanisms;neural nets;noise;Bayesian error;Bayesian inference;Laplace approximation;Markov chain;Monte Carlo method;estimation theory;input noise;neural-network modeling;uncertainty;Bars;Bayesian methods;Estimation error;Learning systems;Multilayer perceptrons;Neural networks;Sampling methods;Sensor phenomena and characterization;Sensor systems;Uncertainty},
  owner     = {sotterson},
  timestamp = {2013.04.10},
}

@Article{Zhang15windSpdFrcstPDBM,
  author    = {C. Y. Zhang and C. L. P. Chen and M. Gan and L. Chen},
  title     = {Predictive Deep Boltzmann Machine for Multiperiod Wind Speed Forecasting},
  journal   = {IEEE Transactions on Sustainable Energy},
  year      = {2015},
  volume    = {6},
  number    = {4},
  pages     = {1416--1425},
  month     = oct,
  issn      = {1949-3029},
  abstract  = {It is important to forecast the wind speed for managing operations in wind power plants. However, wind speed prediction is extremely complex and difficult due to the volatility and deviation of the wind. As existing forecasting methods directly model the raw wind speed data, it is difficult for them to provide higher inference accuracy. Differently, this paper presents a sophisticated deep-learning technique for short-term and long-term wind speed forecast, i.e., the predictive deep Boltzmann machine (PDBM) and corresponding learning algorithm. The proposed deep model forecasts wind speed by analyzing the higher level features abstracted from lower level features of the wind speed data. These automatically learnt features are very informative and appropriate for the prediction. The proposed PDBM is a deep stochastic model that can represent the wind speed very well, and is inspired by two aspects. 1) The stochastic model is suitable to capture the probabilistic characteristics of wind speed. 2) Recent developments in neural networks with deep architectures show that deep generative models have competitive capability to approximate nonlinear and nonsmooth functions. The evaluation of the proposed PDBM model is depicted by both hour-ahead and day-ahead prediction experiments based on real wind speed datasets. The prediction accuracy of the PDBM model outperforms existing methods by more than 10\%.},
  comment   = {Works better than MLPs at ST and DA wind /speed/ horizons.  Uses a Predictive Deep Boltzman Machine, which is probabilistic on some way (I should read...)},
  doi       = {10.1109/TSTE.2015.2434387},
  file      = {Zhang15windSpdFrcstPDBM.pdf:Zhang15windSpdFrcstPDBM.pdf:PDF},
  keywords  = {Boltzmann machines, load forecasting, power system management, stochastic processes, wind power plants, long-term wind speed forecast, multiperiod wind speed forecasting, predictive deep Boltzmann machine, short-term wind speed forecast, stochastic model, wind power plants, wind speed prediction, Machine learning, Predictive models, Time series analysis, Training, Wind forecasting, Wind power generation, Wind speed, Deep Boltzmann machine (DBM), deep learning, time series, wind speed prediction},
  owner     = {sotterson},
  timestamp = {2017.03.08},
}

@Article{Piepho05permTestCorrDist,
  author   = {Piepho, H.P.},
  title    = {Permutation tests for the correlation among genetic distances and measures of heterosis},
  journal  = {Theoretical and Applied Genetics (TAG)},
  year     = {2005},
  volume   = {111},
  number   = {1},
  pages    = {95--99},
  month    = jun,
  abstract = {It is often found that heterosis tends to increase with genetic distance of the parents, though the correlation is not usually very close. It is therefore important to test the null hypothesis that the correlation is zero. The present work shows that standard procedures tend to yield too liberal tests, owing to the lack of independence among genetic distances and among heterosis estimates. A valid alternative is to use a permutation test, which was first suggested by Mantel [(1967) Cancer Res 27: 209?220). This test is well-known among plant breeders and geneticists, who often use it to test the correlation among two distance matrices. Its use is not restricted to the comparison of distance matrices. This is demonstrated in the present work, using two published datasets on marker-based genetic distances of maize inbreds or populations and heterosis of their crosses. It is shown that the test is also applicable in the presence of missing data.},
  comment  = {Correlation significance test that's robust to missing data},
  doi      = {10.1007/s00122-005-1995-7},
  file     = {Piepho05permTestCorrDist.pdf:Piepho05permTestCorrDist.pdf:PDF;Piepho05permTestCorrDist.pdf:Piepho05permTestCorrDist.pdf:PDF},
  language = {German},
  url      = {http://www.springerlink.com/content/w0v7u560777p4l63/},
}

@InProceedings{Papaefthymiou08spatioTempModel,
  author    = {Papaefthymiou, G. and Pinson, P.},
  title     = {Modeling of Spatial Dependence in Wind Power Forecast Uncertainty},
  booktitle = {Probabilistic Methods Applied to Power Systems (PMAPS)},
  year      = {2008},
  pages     = {1--9},
  abstract  = {It is recognized today that short-term (up to 2-3 days ahead) probabilistic forecasts of wind power provide forecast users with a paramount information on the uncertainty of expected wind generation. When considering different areas covering a region, they are produced independently, and thus neglect the interdependence structure of prediction errors, induced by movement of meteorological fronts, or more generally by inertia of meteorological systems. This issue is addressed here by describing a method that permits to generate interdependent scenarios of wind generation for spatially distributed wind power production for specific look-ahead times. The approach is applied to the case of western Denmark split in 5 zones, for a total capacity of more than 2.1 GW. The interest of the methodology for improving the resolution of probabilistic forecasts, for a range of decision-making problems, or simply for better understanding the characteristics of forecast uncertainty, is discussed.},
  comment   = {Copula probabilistic forecast of DK's 5 control zones.  "Scenarios" have no temporal error correlation but they do result in a scenario forecast sum that might be a bit better than QR done directly on the sum.  NOT WHAT I FOUND.

Copula forecast algorithm
* Marginal to uniform xform done with probabilistic forecasts  (LOOKUP method)
   - Prob forecasts come from some kind of resampling from Pierr's 2006 thesis
   - some kind of power-level sensitivity
   - some kind of exponential tail on the extreme quantile
   - done uniquely for each horizon
   - exponential forgetting
   - assumes the value of zeroth quantile is 0 (WRONG THING TO DO, actually)
   - QR not used in Papaefthymiou09copulaPowUncert, marginal transform, just empirical cdf
* Spatial correlation from Gaussian distribution
   - no temporal correlation of errors in model, just spatial
   - uniform is transformed to normal
   - correlation estimated
   - norm dist is sampled a bunch of times
   - inverse norm xform back to unif
* Uniform dist to power distribution with inverse of QR forecast, assumed to be monotonically increasing
   - WRONG near zero, or when saturated @ max power


Correlation properties
   1) increasing corr. w/ horizon (NWP makes more systematic error w/ horizon
   2.) more corr. when regions are closer
   3) more corr if are on same N-S axis

Forecast from sum of scenarios
* added the scenarios
* then did QR o nthis
* Result compared to QR forecast done on the sum of deterministic forecasts.
  - same reliability (a marginal-over-time property I guess)
  - better skill (better conditionality, I guess)

Very similar to Pierre's 2009 paper.

May have been used in Wang16evalStochMethScen (but was 2009 paper above actually used?)},
  file      = {Papaefthymiou08spatioTempModel.pdf:Papaefthymiou08spatioTempModel.pdf:PDF},
  groups    = {ErrDistProps, doReadNonWPV_1},
  keywords  = {decision making, load forecasting, probability, wind power, decision-making problems, meteorological systems, prediction errors, probabilistic forecasting, spatially distributed wind power production, western Denmark, wind generation, wind power forecast uncertainty},
  owner     = {scot},
  timestamp = {2010.11.29},
  url       = {http://ieeexplore.ieee.org/document/4912606/},
}

@TechReport{Unknown13boxCoxSeasonal,
  author      = {Unknown},
  title       = {Research Paper: Appropriate Use of Box-Cox Transforms for Seasonal Adjustment (Methodology Advisory Committee)},
  institution = {Australian Bureau of Statistics},
  year        = {2013},
  number      = {1352.0.55.116},
  month       = oct,
  abstract    = {It is well established that appropriate Box?Cox transformation of data is in some cases desirable when using standard analytical techniques. For example, the use of such transforms for variance stabilization in regression and ARIMA time series modelling were developed in Box and Cox (1964) and Box and Jenkins (1970) respectively.
In the setting of seasonal adjustment of time series such transforms offer a compromise between, and extension beyond, the standard additive and multiplicative options of decomposition models. In particular, an appropriate transformation may lead to more stable seasonal factor estimates and in turn reduce current end revisions to seasonally adjusted estimates obtained using the ABS X11-based concurrent method.
The empirical study presented here evaluates two existing methods of selecting the Box?Cox parameter, and proposes two new methods for the purposes of seasonal adjustment. These methods of transformation selection are compared to an optimal transform found by a simple search method. Quality is assessed via measures relating to the volatility of, and current end revisions to, the resulting seasonally adjusted and trend series.
The existing methods evaluated are a maximum likelihood approach, given a seasonal ARIMA model (Hipel et al., 1977), and a time series variance stabilisation method (Guerrero, 1993). A simple alternative is trialled that uses appropriate seasonal dummy variables in a regression ARIMA model. The aim of this latter approach is to apply a transform that results in stable additive seasonal factors. An additional method optimises the Box-Cox parameter with respect to a quality indicator developed by Statistics Canada, known as the M7 value, which provides a measure of the reliability of the seasonal adjustment.},
  comment     = {Explains how to pick lambda in box-cox transform, which can smoothing adjust between multiplicative and additive relationships. Here, it's for seasonality, but maybe it could be used for, example, wind direction interaction with the wind speed power curve.},
  file        = {Unknown13boxCoxSeasonal.pdf:Unknown13boxCoxSeasonal.pdf:PDF},
  url         = {http://www.abs.gov.au/ausstats/abs@.nsf/mf/1352.0.55.116},
}

@Article{Karunamuni05kernBndryCrct,
  author    = {Karunamuni, Rhoana J and Alberts, Tom},
  title     = {On boundary correction in kernel density estimation},
  journal   = {Statistical Methodology},
  year      = {2005},
  volume    = {2},
  number    = {3},
  pages     = {191--212},
  abstract  = {It is well known now that kernel density estimators are not consistent when estimating a density near the finite end points of the support of the density to be estimated. This is due to boundary effects that occur in nonparametric curve estimation problems. A number of proposals have been made in the kernel density estimation context with some success. As of yet there appears to be no single dominating solution that corrects the boundary problem for all shapes of densities. In this paper, we propose a new general method of boundary correction for univariate kernel density estimation. The proposed method generates a class of boundary corrected estimators. They all possess desirable properties such as local adaptivity and non-negativity. In simulation, it is observed that the proposed method perform quite well when compared with other existing methods available in the literature for most shapes of densities, showing a very important robustness property of the method. The theory behind the new approach and the bias and variance of the proposed estimators are given. Results of a data analysis are also given.
MSC

Keywords

Density estimation;
Mean squared error;
Kernel estimation;
Reflection},
  comment   = {Can use this instead of beta for bounded kernel density estimation. In fact, it seems that can use any standard kernel function and it will work like it always did in the interior region.

Possibly, this could yield a better wind power error distribution estimate.

Authors had an apparently similar idea for one sided boundaries in 2007:
"Some improvements on a boundary corrected kernel density estimator"
http://www.sciencedirect.com/science/article/pii/S0167715207002817},
  file      = {Karunamuni05kernBndryCrct.pdf:Karunamuni05kernBndryCrct.pdf:PDF},
  groups    = {PointDerived, doReadWPV_2, doReadNonWPV_2},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2013.10.25},
  url       = {http://www.sciencedirect.com/science/article/pii/S1572312705000195},
}

@Article{Simar10mofnBtstrp,
  author      = {Simar, L{\'e}opold and Wilson, Paul},
  title       = {Inference by the m out of n bootstrap in nonparametric frontier models},
  journal     = {Journal of Productivity Analysis},
  year        = {2010},
  pages       = {1--21},
  issn        = {0895-562X},
  abstract    = {It is well-known that the naive bootstrap yields inconsistent inference in the context of data envelopment analysis (DEA) or free disposal hull (FDH) estimators in nonparametric frontier models. For inference about efficiency of a single, fixed point, drawing bootstrap pseudo-samples of size ? provides consistent inference, although coverages are quite sensitive to the choice of subsample size m. We provide a probabilistic framework in which these methods are shown to valid for statistics comprised of functions of DEA or FDH estimators. We examine a simple, data-based rule for selecting m suggested by Politis et??al. (Stat Sin 11:1105???1124, 2001), and provide Monte Carlo evidence on the size and power of our tests. Our methods (i) allow for heterogeneity in the inefficiency process, and unlike previous methods, (ii) do not require multivariate kernel smoothing, and (iii) avoid the need for solutions of intermediate linear programs.},
  affiliation = {Institut de Statistique, Universit?? Catholique de Louvain, Voie du Roman Pays 20, 1348 Louvain-la-Neuve, Belgium},
  comment     = {May explain how to pick the subsample size for subsampling bootstrap * pick the subsample size in Politis01asympSubsamp and Geyer06subSampBootStrap * mention Bickel08choiceOfMsubsampBtstrp * but they actually use the algorithm in Politis01asympSubsamp (I think) * example of when can't bootstrap w/ replacement is K-NN MI, which Francois06permTestMutInf has found is biased by this (I have found the same thing in my ramp experiments). * also talks about high dimensional estimation and model structure selection w/ subsampling * I _think_ the bootstrap part of this paper is about estimating the sample maximum, which is why they used subsampling Why subsampling is better for estimating confidence intervals for a maximum value estimate: Subsampling is better for estimating the maximum of a distributionbecause the true maximum is selected less frequently. Allows you to estimate probabilities of maxima bigger than the one you've seen in this subsample. For full sample bootstrap, you'll almost always get the true maximum so the distributionof maximums across the full bootstraps will be vary norrow, w/ high prob of the full sample max.},
  doi         = {10.1007/s11123-010-0200-4},
  file        = {Simar10mofnBtstrp.pdf:Simar10mofnBtstrp.pdf:PDF},
  groups      = {Read},
  keyword     = {Engineering},
  owner       = {scot},
  publisher   = {Springer Netherlands},
  timestamp   = {2011.06.21},
}

@Misc{Lawrence16deepLrnEffic,
  author       = {Neil Lawrence},
  title        = {Deep Learning, Pachinko, and James Watt: Efficiency is the Driver of Uncertainty},
  howpublished = {Blog: inverseprobability.com},
  month        = mar,
  year         = {2016},
  abstract     = {It seems it may only be a matter of time before the best Go player on the planet is a computer. AlphaGo beat the European champion in Go and was driven by machine learning, a technology that has underpinned the recent major advances in artificial intelligence in computer vision, speech recognition and language translation.1
Machine learning is a data driven approach to artificial intelligence. AlphaGo learnt how to play Go by many games played against itself, and by observing a large history of games played by professional players.
The end result is that by the time of its first match against the European Champion AlphaGo had already played many more games of Go than any human could possibly play in their lifetime. And since that win AlphaGo has been actively learning to improve itself. Relentlessly playing all day and all night in an effort to ready itself to play the world champion.},
  comment      = {Good facts about how much data deep learning needs, especially about Gaussian Variational Processes that can make it work with less data.},
  file         = {Lawrence16deepLrnEffic.pdf:Lawrence16deepLrnEffic.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2017.01.25},
  url          = {http://inverseprobability.com/2016/03/04/deep-learning-and-uncertainty},
}

@InProceedings{Tian13localPLSvoiceMap,
  author    = {Xiaohai Tian and Zhizheng Wu and Eng Siong Chng},
  title     = {Local partial least square regression for spectral mapping in voice conversion},
  booktitle = {Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2013 Asia-Pacific},
  year      = {2013},
  pages     = {1--6},
  abstract  = {Joint density Gaussian mixture model (JD-GMM) based method has been widely used in voice conversion task due to its flexible implementation. However, the statistical averaging effect during estimating the model parameters will result in over-smoothing the target spectral trajectories. Motivated by the local linear transformation method, which uses neighboring data rather than all the training data to estimate the transformation function for each feature vector, we proposed a local partial least square method to avoid the over-smoothing problem of JD-GMM and the over-fitting problem of local linear transformation when training data are limited. We conducted experiments using the VOICES database and measure both spectral distortion and correlation coefficient of the spectral parameter trajectory. The experimental results show that our proposed method obtain better performance as compared to baseline methods.},
  comment   = {A clustered regression/dimension reduction approach to local linear voice mapping. Foreach test point, pick k-nearest neighbhors (input euclidean distance). Then compute a linear transform mapping speech features to the other speaker. Uses parital least squares to dimension reduce while doing the mapping. num. PLS dims and KNN k chosen with informal cross-validation.

Could replace PLS w/ lasso-ilike featsel, as in Zhang12rflctLocLinKNN},
  doi       = {10.1109/APSIPA.2013.6694332},
  file      = {Tian13localPLSvoiceMap.pdf:Tian13localPLSvoiceMap.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2014.04.21},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6694332},
}

@Article{Kazuhito06windPredLoc,
  author    = {Fukao Kazuhito and Yoshino Jun and Tanaka Akira and Kobayashi Tomonao and Yasuda Takashi},
  title     = {Wind power generation forecast using the real-time local weather forecasting system},
  journal   = {Wind Energy (Japan)},
  year      = {2006},
  volume    = {30},
  number    = {1},
  pages     = {92--98},
  abstract  = {June, 2005, the prediction result using the forecasting system of the title has been opened to public in website of Gifu Univ. Here, the outline of the local field weather prediction system was introduced. And, the error correction by Kalman filter was carried out in wind predicted data in order to raise prediction accuracy, in addition. The conclusion under the result was got. 1) By using the Kalman filter, the accuracy of the wind velocity improved on 40.2\%, and The RMS error reduced almost by half. 2) It was connected with drastic accuracy improvement in the electric power generation of 97.5\% in the average, and the RMS error decreased in about 1/5. 3) By conducting the Kalman filter to wind predicted data of local weather forecasting system, the wind prediction considering the difference between minute landform and land use became possible. 4) The effect of the combination of prediction output data of the mesoscale meteorological model and statistics corrective action by the Kalman filter, is big for the improvement in the prediction accuracy of the wind power generation quantity.},
  comment   = {Big accuracy improvement w/ Kalman filter. Kalman filter also used to combine mesoscale and local forecast systems?},
  owner     = {scotto},
  timestamp = {2008.07.06},
  url       = {http://sciencelinks.jp/j-east/article/200615/000020061506A0469306.php},
}

@Article{Chen99betaKern,
  author    = {Song Xi Chen},
  title     = {Beta kernel estimators for density functions},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {1999},
  volume    = {31},
  number    = {2},
  pages     = {131--145},
  issn      = {0167-9473},
  abstract  = {Kernel estimators using non-negative kernels are considered to estimate probability density functions with compact supports. The kernels are chosen from a family of beta densities. The beta kernel estimators are free of boundary bias, non-negative and achieve the optimal rate of convergence for the mean integrated squared error. The proposed beta kernel estimators have two features. One is that the different amount of smoothing is allocated by naturally varying kernel shape without explicitly changing the value of the smoothing bandwidth. Another feature is that the support of the beta kernels can match the support of the density function; this leads to larger effective sample sizes used in the density estimation and can produce density estimates that have smaller finite-sample variance than some other estimators.},
  comment   = {The kernel used for double bounded wind power in Mendes11statWindFrcst

Maybe could also use this: Karunamuni05kernBndryCrct},
  doi       = {10.1016/S0167-9473(99)00010-9},
  file      = {Chen99betaKern.pdf:Chen99betaKern.pdf:PDF},
  groups    = {PointDerived, doReadWPV_1},
  keywords  = {Beta kernels},
  owner     = {sotterson},
  timestamp = {2013.10.25},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167947399000109},
}

@Electronic{xian11gkDistQuant,
  author    = {X'ian},
  year      = {2011},
  title     = {Quantile distributions},
  note      = {Blog page. Author's last name is unknown.},
  url       = {http://xianblog.wordpress.com/2011/06/29/quantile-distributions/},
  abstract  = {Kerrie Mengersen, who is visiting CREST and Dauphine this month, showed me a 2009 paper she had published in Statistics and Computing along with D. Allingham and R. King on an application of ABC to quantile distributions. Those distributions are defined by a closed-form quantile function, which makes them easy to simulate by a simple uniform inversion, and a mostly unavailable density function, which makes any approach but ABC difficult or at least costly to implement. For instance, the g-and-k distribution is given by

Q(u;A,B,g,k) = \qquad\qquad\qquad

\qquad A + B\left[1+c\dfrac{1-\exp\{-g\Phi(u)\}}{1+\exp\{-g\Phi(u)\}}\right]\{1+\Phi(u)^2\}^k\Phi(u)

hence can be simulated by a single call to a normal simulation. This is therefore a good benchmark for realistic albeit simple examples to use in ABC calibration and we are currently experimenting with it.},
  comment   = {g-and-k distribution is a good test case for quantile estimation: it's easy to simulate but (I think) has an intractable density function.

A commeter mentioned that his ABC R code:

http://www.maths.lancs.ac.uk/~prangle/pub.html

Implements this distribution},
  groups    = {Test, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.04},
}

@TechReport{Heller14PredWindPowGtAcc,
  author      = {Arnie Heller},
  title       = {Predicting Wind Power with Greater Accuracy},
  institution = {Lawrence Livermore National Laboratory},
  year        = {2014},
  type        = {ST\&TR},
  month       = apr,
  abstract    = {Key Words: atmospheric boundary layer,
CGWind, computational fluid dynamics (CFD),
electric power grid, Gaussian Process Model,
Generalized Actuator Disk (GAD), HELIOS,
immersed boundary method (IBM), lidar,
mesoscale, sodar, supervisory control and data
acquisition (SCADA), turbulence, Weather
Research Forecasting (WRF), wind turbine
farm, wind power forecast.},
  comment     = {Lawrence Livermore's wind power forecasting program blurb, including their dynamic power curve. Nice graphs showing affects of atmospheric stability, etc. on power curve. Also shading.},
  file        = {Heller14PredWindPowGtAcc.pdf:Heller14PredWindPowGtAcc.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2014.11.02},
  url         = {https://str.llnl.gov/content/pages/april-2014/pdf/04.14.1.pdf},
}

@Article{Perez19ovrbldCurtailFirmPV_draft,
  author   = {Marc Perez and Richard Perez and Karl R. R\'abago and Morgan Putnam},
  title    = {Overbuilding \& Curtailment: The cost-effective enablers of firm PV generation},
  journal  = {Solar Energy Journal},
  year     = {2019},
  abstract = {Key Words: photovoltaics; firm power generation, renewable intermittency mitigation; high PV penetration.
Abstract: Current thinking considers that PV output curtailment is a last resort measure to be avoided. In this article, we argue that supply-shaping, achieved through proactive curtailment associated with PV oversupply, is actually critical to achieving intermittency mitigation and delivering firm PV generation at the lowest cost. 
We investigate the premium to transform a low-cost, but intermittent solar kWh into a firm, effectively dispatchable kWh. We show that a fundamental ingredient of minimizing this premium is to optimally overbuild and, as necessary and appropriate, curtail PV generation. Drawing on a case study in the State of Minnesota, we show that firm, high-penetration-ready PV generation could be achieved at a production cost at or below current conventional generation, especially when optimally coupled with wind generation.
We conclude with a recommendation that in order to achieve this lowest cost firm generation potential, proactive curtailment strategies should inform future transactional PV remuneration systems.
},
  file     = {Draft on Jan 6, 2019:Perez19ovrbldCurtailFirmPV_draft.docx:Word 2007+},
  comment = {See also https://www.evernote.com/shard/s13/nl/1523219/84a7e829-a24f-4048-b05e-e5d4158f84ef/},
}

@Article{Hamill04introNWPens_Slides,
  author    = {Tom Hamill},
  title     = {Introduction to Numerical Weather Prediction and Ensemble Weather Forecasting},
  journal   = {NOAA-CIRES Climate Diagnostics Center Boulder, Colorado USA},
  year      = {2004},
  abstract  = {keywords CIRES, ensemble, HEPEX, hydrological, NOAA, workshop, ECMWF},
  file      = {Hamill04introNWPens_Slides.ppt:Hamill04introNWPens_Slides.ppt:PowerPoint},
  owner     = {sotterson},
  timestamp = {2017.03.31},
  url       = {http://www.ecmwf.int/en/elibrary/14092-introduction-numerical-weather-prediction-and-ensemble-weather-forecasting},
}

@Article{Greenewald15krnckrProdPCAspatTempCov,
  author    = {K. Greenewald and A. O. Hero},
  title     = {Robust Kronecker Product {PCA} for Spatio-Temporal Covariance Estimation},
  journal   = IEEE_J_SP,
  year      = {2015},
  volume    = {63},
  number    = {23},
  pages     = {6368--6378},
  month     = dec,
  issn      = {1053-587X},
  abstract  = {Kronecker PCA involves the use of a space versus time Kronecker product decomposition to estimate spatio-temporal covariances. In this paper, the addition of a sparse correction factor is considered, which corresponds to a model of the covariance as a sum of Kronecker products of low (separation) rank and a sparse matrix. This sparse correction extends the diagonally corrected Kronecker PCA of [Greenewald, and Hero, 2014] to allow for sparse unstructured {\textquotedblleft}outliers{\textquotedblright} anywhere in the covariance matrix, e.g., arising from variables or correlations that do not fit the Kronecker model well, or from sources such as sensor noise or sensor failure. We introduce a robust PCA-based algorithm to estimate the covariance under this model. An extension to Toeplitz temporal factors is also provided, producing a parameter reduction for temporally stationary measurement modeling. High dimensional MSE performance bounds are given for these extensions. Finally, the proposed extension of KronPCA is evaluated on both simulated and real data coming from yeast cell cycle experiments. This establishes the practical utility of robust Kronecker PCA in biological and other applications.},
  comment   = {A spatial/temporal Kronecker decomp of a covariance matrix.  Could be used for matrix-normal models, somehow, as they mention that this is one form that has this kind of cov mat.},
  doi       = {10.1109/TSP.2015.2472364},
  file      = {:papers\\Greenewald15krnckrProdPCAspatTempCov.pdf:PDF},
  keywords  = {covariance analysis, estimation theory, principal component analysis, signal processing, Kronecker product decomposition, parameter reduction, robust Kronecker product PCA, robust PCA based algorithm, spatio-temporal covariance estimation, temporally stationary measurement, Brain modeling, Covariance matrices, Linear programming, Optimization, Principal component analysis, Robustness, Sparse matrices, Kronecker product decompositions, Structured covariance estimation, high dimensional convergence rates, mean-square error, multivariate prediction, robust penalized least squares},
  owner     = {sotterson},
  timestamp = {2017.07.11},
}

@InProceedings{Zeiler14vizUnderstandConvNN,
  author       = {Zeiler, Matthew D and Fergus, Rob},
  title        = {Visualizing and understanding convolutional networks},
  booktitle    = {European conference on computer vision},
  year         = {2014},
  pages        = {818--833},
  organization = {Springer},
  abstract     = {Large Convolutional Network models have
recently demonstrated impressive classifica-
tion performance on the ImageNet bench-
mark (Krizhevsky et al., 2012). However
there is no clear understanding of why they
perform so well, or how they might be im-
proved. In this paper we address both issues.
We introduce a novel visualization technique
that gives insight into the function of inter-
mediate feature layers and the operation of
the classifier. Used in a diagnostic role, these
visualizations allow us to find model architec-
tures that outperform Krizhevsky et al. on
the ImageNet classification benchmark. We
also perform an ablation study to discover
the performance contribution from different
model layers. We show our ImageNet model
generalizes well to other datasets: when the
softmax classifier is retrained, it convincingly
beats the current state-of-the-art results on
Caltech-101 and Caltech-256 datasets.},
  comment      = {Mentioned by Karpathy tutorial as a good example of unspervised feature learning for convolutional nets.

http://karpathy.github.io/2014/07/03/feature-learning-escapades/},
  file         = {Zeiler14vizUnderstandConvNN.pdf:Zeiler14vizUnderstandConvNN.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2017.01.24},
  url          = {http://www.matthewzeiler.com/pubs/arxive2013/arxive2013.pdf},
}

@Article{Wu07litRevWindForecast,
  author    = {Yuan-Kang Wu and Jing-Shan Hong},
  title     = {A literature review of wind forecasting technology in the world},
  journal   = {Power Technology, IEEE},
  year      = {2007},
  pages     = {504--509},
  month     = jul,
  abstract  = {Large intermittent generations have grown the influence on the grid security, system operation, and market economics. Although wind energy may not be dispatched, the cost impacts of wind can be substantially reduced if the wind energy can be scheduled using accurate wind forecasting. In other words, the improvement of the performance of wind power forecasting tool has significant technology and economic impact on the system operation with increased wind power penetration. Forecasting has been a vital part of business planning in today's competitive environment, especially in areas characterized by a high concentration of wind generation and a limited capacity of network. The target of this paper is to present a critical literature review and an up-to-date bibliography on wind forecasting technologies over the world. Various forecasting aspects concerning the wind speed and power have been highlighted. These technologies based on numeric weather prediction (NWP) methods, statistical methods, methods based upon artificial neural networks (ANNs), and hybrid forecasting approaches will be discussed. Furthermore, the difference between wind speed and power forecasting, the lead time of forecasting, and the further research will also be discussed in this paper.},
  comment   = {Broad wind forecasting overview: also stuff useful for optimization Useful refs:
* economic benefits fof forecasting
* wind forecast models s/b periodically changed
* wind is highly nonlinear random process Statistical and NWP approaches

* says MM5 NWP is good! (Eric and MarkS say it's bad
* NWP w/ better terrain mapping is better but cost means only best for > 6 hrs
* statistical and neural better than NWP for shorter
* spectrograms show periodicity: load (fairly) > price (some) > wind speed (not at all!) load forecasting 1.5\% error vs. 10-20\% error for wind (24 hour) Sea vs. Land wind sites
* low roughness makes sea wind speed more stable
* sea more persisten than onshore Model Combination (ensembles)
* Hybrid models
-- combined statistical and NWP
-- ARX and NWP optimally weighted
* Forecast horizion
-- dynamically weighted ensemble for different lookaheads

Power Curve
* Power curve s/b treated as a stochastic time-varying function of wind speed
* measured power curve: 20\% improvement over manufacturer power curve

* NN power curve learner has pressure + temperature inputs, is better than other stuff.
* another NN PC learner (speed/dir inputs) learned turbine "dynamic performance" (but is only feedforward) Forecastng algs
* ARMA forecasting: adjust every month (1-10 hour ahead)
* California ISO says s/ forecast energy (MWh) instead of power. Not sure why
* neural nets seem better, than ARIMA not clear if recurrent.
- ARIMA: ARMA but w/ differences.
- A little like my BPA difference inputs (NowCaster)
* forecasts including risks:
-- meteorological risk (MRI) index and production risk (PRI) index
-- also some calculate a wind stability factor
-- are these useful for?
1.) ensemble methods, like BMA (Bayesian Model Averaging)
2.) optimization (transmission/demand)
3.) Bayesian Risk optimization (arbitrary objective function for forecasts, maybe this plus "cost of error")

Offsite Selection
* correlation based selection
* is fed directly into a NN estimating power output (no lag determination??)


Wind farm aggregation (related to BPA NowCaster sub-hourly project)

* correlation smoothing for aggregated forecasts across many farms

* aggregation across plants reduces errors by 30-50\%},
  doi       = {10.1109/PCT.2007.4538368},
  file      = {Wu07litRevWindForecast.pdf:Wu07litRevWindForecast.pdf:PDF;Wu07litRevWindForecast.pdf:Wu07litRevWindForecast.pdf:PDF},
  groups    = {Read},
  keywords  = {neural nets, power engineering computing, power generation economics, power generation planning, power grids, wind power plantsartificial neural networks, business planning, grid security, market economics, numeric weather prediction methods, statistical methods, system operation, wind forecasting technology, wind power forecasting, wind power penetration},
  owner     = {sotterson},
  timestamp = {2009.01.08},
}

@Article{Hagspiel12copulaWindPowEur,
  author    = {Simeon Hagspiel and Antonis Papaemannouil and Matthias Schmid and G??ran Andersson},
  title     = {Copula-based modeling of stochastic wind power in Europe and implications for the Swiss power grid},
  journal   = {Applied Energy},
  year      = {2012},
  volume    = {96},
  pages     = {33 - 44},
  issn      = {0306-2619},
  note      = {Smart Grids},
  abstract  = {Large scale integration of wind energy poses new challenges to the European power system due to its stochastic nature and often remote location. In this paper a multivariate uncertainty analysis problem is formulated for the integration of stochastic wind energy in the European grid. By applying copula theory a synthetic set of data is generated from scarce wind speed reanalysis data in order to achieve the increased sample size for the subsequent Monte Carlo simulation. In the presented case study, European wind power samples are generated from the modeled stochastic process. Under the precondition of a modeled perfect market environment, wind power impacts dispatch decisions and therefore leads to alterations in power balances. Stochastic power balances are implemented in a detailed model of the European electricity network, based on the generated samples. Finally, a Monte Carlo method is used to determine power flows and contingencies in the system. An indicator is elaborated in order to analyze risk of overloading and to prioritize necessary grid reinforcements. Implications for the Swiss power grid are investigated in detail, revealing that the current system is significantly put at risk in certain areas by the further integration of wind power in Europe. It is the first time that the results of a probabilistic model for wind energy are further deployed within a power system analysis of the interconnected European grid. The method presented in this paper allows to account for stochastic wind energy in a load flow analysis and to evaluate deterministic indicators, such as the N ??? 1 criterion, on a probabilistic basis. Thus, it constitutes an important extension of deterministic models.},
  comment   = {Another wind power coupla.  Compare with: Tastu15spcTimeTrajGaussCpla},
  doi       = {https://doi.org/10.1016/j.apenergy.2011.10.039},
  file      = {Hagspiel12copulaWindPowEur.pdf:Hagspiel12copulaWindPowEur.pdf:PDF},
  keywords  = {Stochastic wind power simulations, Monte Carlo method, Copula theory, Swiss power grid, Probabilistic N-1 analysis, Probabilistic transmission lines overload},
  owner     = {sotterson},
  timestamp = {2017.06.07},
  url       = {http://www.sciencedirect.com/science/article/pii/S0306261911006933},
}

@Article{Bremen07windDetProbModelComb,
  author    = {Lueder von Bremen},
  title     = {Combination of Deterministic and Probabilistic Meteorological Models to enhance Wind Farm Power Forecasts},
  journal   = {Journal of Physics: Conference Series},
  year      = {2007},
  volume    = {75},
  abstract  = {Large-scale wind farms will play an important role in the future worldwide energy
supply. However, with increasing wind power penetration all stakeholders on the electricity
market will ask for more skilful wind power predictions regarding save grid integration and to
increase the economic value of wind power. A Neural Network is used to calculate Model
Output Statistics (MOS) for each individual forecast model (ECMWF and HIRLAM) and to
model the aggregated power curve of the Middelgrunden offshore wind farm. We showed that
the combination of two NWP models clearly outperforms the better single model. The
normalized day-ahead RMSE forecast error for Middelgrunden can be reduced by 1 pct
compared to single ECMWF. This is a relative improvement of 6 pct. For lead times >24h it is
worthwhile to use a more sophisticated model combination approach than simple linear
weighting. The investigated principle component regression is able to extract the uncorrelated
information from two NWP forecasts. The spread of Ensemble Predictions is related to the skill
of wind power forecasts. Simple contingency diagrams show that low spread corresponds is
more often related to low forecast errors and high spread to large forecast errors.},
  comment   = {Ensemble spread directly related to point forecast skill. Something to do w/ PCA of ensembles too.

Is this a feature for a BMA or point probability forecast algorithm?},
  doi       = {10.1088/1742-6596/75/1/012050},
  file      = {:Bremen07windDetProbModelComb.pdf:PDF;Bremen07windDetProbModelComb.pdf:Bremen07windDetProbModelComb.pdf:PDF},
  groups    = {Ensemble, doReadNonWPV_1, Wind},
  owner     = {scotto},
  timestamp = {2008.07.04},
  url       = {http://www.iop.org/EJ/abstract/1742-6596/75/1/012050/},
}

@InCollection{Li05lasso2featVecMach,
  author    = {Fan Li and Yiming Yang and Eric Xing},
  title     = {From Lasso regression to Feature vector machine},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  publisher = {MIT Press},
  year      = {2006},
  editor    = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
  volume    = {18},
  pages     = {779--786},
  abstract  = {Lasso regression tends to assign zero weights to most irrelevant or redundant features, and hence is a promising technique for feature selection. Its limitation, however, is that it only offers solutions to linear models. Kernel machines with feature scaling techniques have been studied for feature selection with non-linear models. However, such approaches require to solve hard non-convex optimization problems. This paper proposes a new approach named the Feature Vector Machine (FVM). It reformulates the standard Lasso regression into a form isomorphic to SVM, and this form can be easily extended for feature selection with non-linear models by introducing kernels defined on feature vectors. FVM generates sparse solutions in the nonlinear feature space and it is much more tractable compared to feature scaling kernel machines. Our experiments with FVM on simulated data show encouraging results in identifying the small number of dominating features that are non-linearly correlated to the response, a task the standard Lasso fails to complete.},
  comment   = {Use kernel to make LASSO work for nonlinear * Figure 1 is example of where LASSO misses important feat which is correctly caught by this method},
  file      = {Li05lasso2featVecMach.pdf:Li05lasso2featVecMach.pdf:PDF;Li05lasso2featVecMach.pdf:Li05lasso2featVecMach.pdf:PDF},
  location  = {Cambridge, MA},
}

@Article{Landauer98introLSA,
  author    = {Landauer, Thomas K and Foltz, Peter W. and Laham, Darrell},
  title     = {An introduction to latent semantic analysis},
  journal   = {Discourse Processes},
  year      = {1998},
  volume    = {25},
  number    = {2-3},
  pages     = {259--284},
  abstract  = {Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the contextual usage meaning of words by statistical computations applied to a large corpus of text (Landauer & Dumais, 1997). The underlying idea is that the aggregate of all the word contexts in which a given word does and does not appear provides a set of mutual constraints that largely determines the similarity of meaning of words and sets of words to each other. The adequacy of LSA's reflection of human knowledge has been established in a variety of ways. For example, its scores overlap those of humans on standard vocabulary and subject matter tests; it mimics human word sorting and category judgments; it simulates word-word and passage-word lexical priming data; and, as reported in 3 following articles in this issue, it accurately estimates passage coherence, learnability of passages by individual students, and the quality and quantity of knowledge contained in an essay.},
  comment   = {LSA from a linguist's point of view. Could be a more intuitive way to explain SVD to somebody. Also might be good for clustering binary weather events, regime detection, who knows?},
  doi       = {10.1080/01638539809545028},
  eprint    = {http://www.tandfonline.com/doi/pdf/10.1080/01638539809545028},
  file      = {Landauer98introLSA.pdf:Landauer98introLSA.pdf:PDF},
  owner     = {scotto},
  timestamp = {2013.12.13},
}

@InProceedings{Fried04parCorrGraphDynLatent,
  author    = {Fried, R. and Didelez, V. and Lanius, V.},
  title     = {Partial Correlation Graphs and Dynamic Latent Variables for Physiological Time Series},
  booktitle = {Innovations in Classification, Data Science, and Information Systems (GfKl)},
  year      = {2004},
  pages     = {259266},
  publisher = {Springer Heidelberg-Berlin},
  abstract  = {Latent variable techniques are helpful to reduce high-dimensional time series to a few relevant variables that are easier to model and analyze. An inherent problem is the identifiability of the model and the interpretation of the latent variables. We apply graphical models to find the essential relations in the data and to deduce suitable assumptions leading to meaningful latent variables.},
  comment   = {Graphical model built with partial coherence used to dimension reduce before prediction. Standard dynamic principal component analysis works best, using the graph to pick the dimension, but I wonder if the graph is really better than other techniques, like Minka's (Minka00AutoPCAdim, speakerclust.bib). Also, since graph comes from ANY lag dependence, is there a chance that weak, single lags are missed?
On the other hand, I'm sure that spectral coherence PCA is much less computationally intensive than DPCA over all the the blown out lags.

Graphs cliques are determined using partial correlations in frequency (partial coherence)
 -- an edge if any FFT bin in cross-spectrum is non-zero
 -- So, there's an edge if there is a linear dependence at any lag
 -- deciding what to call "non-zero" sounds involved
 ---- outlier removal pre-processing
 ---- kernel method described in another paper
 -- also, correlated errors can make for fake edges
 ---- errors could be from non-linearities that aren't modeled (I think)
 ---- note that correlated errors is exactly what I'm looking for on the Dong spatial temporal proj: huh... What to do with the graph?
 1.) Variable selection: pick only one variable from each clique.
 2.) Standard DPCA: use 4 components, as suggested by the graph.
 -- Use correlation-based DPCA so get individual lags.
 -- Is a graph any better than using Minka's approach to selecting the num. of comps?
 3.) Latent factor extraction: Run dynamic PCA afterall, running it over each clique and picking 1\textsuperscript{st} princ. comp. from it. Again, they use time domain DPCA, I think.

PERFORMANCE COMPARISON
* Could the dimension reduced data reproduce the orignal ten hemodynamic measurements?
* Graph suggests 4 comps; pick them the 3 ways suggested above
* Repropduction: use "dynamic regression" to predict original measurements (includes new lag est?)

In the end, DPCA worked best but was the least interpretable.
Variable selection was most interpretable but performed worst.
Factor selection is a compromise.},
  doi       = {10.1007/3-540-26981-9_30},
  file      = {Fried04parCorrGraphDynLatent.pdf:Fried04parCorrGraphDynLatent.pdf:PDF;Fried04parCorrGraphDynLatent.pdf:Fried04parCorrGraphDynLatent.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.03.13},
  url       = {http://www.springerlink.com/content/h16t012jw3x115x3/?p=ca5e8d23c45c4bd6b2a9096c5e2a3781&pi=7},
}

@InCollection{Tomasev12HubShrKNN,
  author    = {Toma{\v{s}}ev, Nenad and Mladeni{\'c}, Dunja},
  title     = {Hubness-aware shared neighbor distances for high-dimensional k-nearest neighbor classification},
  booktitle = {Hybrid Artificial Intelligent Systems},
  publisher = {Springer},
  year      = {2012},
  pages     = {116--127},
  abstract  = {Learning from high-dimensional data is usually quite a challenging task, as captured by the well known phrase curse of dimensionality. Most distance-based methods become impaired due to the distance concentration of many widely used metrics in high-dimensional spaces. One recently proposed approach suggests that using secondary distances based on the number of shared k-nearest neighbors between different points might partly resolve the concentration issue, thereby improving overall performance. Nevertheless, the curse of dimensionality also affects the k-nearest neighbor inference in severely negative ways, one such consequence being known as hubness. The impact of hubness on forming shared neighbor distances has not been discussed before and it is what we focus on in this paper. Furthermore, we propose a new method for calculating the secondary distances which is aware of the underlying neighbor occurrence distribution. Our experiments suggest that this new approach achieves consistently superior performance on all considered high-dimensional data sets. An additional benefit is that it essentially requires no extra computations compared to the original methods.},
  comment   = {More on shared neighbhors and hubness. Criticises some otherA probabilistic approach to nearest-neighbor classification: naive hubness bayesian kNNu},
  doi       = {10.1007/978-3-642-28931-6_12},
  file      = {Tomasev12HubShrKNN.pdf:Tomasev12HubShrKNN.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.04.04},
}

@Article{Fraley09larLassoLargeDat,
  author    = {Fraley, Chris and Hesterberg, Tim},
  title     = {Least Angle Regression and LASSO for Large Datasets},
  journal   = {Statistal Analysis and Data Mining},
  year      = {2009},
  volume    = {1},
  number    = {4},
  pages     = {251--259},
  issn      = {1932-1864},
  abstract  = {Least angle regression and LASSO (L1-penalized regression) offer a number of advantages in variable selection applications over procedures such as stepwise or ridge regression, including prediction accuracy, stability, and interpretability. We discuss formulations of these algorithms that extend to datasets in which the number of observations could be so large that it would not be possible to access the matrix of predictors as a unit in computations. Our methods require a single pass through the data for orthogonal transformation, effectively reducing the dimension of the computations required to obtain the regression coefficients and residual sum of squares to the number of predictors, rather than the number of observations.},
  comment   = {How to split gigantic-dimension LARS/LASSO across multiple computers using orthogonal transforms (Tim Hesterberg paper) - - redundant predictors are removed inside of the algorithm, probably making it more robust to correlated predictors -- BUT does it make it more UNSTABLE? Does it have data temporal order or predictor vector arrangement order instability?},
  doi       = {10.1002/sam.v1:4},
  file      = {Fraley09larLassoLargeDat.pdf:Fraley09larLassoLargeDat.pdf:PDF;Fraley09larLassoLargeDat.pdf:Fraley09larLassoLargeDat.pdf:PDF},
  location  = {New York, NY, USA},
  owner     = {sotterson},
  publisher = {John Wiley \& Sons, Inc.},
  timestamp = {2009.08.13},
  url       = {http://portal.acm.org/citation.cfm?id=1526987.1526991&coll=Portal&dl=GUIDE&CFID=48565314&CFTOKEN=31318658#},
}

@Article{Hesterberg08larsLassoReview,
  author    = {Tim Hesterberg and Nam Hee Choi and Lukas Meier and Chris Fraley},
  title     = {Least angle and L1 penalized regression: A review},
  journal   = {Statistics Surveys},
  year      = {2008},
  volume    = {2},
  pages     = {61--93},
  abstract  = {Least Angle Regression is a promising technique for variable selection applications, offering a nice alternative to stepwise regression. It provides an explanation for the similar behavior of LASSO (?1-penalized regression) and forward stagewise regression, and provides a fast implementation of both. The idea has caught on rapidly, and sparked a great deal of research interest. In this paper, we give an overview of Least Angle Regression and the current state of related research.},
  comment   = {Tim Hesterberg's LARS-LASSO review, includes time series stuff. Time series * an input has effects at multiple time points (like a convolution) 1.) fused lasso: penalize difference between adjacent time points -- might be challenging for lots of points 2.) smoothed lasso -- more computationally feasible 3+) several other models mentioned, some especially for autoregressive models},
  doi       = {10.1214/08-SS035},
  file      = {Hesterberg08larsLassoReview.pdf:Hesterberg08larsLassoReview.pdf:PDF;Hesterberg08larsLassoReview.pdf:Hesterberg08larsLassoReview.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.08.13},
  url       = {http://www.projecteuclid.org/DPubS?verb=Display&version=1.0&service=UI&handle=euclid.ssu/1211317636&page=record},
}

@Article{Freitas15MachLrnClass,
  author    = {Nando de Freitas},
  title     = {Machine Learning Class},
  journal   = {University of Oxford},
  year      = {2015},
  abstract  = {Lecture 1: Introduction
Lecture 2: Linear prediction
Lecture 3: Maximum likelihood
Lectures 4 & 5: Regularizers, basis functions and cross-validation
Lecture 6: Optimisation
Lecture 7: Logistic regression
Lecture 8: Back-propagation and layer-wise design of neural nets
Lecture 9: Neural networks and deep learning with Torch
Lecture 10: Convolutional neural networks
Lecture 11: Max-margin learning and siamese networks
Lecture 12: Recurrent neural networks and LSTMs
Lecture 13: Hand-writing with recurrent neural networks (Guest speaker: Alex Graves from Google Deepmind, slides and video are missing
Lecture 14: Variational autoencoders and image generation (Guest speaker: Karol Gregor from Google Deepmind, slides and video are missing)
Lecture 15: Reinforcement learning with direct policy search (incl w/ lect 12?)
Lecture 16: Reinforcement learning with action-value functions (incl w/ lect 12?)},
  comment   = {Intro. machine learning class from Oxford U.  Includes slides, videos and homework.

Lecture 11: Recurrent nets and LSTM was recommended online somehwere
- good explaination of vanishing/exploding backprop derivative in standard RNN
- IDEA: make x_t gate input a vector of past time-window-averaged values,
  - increase window length so can so can smooth response to lagged values in the distant past.
  - like the time-lagged basis idea
  - also like a Haar wavelet, as in done in some load forecasting papers.
  - BUT maybe redundant w/ having many vertical layers, which I think can select both lags and maybe do some kind of temporal smoothing?
- explains Torch Code for LSTM cell and network
   - function for making one cell
   - another function for making a network of them
   - (with cell @ each time lag, I think).
   -- this is kind of like an n_layers order ARMA filter, since state or input influence ends after n_layers.
- LSTM RNN models are HUGE!  eg. 80K sofmax by 1000 dims (for NLP or something?)},
  file      = {Slides:Freitas15MachLrnClass_slides.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.03.10},
  url       = {https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/},
}

@Misc{stackexchange15cholRandGen,
  author    = {Stackexchange commenter},
  title     = {Generating correlated random numbers: Why does Cholesky decomposition work?},
  year      = {2015},
  abstract  = {Let's say I want to generate correlated random variables. I understand that I can use Cholesky decomposition
matrix to obtain the correlated values. If C is the correlation matrix, then we can do the cholesky decomposition:
LLT = C
Then I can easily generate correlated random variables:
LX = Y ,
where X are uncorrelated values and Y are correlated values. If I want two correlated random variables
L = [1 0 ]
?1?????2
I understand that this works, but I don't really understand why... My question is: Why does this work?},
  comment   = {Really short explanation of why/how to use Cholesky to generate RV's with a known variance.},
  file      = {stackexchange15cholRandGen.pdf:stackexchange15cholRandGen.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.12.22},
  url       = {http://math.stackexchange.com/questions/163470/generating-correlated-random-numbers-why-does-cholesky-decomposition-work},
}

@Article{Ueckerdt13systemLCOE,
  author    = {Ueckerdt, Falko and Hirth, Lion and Luderer, Gunnar and Edenhofer, Ottmar},
  title     = {System {LC}OE: What are the costs of variable renewables?},
  journal   = {Energy},
  year      = {2013},
  volume    = {63},
  pages     = {61--75},
  abstract  = {Levelized costs of electricity (LCOE) are a common metric for comparing
power generating technologies. However, there is qualified criticism particularly towards
evaluating variable renewables like wind and solar power based on LCOE because it
ignores integration costs that occur at the system level. In this paper we propose a new
measure System LCOE as the sum of generation and integration costs per unit of VRE.
For this purpose we develop a conclusive definition of integration costs. Furthermore we
decompose integration costs into different cost components and draw conclusions for
integration options like transmission grids and energy storage. System LCOE are
quantified from a power system model and a literature review. We find that at moderate
wind shares (~20%) integration costs can be in the same range as generation costs of
wind power and conventional plants. Integration costs further increase with growing wind
shares. We conclude that integration costs can become an economic barrier to deploying
VRE at high shares. This implies that an economic evaluation of VRE must not neglect
integration costs. A pure LCOE comparison would significantly underestimate the costs
of VRE at high shares. System LCOE give a framework of how to consistently account
for integration costs and thus guide policy makers and system planers in designing a cost-
efficient power system.
Index Terms ? renewable energy, integration costs, levelized costs of electricity, LCOE,
environmental economics, power generation economics, wind power, solar power,
electricity market, market integration},
  comment   = {Cost of energy including grid integration, quite a few cites. Shows how generic wind and solar system LCOE increase with penetration. Highest integration cost seems to be short term balancing type costs.},
  file      = {Ueckerdt13systemLCOE.pdf:Ueckerdt13systemLCOE.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.12.13},
  url       = {http://www.sciencedirect.com/science/article/pii/S0360544213009390},
}

@InProceedings{Arif09onlineLevMarqPowSys,
  author    = {J. Arif and N. Ray Chaudhuri and S. Ray and B. Chaudhuri},
  title     = {Online Levenberg-Marquardt algorithm for neural network based estimation and control of power systems},
  booktitle = {Proc. Int. Joint Conf. Neural Networks},
  year      = {2009},
  pages     = {199--206},
  month     = jun,
  abstract  = {Levenberg-Marquardt (LM) algorithm, a powerful off-line batch training method for neural networks, is adapted here for online estimation of power system dynamic behavior. A special form of neural network compatible with the feedback linearization framework is used to enable non-linear self-tuning control. Use of LM is shown to yield better closed-loop performance compared to conventional recursive least square (RLS) approach. For successive disturbance use of LM in conjunction with non-linear neural network structure yields faster convergence compared to RLS. A case study on a test system demonstrates the effectiveness of the online LM method for both linear and nonlinear estimation over RLS estimation (linear).},
  comment   = {Yes, Levenburg Marquardt can be run in online mode, not just batch.  Could be used for NN adaptiation.},
  doi       = {10.1109/IJCNN.2009.5179071},
  file      = {:Arif09onlineLevMarqPowSys.pdf:PDF},
  issn      = {2161-4393},
  keywords  = {adaptive control, closed loop systems, convergence of numerical methods, feedback, learning (artificial intelligence), learning systems, least squares approximations, linearisation techniques, neurocontrollers, nonlinear control systems, nonlinear dynamical systems, nonlinear estimation, power system control, recursive estimation, self-adjusting systems, variable structure systems, RLS, closed-loop system, convergence, feedback linearization framework, linear estimation, neural network, nonlinear estimation, nonlinear self-tuning control, off-line batch training method, online Levenberg-Marquardt algorithm, power system dynamic behavior control, recursive least square approach, sliding window mode, Control systems, Convergence, Least squares methods, Linear feedback control systems, Neural networks, Neurofeedback, Power system control, Power system dynamics, Power systems, Resonance light scattering, Damping, Feedback linearization, Levenberg-Marquardt, Power system oscillations, Self-tuning controller},
}

@Article{Gould13limitOrderBks,
  author    = {Gould, Martin D and Porter, Mason A and Williams, Stacy and McDonald, Mark and Fenn, Daniel J and Howison, Sam D},
  title     = {Limit order books},
  journal   = {Quantitative Finance},
  year      = {2013},
  volume    = {13},
  number    = {11},
  pages     = {1709--1742},
  abstract  = {Limit order books (LOBs) match buyers and sellers in more than half of the world?s financial markets. This survey highlights the insights that have emerged from the wealth of empirical and theoretical studies of LOBs. We examine the findings reported by statistical analyses of historical LOB data and discuss how several LOB models provide insight into certain aspects of the mechanism. We also illustrate that many such models poorly resemble real LOBs and that several well-established empirical facts have yet to be reproduced satisfactorily. Finally, we identify several key unresolved questions about LOBs.

Keywords
Limit order books, Data analysis, Modelling, Stylized facts, Complex systems},
  comment   = {How power bids are handled on DE EPEX Spot Intraday Market, I believe.  Compares statistical and economic market model approaches.  This is the paper than Henry Martin found.

Toke11mktMkingOBsprd is one use refered to here that uses the Hawkes Process model.

See Smirni16cs426dscrtEvntSimCrsNotes for Poisson arrival stuff},
  doi       = {10.1080/14697688.2013.803148},
  file      = {Gould13limitOrderBks.pdf:Gould13limitOrderBks.pdf:PDF},
  publisher = {Taylor \& Francis},
}

@InBook{Gelman06linRgrssnIntrctCh3,
  chapter   = {Chapter 3: Linear regression: the basics},
  pages     = {31--51},
  title     = {Data analysis using regression and multilevel/hierarchical models},
  publisher = {Cambridge University Press},
  year      = {2006},
  author    = {Gelman, Andrew and Hill, Jennifer},
  abstract  = {Linear regression is a method that smmwtrizes how the average values of a numerical
o'Utcurne variable vary overr subpopulations defined by linear functions of pn:rlictors.
Introductory statistics a.nd regression texts often tocus on how regression can be
used to represent relationships between variables, rather than as a comparison of
average outcomes. l3y focusing on regression a.-; a comparison of averages, we are
being explicit about its limitations for defining these relationships causally, an issue
to which we return i~1 Chapter 9. Regression can be used to predict an outcome
given a lint'ar function of these predictors, and regression coefficients can be thought
of as comparisons across predicted values or a,.., comparisons among averages in the
data.},
  comment   = {Linear regression and interactions. Good example of why need them:

p. 36: In practice, inputs that have large main effects also tend to have large interactions with other inputs (however, small main effects do not preclude the possibility of large interactions). For example, smoking has a huge effect on cancer. In epidemiologial studies of other carcinogens, it is crucial to adjust for smoking both as a main effect and as an interaction[...]: high levels of radon are associated with greater likelihood of cancer but this difference is much greater for smokers than for nonsmokers.

Including interactions is a way to allow a model to be fit differently to different
subsets of data. These two approaches are related, as we discuss later in the context
of multilevel models.},
  file      = {Gelman06linRgrssnIntrctCh3.pdf:Gelman06linRgrssnIntrctCh3.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.02.17},
  url       = {http://faculty.fuqua.duke.edu/~charlesw/s591/Methods/c02_Charlie/gelman\&hill_chapter3.pdf},
}

@Article{Garcia10cmpltLazyLrn,
  author    = {Garcia, E.K. and Feldman, S. and Gupta, M.R. and Srivastava, S.},
  title     = {Completely Lazy Learning},
  journal   = {IEEE Transactions on Knowledge and Data Engineering},
  year      = {2010},
  volume    = {22},
  number    = {9},
  pages     = {1274--1285},
  abstract  = {Local classifiers are sometimes called lazy learners because they do not train a classifier until presented with a test sample. However, such methods are generally not completely lazy because the neighborhood size k (or other locality parameter) is usually chosen by cross validation on the training set, which can require significant preprocessing and risks overfitting. We propose a simple alternative to cross validation of the neighborhood size that requires no preprocessing: instead of committing to one neighborhood size, average the discriminants for multiple neighborhoods. We show that this forms an expected estimated posterior that minimizes the expected Bregman loss with respect to the uncertainty about the neighborhood choice. We analyze this approach for six standard and state-of-the-art local classifiers, including discriminative adaptive metric kNN (DANN), a local support vector machine (SVM-KNN), hyperplane distance nearest neighbor (HKNN), and a new local Bayesian quadratic discriminant analysis (local BDA). The empirical effectiveness of this technique versus cross validation is confirmed with experiments on seven benchmark data sets, showing that similar classification performance can be attained without any training.
Index Terms?lazy learning, Bayesian estimation, cross-validation, local learning, quadratic discriminant analysis},
  comment   = {KNN classifier without selection of K. Trick is to put a prior on k and then run Bayesian max likelihood estimation with log-uniform k prior. Result is called "Bayesian neighborhoods." But applications are all classification (although local linear regression is mentioned) and the full equations are not given. Maybe this will take me too long to implement?

Also see: Gupta08adaptLocLinKNN},
  doi       = {10.1109/TKDE.2009.159},
  file      = {Garcia10cmpltLazyLrn.pdf:Garcia10cmpltLazyLrn.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2014.04.19},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5161262},
}

@Article{Gupta08adaptLocLinKNN,
  author    = {Gupta, M.R. and Garcia, E.K. and Chin, E.},
  title     = {Adaptive Local Linear Regression With Application to Printer Color Management},
  journal   = {IEEE Transactions on Image Processing},
  year      = {2008},
  volume    = {17},
  number    = {6},
  pages     = {936--945},
  abstract  = {Local learning methods, such as local linear regression and nearest neighbor classifiers, base estimates on nearby training samples, neighbors. Usually, the number of neighbors used in estimation is fixed to be a global ldquooptimalrdquo value, chosen by cross validation. This paper proposes adapting the number of neighbors used for estimation to the local geometry of the data, without need for cross validation. The term enclosing neighborhood is introduced to describe a set of neighbors whose convex hull contains the test point when possible. It is proven that enclosing neighborhoods yield bounded estimation variance under some assumptions. Three such enclosing neighborhood definitions are presented: natural neighbors, natural neighbors inclusive, and enclosing k-NN. The effectiveness of these neighborhood definitions with local linear regression is tested for estimating lookup tables for color management. Significant improvements in error metrics are shown, indicating that enclosing neighborhoods may be a promising adaptive neighborhood definition for other local learning tasks as well, depending on the density of training samples.},
  comment   = {Test point-wise convex hull KNN nhbd k-picker good for low dims, and is cross-validation free. Not good for high dim but maybe Mendez13hiDimVoronoiAdj can help? Tons of references to KNN distance scalers that might do the same thing but work in high dims. Also did ridge regression while doing local linear, which helped.

Why select a different K for each point?
* When using KNN to find local linear regression triaining point neighbors can lead to a high variance output, which can be bounded if training points contain the test point in their convex hull. T
* This can also be done without cross-validation.
* intuition is that interpolation is better than extrapolation

Application: color printer lookup table
* 3D (good for convex hull)
* Don't need the fancy isotropic distance scalars since the colorspace is already correctly dimensioned for human perception
* cross validation not possible, or easy

PROBLEMS WITH THESE IDEAS
* convex hull grows exponentially with dimension (tests in Eweline/frcst/common/test)
* voronoi neighborhoods increase even fastter
* SO: only good for low dim problems, say under 7 for convex hull, maybe 5 or six for voronoi.

RESULTS
* good, I think, when compared against a fixed value of k=15 (known to be good from past work)
* ridge regression often helped
* had to sometimes require a min of k=15 to avoid big errors (assumptions of a Theorem weren't quite right).
* probably the "enclosing k-NN min 15" approach was best but not overwhelmingly
* the Voronoi-based natural neighbors ideas was sometimes good too.

Also see: Garcia10cmpltLazyLrn},
  doi       = {10.1109/TIP.2008.922429},
  file      = {Gupta08adaptLocLinKNN.pdf:Gupta08adaptLocLinKNN.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2014.04.19},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4515970},
}

@InProceedings{Ormoneit99optKernLocLinRgrsn,
  author    = {Ormoneit, Dirk and Hastie, Trevor},
  title     = {Optimal kernel shapes for local linear regression},
  booktitle = {NIPS},
  year      = {1999},
  pages     = {540--546},
  abstract  = {Local linear regression performs very well in many low-dimensional forecasting
problems. In high-dimensional spaces, its performance typically decays due to the well-known
"curse-of-dimensionality". Specifically, the volume of a weighting kernel that contains a fixed
number of samples increases exponentially with the number of dimensions. The bias of a local
linear estimate may thus become unacceptable for many real-world data sets. A possible way to
control the bias is by varying the "shape" of the weighting kernel. In this work we suggest a new,
data-driven method to estimating the optimal kernel shape. Experiments using two artificially
generated data sets and data from the UC Irvine repository show the benefits of kernel shaping.
Keywords: Local Linear Regression, Smoothing, Lazy Learning, Entropy, Bandwidth Selection,
Nearest Neighbors, Projection Pursuit Regression, Sliced Inverse Regression},
  comment   = {Quite old article on local linear regression kernel shapes -- Mexican hat and stuff -- but maybe a reference for the fundamental problem? But maybe it is also obsoleted by deep learning stuff?},
  file      = {Ormoneit99optKernLocLinRgrsn.pdf:Ormoneit99optKernLocLinRgrsn.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.04.23},
  url       = {https://statistics.stanford.edu/sites/default/files/1999-11.pdf},
}

@PhdThesis{Taylor12locRgrsnStrat,
  author      = {Taylor, James},
  title       = {Strategies for mean and modal multivariate local regression},
  year        = {2012},
  abstract    = {Local polynomial fitting for univariate data has been widely studied and discussed,
but up until now the multivariate equivalent has often been deemed
impractical, due to the so-called curse of dimensionality. Here, rather than
discounting it completely, density is used as a threshold to determine where
over a data range reliable multivariate smoothing is possible, whilst accepting
that in large areas it is not. Further, the challenging issue of multivariate
bandwidth selection, which is known to be affected detrimentally by sparse
data which inevitably arise in higher dimensions, is considered. In an effort
to alleviate this problem, two adaptations to generalized cross-validation are
implemented, and a simulation study is presented to support the proposed
method. It is also discussed how the density threshold and the adapted generalized
cross-validation technique introduced herein work neatly together.
Whilst this is the major focus of this thesis, modal regression via mean shift
is discussed as an alternative multivariate regression technique. In a slightly
different vein, bandwidth selection for univariate kernel density estimation
is also examined, and a different technique is proposed for a density with a
multimodal distribution. This is supported by a simulation study and its
relevance in modal regression is also discussed.},
  comment     = {PhD thesis corresponding to: Taylor13dimCurseLocRgrsn

How to kind of get around curse of dimensionality w/ local regression models + some other stuff in the title that I don't know about yet.},
  file        = {Taylor12locRgrsnStrat.pdf:Taylor12locRgrsnStrat.pdf:PDF},
  groups      = {PointDerived, doReadNonWPV_1},
  institution = {Durham University},
  owner       = {sotterson},
  timestamp   = {2014.02.06},
  url         = {http://etheses.dur.ac.uk/3514/},
}

@Article{Taylor13dimCurseLocRgrsn,
  author    = {Taylor, James and Einbeck, Jochen},
  title     = {Challenging the curse of dimensionality in multivariate local linear regression},
  journal   = {Computational Statistics},
  year      = {2013},
  volume    = {28},
  number    = {3},
  pages     = {955--976},
  issn      = {0943-4062},
  abstract  = {Local polynomial fitting for univariate data has been widely studied and discussed, but up until now the multivariate equivalent has often been deemed impractical, due to the so-called curse of dimensionality. Here, rather than discounting it completely, we use density as a threshold to determine where over a data range reliable multivariate smoothing is possible, whilst accepting that in large areas it is not. The merits of a density threshold derived from the asymptotic influence function are shown using both real and simulated data sets. Further, the challenging issue of multivariate bandwidth selection, which is known to be affected detrimentally by sparse data which inevitably arise in higher dimensions, is considered. In an effort to alleviate this problem, two adaptations to generalized cross-validation are implemented, and a simulation study is presented to support the proposed method. It is also discussed how the density threshold and the adapted generalized cross-validation technique introduced herein work neatly together.},
  comment   = {Maybe how to avoid additive QR model probs w/ local model cross-validation. Local linear or polynomial regression breaks down w/ more than a couple dims; a cross-validation technique shows how to identify spaces where there are enough points. Seems to partly get around the curse of dimensionality.

The C.O.D is why DTU prefers additive models for multivariate quantile regression. e.g. Nielsen06quantRegr

Another way is kernel shaping:
http://robotics.stanford.edu/~ormoneit/research/node1.html

Author's PhD thesis explains the idea here (I think) + other stuff that I don't know about yet: Taylor12locRgrsnStrat},
  doi       = {10.1007/s00180-012-0342-0},
  file      = {:Taylor13dimCurseLocRgrsn.pdf:PDF},
  keywords  = {Multivariate smoothing; Density estimation; Bandwidth selection; Influence function},
  language  = {English},
  owner     = {sotterson},
  publisher = {Springer-Verlag},
  timestamp = {2014.02.14},
}

@Article{Gneiting11quantOptFrcst,
  author    = {Tilmann Gneiting},
  title     = {Quantiles as optimal point forecasts},
  journal   = {International Journal of Forecasting},
  year      = {2011},
  volume    = {27},
  number    = {2},
  pages     = {197--207},
  issn      = {0169-2070},
  abstract  = {Loss functions play a central role in the theory and practice of forecasting. If the loss function is quadratic, the mean of the predictive distribution is the unique optimal point predictor. If the loss is symmetric piecewise linear, any median is an optimal point forecast. Quantiles arise as optimal point forecasts under a general class of economically relevant loss functions, which nests the asymmetric piecewise linear loss, and which we refer to as generalized piecewise linear (GPL). The level of the quantile depends on a generic asymmetry parameter which reflects the possibly distinct costs of underprediction and overprediction. Conversely, a loss function for which quantiles are optimal point forecasts is necessarily GPL. We review characterizations of this type in the work of Thomson, Saerens and Komunjer, and relate to proper scoring rules, incentive-compatible compensation schemes and quantile regression. In the empirical part of the paper, the relevance of decision theoretic guidance in the transition from a predictive distribution to a point forecast is illustrated using the Bank of England's density forecasts of United Kingdom inflation rates, and probabilistic predictions of wind energy resources in the Pacific Northwest.},
  comment   = {I think this says that a quantile forecast yields the optimal point forecast matching some error metric e.g. if the cost function is asymmetric, the quantile forecast can take this into account. Examples from wind power. I haven't read it yet.

Related paper, along with web commentary: Gneiting11ptFrcstMkEval},
  doi       = {10.1016/j.ijforecast.2009.12.015},
  file      = {Tech Report (2008):Gneiting11quantOptFrcst_TR.pdf:PDF},
  groups    = {Test, Use, doReadWPV_1},
  keywords  = {Decision making},
  owner     = {sotterson},
  timestamp = {2013.10.23},
  url       = {http://www.sciencedirect.com/science/article/pii/S0169207010000063},
}

@Article{Salehin09eigLungCirc,
  author      = {Salehin, S. and Abhayapala, Thushara},
  title       = {Localizing Lung Sounds: Eigen Basis Decomposition for Localizing Sources Within a Circular Array of Sensors},
  journal     = {Journal of Signal Processing Systems},
  year        = {2009},
  pages       = {1--17},
  issn        = {1939-8018},
  abstract    = {Lung disorders or injury can result in changes in the production of lung sounds both spectrally and regionally. Localizing these lung sounds can provide information to the extent and location of the disorder. Difference in arrival times at a set of sensors and triangulation were previously proposed for acoustic imaging of the chest. We propose two algorithms for acoustic imaging using a set of eigen basis functions of the Helmholtz wave equation. These algorithms remove the sensor location contribution from the multi sensor recordings using either an orthogonality property or a least squares based estimation after which a spatial minimum variance (MV) spectrum is applied to estimate the source locations. The use of these eigen basis functions allows possible extension to a lung sound model consisting of layered cylindrical media. Theoretical analysis of the relationship of resolution to frequency and noise power was derived and simulations verified the results obtained. Further, a Nyquist?s criteria for localizing sources within a circular array shows that the radius of region where sources can be localized is inversely proportional to the frequency of sound.The resolution analysis and modified Nyquist criteria can be used for determining the number of sensors required at a given noise level, for a required resolution, frequency range, and radius of region for which sources need to be localized.},
  affiliation = {Australian National University Applied Signal Processing Group, Research School of Information Sciences and Engineering (RSISE) Canberra Australian Capital Territory 0200 Australia},
  comment     = {cylindrical harmonics for 2D sound localization
- cylindrical harmonics could be be a spd/dir basis function for a wind measurement's regression input
- probably more efficient than brute force inclusion of basis-projected variables in a linear regression input
- only 2D so a partial solution to lagged wind velocity basis function
---- Sernelius10laplacePoisson has 3D; maybe Padoan083dHarmLect2
- better than Tewfik87eigCylindHarm ?},
  doi         = {10.1007/s11265-009-0435-3},
  file        = {Salehin09eigLungCirc.pdf:Salehin09eigLungCirc.pdf:PDF},
  keyword     = {Electrical Engineering},
  owner       = {scotto},
  publisher   = {Springer New York},
  timestamp   = {2010.08.19},
}

@Article{Domingos12afutkMachLrn,
  author   = {Domingos, Pedro},
  title    = {A Few Useful Things to Know About Machine Learning},
  year     = {2012},
  volume   = {55},
  number   = {10},
  pages    = {78--87},
  issn     = {0001-0782},
  doi      = {10.1145/2347736.2347755},
  url      = {http://doi.acm.org/10.1145/2347736.2347755},
  urldate  = {2016-10-31},
  abstract = {Machine learning algorithms can figure out how to perform
important tasks by generalizing from examples. This is of-
ten feasible and cost-effective where manual programming
is not. As more data becomes available, more ambitious
problems can be tackled. As a result, machine learning is
widely used in computer science and other fields. However,
developing successful machine learning applications requires
a substantial amount of ?black art? that is hard to find in
textbooks. This article summarizes twelve key lessons that
machine learning researchers and practitioners have learned.
These include pitfalls to avoid, important issues to focus on,
and answers to common questions.},
  file     = {Domingos12afutkMachLrn.pdf:Domingos12afutkMachLrn.pdf:PDF},
  journal  = {Commun. {ACM}},
}

@Article{Coulston16approxUncertRandFrstRgrssn,
  author    = {Coulston, John W and Blinn, Christine E and Thomas, Valerie A and Wynne, Randolph H},
  title     = {Approximating prediction uncertainty for random forest regression models},
  journal   = {Photogrammetric Engineering \& Remote Sensing},
  year      = {2016},
  volume    = {82},
  number    = {3},
  pages     = {189--197},
  abstract  = {Machine learning approaches such as random forest have increased for the spatial modeling and mapping of continuous variables. Random forest is a non-parametric ensemble approach, and unlike traditional regression approaches there is no direct quantification of prediction error. Understanding prediction uncertainty is important when using model-based continuous maps as inputs to other modeling applications such as fire modeling. Here we use a Monte Carlo approach to quantify prediction uncertainty for random forest regression models. We test the approach by simulating maps of dependent and independent variables with known characteristics and comparing actual errors with prediction errors. Our approach produced conservative prediction intervals across most of the range of predicted values. However, because the Monte Carlo approach was data driven, prediction intervals were either too wide or too narrow in sparse parts of the prediction distribution. Overall, our approach provides reasonable estimates of prediction uncertainty for random forest regression models. },
  comment   = {Somehow uses MCMC to make confidence intervals on random forest regression results.  Evaluation is heuristic and "look at plots," so it's not the best paper.},
  file      = {:Coulston16approxUncertRandFrstRgrssn.pdf:PDF},
  publisher = {American Society for Photogrammetry and Remote Sensing},
  url       = {https://www.ingentaconnect.com/content/asprs/pers/2016/00000082/00000003/art00016},
}

@Misc{Viola18clustElecProfileKmeansPython,
  author       = {Luciano Guivant Viola},
  title        = {Clustering electricity usage profiles with K-means},
  howpublished = {Medium: Towards Data Science},
  month        = sep,
  year         = {2018},
  abstract     = {Machine Learning has a wide range of applications for the energy sector. A very exciting one is extracting insights into electricity consumption behavior. The way in which an individual or family uses energy across the day is also known as “energy fingerprint”.

In this article, we will go through how to find patterns in the daily load profiles of a single household with the K-means clustering algorithm.

The dataset contains 2075259 measurements gathered between December 2006 and November 2010 (47 months). You can find it here.},
  comment      = {Super short example of load profile clustering with Python.  A quick way to get a baseline.},
  file         = {:Viola18clustElecProfileKmeansPython.pdf:PDF},
}

@Article{Giabardo10feedbackElecMkt,
  author    = {Paolo Giabardo and Marco Zugno and Pierre Pinson and Henrik Madsen},
  title     = {Feedback, competition and stochasticity in a day ahead electricity market},
  journal   = {Energy Economics},
  year      = {2010},
  volume    = {32},
  number    = {2},
  pages     = {292--301},
  issn      = {0140-9883},
  abstract  = {Major recent changes in electricity markets relate to the process for their deregulation, along with increasing participation of renewable (stochastic) generation e.g. wind power. Our general objective is to model how feedback, competition and stochasticity (on the production side) interact in electricity markets, and eventually assess what their effects are on both the participants and the society. For this, day ahead electricity markets are modeled as dynamic closed loop systems, in which the feedback signal is the market price. In parallel, the Cournot competition model is considered. Mixed portfolios with significant share of renewable energy are based on stochastic threshold cost functions. Regarding trading strategies, it is assumed that generators are looking at optimizing their individual profits. The point of view of the society is addressed by analyzing market behavior and stability. The performed simulations show the beneficial effects of employing long term bidding strategies for both generators and society. Sensitivity analyses are performed in order to evaluate the effects of demand elasticity. It is shown that increase in demand elasticity reduces the possibility for the generators to exploit their market power. Furthermore, the results suggest that introduction of wind power generation in the market is beneficial both for the generators and the society.},
  comment   = {A market model the simulates the feedback loop where energy sold effects price, which effects the amount sold,....

Tryggvi had a 2013 price forecasting paper (Jonsson13elecPriceQR). Did it use this idea?},
  doi       = {10.1016/j.eneco.2009.09.006},
  file      = {Submitted in 2009 (editable):Giabardo10feedbackElecMkt_submitted.pdf:PDF;Journal article 2010 (not editable):Giabardo10feedbackElecMkt.pdf:PDF},
  keywords  = {Electricity markets},
  owner     = {sotterson},
  timestamp = {2015.03.12},
  url       = {http://www.sciencedirect.com/science/article/pii/S0140988309001625},
}

@Article{Zarwi17discChoiceAdoptDiffTranspo,
  author   = {Feras El Zarwi and Akshay Vij and Joan L. Walker},
  title    = {A discrete choice framework for modeling and forecasting the adoption and diffusion of new transportation services},
  journal  = {Transportation Research Part C: Emerging Technologies},
  year     = {2017},
  volume   = {79},
  pages    = {207 - 223},
  issn     = {0968-090X},
  abstract = {Major technological and infrastructural changes over the next decades, such as the introduction of autonomous vehicles, implementation of mileage-based fees, carsharing and ridesharing are expected to have a profound impact on lifestyles and travel behavior. Current travel demand models are unable to predict long-range trends in travel behavior as they do not entail a mechanism that projects membership and market share of new modes of transport (Uber, Lyft, etc.). We propose integrating discrete choice and technology adoption models to address the aforementioned issue. In order to do so, we build on the formulation of discrete mixture models and specifically Latent Class Choice Models (LCCMs), which were integrated with a network effect model. The network effect model quantifies the impact of the spatial/network effect of the new technology on the utility of adoption. We adopted a confirmatory approach to estimating our dynamic LCCM based on findings from the technology diffusion literature that focus on defining two distinct types of adopters: innovator/early adopters and imitators. LCCMs allow for heterogeneity in the utility of adoption for the various market segments i.e. innovators/early adopters, imitators and non-adopters. We make use of revealed preference (RP) time series data from a one-way carsharing system in a major city in the United States to estimate model parameters. The data entails a complete set of member enrollment for the carsharing service for a time period of 2.5years after being launched. Consistent with the technology diffusion literature, our model identifies three latent classes whose utility of adoption have a well-defined set of preferences that are significant and behaviorally consistent. The technology adoption model predicts the probability that a certain individual will adopt the service at a certain time period, and is explained by social influences, network effect, socio-demographics and level-of-service attributes. Finally, the model was calibrated and then used to forecast adoption of the carsharing system for potential investment strategy scenarios. A couple of takeaways from the adoption forecasts were: (1) placing a new station/pod for the carsharing system outside a major technology firm induces the highest expected increase in the monthly number of adopters; and (2) no significant difference in the expected number of monthly adopters for the downtown region will exist between having a station or on-street parking.},
  comment  = {Compares Bass diffusion model to their new model for electric vehicle (among other thrings) adoption.

Sounds similar to SantaEulalia11discrChoiceBassEV},
  doi      = {https://doi.org/10.1016/j.trc.2017.03.004},
  file     = {:Zarwi17discChoiceAdoptDiffTranspo.pdf:PDF},
  keywords = {Technology diffusion, Dynamics, Latent class choice models, Social influences, Spatial effect, Demand forecasting},
  url      = {http://www.sciencedirect.com/science/article/pii/S0968090X17300694},
}

@InProceedings{Mairal13stochMMlargeOpt,
  author    = {Mairal, Julien},
  title     = {Stochastic majorization-minimization algorithms for large-scale optimization},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2013},
  pages     = {2283--2291},
  abstract  = {Majorization-minimization algorithms consist of iteratively minimizing a majorizing
surrogate of an objective function. Because of its simplicity and its wide
applicability, this principle has been very popular in statistics and in signal processing.
In this paper, we intend to make this principle scalable. We introduce
a stochastic majorization-minimization scheme which is able to deal with largescale
or possibly infinite data sets. When applied to convex optimization problems
under suitable assumptions, we show that it achieves an expected convergence
rate of O(1/?n) after n iterations, and of O(1/n) for strongly convex functions.
Equally important, our scheme almost surely converges to stationary points for
a large class of non-convex problems. We develop several efficient algorithms
based on our framework. First, we propose a new stochastic proximal gradient
method, which experimentally matches state-of-the-art solvers for large-scale ?1-
logistic regression. Second, we develop an online DC programming algorithm for
non-convex sparse estimation. Finally, we demonstrate the effectiveness of our
approach for solving large-scale structured matrix factorization problems},
  comment   = {Stochastic optimization technique that works for millions of points. Can do Expectation Maximization with this, or quantile regression, as in Hunter00qrMMalg. Seems to be good for sparse learning too.

Has C++ code},
  file      = {Mairal13stochMMlargeOpt.pdf:Mairal13stochMMlargeOpt.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.07},
  url       = {http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0CCsQFjAB&url=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5129-stochastic-majorization-minimization-algorithms-for-large-scale-optimization.pdf&ei=DORcVNPCD4HraKSzgMAG&usg=AFQjCNGqa6q5ysc5TcQmxcVja6vJGx3u3A&bvm=bv.79184187,d.d2s&cad=rja},
}

@TechReport{Monteiro09windFrcstStArt,
  author      = {Monteiro, C and Bessa, R and Miranda, V and Botterud, A and Wang, J and Conzelmann, G and others},
  title       = {Wind power forecasting: state-of-the-art 2009.},
  institution = {Argonne National Laboratory (ANL)},
  year        = {2009},
  type        = {Tech Report},
  number      = {ANL/DIS-10-1},
  month       = nov,
  abstract    = {Many countries and regions are introducing policies aimed at reducing the environmental footprint from the energy sector and increasing the use of renewable energy. In the United States, a number of initiatives have been taken at the state level, from renewable portfolio standards (RPSs) and renewable energy certificates (RECs), to regional greenhouse gas emission control schemes. Within the U.S. Federal government, new energy and environmental policies and goals are also being crafted, and these are likely to increase the use of renewable energy substantially. The European Union is pursuing implementation of its ambitious 20/20/20 targets, which aim (by 2020) to reduce greenhouse gas emissions by 20\% (as compared to 1990), increase the amount of renewable energy to 20\% of the energy supply, and reduce the overall energy consumption by 20\% through energy efficiency. With the current focus on energy and the environment, efficient integration of renewable energy into the electric power system is becoming increasingly important. In a recent report, the U.S. Department of Energy (DOE) describes a model-based scenario, in which wind energy provides 20\% of the U.S. electricity demand in 2030. The report discusses a set of technical and economic challenges that have to be overcome for this scenario to unfold. In Europe, several countries already have a high penetration of wind power (i.e., in the range of 7 to 20\% of electricity consumption in countries such as Germany, Spain, Portugal, and Denmark). The rapid growth in installed wind power capacity is expected to continue in the United States as well as in Europe. A large-scale introduction of wind power causes a number of challenges for electricity market and power system operators who will have to deal with the variability and uncertainty in wind power generation when making their scheduling and dispatch decisions. Wind power forecasting (WPF) is frequently identified as an important tool to address the variability and uncertainty in wind power and to more efficiently operate power systems with large wind power penetrations. Moreover, in a market environment, the wind power contribution to the generation portofolio becomes important in determining the daily and hourly prices, as variations in the estimated wind power will influence the clearing prices for both energy and operating reserves. With the increasing penetration of wind power, WPF is quickly becoming an important topic for the electric power industry. System operators (SOs), generating companies (GENCOs), and regulators all support efforts to develop better, more reliable and accurate forecasting models. Wind farm owners and operators also benefit from better wind power prediction to support competitive participation in electricity markets against more stable and dispatchable energy sources. In general, WPF can be used for a number of purposes, such as: generation and transmission maintenance planning, determination of operating reserve requirements, unit commitment, economic dispatch, energy storage optimization (e.g., pumped hydro storage), and energy trading. The objective of this report is to review and analyze state-of-the-art WPF models and their application to power systems operations. We first give a detailed description of the methodologies underlying state-of-the-art WPF models. We then look at how WPF can be integrated into power system operations, with specific focus on the unit commitment problem. Subject:17 WIND ENERGY; 24 POWER TRANSMISSION AND DISTRIBUTION; 29 ENERGY PLANNING, POLICY AND ECONOMY; ECONOMICS; ELECTRIC POWER INDUSTRY; ELECTRICITY; ENERGY CONSUMPTION; ENERGY EFFICIENCY; ENERGY SOURCES; ENERGY STORAGE; FORECASTING; GREENHOUSE GASES; IMPLEMENTATION; MAINTENANCE; MARKET; NATIONAL GOVERNMENT; OPTIMIZATION; PLANNING; POWER SYSTEMS; STORAGE; WIND POWER; WIND TURBINE ARRAYS},
  comment     = {A big section on regime learning/detection and the effect of regimes on forecast errors. I think the forecasting test case of Alberta is mentioned. Wasn't this the contest where 3TIER lost badly?},
  doi         = {DOI 10.2172/968212},
  file        = {Monteiro09windFrcstStArt.pdf:Monteiro09windFrcstStArt.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2013.03.13},
  url         = {http://www.osti.gov/energycitations/product.biblio.jsp?osti_id=968212},
}

@Article{Fearnhead10semiAutoABC,
  author    = {Fearnhead, Paul and Prangle, Dennis},
  title     = {Semi-automatic approximate {Bayes}ian computation},
  journal   = {Arxiv preprint arXiv},
  year      = {2010},
  volume    = {1004},
  abstract  = {Many modern statistical applications involve inference for complex stochastic models,
where it is easy to simulate from the models, but impossible to calculate likelihoods.
Approximate Bayesian Computation (ABC) is a method of inference for such models.
It replaces calculation of the likelihood by a step which involves simulating artificial
data for different parameter values, and comparing summary statistics of the simulated
data to summary statistics of the observed data. Here we show how to construct
appropriate summary statistics for ABC in a semi-automatic manner. Theoretical results
show that, in some sense, optimal summary statistics are the posterior means
of the parameters. While these cannot be calculated analytically, we propose using
an extra stage of simulation to estimate how the posterior means vary as a function
of the data; and then use these estimates of our summary statistics within ABC. Our
approach compares with the current norm of the person implementing ABC choosing
summary statistics that they think are informative about the parameters. Empirical
results, based on two examples from the literature, show that our simulation-based
approach to choosing summary statistics can be orders of magnitude more accurate
than this alternative.
Keywords: Epidemics, Indirect Inference, Likelihood-free inference, Markov chain
Monte Carlo, Population genetics, Simulation},
  comment   = {An Approximate Bayesian Computation paper that does some kinda feature selection to pick the best summary stats. Is much more accurate than ad hoc methods in the literature. Combined w/ auto-bandwidth selection, ABC might be usable.

ABC is good when don't know the true error distribution. Maybe good for forecasting. Could be another way of evaluating probabilistic forecasts.

Was eventually published here: Fearnhead12semiAutoABCconstr

Some intro slides by Prangle are here: Prangle11sumStatSeqABC},
  file      = {Fearnhead10semiAutoABC.pdf:Fearnhead10semiAutoABC.pdf:PDF},
  groups    = {Ensemble, PointDerived, Test, doReadNonWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.04},
}

@PhdThesis{Prangle11sumStatSeqABC,
  author      = {Prangle, Dennis},
  title       = {Summary statistics and sequential methods for approximate {Bayes}ian computation},
  year        = {2011},
  abstract    = {Many modern statistical applications involve inference for complex stochastic models,
where it is easy to simulate from the models, but impossible to calculate likelihoods.
Approximate Bayesian computation (ABC) is a method of inference for
such models. It replaces calculation of the likelihood by a step which involves
simulating artificial data for different parameter values, and comparing summary
statistics of the simulated data to summary statistics of the observed data. This
thesis looks at two related methodological issues for ABC.
Firstly a method is proposed to construct appropriate summary statistics for
ABC in a semi-automatic manner. The aim is to produce summary statistics which
will enable inference about certain parameters of interest to be as accurate as
possible. Theoretical results show that, in some sense, optimal summary statistics
are the posterior means of the parameters. While these cannot be calculated
analytically, an extra stage of simulation is used to estimate how the posterior
means vary as a function of the data, and these estimates are then used as summary
statistics within ABC. Empirical results show that this is a robust method for
choosing summary statistics, that can result in substantially more accurate ABC
analyses than previous approaches in the literature.
Secondly, ABC inference for multiple independent data sets is considered. If
there are many such data sets, it is hard to choose summary statistics which capture
the available information and are appropriate for general ABC methods. An
alternative sequential ABC approach is proposed in which simulated and observed
data are compared for each data set and combined to give overall results. Several
algorithms are proposed and their theoretical properties studied, showing that exploiting
ideas from the semi-automatic ABC theory produces consistent parameter
estimation. Implementation details are discussed, with several simulation examples
illustrating these and application to substantive inference problems.},
  comment     = {May be useful for probabilistic analog ensemble forecasting, ensemble member selection, or maybe even point probabilsitic forecasting, since the point of ABC is to generate a distribution whose parameters are intractible, like power, or maybe even PV probabilsitic forecast errors.

Intro ABC slides as well as on this and related topics also attached. These also introduce what I think are the semi-auto ideas of: Fearnhead10semiAutoABC},
  file        = {Prangle11sumStatSeqABC.pdf:Prangle11sumStatSeqABC.pdf:PDF;ABC introduction slides\: 2011:Prangle11ABC_slides.pdf:PDF;ABC introduction slides\: 2013 (updated 2011 version):Prangle13ABC_slides.pdf:PDF;ABC semi-Auto slides:Prangle11semiAutoABC_slides.pdf:PDF;ABC summary statistics slides:Prangle13sumStatSeqABC_slides.pdf:PDF},
  groups      = {Ensemble, PointDerived, Test, doReadNonWPV_2},
  institution = {Lancaster University},
  owner       = {sotterson},
  timestamp   = {2013.10.04},
  url         = {http://eprints.lancs.ac.uk/62703/},
}

@InCollection{Borak05stableDist,
  author    = {Borak, Szymon and H{\"a}rdle, Wolfgang and Weron, Rafa{\l}},
  title     = {Stable distributions},
  booktitle = {Statistical tools for finance and insurance},
  publisher = {Springer},
  year      = {2005},
  pages     = {21--44},
  abstract  = {Many of the concepts in theoretical and empirical finance developed over the
past decades ? including the classical portfolio theory, the Black-Scholes-Merton
option pricing model and the RiskMetrics variance-covariance approach to
Value at Risk (VaR) ? rest upon the assumption that asset returns follow
a normal distribution. However, it has been long known that asset returns
are not normally distributed. Rather, the empirical observations exhibit fat
tails. This heavy tailed or leptokurtic character of the distribution of price
changes has been repeatedly observed in various markets and may be quan-
titatively measured by the kurtosis in excess of 3, a value obtained for the
normal distribution (Bouchaud and Potters, 2000; Carr et al., 2002; Guillaume
et al., 1997; Mantegna and Stanley, 1995; Rachev, 2003; Weron, 2004).
It is often argued that financial asset returns are the cumulative outcome of a
vast number of pieces of information and individual decisions arriving almost
continuously in time (McCulloch, 1996; Rachev and Mittnik, 2000). As such,
since the pioneering work of Louis Bachelier in 1900, they have been modeled
by the Gaussian distribution. The strongest statistical argument for it is based
on the Central Limit Theorem, which states that the sum of a large number of
independent, identically distributed variables from a finite-variance distribution
will tend to be normally distributed. However, as we have already mentioned,
financial asset returns usually have heavier tails.
In response to the empirical evidence Mandelbrot (1963) and Fama (1965) pro-
posed the stable distribution as an alternative model. Although there are other
heavy-tailed alternatives to the Gaussian law ? like Student?s t, hyperbolic, nor-
mal inverse Gaussian, or truncated stable ? there is at least one good reason
for modeling financial variables using stable distributions. Namely, they are
supported by the generalized Central Limit Theorem, which states that sta-
ble laws are the only possible limit distributions for properly normalized and
centered sums of independent, identically distributed random variables.
Since stable distributions can accommodate the fat tails and asymmetry, they
often give a very good fit to empirical data. In particular, they are valuable
models for data sets covering extreme events, like market crashes or natural
catastrophes. Even though they are not universal, they are a useful tool in
the hands of an analyst working in finance or insurance. Hence, we devote
this chapter to a thorough presentation of the computational aspects related
to stable laws. In Section 1.2 we review the analytical concepts and basic
characteristics. In the following two sections we discuss practical simulation and
estimation approaches. Finally, in Section 1.5 we present financial applications
of stable laws.},
  comment   = {Another stable distribution intro (also see Nolan15StableDistBootCh1), which has a bit on estimation of parameters from quantiles, and mentions (but does not explain) truncated stable distributions (also see: Menn09smthTruncStblDist)},
  doi       = {10.1007/3-540-27395-6_1.pdf},
  file      = {Borak05stableDist.pdf:Borak05stableDist.pdf:PDF},
}

@Article{Robinzonov12BoostNonLinTser,
  author    = {Robinzonov, Nikolay and Tutz, Gerhard and Hothorn, Torsten},
  title     = {Boosting techniques for nonlinear time series models},
  journal   = {AStA Advances in Statistical Analysis},
  year      = {2012},
  volume    = {96},
  number    = {1},
  pages     = {99--122},
  abstract  = {Many of the popular nonlinear time series models require a priori the choice of parametric
functions which are assumed to be appropriate in specific applications. This approach is used
mainly in financial applications, when sufficient knowledge is available about the nonlinear
structure between the covariates and the response. One principal strategy to investigate a
broader class on nonlinear time series is the Nonlinear Additive AutoRegressive (NAAR) model.
The NAAR model estimates the lags of a time series as flexible functions in order to detect nonmonotone
relationships between current observations and past values. We consider linear and
additive models for identifying nonlinear relationships. A componentwise boosting algorithm
is applied to simultaneous model fitting, variable selection, and model choice. Thus, with the
application of boosting for fitting potentially nonlinear models we address the major issues in
time series modelling: lag selection and nonlinearity. By means of simulation we compare the
outcomes of boosting to the outcomes obtained through alternative nonparametric methods.
Boosting shows an overall strong performance in terms of precise estimations of highly nonlinear
lag functions. The forecasting potential of boosting is examined on real data where the target
variable is the German industrial production (IP). In order to improve the model?s forecasting
quality we include additional exogenous variables. Thus we address the second major aspect
in this paper which concerns the issue of high-dimensionality in models. Allowing additional
inputs in the model extends the NAAR model to an even broader class of models, namely
the NAARX model. We show that boosting can cope with large models which have many
covariates compared to the number of observations.
keywords: componentwise boosting, forecasting, nonlinear times series, autoregressive additive
models, lag selection.},
  comment   = {Adapt for quantile regression? Does lag selection too.},
  doi       = {10.1007/s10182-011-0163-4},
  file      = {Robinzonov12BoostNonLinTser_TechRep.pdf:Robinzonov12BoostNonLinTser_TechRep.pdf:PDF},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2014.07.24},
}

@InProceedings{Bieringer03terrainTargObs,
  author    = {Paul E. Bieringer},
  title     = {Utilizing Local Terrain to Determine Targeted Weather Observation Locations},
  booktitle = {BACIMO},
  year      = {2003},
  abstract  = {Many of the recent conflicts where the United States (US) military forces have been deployed are regions that contain complex terrain (i.e. Korea, Kosovo, Afghanistan, and northern Iraq). Accurate weather forecasts are critical to the success of operations in these regions and are typically supplied by numerical weather prediction (NWP) models like the US Navy NOGAPS, CAOMPS, and US Airforce MM5. Unfortunately the weather observations required to generate accurate initial conditions needed by these models are often not available. In these cases it is desirable to deploy additional weather sensors. The question then becomes: Where should the military planners deploy their sensor resources? This study demonstrates that knowledge of just the terrain within the model domain may be a useful factor for military planners to consider. For NWP, model forecast errors in mountainous areas are typically thought to be due to poorly resolved terrain, or model physics not suited for use in a complex terrain environment. Recent advances in computational technology are making it possible to run these models at resolutions where many of the significant terrain features are now being well resolved. While terrain can be accurately specified, often the gradients in wind, temperature, and moisture fields associated with the higher resolution terrain are not. As a result, initial conditions in complex terrain environments are not be adequately specified. Since not all initial condition errors contribute significantly to model forecast error, knowledge of terrain induced NWP model forecast sensitivity may be important when developing and deploying a weather sensor network to support a regional scale NWP model. The terrain induced model sensitivity can provide an indication of which variables in the initial conditions have a significant influence on the forecast and where initial conditions need to be most accurate to minimize model forecast error. A sensor network can then be designed to minimize these errors by deploying critical sensors in sensitive locations, thereby reducing relevant initial condition error without the costly deployment of a high-density sensor network. This is similar to the targeted observation technique first suggested by Emanuel et al. (1995), except that in this example the targeted observations would be designed to reduce initial condition error associated with poorly resolved atmospheric features created by the terrain. This paper is organized as follows. Section 2 contains a brief description of the data collection effort designed to support this study. The experimental design and the specifics of the case used in this study are described in section 3. The analysis and results from both the forward and adjoint simulations are presented in section 4. Section 5 contains a summary of the results, and a brief discussion of their implications.},
  comment   = {Useful for targeted objects proposal?},
  file      = {:Bieringer03terrainTargObs.pdf:PDF;Bieringer03terrainTargObs.pdf:Bieringer03terrainTargObs.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.02.27},
}

@Article{Manshadi16genBassNtwk,
  author      = {Vahideh H. Manshadi and Sidhant Misra},
  title       = {A Generalized Bass Model for Product Growth in Networks},
  journal     = {arXiv},
  year        = {2016},
  number      = {arXiv:1606.03386 [cs.SI]},
  abstract    = {Many products and innovations become well-known and widely adopted through the social interactions of individuals in a population. The Bass diffusion model has been widely used to model the temporal evolution of adoption in such social systems. In the model, the likelihood of a new adoption is proportional to the number of previous adopters, implicitly assuming a global (or homogeneous) interaction among all individuals in the network. Such global interactions do not exist in many large social networks, however. Instead, individuals typically interact with a small part of the larger population. To quantify the growth rate (or equivalently the adoption timing) in networks with limited interactions, we study a stochastic adoption process where the likelihood that each individual adopts is proportional to the number of adopters among the small group of persons he/she interacts with (and not the entire population of adopters). When the underlying network of interactions is a random $k$-regular graph, we compute the sample path limit of the fraction of adopters. We show the limit coincides with the solution of a differential equation which can viewed as a generalization of the Bass diffusion model. When the degree $k$ is bounded, we show the adoption curve differs significantly from the one corresponds to the Bass diffusion model. In particular, the adoption grows more slowly than what the Bass model projects. In addition, the adoption curve is asymmetric, unlike that of the Bass diffusion model. Such asymmetry has important consequences for the estimation of market potential. Finally, we calculate the timing of early adoptions at finer scales, e.g., logarithmic in the population size.},
  date        = {2016-06-10},
  eprint      = {http://arxiv.org/abs/1606.03386v1},
  eprintclass = {cs.SI},
  eprinttype  = {arXiv},
  file        = {:Manshadi16genBassNtwk.pdf:PDF},
  keywords    = {cs.SI, physics.soc-ph},
  url         = {https://arxiv.org/abs/1606.03386},
}

@Article{Genest09copulaGoodFitPow,
  author    = {Genest, C. and R{\'e}millard, B. and Beaudoin, D.},
  title     = {Goodness-of-fit tests for copulas: A review and a power study},
  journal   = {Insurance: Mathematics and Economics},
  year      = {2009},
  volume    = {44},
  number    = {2},
  pages     = {199--213},
  issn      = {0167-6687},
  abstract  = {Many proposals have been made recently for goodness-of-fit testing of copula models. After reviewing them briefly, the authors concentrate on "blanket tests", i.e.,those whose implementation requires neither an arbitrary categorization of the data nor any strategic choice of smoothing parameter, weight function, kernel, window, etc. The authors present a critical review of these procedures and suggest new ones. They describe and interpret the results of a large Monte Carlo experiment designed to assess the effect of the sample size and the strength of dependence on the level and power of the blanket tests for various combinations of copula models under the null hypothesis and the alternative. To circumvent problems in the determination of the limiting distribution of the test statistics under composite null hypotheses, they recommend the use of a double parametric bootstrap procedure, whose implementation is detailed. They conclude with a number of practical recommendations.},
  comment   = {spinning reserve: how well does the copula fit},
  doi       = {10.1016/j.insmatheco.2007.10.005},
  file      = {Genest09copulaGoodFitPow.pdf:Genest09copulaGoodFitPow.pdf:PDF},
  owner     = {scot},
  publisher = {Elsevier},
  timestamp = {2010.12.06},
  url       = {http://www.sciencedirect.com/science/article/B6V8N-4PYHNS9-1/2/33ee9b28a508bcab5fc700e00c96e104},
}

@InBook{Pedregosa16SckikitLearnUserCovEstChap,
  chapter   = {Covariance Estimation},
  title     = {Sckikit-Learn User Guide, v 0.18.1},
  year      = {2016},
  author    = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  abstract  = {Many statistical problems require at some point the estimation of a population?s covariance matrix, which can be seen as an estimation of data set scatter plot shape. Most of the time, such an estimation has to be done on a sample whose properties (size, structure, homogeneity) has a large influence on the estimation?s quality. The sklearn.covariance package aims at providing tools affording an accurate estimation of a population?s covariance matrix under various settings.
We assume that the observations are independent and identically distributed (i.i.d.).},
  comment   = {What Python can do about sparse covariance matrix learning.  Some advice about graph learning approaches (it is hard).

Paper for the library: Pedregosa11ScikitlearnMachLrnPython},
  file      = {Pedregosa16SckikitLearnUserCovEstChap.pdf:Pedregosa16SckikitLearnUserCovEstChap.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.14},
  url       = {http://scikit-learn.org/stable/modules/covariance.html},
}

@Article{Tayman11popFrcstAccRgrssn,
  author    = {Tayman, Jeff and Smith, Stanley K and Rayer, Stefan},
  title     = {Evaluating population forecast accuracy: A regression approach using county data},
  journal   = {Population Research and Policy Review},
  year      = {2011},
  volume    = {30},
  number    = {2},
  pages     = {235--262},
  abstract  = {Many studies have evaluated the impact of differences in population
size and growth rate on population forecast accuracy. Virtually all these studies
have been based on aggregate data; that is, they focused on average errors for places
with particular size or growth rate characteristics. In this study, we take a different
approach by investigating forecast accuracy using regression models based on data
for individual places. Using decennial census data from 1900 to 2000 for 2,482
counties in the US, we construct a large number of county population forecasts and
calculate forecast errors for 10- and 20-year horizons. Then, we develop and
evaluate several alternative functional forms of regression models relating popu-
lation size and growth rate to forecast accuracy; investigate the impact of adding
several other explanatory variables; and estimate the relative contributions of each
variable to the discriminatory power of the models. Our results confirm several
findings reported in previous studies but uncover several new findings as well. We
believe regression models based on data for individual places provide powerful but
under-utilized tools for investigating the determinants of population forecast
accuracy.

Keywords Forecast accuracy  Regression models  Panel data 
Counties},
  comment   = {Uses regression model to predict acccuracy of population forecasts.  Inputs are poplation size and growth rate, forecast horizon, and a bunch of other stuff.

Is a bit like quantile regression.},
  file      = {:Tayman11popFrcstAccRgrssn.pdf:PDF},
  publisher = {Springer},
  url       = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3061008/},
}

@InProceedings{Cheng08semiSupCalibForecast,
  author    = {Cheng,, Haibin and Tan,, Pang-Ning},
  title     = {Semi-supervised learning with data calibration for long-term time series forecasting},
  booktitle = {Knowledge Discovery and Data Mining (KDD)},
  year      = {2008},
  pages     = {133--141},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Many time series prediction methods have focused on single step or short term prediction problems due to the inher- ent di?culty in controlling the propagation of errors from one prediction step to the next step. Yet, there is a broad range of applications such as climate impact assessments and urban growth planning that require long term forecast- ing capabilities for strategic decision making. Training an accurate model that produces reliable long term predictions would require an extensive amount of historical data, which are either unavailable or expensive to acquire. For some of these domains, there are alternative ways to generate po- tential scenarios for the future using computer-driven sim- ulation models, such as global climate and tra?c demand models. However, the data generated by these models are currently utilized in a supervised learning setting, where a predictive model trained on past observations is used to es- timate the future values. In this paper, we present a semi- supervised learning framework for long-term time series fore- casting based on Hidden Markov Model Regression. A co- variance alignment method is also developed to deal with the issue of inconsistencies between historical and model simu- lation data. We evaluated our approach on data sets from a variety of domains, including climate modeling. Our ex- perimental results demonstrate the e?cacy of the approach compared to other supervised learning methods for long- term time series forecasting.},
  comment   = {HMM-based long term forecasting, combining regression of known input/output relationship w/ unlabeled data coming a climate simulation?
* maybe use for blending statistical forecast w/ NWP?
* use the blending trick to somehow to handle NWP ringup problem in hour ahead forecasting?
* use this for a more sophisticated NWP downscale from GFS (the model they use)},
  doi       = {10.1145/1401890.1401911},
  file      = {Cheng08semiSupCalibForecast.pdf:Cheng08semiSupCalibForecast.pdf:PDF;Cheng08semiSupCalibForecast.pdf:Cheng08semiSupCalibForecast.pdf:PDF},
  isbn      = {978-1-60558-193-4},
  location  = {Las Vegas, Nevada, USA},
  owner     = {sotterson},
  timestamp = {2009.03.02},
}

@Article{Liu97mleEstTdistEM,
  author    = {Chuanhai Liu},
  title     = {ML Estimation of the Multivariate \em{t} Distribution and the EM Algorithm},
  journal   = {Journal of Multivariate Analysis},
  year      = {1997},
  volume    = {63},
  number    = {2},
  pages     = {296 - 312},
  issn      = {0047-259X},
  abstract  = {Maximum likelihood estimation of the multivariatetdistribution, especially with unknown degrees of freedom, has been an interesting topic in the development of the EM algorithm. After a brief review of the EM algorithm and its application to finding the maximum likelihood estimates of the parameters of thetdistribution, this paper provides new versions of the ECME algorithm for maximum likelihood estimation of the multivariatetdistribution from data with possibly missing values. The results show that the new versions of the ECME algorithm converge faster than the previous procedures. Most important, the idea of this new implementation is quite general and useful for the development of the EM algorithm. Comparisons of different methods based on two datasets are presented.},
  comment   = {What Henry Martin is using to estimate student t copulas for the Prime project.  Also is using it to fill in missing measurements and forecasts.},
  doi       = {http://dx.doi.org/10.1006/jmva.1997.1703},
  file      = {:papers\\Liu97mleEstTdistEM.pdf:PDF},
  keywords  = {EM, ECM, ECME, incomplete data, Newton-Raphson},
  owner     = {sotterson},
  timestamp = {2017.08.22},
  url       = {http://www.sciencedirect.com/science/article/pii/S0047259X97917036},
}

@Article{Giacomini09copulaeTimVar,
  author    = {Enzo Giacomini and Wolfgang H\"{a}rdle and Vladimir Spokoiny},
  title     = {Inhomogeneous Dependence Modeling with Time-Varying Copulae},
  year      = {2009},
  volume    = {27},
  number    = {2},
  pages     = {224--234},
  doi       = {10.1198/jbes.2009.0016},
  eprint    = {http://pubs.amstat.org/doi/pdf/10.1198/jbes.2009.0016},
  abstract  = {Measuring dependence in multivariate time series is tantamount to modeling its dynamic structure in space and time. In risk management, the nonnormal behavior of most financial time series calls for non-Gaussian dependences. The correct modeling of non-Gaussian dependences is, therefore, a key issue in the analysis of multivariate time series. In this article we use copula functions with adaptively estimated time-varying parameters for modeling the distribution of returns. Furthermore, we apply copulae to the estimation of Value-at-Risk of portfolios and show their better performance over the RiskMetrics approach.},
  journal   = {Journal of Business and Economic Statistics},
  owner     = {scot},
  timestamp = {2010.12.06},
}

@TechReport{Prinn09climImpctlargeWindFrmsTechRep,
  author      = {Prinn, Ronald G and Wang, Chien},
  title       = {Potential climatic impacts and reliability of very large-scale wind farms},
  institution = {MIT Joint Program on the Science and Policy of Global Change},
  year        = {2009},
  abstract    = {Meeting future world energy needs while addressing climate change requires large-scale
deployment of low or zero greenhouse gas (GHG) emission technologies such as wind energy. The
widespread availability of wind power has fueled legitimate interest in this renewable energy source
as one of the needed technologies. For very large-scale utilization of this resource, there are
however potential environmental impacts, and also problems arising from its inherent intermittency,
in addition to the present need to lower unit costs. To explore some of these issues, we use a three-
dimensional climate model to simulate the potential climate effects associated with installation of
wind-powered generators over vast areas of land or coastal ocean. Using windmills to meet 10% or
more of global energy demand in 2100, could cause surface warming exceeding 1oC over land
installations. In contrast, surface cooling exceeding 1oC is computed over ocean installations, but
the validity of simulating the impacts of windmills by simply increasing the ocean surface drag needs
further study. Significant warming or cooling remote from both the land and ocean installations, and
alterations of the global distributions of rainfall and clouds also occur. These results are influenced
by the competing effects of increases in roughness and decreases in wind speed on near-surface
turbulent heat fluxes, the differing nature of land and ocean surface friction, and the dimensions of
the installations parallel and perpendicular to the prevailing winds. These results are also
dependent on the accuracy of the model used, and the realism of the methods applied to simulate
windmills. Additional theory and new field observations will be required for their ultimate
validation. Intermittency of wind power on daily, monthly and longer time scales as computed in
these simulations and inferred from meteorological observations, poses a demand for one or more
options to ensure reliability, including backup generation capacity, very long distance power
transmission lines, and onsite energy storage, each with specific economic and/or technological
challenges.},
  comment     = {Technical report that preceded the published work in Wang10climImpctlargeWindFrms

Was referenced by commenter here:
https://www.quora.com/Do-windmills-slow-the-wind-down

Found that huge amounts of windpower would increase suface temperature on land, decrease in the ocean.  Also, can't extract enough of power from wind to power the world, I think b/c of turbulence.  Also, would increase wind doldrums in at peak airconditioning times, many places.

For solar power impacts, see: Hu16solPnlClimImpct

For mesocscale impact, see Adams13glblWindPowOvrEst

But also see rebuttals (kind of):
*  https://www.evernote.com/shard/s13/nl/1523219/0f9b2965-6edc-4b0b-9db1-6afd9ac9eb4f
*  https://www.evernote.com/shard/s13/nl/1523219/15d711f3-2f6f-4f81-884e-ac5a6b864b27},
  file        = {2009 tech report:Prinn09climImpctlargeWindFrmsTechRep:PDF},
  owner       = {sotterson},
  timestamp   = {2017.04.26},
  url         = {http://globalchange.mit.edu/files/document/MITJPSPGC_Rpt175.pdf},
}

@Article{Wang10climImpctlargeWindFrms,
  author    = {Wang, Chien and Prinn, Ronald G},
  title     = {Potential climatic impacts and reliability of very large-scale wind farms},
  journal   = {Atmospheric Chemistry and Physics},
  year      = {2010},
  volume    = {10},
  number    = {4},
  pages     = {2053--2061},
  abstract  = {Meeting future world energy needs while addressing climate change requires large-scale deployment of low or zero greenhouse gas (GHG) emission technologies such as wind energy. The widespread availability of wind power has fueled substantial interest in this renewable energy source as one of the needed technologies. For very large-scale utilization of this resource, there are however potential environmental impacts, and also problems arising from its inherent intermittency, in addition to the present need to lower unit costs. To explore some of these issues, we use a three-dimensional climate model to simulate the potential climate effects associated with installation of wind-powered generators over vast areas of land or coastal ocean. Using wind turbines to meet 10% or more of global energy demand in 2100, could cause surface warming exceeding 1 ?C over land installations. In contrast, surface cooling exceeding 1 ?C is computed over ocean installations, but the validity of simulating the impacts of wind turbines by simply increasing the ocean surface drag needs further study. Significant warming or cooling remote from both the land and ocean installations, and alterations of the global distributions of rainfall and clouds also occur. These results are influenced by the competing effects of increases in roughness and decreases in wind speed on near-surface turbulent heat fluxes, the differing nature of land and ocean surface friction, and the dimensions of the installations parallel and perpendicular to the prevailing winds. These results are also dependent on the accuracy of the model used, and the realism of the methods applied to simulate wind turbines. Additional theory and new field observations will be required for their ultimate validation. Intermittency of wind power on daily, monthly and longer time scales as computed in these simulations and inferred from meteorological observations, poses a demand for one or more options to ensure reliability, including backup generation capacity, very long distance power transmission lines, and onsite energy storage, each with specific economic and/or technological challenges.},
  comment   = {Published version of Prinn09climImpctlargeWindFrmsTechRep (more comments there)},
  file      = {Wang10climImpctlargeWindFrms.pdf:Wang10climImpctlargeWindFrms.pdf:PDF},
  owner     = {sotterson},
  publisher = {Copernicus GmbH},
  timestamp = {2017.04.26},
  url       = {http://www.atmos-chem-phys.net/10/2053/2010/},
}

@Conference{Nielsen04windPowEnsembForecast,
  author    = {H. A. Nielsen and H. Madsen and T. S. Nielsen and J. Badger and G. Giebel and L. Landberg and K. Sattler and H. Feddersen},
  title     = {Wind Power Ensemble Forecasting},
  booktitle = {Windpower},
  year      = {2004},
  abstract  = {Meteorological ensemble forecasts aim at quantifying the uncertainty of a forecast by offering several scenarios of the future development of the weather. Ideally, we would think of the ensembles as samples from a probability distribution function reflecting the uncertainty of the unperturbed forecast. In this paper we address the problems of (i) transforming the meteorological ensembles to wind power ensembles and, (ii) correcting the ensemble quantiles to allow a probabilistic interpretation. The methods are applied to two wind farms using ensembles from the European Centre for Medium-Range Weather Forecasts. It is shown that the resulting ensemble quantiles are able to distinguish between situations with low and high uncertainty. Furthermore, often the quantiles indicate uncertainty which is significantly smaller than the uncertainty which follow from historical (climatological) data. However, quite often the actual wind power production is outside the range of ensemble forecast and therefore it is not possible to obtain information regarding the extreme quantiles.},
  comment   = {Calculating wind power forecast uncertainty from NWP wind ensembles. Power curve learning w/ simple conditional logistic shape.

Maybe related to ensemble BMA?},
  file      = {Nielsen04windPowEnsembForecast.pdf:Nielsen04windPowEnsembForecast.pdf:PDF},
  groups    = {Ensemble, doReadWPV_1},
  location  = {Chicago, Illinois, {USA}},
  owner     = {sotterson},
  timestamp = {2009.01.05},
  url       = {http://www2.imm.dtu.dk/pubdb/p.php?3128},
}

@InBook{Markowski11mesoCh1,
  chapter   = {Chapter 1: What is Mesocale?},
  title     = {Mesoscale meteorology in midlatitudes},
  publisher = {John Wiley \& Sons},
  year      = {2011},
  author    = {Markowski, Paul and Richardson, Yvette},
  volume    = {2},
  abstract  = {Meteorological phenomena occur over a wide range of space and time scales. Phenomena
having short time scales also tend to have small spatial scales, and vice versa (Fig. 1.1).
Curiously, the ratio of (horizontal) space to time scales, which has units of velocity (m s?1),
is roughly the same order of magnitude for all features (?10 m s?1). If a phenomenon?s width

is much greater than its depth, it may be inferred that the phenomenon is approximately
hydrostatic (Fig. 1.2). This is a consequence of the first law of thermodynamics, mass
continuity, and the equations of motion. We will revisit this issue at the end of this chapter.
Before defining what is meant by ?mesoscale,? it may be easiest first to define what
is meant by the ?synoptic? scale. The adjective ?synoptic? is defined in the American
Meteorological Society?s Glossary of Meteorology as referring to meteorological data that
are obtained simultaneously over a wide area in order to present a nearly instantaneous
snap-shot of the state of the atmosphere. The early synoptic charts displayed the limited
amount of data that could be collected routinely at the same times on a daily basis, and
disturbances that could be resolved on these charts eventually were referred to as ?synoptic-
scale? disturbances. Thus, the term ?synoptic,? though not initially intended to define a
scale, ultimately became a term used to describe the scale of large-scale weather systems,
which were the only types of meteorological phenomena that could be resolved regularly by
the coarse resolution observing platforms of the middle 19th century.},
  comment   = {Defines meteorological spatio-temporal scales (vertical and horizontal) and how fast things change v.s. size.  Fig 1.1  would have been nice for REMENA classes.

What is Mesocale?: 2?2000 km (horizontal):
* meso-alpha: 200?2000 km
* meso-beta: 20-200 km
* meso-gamma: 2?20 km

(vertical): There's a logarithmic relationship between horizontal and vertical scales

< 2km ==> microscale

Ratio of (horizontal) space to time scale is ~10 m/s
* so I think, a 1 km feature changes at a rate of 1km / 10 (m/s) = = 100 secs
* but I not sure what "change" means
* ratio is same order of magnitude not matter what the "feature" is},
  file      = {Markowski11mesoCh1.pdf:Markowski11mesoCh1.pdf:PDF},
  url       = {http://www.rsmas.miami.edu/users/bmapes/teaching/MPO663_Intro_Fourier.html},
}

@Article{Demetriou02lsqFitMonoConv,
  author    = {Demetriou, I. C.},
  title     = {Signs of divided differences yield least squares data fitting with constrained monotonicity or convexity},
  journal   = {Journal of Computational and Applied Mathematics},
  year      = {2002},
  volume    = {146},
  number    = {2},
  pages     = {179--211},
  issn      = {0377-0427},
  abstract  = {Methods are presented for least squares data smoothing by using the signs of divided differences of the smoothed values. Professor M.J.D. Powell initiated the subject in the early 1980s and since then, theory, algorithms and FORTRAN software make it applicable to several disciplines in various ways.Let us consider n data measurements of a univariate function which have been altered by random errors. Then it is usual for the divided differences of the measurements to show sign alterations, which are probably due to data errors. We make the least sum of squares change to the measurements, by requiring the sequence of divided differences of order m to have at most q sign changes for some prescribed integer q. The positions of the sign changes are integer variables of the optimization calculation, which implies a combinatorial problem whose solution can require about O(nq) quadratic programming calculations in n variables and n - m constraints.Suitable methods have been developed for the following cases. It has been found that a dynamic programming procedure can calculate the global minimum for the important cases of piecewise monotonicity m = 1, q ? 1 and piecewise convexity/concavity m = 2, q ? 1 of the smoothed values. The complexity of the procedure in the case of m = 1 is O(n2 + qn log2 n) computer operations, while it is reduced to only O(n) when q = 0 (monotonicity) and q = 1 (increasing/decreasing monotonicity). The case m = 2, q ? 1 requires O(qn2) computer operations and n2 quadratic programming calculations, which is reduced to one and n - 2 quadratic programming calculations when m = 2, q = 0, i.e. convexity, and m = 2, q = 1, i.e. convexity/concavity, respectively.Unfortunately, the technique that receives this efficiency cannot generalize for the highly nonlinear case m ? 3,q ? 2. However, the case m ? 3,q = 0 is solved by a special strictly convex quadratic programming calculation, and the case m ? 3,q = 1 can be solved by at most 2(n - m) applications of the previous algorithm. Also, as m gets larger, large sets of active constraints usually occur at the optimal approximation, which makes the calculation for higher values of q less expensive than what it seems to. Further, attention is given to the sensitivity of the solution with respect to changes in the constraints and the data.The smoothing technique is an active research topic and there seems to be room for further developments. One strong reason for studying methods that make use of divided differences for data smoothing is that, whereas only the data are provided, the achieved accuracy goes much beyond the accuracy of the data at an order determined automatically by the chosen divided differences.},
  comment   = {Least squares fitting with monotonicity or convexity constraints; Fortran might be available. Maybe relevant to optimal spinning reserves project, where we need to estimate the cost of reserves. The idea was to empirically derive the cost curve market data. This has to be monotonically increasing with reserve size. This method could enforce that. He has another paper that talks about his Fortran code for implementing this (I think)},
  doi       = {10.1016/S0377-0427(02)00353-9},
  location  = {Amsterdam, The Netherlands, The Netherlands},
  owner     = {scot},
  publisher = {Elsevier Science Publishers B. V.},
  timestamp = {2010.08.05},
}

@Article{Horton07adoMissDatComp,
  author               = {Horton and J. Nicholas and Kleinman and P. Ken},
  title                = {Much Ado About Nothing: A Comparison of Missing Data Methods and Software to Fit Incomplete Data Regression Models},
  journal              = {The American Statistician},
  year                 = {2007},
  volume               = {61},
  number               = {1},
  pages                = {79--90},
  month                = feb,
  issn                 = {0003-1305},
  abstract             = {Missing data are a recurring problem that can cause bias or lead to inefficient analyses. Statistical methods to address missingness have been actively pursued in recent years, including imputation, likelihood, and weighting approaches. Each approach is more complicated when there are many patterns of missing values, or when both categorical and continuous random variables are involved. Implementations of routines to incorporate observations with incomplete variables in regression models are now widely available. We review these routines in the context of a motivating example from a large health services research dataset. While there are still limitations to the current implementations, and additional efforts are required of the analyst, it is feasible to incorporate partially observed values, and these methods should be used in practice. KEYWORDS: Conditional Gaussian; Health services research; Maximum},
  citeulike-article-id = {1031740},
  citeulike-linkout-0  = {http://dx.doi.org/10.1198/000313007X172556},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/asa/tas/2007/00000061/00000001/art00015},
  citeulike-linkout-2  = {http://view.ncbi.nlm.nih.gov/pubmed/17401454},
  citeulike-linkout-3  = {http://www.hubmed.org/display.cgi?uids=17401454},
  comment              = {Comparison of R libraries for missing data imputation. Amelia II liked.

Types of fill-in
* Complete case
 -- just dropping incomplete samples
 -- is statistically inefficient, may cause bias
 -- from elsewhere, I know that bias could happen if missingness is not totally random
* Ad-hoc
 -- including a "missing" flag in the input
 -- can induce bias, not recommended
* Multiple imputation
 -- model input dependencies, produce multiple guesses of them (imputations)
 -- 5-10 imputations seems good enough
 ---- generally? in what kind of problems?
 ---- explanation seems to be Rubin's Rule: http://sites.stat.psu.edu/~jls/mifaq.html
* Likelihood
 -- create ML estimators of missing values
 -- use EM
* Weighting
 -- probability of missingness is used to weight samples
 -- like a confidence value?
* Bayesian
 -- WinBugs/OpenBugs is example
 -- not sure how this is different from ML, or Amelia Types of multiple imputation
* conditional Gaussian
 -- use when mix of continuous and discrete
 -- discrete vars get log-linear treatment
 -- contin variables are conditioned on this, modeled w/ multivariate Gaussian
* chained equations
 -- separately predict each var from others
 -- imputed value used to impute next var in chain
 -- use a Gibbs sampler and wait for convergence
 -- may not converge
* monotone datasets
 -- somehow grab the value from a data "nearby" datapoint that doesn't have that variable missing
 -- can build up from no missing values to N missing values
 -- can be biased
 -- problematic for mixtures of discrete and continuous Programs
* Amelia II
* Hmisc
* ICE/Stat
* IVEWARE
* LogXact
* MICE
* SAS Proc MI
* S-Plus
* mention others

COMPARISON
* for 2 cases, similar answers for tested missing data programs relative to complete case
* for 1 case, complete case approach is much worse than if use a missing data model
* didn't seem to test WinBugs/OpenBugs},
  doi                  = {10.1198/000313007X172556},
  file                 = {Horton07adoMissDatComp.pdf:Horton07adoMissDatComp.pdf:PDF},
  groups               = {Read},
  keywords             = {missing\_data, regression, software},
  owner                = {scot},
  posted-at            = {2008-02-08 12:52:45},
  publisher            = {American Statistical Association},
  timestamp            = {2010.09.09},
}

@Article{Glahn72modelOutStatWeath,
  author    = {Glahn, Harry R.and Lowry, Dale A.},
  title     = {The Use of Model Output Statistics ({MOS}) in Objective Weather Forecasting},
  year      = {1972},
  volume    = {11},
  number    = {8},
  pages     = {1203--1211},
  doi       = {10.1175/1520-0450(1972)011},
  url       = {http://adsabs.harvard.edu/abs/1972JApMe..11.1203G},
  abstract  = {Model Output Statistics (MOS) is an objective weather forecasting technique which consists of determining a statistical relationship between a predictand and variables forecast by a numerical model at some projection time(s). It is, in effect, the determination of the `weather related' statistics of a numerical model. This technique, together with screening regression, has been applied to the prediction of surface wind, probability of precipitation, maximum temperature, cloud amount, and conditional probability of frozen precipitation. Predictors used include surface observations at initial time and predictions from the Subsynoptic Advection Model (SAM) and the Primitive Equation model used operationally by the National Weather Service. Verification scores have been computed, and, where possible, compared to scores for forecasts from other objective techniques and for the official forecasts. MOS forecasts of surface wind, probability of precipitation, and conditional probability of frozen precipitation are being disseminated by the National Weather Service over teletype and facsimile. It is concluded that MOS is a useful technique in objective weather forecasting},
  groups    = {DOE-PNL09},
  journal   = {Journal of Applied Meteorology},
  owner     = {sotterson},
  timestamp = {2009.03.03},
}

@Article{Winker00multiVarLagSel,
  author    = {Peter Winker},
  title     = {Optimized Multivariate Lag Structure Selection},
  journal   = {Computational Economics},
  year      = {2000},
  volume    = {16},
  number    = {1/2},
  pages     = {87--103},
  month     = oct,
  abstract  = {Model selection ? choosing the relevant variables and structures ? is a central task in econometrics. Given a limited number of observations, estimation and inference depend on this choice. A frequently treated model-selection problem arises in multivariate autoregressive models, where the problem reduces to the choice of a dynamic structure. In most applications this choice is based either on some ad hoc procedure or on a search within a very small subset of all possible models. In this paper the selection is performed using an explicit optimization approach for a given information criterion. Since complete enumeration of all possible lag structures is infeasible even for moderate dimensions, the global optimization heuristic of threshold accepting is implemented. A simulation study compares this approach with the standard ?take all up to the kth lag? approach. It is found that, if the lag structure of the true model is sparse, the threshold accepting optimization approach gives far better approximations.},
  comment   = {Selects lags for vector AR process w/ AIC and a genetic-like combinatorial search * is modeling a process for one step ahead forecastng * output of process is a vector; input is past outputs: is a vector autoregressive model (VAR) * Expressions for AIC and vector outputs -- might be useful -- esp. how need to use SURE algorithm to correct for covariance w/ "holes" in it (unselected lags) * stochastic search moves among random combinations of lags, constrained to a "neighborhood" kind of like a genetic algorithm would * maybe they really should just do a GA, w/ more formalized heuristics * on simulated data, approach does better than selecting "all lags up to K" method -- in the sense of AIC loss function -- on data generated with a process that really was missing lags},
  file      = {Winker00multiVarLagSel.pdf:Winker00multiVarLagSel.pdf:PDF;Winker00multiVarLagSel.pdf:Winker00multiVarLagSel.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.02.09},
  url       = {http://ideas.repec.org/a/kap/compec/v16y2000i1-2p87-103.html},
}

@InCollection{Verleysen05cursDimMinTser,
  author    = {Verleysen, Michel and Fran\c{c}ois, Damien},
  title     = {The Curse of Dimensionality in Data Mining and Time Series Prediction},
  booktitle = {Computational Intelligence and Bioinspired Systems},
  publisher = {Springer Berlin Heidelberg},
  year      = {2005},
  editor    = {Cabestany, Joan and Prieto, Alberto and Sandoval, Francisco},
  volume    = {3512},
  series    = {Lecture Notes in Computer Science},
  pages     = {758--770},
  isbn      = {978-3-540-26208-4},
  abstract  = {Modern data analysis tools have to work on high-dimensional data, whose components are not independently distributed. High-dimensional spaces show surprising, counter-intuitive geometrical properties that have a large influence on the performances of data analysis tools. Among these properties, the concentration of the norm phenomenon results in the fact that Euclidean norms and Gaussian kernels, both commonly used in models, become inappropriate in high-dimensional spaces. This papers presents alternative distance measures and kernels, together with geometrical methods to decrease the dimension of the space. The methodology is applied to a typical time series prediction example.},
  comment   = {Simple p-Gaussian kernel and several distance preserving dimension reduction methods improve discrimination in high dim kernels. The time series example wasn't convincing but many later papers use the p-Gaussian.

Most well known high dim problems
* w/ too few samples get
 -- collinearity (because things look more correlated in high dims)
 -- overfitting
* num. of training samples needed increases exponentially w/ num. dims
* but this is only one of the high dim problems...

Paper goals
* account for high dim dependencies, this avoiding a lot of model params
* adapt model to high dim case
* reduce dimensions

In high dim, normal distance measures fail in weird ways:
1. volume of unit sphere increases until dim 5; and then converges to zero!
2. uniformly drawn samples are almost all in the hypercube corners
3. uniform samples in a sphere are almost all on the outer surface
4. Gaussian samples are almost all in the tails!
5. Expected distance of samples drawn from a Gaussian increases w/ dim, avg. is actually concentrated in a narrow distance.

Better distance functions (better means "points are distinguishable based on this distance")
* avoid the "concentration of norms" problem (largest and smallest distances are nearly the same in high dim)
* L-p norm (also called "Minkowski Norm")
 -- p>2 is useless in high dim
 -- p=2 (Euclidean) still seems OK for Gaussian distributed noise
 -- p=1: contrast increases w/ dimension
 -- fractional norm (p non-integer): "relative contrast" goes to zero faster w/ higher p (again p>2 is useless?)
 -- p<1 ("fractional") better than Euclidean in "colored noise," whatever that is (could read ref).

Ameliorating Curse of Dimensionality

I. A better kernel

* GAUSSIAN NOT RECOMMENDED FOR HIGH DIM
* p-Gaussian better for high dim
 -- idea is to adjust kernel curve so that it has significant slope at distances where distribution samples are
 -- very easy to tune using data distance percentiles: Francois05locKernHiDim
 -- but GutierrezOsuna11kdnnLecNotes doesn't like distance weighting for KNN?
 -- compare with Schnitzer12Localandglobal

II. Dimension reduction

 -- want "bijection" (one-to-one relationship) after dim reduce
 -- PCA can help but loses nonlinearities
 -- local PCA better but not continuous
 -- Kernel PCA better yet, but initial transform hard to pick
 -- Distance Preserving Techniques
 ---- Sammon's nonlinear mapping can unfold some spaces, so maybe better
 ---- but Curvilinear Component Analysis (CCA) is much better than Sammon's because it weighs short distance more in projected space
 ---- Curvilinear Distance Analysi (CDA) computes distance along the manifold
 ---- Isomap is similar
 -- SOM works but not as good as distance preserving techniques
 -- bottleneck MLP is bijective but has numerical difficulties (as Jon Malkin found at SSLI)

 -- Conclusion: CCA, CDA and Isomap seem to be best (circa 2005)

Time Series Prediction using these fancy ideas
* Use Takin's Theorem: can find an intrinisic dimension in lag space and not lose info is compress to that
 -- mentions a way to estimate this
* Example: Autoregressive forecast of stock market index

 inputs --> PCA --> CCA (intrinsic dim) --> RBF classifier --> sign forecast
 dim=42 25 9 1

* features were things like sliding mean, which is maybe like a lag
* but I didn't totally get how the intrinsic dim (9) was related
* results weren't spectacular anyway},
  doi       = {10.1007/11494669_93},
  file      = {Verleysen05cursDimMinTser.pdf:Verleysen05cursDimMinTser.pdf:PDF},
  groups    = {Read, PointDerived, doReadNonWPV_2},
  owner     = {sotterson},
  timestamp = {2014.03.26},
}

@Article{Reiss12smthPenSplnQR,
  author    = {Reiss, Philip T and Huang, Lei},
  title     = {Smoothness selection for penalized quantile regression splines},
  journal   = {The International Journal of Biostatistics},
  year      = {2012},
  volume    = {8},
  number    = {1},
  abstract  = {Modern data-rich analyses may call for fitting a large number of nonparametric quantile regressions. For example, growth charts may be constructed for each of a collection of variables, to identify those for which individuals with a disorder tend to fall in the tails of their age-specific distribution; such variables might serve as developmental biomarkers. When such a large set of analyses are carried out by penalized spline smoothing, reliable automatic selection of the smoothing parameter is particularly important. We show that two popular methods for smoothness selection may tend to overfit when estimating extreme quantiles as a smooth function of a predictor such as age; and that improved results can be obtained by multifold cross-validation or by a novel likelihood approach. A simulation study, and an application to a functional magnetic resonance imaging data set, demonstrate the favorable performance of our methods.

Keywords: asymmetric Laplace distribution; functional connectivity; generalized approximate cross-validation; growth chart; nonparametric quantile regression; smoothing parameter},
  comment   = {Penalized QR using the coeff roughness penalty. Finds that SIC and GACV (fancy leave-one-out CV) severly overfits, but ordinary multifold CV (k==5 recommended) is much better that way.

Uses a kind of IRLS to do the optimization
- called by penalized iteratively reweighted least squares (PIRLS)
- described in Pratesi09mQRpenSpln and in one other paper mentioned in this one
- may also be described in Yoshida13asympPenSplnQR

* the spline penalty integral used here has "computational difficulty," according to Yoshida13asympPenSplnQR},
  file      = {Author's manuscript:Reiss12smthPenSplnQR.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.21},
  url       = {http://www.degruyter.com/view/j/ijb.2012.8.issue-1/1557-4679.1381/1557-4679.1381.xml},
}

@Article{Liu16rootCauseTSanomGrphMdl,
  author    = {Liu, Chao and Lore, Kin Gwn and Sarkar, Soumik},
  title     = {Root-cause analysis for time-series anomalies via spatiotemporal causal graphical modeling},
  journal   = {arXiv preprint arXiv:1605.06421},
  year      = {2016},
  abstract  = {Modern distributed cyber-physical systems encounter a large variety of anomalies
and in many cases, they are vulnerable to catastrophic fault propagation scenarios
due to strong connectivity among the sub-systems. In this regard, root-cause anal-
ysis becomes highly intractable due to complex fault propagation mechanisms in
combination with diverse operating modes. This paper presents a new data-driven
framework for root-cause analysis for addressing such issues. The framework is
based on a spatiotemporal feature extraction scheme for multivariate time series
built on the concept of symbolic dynamics for discovering and representing causal
interactions among subsystems of a complex system. We propose sequential state
switching (S3) and artificial anomaly association (A3) methods to implement root-
cause analysis in an unsupervised and semi-supervised manner respectively. Syn-
thetic data from cases with failed pattern(s) and anomalous node are simulated to
validate the proposed approaches, then compared with the performance of vector
autoregressive (VAR) model-based root-cause analysis. The results show that: (1)
S3 and A3 approaches can obtain high accuracy in root-cause analysis and success-
fully handle multiple nominal operation modes, and (2) the proposed tool-chain is
shown to be scalable while maintaining high accuracy.},
  file      = {Liu16rootCauseTSanomGrphMdl.pdf:Liu16rootCauseTSanomGrphMdl.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.01.26},
  url       = {https://arxiv.org/abs/1605.06421},
}

@Article{Solomatine06modLrnFrcst,
  author    = {D.P. Solomatine and M.B. Siek},
  title     = {Modular learning models in forecasting natural phenomena},
  journal   = {Neural Networks},
  year      = {2006},
  volume    = {19},
  number    = {2},
  pages     = {215--224},
  issn      = {0893-6080},
  note      = {<ce:title>Earth Sciences and Environmental Applications of Computational Intelligence</ce:title>},
  abstract  = {Modular model is a particular type of committee machine and is comprised of a set of specialized (local) models each of which is responsible for a particular region of the input space, and may be trained on a subset of training set. Many algorithms for allocating such regions to local models typically do this in automatic fashion. In forecasting natural processes, however, domain experts want to bring in more knowledge into such allocation, and to have certain control over the choice of models. This paper presents a number of approaches to building modular models based on various types of splits of training set and combining the models??? outputs (hard splits, statistically and deterministically driven soft combinations of models, ???fuzzy committees???, etc.). An issue of including a domain expert into the modeling process is also discussed, and new algorithms in the class of model trees (piece-wise linear modular regression models) are presented. Comparison of the algorithms based on modular local modeling to the more traditional ???global??? learning models on a number of benchmark tests and river flow forecasting problems shows their higher accuracy and transparency of the resulting models.},
  comment   = {A kind of regime learner with a hard splitting of input space in feature subspaces, a bit like in adaboost based Lillywhite13featCnstrct},
  doi       = {10.1016/j.neunet.2006.01.008},
  file      = {Solomatine06modLrnFrcst.pdf:Solomatine06modLrnFrcst.pdf:PDF},
  keywords  = {Local models},
  owner     = {sotterson},
  timestamp = {2014.01.21},
}

@Article{Karvanen06quanMixLmomTrim,
  author    = {Juha Karvanen},
  title     = {Estimation of quantile mixtures via L-moments and trimmed L-moments},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {2006},
  volume    = {51},
  number    = {2},
  pages     = {947--959},
  issn      = {0167-9473},
  abstract  = {Moments or cumulants have been traditionally used to characterize a probability distribution or an observed data set. Recently, L-moments and trimmed L-moments have been noticed as appealing alternatives to the conventional moments. This paper promotes the use of L-moments proposing new parametric families of distributions that can be estimated by the method of L-moments. The theoretical L-moments are defined by the quantile function i.e. the inverse of cumulative distribution function. An approach for constructing parametric families from quantile functions is presented. Because of the analogy to mixtures of densities, this class of parametric families is called quantile mixtures. The method of L-moments is a natural way to estimate the parameters of quantile mixtures. As an example, two parametric families are introduced: the normal-polynomial quantile mixture and the Cauchy-polynomial quantile mixture. The proposed quantile mixtures are applied to model monthly, weekly and daily returns of some major stock indexes.},
  comment   = {Quantile estimate is a weighted combination of a set of other quantiles -- in this paper, one is a traditional distribution ( Normal or Gaussian in this paper but others are possible) and the rest are polynomials of tau. Advantage is that a very flexible distribution can be estimated as a linear combo of four robust moments. Has R library. Would be interesting for forecasting if distribution could be made conditional (maybe see Geweke07smoothMixRegress?)

Tests two mixture types
 * normal-polynomial-quantile-mixture
 -- is somewhat limited in terms of skew, etc.
 -- but can be made bounded by keeping kurtosis=0
 * cauchy-polynomial-quantile mixture
 -- flexible but long tailed
 * finds that they can fit financial data with small sample size than standard distributions.

Mentions these traditional disributions:
* Laplacian
* Student's-t
* Normal (can be made bounded, I think)
* Cauchy (good for wind power error: Hodge11frcstErrDistTime)
* logit (seems like this could also be bounded, if polyomial stuff is right)

Could beta distribution (bounded) work?

Also finds covariance matrix of the parameters, but doesn't say how this would be used.

Possibly good for
* regime detection (only a few coeffs to detect changes in)?
* general quantile forecasting if can be made conditional (this could take place of the spline in splineQR).

Related to Lichtendahl13probOrQuantAvg

Implemented in R package: Lmoments
R mixtools library might be able to do the same thing more economically},
  doi       = {10.1016/j.csda.2005.09.014},
  file      = {Karvanen06quanMixLmomTrim.pdf:Karvanen06quanMixLmomTrim.pdf:PDF},
  groups    = {Read, Ensemble, PointDerived, doReadNonWPV_2},
  keywords  = {Order statistics},
  owner     = {sotterson},
  timestamp = {2013.11.13},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167947305002513},
}

@Article{Lee08mutInfRelNet,
  author    = {Lee, P.W.-H. and Wang, Z.J. and McKeown, M.J.},
  title     = {Mutual information based relevance network analysis: a Parkinson?S disease study},
  journal   = {Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2008},
  pages     = {497--500},
  month     = {31 2008-April 4},
  issn      = {1520-6149},
  abstract  = {Monitoring the dynamics of networks in the brain is of central importance in normal and disease states. Current methods of detecting networks in the recorded EEG such as correlation and coherence only explore linear dependencies, which may be unsatisfactory. We propose applying mutual information as an alternative metric for assessing possible nonlinear statistical dependencies between EEG channels. However, EEG data are complicated by the fact that data are inherently non-stationary and also the brain may not work on the task continually. To address these concerns, we propose a novel EEG segmentation method based on the temporal dynamics of the cross-spectra of computed independent components. A real case study in Parkinson's disease and further group analysis employing ANOVA demonstrate different brain connectivity between tasks and between subject groups and also a plausible mechanism for the beneficial effects of medication used in this disease. The proposed method appears to be a promising approach for EEG analysis and warrants further study.},
  comment   = {Sensor dependence via mutual information and ICA. Can handle nonstationarity},
  doi       = {10.1109/ICASSP.2008.4517655},
  file      = {Lee08mutInfRelNet.pdf:Lee08mutInfRelNet.pdf:PDF;Lee08mutInfRelNet.pdf:Lee08mutInfRelNet.pdf:PDF},
  keywords  = {diseases, electroencephalography, independent component analysis, medical signal processingANOVA, EEG segmentation method, Parkinson's disease, brain, electroencephalogram segmentation method, independent components, mutual information, network dynamics monitoring, nonlinear statistical dependencies, relevance network analysis, temporal dynamics},
  owner     = {sotterson},
  timestamp = {2009.01.28},
}

@InProceedings{Oates12repDiversTSsax,
  author    = {Oates, T. and Mackenzie, C.F. and Stein, D.M. and Stansbury, L.G. and DuBose, J. and Aarabi, B. and Hu, P.F.},
  title     = {Exploiting Representational Diversity for Time Series Classification},
  booktitle = {Machine Learning and Applications (ICMLA), 2012 11\textsuperscript{th} International Conference on},
  year      = {2012},
  volume    = {2},
  pages     = {538--544},
  abstract  = {More than a decade of research has produced numerous representations and similarity measures to support time series classification and clustering. Yet most of the work in the field is so focused on the representation or similarity measure that it ignores the possibility of improving performance using ensembles of representations or classifiers. This paper explores ways of exploiting representational diversity for time series classification via ensembles of representations. We focus on the Symbolic Aggregate approXimation (SAX) discretization method coupled with the bag-of-patterns (BoP) representation because of their state-of-the-art performance in the single representation/classifier case. Experiments with a number of standard benchmark time series datasets and a new dataset of vital signs collected from patients suffering from traumatic brain injury demonstrate the power of the ensemble approaches. The result is a single method that is often significantly better than vanilla SAX/BoP and compares favorably on a per dataset basis with the best methods reported in the literature for each dataset.},
  comment   = {Maybe useful for regime learning? However, it's doing binary predition in the end, I think.

Possibly, the binary thing could be "which analog ensemble members would be good for a probabilistic forecast right now"},
  doi       = {10.1109/ICMLA.2012.186},
  file      = {Oates12repDiversTSsax.pdf:Oates12repDiversTSsax.pdf:PDF},
  groups    = {Ensemble, doReadWPV_2},
  keywords  = {approximation theory;brain;injuries;medical computing;pattern classification;pattern clustering;time series;BoP representation ensembles;SAX discretization method;bag-of-patterns representation;classifier ensembles;performance improvement;representation measurement;similarity measurement;standard benchmark time series datasets;symbolic aggregate approximation discretization method;time series classification;time series clustering;traumatic brain injury patients;Accuracy;Entropy;Error analysis;Heart rate;Standards;Time series analysis;Training;classification;ensemble;traumatic brain injury;vital signs},
  owner     = {sotterson},
  timestamp = {2013.03.13},
}

@InProceedings{Xiaoyan08spectClustSelfAdapt,
  author    = {Cai Xiaoyan and Dai Guanzhong and Yang Libin and Zhang Guoqing},
  title     = {A self-adaptive spectral clustering algorithm},
  booktitle = {Control Conference, 2008. CCC 2008. 27\textsuperscript{th} Chinese},
  year      = {2008},
  pages     = {551--553},
  month     = jul,
  abstract  = {Most existing algorithms on spectral clustering are not able to determine the number of clusters. In this paper, we prove theoretically that the eigenvectors of the affinity matrix can be used directly to cluster the data points. And we suggest exploiting the structure of the eigenvectors to infer automatically the number of clusters. As a result, a self-adaptive spectral clustering algorithm based on affinity matrix is proposed. The experimental results on the UCI data sets show that the algorithm is more effective than previous algorithms.},
  comment   = {Determines the num. of clusters for a ng01specClust type algorithm. This paper isn't cited (2011) so I'm not sure if it's good.},
  doi       = {10.1109/CHICC.2008.4605517},
  keywords  = {UCI data sets;affinity matrix;eigenvectors;self-adaptive spectral clustering algorithm;eigenvalues and eigenfunctions;pattern clustering;},
  owner     = {sotterson},
  timestamp = {2011.11.17},
}

@Article{Tomasev13imbalKnnHubs,
  author    = {Toma{\v{s}}ev, Nenad and Mladeni{\'c}, Dunja},
  title     = {Class imbalance and the curse of minority hubs},
  journal   = {Knowledge-Based Systems},
  year      = {2013},
  volume    = {53},
  pages     = {157--172},
  abstract  = {Most machine learning tasks involve learning from high-dimensional data, which is often quite difficult
to handle. Hubness is an aspect of the curse of dimensionality that was shown to be highly detrimental to
k-nearest neighbor methods in high-dimensional feature spaces. Hubs, very frequent nearest neighbors,
emerge as centers of influence within the data and often act as semantic singularities. This paper deals
with evaluating the impact of hubness on learning under class imbalance with k-nearest neighbor meth-
ods. Our results suggest that, contrary to the common belief, minority class hubs might be responsible for
most misclassification in many high-dimensional datasets. The standard approaches to learning under
class imbalance usually clearly favor the instances of the minority class and are not well suited for han-
dling such highly detrimental minority points. In our experiments, we have evaluated several state-of-
the-art hubness-aware kNN classifiers that are based on learning from the neighbor occurrence models
calculated from the training data. The experiments included learning under severe class imbalance, class
overlap and mislabeling and the results suggest that the hubness-aware methods usually achieve prom-
ising results on the examined high-dimensional datasets. The improvements seem to be most pro-
nounced when handling the difficult point types: borderline points, rare points and outliers. On most
examined datasets, the hubness-aware approaches improve the classification precision of the minority
classes and the recall of the majority class, which helps with reducing the negative impact of minority
hubs. We argue that it might prove beneficial to combine the extensible hubness-aware voting frame-
works with the existing class imbalanced kNN classifiers, in order to properly handle class imbalanced
data in high-dimensional feature spaces.},
  comment   = {How to improve KNN when data has class imbalance, especially in high dim. could this be used in regression mixtures or Extreme event ID? Maybe just rare production on the high end of the power curve? Some low dim techniques do the exact wrong thing. Tests done using algs. specifically designed for hubs, leaving me unsure about MP in Flexer13shrKNNhubRdc. Has a hubness visualization tool [ref, 77]

THE LOW/HIGH DIM ERROR SWAP

Low dimension
* KNN has "high specificity bias" since it retains all training data, normally considered good with unbalanced data
 - but KNN not good at estimating probabilities when the density is low, so somewhat sensitive
* algs w/ high "generality bias bad on imbalanced data -- the overgeneralize majority, wiping out the miniority.

High dimension
* The minority class induces errors ON THE MAJORITY class!
 - especially bad if minority are high dim hubs
 --- example: only 5 bad points in a dataset of 2731 cases reduced accuracy from 90\% to 21.2\%
 --- hubness allows errors to propagate through the data
* so, low dim methods that enhance minortity class are the wrong thing for high dim
* random forests bad when lots of class overlap (combined w/ high dims, I think)

HUBNESS AWARE KNN CLASSIFIERS
* hw-KNN
* h-FNN
* NHBNN:
 - naive Bayes inference
 - simple yet has especially good performance
* HIKNN
* none of these are the methods used in Schnitzer12Localandglobal

RESULTS
* on artificial test data
* the methods do reduce classification error due to imbalanced, high dims
* NHBNN was especially good at borderline examples, rare points, and outliers

* Hubness visulazation tool, the "image hub explorer," ref 77 is available for download.},
  file      = {Tomasev13imbalKnnHubs.pdf:Tomasev13imbalKnnHubs.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.04.04},
  url       = {http://www.sciencedirect.com/science/article/pii/S0950705113002682},
}

@Article{Hunter04mmQRtutorial,
  author    = {Hunter, David R and Lange, Kenneth},
  title     = {A tutorial on MM algorithms},
  journal   = {The American Statistician},
  year      = {2004},
  volume    = {58},
  number    = {1},
  pages     = {30--37},
  abstract  = {Most problems in frequentist statistics involve optimization of a function
such as a likelihood or a sum of squares. EM algorithms are among the most
effective algorithms for maximum likelihood estimation because they consistently
drive the likelihood uphill by maximizing a simple surrogate function for
the loglikelihood. Iterative optimization of a surrogate function as exemplified
by an EM algorithm does not necessarily require missing data. Indeed, every
EM algorithm is a special case of the more general class of MM optimization
algorithms, which typically exploit convexity rather than missing data in majorizing
or minorizing an objective function. In our opinion, MM algorithms
deserve to part of the standard toolkit of professional statisticians. The current
article explains the principle behind MM algorithms, suggests some methods
for constructing them, and discusses some of their attractive features. We
include numerous examples throughout the article to illustrate the concepts
described. In addition to surveying previous work on MM algorithms, this article
introduces some new material on constrained optimization and standard
error estimation.
Key words and phrases: constrained optimization, EM algorithm, majorization,
minorization, Newton-Raphson},
  comment   = {Tutorial for Hunter00qrMMalg . Said to have good potential for high dimensional optimization algorithms.

Good in high dim optimization:
http://projecteuclid.org/euclid.ss/1300108233},
  doi       = {10.1198/0003130042836},
  file      = {Hunter04mmQRtutorial.pdf:Hunter04mmQRtutorial.pdf:PDF},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2014.05.13},
}

@Article{Biau11seqQuantRgrsnTS,
  author    = {Biau, G. and Patra, B.},
  title     = {Sequential Quantile Prediction of Time Series},
  journal   = {Information Theory, IEEE Transactions on},
  year      = {2011},
  volume    = {57},
  number    = {3},
  pages     = {1664--1674},
  issn      = {0018-9448},
  abstract  = {Motivated by a broad range of potential applications, we address the quantile prediction problem of real-valued time series. We present a sequential quantile forecasting model based on the combination of a set of elementary nearest neighbor-type predictors called "experts" and show its consistency under a minimum of conditions. Our approach builds on the methodology developed in recent years for prediction of individual sequences and exploits the quantile structure as a minimizer of the so-called pinball loss function. We perform an in-depth analysis of real-world data sets and show that this nonparametric strategy generally outperforms standard quantile prediction methods.

Index Terms - Consistency, expert aggregation, nearest neighbor
estimation, pinball loss, quantile prediction, sequential prediction,
time series.},
  comment   = {A non-parametric quantile forecaster that maybe is like particle filtering. Somehow directly uses "check" or "pinball" loss function.

I don't know if this is like streaming regression or is just adaptive.


May or may not respect temporal correlation.

Will have to read the paper eventually.

This could be related to the continuous adaboost ideas, or to quantile regression forests},
  doi       = {10.1109/TIT.2011.2104610},
  file      = {Biau11seqQuantRgrsnTS.pdf:Biau11seqQuantRgrsnTS.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_1},
  keywords  = {sequential estimation;time series;real-valued time series;sequential quantile forecasting model;Context;Forecasting;Minimization;Nearest neighbor searches;Predictive models;Random variables;Time series analysis;Consistency;expert aggregation;nearest neighbor estimation;pinball loss;quantile prediction;sequential prediction;time series},
  owner     = {sotterson},
  timestamp = {2013.11.07},
}

@Article{Zhu12multiVaryCfsFncResp,
  author    = {Zhu, Hongtu and Li, Runze and Kong, Linglong},
  title     = {Multivariate varying coefficient model for functional responses},
  journal   = {Annals of statistics},
  year      = {2012},
  volume    = {40},
  number    = {5},
  pages     = {2634},
  abstract  = {Motivated by recent work studying massive imaging data in the neuroimaging literature, we
propose multivariate varying coefficient models (MVCM) for modeling the relation between
multiple functional responses and a set of covariates. We develop several statistical inference
procedures for MVCM and systematically study their theoretical properties. We first establish the
weak convergence of the local linear estimate of coefficient functions, as well as its asymptotic
bias and variance, and then we derive asymptotic bias and mean integrated squared error of
smoothed individual functions and their uniform convergence rate. We establish the uniform
convergence rate of the estimated covariance function of the individual functions and its
associated eigenvalue and eigenfunctions. We propose a global test for linear hypotheses of
varying coefficient functions, and derive its asymptotic distribution under the null hypothesis. We
also propose a simultaneous confidence band for each individual effect curve. We conduct Monte
Carlo simulation to examine the finite-sample performance of the proposed procedures. We apply
MVCM to investigate the development of white matter diffusivities along the genu tract of the
corpus callosum in a clinical study of neurodevelopment.
Keywords and phrases
Functional response; Global test statistic; Multivariate varying coefficient model; Simultaneous
confidence band; Weak convergence},
  comment   = {Potential base learners for boosting, lagged wind velocity basis function.
Starts out in much the same way as: Zhu14SpatVarCoeffJump},
  file      = {Zhu12multiVaryCfsFncResp.pdf:Zhu12multiVaryCfsFncResp.pdf:PDF},
  owner     = {sotterson},
  publisher = {NIH Public Access},
  timestamp = {2014.11.10},
  url       = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3641708/},
}

@Article{Singh03knnEntrpyEst,
  author    = {Singh, Harshinder and Misra, Neeraj and Hnizdo, Vladimir and Fedorowicz, Adam and Demchuk, Eugene},
  title     = {Nearest neighbor estimates of entropy},
  journal   = {Nearest neighbor estimates of entropy},
  year      = {2003},
  volume    = {23},
  number    = {3-4},
  pages     = {301--321},
  abstract  = {Motivated by the problems in molecular sciences, we introduce new non-
parametric estimators of entropy which are based on the k? nearest neighbor
distances between the 11 sample points, where k (5 n -- 1) is a fixed posi-
tive integer. These provide competing estimators to an estimator pmposed by
Kozachenko and Leonenko (1987), which is based on the first nearest neighbor
distances of the sample points. These estimators are helpful in the evaluation
of entropies of random vectors. We establish the asymptotic unbiasedness and
consistency of the proposed estimators. For some standard distributions, we
also investigate their performance for finite sample sizes using Monte Carlo
simulations. The proposed estimators are applied to estimate the entropy of
internal rotation in the methanol molecule, which can be characterized by a
one-dimensional random vector, and of diethyl ether, which is described by a
four-dimensional random vector.},
  comment   = {Shows that the naive KNN entropy estimator in Gao15mutInfoStrongDep is asympotically unbiased.  I believe this was called the "Kozachenko and Leonenko " estimator in Kraskov04EstMutInfKNN},
  doi       = {10.1080/01966324.2003.10737616},
  file      = {Singh03knnEntrpyEst.pdf:Singh03knnEntrpyEst.pdf:PDF},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2016.09.14},
}

@Article{Zuber09geneRankCorr,
  author    = {Zuber, V. and Strimmer, K.},
  title     = {Gene ranking and biomarker discovery under correlation},
  journal   = {Bioinformatics},
  year      = {2009},
  volume    = {25},
  number    = {20},
  pages     = {2700},
  abstract  = {Motivation: Biomarker discovery and gene ranking is a standard task in genomic high-throughput analysis. Typically, the ordering of markers is based on a stabilized variant of the t-score, such as the moderated t or the SAM statistic. However, these procedures ignore gene?gene correlations, which may have a profound impact on the gene orderings and on the power of the subsequent tests.Results: We propose a simple procedure that adjusts gene-wise t-statistics to take account of correlations among genes. The resulting correlation-adjusted t-scores (?cat? scores) are derived from a predictive perspective, i.e. as a score for variable selection to discriminate group membership in two-class linear discriminant analysis. In the absence of correlation the cat score reduces to the standard t-score. Moreover, using the cat score it is straightforward to evaluate groups of features (i.e. gene sets). For computation of the cat score from small sample data, we propose a shrinkage procedure. In a comparative study comprising six different synthetic and empirical correlation structures, we show that the cat score improves estimation of gene orderings and leads to higher power for fixed true discovery rate, and vice versa. Finally, we also illustrate the cat score by analyzing metabolomic data.Availability: The shrinkage cat score is implemented in the R package ?st?, which is freely available under the terms of the GNU General Public License (version 3 or later) from CRAN (http://cran.r-project.org/web/packages/st/).Contact: strimmer\@uni-leipzig.de},
  comment   = {* shrinkage method used in Zuber10varImpSelDecorr (wasn't it also the same as Schafer05shrinkCov?)},
  file      = {paper:Zuber09geneRankCorr.pdf:PDF;Zuber09geneRankCorr.pdf:Zuber09geneRankCorr.pdf:PDF},
  owner     = {scotto},
  publisher = {Oxford Univ Press},
  timestamp = {2010.09.26},
}

@Article{Oba03pcaImp,
  author    = {Oba, Shigeyuki and Sato, Masa-aki and Takemasa, Ichiro and Monden, Morito and Matsubara, Ken-ichi and Ishii, Shin},
  title     = {A {Bayesian} missing value estimation method for gene expression profile data},
  journal   = {Bioinformatics},
  year      = {2003},
  volume    = {19},
  number    = {16},
  pages     = {2088--2096},
  abstract  = {Motivation: Gene expression profile analyses have been used in numerous studies covering a broad range of areas in biology. When unreliable measurements are excluded, missing values are introduced in gene expression profiles. Although existing multivariate analysis methods have difficulty with the treatment of missing values, this problem has received little attention. There are many options for dealing with missing values, each of which reaches drastically different results. Ignoring missing values is the simplest method and is frequently applied. This approach, however, has its flaws. In this article, we propose an estimation method for missing values, which is based on Bayesian principal component analysis (BPCA). Although the methodology that a probabilistic model and latent variables are estimated simultaneously within the framework of Bayes inference is not new in principle, actual BPCA implementation that makes it possible to estimate arbitrary missing variables is new in terms of statistical methodology.Results: When applied to DNA microarray data from various experimental conditions, the BPCA method exhibited markedly better estimation ability than other recently proposed methods, such as singular value decomposition and K-nearest neighbors. While the estimation performance of existing methods depends on model parameters whose determination is difficult, our BPCA method is free from this difficulty. Accordingly, the BPCA method provides accurate and convenient estimation for missing values.Availability: The software is available at http://hawaii.aist-nara.ac.jp/~shige-o/tools/},
  comment   = {EM-like estimation of missing values, but using BPCA. Has Matlab, Java, R. Good explanations. The PCA trick could be used to generate missing values on a separate test set, using the covariance matrix estimated by Amelia II (Honaker10missValTseries) How to estimate missing values (p. 2089-2090): 1.) normalize inputs 2.) compute their covariance matrix (on fully-observed data, with EM (like Amelia II), or with the EM-like trick in this paper) 3.) find the cov matrix principal components: these form an orthogonal basis for estimating missing values 4.) estimate the weight of each eigenvector for reconstructing a complete observation by finding the least squares weight on the observed parts only 5.) calculate the missing observations by weighting the appropriate parts of the eigenvectors using these weights. This PCA idea is in quite a few papers, I think. Matlab available at: http://hawaii.sys.i.kyoto-u.ac.jp/~oba/tools/BPCAFill.html Note that the selection of the number of PCA comps during BPCA is kind of like regularization, as in RegEM, Schneider01missImputMeanCov. Also, Schneider01missImputMeanCov does select the num. comps (using AIC and alternatives) if its using a PCA approach like this one.},
  doi       = {10.1093/bioinformatics/btg287},
  eprint    = {http://bioinformatics.oxfordjournals.org/content/19/16/2088.full.pdf+html},
  file      = {Oba03pcaImp.pdf:Oba03pcaImp.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2012.07.25},
  url       = {http://bioinformatics.oxfordjournals.org/content/19/16/2088.abstract},
}

@Article{Sales11parMutInfoEst,
  author    = {Sales, G. and Romualdi, C.},
  title     = {parmigene--a parallel {R} package for mutual information estimation and gene network reconstruction},
  journal   = {Bioinformatics},
  year      = {2011},
  volume    = {27},
  number    = {13},
  pages     = {1876--7},
  abstract  = {MOTIVATION: Inferring large transcriptional networks using mutual information has been shown to be effective in several experimental setup. Unfortunately, this approach has two main drawbacks: (i) several mutual information estimators are prone to biases and (ii) available software still has large computational costs when processing thousand of genes. RESULTS: Here, we present parmigene (PARallel Mutual Information estimation for GEne NEtwork reconstruction), an R package that tries to fill the above gaps. It implements a mutual information estimator based on k-nearest neighbor distances that is minimally biased with respect to the other methods and uses a parallel computing paradigm to reconstruct gene regulatory networks. We test parmigene on in silico and real data. We show that parmigene gives more precise results than existing softwares with strikingly less computational costs. AVAILABILITY AND IMPLEMENTATION: The parmigene package is available on the CRAN network at http://cran.r-project.org/web/packages/},
  comment   = {Kraskov MI estimator less biased than other methods tested, produces better gene network results.
* I'm not clear on the dimensionality of the problem.
* Seems to be similar to building a graphical model based on MI
* This is a good application for Kraskov method (Kraskov04EstMutInfKNN), since it produces zero (independence) when it's supposed to (but may have bias for positive MI cases)},
  file      = {Sales11parMutInfoEst.pdf:Sales11parMutInfoEst.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2012.02.17},
}

@Article{Motakis06varStabNorm,
  author               = {Motakis and { E. S.} and Nason and { G. P.} and Fryzlewicz and { P.} and Rutter and { G. A.}},
  title                = {Variance stabilization and normalization for one-color microarray data using a data-driven multiscale approach},
  journal              = {Bioinformatics},
  year                 = {2006},
  volume               = {22},
  number               = {20},
  pages                = {2547--2553},
  month                = oct,
  issn                 = {1367-4803},
  abstract             = {Motivation: Many standard statistical techniques are effective on data that are normally distributed with constant variance. Microarray data typically violate these assumptions since they come from non- Gaussian distributions with a non-trivial mean?variance relationship. Several methods have been proposed that transform microarray data to stabilize variance and draw its distribution towards the Gaussian. Some methods, such as log or generalized log, rely on an underlying model for the data. Others, such as the spread-versus-level plot, do not. We propose an alternative data-driven multiscale approach, called the Data-Driven Haar?Fisz for microarrays (DDHFm) with replicates. DDHFm has the advantage of being ?distribution-free? in the sense that no parametric model for the underlying microarray data is required to be specified or estimated; hence, DDHFm can be applied very generally, not just to microarray data. Results: DDHFm achieves very good variance stabilization of microarray data with replicates and produces transformed intensities that are approximately normally distributed. Simulation studies show that it performs better than other existing methods. Application of DDHFm to real one-color cDNA data validates these results. Availability: The R package of the Data-Driven Haar?Fisz transform (DDHFm) for microarrays is available in Bioconductor and CRAN. Contact: g.p.nason\@bristol.ac.uk},
  citeulike-article-id = {898029},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/bioinformatics/btl412},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/oup/cabios/2006/00000022/00000020/art02547},
  comment              = {Gaussianization technique in DDHFm R package Binonmial implementation is in binhf R package Harr approach seems to expect piecewise constant multivariate means (when viewed in the time direction) so I'm not sure if this could be useful for scenario generation. But maybe it would help for regime switching or something?},
  day                  = {15},
  doi                  = {10.1093/bioinformatics/btl412},
  file                 = {Motakis06varStabNorm.pdf:Motakis06varStabNorm.pdf:PDF},
  owner                = {scot},
  posted-at            = {2006-10-15 11:48:37},
  publisher            = {Oxford University Press},
  timestamp            = {2010.08.02},
}

@InProceedings{Li16waveletLSTMeegRecog,
  author    = {M. Li and M. Zhang and X. Luo and J. Yang},
  title     = {Combined long short-term memory based network employing wavelet coefficients for MI-{EEG} recognition},
  booktitle = {Proc. IEEE Int. Conf. Mechatronics and Automation},
  year      = {2016},
  pages     = {1971--1976},
  month     = aug,
  abstract  = {Motor Imagery Electroencephalography (MI-
EEG) plays an important role in brain computer interface (BCI)
based rehabilitation robot, and its recognition is the key problem.
The Discrete Wavelet Transform (DWT) has been applied to
extract the time-frequency features of MI-EEG. However, the
existing EEG classifiers, such as support vector machine (SVM),
linear discriminant analysis (LDA) and BP network, did not make
full use of the time sequence information in time-frequency
features, the resulting recognition performance were not very
ideal. In this paper, a Long Short-Term Memory (LSTM) based
recurrent Neural Network (RNN) is integrated with Discrete
Wavelet Transform (DWT) to yield a novel recognition method,
denoted as DWT-LSTM. DWT is applied to analyze the each
channel of MI-EEG and extract its effective wavelet coefficients,
representing the time-frequency features. Then a LSTM based
RNN is used as a classifier for the patten recognition of observed
MI-EEG data. Experiments are conducted on a publicly available
dataset, and the 5-fold cross validation experimental results show
that DWT-LSTM yields relatively higher classification accuracies
compared to the existing approaches. This is helpful for the
further research and application of RNN in processing of MI-
EEG.

Index Terms - Brain computer interface; Long Short-Term
Memory; Recurrent Neural Network; Discrete Wavelet Transform;
motor imagery EEG;},
  comment   = {Wavelet decomp and LSTM is better for time series pattern recog than other things.  The LSTM is predicting a categorical variable.  Wavelet packet basis choice seems to be done by hand, rather than with an algorithm.

Maybe related to: Madiraju18deepUnsupTSclust},
  doi       = {10.1109/ICMA.2016.7558868},
  file      = {:papers\\Li16waveletLSTMeegRecog.pdf:PDF},
  keywords  = {backpropagation, brain-computer interfaces, discrete wavelet transforms, electroencephalography, feature extraction, medical signal processing, neural nets, patient rehabilitation, signal classification, support vector machines, time-frequency analysis, BCI, BP network, DWT-LSTM, EEG classifiers, LDA, LSTM based RNN, MI-EEG data, MI-EEG recognition, Motor Imagery Electroencephalography, SVM, brain computer interface based rehabilitation robot, discrete wavelet transform, effective wavelet coefficients, linear discriminant analysis, long short-term memory based network, patten recognition, recognition performance, recurrent neural network, support vector machine, time sequence information, time-frequency feature extraction, Discrete wavelet transforms, Electroencephalography, Feature extraction, Logic gates, Recurrent neural networks, Time-frequency analysis, Brain computer interface, Discrete Wavelet Transform, Long Short-Term Memory, Recurrent Neural Network, motor imagery EEG},
  owner     = {sotterson},
  timestamp = {2017.07.04},
}

@Article{Kalpakis01distClustARIMA,
  author    = {Kalpakis, K. and Gada, D. and Puttagunta, V.},
  title     = {Distance measures for effective clustering of ARIMA time-series},
  journal   = {International Conference on Data Mining (ICDM)},
  year      = {2001},
  pages     = {273--280},
  abstract  = {Much environmental and socioeconomic time-series data can be adequately modeled using autoregressive integrated moving average (ARIMA) models. We call such time series "ARIMA time series". We propose the use of the linear predictive coding (LPC) cepstrum for clustering ARIMA time series, by using the Euclidean distance between the LPC cepstra of two time series as their dissimilarity measure. We demonstrate that LPC cepstral coefficients have the desired features for accurate clustering and efficient indexing of ARIMA time series. For example, just a few LPC cepstral coefficients are sufficient in order to discriminate between time series that are modeled by different ARIMA models. In fact, this approach requires fewer coefficients than traditional approaches, such as DFT (discrete Fourier transform) and DWT (discrete wavelet transform). The proposed distance measure can be used for measuring the similarity between different ARIMA models as well. We cluster ARIMA time series using the "partition around medoids" method with various similarity measures. We present experimental results demonstrating that, using the proposed measure, we achieve significantly better clusterings of ARIMA time series data as compared to clusterings obtained by using other traditional similarity measures, such as DFT, DWT, PCA (principal component analysis), etc. Experiments were performed both on simulated and real data},
  comment   = {Cited by a huge num. of papers (102, last count) http://scholar.google.com/scholar?hl=en&client=firefox-a&rls=org.mozilla:en-US:official&hs=WoE&q=Distance+Measures+for+Effective+Clustering+of+ARIMA+Time-Series&oe=UTF-8&um=1&ie=UTF-8&sa=N&tab=ws&ei=pMu6SdTUMZK2sAOMpNQt&oi=property_suggestions&resnum=0&ct=property-revision&cd=1},
  doi       = {10.1109/ICDM.2001.989529},
  file      = {Kalpakis01distClustARIMA.pdf:Kalpakis01distClustARIMA.pdf:PDF;Kalpakis01distClustARIMA.pdf:Kalpakis01distClustARIMA.pdf:PDF},
  keywords  = {autoregressive moving average processes, cepstral analysis, data mining, economic cybernetics, environmental factors, linear predictive coding, pattern clustering, social sciences, socio-economic effects, temporal databases, time seriesARIMA time-series clustering, Euclidean distance, LPC cepstral coefficients, autoregressive integrated moving average, dissimilarity measure, distance measure, environmental data, indexing, linear predictive coding, partition-around-medoids method, similarity measures, socioeconomic data},
  owner     = {sotterson},
  timestamp = {2009.03.13},
}

@Book{Koenker05QuantRgrssnBook,
  title     = {Quantile regression},
  publisher = {Cambridge University Press},
  year      = {2005},
  author    = {Koenker, Roger},
  number    = {38},
  abstract  = {Much of applied statistics may be viewed as an elaboration of the linear regression
model and associated estimation methods of least squares. In beginning to describe
these techniques, Mosteller and Tukey (1977), in their influential text, remark:
What the regression curve does is give a grand summary for the averages of the distributions
corresponding to the set of xs. We could go further and compute several different regression curves
corresponding to the various percentage points of the distributions and thus get a more complete picture of
the set. Ordinarily this is not done, and so regression often gives a rather incomplete picture. Just as the
mean gives an incomplete picture of a single distribution, so the regression curve gives a correspondingly
incomplete picture for a set of distributions.
My objective in the following pages is to describe explicitly how to ?go further.?
Quantile regression is intended to offer a comprehensive strategy for completing
the regression picture.
Why does least-squares estimation of the linear regression model so pervade
applied statistics? What makes it such a successful tool? Three possible answers
suggest themselves. One should not discount the obvious fact that the computational
tractability of linear estimators is extremely appealing. Surely this was the initial
impetus for their success. Second, if observational noise is normally distributed
(i.e., Gaussian), least-squares methods are known to enjoy a certain optimality.
But, as it was for Gauss himself, this answer often appears to be an ex post
rationalization designed to replace the first response. More compelling is the
relatively recent observation that least-squares methods provide a general
approach to estimating conditional mean functions.
And yet, as Mosteller and Tukey suggest, the mean is rarely a satisfactory end in
itself, even for statistical analysis of a single sample. Measures of spread,
skewness, kurtosis, boxplots, histograms, and more sophisticated density
estimation are all frequently employed to gain further insight. Can something
similar be done in regression? A natural starting place for this would be to supplement the conditional mean surfaces estimated by least squares with several
estimated conditional quantile surfaces. In the chapters that follow, methods are
described to accomplish this task. The basic ideas go back to the earliest work on
regression by Boscovich in the mid-18\textsuperscript{th} century to Edgeworth at the end of the
19\textsuperscript{th} century.},
  comment   = {The classic quantile regression book.

quantile regression
penalized spline quantile regression
* penalized splines: references: Meyer00degFreedomShpCnstr
...},
  file      = {Koenker05QuantRgrssnBook.pdf:Koenker05QuantRgrssnBook.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.05.09},
  url       = {http://www.cambridge.org/de/academic/subjects/economics/econometrics-statistics-and-mathematical-economics/quantile-regression?format=HB},
}

@Article{Stuart09missDatLargeDat,
  author    = {Stuart, Elizabeth A. and Azur, Melissa and Frangakis, Constantine and Leaf, Philip},
  title     = {Multiple Imputation With Large Data Sets: A Case Study of the Children's Mental Health Initiative},
  journal   = {American Journal of Epidemiology},
  year      = {2009},
  volume    = {169},
  number    = {9},
  pages     = {1133--1139},
  abstract  = {Multiple imputation is an effective method for dealing with missing data, and it is becoming increasingly common in many fields. However, the method is still relatively rarely used in epidemiology, perhaps in part because relatively few studies have looked at practical questions about how to implement multiple imputation in large data sets used for diverse purposes. This paper addresses this gap by focusing on the practicalities and diagnostics for multiple imputation in large data sets. It primarily discusses the method of multiple imputation by chained equations, which iterates through the data, imputing one variable at a time conditional on the others. Illustrative data were derived from 9,186 youths participating in the national evaluation of the Community Mental Health Services for Children and Their Families Program, a US federally funded program designed to develop and enhance community-based systems of care to meet the needs of children with serious emotional disturbances and their families. Multiple imputation was used to ensure that data analysis samples reflect the full population of youth participating in this program. This case study provides an illustration to assist researchers in implementing multiple imputation in their own data.},
  comment   = {How to use R package "mice" for multiple imputation on large datasets},
  doi       = {10.1093/aje/kwp026},
  eprint    = {http://aje.oxfordjournals.org/content/169/9/1133.full.pdf+html},
  file      = {Stuart09missDatLargeDat.pdf:Stuart09missDatLargeDat.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.09.09},
  url       = {http://aje.oxfordjournals.org/content/169/9/1133.abstract},
}

@Article{Nathans12mlrVarImp,
  author    = {Nathans, Laura L and Oswald, Frederick L and Nimon, Kim},
  title     = {Interpreting multiple linear regression: A guidebook of variable importance},
  journal   = {Practical Assessment, Research \& Evaluation},
  year      = {2012},
  volume    = {17},
  number    = {9},
  abstract  = {Multiple regression (MR) analyses are commonly employed in social science fields. It is also common for interpretation of results to typically reflect overreliance on beta weights (cf. Courville & Thompson, 2001; Nimon, Roberts, & Gavrilova, 2010; Zientek, Capraro, & Capraro, 2008), often resulting in very limited interpretations of variable importance. It appears that few researchers employ other methods to obtain a fuller understanding of what and how independent variables contribute to a regression equation. Thus, this paper presents a guidebook of variable importance measures that inform MR results, linking measures to a theoretical framework that demonstrates the complementary roles they play when interpreting regression findings. We also provide a data-driven example of how to publish MR results that demonstrates how to present a more complete picture of the contributions variables make to a regression equation. We end with several recommendations for practice regarding how to integrate multiple variable importance measures into MR analyses},
  comment   = {Assessing the importance of a variable in linear regression.  Best method and depends upon the goal and the data.  But surprisingly, stepwise regression is strongly unrecommended.

Stepwise
* prone to selecting too many variables.
* entry order does NOT correspond to variable importance
* entry order is vulnerable to tiny amounts of sampling error

Picking PCA components for regression

But for picking the most important PCA components in a regression problem, where the components are totally uncorrelated, then the square of the beta coefficent tells you the importance.  For this case, the say no more complicated approaches are needed, not even stepwise regression, which they criticize.

* Beta coefficient: the linear regression coefficient when the inputs are z-scored.
* But if have the ranking, how many should you pick?  Based on significance, somehow?
* Related to Zuber10varImpSelDecorr

},
  file      = {:Nathans12mlrVarImp.pdf:PDF},
  publisher = {Practical Assessment, Research \& Evaluation},
  url       = {https://scholarship.rice.edu/handle/1911/71096},
}

@InProceedings{Sutiene07scenCopula,
  author    = {K. Sutiene and H. Pranevicius},
  title     = {Scenario Generation Employing Copulas},
  booktitle = {Proceedings of the World Congress on Engineering},
  year      = {2007},
  abstract  = {Multistage stochastic programs are effective for solving long-term planning problems under uncertainty. Such programs are usually based on scenario generation model about future environment developments. In the present paper, the scenario model is developed for the case when enough data paths can be generated, but due to solvability of stochastic program the scenario tree has to be constructed. The proposed strategy is to generate multistage scenario tree from the set of individual scenarios by bundling scenarios based on cluster analysis. The K-means clustering approach is modified to capture the interstage dependencies. Such generation of scenario tree can be useful in cases when it is difficult to construct the adequate scenario tree from the stochastic differential equations or time-series models, and the sampled paths can be obtained by sampling or resampling techniques. While generating the initial fan of individual scenarios, the copula is employed for modeling the dependence between stochastic variables in a multivariate structure. This allows to model nonlinear dependencies between non-elliptically distributed stochastic variables. While investigating the copula effect on the scenario tree structure, we will try to answer the question: does the copula features are captured in the approximate representation of uncertainty in the form of scenario tree. The proposed scenario tree generation method is implemented on sampled data of discount bond yields. The Gaussian copula and Student's t-copula are employed while generating the set of individual scenarios in the multivariate structure.

Index Terms: Copula, K-means clustering, Multistage scenario tree construction, Stochastic programming.},
  comment   = {Why linear correlation fails for scenario dependence. Maybe a good tutorial on scenarios with copulas? Scenarios squashed with k-means},
  file      = {Sutiene07scenCopula.pdf:Sutiene07scenCopula.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.11.23},
  url       = {http://www.doaj.org/doaj?func=abstract&id=550788&recNo=7&toc=1},
}

@InProceedings{BenTaieb14BoostMultiStep,
  author    = {Ben Taieb, Souhaib and Hyndman, Rob},
  title     = {Boosting multi-step autoregressive forecasts},
  booktitle = {Proceedings of The 31\textsuperscript{st} International Conference on Machine Learning},
  year      = {2014},
  pages     = {109--117},
  abstract  = {Multi-step forecasts can be produced recursively
by iterating a one-step model, or directly using a
specific model for each horizon. Choosing between
these two strategies is not an easy task
since it involves a trade-off between bias and estimation
variance over the forecast horizon. Using
a nonlinear machine learning model makes the
tradeoff even more difficult. To address this issue,
we propose a new forecasting strategy which
boosts traditional recursive linear forecasts with
a direct strategy using a boosting autoregression
procedure at each horizon. First, we investigate
the performance of the proposed strategy in terms
of bias and variance decomposition of the error
using simulated time series. Then, we evaluate
the proposed strategy on real-world time series
from two forecasting competitions. Overall, we
obtain excellent performance with respect to the
standard forecasting strategies.},
  comment   = {I think this was the technique that placed 5\textsuperscript{th} out of 105 in a load forecasting contest},
  file      = {BenTaieb14BoostMultiStep.pdf:BenTaieb14BoostMultiStep.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.07.24},
  url       = {http://jmlr.org/proceedings/papers/v32/taieb14.html},
}

@Conference{Taieb14boostMultiStepARfrcst,
  author      = {Taieb, Souhaib Ben and Hyndman, Rob},
  title       = {Boosting multi-step autoregressive forecasts},
  booktitle   = {Proceedings of The 31\textsuperscript{st} International Conference on Machine Learning},
  year        = {2014},
  pages       = {109--117},
  abstract    = {Multi-step forecasts can be produced recursively
by iterating a one-step model, or directly using a
specific model for each horizon. Choosing between
these two strategies is not an easy task
since it involves a trade-off between bias and estimation
variance over the forecast horizon. Using
a nonlinear machine learning model makes the
tradeoff even more difficult. To address this issue,
we propose a new forecasting strategy which
boosts traditional recursive linear forecasts with
a direct strategy using a boosting autoregression
procedure at each horizon. First, we investigate
the performance of the proposed strategy in terms
of bias and variance decomposition of the error
using simulated time series. Then, we evaluate
the proposed strategy on real-world time series
from two forecasting competitions. Overall, we
obtain excellent performance with respect to the
standard forecasting strategies.},
  comment     = {A way to do very short term forecasts, when NWP doesn't work; also maybe another way to do statistical scenarios, since it's all recursive.

Interaction terms might be selectable using: Duvenaud11AddGausProcIntrct},
  file        = {Taieb14boostMultiStepARfrcst.pdf:Taieb14boostMultiStepARfrcst.pdf:PDF},
  institution = {Monash University, Department of Econometrics and Business Statistics},
  owner       = {sotterson},
  timestamp   = {2014.11.04},
  url         = {http://jmlr.org/proceedings/papers/v32/taieb14.pdf},
}

@Article{Ruder17multiTskDeepNN,
  author   = {Ruder, Sebastian},
  title    = {An overview of multi-task learning in deep neural networks},
  journal  = {arXiv preprint arXiv:1706.05098},
  year     = {2017},
  abstract = {Multi-task learning (MTL) has led to successes in many applications of machine
learning, from natural language processing and speech recognition to computer
vision and drug discovery. This article aims to give a general overview of MTL,
particularly in deep neural networks. It introduces the two most common methods
for MTL in Deep Learning, gives an overview of the literature, and discusses
recent advances. In particular, it seeks to help ML practitioners apply MTL
by shedding light on how MTL works and providing guidelines for choosing
appropriate auxiliary tasks.},
  comment  = {An intro to deep learning but forcused on NN's.  Should read Caruana97multiTaskLearninng first.},
  groups   = {Scott:1},
}

@Article{Caruana97multiTaskLearninng,
  author   = {Caruana, Rich},
  title    = {Multitask Learning},
  journal  = {Machine Learning},
  year     = {1997},
  volume   = {28},
  number   = {1},
  pages    = {41--75},
  month    = {Jul},
  issn     = {1573-0565},
  abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.

Keywords: inductive transfer, parallel transfer, multitask learning, backpropagation, k-nearest neighbor, kernel
regression, supervised learning, generalization},
  comment  = {Seems to be the seminal paper for multitask learning.  Useful for multi-DER adoption forecasting, probably, where there is way more PV dat than there is for, example, batteries (multitask learning helped with that in: Zou18multiTaskDisease)

A follow on paper to read could be Ruder17multiTskDeepNN, which sounds like a similar paper but updated, and focuses on deep NN's

Side notes from Andrew Ng online course on Youtube
* transfer learning more popular (2018?), except in multiclasse image classificiation (I think he said)
* only time it hurts is when the net is not complex enough (some my Fraunhofer DR1NN net was likely not complex enough!)},
  day      = {01},
  doi      = {10.1023/A:1007379606734},
  file     = {:Caruana97multiTaskLearninng.pdf:PDF},
  groups   = {Scott:1},
  url      = {https://doi.org/10.1023/A:1007379606734},
}

@InProceedings{Bessa16gaussCplaDcsns,
  author    = {R. J. Bessa},
  title     = {On the quality of the Gaussian copula for multi-temporal decision-making problems},
  booktitle = {Power Systems Computation Conference (PSCC)},
  year      = {2016},
  pages     = {1-7},
  month     = {June},
  abstract  = {Multi-temporal decision-making problems require information about the potential temporal trajectories of wind generation for a given time horizon. Typically, the Gaussian copula is used for modelling the dependency between probabilistic forecasts from different lead-times. This paper explores the vine copula framework as a benchmark model since it captures complex multivariate dependence structures with mixed types of dependencies. The results show that a Gaussian copula with a suitable covariance matrix suffice to generate high quality temporal trajectories.},
  comment   = {Gaussian Copula is a bit better than a vine copula for single farm wind power scenario generation, in terms of a multifarm score, but not better or worse for a ramp detection Brier score test.
This is only single farm, but they somehow evaluate with multivariate scores.  Huh?

Vine Copulas
* considered C and D vines
   - D-Vine: best for temporal sequence
   - C-Vine: best when one variable has lots of influence on all other variables
* they used D-vine since they're mainly modelling temporal dependence
* Can choose Frank, Gaussian (pair) or T0copula
* Two optimization times
   - sequential
      - choose one with min. AIC or BIC
      - estimate paremeters
      - use it to compute inputs to next copula tree
  - Partical swarm (EPSO).  they used this one.

Gausssian Copula
* they don't mention again the Gaussian copula tail problem
* Marginals are quantile copula forcasts in Bessa12adaptQuantCopulaFrcst
* Temporal dependence
   - modeled in covariance, not precision matrix.
   - uses exponential covariance function
     - time-distance exponential covariance dropoff
     - requires much less dat than, for example, D-vines
   - huge horizon: 1-48 hours ahead
   - cov mat with exponential  autocorr restriction much better than empirical cov.
      - spatio-temporal study,  Tastu15spcTimeTrajGaussCpla, found that empirical cov. mat was better
      - problems w/ spatial part of Tastu's cov. mat constraints?
      - is Tastu's temporal constraint different than the one here?
      - different data?
      - is spatio-temporal different enough from single farm that a different model is better?

Multivariate Performance Metrics
* CRPS
   - is scalar only
* energy score (ES), is multivariate
   - low discrimination of dependence structure
* P-variogram Score, is multivariate
   - much better than ES in terms of discrimination

Comparison with D-vines
* Have two datasets
  - Energy Score and P-Variogram Score
    - Farm 1,2: Gaussian Copula best
    - Farm 3: D-vine slightly best
  - Wind power ramp Brier score
     - the usual dumb and arbitrary ramp definition
     - "not possible" to say that D-Vine is better than copual},
  doi       = {10.1109/PSCC.2016.7541001},
  file      = {Bessa16gaussCplaDcsns.pdf:Bessa16gaussCplaDcsns.pdf:PDF},
  keywords  = {Gaussian processes;covariance matrices;decision making;load forecasting;wind power plants;Gaussian copula quality;complex multivariate dependence structures;covariance matrix;dependency modelling;high quality temporal trajectory generation;multitemporal decision making problems;probabilistic forecasts;vine copula framework;wind generation;Decision making;Density functional theory;Lead;Trajectory;Uncertainty;Wind forecasting;Wind power generation;Wind power;copula;decision-making;temporal trajectories;uncertainty;vines},
  owner     = {sotterson},
  timestamp = {2017.06.07},
}

@Article{Erdogmus06gaussianiz,
  author    = {Erdogmus, Deniz and Jenssen, Robert and Rao, Yadunandana N. and Pr\'{i}ncipe, Jos\'{e} C.},
  title     = {{Gauss}ianization: An Efficient Multivariate Density Estimation Technique for Statistical Signal Processing},
  journal   = {Journal of VLSI Signal Processing Systems},
  year      = {2006},
  volume    = {45},
  number    = {1-2},
  pages     = {67--83},
  issn      = {0922-5773},
  abstract  = {Multivariate density estimation is an important problem that is frequently encountered in statistical learning and signal processing. One of the most popular techniques is Parzen windowing, also referred to as kernel density estimation. Gaussianization is a procedure that allows one to estimate multivariate densities efficiently from the marginal densities of the individual random variables. In this paper, we present an optimal density estimation scheme that combines the desirable properties of Parzen windowing and Gaussianization, using minimum Kullback?Leibler divergence as the optimality criterion for selecting the kernel size in the Parzen windowing step. The utility of the estimate is illustrated in classifier design, independent components analysis, and Prices? theorem.},
  comment   = {Gaussianization: could use for lots of stuff. Not sure if there's an implementation ancestor paper: Erdogmus04gaussianizationICA},
  doi       = {10.1007/s11265-006-9772-7},
  file      = {Erdogmus06gaussianiz.pdf:Erdogmus06gaussianiz.pdf:PDF},
  location  = {Hingham, MA, USA},
  owner     = {scot},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2010.08.02},
}

@Article{Blumentritt11mutInfoAssoc,
  author    = {Thomas Blumentritt and Friedrich Schmid},
  title     = {Mutual information as a measure of multivariate association: analytical properties and statistical estimation},
  journal   = {Journal of Statistical Computation and Simulation},
  year      = {2011},
  volume    = {iFirst},
  pages     = {1--18},
  abstract  = {Mutual information (also known as Kullback--Leibler divergence) can be viewed as a measure of multivariate association in a random vector. The definition incorporates the joint density as well as the marginal densities. We will focus on a representation of mutual information in terms of copula densities that is thus independent of the marginal distributions. This representation yields a different approach to estimating mutual information than the original definition does, as only the copula density has to be estimated. We review analytical properties and examples for selected distributions and discuss methods of nonparametric estimation of copula densities and hence of the mutual information from a sample. Based on a simulation study, we compare the performance of these estimators with respect to bias, standard deviation, and the root mean squared error. The Gauss and the Frank copula are considered as examples. Keywords: mutual information, Kullback?Leibler divergence, copula density, nonparametric estimation, histogram, nearest neighbour estimator, Bernstein estimator, Beta kernel, Monte Carlo simulation},
  comment   = {Proofs on copula information; more on KNN bias w/ high dim, low sample size MI. Somewhat inconclusive, but truncated KNN may help reduce Kraskov bias. I only read this casually, but see that they have bias studies with results similar to what I found in the ramps paper.},
  doi       = {10.1080/00949655.2011.575782},
  file      = {Blumentritt11mutInfoAssoc.pdf:Blumentritt11mutInfoAssoc.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2012.02.17},
}

@Article{Ma11mutInfoCopulaEnt,
  author    = {Ma, J. and Sun, Z.},
  title     = {Mutual information is copula entropy},
  journal   = {Tsinghua Science \& Technology},
  year      = {2011},
  volume    = {16},
  number    = {1},
  pages     = {51--54},
  abstract  = {Mutual information (MI) is a basic concept in information theory. Therefore, estimates of the MI are fundamentally important in most information theory applications. This paper provides a new way of understanding and estimating the MI using the copula function. First, the entropy of the copula, named the copula entropy, is defined as a measure of the dependence uncertainty represented by the copula function and then the MI is shown to be equivalent to the negative copula entropy. With this equivalence, the MI can be estimated by first estimating the empirical copula and then estimating the entropy of the empirical copula. Thus, the MI estimate is an estimation of the entropy, which reduces the complexity and computational requirements. Tests show that the method is more effective than the traditional method. Key words: copula entropy; mutual information; estimation; empirical copula},
  comment   = {Equiv. to Kraskov04EstMutInfKNN on a test signal but far less computationally intensive. Total entropy, not pairwise.

What's the diff between this and Poczos10rankEstRenInfo ?

Is this another way to estimate McGill Interaction entropy?},
  file      = {Ma11mutInfoCopulaEnt.pdf:Ma11mutInfoCopulaEnt.pdf:PDF},
  owner     = {scotto},
  publisher = {Elsevier},
  timestamp = {2011.05.14},
}

@Article{Ross14MutInfoCntnAndDscrt,
  author    = {Ross, Brian C.},
  title     = {Mutual Information between Discrete and Continuous Data Sets},
  journal   = {PLoS ONE},
  year      = {2014},
  volume    = {9},
  number    = {2},
  pages     = {1-5},
  month     = {02},
  abstract  = {Mutual information (MI) is a powerful method for detecting relationships between data sets. There are accurate methods for estimating MI that avoid problems with binning when both data sets are discrete or when both data sets are continuous. We present an accurate, non-binning MI estimator for the case of one discrete data set and one continuous data set. This case applies when measuring, for example, the relationship between base sequence and gene expression level, or the effect of a cancer drug on patient survival time. We also show how our method can be adapted to calculate the Jensen-Shannon divergence of two or more data sets.},
  comment   = {How to do mutual info for mix of discrete and continuous varibles.  Says Kraskov (Kraskov04EstMutInfKNN) only works when BOTH variables are continous since NN of a discrete variable not defined.

So, Kraskov and probably PMI I used wasn't good for nnDR binary threshold prediction.
Note that JIDT toolbox has discrete MI and PMI functions: Lizier14JIDTinfoToolkit},
  doi       = {10.1371/journal.pone.0087357},
  file      = {Ross14MutInfoCntnAndDscrt.pdf:Ross14MutInfoCntnAndDscrt.pdf:PDF},
  publisher = {Public Library of Science},
  url       = {http://dx.doi.org/10.1371%2Fjournal.pone.0087357},
}

@InProceedings{Doquire12compMutInfoFeatSel,
  author    = {Doquire, Gauthier and Verleysen, Michel and others},
  title     = {A Comparison of Multivariate Mutual Information Estimators for Feature Selection.},
  booktitle = {ICPRAM (1)},
  year      = {2012},
  pages     = {176--185},
  abstract  = {Mutual Information estimation is an important task for many data mining and machine learning applications.
In particular, many feature selection algorithms make use of the mutual information criterion and could thus
benefit greatly from a reliable way to estimate this criterion. More precisely, the multivariate mutual informa-
tion (computed between multivariate random variables) can naturally be combined with very popular search
procedure such as the greedy forward to build a subset of the most relevant features. Estimating the mutual
information (especially through density functions estimations) between high-dimensional variables is however
a hard task in practice, due to the limited number of available data points for real-world problems. This paper
compares different popular mutual information estimators and shows how a nearest neighbors-based estimator
largely outperforms its competitors when used with high-dimensional data.

Keywords: Mutual information estimation, Feature selection, Nearest neighbors.},
  comment   = {Compares  MI estimators for featsel, has some tips about KSG estimtaor, which it finds to be best

* MI estimators:
 - histogram,
  - kernel density,
   - b-spline fit of histo
 - adaptive partitioning of bins (adaptive histogram)
  - Kraskov (KSG nearest neighbor)

* says Kraskov KSG clearly the best

KSG tips
*  Recommends setting bogs neg. MI values from KSG estimator to zero.

Has two ways of selecting num nearest neighbhors, k.
1.) average MI over several values of k
2.) somehow choose k using permutation test},
  file      = {Vlachos10nonUnifStSpcMI.pdf:Vlachos10nonUnifStSpcMI.pdf:PDF;Doquire12compMutInfoFeatSel.pdf:Doquire12compMutInfoFeatSel.pdf:PDF},
}

@InProceedings{Keller15EstMutualInfStrm,
  author    = {Keller, Fabian and M\"{u}ller, Emmanuel and B\"{o}hm, Klemens},
  title     = {Estimating Mutual Information on Data Streams},
  booktitle = {Proceedings of the 27th International Conference on Scientific and Statistical Database Management},
  year      = {2015},
  series    = {SSDBM '15},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Mutual information is a well-established and broadly used concept in information theory. It allows to quantify the mutual dependence between two variables -- an essential task in data analysis. For static data, a broad range of techniques addresses the problem of estimating mutual information. However, the assumption of static data is not applicable for today's dynamic data sources such as data streams: In contrast to static approaches, an online estimator must be able to deal with the evolving, changing, and infinite nature of the stream. Furthermore, some tasks require the estimation to be available online while processing the raw data stream. Our proposed solution Mise (Mutual Information Stream Estimation) allows a user to issue mutual information queries in arbitrary time windows. As a key feature, we introduce a novel sampling scheme, which ensures an equal treatment of queries over multiple time scales, e.g., ranging from milliseconds up to decades. We thoroughly analyze the requirements of such a multiscale sampling scheme, and evaluate the resulting quality of Mise in a broad range of experiments.},
  acmid     = {2791348},
  articleno = {3},
  comment   = {Mutual information estimate on lag windows of arbitrary length, where length increases w/ lag, as you'd expect, and as related to some kind of sampling theory they develop.  Good for lagged basis/feature selection, regime learning, streamwise regression, etc.  Some kind of querying idea avoids storing all data (didn't understand this part).


Streamwise featsel:  Zhou06streamFeatSel},
  doi       = {10.1145/2791347.2791348},
  file      = {Keller15EstMutualInfStrm.pdf:Keller15EstMutualInfStrm.pdf:PDF},
  isbn      = {978-1-4503-3709-0},
  location  = {La Jolla, California},
  numpages  = {12},
  url       = {http://doi.acm.org/10.1145/2791347.2791348},
}

@InProceedings{Simon06lagSelRgrsnMutInf,
  author    = {Geoffroy Simon and Michel Verleysen},
  title     = {Lag Selection for Regression Models Using High-Dimensional Mutual Information},
  booktitle = {European Symposium on Artificial Neural Networks (ESANN)},
  year      = {2006},
  pages     = {395--400},
  abstract  = {Mutual information may be used to select the embedding lag of a time series. However, this lag selection is usually limited to the analysis of the mutual information between a pair of lagged values in the series. In this paper, generalized mutual information estimators are proposed to take into account more than two variables in the lag selection. Experimental results show that lag selection using mutual information should also take into account the output of the regression model.},
  comment   = {Pick lags w/ multi-dim mutual information of the output with inputs at candidate lags; no input redundancy penalty This paper is really preliminary, with only suggestive casual inspection. Conclusion is suggested by visual inspection but isn't backed up by experimental results. Only 3 cites in 2011 so maybe it wasn't too great. * focus is picking lags for chaotic system modeling -- some laser data and a couple synthetic data -- for some reason, they pick one lag and then consider only multples of that lag -- picking that base lag while considering the impact of the multiples is the subject of this paper * conclusion: minimizing MI of lags w/ each other not as good as maximizing MI of lags w/ output to be predicted -- purely visual analysis: no results demonstrated -- no redundance penalty like Peng's mRmR -- the time series is univariate so the total dimension of the lags isn't so big * use Kraskov mutual information estimator},
  file      = {Simon06lagSelRgrsnMutInf.pdf:Simon06lagSelRgrsnMutInf.pdf:PDF;Simon06lagSelRgrsnMutInf.pdf:Simon06lagSelRgrsnMutInf.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.03.10},
  url       = {http://www.dice.ucl.ac.be/~verleyse/papers/esann06gs.pdf},
}

@Article{Karpathy14featLrnEscpdsBlog,
  author    = {Andrej Karpathy},
  title     = {Feature Learning Escapades},
  journal   = {Andrej Karpathy blog},
  year      = {2014},
  month     = jul,
  abstract  = {My summer internship work at Google has turned into a CVPR 2014 Oral titled ?Large-scale Video Classification with Convolutional Neural Networks? (project page). Politically correct, professional, and carefully crafted scientific exposition in the paper and during my oral presentation at CVPR last week is one thing, but I thought this blog might be a nice medium to also give a more informal and personal account of the story behind the paper and how it fits into a larger context.},
  comment   = {Informative personal history of attempts to learn image features: unupervised & supervized.  In 2014, he decided that the way to go (and the way taken by many others) is supervized feature learning on related tasks followed by transfer learning (Did he mean with output hinting? Several outputs for classification or whatever, each for a different task?)

* The supervised objective could be some other task
* Transfer Learning could be super simple
 -- snip off the final classification layer
 -- keep the previous layers as a fixed feature generator
 -- Add and train a new classifer
 -- Matlab's deep conv. NN example does exactly this, and with the starter net Karpathy mentions:
     http://tinyurl.com/zutp2a5

Wind Power Forecast Error Regime Learing IDEA
* train a giant convolutional network to predict NWP wind speed in every NWP grid
  -- error could be
      -- against weather model grid analysis wind speed
      -- or powercurved NWP wind speed against measured reference farm wind power
      -- or could be two step
          1.) predict analysis grid
           2.) take this and predict NWP @ reference farms
* use these results to predict power within a region in a supervised way},
  file      = {Karpathy14featLrnEscpdsBlog.pdf:Karpathy14featLrnEscpdsBlog.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.01.25},
  url       = {http://karpathy.github.io/2014/07/03/feature-learning-escapades/},
}

@TechReport{Gagnon17netMtrAdoptPV,
  author      = {Pieter Gagnon and Ben Sigrin and Mike Gleason},
  title       = {The Impacts of Changes to Nevada’s Net Metering Policy on the Financial Performance and Adoption of Distributed Photovoltaics},
  institution = {NREL},
  year        = {2017},
  number      = {NREL/TP-6A20-66765},
  month       = jan,
  abstract    = {Net energy metering (NEM) is a billing mechanism that has historically compensated owners of 
distributed generation systems at retail rates for any electricity that they export back to the grid 
rather than consume on-site. NEM can significantly enhance the financial performance of 
distributed generation systems from the owner’s perspective. For example, a large residential 
rooftop photovoltaic (PV) system could have a payback period that is 4–8 years shorter if excess 
generation were credited at the full-retail rate instead of a wholesale electricity rate (Barbose et 
al. 2016). As of 2016, NEM policy is widespread in the United States—41 states and the District 
of Columbia have passed legislation mandating full-retail net metering (North Carolina Clean 
Energy Technology Center 2016). 
NEM policies have contributed to the rapid growth of the distributed photovoltaic (DPV) 
industry in the United States. However, as DPV has reached higher levels of market penetration, 
a national conversation has begun about the efficacy of full-retail NEM as a means of achieving 
the diverse objectives of electricity pricing (Hledik et al. 2016; Barbose et al. 2016). Utilities 
have raised concerns about cost shifting to non-solar customers and utility shareholder 
profitability. Solar advocates have countered by pointing to a portfolio of studies showing that, at 
low penetrations and under certain sets of assumptions, the marginal value of solar can be 
calculated as exceeding the retail rate in some areas (Muro and Saha 2016). Still others describe 
NEM as a blunt instrument that sends inaccurate price signals to the demand side of the market 
(King 2015; Kann 2016). Altogether, this conversation has commanded increasing regulatory 
attention; in the first quarter of 2016 alone, regulators and legislatures revisited NEM policies in 
22 states (North Carolina Clean Energy Technology Center 2016). 
The following analysis was designed to illustrate the potential impact of NEM policy and tariff 
changes on the financial performance and adoption of rooftop PV, where a tariff change 
implemented in early 2016 in Nevada is taken as a case study. Although illustrative of the 
potential impact that changes to NEM policy could have on the DPV sector, this analysis 
provides only a partial picture of the context in which these changes are usually made. Other 
considerations, such as the impacts of NEM policy on the utilities themselves, other electric 
customers, total renewable energy deployment, and energy sector employment, are not addressed 
in this study. Nonetheless, the relatively narrow focus of the study allows a more thorough 
exploration of the interaction of distributed PV and retail electricity tariffs. This methodology 
provides one component within a broader analytical framework that can be used to inform 
ongoing and future decisions about NEM policy and the design of tariffs in other states. },
  comment     = {NREL report possibly related to the PV and DER adoption study that Clean Power Research might want me to work on.

See also: Dong17resPVdeployFrcst
Rogers83diffusInnovBk (cited as reference to diffusion of innovation models, but mabye not the Bass version that maybe was used here)},
  file        = {:Gagnon17netMtrAdoptPV.pdf:PDF},
  url         = {https://www.nrel.gov/analysis/dgen/publications.html},
}

@PhdThesis{Dereszynski12sensNtwkMissDat,
  author      = {Dereszynski, Ethan W},
  title       = {Probabilistic models for quality control in environmental sensor networks},
  year        = {2012},
  month       = jun,
  abstract    = {Networks of distributed, remote sensors are providing ecological scientists with a view of our environment that is unprecedented in detail. However, these networks are subject to harsh conditions, which lead to malfunctions in individual sensors and failures in network communications. This behavior manifests as corrupt or missing measurements in the data. Consequently, before the data can be used in ecological models, future experiments, or even policy decisions, it must be quality controlled (QC'd) to flag affected measurements and impute corrected values. This dissertation describes a probabilistic modeling approach for real-time automated QC that exploits the spatial and temporal correlations in the data to distinguish sensor failures from valid observations. The model adapts to a site by learning a Bayesian network structure that captures spatial relationships among sensors, and then extends this structure to a dynamic Bayesian network to incorporate temporal correlations. The final QC model contains both discrete and continuous variables, which makes inference intractable for large sensor networks. Consequently, we examine the performance of three approximate methods for inference in this probabilistic framework. Two of these algorithms represent contemporary approaches to inference in hybrid models, while the third is a greedy search-based method of our own design. We demonstrate the results of these algorithms on synthetic datasets and real environmental sensor data gathered from an ecological sensor network located in western Oregon. Our results suggest that we can improve performance over networks with less sensors that use exhaustive asynchronic inference by including additional sensors and applying approximate algorithms.},
  comment     = {{Bayes}ian network learns spatio-temporal dependencies across a large sensor net. Detects and fixes sensor errors.

Cites Barber10arHMMwindFrcst (Joe Bockhorst)

Advisor: Dietterich, Thomas G. ?? From GE?},
  file        = {Dereszynski12sensNtwkMissDat.pdf:Dereszynski12sensNtwkMissDat.pdf:PDF},
  institution = {Oregon State University},
  owner       = {sotterson},
  timestamp   = {2013.10.16},
  url         = {http://hdl.handle.net/1957/30896},
}

@InBook{Shinomoto10EstFireRatePoisson,
  pages     = {21--35},
  title     = {Estimating the Firing Rate},
  publisher = {Springer US},
  year      = {2010},
  author    = {Shinomoto, Shigeru},
  editor    = {Gr{\"u}n, Sonja and Rotter, Stefan},
  address   = {Boston, MA},
  isbn      = {978-1-4419-5675-0},
  abstract  = {Neuronal activity is measured by the number of stereotyped action po-
tentials, called spikes, elicited in response to a stimulus or the behavioral conditions
of an animal. Any nonparametric method for grasping the time-varying rate of spike
firing contains a single parameter that controls the jaggedness of the estimated rate,
such as the binsize of the time histogram or the bandwidth of the kernel smoother.
In most neurophysiological studies, the parameter that determines the interpretation
of neuronal activity has been selected subjectively by individual researchers. Re-
cently, theories for objectively selecting the parameter have been developed. This
chapter introduces the standard rate estimation tools, such as the peri-stimulus time
histogram (PSTH), kernel density estimation, or Bayes estimation, and shows ways
of selecting their parameters under the principles of minimizing the mean integrated
squared error or maximizing the likelihood. We also sum up the methods in handy
recipes that may be useful in practical data analysis.},
  booktitle = {Analysis of Parallel Spike Trains},
  comment   = {Estimating the continuous firing rate of a poisson distributed thing seems to be a matter of binning, estimating a piecewise constant rate, and then smoothing with e.g. KDE.  There are ML likelihood ways of doing this.},
  doi       = {10.1007/978-1-4419-5675-0_2},
  file      = {Shinomoto10EstFireRatePoisson.pdf:Shinomoto10EstFireRatePoisson.pdf:PDF},
  url       = {http://dx.doi.org/10.1007/978-1-4419-5675-0_2},
}

@Article{Benjamin18mdrnMLvsGLMpoisson,
  author   = {Benjamin, Ari S. and Fernandes, Hugo L. and Tomlinson, Tucker and Ramkumar, Pavan and VerSteeg, Chris and Chowdhury, Raeed H. and Miller, Lee E. and Kording, Konrad P.},
  title    = {Modern Machine Learning as a Benchmark for Fitting Neural Responses},
  journal  = {Frontiers in Computational Neuroscience},
  year     = {2018},
  volume   = {12},
  pages    = {56},
  issn     = {1662-5188},
  abstract = {Neuroscience has long focused on finding encoding models that effectively ask “what predicts neural spiking?” and generalized linear models (GLMs) are a typical approach. It is often unknown how much of explainable neural activity is captured, or missed, when fitting a model. Here we compared the predictive performance of simple models to three leading machine learning methods: feedforward neural networks, gradient boosted trees (using XGBoost), and stacked ensembles that combine the predictions of several methods. We predicted spike counts in macaque motor (M1) and somatosensory (S1) cortices from standard representations of reaching kinematics, and in rat hippocampal cells from open field location and orientation. Of these methods, XGBoost and the ensemble consistently produced more accurate spike rate predictions and were less sensitive to the preprocessing of features. These methods can thus be applied quickly to detect if feature sets relate to neural activity in a manner not captured by simpler methods. Encoding models built with a machine learning approach accurately predict spike rates and can offer meaningful benchmarks for simpler models.},
  comment  = {Newer ML techniques are better at spike prediction than poisson GLM.  Has Python.  Could be a quick way to compare models for MN SP.

One question: how make modern ML techniques predict a "probability of count" distribution like GLM poisson does.  I think the NN can do this but what about the others?

Code is here: https://github.com/KordingLab/spykesML},
  doi      = {10.3389/fncom.2018.00056},
  file     = {:Benjamin18mdrnMLvsGLMpoisson.pdf:PDF},
  url      = {https://www.frontiersin.org/article/10.3389/fncom.2018.00056},
}

@Article{doi:10.1080/03610918.2015.1122048,
  author    = {Hrishikesh D. Vinod},
  title     = {Generalized correlation and kernel causality with applications in development economics},
  journal   = {Communications in Statistics - Simulation and Computation},
  year      = {2017},
  volume    = {46},
  number    = {6},
  pages     = {4513-4534},
  abstract  = {New generalized correlation measures of 2012, GMC(Y|X), use Kernel regressions to overcome the linearity of Pearson's correlation coefficients. A new matrix of generalized correlation coefficients is such that when |r*ij| > |r*ji|, it is more likely that the column variable Xj is what Granger called the “instantaneous cause” or what we call “kernel cause” of the row variable Xi. New partial correlations ameliorate confounding. Various examples and simulations support robustness of new causality. We include bootstrap inference, robustness checks based on the dependence between regressor and error, and on the out-of-sample forecasts. Data for 198 countries on nine development variables support growth policy over redistribution and Deaton's criticism of foreign aid. Potential applications include Big Data, since our R code is available in the online supplementary material. },
  comment   = {A newish method of measuring causality and factoring out confounders.  Was used to determine cause of insect population drops here:

https://www.washingtonpost.com/science/2018/10/15/hyperalarming-study-shows-massive-insect-loss/?utm_term=.4a8347112169},
  doi       = {10.1080/03610918.2015.1122048},
  eprint    = {https://doi.org/10.1080/03610918.2015.1122048},
  publisher = {Taylor \& Francis},
  url       = { 
        https://doi.org/10.1080/03610918.2015.1122048
    
},
}

@Article{Hardesty15windFarmSiteCopula,
  author    = {Larry Hardesty},
  title     = {Siting wind farms more quickly, cheaply},
  journal   = {MIT News Office},
  year      = {2015},
  month     = jul,
  abstract  = {New model predicts wind speeds more accurately with three months of data than others do with 12.},
  comment   = {MIT statistical technique used to predict wind speeds at a wind farm site using only 3 months of data (most algs, like MPC need 8-12 mos.).  The paper, presented at IJCAI, seems to have been written by Kalyan Veeramachaneni.

They used weather station data from 15 or more other sites (not especially high dimensional).

The article links to an MIT article about Copula Graphical Model but it's not clear if that was the method above, or the improved method, mear this article's end, which would double the accuracy.  The article is Veeramachaneni15cplaGrphMdlsWindRsrcEst

Seems clear that this could also be used for wind power forecasting when there's a new farm.},
  file      = {Hardesty15windFarmSiteCopula.pdf:Hardesty15windFarmSiteCopula.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.27},
  url       = {http://news.mit.edu/2015/siting-wind-farms-quickly-cheaply-0717},
}

@Article{Corradi76distLagSpline,
  author    = {Corradi, C and Gambetta, G},
  title     = {The Estimation of Distributed Lags by Spline Functions},
  journal   = {Empirical Economics},
  year      = {1976},
  volume    = {1},
  number    = {1},
  pages     = {41--51},
  abstract  = {No abstract is available for this item.},
  comment   = {Possibly the 1\textsuperscript{st} paper doing distributed lag regression w/ splines, which estimates smoothed lag coeffients in the backwards time direction. This has advantages over polynomial distributed lag regression. Some (inconsistent) advice on polynomial order vs. num. knots. Paper is not very readable.

Given a regression target and a (scalar) input, can built up a matrix equation that can be solved by ordinary least squares (p. 44) but I couldn't find definitions for all the matrices in it.

Advantages of splines over polynomials
1. can increase the num. of model parameters w/o causing irregular lag profile shapes
2. spline smoothness independent of num. of knots
3. polynomicals are hard to control above order 3, but this may too simple to express needed lag shape
 (but wouldn't this apply to the polynomial in spline bases too? Later it says increasing poly order can be better than more knots
4. splines can have local behavior; for polynomials a tiny region defined behavior everywhere (called analiticity)
5. estimated poly coeffs are invariant to where interpolating points are located; this is knot true for spline knots, which can adust shapes more locally.
6. polynomials can't model a zero coeff (reliably?)

Knot/order picking advice
* spline order 1 or 3 are usually good enough
* in general, picking good enough knot locations requires little effort
* increasing order of spline, rather than num. of points can improve results
 - this reduces the degrees of freedom
 - seem to contradict other advice?

can be too wiggly w/o penalty (Zanobetti00addDistLagMort) may need to enforce more constraints (Welty09bayesDistLagSplPol) I think and R package does them (and more, dlmn?)},
  file      = {Corradi76distLagSpline.pdf:Corradi76distLagSpline.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  timestamp = {2010.08.04},
  url       = {http://ideas.repec.org/a/spr/empeco/v1y1976i1p41-51.html},
}

@InProceedings{Presser16ImprvNonHomogProbFrcst,
  author     = {Presser, Manuel and Messner, Jakob W. and Mayr, Georg J. and Zeileis, Achim},
  title      = {Improving non-homogeneous regression for probabilistic precipitation forecasts},
  volume     = {18},
  pages      = {14459},
  abstract   = {Non-homogenous regression is a state-of-the-art ensemble post-processing technique that statistically corrects ensemble forecasts and predicts a full probability distribution. Originally, a Gaussian model is employed that linearly links the predicted distribution mean and variance to the ensemble mean and variance, respectively. Regarding non-normally distributed precipitation data, this model can be censored at zero to account for periods without precipitation. We improve this regression approach in several directions. First, we consider link functions in the variance sub-model that assure positivity of the model variance. Second, we consider a censored Logistic (instead of censored Gaussian)
distribution to accommodate more frequent events with high
precipitation. Third, we introduce a splitting procedure, which
appropriately accounts for perfect prediction cases, i.e., where no precipitation is observed when all ensemble members predict no
precipitation. This study is applied to different accumulation periods (3, 6, 12, 24 hours) for short-range precipitation forecasts in Northern Italy. The choice of link function for the variance parameter, the splitting procedure, and an appropriate distribution assumption for precipitation data significantly improve the probabilistic forecast skill, especially for shorter accumulation periods. {KEYWORDS}:
heteroscedastic ensemble post-processing, censored distribution, maximum likelihood estimation, probabilistic precipitation forecasting},
  comment    = {non homogenous regression, ensembles.},
  date       = {2016-04-01},
  eventtitle = {{EGU} General Assembly Conference Abstracts},
  file       = {Presser16ImprvNonHomogProbFrcst.pdf:Presser16ImprvNonHomogProbFrcst.pdf:PDF},
  owner      = {sotterson},
  timestamp  = {2016.11.10},
  url        = {http://adsabs.harvard.edu/abs/2016EGUGA..1814459P},
  urldate    = {2016-11-10},
}

@InCollection{Erdogmus04gaussianizationICA,
  author      = {Erdogmus, Deniz and Rao, Yadunandana N. and Pr\'{i}ncipe, Jos\'{e} Carlos},
  title       = {{Gauss}ianizing Transformations for {ICA}},
  booktitle   = {Independent Component Analysis and Blind Signal Separation},
  publisher   = {Springer Berlin / Heidelberg},
  year        = {2004},
  editor      = {Puntonet, Carlos G. and Prieto, Alberto},
  volume      = {3195},
  series      = {Lecture Notes in Computer Science},
  pages       = {26--32},
  abstract    = {Nonlinear principal components analysis is shown to generate some of the most common criteria for solving the linear independent components analysis problem. These include minimum kurtosis, maximum likelihood and the contrast score functions. In this paper, a topology that can separate the independent sources from a linear mixture by specifically utilizing a Gaussianizing nonlinearity is demonstrated. The link between the proposed topology and nonlinear principal components is established. Possible extensions to nonlinear mixtures and several implementation issues are also discussed.},
  affiliation = {CNEL, Electrical and Computer Engineering Department, University of Florida, Gainesville, Florida 32611, USA, http://www.cnel.ufl.edu ??},
  comment     = {spinning reserves. shows how to invert a gaussianization transform? Erdogmus06gaussianiz is the update},
  doi         = {10.1007/978-3-540-30110-3_4},
  file        = {Erdogmus04gaussianizationICA.pdf:Erdogmus04gaussianizationICA.pdf:PDF},
  owner       = {scot},
  timestamp   = {2010.12.07},
}

@Article{Altman92kernKNNrgrssn,
  author    = {Altman, Naomi S},
  title     = {An introduction to kernel and nearest-neighbor nonparametric regression},
  journal   = {The American Statistician},
  year      = {1992},
  volume    = {46},
  number    = {3},
  pages     = {175--185},
  abstract  = {Nonparametric regression is a set of techniques for estimating a regression curve
without making strong assumptions about the shape of the true regression function. These
techniques are therefore useful for building and checking parametric models, as well as for
data description. Kernel and nearest neighbor regression estimators are local versions
of univariate location estimators, and so they can readily be introduced to beginning
students, and consulting clients who are familiar with such summaries as the sample
mean and median.

Key Words: Confidence intervals; Local linear regression; Model building; Model check-
ing; Smoothing.},
  comment   = {
Newer (?) version here: Jolliffe12frcstVerif},
  file      = {:Altman92kernKNNrgrssn.pdf:PDF},
  publisher = {Taylor \& Francis},
  url       = {https://www.tandfonline.com/doi/abs/10.1080/00031305.1992.10475879},
}

@Article{Pimentel14revNovDet,
  author    = {Pimentel, Marco AF and Clifton, David A and Clifton, Lei and Tarassenko, Lionel},
  title     = {A review of novelty detection},
  journal   = {Signal Processing},
  year      = {2014},
  volume    = {99},
  pages     = {215--249},
  abstract  = {Novelty detection is the task of classifying test data that differ in some respect from the data that are available during training. This may be seen as ?one-class classification?, in which a model is constructed to describe ?normal? training data. The novelty detection approach is typically used when the quantity of available ?abnormal? data is insufficient to construct explicit models for non-normal classes. Application includes inference in datasets from critical systems, where the quantity of available normal data is very large, such that ?normality? may be accurately modelled. In this review we aim to provide an updated and structured investigation of novelty detection research papers that have appeared in the machine learning literature during the last decade.

Keywords
Novelty detection; One-class classification; Machine learning},
  comment   = {Big review of novelty detection, discusses graphical models for novelty detection.

CLASSIFICATION OF APPROACHES

Novelty, anomaly, outlier, and change-point detection are either synonymous or "closely related" (change-point)

He prefers this classification:
  o probabilistic: usually density estimation of /normal/" class
  o distance-based: dist based clustering, /normal/ data is tightly clustered
  o reconstruction based: big regression model error ==> /novel/
  o domain based: define a boundary around /normal/
  o information theoretic: est. entropy, etc.: /novel/ data increases info content

},
  doi       = {http://dx.doi.org/10.1016/j.sigpro.2013.12.026},
  file      = {Pimentel14revNovDet.pdf:Pimentel14revNovDet.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2016.12.19},
  url       = {http://www.sciencedirect.com/science/article/pii/S016516841300515X},
}

@InBook{Abedi13impStochModRen,
  chapter   = {Chapter 5. Improved Stochastic Modeling: An Essential Tool for Power System Scheduling in the Presence of Uncertain Renewables},
  title     = {New Developments in Renewable Energy},
  publisher = {InTech},
  year      = {2013},
  author    = {Abedi, Sajjad and Riahy, G.H and Hosseinian, S.H and Farhadkhani, M.},
  editor    = {Hasan Arman and Ibrahim Yuksel},
  number    = {ISBN 978-953-51-1040-8},
  abstract  = {Nowadays, governments are developing ambitious goals toward the future green and sustainable sources of energy. In the U.S., the penetration level of wind energy is expected to be 20 pct by the year 2030 [1]. Several European countries already exhibit the adoption level in the range of 5 pct?20 pct of the entire annual demand. Also with further developments in the solar cells technology and lower manufacturing costs, the outlook is that the photovoltaic (PV) power will possess a larger share of electric power generation in the near future. Grid-connected PV is ranked as the fastest-growing power generation technology [2]. PV generates pollution-free and very cost-effective power which relies on a free and abundant source of energy.

Due to the increasing wind and solar penetrations in power systems, the impact of system variability has been receiving increasing research focus from market participants, regulators, system operators and planners with the aim to improve the controllability and predictability of the available power from the uncertain resources. The produced power from these resources is often treated as non-dispatchable and takes the highest priority of meeting demand, leaving conventional units to meet the remaining or net demand. This issue makes the optimum scheduling of power plants in power system cumbersome as embeds the stochastic parameters into the problem to be handled. The unpredictability along with potential sudden changes in the net demand, may face operators with technical challenges such as ramp up and down adaptation and reserve requirement problems [3-4].

Several investigations aiming at handling the uncertain nature of wind and solar energy resources have been reported. Basically, the methods found in the literature can be classified into three groups: methods that deal with the prediction of uncertain variables as an input data pre-processing, methods that use stochastic scenario-based approach within the optimization procedure to cover all the outcomes per the probable range of uncertain variables, and methods based on a combination of these two approaches. The studies presented in [5-7] can be mentioned as one of the most recent efforts lying in the first group. In [5-6] an Artificial Neural Network (ANN) forecast technique is employed and followed by risk analysis based on the error in the forecast data. Then, the so called pre-processed data is directly taken as the input to the optimization process. Relying on the forecast tools, such methods suffer from high inaccuracy or ex-ante underestimation of the available power which increases the scheduled generation and reserve costs. Anyway, this approach is useful as it accounts for the temporal correlation between the random variables representative of each time step of the scheduling period, in terms of time-series models. On the other hand, in [8-9] which belong to the second group, the focus is on the stochastic scenario analysis rather than the forecasting methods. The usage of this approach also has its own advantages, as it tries to model the likely range of values for the random variables. However, the efficiency of this approach largely depends on the accuracy and reliability of their probabilistic analysis; based on which the potential scenarios are built.

The most effective approach is associated with the third group, which applies the advantages of both forecast techniques and scenario-based optimization approach. Reference [10] presents a computational framework for integrating a numerical weather prediction (NWP) model in stochastic unit commitment/economic dispatch formulations that describes the wind power uncertainty. In [11], the importance of stochastic optimization tools from the viewpoint of the profit maximization of power generation companies is investigated. The exposed financial losses regarding the wind speed forecast errors are discussed. A stochastic model is also presented in [12]which uses a heuristic optimization method for the reduction of random wind power scenarios. The wind speed data is assumed to follow the normal PDF. A similar approach is introduced in [13] whereas the wind speed error distribution is considered as a constant percentage of the forecasted data. In [14], the Auto-Regressive Moving Average (ARMA) time series model was chosen to estimate the wind speed volatility. Based on the model, the temporal correlation of wind speed at a time step with respect to the prior time steps is well analyzed.

In this chapter, the authors present a framework for stochastic modeling of random processes including wind speed and solar irradiation which are involved in the power generation scheduling optimization problems. Based on a thorough statistical analysis of the accessible historical observations of the random variables, a set of scenarios representing the available level of wind and solar power for each time step of scheduling are estimated. To this aim, the Kernel Density Estimation (KDE) method is proposed to improve the accuracy in modeling the Probability Distribution Function (PDF) of wind and solar random variables. In addition, the concept of aggregation of multi-area wind/solar farms is analyzed using Copula method. Taking the advantage of this method, we can reflect the interdependency and spatial correlation of the power generated by several wind farms or PV farms that are spread over different locations in the power system. A final framework is developed to perform the stochastic analysis of the random variables to be input into the stochastic optimization process, as discussed in the following sections.},
  booktitle = {New Developments in Renewable Energy},
  comment   = {Not a great paper. Basic prob forecasting, but covers several methods, likes NN. Has a bit about scenario reduction and risk. Is also joint wind-wind and pv-stats (as far as I can tell).},
  doi       = {10.5772/52161},
  file      = {Abedi13impStochModRen.pdf:Abedi13impStochModRen.pdf:PDF},
  groups    = {Read, Ensemble, PointDerived, Use, CitaviImport1, doReadWPV_1, PV},
  ncite     = {0},
  owner     = {sotterson},
  timestamp = {2013.09.26},
  url       = {http://www.intechopen.com/books/new-developments-in-renewable-energy/improved-stochastic-modeling-an-essential-tool-for-power-system-scheduling-in-the-presence-of-uncert},
}

@Article{Leutbecher08ensFrcst,
  author    = {Leutbecher, M. and Palmer, T.N},
  title     = {Ensemble forecasting},
  journal   = {Journal of Computational Physics},
  year      = {2008},
  volume    = {227},
  number    = {7},
  pages     = {3515--3539},
  month     = feb,
  issn      = {0021-9991},
  abstract  = {Numerical weather prediction models as well as the atmosphere itself can be viewed as nonlinear dynamical systems in
which the evolution depends sensitively on the initial conditions. The fact that estimates of the current state are inaccurate
and that numerical models have inadequacies, leads to forecast errors that grow with increasing forecast lead time. The
growth of errors depends on the flow itself. Ensemble forecasting aims at quantifying this flow-dependent forecast
uncertainty.
The sources of uncertainty in weather forecasting are discussed. Then, an overview is given on evaluating probabilistic
forecasts and their usefulness compared with single forecasts. Thereafter, the representation of uncertainties in ensemble
forecasts is reviewed with an emphasis on the initial condition perturbations. The review is complemented by a detailed
description of the methodology to generate initial condition perturbations of the Ensemble Prediction System (EPS) of
the European Centre for Medium-Range Weather Forecasts (ECMWF). These perturbations are based on the leading part
of the singular value decomposition of the operator describing the linearised dynamics over a finite time interval. The perturbations
are flow-dependent as the linearisation is performed with respect to a solution of the nonlinear forecast model.
The extent to which the current ECMWF ensemble prediction system is capable of predicting flow-dependent variations
in uncertainty is assessed for the large-scale flow in mid-latitudes.
 2007 Elsevier Inc. All rights reserved.
Keywords: Uncertainty; Numerical weather prediction; Predictability},
  booktitle = {Journal of Computational Physics},
  comment   = {How to generate ensemble initial conditions. Linearized, flow-dependent, SVD and SVD based.

Good background, maybe for the 1000K ensembles project, but is probably too detailed for Eweline.},
  file      = {Leutbecher08ensFrcst.pdf:Leutbecher08ensFrcst.pdf:PDF},
  groups    = {Ensemble, CitaviImport1, doReadNonWPV_2},
  ncite     = {142},
  owner     = {sotterson},
  timestamp = {2013.09.26},
  url       = {http://www.elsevier.com/locate/jcp},
}

@Book{Patrikalakis09ShapInterCADspline,
  title     = {Shape interrogation for computer aided design and manufacturing},
  publisher = {Springer Science \& Business},
  year      = {2001},
  author    = {Patrikalakis, Nicholas M and Maekawa, Takashi},
  abstract  = {Objectives and Features
Shape interrogation is the process of extraction of information from a geometric
model. Shape interrogation is a fundamental component of Computer
Aided Design and Manufacturing (CAD/CAM) systems and was first used in
such context by M. Sabin, one of the pioneers of CAD/CAM, in the late sixties.
The term surface interrogation has been used by I. Braid and A. Geisow
in the same context. An alternate term nearly equivalent to shape interrogation
is geometry processing first used by R. E. Barnhill, another pioneer of
this field. In this book we focus on shape interrogation of geometric models
bounded by free-form surfaces. Free-form surfaces, also called sculptured surfaces,
are widely used in scientific and engineering applications. For example,
the hydrodynamic shape of propeller blades has an important role in marine
applications, and the aerodynamic shape of turbine blades determines the
performance of aircraft engines. Free-form surfaces arise also in the bodies of
ships, automobiles and aircraft, which have both functionality and attractive
shape requirements. Many electronic devices as well as consumer products
are designed with aesthetic shapes, which involve free-form surfaces.
When engineers or stylists design geometric models bounded by free-form
surfaces, they need tools for shape interrogation to check whether the designed
object satisfies the functionality and aesthetic shape requirements.
This book provides the mathematical fundamentals as well as algorithms for
various shape interrogation methods including nonlinear polynomial solvers,
intersection problems, differential geometry of intersection curves, distance
functions, curve and surface interrogation, umbilics and lines of curvature,
geodesics, and offset curves and surfaces.
The book can serve as a textbook for teaching advanced topics of geometric
modeling for graduate students as well as professionals in industry. It has
been used as one of the textbooks for the graduate course \Computational
Geometry" at the Massachusetts Institute of Technology (MIT). Currently
there are several excellent books in the area of geometric modeling and in
the area of solid modeling. This book provides a bridge between these two
areas. Apart from the differential geometry topics covered, the entire book is
based on the unifying concept of recasting},
  comment   = {CAD book has some useful stuff on b-splines (see bookmarks)
book (or course?) web page:
http://web.mit.edu/hyperbook/Patrikalakis-Maekawa-Cho/mathe.html},
  file      = {Patrikalakis09ShapInterCADspline.pdf:Patrikalakis09ShapInterCADspline.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.10},
  url       = {http://deslab.mit.edu/DesignLab/tmaekawa/book/mathe.pdf},
}

@TechReport{Brauns14RegelWindLeist,
  author      = {Steffen Brauns and Markus Stobrawe and Malte Jansen and Werner Bohlen and Dominik Jost and Eike Erdmann and Malte Siefert and Ren{\'e} Just, and Markus Speckmann and Niklas Netzel and Martin Widdel},
  title       = {Regelenergie Durch Windkraftanlagen: Abschlussbericht},
  institution = {Fraunhofer IWES},
  year        = {2014},
  month       = mar,
  abstract    = {Objectives
Due to the forecast inaccuracy and Dargebotsabh?ngigkeit the wind energy is unclear, as the quotation
and the verification at the Regelleistungsbe- provision can be done by wind turbines, because the
previous supplier of control power are not faced with these uncertainties. Therefore, it was to develop the
overarching goal of the project, a quotation and a detection method for the control power supply through
wind turbines and related ICT solutions.
Quotation
? Using probabilistic forecasts for the wind supply wind can park pools control power with the same
reliability as current provider to offer.
? In the project different offer strategies for a day before auction and a sub-tags auction with a lead time of
one hour have been developed and investigated the associated potential.
? show potential studies that a single wind farm can offer virtually no control power at a day before the
auction. In the investigated wind park pool with a size of 1 GW and 30 GW on the other hand results in
an appreciable range potential that is performance at 30 GW wind park pool by far the largest in terms of
nominal.
* In the case of a sub-tags auction result for 1 GW and 30 GW wind park pools in relation to the nominal
power on the other hand almost equal appropriateness botspotenziale. This calls for the introduction of an
energy price market with a sub-tags auction, since this would allow, in contrast to a previous day auction,
many vendors with wind farm pools participation.
* Further, the potential are increased by the pooling of controllable systems.
* A year-round supply of control power by However, wind farm pools is not possible, since there are
phases in which the wind volume is not sufficient.
detection methods
* In the project the two detection methods "schedule" and "potential supply" were defined and criteria
based compared in order to identify the best of overall system view method.
* In the process of "potential supply" the provision of spect is relative to potential supply. The possible
supply of a wind farm is the power that would have produced the wind farm, if he had not been governed|.
Summary
* In the process of "roadmap" the rule of performance is relative to a previously reported schedule, in turn
the result of trading activities in the energy market is. This method is currently used for the provision of
balancing power in Germany. In the case of wind energy de would the timetable based on a probabilistic
forecast can be created, which the wind park pool allows the feed with the indication of a certain reliability
to predict.not
* The tested under the project method for the determination of the possible supply have a sufficient
rule for the service provision accuracy. Future research activities and Ausgleichsef- effects through the
pooling of wind farms, however, have a sufficiently accurate determination of the possible feeding hope.
The detection method in the process "roadmap" has already speed over a sufficient accuracy.
* As the partner weight the criteria differently, recommend the Fraunhofer IWES, energy source and
ENERCON the process "possible infeed" and Amprion and Tennet TSO the process "roadmap". The
recommendation of the process "potential supply" presupposes, however, that this is technically also
accurate enough to implement.
ICT solutions and field test results
* In field tests it was shown that the control of wind farms with a three-second clocking possible is.
* The results show that the wind farm control in the case of minute reserve is already sufficiently accurate
to wind farm level, ignoring the inaccuracy of the determination of the possible supply. This does not
apply in the case of primary and secondary control, but can probably be achieved structurally through a
pooling of several wind parks and the optimization of regulatory|.},
  comment     = {1\textsuperscript{st} Tech Report for regulation of wind turbines

Follow-on project ReWP: IWES14totProjDescReWPR},
  file        = {English:Brauns14RegelWindLeist_trans.pdf:PDF;German:Brauns14RegelWindLeist.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2014.11.05},
}

@Article{Williams10nonNegMutInfDecomp,
  author    = {Williams, P.L. and Beer, R.D.},
  title     = {Nonnegative Decomposition of Multivariate Information},
  journal   = {Arxiv preprint arXiv:1004.2515},
  year      = {2010},
  abstract  = {Of the various attempts to generalize information theory to multiple variables, the most widely utilized, interaction information, suffers from the problem that it is sometimes negative. Here we reconsider from First principles the general structure of the information that a set of sources provides about a given variable. We begin with a new deenition of redundancy as the minimum information that any source provides about each possible outcome of the variable, averaged over all possible outcomes. We then show how this measure of redundancy induces a lattice over sets of sources that clarifies the general structure of multivariate information. Finally, we use this redundancy lattice to propose a definition of partial information atoms that exhaustively decompose the Shannon information in a multivariate system in terms of the redundancy between synergies of subsets of the sources. Unlike interaction information, the atoms of our partial information decomposition are never negative and always support a clear interpretation as informational quantities. Our analysis also demonstrates how the negativity of interaction information can be explained by its confounding of redundancy and synergy. Keywords: information theory, interaction information, redundancy, synergy, multivariate interactio},
  comment   = {Another mutual information decomposition but doesn't have negative values like McGill interaction information. I think it's 3-tuples, blown out on a lattice, which goes to infinity pretty fast. But maybe there are some useful bits in here...},
  file      = {Williams10nonNegMutInfDecomp.pdf:Williams10nonNegMutInfDecomp.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2011.12.01},
}

@Article{Jeon12condKDEwindFrcst,
  author    = {Jeon, Jooyoung and Taylor, James W.},
  title     = {Using Conditional Kernel Density Estimation for Wind Power Density Forecasting},
  journal   = {Journal of the American Statistical Association},
  year      = {2012},
  volume    = {107},
  number    = {497},
  pages     = {66--79},
  abstract  = {Of the various renewable energy resources, wind power is widely recognized as one of the most promising. The management of wind farms and electricity systems can benefit greatly from the availability of estimates of the probability distribution of wind power generation. However, most research has focused on point forecasting of wind power. In this article, we develop an approach to producing density forecasts for the wind power generated at individual wind farms. Our interest is in intraday data and prediction from 1 to 72 hours ahead. We model wind power in terms of wind speed and wind direction. In this framework, there are two key uncertainties. First, there is the inherent uncertainty in wind speed and direction, and we model this using a bivariate vector autoregressive moving average-generalized autoregressive conditional heteroscedastic (VARMA-GARCH) model, with a Student t error distribution, in the Cartesian space of wind speed and direction. Second, there is the stochastic nature of the relationship of wind power to wind speed (described by the power curve), and to wind direction. We model this using conditional kernel density (CKD) estimation, which enables a nonparametric modeling of the conditional density of wind power. Using Monte Carlo simulation of the VARMA-GARCH model and CKD estimation, density forecasts of wind speed and direction are converted to wind power density forecasts. Our work is novel in several respects: previous wind power studies have not modeled a stochastic power curve; to accommodate time evolution in the power curve, we incorporate a time decay factor within the CKD method; and the CKD method is conditional on a density, rather than a single value. The new approach is evaluated using datasets from four Greek wind farms.},
  comment   = {Adaptation of KDE via sliding windows.},
  doi       = {10.1080/01621459.2011.643745},
  eprint    = {http://www.tandfonline.com/doi/pdf/10.1080/01621459.2011.643745},
  file      = {Jeon12condKDEwindFrcst.pdf:Jeon12condKDEwindFrcst.pdf:PDF},
  groups    = {PointDerived, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2014.02.26},
}

@InProceedings{Jost16dynSizFreqResLen,
  author       = {Jost, Dominik and Braun, Axel and Fritz, Rafael and Otterson, Scott},
  title        = {Dynamic sizing of automatic and manual frequency restoration reserves for different product lengths},
  booktitle    = {European Energy Market (EEM), 2016 13th International Conference on the},
  year         = {2016},
  pages        = {1--5},
  organization = {IEEE},
  abstract     = {On last year’s EEM conference a method for the 
dynamic sizing of frequency restoration reserve capacity based 
on quantile regression was presented. Further research has 
improved the method and has made it ready for use. It contains 
the following new features, which will be presented in this 
paper: an adaptive bias correction function, the allocation of 
frequency restoration reserves (FRR) to automatic FRR (aFRR) 
and manual FRR (mFRR) and the calculation of needed reserve 
capacity for different product lengths. The results will show the 
advantages of the adaptive bias correction, estimate the needed 
reserves for aFRR and mFRR, and demonstrate the influence of 
different product lengths on the needed reserve capacities.  
Index Terms - Dynamic reserve sizing, frequency restoration 
reserve, quantile regression, secondary control, tertiary control },
  comment      = {Explains Dominik's dynamic freq resrv sizing algorithm.  Jost15dynDimFrqRsrvQR explains more of the details.},
  file         = {:Jost16dynSizFreqResLen.pdf:PDF},
}

@Article{Rippel13highDimProbDeepNN,
  author    = {Rippel, Oren and Adams, Ryan Prescott},
  title     = {High-Dimensional Probability Estimation with Deep Density Models},
  journal   = {arXiv preprint arXiv:1302.5125},
  year      = {2013},
  abstract  = {One of the fundamental problems in machine learning is the estimation of a probability distribution from data. Many techniques have been proposed to study the structure of data, most often building around the assumption that observations lie on a lower-dimensional manifold of high probability. It has been more difficult, however, to exploit this insight to build explicit, tractable density models for high-dimensional data. In this paper, we introduce the deep density model (DDM), a new approach to density estimation. We exploit insights from deep learning to construct a bijective map to a representation space, under which the transformation of the distribution of the data is approximately factorized and has identical and known marginal densities. The simplicity of the latent distribution under the model allows us to feasibly explore it, and the invertibility of the map to characterize contraction of measure across it. This enables us to compute normalized densities for out-of-sample data. This combination of tractability and flexibility allows us to tackle a variety of probabilistic tasks on high-dimensional datasets, including: rapid computation of normalized densities at test-time without evaluating a partition function; generation of samples without MCMC; and characterization of the joint entropy of the data.},
  comment   = {Another invertible NN (another one is Baird05oneStepInvrtNN)

* seems to be a deep learning thing
* could be used for distribution learning instead of logit in Koenker13DistributionalvsQuantile
* Maybe updated info here: http://arxiv.org/abs/1402.5836},
  file      = {Rippel13highDimProbDeepNN.pdf:Rippel13highDimProbDeepNN.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.07.11},
  url       = {http://arxiv.org/abs/1302.5125},
}

@Misc{Grace-Martin19hazardFunc,
  author       = {Karen Grace-Martin},
  title        = {What Is a Hazard Function in Survival Analysis?},
  howpublished = {The Analysis Factor: Making Statistics Make Sense (web page)},
  year         = {2019},
  abstract     = {One of the key concepts in Survival Analysis is the Hazard Function.

But like a lot of concepts in Survival Analysis, the concept of “hazard” is similar, but not exactly the same as, its meaning in everyday English. Since it’s so important, though, let’s take a look.},
  comment      = {A hazard function is the probability of an event occuring in some time range, more or less, the number of events in the range divided by the number of candidates in the range.  Example given is PhD graduation, where the number of candidates changes over time (some drop out, some graduate,...).  For some reason Altman09glmNonlinMdlCourseNts says a hazard function is not a probability, but here, it's clear that it is.

If the hazard function is in continuous time, you have to integrate in order to get the probability in some range.
},
  file         = {:Grace-Martin19hazardFunc.pdf:PDF},
  url          = {https://www.theanalysisfactor.com/what-is-a-hazard-function-in-survival-analysis/},
}

@Article{Bergmeir18crossValARfrcst,
  author   = {Christoph Bergmeir and Rob J. Hyndman and Bonsoo Koo},
  title    = {A note on the validity of cross-validation for evaluating autoregressive time series prediction},
  journal  = {Computational Statistics \& Data Analysis},
  year     = {2018},
  volume   = {120},
  pages    = {70 - 83},
  issn     = {0167-9473},
  abstract = {One of the most widely used standard procedures for model evaluation in classification and regression is K-fold cross-validation (CV). However, when it comes to time series forecasting, because of the inherent serial correlation and potential non-stationarity of the data, its application is not straightforward and often replaced by practitioners in favour of an out-of-sample (OOS) evaluation. It is shown that for purely autoregressive models, the use of standard K-fold CV is possible provided the models considered have uncorrelated errors. Such a setup occurs, for example, when the models nest a more appropriate model. This is very common when Machine Learning methods are used for prediction, and where CV can control for overfitting the data. Theoretical insights supporting these arguments are presented, along with a simulation study and a real-world example. It is shown empirically that K-fold CV performs favourably compared to both OOS evaluation and other time-series-specific techniques such as non-dependent cross-validation.

Keywords: cross-validation, time series, autoregression.},
  comment  = {Compares the idea k-fold cross validation with guard bands for AR model lags (like I've done in the past at Fraunhofer).  For a purely AR model (no exogenous variables, I guess).  5 fold CV error estimate is worse if you use the guard bands (purely AR model, small data set e.g. 289 points, AR lag ~ 9).  They say something about standard CV being OK when errors have no serial correlation (temporal dependence) -- say Ljung-Box test can check for this dependence.  But I haven't really read this paper, and so don't know what this all means in general real life problems.

Note that this (forecasting) professor (Hyndemann) doesn't worry about causaility of train and test data, as does Cochrane18crossValTimeSer},
  doi      = {https://doi.org/10.1016/j.csda.2017.11.003},
  keywords = {Cross-validation, Time series, Autoregression},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167947317302384},
}

@Article{Minku10ensDiversDrift,
  author    = {Minku, L.L. and White, A.P. and Xin Yao},
  title     = {The Impact of Diversity on Online Ensemble Learning in the Presence of Concept Drift},
  journal   = {Knowledge and Data Engineering, IEEE Transactions on},
  year      = {2010},
  volume    = {22},
  number    = {5},
  pages     = {730--742},
  issn      = {1041-4347},
  abstract  = {Online learning algorithms often have to operate in the presence of concept drift (i.e., the concepts to be learned can change with time). This paper presents a new categorization for concept drift, separating drifts according to different criteria into mutually exclusive and nonheterogeneous categories. Moreover, although ensembles of learning machines have been used to learn in the presence of concept drift, there has been no deep study of why they can be helpful for that and which of their features can contribute or not for that. As diversity is one of these features, we present a diversity analysis in the presence of different types of drifts. We show that, before the drift, ensembles with less diversity obtain lower test errors. On the other hand, it is a good strategy to maintain highly diverse ensembles to obtain lower test errors shortly after the drift independent on the type of drift, even though high diversity is more important for more severe drifts. Longer after the drift, high diversity becomes less important. Diversity by itself can help to reduce the initial increase in error caused by a drift, but does not provide the faster recovery from drifts in long-term.},
  comment   = {Many ensembles (or bootstrap samples, I think) are required for adaptivity, and for learning regimes under concept drift. Use for regime detection, conditional bootstrap sample selction, analog ensemble forecasting....},
  doi       = {10.1109/TKDE.2009.156},
  file      = {Minku10ensDiversDrift.pdf:Minku10ensDiversDrift.pdf:PDF},
  groups    = {Ensemble, PointDerived, doReadNonWPV_1},
  keywords  = {learning (artificial intelligence);neural nets;concept drift;diversity analysis;exclusive category;learning machines;nonheterogeneous category;online ensemble learning;Concept drift;diversity.;neural network ensembles;online learning},
  owner     = {sotterson},
  timestamp = {2013.10.02},
}

@InProceedings{NguyenTuong10incSparse,
  author    = {Duy Nguyen-Tuong and Jan Peters},
  title     = {Incremental Sparsification for Real-time Online Model Learning},
  booktitle = {Journal of Machine Learning Research},
  year      = {2010},
  volume    = {9},
  pages     = {557--564},
  abstract  = {Online model learning in real-time is required by many applications, for example, robot tracking control. It poses a difficult problem, as fast and incremental online regression with large data sets is the essential component and cannot be realized by straightforward usage of off-the-shelf machine learning methods such as Gaussian process regression or support vector regression. In this paper, we propose a framework for online, incremental sparsification with a fixed budget designed for large scale real-time model learning. The proposed approach combines a sparsification method based on an independency measure with a large scale database. In combination with an incremental learning approach such as sequential support vector regression, we obtain a regression method which is applicable in real-time online learning. It exhibits competitive learning accuracy when compared with standard regression techniques. Implementation on a real robot emphasizes the applicability of the proposed approach in real-time online model learning for real world systems.},
  comment   = {An adaptive, online "speaker ID UBM" for nonlinear regression
- learns model dynamics by maintaining a dictionary of basis vectors

- adds/deletes adaptively - dictionary can be pre-loaded w/ points from other systems (how do that?)
- is nonlinear, gaussian process based, w/ dynamics

Could be used for analog ensembles, regime detection, scenario tree reduction (maybe, somehow).},
  file      = {Nguyen_Tuong10incSparse.pdf:Nguyen_Tuong10incSparse.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.06.24},
  url       = {http://jmlr.csail.mit.edu/proceedings/papers/v9/},
}

@InProceedings{Davarifar14realtimePVdiagSPRT,
  author    = {M. Davarifar and A. Rabhi and A. Hajjaji and Z. Daneshifar},
  title     = {Real-time diagnosis of {PV} system by using the Sequential Probability Ratio Test (SPRT)},
  booktitle = {Proc. 16th Int. Power Electronics and Motion Control Conf. and Exposition},
  year      = {2014},
  pages     = {508--513},
  month     = sep,
  abstract  = {Online monitoring of PV performance is prevalent in household application and fault supervisory system is new expectation of home user investors. In this regards, this paper has proposed Online PV fault Diagnosis by using Sequential Probability Ratio Test (SPRT) for improve measured signal value and anomaly detection. To identify defect characteristic of PV system, voltage and current of PV system are measured online and compared with simulated values, then residual signal is generated. SPRT let us to detect anomaly condition in-situ, and improve making decision for residual signal. With investigating of two residual signal of current and voltage, fault diagnosis procedure is suggested to classify types of fault and eventually find probability of fault location.},
  doi       = {10.1109/EPEPEMC.2014.6980544},
  file      = {:papers\\Davarifar14realtimePVdiagSPRT.pdf:PDF},
  keywords  = {fault location, photovoltaic power systems, probability, PV system real-time diagnosis, SPRT, anomaly detection, fault location probability, measured signal value improvement, online PV fault diagnosis, sequential probability ratio test, Circuit faults, Junctions, Measurement uncertainty, Monitoring, Standards, Uncertainty, Voltage measurement, Faults diagnosis, PV System, Real time modeling},
  owner     = {sotterson},
  timestamp = {2017.07.19},
}

@Article{Adapa05dynThermRat,
  author    = {Adapa, R. and Douglass, D.A.},
  title     = {Dynamic thermal ratings: monitors and calculation methods},
  journal   = {Power Engineering Society General Meeting, IEEE},
  year      = {2005},
  pages     = {163--167},
  month     = jul,
  abstract  = {Opening of the transmission system to independent generators and reduction in traditional regulation has led many utilities, both in the United States and around the world, to employ methods where transmission lines and equipment can be operated reliably at higher loadings. Since the mid-1980's, considerable attention has been paid to increasing the power flow of overhead lines, power transformers, underground cables, and substation terminal equipment by means of monitoring weather and the equipment thermal state and by developing more accurate thermal models. The resulting dynamic thermal rating techniques have typically yielded increases of 5 to 15 pct in capacity. In this paper, dynamic thermal rating models and monitoring methods for lines, underground cables, power transformers, and substation terminal equipment are discussed and explained. It is shown that overhead line ratings are very dependent on wind speed and direction, that line temperature and sag respond very quickly to changes in line current and wind, and that, in contrast, the soil temperature and thermal resistivity of the earth, which determine the thermal rating of underground cable, change very slowly with weather and current loading. While the thermal response of underground cable and overhead line circuits is quite different., they share the requirement for careful monitoring at multiple locations along their routes. Power transformers, substation terminal equipment and underground cables are shown to be sensitive both to circuit current and air temperature. Transformer ratings are determined not only by oil and winding temperatures but also by degradation of the winding insulation. Relatively simple thermal rating algorithms are suggested for substation terminal equipment and new, simpler, field temperature calibration methods are explored. Finally., the application of dynamic rating methods to complex interfaces is also explored. Path15 in California is discussed as an example of how dynamic r- - ating methods can be combined with load reduction procedures to increase power transfer levels in complex interfaces},
  comment   = {power line temp/sag response to wind speed/dir and other power system thermal responses (but this paper contains no information!)},
  doi       = {10.1109/PESAFR.2005.1611807},
  file      = {Adapa05dynThermRat.pdf:Adapa05dynThermRat.pdf:PDF;Adapa05dynThermRat.pdf:Adapa05dynThermRat.pdf:PDF},
  groups    = {Read},
  keywords  = {load flow, meteorology, power overhead lines, power transformers, substations, thermal conductivity, underground cablesdynamic rating methods, dynamic thermal rating techniques, equipment thermal state, field temperature calibration methods, load reduction, monitors-calculation methods, overhead line ratings, power flow, power transfer, power transformers, soil temperature, substation terminal equipment, thermal resistivity, thermal response, transmission system, underground cables},
  owner     = {sotterson},
  timestamp = {2009.02.23},
}

@Article{Pinson09skillEnsWndPow,
  author    = {Pinson, Pierre and Nielsen, H Aa and Madsen, Henrik and Kariniotakis, Georges},
  title     = {Skill forecasting from ensemble predictions of wind power},
  journal   = {Applied Energy},
  year      = {2009},
  volume    = {86},
  number    = {7},
  pages     = {1326--1334},
  abstract  = {Optimal management and trading of wind generation calls for the providing of uncertainty estimates
along with the commonly provided short-term wind power point predictions. Alternative approaches
for the use of probabilistic forecasting are introduced. More precisely, focus is given to prediction risk
indices aiming to give a comprehensive signal on the expected level of forecast uncertainty. Ensemble
predictions of wind generation are used as input. A proposal for the definition of prediction risk indices
is given. Such skill forecasts are based on the spread of ensemble forecasts (i.e. a set of alternative scenarios
for the coming period) for a single prediction horizon or over a look-ahead period. It is shown
on the test case of a Danish offshore wind farm how these prediction risk indices may be related to several
levels of forecast uncertainty (and potential energy imbalances). Wind power ensemble predictions
are derived from the conversion of ECMWF and NCEP ensemble forecasts of meteorological variables to
wind power ensemble forecasts, as well as by a lagged average approach alternative. The ability of prediction
risk indices calculated from the various types of ensembles forecasts to resolve among situations
with different levels of uncertainty is discussed.},
  comment   = {Instantaneous ensemble spread not useful, but can use it over time (I think it was said in Geibel05*, which referenced a much older paper of Pierre's on the same topic as this one). That old one was on poor man's ensembles (I think Gregor said), and this one is on real ensembles (I think).},
  file      = {Pinson09skillEnsWndPow.pdf:Pinson09skillEnsWndPow.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.07.09},
  url       = {http://www.sciencedirect.com/science/article/pii/S0306261908002602},
}

@Article{Zheng14stocOptUCreview,
  author    = {Zheng, Q.P. and Wang, J. and Liu, A.L.},
  title     = {Stochastic Optimization for Unit Commitment -- A review},
  year      = {2014},
  number    = {99},
  pages     = {1--12},
  issn      = {0885-8950},
  doi       = {10.1109/TPWRS.2014.2355204},
  abstract  = {Optimization models have been widely used in the power industry to aid the decision-making process of scheduling and dispatching electric power generation resources, a process known as unit commitment (UC). Since UC's birth, there have been two major waves of revolution on UC research and real life practice. The first wave has made mixed integer programming stand out from the early solution and modeling approaches for deterministic UC, such as priority list, dynamic programming, and Lagrangian relaxation. With the high penetration of renewable energy, increasing deregulation of the electricity industry, and growing demands on system reliability, the next wave is focused on transitioning from traditional deterministic approaches to stochastic optimization for unit commitment. Since the literature has grown rapidly in the past several years, this paper is to review the works that have contributed to the modeling and computational aspects of stochastic optimization (SO) based UC. Relevant lines of future research are also discussed to help transform research advances into real-world applications.},
  journal   = {Power Systems, IEEE Transactions on},
  keywords  = {Computational modeling;Load modeling;Optimization;Probabilistic logic;Robustness;Stochastic processes;Uncertainty;Electricity market operations;mixed integer programming;pricing;risk constraints;robust optimization;stochastic programming;uncertainty;unit commitment},
  owner     = {sotterson},
  timestamp = {2014.11.18},
}

@Article{Yu13gmixGPcplaWindFrcst,
  author    = {Jie Yu and Kuilin Chen and Junichi Mori and Mudassir M. Rashid},
  title     = {A {Gauss}ian mixture copula model based localized {Gauss}ian process regression approach for long-term wind speed prediction},
  journal   = {Energy},
  year      = {2013},
  number    = {0},
  issn      = {0360-5442},
  abstract  = {Optimizing wind power generation and controlling the operation of wind turbines to efficiently harness the renewable wind energy is a challenging task due to the intermittency and unpredictable nature of wind speed, which has significant influence on wind power production. A new approach for long-term wind speed forecasting is developed in this study by integrating \{GMCM\} (Gaussian mixture copula model) and localized \{GPR\} (Gaussian process regression). The time series of wind speed is first classified into multiple non-Gaussian components through the Gaussian mixture copula model and then Bayesian inference strategy is employed to incorporate the various non-Gaussian components using the posterior probabilities. Further, the localized Gaussian process regression models corresponding to different non-Gaussian components are built to characterize the stochastic uncertainty and non-stationary seasonality of the wind speed data. The various localized \{GPR\} models are integrated through the posterior probabilities as the weightings so that a global predictive model is developed for the prediction of wind speed. The proposed GMCM-GPR approach is demonstrated using wind speed data from various wind farm locations and compared against the GMCM-based \{ARIMA\} (auto-regressive integrated moving average) and \{SVR\} (support vector regression) methods. In contrast to GMCM-ARIMA and GMCM-SVR methods, the proposed GMCM-GPR model is able to well characterize the multi-seasonality and uncertainty of wind speed series for accurate long-term prediction.},
  comment   = {Wind power forecasting using Gaussian processes, mixtures, copulas, conditionality... I think this might be useful for probabilistic forecasting.

Does better than ARIMA or SVN based models},
  doi       = {10.1016/j.energy.2013.09.013},
  file      = {Yu13gmixGPcplaWindFrcst.pdf:Yu13gmixGPcplaWindFrcst.pdf:PDF},
  groups    = {PointDerived, doReadWPV_2},
  keywords  = {Renewable wind power},
  owner     = {sotterson},
  timestamp = {2013.10.08},
  url       = {http://www.sciencedirect.com/science/article/pii/S0360544213007640},
}

@Article{Fenske11idRiskBoostAddQR,
  author    = {Fenske, Nora and Kneib, Thomas and Hothorn, Torsten},
  title     = {Identifying risk factors for severe childhood malnutrition by boosting additive quantile regression},
  journal   = {Journal of the American Statistical Association},
  year      = {2011},
  volume    = {106},
  number    = {494},
  abstract  = {Ordinary linear and generalized linear regression models relate the mean of a
response variable to a linear combination of covariate effects and, as a consequence,
focus on average properties of the response. Analyzing childhood malnutrition in
developing or transition countries based on such a regression model implies that
the estimated effects describe the average nutritional status. However, it is of even
larger interest to analyze quantiles of the response distribution such as the 5% or
10pct quantile that relate to the risk of children for extreme malnutrition. In this
paper, we analyze data on childhood malnutrition collected in the 2005/2006 India
Demographic and Health Survey based on a semiparametric extension of quantile
regression models where nonlinear effects are included in the model equation, leading
to additive quantile regression. The variable selection and model choice problems
associated with estimating an additive quantile regression model are addressed by a
novel boosting approach. Based on this rather general class of statistical learning
procedures for empirical risk minimization, we develop, evaluate and apply a boosting
algorithm for quantile regression. Our proposal allows for data-driven determination
of the amount of smoothness required for the nonlinear effects and combines model
selection with an automatic variable selection property. The results of our empirical
evaluation suggest that boosting is an appropriate tool for estimation in linear and
additive quantile regression models and helps to identify yet unknown risk factors for
childhood malnutrition.
Keywords: functional gradient boosting, penalized splines, additive models, variable
selection, model choice.},
  comment   = {Boosting high dim QR w/ featsel built in. Is additive but terms are nonlinear and can include interactions. The author's PhD thesis (Fenske12strctAddQRthesis) is more clear than this paper, so I have more notes there. Is implemented in R. The focus is on doing an interpretable, purely additive model, which doesn't inherently have constrained output; the related technique in Mayr12genAddModelsHiDimBoost might be more appropriate for wind power quantile regression.

Basic Idea: the quantile prediction is a weighted sum of "base learners" -- simple regression models fed single inputs or simple interactions of them. At each iteration, the negative gradient of the overall quantile error cost function is computed, and then the base learners are trained to predict it; the coeffs of the base learner that predicts the cost function the best is updated by weighted averaging with the previous iteration's coeffs. This is a gradient boosting algorith, so unlike adaboost or whatever, it seems that there is no random resampling.

Advantages claimed:
i.) estimation and variable selection are combined, as well as the choice of linear or nonlinear functions of them
ii.) smoother since don't need to do piecewise linear, as in linear programming (but what about mmqr??). Anyway, this is because differentiability isn't required
iii.) can have more complex models, with varying coeff terms (varying coeff: a coeff multiplied by a function of some other input)
 --- could be used for wind power dir dep, as in Nielsen02windPowVarCoeff
iv.) can use standard boosting software
v.) subsampling replication and stability selection (why is this an advantage?)

Base Learners
* varying coefficients and interaction terms somewhat described here: Kneib09varSelGeoaddRgrssn

Weight updates
* base learners are all updatable by LSQ predicting the neg cost function gradient
 - cost is quantile error
 - LSQ is great b/c can do standard training
 * update is by weighted average w/ prev coeffs
 - weight is a tweakable step size
 - so base learners must be expressible in terms of linear coeffs
* only the best is updated, and since some may never get updated, this is like coefficient "shrinkage"
 -- Kneib09varSelGeoaddRgrssn says it's important to penalize more complex base learners on similar problem. Is that done here?
* "neg cost function" is just the quantile tau or 1-tau, depending upon sum of active (non-zero) base learners
* LSQ is great because can use standard base learner training

Feature selection
* partly, this is just leaving coeffs set to zero because they are never the best and therefore aren't updated
* but they also mention "stability selection" which they don't define (see: Meinshausen10Stabilityselection)
* in energytop.org, see: <<Feature selection for varying coefficient models>>

Software
* this seems to be in gamboost or mboost, in R

Performance
* works better than several other boosting techniques, like trees (see paper for the list)

Said to be more flexible than Koenker's additive QR approach.

Slides are from a related talk in 2009

Make it beta-bounded with: Schmid13betaBoostRgrssn ?
General boosted regression with feature construction: Lillywhite13featCnstrct
Gradient boost description wasn't too clear: is this better? Mayr14evoBoost
Make it adaptive with Pardoe10BoostRgrsnXfer ?
Maybe boosted p-splines are better described in: Schmid07BoostAddPspline
Feature selection used: Meinshausen10Stabilityselection
Fenske's thesis: Fenske12strctAddQRthesis},
  doi       = {10.1198/jasa.2011.ap09272},
  file      = {Journal Paper w/ Supplements:Fenske11idRiskBoostAddQR.pdf:PDF;Technote 2009:Fenske11idRiskBoostAddQR_TechNote.pdf:PDF;Slides 2009:Fenske11idRiskBoostAddQR_Slides.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.07.04},
}

@Article{Jain10datClust50km,
  author    = {Jain, Anil K},
  title     = {Data clustering: 50 years beyond K-means},
  journal   = {Pattern Recognition Letters},
  year      = {2010},
  volume    = {31},
  number    = {8},
  pages     = {651--666},
  abstract  = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering.},
  comment   = {Award winning clustering review. At least look at the slides (attached too)},
  file      = {Paper:Jain10datClust50km.pdf:PDF;Slides:Jain10datClust50km_Slides.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.04.24},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167865509002323},
}

@Article{Leslie07genHeteroLinRegr,
  author    = {Leslie,, David S. and Kohn,, Robert and Nott,, David J.},
  title     = {A general approach to heteroscedastic linear regression},
  year      = {2007},
  volume    = {17},
  number    = {2},
  pages     = {131--146},
  issn      = {0960-3174},
  doi       = {10.1007/s11222-006-9013-8},
  abstract  = {Our article presents a general treatment of the linear regression model, in which the error distribution is modelled nonparametrically and the error variances may be heteroscedastic, thus eliminating the need to transform the dependent variable in many data sets. The mean and variance components of the model may be either parametric or nonparametric, with parsimony achieved through variable selection and model averaging. A Bayesian approach is used for inference with priors that are data-based so that estimation can be carried out automatically with minimal input by the user. A Dirichlet process mixture prior is used to model the error distribution nonparametrically; when there are no regressors in the model, the method reduces to Bayesian density estimation, and we show that in this case the estimator compares favourably with a well-regarded plug-in density estimator. We also consider a method for checking the fit of the full model. The methodology is applied to a number of simulated and real examples and is shown to work well.},
  file      = {Leslie07genHeteroLinRegr.pdf:Leslie07genHeteroLinRegr.pdf:PDF},
  journal   = {Statistics and Computing},
  location  = {Hingham, MA, USA},
  owner     = {sotterson},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2009.02.09},
}

@Article{Trosset08mdsOOS,
  author      = {Trosset, Michael W and Priebe, Carey E},
  title       = {The out-of-sample problem for classical multidimensional scaling},
  journal     = {Computational statistics \& data analysis},
  year        = {2008},
  volume      = {52},
  number      = {10},
  pages       = {4635--4642},
  month       = dec,
  abstract    = {Out-of-sample embedding techniques insert additional points into previously constructed configurations. An out-of-sample extension of classical multidimensional scaling is presented. The out-of-sample extension is formulated as an unconstrained nonlinear least-squares problem. The objective function is a fourth-order polynomial, easily minimized by standard gradient-based methods for numerical optimization. Two examples are presented.},
  comment     = {How to embed new points in trained MDS space without recomputing the eigen problem. Trick is to avoid the eigen problem and solve with unconstrained, nonlinear least squares. Empirically, this seeems to give an exact aswer. Is also extended to adding k>1 new points.

In a leave-one-out cross-validation situation, redoing the eigencomputation would disturb the original training point locations (and therefore, be kinda test on train).

I am not sure, however, if the nonlin LSQ computational load is any lower than just redoing the eigenvalues (but this is not the author's concern).

Says Bengio04OutOfSmplExt reconstructs eigenmaps and is not satisfied with that, I think, because it's not exact.

The tech report is attached, not the article},
  file        = {Trosset08mdsOOS.pdf:Trosset08mdsOOS.pdf:PDF},
  groups      = {Read},
  institution = {Department of Statistics. Indiana University},
  location    = {Bloomington, IN},
  owner       = {sotterson},
  publisher   = {Elsevier},
  timestamp   = {2014.06.27},
  type        = {Technical Report},
  url         = {http://www.sciencedirect.com/science/article/pii/S0167947308001515},
}

@Article{Yan12autoNNtsFrcst,
  author    = {Weizhong Yan},
  title     = {Toward Automatic Time-Series Forecasting Using Neural Networks},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  year      = {2012},
  volume    = {23},
  number    = {7},
  pages     = {1028--1039},
  abstract  = {Over the past few decades, application of artificial neural networks (ANN) to time-series forecasting (TSF) has been growing rapidly due to several unique features of ANN models. However, to date, a consistent ANN performance over different studies has not been achieved. Many factors contribute to the inconsistency in the performance of neural network models. One such factor is that ANN modeling involves determining a large number of design parameters, and the current design practice is essentially heuristic and ad hoc, this does not exploit the full potential of neural networks. Systematic ANN modeling processes and strategies for TSF are, therefore, greatly needed. Motivated by this need, this paper attempts to develop an automatic ANN modeling scheme. It is based on the generalized regression neural network (GRNN), a special type of neural network. By taking advantage of several GRNN properties (i.e., a single design parameter and fast learning) and by incorporating several design strategies (e.g., fusing multiple GRNNs), we have been able to make the proposed modeling scheme to be effective for modeling large-scale business time series. The initial model was entered into the NN3 time-series competition. It was awarded the best prediction on the reduced dataset among approximately 60 different models submitted by scholars worldwide.},
  comment   = {Maybe good for Enercast, since it's automatic},
  doi       = {10.1109/TNNLS.2012.2198074},
  file      = {Yan12autoNNtsFrcst.pdf:Yan12autoNNtsFrcst.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2013.03.15},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6210391},
}

@PhdThesis{Ranjan09CombEvalProbFrcsts,
  author      = {Roopesh Ranjan},
  title       = {Combining and Evaluating Probabilistic Forecasts},
  year        = {2009},
  abstract    = {Over the past one to two decades, there has been a shift of paradigms from deterministic
(or point) forecasts to probabilistic (or distributional) forecasts. Probabilistic forecasts take
uncertainty in the prediction into account and forecast a probability distribution function
(pdf ) for the unknown quantity of interest. In the case of binary events, the probabilistic
forecast is the probability that the event will occur. In the case of continuous variables,
the probabilistic forecast is the predictive density or distribution for the variable of interest.
Calibration and sharpness are two important components of a probabilistic forecast. Cali-
bration refers to statistical consistency between the forecasts and the realizations. Sharpness
refers to the spread of the forecast pdf. The narrower the pdf, the sharper the forecast.
Proper scoring rules combine calibration and sharpness together. They are a function of
the probability forecast and observation that materializes. Using proper scoring rules a
forecaster maximizes his expected gain by giving his true belief.
We propose a method for comparing density forecasts which is based on weighted versions
of the continuous ranked probability score (CRPS). The weighting emphasizes regions of
interest, such as the tails or the center of a variable?s range, while encouraging the forecaster
to give his true belief (propriety), as opposed to a recently developed weighted likelihood ra-
tio test which encourages forecasters to deviate from their true beliefs (hedging). Threshold
and quantile based decompositions of the CRPS can be illustrated graphically and prompt
insights into the strengths and deficiencies of a forecasting method. We illustrate the use
of the weighted CRPS and graphical tools in case studies on the Bank of England?s density
forecasts of quarterly inflation rates in the United Kingdom, and probabilistic predictions
of wind resources in the Pacific Northwest.
We also consider the problem of combining probabilistic forecasts. Linear pooling is by
far the most popular method for combining probabilistic forecasts. However, any nontriv-
ial weighted average of two or more distinct calibrated probability or density forecasts is
necessarily uncalibrated and lacks sharpness. In view of this, linear pooling requires recal-
ibration, even in the ideal case in which the individual forecasts are calibrated. Toward
this end, we propose a beta transformed linear opinion pool (BLP) for the aggregation of
probability forecasts or densities from distinct, calibrated or uncalibrated sources. The BLP
method fits an optimal nonlinearly recalibrated forecast combination, by compositing a beta
transform and the traditional linear opinion pool. The technique is illustrated in simulation
examples and case studies on probability of precipitation forecasts in the Pacific Northwest
and density forecasts of temperature at the Sea-Tac Airport.},
  comment     = {How to combine forecasts. Advocates recalibration. Good for ReWP?},
  file        = {Ranjan09CombEvalProbFrcsts.pdf:Ranjan09CombEvalProbFrcsts.pdf:PDF},
  institution = {University of Washington, Seattle},
  url         = {https://www.amstat.org/sections/bus_econ/ranjan_thesis.pdf},
}

@Article{McKenzie10expSmthDampTrnd,
  author    = {McKenzie, Eddie and Gardner, Everette S},
  title     = {Damped trend exponential smoothing: A modelling viewpoint},
  journal   = {International Journal of Forecasting},
  year      = {2010},
  volume    = {26},
  number    = {4},
  pages     = {661--665},
  abstract  = {Over the past twenty years, damped trend exponential smoothing has performed well in numerous empirical studies, and it is
now well established as an accurate forecasting method. The original motivation for this method was intuitively appealing, but
said very little about why or when it provided an optimal approach. The aim of this paper is to provide a theoretical rationale for
the damped trend method based on Brown?s original thinking about the form of underlying models for exponential smoothing.
We develop a random coefficient state space model for which damped trend smoothing provides an optimal approach, and
within which the damping parameter can be interpreted directly as a measure of the persistence of the linear trend.

Keywords: Time series; Exponential smoothing; ARIMA models; State space models},
  comment   = {Exponential smoothing (kind of Holt-Winters) with trend can overshoot. If damp the trend, it gets better.},
  file      = {McKenzie10expSmthDampTrnd.pdf:McKenzie10expSmthDampTrnd.pdf:PDF},
  publisher = {Elsevier},
  url       = {http://www.researchgate.net/profile/Eddie_McKenzie/publication/223667578_Damped_trend_exponential_smoothing_A_modelling_viewpoint/links/00b7d5283702350e48000000.pdf},
}

@InProceedings{Muhr08weathParamLineMon,
  author    = {M. Muhr and S. Pack and S. Jaufer and W. Haimbl and A. Messner},
  title     = {Experiences with the Weather Parameter Method for the use in Overhead Line Monitoring Systems},
  booktitle = {International Conference on Large Electric Systems (CIGRE)},
  year      = {2008},
  abstract  = {Overhead lines are essential components of high voltage power systems worldwide. The performance of this network element is important for a save and reliable transmission of electrical energy. The load capability of a line is related basically to the line design itself and takes centre stage of the economic operation whereas the sag behaviour and the technical condition of the conductor and their compo-nents are more related to the line and network safety. Based on this situation overhead line monitoring systems are actually under discussion in many com-mittees worldwide. This auxiliary equipment for overhead power lines helps to evaluate the actual transfer capability and/or to increase the system reliability. Therefore a number of systems using different measuring methods and monitoring techniques are available on the market. More or less all methods are based on physical data and process information to achieve a suitable output to control the line stress. One of these methods is called the Weather Parameter Method, which uses local metrologi-cal weather data and system load information to estimate the conductor temperature. In a research project running over years an overhead line monitoring system was set up in cooperation with the Austrian transmission utility APG. After these years of experience a number of information and data based on the principle of the Weather Parameter Method is available and gives the scientific basement for this contribution. Three observation stations are collecting data all over the year; two stations are at a 220kV overhead line and one station at the open air test field of the IHS in Graz in use. This contribution focus on the coherences of the different parameters collected for the evaluation of the conductor temperature and it is not the main goal to discuss the improved performance by using a thermal management system, as mentioned in many other papers. By the knowledge of the punch-through of significant parameters in combination with the appearance probability a more accurate scheduling of the utilisation of the line under observation will be possible. A well prepared processing of many significant parameters for an overhead line monitoring system and},
  comment   = {Measured wind speed \& temp vs. temperature on a dummy power line
 * equation for power line thermal inputs and outputs (compare to Daconti03incrPowTrans)
* line current limited for the combo of 35?C, 0.6m/s
 -- (Daconti03incrPowTrans says temp accuracy below 1.5 m/s is crucial)
* but >30?C, low wind is improbable
* say moderate temps around 30 are more imp, since windless (\& overheating) more probable
* topography and shadowing must be accounted for ("decisive")},
  file      = {Muhr08weathParamLineMon.pdf:Muhr08weathParamLineMon.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.02.23},
}

@Book{Bengfort16hdoopIntroDSbook,
  title     = {Data Analytics with Hadoop: An Introduction for Data Scientists},
  publisher = {" O'Reilly Media, Inc."},
  year      = {2016},
  author    = {Bengfort, Benjamin and Kim, Jenny},
  abstract  = {Overview of Chapters
This book is intended to be a guided walk through of the Hadoop ecosystem, and as
such we’ve laid out the book in two broad parts split across the halves of the book.
Part I (Chapters 1–5) introduces distributed computing at a very high level, discus‐
sing how to run computations on a cluster. Part II (Chapters 6–10) focuses more
specifically on tools and techniques that should be recognizable to data scientists, and
intends to provide a motivation for a variety of analytics and large-scale data manage‐
ment. (Chapter 5 serves as a transition from the broad discussion of distributed com‐
puting to more specific tools and an implementation of the big data science pipeline.)
The chapter break down is as follows:
Chapter 1, The Age of the Data Product
We begin the book with an introduction to the types of applications that big data
and data science produce together: data products. This chapter discusses the
workflow behind creating data products and specifies how the sequential model
of data analysis fits into the distributed computing realm.
Chapter 2, An Operating System for Big Data
Here we provide an overview of the core concepts behind Hadoop and what
makes cluster computing both beneficial and difficult. The Hadoop architecture
is discussed in detail with a focus on both YARN and HDFS. Finally, this chapter
discusses interacting with the distributed storage system in preparation for per‐
forming analytics on large datasets.
Chapter 3, A Framework for Python and Hadoop Streaming
This chapter covers the fundamental programming abstraction for distributed
computing: MapReduce. However, the MapReduce API is written in Java, a pro‐
gramming language that is not popular for data scientists. Therefore, this chapter
focuses on how to write MapReduce jobs in Python with Hadoop Streaming.
Chapter 4, In-Memory Computing with Spark
While understanding MapReduce is essential to understanding distributed com‐
puting and writing high-performance batch jobs such as ETL, day-to-day interac‐
tion and analysis on a Hadoop cluster is usually done with Spark. Here we
introduce Spark and how to program Python Spark applications to run on YARN
either in an interactive fashion using PySpark or in cluster mode.
Chapter 5, Distributed Analysis and Patterns
In this chapter, we take a practical look at how to write distributed data analysis
jobs through the presentation of design patterns and parallel analytical algo‐
rithms. Coming into this chapter you should understand the mechanics of writ‐
ing Spark and MapReduce jobs and coming out of the chapter, you should feel
comfortable actually implementing them.
Chapter 6, Data Mining and Warehousing
Here we present an introduction to data management, mining, and warehousing
in a distributed context, particularly in relation to traditional database systems.
This chapter will focus on Hadoop’s most popular SQL-based querying engine,
Hive, as well as its most popular NoSQL database, HBase. Data wrangling is the
second step in the data science pipeline, but data needs somewhere to be ingested
to—and this chapter explores how to manage very large datasets.
Chapter 7, Data Ingestion
Getting data into a distributed system for computation may actually be one of the
biggest challenges given the magnitude of both the volume and velocity of data.
This chapter explores ingestion techniques from relational databases using Sqoop
as a bulk loading tool, as well as the more flexible Apache Flume for ingesting
logs and other unstructured data from network sources.
Chapter 8, Analytics with Higher-Level APIs
Here we offer a review of higher-order tools for programming complex Hadoop
and Spark applications, in particular with Apache Pig and Spark’s DataFrames
API. In Part I, we discussed the implementation of MapReduce and Spark for
executing distributed jobs, and how to think of algorithms and data pipelines as
data flows. Pig allows you to more easily describe the data flows without actually
implementing the low-level details in MapReduce. Spark provides integrated
modules that provide the ability to seamlessly mix procedural processing with
relational queries and open the door to powerful analytic customizations.
Chapter 9, Machine Learning
Most of the benefits of big data are realized in a machine learning context: a
greater variety of features and wider input space mean that pattern recognition
techniques are much more effective and personalized. This chapter introduces
classification, clustering, and collaborative filtering. Rather than discuss model‐
ing in detail, we will instead get you started on scalable learning techniques using
Spark’s MLlib.
Chapter 10, Summary: Doing Distributed Data Science
To conclude, we present a summary of doing distributed data science as a com‐
plete view: integrating the tools and techniques that were discussed in isolation in
the previous chapters. Data science is not a single activity but rather a lifecycle
that involves data ingestion, wrangling, modeling, computation, and operational‐
ization. This chapter discusses architectures and workflows for doing distributed
data science at a 20,000-foot view.
Appendix A, Creating a Hadoop Pseudo-Distributed Development Environment
This appendix serves as a guide to setting up a development environment on
your local machine in order to program distributed jobs. If you don’t have a clus‐
ter available to you, this guide is essential in order to prepare to run the examples
provided in the book.
Appendix B, Installing Hadoop Ecosystem Products
An extension to the guide found in Appendix A, this appendix offers instructions
for installing the many ecosystem tools and products that we discuss in the book.
Although a common methodology for installing services is proposed in Appen‐
dix A, this appendix specifically looks at gotchas and caveats for installing the
services to run the examples you will find as you read.
As you can see, this is a lot of topics to cover in such a short book! },
  comment   = {Book on Hadoop, Spark, etc.  Easy to read, and github site (URL for this entry) },
  file      = {:Bengfort16hdoopIntroDSbook.pdf:PDF},
  url       = {https://github.com/kishorevbhosale/Hadoop-Books/blob/master/Books/Data%20Analytics%20with%20Hadoop%20-%20An%20Introduction%20for%20Data%20Scientists.pdf},
}

@Article{Cannon10GEVcdn,
  author    = {Cannon, Alex J.},
  title     = {A flexible nonlinear modelling framework for nonstationary generalized extreme value analysis in hydroclimatology},
  journal   = {Hydrological Processes},
  year      = {2010},
  volume    = {24},
  number    = {6},
  pages     = {673--685},
  issn      = {1099-1085},
  abstract  = {Parameters in a generalized extreme value (GEV) distribution are specified as a function of covariates using a conditional density network (CDN), which is a probabilistic extension of the multilayer perceptron neural network. If the covariate is time or is dependent on time, then the GEV-CDN model can be used to perform nonlinear, nonstationary GEV analysis of hydrological or climatological time series. Owing to the flexibility of the neural network architecture, the model is capable of representing a wide range of nonstationary relationships. Model parameters are estimated by generalized maximum likelihood, an approach that is tailored to the estimation of GEV parameters from geophysical time series. Model complexity is identified using the Bayesian information criterion and the Akaike information criterion with small sample size correction. Monte Carlo simulations are used to validate GEV-CDN performance on four simple synthetic problems. The model is then demonstrated on precipitation data from southern California, a series that exhibits nonstationarity due to interannual/interdecadal climatic variability. Copyright: 2009 Her Majesty the Queen in right of Canada. Published by John Wiley & Sons, Ltd.},
  comment   = {A neural network tailored to predict the extreme value distirbution. Seems best for spinning reserves, where really only want the upper quantile?

R library:
http://cran.r-project.org/web/packages/GEVcdn/citation.html},
  doi       = {10.1002/hyp.7506},
  file      = {Cannon10GEVcdn.pdf:Cannon10GEVcdn.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_2},
  keywords  = {extreme value analysis, nonstationary, statistical modelling, neural network, nonlinear hydroclimatology},
  owner     = {sotterson},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.10.22},
}

@Article{Chen09copulaARQR,
  author    = {Chen, Xiaohong and Koenker, Roger and Xiao, Zhijie},
  title     = {Copula-based nonlinear quantile autoregression},
  journal   = {The Econometrics Journal},
  year      = {2009},
  volume    = {12},
  number    = {s1},
  pages     = {S50--S67},
  abstract  = {Parametric copulas are shown to be attractive devices for specifying quantile
autoregressive models for nonlinear time-series. Estimation of local, quantile-specific copula-
based time series models offers some salient advantages over classical global parametric
approaches. Consistency and asymptotic normality of the proposed quantile estimators are
established under mild conditions, allowing for global misspecification of parametric copulas
and marginals, and without assuming any mixing rate condition. These results lead to a general
framework for inference and model specification testing of extreme conditional value-at-risk
for financial time series data.

Keywords: Copula, Ergodic nonlinear Markov models, Quantile autoregression.},
  comment   = {For Henry Martin's autoregressive Poisson model (Hawke's process in Toke11mktMkingOBsprd).  Could also be used for short term forecasting?  Better yet, grid node forecasting, since Malte has had luck with copulas and the Vine copula seems interesting.},
  file      = {Chen09copulaARQR.pdf:Chen09copulaARQR.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2016.12.06},
  url       = {http://onlinelibrary.wiley.com/doi/10.1111/j.1368-423X.2008.00274.x/abstract},
}

@Article{Johnson78angLinDistRgrsn,
  author              = {Johnson, Richard A. and Wehrly, Thomas E.},
  title               = {Some Angular-Linear Distributions and Related Regression Models},
  journal             = {Journal of the American Statistical Association},
  year                = {1978},
  volume              = {73},
  number              = {363},
  pages               = {602--606},
  issn                = {0162-1459},
  abstract            = {Parametric models are proposed for the joint distribution of bivariate random variables when one variable is directional and one is scalar. These distributions are developed on the basis of the maximum entropy principle and by the specification of the marginal distributions. The properties of these distributions and the statistical analysis of regression models based on these distributions are explored. One model is extended to several variables in a form that justifies the use of least squares for estimation of parameters, conditional on the observed angles.},
  comment             = {For lagged wind velocity basis?},
  copyright           = {Copyright 1978 American Statistical Association},
  file                = {Johnson78angLinDistRgrsn.pdf:Johnson78angLinDistRgrsn.pdf:PDF},
  jstor_articletype   = {research-article},
  jstor_formatteddate = {Sep., 1978},
  language            = {English},
  owner               = {scot},
  publisher           = {American Statistical Association},
  timestamp           = {2010.12.22},
  url                 = {http://www.jstor.org/stable/2286608},
}

@InProceedings{Gregorcic02dynNonLinGausProc,
  author    = {Gregor Gregorcic and Gordon Lightbody},
  title     = {{Gauss}ian Processes for Modelling of Dynamic Non-linear Systems},
  booktitle = {Irish Signals and Systems Conference (ISSC)},
  year      = {2002},
  month     = jun,
  abstract  = {Parametric multiple model techniques have recently been proposed for the modelling of non?linear systems and use in nonlinear control. Research effort has focused on issues such as the selection of the structure, constructive learning techniques, computational issues, the curse of dimensionality, off?equilibrium behavior etc. To reduce these problems, the use of non?parametrical modelling approaches have been proposed. This paper introduces the Gaussian process prior approach for the modelling of non?linear dynamic systems. Issues such as selection of the input space dimension and multi?step ahead prediction are discussed in this paper. The Gaussian process modelling technique is demonstrated on the simulated example of the non?linear hydraulic system.},
  comment   = {Does multi-step ahead, and multiple model combo w/ GP's},
  file      = {Gregorcic02dynNonLinGausProc.pdf:Gregorcic02dynNonLinGausProc.pdf:PDF;Gregorcic02dynNonLinGausProc.pdf:Gregorcic02dynNonLinGausProc.pdf:PDF},
  location  = {Cork, Ireleand},
  owner     = {scotto},
  timestamp = {2008.10.04},
}

@Article{Genton07BlowWind,
  author    = {Marc Genton and Amanda Hering},
  title     = {Blowing in the wind},
  journal   = {Significance},
  year      = {2007},
  volume    = {4},
  number    = {1},
  pages     = {11--14},
  month     = mar,
  abstract  = {Part of the answer to rising energy needs and costs may literally be blowing in the wind. Among sustainable sources of electricity, only wind energy has the capacity and technology needed to compete in the open marketplace. The largest onshore wind farm in Europe is being built in Scotland, the largest in the USA is planned for southern California, and the biggest offshore wind farm production in the world is slated for the Thames Estuary. But wind is intermittent. Marc Genton and Amanda Hering explain how advanced statistical techniques will enable wind energy to be more efficiently incorporated into the electrical grid},
  comment   = {Short overview recommended by Kristin. I haven't read it yet.},
  doi       = {10.1111/j.1740-9713.2007.00212.x},
  file      = {Genton07BlowWind.pdf:Genton07BlowWind.pdf:PDF;Genton07BlowWind.pdf:Genton07BlowWind.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.04.08},
}

@Article{Schelter09strengthParDirCoher,
  author   = {Bjrn Schelter and Jens Timmer and Michael Eichler},
  title    = {Assessing the strength of directed influences among neural signals using renormalized partial directed coherence},
  year     = {2009},
  volume   = {179},
  number   = {1},
  pages    = {121--130},
  issn     = {0165-0270},
  doi      = {DOI: 10.1016/j.jneumeth.2009.01.006},
  url      = {http://www.sciencedirect.com/science/article/B6T04-4VDS87P-5/2/78fc6d5f7b58adb00d39fbc9312de8e9},
  abstract = {Partial directed coherence is a powerful tool used to analyze interdependencies in multivariate systems based on vector autoregressive modeling. This frequency domain measure for Granger-causality is designed such that it is normalized to [0,1]. This normalization induces several pitfalls for the interpretability of the ordinary partial directed coherence, which will be discussed in some detail in this paper. In order to avoid these pitfalls, we introduce renormalized partial directed coherence and calculate confidence intervals and significance levels. The performance of this novel concept is illustrated by application to model systems and to electroencephalography and electromyography data from a patient suffering from Parkinsonian tremor.},
  journal  = {Journal of Neuroscience Methods},
  keywords = {Partial directed coherence},
}

@InCollection{Rosipal06plsOverviewRecent,
  author    = {Rosipal, Roman and Kr{\"a}mer, Nicole},
  title     = {Overview and recent advances in partial least squares},
  booktitle = {Subspace, latent structure and feature selection},
  publisher = {Springer},
  year      = {2006},
  pages     = {34--51},
  abstract  = {Partial Least Squares (PLS) is a wide class of methods for modeling relations
between sets of observed variables by means of latent variables. It comprises
of regression and classification tasks as well as dimension reduction techniques
and modeling tools. The underlying assumption of all PLS methods is that the
observed data is generated by a system or process which is driven by a small
number of latent (not directly observed or measured) variables. Projections of
the observed data to its latent structure by means of PLS was developed by
Herman Wold and coworkers [48, 49, 52].
PLS has received a great amount of attention in the field of chemomet-
rics. The algorithm has become a standard tool for processing a wide spectrum
of chemical data problems. The success of PLS in chemometrics resulted in a
lot of applications in other scientific areas including bioinformatics, food re-
search, medicine, pharmacology, social sciences, physiology?to name but a few
[28, 25, 53, 29, 18, 22].
This chapter introduces the main concepts of PLS and provides an overview
of its application to different data analysis problems. Our aim is to present a
concise introduction, that is, a valuable guide for anyone who is concerned with
data analysis.
In its general form PLS creates orthogonal score vectors (also called latent
vectors or components) by maximising the covariance between different sets of
variables. PLS dealing with two blocks of variables is considered in this chapter,
although the PLS extensions to model relations among a higher number of sets
exist [44, 46, 47, 48, 39]. PLS is similar to Canonical Correlation Analysis (CCA)
where latent vectors with maximal correlation are extracted [24]. There are dif-
ferent PLS techniques to extract latent vectors, and each of them gives rise to a
variant of PLS.
PLS can be naturally extended to regression problems. The predictor and
predicted (response) variables are each considered as a block of variables. PLS
then extracts the score vectors which serve as a new predictor representation

and regresses the response variables on these new predictors. The natural asym-
metry between predictor and response variables is reflected in the way in which
score vectors are computed. This variant is known under the names of PLS1 (one
response variable) and PLS2 (at least two response variables). PLS regression
used to be overlooked by statisticians and is still considered rather an algorithm
than a rigorous statistical model [14]. Yet within the last years, interest in the
statistical properties of PLS has risen. PLS has been related to other regression
methods like Principal Component Regression (PCR) [26] and Ridge Regression
(RR) [16] and all these methods can be cast under a unifying approach called
continuum regression [40, 9]. The effectiveness of PLS has been studied theoret-
ically in terms of its variance [32] and its shrinkage properties [12, 21, 7]. The
performance of PLS is investigated in several simulation studies [11, 1].
PLS can also be applied to classification problems by encoding the class mem-
bership in an appropriate indicator matrix. There is a close connection of PLS
for classification to Fisher Discriminant Analysis (FDA) [4]. PLS can be applied
as a discrimination tool and dimension reduction method?similar to Principal
Component Analysis (PCA). After relevant latent vectors are extracted, an ap-
propriate classifier can be applied. The combination of PLS with Support Vector
Machines (SVM) has been studied in [35].
Finally, the powerful machinery of kernel-based learning can be applied to
PLS. Kernel methods are an elegant way of extending linear data analysis tools
to nonlinear problems [38].},
  comment   = {The most recent reference in Matlab R2014b plsregress()},
  doi       = {10.1007/11752790_2},
  file      = {Rosipal06plsOverviewRecent.pdf:Rosipal06plsOverviewRecent.pdf:PDF},
}

@Article{Zhao14MultiNonLinPLSoverview,
  author    = {Zhao, Qibin and Zhang, Liqing and Cichocki, Andrzej},
  title     = {Multilinear and nonlinear generalizations of partial least squares: an overview of recent advances},
  journal   = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  year      = {2014},
  volume    = {4},
  number    = {2},
  pages     = {104--115},
  issn      = {1942-4795},
  abstract  = {Partial least squares (PLS) is an efficient multivariate statistical regression technique that has shown to be particularly useful for analysis of highly collinear data. To predict response variables Y based independent variables X, PLS attempts to find a set of common orthogonal latent variables by projecting both X and Y onto a new subspace respectively. As an increasing interest in multi-way analysis, the extension to multilinear regression model is also developed with the aim to analyzing two-multidimensional tensor data. In this article, we overview the PLS-related methods including linear, multilinear, and nonlinear variants and discuss the strength of the algorithms. As canonical correlation analysis (CCA) is another similar technique with the aim to extract the most correlated latent components between two datasets, we also briefly discuss the extension of CCA to tensor space. Finally, several examples are given to compare these methods with respect to the regression and classification techniques.Conflict of interest: The authors have declared no conflicts of interest for this article.

Index Terms?Tensor decomposition, Partial least squares (PLS), Canonical Correlation Analysis (CCA), Electrocorticogram (ECoG), Kernel machines.},
  comment   = {Review of tensor and many kindso of PLS. Also CCA. Read this before Cichocki14TensorNetworksBigOpt},
  doi       = {10.1002/widm.1120},
  file      = {Zhao14MultiNonLinPLSoverview.pdf:Zhao14MultiNonLinPLSoverview.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Periodicals, Inc},
  timestamp = {2015.02.09},
}

@Article{Boulesteix07plsHiDimGenom,
  author    = {Boulesteix, Anne-Laure and Strimmer, Korbinian},
  title     = {Partial least squares: a versatile tool for the analysis of high-dimensional genomic data},
  journal   = {Briefings in bioinformatics},
  year      = {2007},
  volume    = {8},
  number    = {1},
  pages     = {32--44},
  abstract  = {Partial least squares (PLS) is an efficient statistical regression technique that is highly suited for the analysis of genomic and proteomic data. In this article, we review both the theory underlying PLS as well as a host of bioinformatics applications of PLS. In particular, we provide a systematic comparison of the PLS approaches currently employed, and discuss analysis problems as diverse as, e.g. tumor classification from transcriptome data, identification of relevant genes, survival analysis and modeling of gene networks and transcription factor activities.
Key words
 partial least squares (PLS)
 high-dimensional genomic data
 gene expression
 classification
 dimension reduction},
  comment   = {Highly cited review of PLS, I think w/ weights too?},
  file      = {Boulesteix07plsHiDimGenom.pdf:Boulesteix07plsHiDimGenom.pdf:PDF},
  owner     = {sotterson},
  publisher = {Oxford Univ Press},
  timestamp = {2014.04.27},
  url       = {http://bib.oxfordjournals.org/content/8/1/32.short},
}

@Article{Dodge09partialQR,
  author    = {Dodge, Yadolah and Whittaker, Joe},
  title     = {Partial quantile regression},
  journal   = {Metrika},
  year      = {2009},
  volume    = {70},
  number    = {1},
  pages     = {35--57},
  abstract  = {Partial least squares regression (PLSR) is a method of finding a reliable predictor of the response variable when there are more regressors than observations. It does so by eliciting a small number of components from the regressors that are inherently informative about the response. Quantile regression (QR) estimates the quantiles of the response distribution by regression functions of the covariates, and so gives a fuller description of the response than does the usual regression for the mean value of the response. We extend QR to partial quantile regression (PQR) when there are more regressors than observations. For each percentile the method provides a low dimensional approximation to the joint distribution of the covariates and response with a given coverage probability and which, under further linearity assumptions, estimates the corresponding quantile of the conditional distribution. The methodology parallels the procedure for PLSR using a quantile covariance that is appropriate for predicting a quantile rather than the usual covariance which is appropriate for predicting a mean value. The analysis suggests a new measure of risk associated with the quantile regressions. Examples are given that illustrate the methodology and the benefits accrued, based on simulated data and the analysis of spectrometer data.},
  comment   = {Dimension reduction for QR using something that looks like partial least squares (PLS). Advantage is the dim reduce would be in the direction that tells you something about the quantiles.


I'm not sure how well its tested against benchmarks (don't have the paper yet), and note that it has only one cite in google scholar.
Giglio13riskPartialQR mentions this paper, says it proves consistency (if it's the same algorithm, I don't know)},
  doi       = {10.1007/s00184-008-0177-4},
  file      = {Dodge09partialQR.pdf:Dodge09partialQR.pdf:PDF},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2014.04.21},
}

@Article{Nami18cardFraudDynRandFrstKnn,
  author   = {Sanaz Nami and Mehdi Shajari},
  title    = {Cost-sensitive payment card fraud detection based on dynamic random forest and k-nearest neighbors},
  journal  = {Expert Systems with Applications},
  year     = {2018},
  volume   = {110},
  pages    = {381 - 392},
  issn     = {0957-4174},
  abstract = {Payment card fraud leads to heavy annual financial losses around the world, thus giving rise to the need for improvements to the fraud detection systems used by banks and financial institutions. In the academe, as well, payment card fraud detection has become an important research topic in recent years. With these considerations in mind, we developed a method that involves two stages of detecting fraudulent payment card transactions. The extraction of suitable transactional features is one of the key issues in constructing an effective fraud detection model. In this method, additional transaction features are derived from primary transactional data. A better understanding of cardholders’ spending behaviors is created by these features. After which the first stage of detection is initiated. A cardholder's spending behaviors vary over time so that new behavior of a cardholder is closer to his/her recent behaviors. Accordingly, a new similarity measure is established on the basis of transaction time in this stage. This measure assigns greater weight to recent transactions. In the second stage, the dynamic random forest algorithm is employed for the first time in initial detection, and the minimum risk model is applied in cost-sensitive detection. We tested the proposed method on a real transactional dataset obtained from a private bank. The results showed that the recent behavior of cardholders exerts a considerable effect on decision-making regarding the evaluation of transactions as fraudulent or legitimate. The findings also indicated that using both primary and derived transactional features increases the F-measure. Finally, an average 23% increase in prevention of damage (PoD) is achieved with the proposed cost-sensitive approach.},
  comment  = {Dynamic RF w/ KNN and apparently data age weighting.  Maybe there is a trick here for WattPlanGrid?  Could at least de-emphasize older data points during training.

I couldn't find a paper for this, but requested a copy on ResearchGate

Dynamic Random forest: Bernard12dynRandForest},
  doi      = {https://doi.org/10.1016/j.eswa.2018.06.011},
  file     = {:Nami18cardFraudDynRandFrstKnn.pdf:PDF},
  keywords = {Payment card fraud detection, Dynamic random forest, Cost-sensitive detection, Minimum risk},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417418303579},
}

@Article{VanDerLinde03pcaDimRedSpline,
  author    = {Van Der Linde, Angelika},
  title     = {{PCA}-based dimension reduction for splines},
  journal   = {Journal of Nonparametric Statistics},
  year      = {2003},
  volume    = {15},
  number    = {1},
  pages     = {77--92},
  abstract  = {PCA in a reproducing kernel Hilbert space is analysed as probabilistically
optimal procedure of dimension reduction given a covariance structure
by the reproducing kernel. It provides a unifying framework for various
seemingly disparate and special techniques of dimension reduction applied
to splines, in geostatistical ?kriging? or in interpolation of data resulting
from computer experiments. Regarding the covariance as de?ning a prior
for Bayesian analyses in a Hilbert function space several suggestions are
derived for data analyses involving functions particularly on multivariate
domains, including the choice of a parsimonious interpolation spline as regression
function in generalized models, the use of a Demmler-Reinsch-like
basis in kriging or interpolation and derivation of principal modes of variation
in collections of surfaces.
Keywords
principal components, splines, kriging, computer experiments},
  comment   = {Use for cfpcQR?},
  doi       = {10.1080/10485250306037},
  file      = {VanDerLinde03pcaDimRedSpline.pdf:VanDerLinde03pcaDimRedSpline.pdf:PDF},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2014.11.03},
}

@Article{Yoshida14psilneDrctSmth,
  author    = {Yoshida, Takuma},
  title     = {Direct Determination of Smoothing Parameter for Penalized Spline Regression},
  journal   = {Journal of Probability and Statistics},
  year      = {2014},
  volume    = {2014},
  abstract  = {Penalized spline estimator is one of the useful smoothing methods. To construct the estimator, having goodness of fit and smoothness, the smoothing parameter should be appropriately selected. The purpose of this paper is to select the smoothing parameter using the asymptotic property of the penalized splines.The new smoothing parameter selectionmethod is established in the context of minimization asymptotic form of MISE of the penalized splines.The mathematical and the numerical properties of the proposedmethod are studied. First we organize the newmethod in univariate regressionmodel. Next we extend to the additive models. A simulation study to confirm the efficiency of the proposed method is addressed.},
  comment   = {A way of calculating p-spline smoothing for regression w/o doing cross-validation.},
  file      = {Yoshida14psilneDrctSmth.pdf:Yoshida14psilneDrctSmth.pdf:PDF},
  owner     = {sotterson},
  publisher = {Hindawi Publishing Corporation},
  timestamp = {2014.10.30},
  url       = {http://www.hindawi.com/journals/jps/2014/203469/abs/},
}

@Article{Eilers10SplnsKntsPnlties,
  author    = {Eilers, Paul H. C. and Marx, Brian D.},
  title     = {Splines, knots, and penalties},
  journal   = {Wiley Interdisciplinary Reviews: Computational Statistics},
  year      = {2010},
  volume    = {2},
  number    = {6},
  pages     = {637--653},
  issn      = {1939-0068},
  abstract  = {Penalized splines have gained much popularity as a flexible tool for smoothing and semi-parametric models. Two approaches have been advocated: (1) use a B-spline basis, equally spaced knots, and difference penalties [Eilers PHC, Marx BD. Flexible smoothing using B-splines and penalized likelihood (with Comments and Rejoinder). Stat Sci 1996, 11:89?121.] and (2) use truncated power functions, knots based on quantiles of the independent variable and a ridge penalty [Ruppert D, Wand MP, Carroll RJ. Semiparametric Regression. New York: Cambridge University Press; 2003]. We compare the two approaches on many aspects: numerical stability, quality of the fit, interpolation/extrapolation, derivative estimation, visual presentation and extension to multidimensional smoothing. We discuss mixed model and Bayesian parallels to penalized regression. We conclude that B-splines with difference penalties are clearly to be preferred.},
  comment   = {Difference roughness penalties are best, the authors say. A probabably good discussion of periodic spline basis building.

* use 1\textsuperscript{st} and 2\textsuperscript{nd} order penalties when need pos. output w/ pos. input (e.g. for power smoothing).
 -- but I didn't understand how to derive the equation
* Eilers96FlexSmthBsplnPnlty has a matlab implementation for B-splines, partly explains the matrix operations
* Eilers03calTemp2Dpspline has analytical expression for 2D splines
* Jonathan14nonStatCondXtrmPenSpln has it for 3D extreme quantile spline
* extended w/ multi-dims, constraints: Pya15shapeCnstrAddMdl
* Eilers15twentyYrsPspline: more comprehensive review},
  doi       = {10.1002/wics.125},
  file      = {Eilers10SplnsKntsPnlties.pdf:Eilers10SplnsKntsPnlties.pdf:PDF},
  keywords  = {P-splines, truncated power functions, difference penalty, interpolation, smoothing},
  owner     = {sotterson},
  publisher = {John Wiley \& Sons, Inc.},
  timestamp = {2014.11.14},
}

@Article{Ruppert02numKntsPenSpln,
  author    = {Ruppert, David},
  title     = {Selecting the number of knots for penalized splines},
  journal   = {Journal of computational and graphical statistics},
  year      = {2002},
  volume    = {11},
  number    = {4},
  abstract  = {Penalized splines, or P-splines, are regression splines fit by least-squares with a roughness penalty.P-splines have much in common with smoothing splines, but the type of penalty used with a P-spline is somewhat more general than for a smoothing spline. Also, the number and location of the knots of a P-spline is not fixed as with a smoothing spline. Generally, the knots of a P-spline are at fixed quantiles of the independent variable and the only tuning parameters to choose are the number of knots and the penalty parameter. In this article, the effects of the number of knots on the performance of P-splines are studied. Two algorithms are proposed for the automatic selection of the number of knots. The myopic algorithm stops when no improvement in the generalized cross-validation statistic (GCV) is noticed with the last increase in the number of knots. The full search examines all candidates in a fixed sequence of possible numbers of knots and chooses the candidate that minimizes GCV.The myopic algorithm works well in many cases but can stop prematurely. The full-search algorithm worked well in all examples examined. A Demmler?Reinsch type diagonalization for computing univariate and additive P-splines is described. The Demmler?Reinsch basis is not effective for smoothing splines because smoothing splines have too many knots. For P-splines, however, the Demmler?Reinsch basis is very useful for super-fast generalized cross-validation.

Key Words: Additive models; Full search; Myopic search; P-spline; Smoothing.},
  comment   = {"Ruppert's Law of Thumb" for picking num. knots in penalized spline, according to Aguilera13psplinePCA and Kagerer13splnLSQregrssnIntro (Kagerer13* summarizes it best)

* Eilers15twentyYrsPspline says "wise choice" is 100, also says Ruppert sometimes means TP splines when he says "penalized splines" so I'm not sure if this is appropriate for what are now called psplines.},
  doi       = {10.1198/106186002853},
  file      = {Ruppert02numKntsPenSpln.pdf:Ruppert02numKntsPenSpln.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.20},
}

@Article{Huang18clustResidElecLdCmnty,
  author   = {Huang, Yunyou and Zhan, Jianfeng and Wang, Nana and Luo, Chunjie and Wang, Lei and Ren, Rui},
  title    = {Clustering Residential Electricity Load Curves via Community Detection in Network},
  journal  = {arXiv preprint arXiv:1811.10356},
  year     = {2018},
  abstract = {Performing analytic of household load curves (LCs) has significant value in predicting in-
dividual electricity consumption patterns, and hence facilitate developing demand-response
strategy, and finally achieve energy efficiency improvement and emission reduction. LC clus-
tering is a widely used analytic technology, which discovers electricity consumption patterns
by grouping similar LCs into same sub-groups. However, previous clustering methods treat
each LC in the data set as an individual time series, ignoring the inherent relationship among
different LCs, restraining the performance of the clustering methods. What’s more, due to the
significant volatility and uncertainty of LCs, the previous LC clustering approaches inevitably
result in either lager number of clusters or huge variances within a cluster, which is unaccept-
able for actual application needs. In this paper, we proposed an integrated approach to address
this issue. First, we converted the LC clustering task into the community detection task in
network. Second, we proposed a clustering approach incorporated with community detection
to improve the performance of LC clustering, which includes network construction, community
detection and typical load profile extraction. The method convert the data set into a network
in which the inherent relationship among LCs is represented by the edges of the network.
Third, we construct a multi-layer typical load profile directory to make the trade-off between
variances within a cluster and the number of the clusters, enabling the researchers to assess
the LCs and the customers in different layers according to practical application requirements.
The experiments demonstrate that our integrated approach outperform the state-of-the-art
methods.},
  comment  = {Perhaps a hacky paper but has some interesting aspects.

DTW --> similarity (I think) --> graph edges from thresholded similarity (I think) --> community detection --> clusters

Thresholding might make it a hack, but is interesting b/c
* is evaluated with many methods
* maybe community is a good idea (seems like I saw another community paper)
* the graph edge is a bit like spectral clustering (if I remember correctly), so could borrow GMM at end to get cluster probs?  Like in Li16loadProfileClustMultiRes
* has a way to pick #  clusters if given a desired range of # clusters.  Also, maybe a bit of a hack
},
  file     = {:Huang18clustResidElecLdCmnty.pdf:PDF},
  url      = {https://arxiv.org/abs/1811.10356},
}

@Article{Walter99perWavltScrtch,
  author    = {Walter, Gilbert G. and Cai, Luchuan},
  title     = {Periodic Wavelets from Scratch},
  year      = {1999},
  volume    = {1},
  issue     = {1},
  pages     = {25--41},
  issn      = {1521-1398},
  doi       = {10.1023/A:1022614519335},
  abstract  = {Periodic wavelets can be constructed from most standard wavelets by periodization. In this work we first derive some of their properties and then construct the periodic wavelets directly from their Fourier series without reference to standard wavelets. Several examples are given some of which are not constructable from the usual wavelets on the real line.},
  file      = {Walter99perWavltScrtch.pdf:Walter99perWavltScrtch.pdf:PDF},
  journal   = {Journal of Computational Analysis and Applications},
  keyword   = {Mathematics and Statistics},
  owner     = {scotto},
  publisher = {Springer Netherlands},
  timestamp = {2010.08.12},
}

@Article{Matheson76,
  author    = {Matheson, James E. and Winkler, Robert L.},
  title     = {Scoring rules for continuous probability distributions},
  year      = {1976},
  volume    = {22},
  number    = {10},
  pages     = {1087--1096},
  issn      = {0025-1909},
  abstract  = {Personal, or subjective, probabilities are used as inputs to many inferential and decision-making models, and various procedures have been developed for the elicitation of such probabilities. Included among these elicitation procedures are scoring rules, which involve the computation of a score based on the assessor's stated probabilities and on the event that actually occurs. The development of scoring rules has, in general, been restricted to the elicitation of discrete probability distributions. In this paper, families of scoring rules for the elicitation of continuous probability distributions are developed and discussed.},
  booktitle = {Management Science},
  comment   = {Related to scoring expert-generated heuristic distributions.

Highly cited bit I can't get the pdf, so far.},
  doi       = {10.2307/2629907?uid=3737864},
  groups    = {Test, CitaviImport1, doReadNonWPV_2},
  ncite     = {204},
  owner     = {sotterson},
  publisher = {INFORMS},
  timestamp = {2013.09.26},
}

@Article{Wu15pvGenPairCpla,
  author   = {W. Wu and K. Wang and B. Han and G. Li and X. Jiang and M. L. Crow},
  title    = {A Versatile Probability Model of Photovoltaic Generation Using Pair Copula Construction},
  journal  = {IEEE Transactions on Sustainable Energy},
  year     = {2015},
  volume   = {6},
  number   = {4},
  pages    = {1337-1345},
  month    = {Oct},
  issn     = {1949-3029},
  abstract = {Photovoltaic (PV) generation is increasingly popular in power systems. The nonlinear dependence associated with a large number of distributed PV sources adds the complexity to construct an accurate probability model and negatively affects confidence levels and reliability, thereby resulting in a more challenging operation of the systems. Most probability models have many restrictions when constructing multiple PV sources with complex dependence. This paper proposes a versatile probability model of PV generation on the basis of pair copula construction. In order to tackle the computational burden required to construct pair copula in high-dimensional cases, a systematic simplification technique is utilized that can significantly reduce the computational effort while preserving satisfactory precision. The proposed method can simplify the modeling procedure and provide a flexible and optimal probability model for the PV generation with complex dependence. The proposed model is tested using a set of historical data from colocated PV sites. It is then applied to the probabilistic load flow (PLF) study of the IEEE 118-bus system. The results demonstrate the effectiveness and accuracy of the proposed model.},
  comment  = {Models load flows with PV models driven by copulas.

See also: Munkhammar17solarIrradSpatCpla},
  doi      = {10.1109/TSTE.2015.2434934},
  file     = {Wu15pvGenPairCpla.pdf:Wu15pvGenPairCpla.pdf:PDF},
  keywords = {load flow;photovoltaic power systems;probability;PLF study;complex dependence;confidence levels;distributed PV sources;nonlinear dependence;optimal probability model;pair copula construction;photovoltaic generation;power systems;probabilistic load flow study;probability models;systematic simplification technique;versatile probability model;Computational modeling;Load flow;Load modeling;Photovoltaic systems;Power system modeling;Probabilistic logic;Multiple dependence;pair copula;photovoltaic (PV) generation;probabilistic load flow (PLF);simplification},
}

@Article{Kavlak18causeCostRedPV,
  author   = {Goksin Kavlak and James McNerney and Jessika E. Trancik},
  title    = {Evaluating the causes of cost reduction in photovoltaic modules},
  journal  = {Energy Policy},
  year     = {2018},
  volume   = {123},
  pages    = {700 - 710},
  issn     = {0301-4215},
  abstract = {Photovoltaic (PV) module costs have declined rapidly over forty years but the reasons remain elusive. Here we advance a conceptual framework and quantitative method for quantifying the causes of cost changes in a technology, and apply it to PV modules. Our method begins with a cost model that breaks down cost into variables that changed over time. Cost change equations are then derived to quantify each variable's contribution. We distinguish between changes observed in variables of the cost model – which we term low-level mechanisms of cost reduction – and research and development, learning-by-doing, and scale economies, which we refer to as high-level mechanisms. We find that increased module efficiency was the leading low-level cause of cost reduction in 1980–2012, contributing almost 25% of the decline. Government-funded and private R&D was the most important high-level mechanism over this period. After 2001, however, scale economies became a more significant cause of cost reduction, approaching R&D in importance. Policies that stimulate market growth have played a key role in enabling PV's cost reduction, through privately-funded R&D and scale economies, and to a lesser extent learning-by-doing. The method presented here can be adapted to retrospectively or prospectively study many technologies, and performance metrics besides cost.},
  doi      = {https://doi.org/10.1016/j.enpol.2018.08.015},
  file     = {:Kavlak18causeCostRedPV.pdf:PDF},
  keywords = {Photovoltaics (PV), Solar energy, PV modules, Cost model, Technological change},
  url      = {http://www.sciencedirect.com/science/article/pii/S0301421518305196},
}

@InProceedings{Vasak05ClustPcwsAffineThrot,
  author    = {Vasak, M. and Mladenovic, L. and Peric, N.},
  title     = {Clustering-based identification of a piecewise affine electronic throttle model},
  booktitle = {Industrial Electronics Society, 2005. IECON 2005. 31\textsuperscript{st} Annual Conference of IEEE},
  year      = {2005},
  abstract  = {Piece wise affine (PWA) model comprises several affine dynamics defined over polyhedral regions in the regressor (state+input) space. Identification of a PWA model is very often a starting point for the controller synthesis of hybrid systems. In this paper we extend the clustering-based procedure for identification of a piece wise autoregressive exogenous (PWARX) model proposed in [Ferrari-Trecate et al., 2003]. By exploiting a priori process knowledge we choose an appropriate linear transformation of the regression vector for a better and more efficient identification of the process nonlinearities. We significantly reduce the computational complexity of the classification algorithm for finding the complete polyhedral partition of the model domain. This modified clustering-based procedure is used to identify a PWARX model of the electronic throttle-a highly nonlinear component that regulates air inflow to the engine of a car.},
  comment   = {For picking local linear regression regions, maybe regime clustering?},
  doi       = {10.1109/IECON.2005.1568900},
  file      = {Vasak05ClustPcwsAffineThrot.pdf:Vasak05ClustPcwsAffineThrot.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.03.31},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1568900},
}

@Article{Bertail99subsampBSunkCnvrg,
  author              = {Bertail, Patrice and Politis, Dimitris N. and Romano, Joseph P.},
  title               = {On Subsampling Estimators with Unknown Rate of Convergence},
  journal             = {Journal of the American Statistical Association},
  year                = {1999},
  volume              = {94},
  number              = {446},
  pages               = {569--579},
  issn                = {0162-1459},
  abstract            = {Politis and Romano have put forth a general subsampling methodology for the construction of large-sample confidence regions for a general unknown parameter ? associated with the probability distribution generating the stationary sequence X1, ..., Xn. The subsampling methodology hinges on approximating the large-sample distribution of a statistic Tn = Tn (X1,..., Xn) that is consistent for ? at some known rate ?n. Although subsampling has been shown to yield confidence regions for ? of asymptotically correct coverage under very weak assumptions, the applicability of the methodology as it has been presented so far is limited if the rate of convergence ?n happens to be unknown or intractable in a particular setting. In this article we show how it is possible to circumvent this limitation by (a) using the subsampling methodology to derive a consistent estimator of the rate ?n, and (b) using the estimated rate to construct asymptotically correct confidence regions for ? based on subsampling.},
  comment             = {Subsampling bootstrap when convergence rate is unknown. Also talks about multivariate confidence intervals.
* Probably same method as in Politis99subsampBook .
* Geyer06subSampBootStrap is a good overview.},
  file                = {Bertail99subsampBSunkCnvrg.pdf:Bertail99subsampBSunkCnvrg.pdf:PDF},
  groups              = {Read},
  jstor_formatteddate = {Jun., 1999},
  language            = {English},
  owner               = {sotterson},
  publisher           = {American Statistical Association},
  timestamp           = {2011.11.22},
  url                 = {http://www.jstor.org/stable/2670177},
}

@Article{McDowell04polyDistLag,
  author    = {Allen McDowell},
  title     = {From the help desk: Polynomial distributed lag models},
  journal   = {The Stata Journal},
  year      = {2004},
  volume    = {4},
  number    = {2},
  pages     = {180189},
  abstract  = {Polynomial distributed lag models (PDLs) are finite-order distributed lag models with the impulse?response function constrained to lie on a polynomial of known degree. You can estimate the parameters of a PDL directly via constrained ordinary least squares, or you can derive a reduced form of the model via a linear transformation of the structural model, estimate the reduced-form parameters, and recover estimates of the structural parameters via an inverse linear transformation of the reduced-form parameter estimates. This article demonstrates both methods using Stata. Keywords: st0065, polynomial distributed lag, Almon, Lagrangian interpolation polynomials},
  comment   = {Polynomial distributed lag: a way to get long regression lag w/o autoregression and w/ fewer parameters This paper has a better explanation than others I've seen.

Brute force handling of lags in linear regression: copy lagged variables into regression input
* For long lags, p, this can cause severe multicollinearity problems
* also a lot of parameter ==> bad estimates
* note: this is what dynamic PCA does

Solution:
* require lagged coeff weights to be a polynomial in lag
* num. of coeffs to estimate is just the order of the polynomial; not the maximum lag
* problem is linear, looks just like normal regression problem so lots of techniques should work!
* this is a way of getting a long w/o infinite impulse autogression
* trick by Almon (1965) reduces collinearity problem
* but neither idea constrains the answer to match lag 0,
 -- which in my case would be the most recent data.
 -- Seems like that would be important.
 -- but maybe it would also be a noise smoother??

Questions:
* doable for multivariate problems?
* doable with splines instead, and would splines be better?
* how to add constraint so matches 0\textsuperscript{th} lag?

Similar techniques covered in Bhattarai10polyDistLag},
  file      = {McDowell04polyDistLag.pdf:McDowell04polyDistLag.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  timestamp = {2010.08.04},
  url       = {http://www.stata-journal.com/sjpdf.html?articlenum=st0065},
}

@TechReport{Milligan08subHrRampBal,
  author      = {Milligan, M.R. and Kirby, B.},
  title       = {An Analysis of Sub-hourly Ramping Impacts of Wind Energy and Balancing Area Size},
  institution = {National Renewable Energy Laboratory},
  year        = {2008},
  number      = {NREL/CP50043434.},
  abstract    = {Pooling loads and resources into a larger balancing area (BA) holds the promise of allowing additional wind to be integrated into the system at lower cost. There are a number of ways that this type of pooling could occur, including consolidation of BAs, or various cooperative approaches. In prior work, we analyzed the impact of combined BA operations on ramping requirements, based on hourly data. We showed that ramping constraints can cause a spike in costs that would not be reflective of the energy cost. We also showed that sub-hourly energy markets can provide strong economic signals to generators on the margin that can provide ramp response with little or no cost. In this paper, we analyze sub-hourly ramping requirements and the benefit of combining BA operations with significant wind penetrations. Our analysis at the sub-hourly level indicates that there can be significant increases in the ramp requirements compared to hourly, and yet these can be better managed by either a fast energy market or by a combined approach to operations. Our data from four BAs show that 5-minute combined load and wind ramp in excess is about 700 MW and can be avoided altogether by a combined approach to BA operations. We analyze high-quality wind power data from a mesoscale numerical weather prediction model, along with synchronized load data. We compare the sub-hourly and hourly ramp savings, and show why integration costs are lower when BAs can manage wind cooperatively, as opposed to separately.},
  comment     = {Combined load and wind producing across larger area requires less fast ramping reserves. * intereresting point: having to reserve fast ramping power costs extra. * wind increases the amount of fast ramping reserve that's needed * w/ more wind, baseload sources won't be able to produce the MINIMUM power (for some reason) * the extra ramping is independent of forecast error. It's bigger w/ perfect forecasts too. * note that the ramps are in the difference between load and generation -- not directly in generation * looking at sub-hourly times (5 min intervals); had a 2007 paper that dealt w/ hourly (similar conclusions, I think?) * combine w/ Milligan2008_WE ? * also see pdf of poster Relevance to my and Pierre's potential ramping forecast paper: * talks about cost or wind ramps: if fast, the cost 9X more than baseload -- baseload costs $10/MWh -- peaker backup costs $90/MWh Idea: ramp forecasts are only about derivatives. If ramps are binary, within some area, then must reserve ramping capacity of X, for any area that a forecast has declared to have that ramp. Then, can compare the cost of reserving this capacity with a ramp forecast to the amount reserved w/ a prob. forecast.},
  file        = {Tech Report:Milligan08subHrRampBal.pdf:PDF;Poster:Milligan08subHrRampBal_Poster.pdf:PDF},
  groups      = {Read},
  owner       = {scot},
  publisher   = {National Renewable Energy Laboratory},
  timestamp   = {2011.05.05},
  url         = {http://www.nrel.gov/docs/fy08osti/43084.pdf},
}

@InProceedings{GroweKuska03scenarioRedTree,
  author    = {Gr{\"o}we-Kuska, N. and Heitsch, H. and R{\"o}misch, W.},
  title     = {Scenario reduction and scenario tree construction for power management problems},
  booktitle = {Power Technology, IEEE},
  year      = {2003},
  volume    = {3},
  pages     = {7},
  abstract  = {Portfolio and risk management problems of power utilities may be modeled by multistage stochastic programs. These models use a set of scenarios and corresponding probabilities to model the multivariate random data process (electrical load, stream flows to hydro units, and fuel and electricity prices). For most practical problems the optimization problem that contains all possible scenarios is too large. Due to computational complexity and to time limitations this program is often approximated by a model involving a (much) smaller number of scenarios. The proposed reduction algorithms determine a subset of the initial scenario set and assign new probabilities to the preserved scenarios. The scenario tree construction algorithms successively reduce the number of nodes of a fan of individual scenarios by modifying the tree structure and by bundling similar scenarios. Numerical experience is reported for constructing scenario trees for the load and spot market prices entering a stochastic portfolio management model of a German utility.},
  comment   = {Very highly cited, has nice graphical example in the slides spinning reserves. slides from: http://www.gams.com/presentations/present_IEEE03.pdf

So far, I've only marked up some of the slides.},
  doi       = {10.1109/PTC.2003.1304379},
  file      = {Paper:Growe-Kuska03scenarioRedTree.pdf:PDF;slides:Growe-Kuska03scenarioRedTree_slides.pdf:PDF},
  groups    = {Ensemble, PointDerived, Use, doReadNonWPV_2},
  keywords  = {computational complexity, optimisation, power markets, risk management, stochastic processes, trees (mathematics), computational complexity, multistage stochastic programs, multivariate random data process, optimization problem, power management problems, power utilities, risk management problems, scenario reduction, stochastic portfolio management model, tree construction},
  owner     = {scot},
  timestamp = {2010.11.24},
}

@Article{Sohoni16critRevWindPowCrvModel,
  author    = {Sohoni, Vaishali and Gupta, SC and Nema, RK},
  title     = {A Critical Review on Wind Turbine Power Curve Modelling Techniques and Their Applications in Wind Based Energy Systems},
  journal   = {Journal of Energy},
  year      = {2016},
  volume    = {2016},
  abstract  = {Power curve of a wind turbine depicts the relationship between output power and hub height wind speed and is an important
characteristic of the turbine. Power curve aids in energy assessment, warranty formulations, and performance monitoring of the
turbines. With the growth of wind industry, turbines are being installed in diverse climatic conditions, onshore and offshore,
and in complex terrains causing significant departure of these curves from the warranted values. Accurate models of power
curves can play an important role in improving the performance of wind energy based systems. This paper presents a detailed
review of different approaches for modelling of the wind turbine power curve. The methodology of modelling depends upon
the purpose of modelling, availability of data, and the desired accuracy. The objectives of modelling, various issues involved
therein, and the standard procedure for power performance measurement with its limitations have therefore been discussed here.
Modelling methods described here use data from manufacturers? specifications and actual data from the wind farms. Classification
of modelling methods, various modelling techniques available in the literature, model evaluation criteria, and application of soft
computing methods for modelling are then reviewed in detail. The drawbacks of the existing methods and future scope of research
are also identified.},
  comment   = {Nice graph showing all the ways of modeling a power curve.  Use for colombia course.},
  file      = {Sohoni16critRevWindPowCrvModel.pdf:Sohoni16critRevWindPowCrvModel.pdf:PDF},
  owner     = {sotterson},
  publisher = {Hindawi Publishing Corporation},
  timestamp = {2017.04.29},
  url       = {https://www.hindawi.com/journals/jen/2016/8519785/},
}

@Article{Gill12powCurvCopula,
  author    = {Gill, S. and Stephen, B. and Galloway, S.},
  title     = {Wind Turbine Condition Assessment Through Power Curve Copula Modeling},
  journal   = {Sustainable Energy, IEEE Transactions on},
  year      = {2012},
  volume    = {3},
  number    = {1},
  pages     = {94--101},
  month     = jan,
  issn      = {1949-3029},
  abstract  = {Power curves constructed from wind speed and active power output measurements provide an established method of analyzing wind turbine performance. In this paper, it is proposed that operational data from wind turbines are used to estimate bivariate probability distribution functions representing the power curve of existing turbines so that deviations from expected behavior can be detected. Owing to the complex form of dependency between active power and wind speed, which no classical parameterized distribution can approximate, the application of empirical copulas is proposed; the statistical theory of copulas allows the distribution form of marginal distributions of wind speed and power to be expressed separately from information about the dependency between them. Copula analysis is discussed in terms of its likely usefulness in wind turbine condition monitoring, particularly in early recognition of incipient faults such as blade degradation, yaw, and pitch errors.},
  comment   = {Wind and turbine pwr are marginally rank-transformed (I think) and then their dependency is estimated with a 2D histogram.  This is a kind of empirical compula.  Then, wind/power data from a different period are transformed with the same rank curves (and I'm not sure what happens with the dependency struct).  Anyway, they develop some metrics for detecting a change in wind/power relationships.  The hope is that they'll detect worn/dirty blades (but I guess other instruments might already do this), and problems with pitch and yaw control. For that, they seem to think that they'll need a fancier copula.

The paper was simple yet I still don't completely understand it.
1.) what happened to the dependency transform when they transformed unseen data
2.) why the straight line?  Is this like a Q-Q plot?

Referenced by Park14windPowCurveMon},
  doi       = {10.1109/TSTE.2011.2167164},
  file      = {Gill12powCurvCopula.pdf:Gill12powCurvCopula.pdf:PDF},
  groups    = {Read},
  keywords  = {Condition monitoring;Estimation;Joints;Power measurement;Wind speed;Wind turbines;condition monitoring;curve fitting;power generation faults;power generation reliability;power measurement;statistical distributions;wind power;wind turbines;active power measurements;condition monitoring;copula modeling;faults recognition;marginal distributions;power curve;probability distribution functions;statistical theory;wind speed;wind turbine;Energy conversion;power generation reliability;wind power generation;},
  owner     = {sotterson},
  timestamp = {2013.02.21},
}

@Article{Chen11Online24hSolFrcstWeathTyp,
  author    = {Chen, Changsong and Duan, Shanxu and Cai, Tao and Liu, Bangyin},
  title     = {Online 24-h solar power forecasting based on weather type classification using artificial neural network},
  journal   = {Solar Energy},
  year      = {2011},
  volume    = {85},
  number    = {11},
  pages     = {2856--2870},
  abstract  = {Power forecasting is an important factor for planning the operations of photovoltaic (PV) system. This paper presents an advanced
statistical method for solar power forecasting based on artificial intelligence techniques. The method requires as input past power measurements
and meteorological forecasts of solar irradiance, relative humidity and temperature at the site of the photovoltaic power system.
A self-organized map (SOM) is trained to classify the local weather type of 24 h ahead provided by the online meteorological
services. A unique feature of the method is that following a preliminary weather type classification, the neural networks can be well
trained to improve the forecast accuracy. The proposed method is suitable for operational planning of transmission system operator,
i.e. forecasting horizon of 24 h ahead and for PV power system operators trading in electricity markets. Application of the forecasting
method on the power production of an actual PV power system shows the validity of the method.

Keywords: Power forecasting; Solar power; Neural network; Weather type; Photovoltaic power system},
  comment   = {Unsupervised SOM weather categories are fed into a day-ahead PV forecast system. NN regression, which is adaptive.},
  file      = {Chen11Online24hSolFrcstWeathTyp.pdf:Chen11Online24hSolFrcstWeathTyp.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2015.02.19},
  url       = {http://www.sciencedirect.com/science/article/pii/S0038092X11003008},
}

@InProceedings{Gensler16autoEncLSTMsolFrcst,
  author    = {A. Gensler and J. Henze and B. Sick and N. Raabe},
  title     = {Deep Learning for solar power forecasting --- An approach using AutoEncoder and LSTM Neural Networks},
  booktitle = {Proc. and Cybernetics (SMC) 2016 IEEE Int. Conf. Systems, Man},
  year      = {2016},
  pages     = {002858--002865},
  month     = oct,
  abstract  = {Power forecasting of renewable energy power plants is a very active research field, as reliable information about the future power generation allow for a safe operation of the power grid and helps to minimize the operational costs of these energy sources. Deep Learning algorithms have shown to be very powerful in forecasting tasks, such as economic time series or speech recognition. Up to now, Deep Learning algorithms have only been applied sparsely for forecasting renewable energy power plants. By using different Deep Learning and Artificial Neural Network algorithms, such as Deep Belief Networks, AutoEncoder, and LSTM, we introduce these powerful algorithms in the field of renewable energy power forecasting. In our experiments, we used combinations of these algorithms to show their forecast strength compared to a standard MLP and a physical forecasting model in the forecasting the energy output of 21 solar power plants. Our results using Deep Learning algorithms show a superior forecasting performance compared to Artificial Neural Networks as well as other reference models such as physical models.},
  doi       = {10.1109/SMC.2016.7844673},
  file      = {Gensler16autoEncLSTMsolFrcst.pdf:Gensler16autoEncLSTMsolFrcst.pdf:PDF},
  keywords  = {belief networks, electric power generation, learning (artificial intelligence), load forecasting, neural nets, power engineering computing, power grids, solar power stations, AutoEncoder, LSTM neural networks, artificial neural network, deep belief networks, deep learning, power generation, power grid, renewable energy power forecasting, renewable energy power plants, solar power forecasting, Forecasting, Machine learning, Neural networks, Neurons, Power generation, Predictive models, Weather forecasting, Forecasting, Multi-layer neural network, Power system analysis computing, Recurrent neural networks, Solar Energy},
  owner     = {sotterson},
  timestamp = {2017.03.08},
}

@Article{Gowdar16reasonWindTurbFail,
  author    = {Gowdar, Rajakumar D and Gowda, MC Mallikarjune},
  title     = {Reasons for wind turbine generator failures: a multi-criteria approach for sustainable power production},
  journal   = {Renewables: Wind, Water, and Solar},
  year      = {2016},
  volume    = {3},
  number    = {1},
  pages     = {1},
  abstract  = {Power generation quantity from wind sector is increasing at much faster rate day by day in the scenario of power systems, which obviously needs reliable operation. Therefore, accurate monitoring and error diagnosis are almost mandatory. This paper aims to identify important errors that affect the performance and can easily detect the faults of wind turbine generators (WTGs). Wind turbines are subjected to different sort of failures; thus, before starting to identify various kinds of errors, it is necessary to identify what kind of failures can be found in the real world which causes healthy operation of WTGs. Out of different errors, error that is caused by the operation of gearbox could stop or reduce the generation of power from WTGs for a long time. Recently, several condition monitoring and fault diagnosis techniques have been introduced in order to minimize downtime and maintenance cost while increasing energy availability and life time service of the wind farms. Different types of sensors have been used for long time in wind turbine fault diagnosis or monitoring systems to collect data of the generator health. Many researchers analyzed wind turbine failures using different software. The present study uses different approaches and prepares a multi-criteria decision-making framework using analytic hierarchy process (AHP). The analysis of the data under AHP frame work revealed overspeed guard/turbine out of control error got the top most impediment to the healthy operation of WTGs, and high brake temperature fits in the fifth position among the five different error groups considered.

Keywords

Sustainable power production Wind turbine generator Multi-criteria approach Analytic hierarchy process},
  comment   = {A good(?) overview of what goes wrong in wind turbines.},
  file      = {Gowdar16reasonWindTurbFail.pdf:Gowdar16reasonWindTurbFail.pdf:PDF},
  owner     = {sotterson},
  publisher = {Springer Singapore},
  timestamp = {2017.01.16},
  url       = {https://jrenewables.springeropen.com/articles/10.1186/s40807-016-0029-1},
}

@Article{Suchwalko16loadFrcst3deepLrnBlog,
  author    = {Artur Suchwalko},
  title     = {Case study: Electric power load forecasting ? a comparison of three approaches},
  journal   = {Quantup Blog},
  year      = {2016},
  month     = sep,
  abstract  = {Power load forecasting remains one of the fundamental business problems faced by the electricity sector. In particular, accurate
forecasts are needed when setting up contracts between providers and consumers of electricity. If the delivered power is lower than
the demand, the provider makes a loss (e.g. pays a penalty). On the other hand, when delivered power is higher than the actual
demand, the provider may bear additional costs too (e.g. costs concerning selling energy on the SPOT / balancing market). Hence,
the more accurate energy load forecasts, the lower costs.
Certainly, there are many kinds of energy contracts. Some of them require paying fees only when the delivered power falls below the
contracted value by a certain amount. Other contracts transfer all the risk to the clients, and oblige them to buy additional energy they
need on the balancing or SPOT market. Different types of contracts undoubtedly require different forecast evaluation schemes.
Choosing appropriate forecasting models that take into account specificity of electricity demand is not an easy task. Firstly, demand
patterns can differ markedly. While some of them exhibit strong and usually complex seasonal behaviour, others may be quite
dissimilar (e.g. without significant seasonal fluctuations). Secondly, different clients may use different time intervals to measure
energy demand (in particular, not all clients have hourly registered demand). Moreover, there is a number of exogenous factors (e.g.
factors related to weather conditions etc.) that may significantly influence the actual energy consumption.},
  comment   = {Forecasts Polish electricity demand with  regression, time series models, and deep neural networks (which  do a probablistic forecast too).  Deep learning seems best but the writeup is confused about input features and forecast horizon so it's hard to tell what these results mean.  Still,it's interesting to see the probabilistic forecast in the middle (could figure out how to make it output real quantiles instead of interval indicators).

Data
*  temperature, cloud coverage and wind speed (components of the chill factor) from 82 stations
* turns into 112 inputs for deep learning
* 13 years of test/train data!

Figure of Merit
* MAPE, not RMSE
* nothing for the probabilistic part of the forecast.

Deep learning
* four layers: 256, 256, 65, 16 neurons
* 112 inputs
* 126K parameters!
* tanh and softmax activations (isn't tanh bad?  See Karpathy16deepReinfLrnPongPix)
* stochastic gradient training w/ momentum (Nesterov), 1K epochs.
* horizon said to be hour-ahead
* does not say that there's an autoregressive power input, while others have it?

One of the middle layers does a kind of probabilistic forecast:  says it's not quantiles but a binary value indicating if power fell into a bin interval.  Not clear but it seems like thresholds must have been fixed and since it's softmax, it really MUST have been a quantile.  Somewhere, this is summed to compute expected power.

But if it was really an interval then it would be easy to get expected value by addition, which is what they say they do, I think  (last layer has "fixed weights").

RESULTS
* deep learning is a bit better but at up to 360 day horizons??
* since deep learning was supposed to be hour-ahead, I don't know what this means},
  file      = {:Suchwalko16loadFrcst3deepLrnBlog.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.02.01},
  url       = {http://quantup.eu/2016/09/02/bez-kategorii-en/case-study-electric-power-load-forecasting-a-comparison-of-three-approaches/},
}

@InProceedings{Gagne12machnLrnEnhanceEns,
  author    = {Gagne, D.J. and McGovern, A. and Ming Xue},
  title     = {Machine learning enhancement of Storm Scale Ensemble precipitation forecasts},
  booktitle = {Intelligent Data Understanding (CIDU), 2012 Conference on},
  year      = {2012},
  pages     = {39--46},
  month     = oct,
  abstract  = {Precipitation forecasts provide both a crucial service for the general populace and a challenging forecasting problem due to the complex, multi-scale interactions required for precipitation formation. The Center for the Analysis and Prediction of Storms (CAPS) Storm Scale Ensemble Forecast (SSEF) system is a promising method of providing high-resolution forecasts of the intensity and uncertainty in precipitation forecasts. The SSEF incorporates multiple models with varied parameterization scheme combinations and produces forecasts every 4 km over the continental US. The SSEF precipitation forecasts exhibit significant negative biases and placement errors. In order to correct these issues, multiple machine learning algorithms have been applied to the SSEF precipitation forecasts to correct the forecasts using the NSSL National Mosaic and Multisensor QPE (NMQ) grid as verification. The 2010 SSEF was used for training. Two levels of post-processing are performed. In the first, probabilities of any precipitation are determined and used to find optimal thresholds for the precipitation areas. Then, three types of forecasts are produced in those areas. First, the probability of the 1-hour accumulated precipitation exceeding a threshold is predicted with random forests, logistic regression, and multivariate adaptive regression splines (MARS). Second, deterministic forecasts based on a correction from the ensemble mean are made with linear regression, random forests, and MARS. Third, fixed probability interval forecasts are made with quantile regressions and quantile regression forests. Models are generated from points sampled from the western, central, and eastern sections of the domain. Verification statistics and case study results show improvements in the reliability and skill of the forecasts compared to the original ensemble while controlling for the over-prediction of the precipitation areas and without sacrificing smaller scale details from the model runs.},
  comment   = {Machine learning generates new ensemble members from existing met model ensemble. Result is a better probabilistic forecast. Mabye interesting for adaboost idea.},
  doi       = {10.1109/CIDU.2012.6382199},
  file      = {Gagne12machnLrnEnhanceEns.pdf:Gagne12machnLrnEnhanceEns.pdf:PDF},
  groups    = {Ensemble, PointDerived, doReadNonWPV_1},
  keywords  = {geophysics computing;lead compounds;learning (artificial intelligence);probability;regression analysis;splines (mathematics);trees (mathematics);weather forecasting;CAPS SSEF system;Center for the Analysis and Prediction of Storms;MARS;NMQ grid;NSSL National Mosaic and Multisensor QPE grid;Storm Scale Ensemble Forecast system;complex multiscale interaction;continental US;deterministic forecast;ensemble mean;fixed probability interval forecast;forecasting problem;high-resolution forecast;linear regression;logistic regression;machine learning algorithm;machine learning enhancement;multivariate adaptive regression splines;negative bias;parameterization scheme;placement error;precipitation area optimal threshold;precipitation formation;precipitation over-prediction;quantile regression forest;random forest;storm scale ensemble precipitation forecast;verification statistics;Logistics;Machine learning algorithms;Mars;Predictive models;Probability;Rain;Storms},
  owner     = {sotterson},
  timestamp = {2014.01.29},
}

@InProceedings{Sreelakshmi08neuralWindPred,
  author    = {K. Sreelakshmi and P. Ramakanthkumar},
  title     = {Neural Networks for Short Term Wind Speed Prediction},
  booktitle = {Proceedings of World Academy of Science, Engineering and Technology},
  year      = {2008},
  volume    = {32},
  pages     = {863--867},
  abstract  = {Predicting short term wind speed is essential in order to prevent systems in-action from the effects of strong winds. It also helps in using wind energy as an alternative source of energy, mainly for Electrical power generation. Wind speed prediction has applications in Military and civilian fields for air traffic control, rocket launch, ship navigation etc. The wind speed in near future depends on the values of other meteorological variables, such as atmospheric pressure, moisture content, humidity, rainfall etc. The values of these parameters are obtained from a nearest weather station and are used to train various forms of neural networks. The trained model of neural networks is validated using a similar set of data. The model is then used to predict the wind speed, using the same meteorological information. This paper reports an Artificial Neural Network model for short term wind speed prediction, which uses back propagation algorithm},
  comment   = {NN's w/ NWP type feature selection},
  file      = {Sreelakshmi08neuralWindPred.pdf:Sreelakshmi08neuralWindPred.pdf:PDF;Sreelakshmi08neuralWindPred.pdf:Sreelakshmi08neuralWindPred.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.02.12},
  url       = {http://www.waset.org/pwaset/v32.html},
}

@Article{Friederichs12extmValPkWindFrcst,
  author    = {Friederichs, Petra and Thorarinsdottir, Thordis L},
  title     = {Forecast verification for extreme value distributions with an application to probabilistic peak wind prediction},
  journal   = {Environmetrics},
  year      = {2012},
  volume    = {23},
  number    = {7},
  pages     = {579--594},
  abstract  = {Predictions of the uncertainty associated with extreme events are a vital component of any prediction system for such events. Consequently, the prediction system ought to be probabilistic in nature, with the predictions taking the form of probability distributions. This paper concerns probabilistic prediction systems where the data are assumed to follow either a generalized extreme value (GEV) distribution or a generalized Pareto distribution. In this setting, the properties of proper scoring rules that facilitate the assessment of the prediction uncertainty are investigated, and closed form expressions for the continuous ranked probability score (CRPS) are provided. In an application to peak wind prediction, the predictive performance of a GEV model under maximum likelihood estimation, optimum score estimation with the CRPS, and a Bayesian framework are compared. The Bayesian inference yields the highest overall prediction skill and is shown to be a valuable tool for covariate selection, while the predictions obtained under optimum CRPS estimation are the sharpest and give the best performance for high thresholds and quantiles.

Keywords:
Bayesian variable selection;continuous ranked probability score;extreme events;optimum score estimation;prediction uncertainty;wind gusts},
  comment   = {GEV for max. wind speed. Might be good if can modify to include a wind speed or photovoltaic power curve.

I have the arXiv paper.},
  doi       = {10.1002/env.2176/abstract},
  file      = {arXiv paper:Friederichs12extmValPkWindFrcst.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2015.04.17},
}

@TechReport{Pinson07frcstEval,
  author      = {P. Pinson and J. K. M{\o}ller and H. A. Nielsen and H. Madsen and G. N. Kariniotakis},
  title       = {Evaluation of Nonparametric Probabilistic Forecasts of Wind Power},
  institution = {Informatics and Mathematical Modelling, Technical University of Denmark, {DTU}},
  year        = {2007},
  abstract    = {Predictions of wind power production for horizons up to 48-72 hour ahead comprise a highly valuable input to the methods for the daily management or trading of wind generation. Today, users of wind power predictions are not only provided with point predictions, which are estimates of the most likely outcome for each look-ahead time, but also with uncertainty estimates given by probabilistic forecasts. In order to avoid assumptions on the shape of predictive distributions, these probabilistic predictions are produced from nonparametric methods, and then take the form of a single or a set of quantile forecasts. The required and desirable properties of such probabilistic forecasts are defined and a framework for their evaluation is proposed. This framework is applied for evaluating the quality of two statistical methods producing full predictive distributions from point predictions of wind power. These distributions are defined by 18 quantile forecasts with nominal proportions spanning the unit interval. The relevance and interest of the introduced evaluation framework are consequently discussed.},
  comment     = {General purpose metrics for probabilistic forecasts; quantile regression a bit better than adapt. samp? Generally, the metrics are:
* sharpness
* reliability
* resolution
But there are a lot of variants in this paper I kind of skimmed it quickly and didn't read much at the end.},
  file        = {Pinson07frcstEval.pdf:Pinson07frcstEval.pdf:PDF},
  groups      = {Test, doReadWPV_2},
  keywords    = {wind power, uncertainty, probabilistic forecasting, quantile forecasts, quality evaluation, reliability, sharpness, resolution, skill},
  location    = {Richard Petersens Plads, Building 321, {DK-}2800 Kgs. Lyngby},
  owner       = {scot},
  series      = {IMM-Technical Report-2007-02},
  timestamp   = {2011.04.12},
  url         = {http://www2.imm.dtu.dk/pubdb/p.php?5024},
}

@InProceedings{Ling03aucVsAccRank,
  author        = {Ling, Charles X and Huang, Jin and Zhang, Harry and others},
  title         = {AUC: a statistically consistent and more discriminating measure than accuracy},
  booktitle     = {Ijcai},
  year          = {2003},
  volume        = {3},
  pages         = {519--524},
  __markedentry = {[Scott:1]},
  abstract      = {Predictive accuracy has been used as the main and
often only evaluation criterion for the predictive
performance of classification learning algorithms.
In recent years, the area under the ROC (Receiver
Operating Characteristics) curve, or simply AUC,
has been proposed as an alternative single-number
measure for evaluating learning algorithms. In this
paper, we prove that AUC is a better measure than
accuracy. More specifically, we present rigourous
definitions on consistency and discriminancy in
comparing two evaluation measures for learning al-
gorithms. We then present empirical evaluations
and a formal proof to establish that AUC is indeed
statistically consistent and more discriminating than
accuracy. Our result is quite significant since we
formally prove that, for the first time, AUC is a bet-
ter measure than accuracy in the evaluation of learn-
ing algorithms.},
  comment       = {Classifier AUC is generally more informative than accuracy (proportion of correctly predicted outcomes, I think) about the value of probabilities coming from binary classifiers -- in fact classifies trained to predict AUC can produce not only better AUC than accuracy-tuned classifers, but better accuracy as well.  

This is because AUC assesses the accuracy of the rankings of classifier probabilities (see the very simple eq. 1, which will make computing AUC really easy).  However, a larger AUC doesn't always imply a lower errorrate (Table 3).

However, they have some way I didn't read explaining how AUC is more consistent and discriminating than accuracy.},
  file          = {:Ling03aucVsAccRank.pdf:PDF},
  url           = {https://cling.csd.uwo.ca/papers/ijcai03.pdf},
}

@Electronic{Microsoft16predMaintPlaybook,
  author    = {Microsoft},
  month     = dec,
  year      = {2016},
  title     = {Cortana Intelligence Solution Template Playbook for predictive maintenance in aerospace and other businesses},
  url       = {https://docs.microsoft.com/en-us/azure/machine-learning/cortana-analytics-playbook-predictive-maintenance},
  abstract  = {Predictive maintenance is one of the most demanded applications of predictive analytics with unarguable benefits including tremendous amount of cost savings. This playbook aims at providing a reference for predictive maintenance solutions with the emphasis on major use cases. It is prepared to give the reader an understanding of the most common business scenarios of predictive maintenance, challenges of qualifying business problems for such solutions, data required to solve these business problems, predictive modeling techniques to build solutions using such data and best practices with sample solution architectures. It also describes the specifics of the predictive models developed such as feature engineering, model development and performance evaluation. In essence, this playbook brings together the business and analytical guidelines needed for a successful development and deployment of predictive maintenance solutions. These guidelines are prepared to help the audience create an initial solution using Cortana Intelligence Suite and specifically Azure Machine Learning as a starting point in their long-term predictive maintenance strategy. The documentation regarding Cortana Intelligence Suite and Azure Machine Learning can be found in Cortana Analytics and Azure Machine Learning pages.},
  comment   = {Maybe a quicker way to get an overview of the mechanics and requirements for predictive maintenance.},
  file      = {Microsoft16predMaintPlaybook.pdf:Microsoft16predMaintPlaybook.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.01.16},
}

@TechReport{Giebel07errAggregWindPow,
  author      = {Gregor Giebel and Poul Sorensen and Hannele Holttinen},
  title       = {Forecast error of aggregated wind power},
  institution = {Ris{\o} National Laboratory},
  year        = {2007},
  number      = {Ris{\o}I2567(EN)},
  abstract    = {Preface This report is written in fulfilment of Task 2.3 in the TradeWind project (EU sponsored, under the Intelligent Energy Europe initiative): Wind Power Integration and Exchange in the Trans-European Power Market. The Task description is as follows: Task 2.3: Forecast error of aggregated wind power Estimates of forecast error of aggregated production for time horizons of intraday and dayahead markets in future will be produced. This will be done by reference to published studies of forecasting for wind generation, and from internal knowledge of WP2 participants. Modelling of wind power fluctuations for aggregated wind generation capacity.},
  comment     = {Nice review of forecasting timescales, statistical dependence of errors across sites (so can see if adding will reduce errors) Figure 1: Statistical model errors are better than NWP model errors up to about 1-2 hours.},
  file        = {:Giebel07errAggregWindPow.pdf:PDF;Giebel07errAggregWindPow.pdf:Giebel07errAggregWindPow.pdf:PDF},
  owner       = {scotto},
  timestamp   = {2008.07.06},
  url         = {http://www.trade-wind.eu/index.php?id=13},
}

@Article{Greybush08ensRegime,
  author    = {Greybush, Steven J. and Haupt, Sue Ellen and Young, George S.},
  title     = {The Regime Dependence of Optimally Weighted Ensemble Model Consensus Forecasts of Surface Temperature},
  journal   = {Weather and Forecasting},
  year      = {2008},
  volume    = {23},
  number    = {6},
  pages     = {1146--1161},
  month     = dec,
  issn      = {0882-8156},
  abstract  = {Previous methods for creating consensus forecasts weight individual ensemble members based upon their
relative performance over the previous N days, implicitly making a short-term persistence assumption about
the underlying flow regime. A postprocessing scheme in which model performance is linked to underlying
weather regimes could improve the skill of deterministic ensemble model consensus forecasts. Here, principal
component analysis of several synoptic- and mesoscale fields from the North American Regional
Reanalysis dataset provides an objective means for characterizing atmospheric regimes. Clustering techniques,
including K-means and a genetic algorithm, are developed that use the resulting principal components
to distinguish among the weather regimes. This pilot study creates a weighted consensus from 48-h
surface temperature predictions produced by the University of Washington Mesoscale Ensemble, a variedmodel
(differing physics and parameterization schemes) multianalysis ensemble with eight members. Different
optimal weights are generated for each weather regime. A second regime-dependent consensus
technique uses linear regression to predict the relative performance of the ensemble members based upon
the principal components. Consensus forecasts obtained by the regime-dependent schemes are compared
using cross validation with traditional N-day ensemble consensus forecasts for four locations in the Pacific
Northwest, and show improvement over methods that rely on the short-term persistence assumption.},
  booktitle = {Weather and Forecasting},
  comment   = {doi: 10.1175/2008WAF2007078.1
Review:
Weight ensembles with via regime clustering instead of using a sliding time window.},
  doi       = {10.1175/2008WAF2007078.1},
  file      = {Greybush08ensRegime.pdf:Greybush08ensRegime.pdf:PDF},
  groups    = {Ensemble, doReadWPV_2},
  owner     = {sotterson},
  publisher = {American Meteorological Society},
  timestamp = {2013.10.02},
}

@Article{Tipping99MixProbPCA,
  author    = {Tipping, Michael E and Bishop, Christopher M},
  title     = {Mixtures of probabilistic principal component analyzers},
  journal   = {Neural computation},
  year      = {1999},
  volume    = {11},
  number    = {2},
  pages     = {443--482},
  abstract  = {Principal component analysis (PCA) is one of the most popular techniques for processing, compressing, and visualizing data, although its effectiveness is limited by its global linearity. While nonlinear variants of PCA have been proposed, an alternative paradigm is to capture data complexity by a combination of local linear PCA projections. However, conventional PCA does not correspond to a probability density, and so there is no unique way to combine PCA models. Therefore, previous attempts to formulate mixture models for PCA have been ad hoc to some extent. In this article, PCA is formulated within a maximum likelihood framework, based on a specific form of gaussian latent variable model. This leads to a well-defined mixture model for probabilistic principal component analyzers, whose parameters can be determined using an expectation-maximization algorithm. We discuss the advantages of this model in the context of clustering, density modeling, and local dimensionality reduction, and we demonstrate its application to image compression and handwritten digit recognition.},
  comment   = {Probabilistic PCA (good for high dim) and mixtures (can cluster different data orientations). Could use to find cluster points by which line they fit on. Has Matlab and R. I'm not sure how good this would be for regression clustering, though, because the dependent variable is just one of many dimensions.
 Also, what about the y intercept?

Matlab:
in netlab: http://www.mathworks.de/matlabcentral/fileexchange/2654-netlab
R:
in MetabolAnalyze package: http://cran.r-project.org/web/packages/MetabolAnalyze/index.html},
  doi       = {10.1162/089976699300016728},
  file      = {Tipping99MixProbPCA.pdf:Tipping99MixProbPCA.pdf:PDF},
  owner     = {sotterson},
  publisher = {MIT Press},
  timestamp = {2014.03.31},
}

@Article{Zou06SparsePCA,
  author    = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
  title     = {Sparse principal component analysis},
  journal   = {Journal of computational and graphical statistics},
  year      = {2006},
  volume    = {15},
  number    = {2},
  pages     = {265--286},
  abstract  = {Principal component analysis (PCA) is widely used in data processing and dimensionality
reduction. However, PCA suffers from the fact that each principal component is a linear
combination of all the original variables, thus it is often difficult to interpret the results.We
introduce a new method called sparse principal component analysis (SPCA) using the lasso
(elastic net) to produce modified principal components with sparse loadings.We first show
that PCA can be formulated as a regression-type optimization problem; sparse loadings are
then obtained by imposing the lasso (elastic net) constraint on the regression coefficients.
Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data
and gene expression arrays. We also give a new formula to compute the total variance of
modified principal components. As illustrations, SPCA is applied to real and simulated data
with encouraging results.
Key Words: Arrays; Gene expression; Lasso/elastic net; Multivariate analysis; Singular
value decomposition; Thresholding.},
  comment   = {Uses LASSO elastic net
Matlab
* in the DTU SpaSM Toolbox (Sjoestrand12spasmMatlab)
* download: http://www.imm.dtu.dk/projects/spasm/},
  doi       = {10.1198/106186006X113430},
  file      = {Zou06SparsePCA.pdf:Zou06SparsePCA.pdf:PDF},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2014.10.23},
}

@InProceedings{Bremnes04quantFrcstVerif,
  author       = {John Bj{\o}rnar Bremnes},
  title        = {Methods for verifying quantile forecasts},
  booktitle    = {International Verification Methods Workshop},
  year         = {2004},
  month        = sep,
  organization = {World Meteorological Organization},
  abstract     = {Probabilistic forecasts of continuous univariate variables, such as wind speed, are ideally fully specified probability distributions. The focus in this presentation is on the slightly simpler case when only a few quantiles are forecasted or available. Good quantile forecasts should possess certain properties, and verification approaches that quantify these are here described.

First, the reliability defined as the degree the fractions of observations below each quantile equal the quantile probabilities in the long run, is proposed assessed by using the chi-square hypothesis test for multinomial data. This test can be applied separately to each quantile or simultaneously to all. In addition, it is discussed how to examine whether reliability is independent of the quantile values. Second, the average length of forecast intervals formed by pairs of quantiles is suggested as a natural measure for sharpness although it is argued that it might be inadequate if multi-modal distributions are frequent. Third, the resolution or degree of variation in the forecasted quantiles (or the lengths of the forecast intervals), can be quantified by statistics such as the standard deviation or simply the range. The presentation is ended by shortly discussing how to rank quantile forecasting models.},
  comment      = {Binomial or Chi squared test of quantile (interval) reliability. Presentation slides with a little more explanation of reliability test used in Bremnes04windLocQR

Explains both per-quantile and all-quantile tests

Per-quantile test (can reject individual quantiles)
* binomial test (preferred)
 - Prob match of counts above and below estimated quantile, I think
* chi-squared test

All-quantile test (chi-squared tests)
* per-quantile test for is true for ALL quantiles
* composite test of bin count match and theoretical count from forecasted quantile difference (preferred)
* Rule of thumb: 5 counts in each cell (how about for binomial?)},
  file         = {Bremnes04quantFrcstVerif.pdf:Bremnes04quantFrcstVerif.pdf:PDF},
  groups       = {Test, doReadWPV_1},
  location     = {Montreal, Quebec, Canada},
  owner        = {sotterson},
  timestamp    = {2014.04.01},
  url          = {http://www.cawcr.gov.au/projects/verification/Workshop2004/Abstracts.html#3.2},
}

@Misc{Pinson12newScenProd,
  author       = {Pierrre Pinson and the WP6 members},
  title        = {New probabilistic forecasting product: Scenarios},
  howpublished = {SafeWind web page for work package 6},
  month        = aug,
  year         = {2012},
  abstract     = {Probabilistic forecasts of renewable energy generation (especially wind power, since having a leading role in number of European countries) are becoming increasingly accepted as an optimal input to a number of decision making problems related to market participation and power systems operations. While from a practical point view they give a very visual information about forecast uncertainty, they also give a mathematically sou description of the range of possibilities for the coming future in terms of wind power generation. A large numb of methods based on already issued point forecasts, metrological ensemble forecasts, stochastic power curve etc., were developed and refined through the lifetime of the EU project SafeWind. Scenarios of short-term wind power generation: The various types of probabilistic forecasts that were issued and communicated operationally a few years a were focusing on providing a probabiistic information for every lead time and location, independently. This while a substantial share of decision makings problems additionally require an information about the spat and/or temporal correlation structure of forecast uncertainty. Example problems include (i) the optimal operati of a wind-storage system in a market environment, (ii) stochastic unit commitment over a control zone, ( optimal maintenant planning offshore, etc. In such cases, scenarios of short-term wind power generati comprise an ideal product since providing the whole information about forecast uncertainty for every lead tim and location, also accounting for spatio-temporal correlation. Methods for generating scenarios of short-term wind power generation: Varied approaches were proposed and investigated, based on time-series analysis methods, copula-bas methods relying on already issued probabilistic forecasts, or alternatively based on meteorological ensemb forecasts. In all cases these methods focused on modeling and mimicing the observed spatio-temporal structu of forecast uncertainty. They were applied and evaluated for a number of test cases in the project, located Spain, Denmark and France. In parallel, the framework for the assessment of the quality of such new foreca products was proposed and illustrated. Example application for the control area of Energinet.dk in Denmark: The control area of Western Denmark (also called DK-1) is operated by Energinet.dk the Transmission Syste Operator in Denmark and split into 15 control zones onshore. As an example application, point forecas probabilistic forecasts in the form of predictive densities, and spatio-temporal scenarios of wind power generati},
  comment      = {Says that scenarios implemented in safewind were "used for demonstratring the use of spatio-temporal scenarios as input to stochastic unit commitement for the whole Irish power system (operated by SONI and EirGrid) in the frame of the EU project Anemos.plus."},
  file         = {Pinson12newScenProd.pdf:Pinson12newScenProd.pdf:PDF},
  groups       = {Use, doReadWPV_2},
  owner        = {sotterson},
  timestamp    = {2013.10.11},
  url          = {http://www.safewind.eu/index.php?option=com_content&view=category&layout=blog&id=76&Itemid=127},
}

@Article{Iversen13solarFrcstSDE,
  author    = {Iversen, Emil B and Morales, Juan M and M{\o}ller, Jan K and Madsen, Henrik},
  title     = {Probabilistic Forecasts of Solar Irradiance by Stochastic Differential Equations},
  journal   = {arXiv preprint arXiv:1310.6904},
  year      = {2013},
  abstract  = {Probabilistic forecasts of renewable energy production provide users with
valuable information about the uncertainty associated with the expected generation.
Current state-of-the-art forecasts for solar irradiance have focused
on producing reliable point forecasts. The additional information included
in probabilistic forecasts may be paramount for decision makers to efficiently
make use of this uncertain and variable generation. In this paper, a stochastic
differential equation (SDE) framework for modeling the uncertainty associated
with the solar irradiance point forecast is proposed. This modeling
approach allows for characterizing both the interdependence structure of prediction
errors of short-term solar irradiance and their predictive distribution.
A series of different SDE models are fitted to a training set and subsequently
evaluated on a one-year test set. The final model proposed is defined on a
bounded and time-varying state space with zero probability almost surely of
events outside this space.
Keywords: Forecasting, Stochastic differential equations, Solar power,},
  comment   = {Stochastic Differential Equations for solar irradiance.

It's done for wind in: Moller13windFrcstSDE},
  file      = {Iversen13solarFrcstSDE.pdf:Iversen13solarFrcstSDE.pdf:PDF},
  groups    = {PointDerived, doReadWPV_1},
  owner     = {sotterson},
  timestamp = {2014.02.04},
  url       = {http://arxiv.org/abs/1310.6904},
}

@TechReport{Sloughter08probWindEnsBayesModAvg,
  author      = {J. McLean Sloughter and Tilmann Gneiting and Adrian E. Raftery},
  title       = {Probabilistic Wind Speed Forecasting using Ensembles and {Bayes}ian Model Averaging},
  institution = {Department of Statistics, University of Washington},
  year        = {2008},
  number      = {544},
  month       = oct,
  abstract    = {Probabilistic forecasts of wind speed are becoming critical as interest grows in wind as a clean and renewable source of energy, in addition to a wide range of other uses, from avia- tion to recreational boating. Statistical approaches to wind forecasting offer two particular challenges: the distribution of wind speeds is highly skewed, and wind observations are re- ported to the nearest whole knot, a much coarser discretization than is seen in other weather quantities. The prevailing paradigm in weather forecasting is to issue deterministic forecasts based on numerical weather prediction models. Uncertainty can then be assessed through ensemble forecasts, where multiple estimates of the current state of the atmosphere are used to generate a collection of deterministic predictions. Ensemble forecasts are often uncali- brated, however, and Bayesian model averaging (BMA) is a statistical way of postprocessing these forecast ensembles to create calibrated predictive probability density functions (PDFs). It represents the predictive PDF as a weighted average of PDFs centered on the individual bias-corrected forecasts, where the weights reflect the forecasts? relative contributions to pre- dictive skill over a training period. In this paper we extend BMA to provide probabilistic forecasts of wind speed, taking account of the skewness of the predictive distributions and the discreteness of the observations. The BMA method is applied to 48-hour ahead fore- casts of maximum wind speed over the North American Pacific Northwest in 2003 using the University of Washington mesoscale ensemble, and is shown to provide calibrated and sharp probabilistic forecasts. Comparisons are made between a number of formulations that account for the discretization of the observations.},
  comment     = {Not wind POWER but talks about BMA, etc. A lower priority for Eweline, probably.},
  file        = {Sloughter08probWindEnsBayesModAvg.pdf:Sloughter08probWindEnsBayesModAvg.pdf:PDF;Sloughter08probWindEnsBayesModAvg.pdf:Sloughter08probWindEnsBayesModAvg.pdf:PDF},
  groups      = {DOE-PNL09, Ensemble, doReadNonWPV_1},
  location    = {Seattle, WA, USA},
  ncite       = {64},
  owner       = {sotterson},
  timestamp   = {2009.03.03},
}

@InProceedings{Sack12windEns2ProbFrcst,
  author    = {Sack, J. and Bremen, L. and Kyrianzis, A. and Donelly, R.},
  title     = {From NWP Ensembles to Probabilistic Wind Energy Production Forecasts},
  booktitle = {EWEA},
  year      = {2012},
  abstract  = {Probabilistic wind power forecast has gained increasing importance in the recent years. The
advantages of probabilistic information, for the integration of wind power to the electrical network,
are discussed in numerous publications which mainly address advantages in energy trading and
electricity network regulation. The value of the probabilistic wind power forecast is based on the
spread of wind power forecast members originating either from a number of NWP models or ensem-
ble products of global models (such as ECMWF, UKMO, Meteo France, NCEP and others). Wind
power members provide an estimate of the expected uncertainty of power predictions. However,
calibration is needed for reliable probabilistic information that is consistent with observations. The
paper addresses the development and assesses the quality of the 100m ECMWF EPS (ensemble
prediction system winds) which were introduced by ECMWF beginning of 2010 in two operational
wind farms (onshore and onshore). In addition, the paper compares differerent calibration method-
ologies of Ensembles (in wind and in wind power mode) and addresses the implications for the
operational environment.
Results show that the probabilistic calibration methodologies examined in wind power mode success-
fully optimise the quantiles ranging from 10\% to 90\% with some outliers outside of these limits,
mainly due to limited amount of available observational data. The paper compares the different
approaches by means of resolution, sharpness, reliability and skill score. We address the training
window of the calibration methodologies in the operational forecast environment and show that sim-
ple models x yield equivalent accurate results in the operational forecast environment with training
periods of less than a month compared with more complex models with longer training periods.
Ensembles that have been calibrated in wind mode ((u,v) components at the same time) are still
well calibrated when transformed into wind power and show good improvements in probabilistic
skill compared to the raw ensemble. The benefit from calibration is less when verification is done
with observed wind power instead of using simulated wind power for Germany. The spread of the
calibrated ensemble is underestimated with respect to real observations mostly because},
  comment   = {NWP ensembles --> wind power quantiles},
  file      = {Sack12windEns2ProbFrcst.pdf:Sack12windEns2ProbFrcst.pdf:PDF},
  groups    = {Ensemble, CitaviImport1, doReadWPV_2},
  keywords  = {Calibration},
  ncite     = {0},
  owner     = {sotterson},
  timestamp = {2013.09.26},
}

@Article{Lenzi16spatTempWndProbFrcst,
  author       = {Lenzi, Amanda and Pinson, Pierre and Clemmensen, Line H. and Guillot, Gilles},
  title        = {Spatial models for probabilistic prediction of wind power with application to annual-average and high temporal resolution data},
  journal      = {Stochastic Environmental Research and Risk Assessment},
  year         = {2016},
  pages        = {1--17},
  abstract     = {Producing accurate spatial predictions for wind power generation together with a quantification of uncertainties is required to plan and design optimal networks of wind farms. Toward this aim, we propose spatial models for predicting wind power generation at two different time scales: for annual average wind power generation, and for a high temporal resolution (typically wind power averages over 15-min time steps). In both cases, we use a spatial hierarchical statistical model in which spatial correlation is captured by a latent Gaussian field. We explore how such models can be handled with stochastic partial differential approximations of Mate'rn Gaussian fields together with Integrated Nested Laplace Approximations. We demonstrate the proposed methods on wind farm data from Western Denmark, and compare the results to those obtained with standard geostatistical methods. The results show that our method makes it possible to obtain fast and accurate predictions from posterior marginals for wind power generation. The proposed method is applicable in scientific areas as diverse as climatology, environmental sciences, earth sciences and epidemiology.ang},
  comment      = {Spatiotemporal latent Gaussian field prob wind forecast.  Good for grid nodes?},
  doi          = {10.1007/s00477-016-1329-0},
  file         = {Lenzi16spatTempWndProbFrcst.pdf:Lenzi16spatTempWndProbFrcst.pdf:PDF},
  langid       = {english},
  shortjournal = {Stoch Environ Res Risk Assess},
  url          = {http://link.springer.com/article/10.1007/s00477-016-1329-0},
  urldate      = {2016-10-31},
}

@Misc{Scheuerer14variogramProperScore,
  author       = {Michael Scheuerer and Thomas M. Hamill},
  title        = {Variogram-based proper scoring rules for probabilistic forecasts of multivariate quantities},
  howpublished = {Unpublished},
  month        = may,
  year         = {2014},
  note         = {National Oceanic and Atmospheric Administration (NOAA), Earth System Research Laboratory},
  abstract     = {Proper scoring rules provide a theoretically principled framework for the
quantitative assessment of the predictive performance of probabilistic forecasts.
While a wide selection of such scoring rules for univariate quantities
exists, there are only few scoring rules for multivariate quantities, and many of
them require that forecasts are given in the form of a probability density function.
The energy score, a multivariate generalization of the continuous ranked
probability score, is the only commonly used score that is applicable in the
important case of ensemble forecasts, where the multivariate predictive distribution
is represented by a finite sample. Unfortunately, its ability to detect
incorrectly specified correlations between the components of the multivariate
quantity is somewhat limited. In this paper we present an alternative class
of proper scores based on the geostatistical concept of variograms. We study
their sensitivity to incorrectly predicted means, variances, and correlations in
a number of examples with simulated observations and forecasts, and show
that the variogram-based scoring rules are distinctly more discriminative with
respect to the correlation structure. This conclusion is confirmed in a case
study with post-processed wind speed forecasts at five wind park locations in
Colorado, U.S.A.},
  comment      = {A scoring rule that measures whether the temporal correlations are correct, as well as the quantiles (?). Tip from Eweline IFP meeting, Nov. 2014},
  file         = {Scheuerer14variogramProperScore.pdf:Scheuerer14variogramProperScore.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2014.12.02},
  url          = {http://www.esrl.noaa.gov/psd/people/michael.scheuerer/variogram-score.pdf.},
}

@InProceedings{Deler01genMultiMargCorr,
  author    = {Deler, B. and Nelson, B.L.},
  title     = {Modeling and generating multivariate time series with arbitrary marginals and autocorrelation structures},
  booktitle = {Conference on Winter Simulation},
  year      = {2001},
  volume    = {1},
  pages     = {275--282},
  abstract  = {Providing accurate and automated input modeling support is one of the challenging problems in the application of computer simulation. The authors present a general-purpose input-modeling tool for representing, fitting, and generating random variates from multivariate input processes to drive computer simulations. We explain the theory underlying the suggested data fitting and data generation techniques, and demonstrate that our framework fits models accurately to both univariate and multivariate input processes},
  comment   = {spinning reserves. VARTA. maybe better than JuanMi? Improved in Biller09copulaMultiVar},
  doi       = {10.1109/WSC.2001.977284},
  file      = {Deler01genMultiMargCorr.pdf:Deler01genMultiMargCorr.pdf:PDF},
  keywords  = {arbitrary marginals;autocorrelation structures;automated input modeling support;computer simulation;computer simulations;data fitting;data generation techniques;general-purpose input-modeling tool;multivariate input processes;multivariate time series;random variates;univariate input processes;data analysis;digital simulation;random processes;time series;},
  owner     = {scot},
  timestamp = {2010.11.24},
}

@Article{Eilers15twentyYrsPspline,
  author   = {Eilers, Paul HC and Marx, Brian D and Durb{\'a}n, Maria},
  title    = {Twenty years of P-splines},
  journal  = {SORT-Statistics and Operations Research Transactions},
  year     = {2015},
  volume   = {39},
  number   = {2},
  pages    = {149--186},
  abstract = {P-splines first appeared in the limelight twenty years ago. Since then they have become popular
in applications and in theoretical work. The combination of a rich B-spline basis and a simple dif-
ference penalty lends itself well to a variety of generalizations, because it is based on regression.
In effect, P-splines allow the building of a ?backbone? for the ?mixing and matching? of a variety
of additive smooth structure components, while inviting all sorts of extensions: varying-coefficient
effects, signal (functional) regressors, two-dimensional surfaces, non-normal responses, quantile
(expectile) modelling, among others. Strong connections with mixed models and Bayesian analy-
sis have been established. We give an overview of many of the central developments during the
first two decades of P-splines.

MSC: 41A15, 41A63, 62G05, 62G07, 62J07, 62J12.
Keywords: B-splines, penalty, additive model, mixed model, multidimensional smoothing.},
  comment  = {Everything about penalized splines.  Positve regression, monotonic regression, ...

Number of knots
* says you can really use 1000 knots to fit 10 points -- the penalty handles it.
* says "wise choice" is 100
* Ruppert02numKntsPenSpln has a formula but it might be for a different "penalized spline"?

* Interpolation can be pre-solved (but I don't see why you'd want to)
* Extrapolation is easy.

* Eilers10SplnsKntsPnlties earlier review.  I took lots of notes.},
  file     = {Eilers15twentyYrsPspline.pdf:Eilers15twentyYrsPspline.pdf:PDF},
  groups   = {Read},
  url      = {http://www.raco.cat/index.php/SORT/article/view/302258},
}

@Article{Shen09spherHarm3D,
  author    = {Shen, L. and Farid, H. and McPeek, M.A.},
  title     = {Modeling Three-Dimensional Morphological Structures Using Spherical Harmonics},
  journal   = {Evolution},
  year      = {2009},
  volume    = {63},
  number    = {4},
  pages     = {1003--1016},
  abstract  = {Quantifying morphological shape is a fundamental issue in evolutionary biology. Recent technological advances (e.g., confocal microscopy, laser scanning, computer tomography) have made the capture of detailed three-dimensional (3D) morphological structure easy and cost-effective. In this article, we develop a 3D analytic framework (SPHARM?spherical harmonics) for modeling the shapes of complex morphological structures from continuous surface maps that can be produced by these technologies. Because the traditional SPHARM methodology has limitations in several of its processing steps, we present new algorithms for two SPHARM processing steps: spherical parameterization and SPHARM registration. These new algorithms allow for the numerical characterization of a much larger class of 3D models. We demonstrate the effectiveness of the method by applying it to modeling the cerci of Enallagma damselflies.},
  comment   = {Detailed howto for spherical harmonics w/ lots of examples},
  file      = {Shen09spherHarm3D.pdf:Shen09spherHarm3D.pdf:PDF},
  owner     = {scotto},
  publisher = {John Wiley \& Sons},
  timestamp = {2010.09.06},
}

@Article{Sun15lassoSpatTempQR,
  author   = {Sun, Ying and Wang, Huixia J and Fuentes, Montserrat},
  title    = {Fused Adaptive Lasso for Spatial and Temporal Quantile Function Estimation},
  year     = {2015},
  abstract = {Quantile functions are important in characterizing the entire probability distribution of a
random variable, especially when the tail of a skewed distribution is of interest. This article
introduces new quantile function estimators for spatial and temporal data with a fused adaptive
Lasso penalty to accommodate the dependence in space and time. This method penalizes the
dierence among neighboring quantiles, hence it is desirable for applications with features ordered
in time or space without replicated observations. The theoretical properties are investigated and
the performance of the proposed methods are evaluated by simulations. The proposed method is
applied to particulate matter (PM) data from the Community Multiscale Air Quality (CMAQ)
model to characterize the upper quantiles, which are crucial for studying spatial association
between PM concentrations and adverse human health effects.},
  comment  = {How to consider spatio-temporal correlation when doing quantile regression. Perhaps and alternative to the DTU stochastic differential equation (SDE) approach.

Use for ReWP?},
  file     = {Sun15lassoSpatTempQR.pdf:Sun15lassoSpatTempQR.pdf:PDF},
  url      = {http://es.kaust.edu.sa/Documents/26lasso.pdf},
}

@Article{Angrist06qrPQRmisspec,
  author    = {Angrist, Joshua and Chernozhukov, Victor and Fern{\'a}ndez-Val, Iv{\'a}n},
  title     = {Quantile regression under misspecification, with an application to the US wage structure},
  journal   = {Econometrica},
  year      = {2006},
  volume    = {74},
  number    = {2},
  pages     = {539--563},
  abstract  = {Quantile regression (QR) fits a linear model for conditional quantiles just as ordinary least squares (OLS) fits a linear model for conditional means. An attractive feature of OLS is that it gives the minimum mean-squared error linear approximation to the conditional expectation function even when the linear model is misspecified. Empirical research using quantile regression with discrete covariates suggests that QR may have a similar property, but the exact nature of the linear approximation has remained elusive. In this paper, we show that QR minimizes a weighted mean-squared error loss function for specification error. The weighting function is an average density of the dependent variable near the true conditional quantile. The weighted least squares interpretation of QR is used to derive an omitted variables bias formula and a partial quantile regression concept, similar to the relationship between partial regression and OLS. We also present asymptotic theory for the QR process under misspecification of the conditional quantile function. The approximation properties of QR are illustrated using wage data from the U.S. census. These results point to major changes in inequality from 1990 to 2000.

Keywords:

 Conditional quantile function;
 best linear predictor;
 wage inequality;
 income distribution},
  comment   = {Maybe the earliest partial quantile regression paper. May have R.


Supplemental material has some code here:
http://www.econometricsociety.org/suppmat.asp?id=89&vid=74&iid=2&aid=671
but I'm not sure what it does},
  doi       = {10.1111/j.1468-0262.2006.00671.x/abstract},
  file      = {Angrist06qrPQRmisspec.pdf:Angrist06qrPQRmisspec.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.04.29},
}

@PhdThesis{Fenske12strctAddQRthesis,
  author      = {Fenske, Nora},
  title       = {Structured additive quantile regression with applications to modelling undernutrition and obesity of children},
  year        = {2012},
  abstract    = {Quantile regression allows to model the complete conditional distribution of a response variable
? expressed by its quantiles ? depending on covariates, and thereby extends classical regression
models which mainly address the conditional mean of a response variable.
The present thesis introduces the generic model class of structured additive quantile regression.
This model class combines quantile regression with a structured additive predictor and thereby
enables a variety of covariate effects to be flexibly modelled. Among other components, the
structured additive predictor comprises smooth non-linear effects of continuous covariates and
individual-specific effects which are particularly important in longitudinal data settings.
Furthermore, this thesis gives an extensive overview of existing approaches for parameter
estimation in structured additive quantile regression models. These approaches are structured
into distribution-free and distribution-based approaches as well as related model classes. Each
approach is systematically discussed with regard to the four previously defined criteria, (i) which
different components of the generic predictor can be estimated, (ii) which properties can be
attributed to the estimators, (iii) if variable selection is possible, and, finally, (iv) if software is
available for practical applications.
The main methodological development of this thesis is a boosting algorithm which is presented as
an alternative estimation approach for structured additive quantile regression. The discussion of
this innovative approach with respect to the four criteria points out that quantile boosting involves
great advantages regarding almost all criteria ? in particular regarding variable selection. In
addition, the results of several simulation studies provide a practical comparison of boosting with
alternative estimation approaches.
From the beginning of this thesis, the development of structured additive quantile regression is
motivated by two relevant applications from the field of epidemiology: the investigation of risk
factors for child undernutrition in India (by a cross-sectional study) and for child overweight and
obesity in Germany (by a birth cohort study). In both applications, extreme quantiles of the
response variables are modelled by structured additive quantile regression and estimated by
quantile boosting. The results are described and discussed in detail.},
  comment     = {Thesis of the author of Fenske11idRiskBoostAddQR -- it's more clear than the paper. Covers additive boosting models (see bibtex for Fenske11idRiskBoostAddQR for a general description).

Possible base learners
* linear
* smooth nonlinear
* varying coefficients (could use for wind pow dir dep, as in Nielsen02windPowVarCoeff)
* bivariate surfaces
* discrete spatial
* cluster-specific

Other models considered (but not compared, I think)
* Expectiles
 -- quantile regression cost function but w/ abs val replaced by squared
 -- squared term allows all the nice gradient-following machinery developed for LSQ
 -- produces a quantile-like thing, but for the general case, it's not easy to convert them to quantiles
* Gaussian STAR
 -- Conditional quantile by adding deterministic prediction (ns(zs)) to a standard Guassian dist. quantile
* GAMLSS: Generalized additive models for location, scale and shape
 -- conditionally predicts a whole distribution which can be converted to quantiles
 -- up to four distribution parameters are possible
 -- boosting as in Fenske11idRiskBoostAddQR can be done for each parameter, so it's kind-of interpretable
 -- no crossover possible!
 -- assumes outputs are independent: does that break it for autocorrelated wind and solar power?
 -- implemented in R packages gammls.dist and gamboostLSS
 -- see Mayr12Generalizedadditivemodels
 -- I think it's the way boosted beta regression was implemented: Schmid13betaBoostRgrssn},
  file        = {Fenske12strctAddQRthesis.pdf:Fenske12strctAddQRthesis.pdf:PDF},
  institution = {lmu},
  owner       = {sotterson},
  timestamp   = {2014.07.28},
  url         = {http://edoc.ub.uni-muenchen.de/15161/},
}

@Article{Pratesi09mQRpenSpln,
  author    = {Pratesi, Monica and Ranalli, M Giovanna and Salvati, Nicola},
  title     = {Nonparametric M-quantile regression using penalised splines},
  journal   = {Journal of Nonparametric Statistics},
  year      = {2009},
  volume    = {21},
  number    = {3},
  pages     = {287--304},
  abstract  = {Quantile regression investigates the conditional quantile functions of a response variable in terms of a set of covariates. M-quantile regression extends this idea by a ?quantile-like? generalisation of regression based on influence functions. In this work, we extend it to nonparametric regression, in the sense that the M-quantile regression functions do not have to be assumed to have a certain parametric form, but can be left undefined and estimated from the data. Penalised splines are employed to estimate them. This choice makes it easy to move to bivariate smoothing and semiparametric modelling. An algorithm based on iteratively reweighted penalised least squares to actually fit the model is proposed. Quantile crossing is addressed using an a posteriori adjustment to the function fits following He [1]. Simulation studies show the finite sample properties of the proposed estimation technique.},
  comment   = {Penalized iteratively reweighted least squares (PIRLS). Can be used for quantile regression but this is not QR here. (REALLY?)

* Reiss12smthPenSplnQR (probably more places too) uses a PIRLS algorithm for penalized quantile regression, references the 2009 version of this paper as the source
* Yoshida13asympPenSplnQR also explains the IRLS algorith in Reiss12smthPenSplnQR; maybe it's the same as PIRLS?

THIS IS THE WRONG PAPER. IT'S AN OLDER 2006 VERSION, IN WHICH THE ABSTRACT ALMOST MATCHES. THE 2009 VERSION. BUT IT SEEMS THAT SMALL UPDATES HAVE BEEN MADE SINCE 2006. English fixes and the mention of quantile crossing fix.},
  file      = {Pratesi09mQRpenSpln.pdf:Pratesi09mQRpenSpln.pdf:PDF},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2015.03.24},
  url       = {http://www.amstat.org/sections/srms/proceedings/y2006/Files/JSM2006-000485.pdf},
}

@Article{Koenker12quantregVig,
  author               = {Koenker, R.},
  title                = {Quantile regression in {R}: A vignette},
  journal              = {cran.r-project.org},
  year                 = {2012},
  abstract             = {Quantile regression is an evolving body of statistical methods for
estimating and drawing inferences about conditional quantile functions. An
implementation of these methods in the R language is available in the package
quantreg. This vignette oers a brief tutorial introduction to the package. R
and the package quantreg are open-source software projects and can be freely downloaded from CRAN: http://cran.r-project.org.},
  citeulike-article-id = {7019644},
  comment              = {2012 vignette of the R quantreg library. Read for a good overview of the field. Could also look at the reference manual for more details and references. Has local linear QR example.},
  file                 = {Koenker12quantregVig.pdf:Koenker12quantregVig.pdf:PDF},
  groups               = {PointDerived, doReadNonWPV_1},
  keywords             = {confidence-interval, cran, quantile, vignette},
  owner                = {sotterson},
  posted-at            = {2010-04-14 17:00:00},
  publisher            = {Citeseer},
  timestamp            = {2013.10.24},
  url                  = {http://cran.r-project.org/web/packages/quantreg/index.html},
}

@Article{Hunter00qrMMalg,
  author    = {Hunter, David R and Lange, Kenneth},
  title     = {Quantile regression via an MM algorithm},
  journal   = {Journal of Computational and Graphical Statistics},
  year      = {2000},
  volume    = {9},
  number    = {1},
  pages     = {60--77},
  abstract  = {Quantile regression is an increasingly popular method for estimating the
quantiles of a distribution conditional on the values of covariates. Regression
quantiles are robust against the influence of outliers, and taken several at a time,
they give a more complete picture of the conditional distribution than a single
estimate of the center. The current paper first presents an iterative algorithm
for finding sample quantiles without sorting and then explores a generalization
of the algorithm to nonlinear quantile regression. Our quantile regression algorithm
is termed an MM, or Majorize-Minimize, algorithm because it entails
majorizing the objective function by a quadratic function followed by minimizing
that quadratic. The algorithm is conceptually simple and easy to code, and
our numerical tests suggest that it is computationally competitive with a recent
interior point algorithm for most problems.
Key words and phrases: L1 regression, majorization, EM algorithm, Gauss-
Newton method.},
  comment   = {An alternative to interior point quantile regression that has advantages on nonlinear problems, and near extreme quantiles. But it can be slower than ipqr for high dimensions. Has Matlab.

* MM more stable, sometimes. For small, nonlinear problems, mmqr converges when ipqr convergence fails.
* mmqr maybe better for boundary q's (q near 0 or 1).
* BUT mmqr slower than Interior Point algorithm for high dimensions.
 -- Reason is the pxp matrix inversion.
 -- But other MM algs avoid matrix inversion. They hope this works here too. LOOK UP
* General QR speedup possible w/ preprocessing: Portnoy97gaussLapQRspeedup
* MM technique in general is said to have good potential in high dim optimization:
 http://projecteuclid.org/euclid.ss/1300108233
* it looks like it would be easy to minimize the majorizer directly (instead of the Gauss-Newton approach they use)
 -- complex step differentiation could get 1\textsuperscript{st} and 2\textsuperscript{nd} derivs (on my manifold/spline/rbf thing) (equs 12 and 13)
 -- but Newton drawbacks are mentioned... (bottom of p. 9)

The MM algorithm used in the matlab code here:

http://sites.stat.psu.edu/~dhunter/code/qrmatlab/

This where the initial version of the code for mmqr.m and ipqr.m came from

More stable for nonlinear problems than Koenker96intPtQR but slower when there are a lot of model params.

Technote attached seems to be the same as the journal paper. Slides are from a talk given 7 years later: quick explanation of MM, followed by example of median regression and statement that it generalizes to quantile regression; this is done in the tech note.

Related tutorial: Hunter04tutorialMMalgorithms},
  doi       = {10.1080/10618600.2000.10474866},
  file      = {Tech Note (1999):Hunter00qrMMalg_techNote.pdf:PDF;Slides (2007):Hunter00qrMMalg_slides.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2014.05.13},
}

@Article{Yoshida13asympPenSplnQR,
  author    = {Yoshida, Takuma},
  title     = {Asymptotics for penalized spline estimators in quantile regression},
  journal   = {Communications in Statistics-Theory and Methods},
  year      = {2013},
  number    = {just-accepted},
  abstract  = {Quantile regression predicts the ?-quantile of the conditional distribution of a response variable given the explanatory variable for ? E (0, 1). The aim of this paper is to establish the asymptotic distribution of the quantile estimator obtained by penalized spline method. A simulation and an exploration of real data are performed concerned with our results.
Asymptotic normality, B-spline, Penalized spline, Quantile regression.},
  comment   = {It's probably OK to do penalized spline QR by penalizing coeff roughness.
Penalized spline QR, where penalty is the 2\textsuperscript{nd} coefficient difference (unlike Koenker05QuantRgrssnBook, where recommends a penalty on the regression result roughness, at least in 1D). These guys note that the coeffs are "in fact" estimated by linear programming but seem to advocate doing the minimization using IRLS. Maybe their asymptotic proofs only work on IRLS?

This paper is the Arxiv version, which might be a bit more bold in its claims than the published version. See marked up abstract.

* Kagerer13splnLSQregrssnIntro (LSQ paper) spells out the difference penalty matrix, Dm, used here
* Says the penalized spline QR penalty in Reiss12smthPenSplnQR is computationally intense.
* the IRLS algorithm here may be the same as the "PIRLS" algorith in Reiss12smthPenSplnQR},
  doi       = {10.1080/03610926.2013.765477},
  file      = {arXiv version from 2012:Yoshida13asympPenSplnQR_arxivVers.pdf:PDF},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2014.11.21},
}

@Article{Lin13varSelQRspine,
  author    = {Lin, Chen-Yen and Bondell, Howard and Zhang, Hao Helen and Zou, Hui},
  title     = {Variable selection for non-parametric quantile regression via smoothing spline analysis of variance},
  journal   = {Stat},
  year      = {2013},
  volume    = {2},
  number    = {1},
  pages     = {255--268},
  issn      = {2049-1573},
  abstract  = {Quantile regression provides a more thorough view of the effect of covariates on a response. Non-parametric quantile regression has become a viable alternative to avoid restrictive parametric assumption. The problem of variable selection for quantile regression is challenging, as important variables can influence various quantiles in different ways. We tackle the problem via regularization in the context of smoothing spline analysis of variance models. The proposed sparse non-parametric quantile regression can identify important variables and provide flexible estimates for quantiles. Our numerical study suggests the promising performance of the new procedure in variable selection and function estimation. Copyright ?? 2013 John Wiley & Sons Ltd},
  comment   = {The title says it.},
  doi       = {10.1002/sta4.33},
  file      = {Lin13varSelQRspine.pdf:Lin13varSelQRspine.pdf:PDF},
  keywords  = {COSSO, kernel quantile regression, model selection, reproducing kernel Hilbert space},
  owner     = {sotterson},
  timestamp = {2014.11.04},
}

@InProceedings{Quadrianto09kernQRreducRevis,
  author    = {Quadrianto, N. and Kersting, K. and Reid, M.D. and Caetano, T.S. and Buntine, W.L.},
  title     = {Kernel Conditional Quantile Estimation via Reduction Revisited},
  booktitle = {Data Mining, 2009. ICDM '09. Ninth IEEE International Conference on},
  year      = {2009},
  pages     = {938--943},
  month     = dec,
  abstract  = {Quantile regression refers to the process of estimating the quantiles of a conditional distribution and has many important applications within econometrics and data mining, among other domains. In this paper, we show how to estimate these conditional quantile functions within a Bayes risk minimization framework using a Gaussian process prior. The resulting non-parametric probabilistic model is easy to implement and allows non-crossing quantile functions to be enforced. Moreover, it can directly be used in combination with tools and extensions of standard Gaussian processes such as principled hyperparameter estimation, sparsification, and quantile regression with input-dependent noise rates. No existing approach enjoys all of these desirable properties. Experiments on benchmark datasets show that our method is competitive with state-of-the-art approaches.},
  comment   = {Gaussian Process kernel quantile regression w/ no crossing, and other good things. Could be an improvement upon Naryada Watson kernel estimator, as no crossing, and all the powerful GP machinery is available},
  doi       = {10.1109/ICDM.2009.82},
  file      = {Quadrianto09kernQRreducRevis.pdf:Quadrianto09kernQRreducRevis.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_1},
  issn      = {1550-4786},
  keywords  = {Bayes methods;Gaussian processes;data mining;econometrics;estimation theory;regression analysis;Bayes risk minimization;Gaussian process;data mining;econometrics;kernel conditional quantile estimation;quantile regression;Australia;Data mining;Econometrics;Gaussian noise;Gaussian processes;Kernel;Machine learning;Predictive models;Probability distribution;Risk management;Gaussian Processes;Quantile Regression;Regression},
  owner     = {sotterson},
  timestamp = {2014.03.30},
}

@Article{Hoff07rankLikSemiParCplaMixed,
  author    = {Hoff, Peter D},
  title     = {Extending the rank likelihood for semiparametric copula estimation},
  journal   = {The Annals of Applied Statistics},
  year      = {2007},
  pages     = {265--283},
  abstract  = {Quantitative studies in many fields involve the analysis of multivariate
data of diverse types, including measurements that we may consider binary,
ordinal and continuous. One approach to the analysis of such mixed data is
to use a copula model, in which the associations among the variables are pa-
rameterized separately from their univariate marginal distributions. The pur-
pose of this article is to provide a simple, general method of semiparametric
inference for copula models via a type of rank likelihood function for the
association parameters. The proposed method of inference can be viewed as
a generalization of marginal likelihood estimation, in which inference for a
parameter of interest is based on a summary statistic whose sampling distri-
bution is not a function of any nuisance parameters. In the context of cop-
ula estimation, the extended rank likelihood is a function of the association
parameters only and its applicability does not depend on any assumptions
about the marginal distributions of the data, thus making it appropriate for
the analysis of mixed continuous and discrete data with arbitrary marginal
distributions. Estimation and inference for parameters of the Gaussian cop-
ula are available via a straightforward Markov chain Monte Carlo algorithm
based on Gibbs sampling. Specification of prior distributions or a parametric
form for the univariate marginal distributions of the data is not necessary.},
  comment   = {How to do mixed discrete and continous Gaussian copulas.  Is an MCMC algorithm.  },
  file      = {:papers\\Hoff07rankLikSemiParCplaMixed.pdf:PDF},
  owner     = {sotterson},
  publisher = {JSTOR},
  timestamp = {2017.07.06},
  url       = {http://projecteuclid.org/download/pdfview_1/euclid.aoas/1183143739},
}

@Article{Brocker07properScore,
  author    = {Br{\"o}cker, Jochen and Smith, Leonard A},
  title     = {Scoring probabilistic forecasts: The importance of being proper},
  journal   = {Weather and Forecasting},
  year      = {2007},
  volume    = {22},
  number    = {2},
  pages     = {382--388},
  abstract  = {Questions remain regarding how the skill of operational probabilistic forecasts is most usefully evaluated or compared, even though probability forecasts have been a long-standing aim in meteorological forecasting. This paper explains the importance of employing proper scores when selecting between the various measures of forecast skill. It is demonstrated that only proper scores provide internally consistent evaluations of probability forecasts, justifying the focus on proper scores independent of any attempt to influence the behavior of a forecaster. Another property of scores (i.e., locality) is discussed. Several scores are examined in this light. There is, effectively, only one proper, local score for probability forecasts of a continuous variable. It is also noted that operational needs of weather forecasts suggest that the current concept of a score may be too narrow; a possible generalization is motivated and discussed in the context of propriety and locality.

Keywords: Probability forecasts, Statistics, Forecast verification},
  comment   = {Seems like a good review of what scores have to have. Fairly highly cited.

Proper Score
A score is proper if it's lowest (best) if compute it using your best knowledge of the true distribution, q(x).

Strictly Proper Score
The term is used here to distinguish between two different scores but I don't know what it means.  I suspsect that it's defined in a 2014 Gneiting Tech report (in the refs)

Local Score
* Local: score depends only upon the predicted pdf value at the true answer, X
* Therefore, CRPS (Hersbach00crpsDecomp) is non-local since it's integrating across whole predicted pdf at each instant.
* Ignorance (Roulston02frcstInfoEval) is the only smooth, proper and local score, but fails if forecast has zero probs
Related to Pinson06ProbFrcstPropsEval
* locality "appears to be desirable" but being proper is more important

Generalized Score
* includes the uncertainty about the measurement used for verification (the "truth").
* if original ungeneralized score is proper then so is the generalized version, but it's not necessarily strictly proper.},
  doi       = {10.1175/WAF966.1},
  file      = {Brocker07properScore.pdf:Brocker07properScore.pdf:PDF},
  groups    = {Test, CitaviImport1, doReadWPV_1},
  ncite     = {67},
  owner     = {sotterson},
  timestamp = {2013.09.26},
}

@TechReport{Habte15radiometUncertGUM,
  author      = {Habte, Aron and Sengupta, Manajit and Reda, Ibrahim},
  title       = {A Method to Estimate Uncertainty in Radiometric Measurement Using the Guide to the Expression of Uncertainty in Measurement (GUM) Method; NREL (National Renewable Energy Laboratory)},
  institution = {National Renewable Energy Lab.(NREL), Golden, CO (United States)},
  year        = {2015},
  abstract    = {Radiometric data with known and traceable uncertainty is essential for climate 
change studies to better understand cloud radiation interactions and the earth 
radiation budget. Further, adopting a known and traceable method of estimating 
uncertainty with respect to SI ensures that the uncertainty quoted for radiometric 
measurements can be compared based on documented methods of derivation.  
   Currently, most radiometric data users rely on manufacturers’ specifications of 
calibration uncertainty to quantify the uncertainty of measurements. However, the 
accuracy of solar radiation measured by radiometers depends not only on the 
specifications of the instrument but also on (a) calibration procedure, (b) 
measurement setup and maintenance, and (c) location and environmental conditions 
[1]. Therefore, statements about the overall measurement uncertainty can only be 
made on an individual basis, taking all relevant factors into account. This poster 
provides guidelines and recommended procedures for estimating the uncertainty in 
calibrations and measurements from radiometers. The approach follows the Guide to 
the Expression of Uncertainty in Measurement (GUM)[2].  },
  comment     = {Slides describing SolarAnywhere uncertainty estimate, accoding to Kubiniec19solarSatelliteTuneGrndTR

More details in the conference paper: Habte14calMeasUncertRadiometric
Yet more details: JCGM08guideUncertMeas},
  file        = {:Habte15radiometUncertGUM.pdf:PDF},
  url         = {https://www.osti.gov/biblio/1215171},
}

@Article{Gallego11frcstRampANN,
  author    = {Gallego, C. and Costa, A. and Cuerva, A.},
  title     = {Improving short-term forecasting during ramp events by means of Regime-Switching Artificial Neural Networks},
  journal   = {Advances in Science \& Research},
  year      = {2011},
  volume    = {6},
  pages     = {55--58},
  abstract  = {Ramp events are large rapid variations within wind power time series. Ramp forecasting can benefit from specific strategies so as to particularly take into account these shifts in the wind power output dynamic. In the short-term context (characterized by prediction horizons from minutes to a few days), a Regime-Switching (RS) model based on Artificial Neural Nets (ANN) is proposed. The objective is to identify three regimes in the wind power time series: Ramp-up, Ramp-down and No-ramp regime. An on-line regime assessment methodology is also proposed, based on a local gradient criterion. The RS-ANN model is compared to a single- ANN model (without regime discrimination), concluding that the regime-switching strategy leads to significant improvements for one-hour ahead forecasts, mainly due to the improvements obtained during ramp-up events. Including other explanatory variables (NWP outputs, local measurements) during the regime assessment could eventually improve forecasts for further horizons.},
  comment   = {Pierre says there are a lot of ramp definitions in here},
  file      = {Gallego11frcstRampANN.pdf:Gallego11frcstRampANN.pdf:PDF},
  owner     = {scot},
  timestamp = {2011.05.02},
  url       = {https://www.adv-sci-res.net/6/55/2011/},
}

@Article{Sevlian13windRampDetStat,
  author    = {Sevlian, R. and Rajagopal, R.},
  title     = {Detection and Statistics of Wind Power Ramps},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2013},
  volume    = {PP},
  number    = {99},
  pages     = {1--11},
  issn      = {0885-8950},
  abstract  = {Ramps events are a significant source of uncertainty in wind power generation. Developing statistical models from historical data for wind power ramps is important for designing intelligent distribution and market mechanisms for a future electric grid. This requires robust detection schemes for identifying wind ramps in data. In this paper, we propose an optimal detection technique for identifying wind ramps for large time series. The technique relies on defining a family of scoring functions associated with any rule for defining ramps on an interval of the time series. A dynamic programming recursion is then used to find all such ramp events. Identified wind ramps are used to propose a new stochastic framework to characterize wind ramps. Extensive statistical analysis is performed based on this framework, characterizing ramping duration and rates as well as other key features needed for evaluating the impact of wind ramps in the operation of the power system. In particular, evaluation of new ancillary services and wind ramp forecasting can benefit from the proposed approach.},
  comment   = {An algorithm for finding wind ramps in data. Might be a good way to detect regimes, analog ensembles, etc. Nice plots showing ramp stats like duration, slopes. Has ramp error metrics too.

These s/b probabilistic forecast algorithm features, somehow.},
  doi       = {10.1109/TPWRS.2013.2266378},
  file      = {Sevlian13windRampDetStat.pdf:Sevlian13windRampDetStat.pdf:PDF},
  groups    = {Test, doReadWPV_2},
  keywords  = {Detecting algorithms;dynamic programming;signal processing algorithms;software algorithms;wind energy;wind power generation},
  owner     = {sotterson},
  timestamp = {2013.10.16},
}

@InProceedings{Tang18whenRandFrstFail,
  author    = {Tang, Cheng and Garreau, Damien and von Luxburg, Ulrike},
  title     = {When do random forests fail?},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2018},
  pages     = {2983--2993},
  abstract  = {Random forests are learning algorithms that build large collections of random treesand make predictions by averaging the individual tree predictions.  In this paper,we consider various tree constructions and examine how the choice of parame-ters affects the generalization error of the resulting random forests as the samplesize goes to infinity.   We show that subsampling of data points during the treeconstruction phase is important:  Forests can become inconsistent with either nosubsampling or too severe subsampling.  As a consequence, even highly random-ized trees can lead to inconsistent forests if no subsampling is used, which impliesthat some of the commonly used setups for random forests can be inconsistent.As a second consequence we can show that trees that have good performance innearest-neighbor search can be a poor choice for random forests.},
  comment   = {
Compare tuning advice here with that in Biau16randFrstGuideTour},
  file      = {:Tang18whenRandFrstFail.pdf:PDF},
  url       = {https://papers.nips.cc/paper/7562-when-do-random-forests-fail.pdf},
}

@Article{Meinshausen06qrForests,
  author     = {Meinshausen, Nicolai},
  title      = {Quantile Regression Forests},
  journal    = {Journal of Machine Learning Research},
  year       = {2006},
  volume     = {7},
  pages      = {983--999},
  month      = dec,
  issn       = {1532-4435},
  abstract   = {Random forests were introduced as a machine learning tool in Breiman (2001) and have
since proven to be very popular and powerful for high-dimensional regression and classification.
For regression, random forests give an accurate approximation of the conditional
mean of a response variable. It is shown here that random forests provide information
about the full conditional distribution of the response variable, not only about the conditional
mean. Conditional quantiles can be inferred with quantile regression forests, a
generalisation of random forests. Quantile regression forests give a non-parametric and
accurate way of estimating conditional quantiles for high-dimensional predictor variables.
The algorithm is shown to be consistent. Numerical examples suggest that the algorithm
is competitive in terms of predictive power.
Keywords: quantile regression, random forests, adaptive neighborhood regression},
  acmid      = {1248582},
  comment    = {Random forests for quanties. Idea is to weight the usual random forests to predict quantiles instead of mean. Good for high dimensional problems. Has R.


R Package: 'quantregForest'

compare with: Bhat11quantRgrsnTrees},
  file       = {Meinshausen06qrForests.pdf:Meinshausen06qrForests.pdf:PDF},
  groups     = {PointDerived, doReadNonWPV_1},
  issue_date = {12/1/2006},
  numpages   = {17},
  owner      = {sotterson},
  publisher  = {JMLR.org},
  timestamp  = {2013.10.24},
  url        = {http://dl.acm.org/citation.cfm?id=1248547.1248582},
}

@Article{Grimmer14estHeteroTrtmntEffects,
  author    = {Grimmer, Justin and Messing, Solomon and Westwood, Sean J},
  title     = {Estimating heterogeneous treatment effects and the effects of heterogeneous treatments with ensemble methods},
  journal   = {Unpublished manuscript, Stanford University, Stanford, CA},
  year      = {2014},
  abstract  = {Randomized experiments are increasingly used to study political phenomena because
they can credibly estimate the average effect of a treatment on a population of interest.
But political scientists are often interested in how effects vary across sub-populations?
heterogeneous treatment effects ?and how differences in the content of the treatment
affects responses?the response to heterogeneous treatments. Several new methods
have been introduced to estimate heterogeneous effects, but it is difficult to know if a
method will perform well for a particular data set. Rather than use only one method,
we show how an ensemble of methods?weighted averages of estimates from individual
models increasingly used in machine learning?accurately measure heterogeneous ef-
fects. Building on a large literature on ensemble methods, we show how the weighting of
methods can contribute to accurate estimation of heterogeneous treatment effects and
demonstrate how pooling models leads to superior performance to individual methods
across diverse problems. We apply the ensemble method to two experiments, illumi-
nating how ensemble method for heterogenous treatment effects facilitates exploratory
analysis of treatment effects.},
  comment   = {Probably interesting and very related to Belloni13progEvalHiDim but for now, I just plucked these definitions out of it:

"heterogeneous treatment effects":
how the result (effect) of the same treatment varies across sub-populations

"response to heterogeneous treatments::
how differences in the content of the treatmentaffects responses.},
  file      = {Grimmer14estHeteroTrtmntEffects.pdf:Grimmer14estHeteroTrtmntEffects.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2016.12.18},
  url       = {http://stanford.edu/~jgrimmer/het.pdf},
}

@Article{Hamill01rankHistoVerif,
  author    = {Hamill, Thomas M.},
  title     = {Interpretation of Rank Histograms for Verifying Ensemble Forecasts},
  journal   = {Monthly Weather Review},
  year      = {2001},
  volume    = {129},
  number    = {3},
  pages     = {550--560},
  month     = mar,
  issn      = {0027-0644},
  abstract  = {Rank histograms are a tool for evaluating ensemble forecasts. They are useful for determining the reliability of ensemble forecasts and for diagnosing errors in its mean and spread. Rank histograms are generated by repeatedly tallying the rank of the verification (usually an observation) relative to values from an ensemble sorted from lowest to highest. However, an uncritical use of the rank histogram can lead to misinterpretations of the qualities of that ensemble. For example, a flat rank histogram, usually taken as a sign of reliability, can still be generated from unreliable ensembles. Similarly, a U-shaped rank histogram, commonly understood as indicating a lack of variability in the ensemble, can also be a sign of conditional bias. It is also shown that flat rank histograms can be generated for some model variables if the variance of the ensemble is correctly specified, yet if covariances between model grid points are improperly specified, rank histograms for combinations of model variables may not be flat. Further, if imperfect observations are used for verification, the observational errors should be accounted for, otherwise the shape of the rank histogram may mislead the user about the characteristics of the ensemble. If a statistical hypothesis test is to be performed to determine whether the differences from uniformity of rank are statistically significant, then samples used to populate the rank histogram must be located far enough away from each other in time and space to be considered independent},
  booktitle = {Monthly Weather Review},
  comment   = {doi: 10.1175/1520-0493(2001)129<0550:IORHFV>2.0.CO;2
Review:
What I think is the original paper on histogram evaluation of ensemble rank histograms (U shaped histo, etc.). I've also got some presentation slides from this guy on the same subject that were produced much later than this paper.},
  doi       = {10.1175/1520-0493(2001)129<0550:IORHFV>2.0.CO;2},
  file      = {Hamill01rankHistoVerif.pdf:Hamill01rankHistoVerif.pdf:PDF},
  groups    = {Test, doReadWPV_2},
  owner     = {sotterson},
  publisher = {American Meteorological Society},
  timestamp = {2013.10.02},
}

@InProceedings{Gallus11rampClimWRFensmbl,
  author    = {William A. Gallus and A. J. Deppe},
  title     = {Wind ramp events at an Iowa wind farm: a climatology and evaluation of WRF ensemble forecast skill},
  booktitle = {Wind ramp events at an Iowa wind farm: a climatology and evaluation of WRF ensemble forecast skill},
  year      = {2011},
  month     = jan,
  abstract  = {Rapid changes in wind speed that lead to extreme changes in wind power output, known as wind ramp events, cause considerable problems for the wind energy industry. Accurate forecasts of the events could be of great benefit to the industry, but because these events occur over short temporal scales, forecasting of them can be difficult. Using 80 m wind data from a meteorological tower at the Pomeroy wind farm in northwestern Iowa, we have classified nearly 300 wind ramp events occurring during a sample of over 100 cases for which we have run an 8 member ensemble of WRF members. An event was considered to be a ramp event if the change in wind power was 50\% or more of total capacity in four hours or less, and this was approximated using a typical wind turbine power curve such that any wind speed increase or decrease of more than 3 m/s within the 6-12 m/s window (where power production varies greatly) in four hours or less would be considered a ramp. In addition, ramp down events occurring due to high wind speed shutdowns were also documented. The WRF simulations available during the ramp events used 10 km grid spacing and varied planetary boundary layer schemes with either 00 UTC NAM or 00 UTC GFS output used for initial and lateral boundary conditions. For some cases, additional runs were available with staggered initialization times. We will present a climatology of the wind ramp events, examining their timing and causes. In addition, we will examine model skill at predicting wind ramp events. We will not only examine model forecasts around the times of observed wind ramp events, but also analyze model forecasts to look for false alarms in the models.},
  comment   = {ramp forecasts w/ WRF ensembles, climatology Says knowing ramp amplitude will help but doesn't say how},
  file      = {Gallus11rampClimWRFensmbl.pdf:Gallus11rampClimWRFensmbl.pdf:PDF},
  owner     = {scot},
  timestamp = {2011.05.02},
  url       = {http://ams.confex.com/ams/91Annual/webprogram/Paper179440.html},
}

@Report{Gebetsberger16tricksNonHomog,
  author      = {Gebetsberger, Manuel and Messner, Jakob W. and Mayr, Georg J. and Zeileis, Achim},
  title       = {Tricks for improving non-homogeneous regression for probabilistic precipitation forecasts: Perfect predictions, heavy tails, and link functions},
  year        = {2016},
  abstract    = {Raw ensemble forecasts display large errors in predicting precipitation amounts and its forecast uncertainty, especially in mountainous regions where local effects are often not captured. Therefore, statistical post-processing is typically applied to obtain automatically corrected weather forecasts where precipitation represents one of the most challenging quantities. This study applies the non-homogenous regression framework as a start-of-the-art ensemble post-processing technique to predict a full forecast distribution and improves its forecast performance with three statistical tricks. First of all, a novel split-type approach effectively accounts for perfect ensemble predictions that can occur. Additionally, the statistical model assumes a censored logistic distribution to deal with the heavy tails of precipitation amounts. Finally, the optimization of regression coefficients for the scale parameter is investigated with suitable link-functions. These three refinements are tested for stations in the European Alps for lead-times from +24h to +48h and accumulation periods of 24 and 6 hours. Results highlight an improvement due to a combination of the three statistical tricks against the default post-processing method which does not account for perfect ensemble predictions. Probabilistic forecasts for precipitation amounts as well as the probability of precipitation events could be improved, especially for 6 hour sums.},
  comment     = {ensemble tail correlation extreme value forecast, non homogenous (Gaussian ?) regression.},
  date        = {2016-10},
  file        = {Gebetsberger16tricksNonHomog.pdf:Gebetsberger16tricksNonHomog.pdf:PDF},
  institution = {Faculty of Economics and Statistics, University of Innsbruck},
  keywords    = {censored logistic distribution, log-link, non-homogeneous regression, operational forecasting, probabilistic precipitation forecasts},
  owner       = {sotterson},
  shorttitle  = {Tricks for improving non-homogeneous regression for probabilistic precipitation forecasts},
  timestamp   = {2016.11.10},
  type        = {Working Paper},
  url         = {http://econpapers.repec.org/paper/innwpaper/2016-28.htm},
  urldate     = {2016-11-10},
}

@Article{Fawcett06introROCanalysis,
  author    = {Fawcett, Tom},
  title     = {An introduction to ROC analysis},
  journal   = {Pattern recognition letters},
  year      = {2006},
  volume    = {27},
  number    = {8},
  pages     = {861--874},
  abstract  = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice.
The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research.  2005 Elsevier B.V. All rights reserved.

Keywords: ROC analysis; Classifier evaluation; Evaluation metrics},
  comment   = {Describes of ROC can be used to evalueate (or I suppose, optimize) a classifier (binary or multiclass).  AUC, is the area under the ROC curve, is advocated for.

See also: Powers11evalPrecRecFrocEtc for F-number and other things},
  file      = {Fawcett06introROCanalysis.pdf:Fawcett06introROCanalysis.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2017.01.16},
  url       = {http://www.sciencedirect.com/science/article/pii/S016786550500303X},
}

@InProceedings{Davis06relationshipPrecisionRecall,
  author       = {Davis, Jesse and Goadrich, Mark},
  title        = {The relationship between Precision-Recall and ROC curves},
  booktitle    = {Proceedings of the 23rd international conference on Machine learning},
  year         = {2006},
  pages        = {233--240},
  organization = {ACM},
  abstract     = {Receiver Operator Characteristic (ROC)
curves are commonly used to present re-
sults for binary decision problems in ma-
chine learning. However, when dealing
with highly skewed datasets, Precision-Recall
(PR) curves give a more informative picture
of an algorithm?s performance. We show that
a deep connection exists between ROC space
and PR space, such that a curve dominates
in ROC space if and only if it dominates
in PR space. A corollary is the notion of
an achievable PR curve, which has proper-
ties much like the convex hull in ROC space;
we show an efficient algorithm for computing
this curve. Finally, we also note differences
in the two types of curves are significant for
algorithm design. For example, in PR space
it is incorrect to linearly interpolate between
points. Furthermore, algorithms that opti-
mize the area under the ROC curve are not
guaranteed to optimize the area under the
PR curve.},
  file         = {Davis06relationshipPrecisionRecall.pdf:Davis06relationshipPrecisionRecall.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2017.01.16},
  url          = {http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf},
}

@Article{Minguez10genXtrmValOcean,
  author    = {M{\'i}nguez, R and Men{\'e}ndez, M and M{\'e}ndez, FJ and Losada, IJ},
  title     = {Sensitivity analysis of time-dependent generalized extreme value models for ocean climate variables},
  journal   = {Advances in Water Resources},
  year      = {2010},
  volume    = {33},
  number    = {8},
  pages     = {833--845},
  issn      = {0309-1708},
  abstract  = {Recent advances in the description of the natural variability of extreme events associated with ocean climate variables include time-dependent variations within a certain time scale (year, season or month). These models allow incorporating smooth time variations of the parameters of the \{GEV\} distribution and also the influence of covariates (NAO, El Ni??o, etc.), providing more reliable results than traditional stationary models. In this paper, a sensitivity analysis of the model parameter estimates with respect to data (selected maxima, time of occurrence and covariates) is presented. The method shows how different data affects location, scale, and shape parameters as well as return level quantiles; gaining an insight into time-dependent generalized extreme value models and increasing the understanding of the models responses. The proposed method is applied to the study of the monthly maxima significant wave height at a particular location around the Brittany coast.},
  comment   = {Generalized extreme value quantiles with temporal dependence. Possibly a better way than Pierre's method.},
  doi       = {10.1016/j.advwatres.2010.05.003},
  file      = {Minguez10genXtrmValOcean.pdf:Minguez10genXtrmValOcean.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_2},
  keywords  = {Generalized extreme value},
  owner     = {sotterson},
  timestamp = {2013.10.22},
  url       = {http://www.sciencedirect.com/science/article/pii/S0309170810001016},
}

@Article{Kousky09cplaFatTailTailDepMicCorr,
  author    = {Kousky, Carolyn and Cooke, Roger M},
  title     = {The unholy trinity: fat tails, tail dependence, and micro-correlations},
  journal   = {Resources for the Future Discussion Paper 09-36-REV. Available at SSRN: https://ssrn.com/abstract=1505426 or http://dx.doi.org/10.2139/ssrn.1505426},
  year      = {2009},
  abstract  = {Recent events in the financial and insurance markets, as well as the looming challenges of a globally changing climate point to the need to re-think the ways in which we measure and manage catastrophic and dependent risks. Management can only be as good as our measurement tools. To that end, this paper outlines detection, measurement, and analysis strategies for fat-tailed risks, tail dependent risks, and risks characterized by micro-correlations. A simple model of insurance demand and supply is used to illustrate the difficulties in insuring risks characterized by these phenomena. Policy implications are discussed.

Keywords: risk, fat tails, tail dependence, micro-correlations, insurance, natural disasters},
  comment   = {What can go wrong with a copula model},
  doi       = {http://dx.doi.org/10.2139/ssrn.1505426},
  file      = {Kousky09cplaFatTailTailDepMicCorr.pdf:Kousky09cplaFatTailTailDepMicCorr.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier {BV}},
  timestamp = {2017.05.22},
  url       = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1505426},
}

@Conference{Giebel14EstAvailPowValid,
  author    = {Gregor Giebel and Tuhfe G{\"o}men Bozkurt and Poul S{\o}rensen and Mahmood Mirzaei and Niels Kj{\o}lstad Poulsen and Mads Rajczyk Skjelmose and Jesper Runge Kristoffersen},
  title     = {Estimation and Experimental Validation of the Available Power of a Downregulated Offshore Wind Power Plant},
  booktitle = {International Workshop on Large-Scale Integration of Wind Power into Power Systems as well as on Transmission Networks for Offshore Wind Power Plants},
  year      = {2014},
  month     = nov,
  abstract  = {Recent offshore wind farms are designed as wind
power plants, expected to contribute to the stability of the grid
by offering ancillary services. One of those services is reserve
power, achieved by down-regulating the wind farm from its
possible (or available) power. While reliable methods exist to
determine the available power of an individual turbine, the
sum of those available powers is more than the available
power of the wind farm simply because a turbine in the wake
of a downregulated turbine sees more wind than would be
available without the downregulation, due to decreased wake
effects. The PossPOW project aims at the development of an
industry standard method for the real-time estimation of the
available power of a wind farm, therefore we invite comments
to the method under development, as well as to the verification
we plan later this year using dedicated experiments on large
offshore wind power plants. The setup (and possibly first
results) of the experiments will be shown in the paper.
The estimation of the available power of a whole wind farm
requires models from various disciplines, including wake
modelling of large offshore wind farms, aerodynamic models
for wind turbines, and stochastic model estimation. Real-time
wake modelling including a transport time is necessary in
order to take into account the change in wakes when the wind
farm is downregulated.
The details of the algorithm are explained in detail in a poster
on this conference. This talk will concentrate on the validation
and dissemination.
Keywords- offshore; wind power plants; ancillary services;
wakes; available power; possible power; downregulation;},
  comment   = {Gregor and Mahmood's available power estimation with downregulated turbines. Malte says it needs pitch angle...},
  file      = {Giebel14EstAvailPowValid.pdf:Giebel14EstAvailPowValid.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.14},
}

@Article{Harezlak07funcRgrsnPenalties,
  author    = {Jaroslaw Harezlak and Brent A. Coull and Nan M. Laird and Shannon R. Magari and David C. Christiani},
  title     = {Penalized solutions to functional regression problems},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {2007},
  volume    = {51},
  number    = {10},
  pages     = {4911--4925},
  issn      = {0167-9473},
  abstract  = {Recent technological advances in continuous biological monitoring and personal exposure assessment have led to the collection of subject-specific functional data. A primary goal in such studies is to assess the relationship between the functional predictors and the functional responses. The historical functional linear model (HFLM) can be used to model such dependencies of the response on the history of the predictor values. An estimation procedure for the regression coefficients that uses a variety of regularization techniques is proposed. An approximation of the regression surface relating the predictor to the outcome by a finite-dimensional basis expansion is used, followed by penalization of the coefficients of the neighboring basis functions by restricting the size of the coefficient differences to be small. Penalties based on the absolute values of the basis function coefficient differences (corresponding to the LASSO) and the squares of these differences (corresponding to the penalized spline methodology) are studied. The fits are compared using an extension of the Akaike Information Criterion that combines the error variance estimate, degrees of freedom of the fit and the norm of the basis function coefficients. The performance of the proposed methods is evaluated via simulations. The LASSO penalty applied to the linearly transformed coefficients yields sparser representations of the estimated regression surface, while the quadratic penalty provides solutions with the smallest L2-norm of the basis function coefficients. Finally, the new estimation procedure is applied to the analysis of the effects of occupational particulate matter (PM) exposure on heart rate variability (HRV) in a cohort of boilermaker workers. Results suggest that the strongest association between PM exposure and HRV in these workers occurs as a result of point exposures to the increased levels of PM corresponding to smoking breaks.},
  comment   = {Ways to penalize splines, etc. Relative to distributed lag models w/ basis functions or maybe Gaussian Processes? Improves results on distributed lag model in Zanobetti00addDistLagMort},
  doi       = {DOI: 10.1016/j.csda.2006.09.034},
  file      = {Harezlak07funcRgrsnPenalties.pdf:Harezlak07funcRgrsnPenalties.pdf:PDF},
  keywords  = {Environmental assessment},
  owner     = {scot},
  timestamp = {2010.08.05},
  url       = {http://www.sciencedirect.com/science/article/B6V8V-4M9S95N-1/2/eae2a38fad1ff55cb3c6e03783dbe066},
}

@Article{May08partMutInfoWaterFrcst,
  author    = {May,, Robert J. and Dandy,, Graeme C. and Maier,, Holger R. and Nixon,, John B.},
  title     = {Application of partial mutual information variable selection to ANN forecasting of water quality in water distribution systems},
  journal   = {Environmental Modeling Software},
  year      = {2008},
  volume    = {23},
  number    = {10-11},
  pages     = {1289--1299},
  issn      = {1364-8152},
  abstract  = {Recent trends in the management of water supply have increased the need for modelling techniques that can provide reliable, efficient, and accurate representation of the complex, non-linear dynamics of water quality within water distribution systems. Statistical models based on artificial neural networks (ANNs) have been found to be highly suited to this application, and offer distinct advantages over more conventional modelling techniques. However, many practitioners utilise somewhat heuristic or ad hoc methods for input variable selection (IVS) during ANN development. This paper describes the application of a newly proposed non-linear IVS algorithm to the development of ANN models to forecast water quality within two water distribution systems. The intention is to reduce the need for arbitrary judgement and extensive trial-and-error during model development. The algorithm utilises the concept of partial mutual information (PMI) to select inputs based on the analysis of relationship strength between inputs and outputs, and between redundant inputs. In comparison with an existing approach, the ANN models developed using the IVS algorithm are found to provide optimal prediction with significantly greater parsimony. Furthermore, the results obtained from the IVS procedure are useful for developing additional insight into the important relationships that exist between water distribution system variables.},
  comment   = {* var sel technique fully explained here: May08varSelpartMutInfo
* very large num. of variables (500) w/ lags considered
* weird numerator/denominator neural network
 - generalized regression neural network (GRNN)
 - not the usual mlp3
 - fast to train
 -- only need to pick kernel bandwidth
 -- only one network topology (no hidden nodes choice)
 - compared to MLP uses lots of memory, is slow on evaluation
 -- OK for smooth continuous data (a problem for binary prediction then?)

* GRNN training uses "ensemble training" which minimizes sample bias/var in  hold-out cross-validation
 - they say it's like in Anctil04nnGenTrnSampStrm
 - seems to be like bagging + stratification
 - maybe their stratification approach is described in May10datSplitNN ?
 - training procedure is fairly elaborate

PMI used for wind power forecasts in Dalto14NshrtWindFrcstNNpmi (but do KDE PMI)},
  doi       = {10.1016/j.envsoft.2008.03.008},
  file      = {May08partMutInfoWaterFrcst.pdf:May08partMutInfoWaterFrcst.pdf:PDF},
  location  = {Amsterdam, The Netherlands, The Netherlands},
  owner     = {sotterson},
  publisher = {Elsevier Science Publishers B. V.},
  timestamp = {2009.03.16},
}

@Article{Govindan06delayParDirCoher,
  author    = {Govindan,, R. B. and Raethjen,, Jan and Arning,, Kathrin and Kopper,, Florian and Deuschl, G{\"u}nther},
  title     = {Time Delay and Partial Coherence Analyses to Identify Cortical Connectivities},
  journal   = {Biological Cybernetics},
  year      = {2006},
  volume    = {94},
  number    = {4},
  pages     = {262--275},
  issn      = {0340-1200},
  abstract  = {Recently it has been demonstrated by Albo that partial coherence analysis is sensitive to signal to noise ratio (SNR) and that it will always identify the signal with the highest SNR among the three signals as the main (driving) influence. We propose to use time delay analysis in parallel to partial coherence analysis to identify the connectivities between the multivariate time series. Both are applied to a theoretical model (used by Albo) to analyse the connections introduced in the model. Time delay analysis identifies the connections correctly. We also apply these analyses to the electroencephalogram (EEG) and electromyogram of essential tremor patients and EEG of normal subjects while bimanually tapping their index fingers. Biologically plausible cortico-muscular and cortico-cortical connections are identified by these methods.},
  comment   = {Time delays to helping (but not quite fix) direct causality problems with partial coherence; multi-channel coherence better than pairwise * problem with partial coherence -- Partial coherence always ID's signal w/ highest SNR; can miss low SNR causal factor -- a zero value of partial coherence between three signals need not necessarily imply that the connection between the two signals are established only through the third signal * time delays estimated from partial coherence spectrum * Multi-channel better than pairwise. * Delay analysis can find direction of information flow, but can't detect direct vs. indirect (for feature selection, maybe we don't care?). * However delay analysis can indicate problems w/ parcohere analysis},
  doi       = {10.1007/s00422-005-0045-5},
  file      = {Govindan06delayParDirCoher.pdf:Govindan06delayParDirCoher.pdf:PDF;Govindan06delayParDirCoher.pdf:Govindan06delayParDirCoher.pdf:PDF},
  groups    = {Read},
  location  = {Secaucus, NJ, USA},
  owner     = {sotterson},
  publisher = {Springer-Verlag New York, Inc.},
  timestamp = {2009.03.16},
}

@Article{Thuraisingham07parDirCorrModOrder,
  author    = {Thuraisingham,, R. A.},
  title     = {A new method using coherence to obtain the model order in the evaluation of partial directed coherence},
  journal   = {Computational Biology and Medicine},
  year      = {2007},
  volume    = {37},
  number    = {9},
  pages     = {1361--1365},
  issn      = {0010-4825},
  abstract  = {Recently partial directed coherence has been introduced to study interrelations in multivariate time series, using the vector autoregressive model. In this procedure the choice of the model order is not clear. The use of spectral density has been suggested [B. Schelter, M. Winterhalder, B. Hellwig, B. Guschlbauer, C.H. Lucking, J. Timmer, Direct or indirect? Graphical models for neural oscillators, J. Physiol. (Paris) 99 (2006) 37?46]. This was examined along with a new method which employs coherence. The studies indicate that coherence provides an accurate estimate of the order unlike the spectral density, which underestimated the value of the order leading to inaccurate values for the partial directed coherence. On the other hand the use of coherence gave the correct value for the order, leading to accurate values for the partial directed coherence.},
  comment   = {How to select the partial directed coherence VAR model order but I can't find this paper!},
  doi       = {10.1016/j.compbiomed.2006.12.004},
  location  = {Elmsford, NY, USA},
  publisher = {Pergamon Press, Inc.},
}

@INPROCEEDINGS{Lin07frcstLimICA,
  Author                   = {Lin,, Jin-Cherng and Li,, Yung-Hsin and Liu,, Cheng-Hsiung},
  Title                    = {Solving the limitations of forecasting time series model by independent component analysis approach},
  Booktitle                = {IASTED International Conference: Modeling and Simulation (MOAS)},
  Year                     = {2007},
  Pages                    = {254--259},
  Address                  = {Anaheim, CA, USA},
  Publisher                = {ACTA Press},
  Abstract                 = {Recently some scholars build time series forecasting model by independent component analysis mechanism. Within component ambiguity, time series approximation and mean difference problems, independent component analysis mechanism has intrinsic limitations for time series forecasting. Solutions for those limitations were purposed in this paper. Under the linear time complexity, those limitations were solved by our proposed methods to ensure the forecasting reward. The empirical data show that our model exactly reveals the flexibility and accuracy in time series forecasting domain.},
  Location                 = {Montreal, Canada},
  Owner                    = {sotterson},
  Timestamp                = {2009.02.10},
  URL                      = {http://portal.acm.org/citation.cfm?id=1295645.1295691}
}

@Article{Xu03recursEntEstAdaptFIlt,
  author    = {Jian-Wu Xu and Erdogmus, D. and Ozturk, M.C. and Principe, J.C.},
  title     = {Recursive Renyi's entropy estimator for adaptive filtering},
  journal   = {Signal Processing and Information Technology (ISSPIT)},
  year      = {2003},
  pages     = {134--137},
  month     = dec,
  abstract  = {Recently we have proposed a recursive estimator for Renyi's quadratic entropy. This estimator can accurately converge the results for stationary signals or track the changing entropy of nonstationary signals. We demonstrate the application of the recursive entropy estimator to supervised and unsupervised training of linear and nonlinear adaptive systems. The simulations suggest a smooth and fast convergence to the optimal solution with a reduced complexity in the algorithm as compared to the batch training approach using the same entropy-based criteria. The presented approach also allows on-line information theoretic adaptation of model parameters.},
  comment   = {Recursive entropy estimator, compuationally efficient, adapts to nonsationarity, good refs to oher online entropy estimators * has kernel-based estimator, but recursion makes it efficient * how would this compare, computationally to a Cholesky partial update for kernel approx? * use for mutual information calc? Does kernel func work for muli-dim signals? * examples of use: -- FIR system ID -- time series prediction w/ time-delay neural net -- unsupervised learning of projection persuit of the direction of the uniformaly distributed projection ---- Not sue what this did, but I guess it worked.},
  doi       = {10.1109/ISSPIT.2003.1341078},
  file      = {Xu03recursEntEstAdaptFIlt.pdf:Xu03recursEntEstAdaptFIlt.pdf:PDF;Xu03recursEntEstAdaptFIlt.pdf:Xu03recursEntEstAdaptFIlt.pdf:PDF},
  groups    = {Read},
  keywords  = { adaptive filters, adaptive signal processing, computational complexity, convergence, entropy, filtering theory, linear systems, nonlinear systems, recursive estimation adaptive filtering, batch training approach, chaotic time-series prediction, computational complexity, convergence, information theory, linear adaptive systems, linear system identification, nonlinear adaptive systems, nonstationary signals, projection pursuit, recursive Renyi entropy estimator, stationary signals, supervised training, unsupervised training},
  owner     = {sotterson},
  timestamp = {2008.10.13},
  url       = {http://ieeexplore.ieee.org.offcampus.lib.washington.edu/search/srchabstract.jsp?arnumber=1341078&isnumber=29546&punumber=9296&k2dockey=1341078@ieeecnfs&query=((+entropy+estimator+for+adaptive+filtering)%3Cin%3Emetadata)&pos=0&access=no},
}

@Article{Gerlach11bayesTimVarQR,
  author    = {Gerlach, Richard H and Chen, Cathy WS and Chan, Nancy YC},
  title     = {{Bayes}ian time-varying quantile forecasting for Value-at-Risk in financial markets},
  journal   = {Journal of Business \& Economic Statistics},
  year      = {2011},
  volume    = {29},
  number    = {4},
  abstract  = {Recently, advances in time-varying quantile modeling have proven effective in financial Value-at-Risk forecasting. Some well-known dynamic conditional autoregressive quantile models are generalized to a fully nonlinear family. The Bayesian solution to the general quantile regression problem, via the Skewed-Laplace distribution, is adapted and designed for parameter estimation in this model family via an adaptive Markov chain Monte Carlo sampling scheme. A simulation study illustrates favorable precision in estimation, compared to the standard numerical optimization method. The proposed model family is clearly favored in an empirical study of 10 major stock markets. The results that show the proposed model is more accurate at Value-at-Risk forecasting over a two-year period, when compared to a range of existing alternative models and methods.},
  comment   = {Time adaptive quantile forecast using MCMC. Optimization finds max likelihood of a Skewed Laplace distribution, which has terms for the linear quantile coeffs -- this turns out to exactly optimize Koenkers QR step function.

Note that Koenker, himself, is not a Bayesian QR fan: Koenker10bayesBanoQR},
  doi       = {10.1198/jbes.2010.08203},
  file      = {Gerlach11bayesTimVarQR.pdf:Gerlach11bayesTimVarQR.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.04.21},
}

@InProceedings{May06critValMI,
  author    = {May, R.J. and Dandy, G.C. and Maier, H.R. and Fernando, T.M.K.},
  title     = {Critical Values of a Kernel Density-based Mutual Information Estimator},
  booktitle = {Neural Networks, 2006. IJCNN '06. International Joint Conference on},
  year      = {2006},
  pages     = {4898--4903},
  abstract  = {Recently, mutual information (MI) has become widely recognized as a statistical measure of dependence that is suitable for applications where data are non-Gaussian, or where the dependency between variables is non-linear. However, a significant disadvantage of this measure is the inability to define an analytical expression for the distribution of MI estimators, which are based upon a finite dataset. This paper deals specifically with a popular kernel density based estimator, for which the distribution is determined empirically using Monte Carlo simulation. The application of the critical values of MI derived from this distribution to a test for independence is demonstrated within the context of a benchmark input variable selection problem.},
  comment   = {Use in ramps MI paper confidence intervals? I think this is bootstrap based too, but maybe there's a trick.},
  doi       = {10.1109/IJCNN.2006.247170},
  file      = {May06critValMI.pdf:May06critValMI.pdf:PDF},
  keywords  = {Monte Carlo simulation;data analysis;kernel density-based mutual information estimator;Monte Carlo methods;data analysis;},
  owner     = {sotterson},
  timestamp = {2011.11.30},
}

@Article{Yin14ReviewDatDrivProcMon,
  author    = {S. Yin and S. X. Ding and X. Xie and H. Luo},
  title     = {A Review on Basic Data-Driven Approaches for Industrial Process Monitoring},
  journal   = {IEEE Transactions on Industrial Electronics},
  year      = {2014},
  volume    = {61},
  number    = {11},
  pages     = {6418-6428},
  month     = {Nov},
  abstract  = {Recently, to ensure the reliability and safety of modern large-scale industrial processes, data-driven methods have been receiving considerably increasing attention, particularly for the purpose of process monitoring. However, great challenges are also met under different real operating conditions by using the basic data-driven methods. In this paper, widely applied data-driven methodologies suggested in the literature for process monitoring and fault diagnosis are surveyed from the application point of view. The major task of this paper is to sketch a basic data-driven design framework with necessary modifications under various industrial operating conditions, aiming to offer a reference for industrial process monitoring on large-scale industrial processes.},
  doi       = {10.1109/TIE.2014.2301773},
  file      = {Yin14ReviewDatDrivProcMon.pdf:Yin14ReviewDatDrivProcMon.pdf:PDF},
  issn      = {0278-0046},
  keywords  = {fault diagnosis;process monitoring;reliability;safety;data-driven approach;fault diagnosis;industrial process monitoring;modern large-scale industrial process;reliability;safety;Correlation;Fault diagnosis;Mathematical model;Matrix decomposition;Monitoring;Principal component analysis;Standards;Data-driven;data-driven;fault diagnosis;industrial operating conditions;process monitoring},
  owner     = {sotterson},
  timestamp = {2016.12.19},
}

@Article{Taherdoost18reviewTechAdopt,
  author   = {Hamed Taherdoost},
  title    = {A review of technology acceptance and adoption models and theories},
  journal  = {Procedia Manufacturing},
  year     = {2018},
  volume   = {22},
  pages    = {960 - 967},
  issn     = {2351-9789},
  note     = {11th International Conference Interdisciplinarity in Engineering, INTER-ENG 2017, 5-6 October 2017, Tirgu Mures, Romania},
  abstract = {Recognition the needs and acceptance of individuals is the beginning stage of any businesses and this understanding would be helpful to find the way of future development, thus academicians are interested to realize the factors that drive users’ acceptance or rejection of technologies. A number of models and frameworks have been developed to explain user adoption of new technologies and these models introduce factors that can affect the user acceptance. In this paper, an overview of theories and models regarding user acceptance of technology has been provided. The existing review will emphasize literature that tries to show how developers and researchers presage the level of admission any information technology will attain.},
  comment  = {Non-quantitive stuff.  Not useful for making predictive models.},
  doi      = {https://doi.org/10.1016/j.promfg.2018.03.137},
  file     = {:Taherdoost18reviewTechAdopt.pdf:PDF},
  keywords = {Acceptance Model, Acceptance Theory, Adoption Model, Adoption Theory, User Acceptance, User Adoption},
  url      = {http://www.sciencedirect.com/science/article/pii/S2351978918304335},
}

@Article{Britz15tutorialRNN,
  author       = {Denny Britz},
  title        = {Recurrent Neural Networks Tutorial},
  journal      = {WildML Blog},
  year         = {2015},
  month        = oct,
  abstract     = {Recurrent Neural Networks (RNNs) are popular models that have shown great promise in many NLP tasks. But despite their recent popularity I?ve only found a limited number of resources that throughly explain how RNNs work, and how to implement them. That?s what this tutorial is about. It?s a multi-part series in which I?m planning to cover the following:

Introduction to RNNs (this post)
Implementing a RNN using Python and Theano
Understanding the Backpropagation Through Time (BPTT) algorithm and the vanishing gradient problem
Implementing a GRU/LSTM RNN},
  comment      = {A ModernWindABS slide came from this blog.  It's probably worth reading on its own merits.},
  file         = {Britz15tutorialRNN.pdf:Britz15tutorialRNN.pdf:PDF},
  howpublished = {WildML Blog},
  owner        = {sotterson},
  timestamp    = {2017.01.27},
  url          = {http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/},
}

@Article{Strobl09rcrsPartForest,
  author    = {Strobl, C. and Malley, J. and Tutz, G.},
  title     = {An introduction to recursive partitioning: rationale, application, and characteristics of classification and regression trees, bagging, and random forests.},
  journal   = {Psychological methods},
  year      = {2009},
  volume    = {14},
  number    = {4},
  pages     = {323},
  issn      = {1939-1463},
  abstract  = {Recursive partitioning methods have become popular and widely used tools for nonparametric regression and classification in many scientific fields. Especially random forests, which can deal with large numbers of predictor variables even in the presence of complex interactions, have been applied successfully in genetics, clinical medicine, and bioinformatics within the past few years. High-dimensional problems are common not only in genetics, but also in some areas of psychological research, where only a few subjects can be measured because of time or cost constraints, yet a large amount of data is generated for each subject. Random forests have been shown to achieve a high prediction accuracy in such applications and to provide descriptive variable importance measures reflecting the impact of each variable in both main effects and interactions. The aim of this work is to introduce the principles of the standard recursive partitioning methods as well as recent methodological improvements, to illustrate their usage for low and high-dimensional data exploration, but also to point out limitations of the methods and potential pitfalls in their practical application. Application of the methods is illustrated with freely available implementations in the R system for statistical computing. Keywords: regression, classification, prediction, variable importance},
  comment   = {high dim. feat sel, classification and regression w/ trees},
  file      = {Strobl09rcrsPartForest.pdf:Strobl09rcrsPartForest.pdf:PDF},
  owner     = {scot},
  publisher = {American Psychological Association},
  timestamp = {2011.04.27},
}

@TechReport{Griggs13penSplnRgrssn,
  author      = {Griggs, Whitney},
  title       = {Penalized Spline Regression and its Applications},
  institution = {Whitman College},
  year        = {2013},
  type        = {Senior Project Report},
  abstract    = {Regression analysis is a branch of statistics that examines and describes the rela-
tionship between different variables of a dataset. In this paper, we investigate penalized
spline fits, a nonparametric method of regression modeling, and compare it to the com-
monly used parametric method of ordinary least-squares (OLS). Using data from our
neuroscience research, we demonstrate several different applications of these penalized
splines as compared to linear regression. We conclude the paper with some exploration
of statistical inference using bootstrapping and randomization of our spline models.},
  comment     = {Friendly senior project report about penalized spline regression. Explains (*)+ notation and other things better than papers do. Also is better at explaining how the spline equation generates bases, starting from linear regression and moving to splines. I should probably read the whole thing!

It's by an undergrad: Is it correct? Anyway, it's readable. Does it have details only on linear splines or can cubics be figured out from this paper too?},
  file        = {Griggs13penSplnRgrssn.pdf:Griggs13penSplnRgrssn.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2014.10.24},
  url         = {http://www.whitman.edu/mathematics/SeniorProjectArchive/2013/},
}

@Article{Schmid13betaBoostRgrssn,
  author    = {Schmid, Matthias and Wickler, Florian and Maloney, Kelly O and Mitchell, Richard and Fenske, Nora and Mayr, Andreas},
  title     = {Boosted beta regression},
  journal   = {PloS one},
  year      = {2013},
  volume    = {8},
  number    = {4},
  abstract  = {Regression analysis with a bounded outcome is a common problem in applied statistics. Typical examples include regression models for percentage outcomes and the analysis of ratings that are measured on a bounded scale. In this paper, we consider beta regression, which is a generalization of logit models to situations where the response is continuous on the interval (0,1). Consequently, beta regression is a convenient tool for analyzing percentage responses. The classical approach to fit a beta regression model is to use maximum likelihood estimation with subsequent AIC-based variable selection. As an alternative to this established - yet unstable - approach, we propose a new estimation technique called boosted beta regression. With boosted beta regression estimation and variable selection can be carried out simultaneously in a highly efficient way. Additionally, both the mean and the variance of a percentage response can be modeled using flexible nonlinear covariate effects. As a consequence, the new method accounts for common problems such as overdispersion and non-binomial variance structures.},
  comment   = {For wind farm regression? A possible way to enforce quantile regression bounds for something like Fenske11idRiskBoostAddQR

Uses gammLSS / gamboostLSS(?), R package(s).

Supporting Information for Boosted Beta Regression},
  file      = {Schmid13betaBoostRgrssn.pdf:Schmid13betaBoostRgrssn.pdf:PDF},
  owner     = {sotterson},
  publisher = {Public Library of Science},
  timestamp = {2014.07.18},
  url       = {http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0061623},
}

@Article{Qian11linRgrssnClustNclust,
  author    = {Qian, Guoqi and Wu, Yuehua},
  title     = {Estimation and Selection in Regression Clustering},
  journal   = {European Journal of Pure and Applied Mathematics},
  year      = {2011},
  volume    = {4},
  number    = {4},
  pages     = {455--466},
  abstract  = {Regression clustering is an important model-based clustering tool having applications in
a variety of disciplines. It discovers and reconstructs the hidden structure for a data set which is a
random sample from a population comprising a fixed, but unknown, number of sub-populations, each
of which is characterized by a class-specific regression hyperplane. An essential objective, as well
as a preliminary step, in most clustering techniques including regression clustering, is to determine
the underlying number of clusters in the data. In this paper, we briefly review regression clustering
methods and discuss how to determine the underlying number of clusters by using model selection
techniques, in particular, the information-based technique. A computing algorithm is developed for
estimating the number of clusters and other parameters in regression clustering. Simulation studies
are also provided to show the performance of the algorithm.
2000 Mathematics Subject Classifications: 62H30, 68T10, 91C20
Key Words and Phrases: Regression clustering, Least squares estimation, Model selection},
  comment   = {A recent, top down splitting linear regression clustering algorithm, with a number-of-clusters stopping critera. Good for local linear regression neighborhood picking, etc.},
  file      = {Qian11linRgrssnClustNclust.pdf:Qian11linRgrssnClustNclust.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.03.31},
  url       = {http://ejpam.com/index.php/ejpam/article/viewArticle/1184},
}

@Article{FernandezDelgado19srvyRgrssnMeth,
  author   = {M. Fernández-Delgado and M.S. Sirsat and E. Cernadas and S. Alawadi and S. Barro and M. Febrero-Bande},
  title    = {An extensive experimental survey of regression methods},
  journal  = {Neural Networks},
  year     = {2019},
  volume   = {111},
  pages    = {11 - 34},
  issn     = {0893-6080},
  abstract = {Regression is a very relevant problem in machine learning, with many different available approaches. The current work presents a comparison of a large collection composed by 77 popular regression models which belong to 19 families: linear and generalized linear models, generalized additive models, least squares, projection methods, LASSO and ridge regression, Bayesian models, Gaussian processes, quantile regression, nearest neighbors, regression trees and rules, random forests, bagging and boosting, neural networks, deep learning and support vector regression. These methods are evaluated using all the regression datasets of the UCI machine learning repository (83 datasets), with some exceptions due to technical reasons. The experimental work identifies several outstanding regression models: the M5 rule-based model with corrections based on nearest neighbors (cubist), the gradient boosted machine (gbm), the boosting ensemble of regression trees (bstTree) and the M5 regression tree. Cubist achieves the best squared correlation ( R2) in 15.7% of datasets being very near to it, with difference below 0.2 for 89.1% of datasets, and the median of these differences over the dataset collection is very low (0.0192), compared e.g. to the classical linear regression (0.150). However, cubist is slow and fails in several large datasets, while other similar regression models as M5 never fail and its difference to the best R2 is below 0.2 for 92.8% of datasets. Other well-performing regression models are the committee of neural networks (avNNet), extremely randomized regression trees (extraTrees, which achieves the best R2 in 33.7% of datasets), random forest (rf) and ε-support vector regression (svr), but they are slower and fail in several datasets. The fastest regression model is least angle regression lars, which is 70 and 2,115 times faster than M5 and cubist, respectively. The model which requires least memory is non-negative least squares (nnls), about 2 GB, similarly to cubist, while M5 requires about 8 GB. For 97.6% of datasets there is a regression model among the 10 bests which is very near (difference below 0.1) to the best R2, which increases to 100% allowing differences of 0.2. Therefore, provided that our dataset and model collection are representative enough, the main conclusion of this study is that, for a new regression problem, some model in our top-10 should achieve R2 near to the best attainable for that problem.},
  comment  = {Comparison of 77 popular regression models over 83  regression datasets.  Best overall is the rules-and-knn based cubist but the ranking varies with problem type.  See at least Table 16 and 17.  There are also graphs showing the effects of dimensionality, etc.

I've never heard of some of these models.  This might be good for the CPR ML class, or just as a reference for me.  I imagine it would also be relevant to autoML.},
  doi      = {https://doi.org/10.1016/j.neunet.2018.12.010},
  file     = {:FernandezDelgado19srvyRgrssnMeth.pdf:PDF},
  keywords = {Regression, UCI machine learning repository, Cubist, M5, Gradient boosted machine, Extremely randomized regression tree},
  url      = {http://www.sciencedirect.com/science/article/pii/S0893608018303411},
}

@InCollection{Zhang12rflctLocLinKNN,
  author    = {Zhang, Wei-Feng and Yang, Peng and Dai, Dao-Qing and Nehorai, Arye},
  title     = {Reflectance estimation using local regression methods},
  booktitle = {Advances in Neural Networks--ISNN 2012},
  publisher = {Springer},
  year      = {2012},
  pages     = {116--122},
  abstract  = {Regression methods have been widely used in the problem of spectral reflectance estimation from camera responses, due to their simple application without needing prior knowledge of the imaging system. These methods can be called global regression methods since the regression functions are trained on all the training samples. Recently, local learning methods have received considerable attention due to their capability in exploiting the local manifold structure of data. In this paper, we propose a set of reflectance estimation methods based on local regression methods. These methods can be seen as the local versions of the traditional global regression methods. The training set is confined to the test point?s k-nearest neighbors. Experimental results show that the local ridge regression has the best generalization performance in the compared methods.},
  comment   = {Lazy local linear regression where individual, regularized training for each point is restricted to its k NN's neighbhorhood. Local ridge regression works best. Input dimension is a respectable 31; and they used k=30 neighbors (so they really needed ridge regression, else it would have been underdetermined).

Good because it's local
* ridge & Co. techniques allow estimation w/ fewer points than dimensions
* computes a function, so may need less points to generalize OK
Bad because
* k NN's are picked with straight Euclidean distance ==> high dim hubness probs
* limiting training to K nn's (not full dataset w/ weights) could make for for jumpy transitions vs. x
* Euclidean distance is unaware of feature importance, as in Hirose12NNRMLknnLocLin
 - yet still seems to be an improvement upon it

Improvements?
* use Hirose12NNRMLknnLocLin distances
* and then run KNN local ridge regression on that neighbhorhood?
* use PLSR instead of featsel so that there's a transform e.g. Tian13localPLSvoiceMap

ALGORITHM

* KNN methods had Gaussian kernel, w/ heuristic weights, apparently.
* regression outupt is one of 3 scalars representng the RGB colors.
* computational burden may not be super large
 - even though a regression is computed for each test point
 - but is done only across the KNN's
 - ridge allows the num. points (k) to be less than the num. dimensions
* limiting it to the k NN's seems like it would make the boundaries jaggedy
* instead of feature selection, could also do dimension reduction e.g. with PLSR, or "partial quantile regression," whatever that means in the QR literature (I have bibtexted a couple paperrs on this: read them).},
  doi       = {10.1007/978-3-642-31346-2_14},
  file      = {Zhang12rflctLocLinKNN.pdf:Zhang12rflctLocLinKNN.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2014.04.22},
}

@TechReport{Harrell13rmsRpkg,
  author      = {Frank E. Harrell},
  title       = {rms: Regression Modeling Strategies (R package)},
  institution = {Vanderbilt University},
  year        = {2013},
  abstract    = {Regression modeling, testing, estimation, validation, graphics, prediction, and typesetting by storing enhanced model design attributes in the fit. rms is a collection of functions that assist with and streamline modeling. It also contains functions for binary and ordinal logistic regression models, ordinal models for continuous Y with a variety of distribution families, and the Buckley-James multiple regression model for right-censored responses, and implements penalized maximum likelihood estimation for logistic and ordinary linear models. rms works with almost any regression model, but it was especially written to work with binary or ordinal regression models, Cox regression, accelerated failure time models, ordinary linear models, the Buckley-James model, generalized least squares for serially or spatially correlated observations, generalized linear models, and quantile regression.},
  comment     = {R has a kind of feature selection for quantile regression, and much else.

* quantile regression feature selection: validate.Rq
-- looks something like stepwise regression, starting from full feature set?

Related book: Harrell15rgrssnMdlStrtgyBook

RMS overview:
http://www.inside-r.org/packages/cran/rms/docs/rmsOverview

RMS main page:
http://biostat.mc.vanderbilt.edu/wiki/Main/Rrms

RMS course (with handouts, notes)
http://biostat.mc.vanderbilt.edu/wiki/Main/CourseBios330Syllabus},
  file        = {Harrell13rmsRpkg.pdf:Harrell13rmsRpkg.pdf:PDF},
  groups      = {PointDerived, doReadNonWPV_1},
  owner       = {sotterson},
  timestamp   = {2013.11.11},
  url         = {http://cran.r-project.org/web/packages/rms/index.html},
}

@Book{Szepesvari10algReinfLrnBook,
  title     = {Algorithms for reinforcement learning},
  publisher = {Morgan \& Claypool Publishers},
  year      = {2010},
  author    = {Szepesv{\'a}ri, Csaba},
  volume    = {4},
  number    = {1},
  abstract  = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective. What distinguishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the algorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book, we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming. We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.},
  comment   = {A more terse, mathematical book with tutorial slides

Recommended here:

http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/

along with Sutton17reinfLrnIntroBook},
  file      = {2010 Book w/ May 2013 updates:Szepesvari10algReinfLrnBook:PDF;2010 Tutorial Slides:Szepesvari10algReinfLrnBook_slides:PDF},
  journal   = {Synthesis lectures on artificial intelligence and machine learning},
  owner     = {sotterson},
  pages     = {1--103},
  timestamp = {2017.01.14},
  url       = {https://sites.ualberta.ca/~szepesva/RLBook.html},
}

@Article{Kumar14FeatSelLitRev,
  author    = {Kumar, Vipin and Minz, Sonajharia},
  title     = {Feature Selection: A literature review},
  journal   = {Smart Computing Review},
  year      = {2014},
  volume    = {4},
  number    = {3},
  pages     = {211--229},
  abstract  = {Relevant feature identification has become an essential task to apply data mining
algorithms effectively in real-world scenarios. Therefore, many feature selection methods have
been proposed to obtain the relevant feature or feature subsets in the literature to achieve their
objectives of classification and clustering. This paper introduces the concepts of feature relevance,
general procedures, evaluation criteria, and the characteristics of feature selection. A
comprehensive overview, categorization, and comparison of existing feature selection methods are
also done, and the guidelines are also provided for user to select a feature selection algorithm
without knowing the information of each algorithm. We conclude this work with real world
applications, challenges, and future research directions of feature selection.



Keywords: feature selection, feature relevance, classification, clustering, real world applications},
  comment   = {Read this to catch up?},
  date      = {June},
  file      = {Kumar14FeatSelLitRev.pdf:Kumar14FeatSelLitRev.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2016.06.18},
}

@Article{Pinson10relDiagSerCorr,
  author    = {Pinson, Pierre and McSharry, Patrick and Madsen, Henrik},
  title     = {Reliability diagrams for non-parametric density forecasts of continuous variables: Accounting for serial correlation},
  journal   = {Quarterly Journal of the Royal Meteorological Society},
  year      = {2010},
  volume    = {136},
  number    = {646},
  pages     = {77--90},
  issn      = {1477-870X},
  abstract  = {Reliability is seen as a primary requirement when verifying probabilistic forecasts, since a lack of reliability would introduce a systematic bias in subsequent decision-making. Reliability diagrams comprise popular and practical diagnostic tools for the reliability evaluation of density forecasts of continuous variables. Such diagrams relate to the assessment of the unconditional calibration of probabilistic forecasts. A reason for their appeal is that deviations from perfect reliability can be visually assessed based on deviations from the diagonal. Deviations from the diagonal may, however, be caused by both sampling effects and serial correlation in the forecast-verification pairs. We build on a recent proposal, consisting of associating reliability diagrams with consistency bars that would reflect the deviations from the diagonal that are potentially observable even if density forecasts are perfectly reliable. Our consistency bars, however, reflect potential deviations originating from the combined effects of limited counting statistics and serial correlation in the forecast-verification pairs. They are generated based on an original surrogate consistency resampling method. Its ability to provide consistency bars with a significantly better coverage against the independent and identically distributed (i.i.d.) resampling alternative is shown from simulations. Finally, a practical example of the reliability assessment of non-parametric density forecasts of short-term wind-power generation is given. Copyright 2010 Royal Meteorological Society},
  comment   = {Pierre's paper that used surrogates, which he thought would be good for missing feature flll-in. Maybe, with some work. Quickly reviewing, the idea seems to be to a scenario time series by Fourier transforming, randomizing the phase and then inverse transforming; the result would have the same marginal and autocorrelation properties, I think. This is for evaluating reliability diagrams of probabilistic forecasts, I think. It's better than random resampling (I think) because time-domain correlation is taken into account, even though phase is random.

Note that the surrogate approach used in this paper is improved in: Kirch11timeFreqBtStrp

For feature filling, this idea, by itself, can't work better than Amelia (Honaker10missValTseries):
1.) It's only 1-D
2.) The generated scenario isn't conditional upon the known-observed data
3.) It's not clear how to do a F. T. on data which has missing values
4.) phase isn't random in a missing-feature-filling problem; there's a prior probability on phase components which comes from the known values.

This means that you can't randomize it but must generate it probabilistically, somehow. Maybe this could be adapted somehow, but it would require a lot of thought.

Maybe:
* represent the N-dim FFT as a Bayesian Fourier transform e.g. http://homepages.inf.ed.ac.uk/amos/missingfft.html
* prior probability of Fourier amplitudes comes from marginal distribution FT's
 -- themselves calculated over a long window, maybe with forgetting functions, averaging, etc
 -- these FT's would also have to handle missing inputs
 -- could model spectrogram as a mixture of spectra to handle some non-stationarity, maybe, like: Smaragdis11timeFreqImpute
 ---- at least this shows how to generate the phase of the missing data (Griffin-Lim algorithm)
* SOMEHOW, linear dependency across the different measures could be modeled
* THEN, use EM to find the ML fit, given the observed data points

The pdf is actually a preprint from: http://pierrepinson.com/docs/pinsonetal09_reliability_rev.pdf},
  doi       = {10.1002/qj.559},
  file      = {Pinson10relDiagSerCorr.pdf:Pinson10relDiagSerCorr.pdf:PDF},
  groups    = {Read, Test, doReadWPV_2},
  keywords  = {probabilistic forecasting, verification, calibration, surrogate, consistency resampling, wind power},
  owner     = {sotterson},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2012.09.28},
}

@Article{ValizadehHaghi10copulaPVwind,
  author    = {Valizadeh Haghi, H. and Tavakoli Bina, M. and Golkar, M.A. and Moghaddas-Tafreshi, S.M.},
  title     = {Using Copulas for analysis of large datasets in renewable distributed generation: {PV} and wind power integration in {Iran}},
  journal   = {Renewable Energy},
  year      = {2010},
  volume    = {35},
  number    = {9},
  pages     = {1991--2000},
  month     = sep,
  issn      = {0960-1481},
  abstract  = {Renewable distributed generation introduced as an environmental friendly alternative energy supply while it provided the power system with ever-growing technical benefits such as loss reduction and feeder voltage improvement. The evaluation of the effects of small residential photovoltaic and wind DG systems on various system operating indices and the system net load is complicated by both the probabilistic nature of their output and the variety of their spatial allocations. The increasing penetration of renewable distributed generation in power systems necessitates the modeling of this stochastic structure in operation and planning studies. An advanced stochastic modeling of the system requires multivariate uncertainty analysis involving non-normal correlated random variables. Such an analysis is to epitomize the aggregate uncertainty corresponding to spatially spread stochastic variables. In this paper, an integration study of photovoltaics and wind turbines, distributed in a distribution network, is investigated based on the stochastic modeling using Archimedean copulas as a new efficient tool. The basic theory concerning the use of copulas for dependence modeling is presented and focus is given on an Archimedean algorithm. A comprehensive case study for Davarzan area in Iran is presented after reviewing Iran's renewable energy status. This study shows an application of the presented technique when large datasets, assuming 10-min interval between data points of PV, wind and load profiles, are involved where a deterministic study is not trivial.},
  comment   = {spinning reserves},
  file      = {ValizadehHaghi10copulaPVwind.pdf:ValizadehHaghi10copulaPVwind.pdf:PDF},
  keywords  = {Copula, Correlation, Dependence, Distributed generation, Monte Carlo simulation, Photovoltaics (PV), Wind energy},
  owner     = {scot},
  timestamp = {2010.11.24},
  url       = {http://www.sciencedirect.com/science/article/B6V4S-4YG7P3P-1/2/109bd692af1b58096ee0a9d24cce8b58},
}

@InProceedings{Bremen08ECMWFprobForecast,
  author    = {Lueder von Bremen and Renate Hagedorn and Bernhard Lange},
  title     = {ECMWF's Ensemble Prediction System in probabilistic Wind Power Forecasting},
  booktitle = {European Meteorological Society (EMS)},
  year      = {2008},
  volume    = {5},
  abstract  = {Renewable energies are considered the only energy source that will become less expensive
in a future of continously increasing energy demand. Since many years wind
power deployment is rapidly increasing and has become a very important market. The
current perspectives for 2030 are that 300 GW of installed wind power capacity will
meet 22\% of Europe?s electricity demand. Increasing wind power capacities require
very good predictions of wind power production to enable save grid integration while
keeping the commonly high level of reliability of the European power supply system.
Continuous improvement is indispensable and is requested by all stakeholder of
the electricity market (Transmission System Operators (TSOs), energy traders, wind
farm operators). The day-ahead (24- 48h) predictions of wind power are nowadays
established products. The root mean square error of the best forecast for Germany
is between 5 and 6 \% (normalized with the rated capacity) using the deterministic
ECMWF forecast. In order to tackle the problem of situations with low predictability
the development of probabilistic wind power forecasts becomes very important.
In particular, the problem of seldom but large forecast error needs to be solved and
requires probabilistic information for decision making as early as possible. Therefore
we focus in our study to lead times of 72 hours.
In this paper we study the skill of ECMWF?s Ensemble Prediction System (EPS)
regarding wind power forecasts for Germany. A very effective wind power forecast
model has been developed to enable computationally efficient integration of all 50 ensemble
members. The wind power prediction model is based on principle component
regression techniques of the wind speed to reduce the degrees of freedom. We investigate
in detail the impact of using 10m wind speeds compared to using model level
winds. Using 10m winds in the logarithmic wind profile has the known disadvantage
that they do not represent the wind speed in hub heights of the turbines (100m) very
well. It is very important to consider the thermal stratification of the atmosphere. Otherwise
strong underestimation of wind speeds at daytime due to enhanced coupling of
the flow in 100m with the surface occurs. During night decoupling can not be captured
with 10m wind speeds. First results indicate that wind power forecast using the 10m
EPS mean wind speed is far better than using the 10m deterministic forecast. It almost
reaches the quality of the deterministic forecast using model level winds.
The evaluation is mainly done using a range of probabilistic skill scores like Brier
Skill Score, ROC Area, Reliability Diagram, and Ignorance Score. Each of the scores
is used to diagnose specific aspects of the quality of the wind power forecast system.},
  comment   = {I can't find the actual paper but at least this shows that Lueder is using wind ensembles to predict wind power, somehow.},
  file      = {:Bremen08ECMWFprobForecast.pdf:PDF;Bremen08ECMWFprobForecast.pdf:Bremen08ECMWFprobForecast.pdf:PDF},
  groups    = {Ensemble, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2008.07.03},
  url       = {http://www.cosis.net/abstracts/EMS2008/00311/EMS2008-A-00311-1.pdf?PHPSESSID=7e78deffacadf271d6ac21583ef661c8},
}

@Article{Pinson13windPowFrcstOpDec,
  author    = {Pinson, Pierre and others},
  title     = {Wind energy: Forecasting challenges for its operational management},
  journal   = {Statistical Science},
  year      = {2013},
  volume    = {28},
  number    = {4},
  pages     = {564--585},
  abstract  = {Renewable energy sources, especially wind energy, are to play
a larger role in providing electricity to industrial and domestic consumers.
This is already the case today for a number of European countries, closely
followed by the US and high growth countries, for example, Brazil, India
and China. There exist a number of technological, environmental and politi-
cal challenges linked to supplementing existing electricity generation capac-
ities with wind energy. Here, mathematicians and statisticians could make a
substantial contribution at the interface of meteorology and decision-making,
in connection with the generation of forecasts tailored to the various opera-
tional decision problems involved. Indeed, while wind energy may be seen
as an environmentally friendly source of energy, full benefits from its usage
can only be obtained if one is able to accommodate its variability and limited
predictability. Based on a short presentation of its physical basics, the impor-
tance of considering wind power generation as a stochastic process is moti-
vated. After describing representative operational decision-making problems
for both market participants and system operators, it is underlined that fore-
casts should be issued in a probabilistic framework. Even though, eventually,
the forecaster may only communicate single-valued predictions. The existing
approaches to wind power forecasting are subsequently described, with focus
on single-valued predictions, predictive marginal densities and space?time
trajectories. Upcoming challenges related to generating improved and new
types of forecasts, as well as their verification and value to forecast users, are
finally discussed.
Key words and phrases: Decision-making, electricity markets, forecast ver-
ification, Gaussian copula, linear and nonlinear regression, quantile regres-
sion, power systems operations, parametric and nonparametric predictive
densities, renewable energy, space?time trajectories, stochastic optimization},
  comment   = {Overview of operational decisions made using wind power forecasting -- a useful read for Eweline "use of forecasts" meeting between Amprion and EDF. Advocates probabilistic treatment but says forecaster might eventually communication only some kind of point forecast.

This was a Project Euclid paper.

Areas for improvement
 + space time dependencies, grid forecasts
 + space time scenarios
 + spatio-temporal dynamics due to coarse spatial-temporal NWP or timing errors: incorporating met stations, farm data to improve regional forecast modeling
 + high dims require new treatment e.g.
 - pooling/foreasting spatially correlated data sets.
 -- is that right? Easier to forecast if UNcorrelated (EERA DTOC result)
 - making the pooling dynamic depending upon weather situation.
 + stochastic power curve
 + joint forecasting of all RES production types, load, market variables
 + Avoiding predictive marginal densities
 + Verification:
 - conditional on all variables known to affect density shape
 - account for sample size, spatial or temporal correlation
 - new verification methods for high dimensional forecasts
 - better link between skill scores (quality) and their impact on decision maker value},
  doi       = {10.1214/13-STS445},
  file      = {Pinson13windPowFrcstOpDec.pdf:Pinson13windPowFrcstOpDec.pdf:PDF},
  groups    = {Read},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Casati08frcstVerifFut,
  author    = {Casati, B. and Wilson, L. J. and Stephenson, D. B. and Nurmi, P. and Ghelli, A. and Pocernich, M. and Damrath, U. and Ebert, E. E. and Brown, B. G. and Mason, S.},
  title     = {Forecast verification: current status and future directions},
  journal   = {Meteorological Applications},
  year      = {2008},
  volume    = {15},
  number    = {1},
  pages     = {3--18},
  issn      = {1469-8080},
  abstract  = {Research and development of new verification strategies and reassessment of traditional forecast verification methods has received a great deal of attention from the scientific community in the last decade. This scientific effort has arisen from the need to respond to changes encompassing several aspects of the verification process, such as the evolution of forecasting systems, or the desire for more meaningful verification approaches that address specific forecast user requirements. Verification techniques that account for the spatial structure and the presence of features in forecast fields, and which are designed specifically for high-resolution forecasts have been developed. The advent of ensemble forecasts has motivated the re-evaluation of some of the traditional scores and the development of new verification methods for probability forecasts. The expected climatological increase of extreme events and their potential socio-economical impacts have revitalized research studies addressing the challenges concerning extreme event verification. Verification issues encountered in the operational forecasting environment have been widely discussed, verification needs for different user communities have been identified, and models to assess the forecast value for specific users have been proposed. Proper verification practice and correct interpretation of verification statistics has been extensively promoted with recent publications and books, tutorials and workshops, and the development of open-source software and verification tools. This paper addresses some of the current issues in forecast verification, reviews some of the most recently developed verification techniques, and provides recommendations for future research. Copyright ? 2008 Royal Meteorological Society and Crown in the right of Canada.},
  comment   = {Possibly a good review, but too detailed for the first reading.},
  doi       = {10.1002/met.52},
  file      = {Casati08frcstVerifFut.pdf:Casati08frcstVerifFut.pdf:PDF},
  groups    = {Test, doReadWPV_2},
  keywords  = {spatial verification approaches, probability forecasts and ensemble verification, extreme events verification, operational verification, verification packages, user-oriented verification, value},
  owner     = {sotterson},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.09.27},
}

@TechReport{Pinson13DiscrimAbilityEnrgyScr,
  author      = {Pinson, Pierre and Tastu, Julija},
  title       = {Discrimination ability of the Energy score},
  institution = {Technical University of Denmark},
  year        = {2013},
  abstract    = {Research on generating and verification of multivariate probabilistic forecasts has gained increased interest
over the last few years. Emphasis is placed here on the evaluation of forecast quality with the Energy
score, which is based on a quadratic scoring rule. While this score may be seen as appealing since being
proper, we show that its discrimination ability may be limited when focusing on the dependence structure
of multivariate probabilistic forecasts. For the case of multivariate Gaussian process, a theoretical upper
for such discrimination ability is derived and discussed. This limited discrimination ability may eventually
get compromised by computational and sampling issues, as dimension increases.
Keywords: probabilistic forecasting, Energy score, discrimination, proper score, multivariate scenarios},
  comment     = {It seems that the energy score of Pinson12scenQualWind may not be able to discriminate high dim forecasts. But in that paper, it was said to have done OK discriminating between forecasts w/ same marginal dists and different temporal correlations. So, I had better read both papers because I don't get it.},
  file        = {Pinson13DiscrimAbilityEnrgyScr.pdf:Pinson13DiscrimAbilityEnrgyScr.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2014.11.18},
  url         = {http://orbit.dtu.dk/fedora/objects/orbit:122326/datastreams/file_b919613a-9043-4240-bb6c-160c88270881/content},
}

@Article{Dong17resPVdeployFrcst,
  author   = {Changgui Dong and Benjamin Sigrin and Gregory Brinkman},
  title    = {Forecasting residential solar photovoltaic deployment in California},
  journal  = {Technological Forecasting and Social Change},
  year     = {2017},
  volume   = {117},
  pages    = {251 - 265},
  issn     = {0040-1625},
  abstract = {Residential distributed photovoltaic (PV) deployment in the United States has experienced robust growth, and policy changes impacting the value of solar are likely to occur at the federal and state levels. To establish a credible baseline and evaluate impacts of potential new policies, this analysis employs multiple methods to forecast residential PV deployment in California, including a time-series forecasting model, a threshold heterogeneity diffusion model, a Bass diffusion model, and National Renewable Energy Laboratory's dSolar model. As a baseline, the residential PV market in California is modeled to peak in the early 2020s, with a peak annual installation of 1.5–2GW across models. We then use the baseline results from the dSolar model and the threshold model to gauge the impact of the recent federal investment tax credit (ITC) extension, the newly approved California net energy metering (NEM) policy, and a hypothetical value-of-solar (VOS) compensation scheme. We find that the recent ITC extension may increase annual PV installations by 12\%–18\% (roughly 500MW) for the California residential sector in 2019–2020. The new NEM policy only has a negligible effect in California due to the relatively small new charges (<100MW in 2019–2020). Furthermore, impacts of the VOS compensation scheme (\$0.12 per kilowatt-hour) are larger, reducing annual PV adoption by 32\% (or 900–1300MW) in 2019–2020.},
  comment  = {NREL PV adoption rate study that might be related to the stuff Clean Power Research might want me to do.

The NREL group's publishing page is here:
https://www.nrel.gov/analysis/dgen/publications.html

See also:  
Gagnon17netMtrAdoptPV  (also does PV)
Rogers83diffusInnovBk (cited as reference to diffusion of innovation models, but mabye not the Bass version used here)
Fancy Bass w/ derivation and multivariate inputs: Bass94bassWithoutDecVars
Bass w/ SDE: Kapur12bassDiffuseSDE
Another dynamic Bass:  Cosguner18dynPriceDynBass
Spatial Bass: Duggan17bassMetaSpace
Inhomog Bass: Nafidi18inhomogDiffProcBassSDE},
  doi      = {https://doi.org/10.1016/j.techfore.2016.11.021},
  file     = {:Dong17resPVdeployFrcst.pdf:PDF},
  keywords = {Solar PV, Forecasting, Diffusion models, Policy impact},
  url      = {http://www.sciencedirect.com/science/article/pii/S0040162516307569},
}

@InProceedings{Cannon16skilRngProbFrcst,
  author    = {D. J. Cannon and D. J. Brayshaw and J. Methven and D. R. Drew},
  title     = {Determining the bounds of skilful forecast range for probabilistic prediction of system-wide wind power generation},
  booktitle = {ICEM 3rd Inter'l Conf. on Energy \& Meteorology},
  year      = {2016},
  abstract  = {REVIEW ABSTRACT
State-of-the-art wind power forecasts beyond a few hours ahead rely on global numerical weather prediction models to forecast the future large-scale atmospheric state. Often they provide initial and boundary conditions for nested high resolution simulations. In this paper, both upper and lower bounds on forecast range are identified within which global ensemble forecasts provide skilful information for system-wide wind power applications. The power system of Great Britain (GB) is used as an example because independent verifying data is available from National Grid. An upper bound on forecast range is associated with the limit of predictability, beyond which forecasts have no more skill than predictions based on climatological statistics. A lower bound is defined at the lead time beyond which the "resolved" uncertainty associated with estimating the future large-scale atmospheric state is larger than the "unresolved" uncertainty associated with estimating the system-wide wind power response to a given large-scale state.

The bounds of skilful forecast range are quantified for three leading global forecast systems. The upper bound defined by forecasts of GB-total wind power generation at a specific point in time are found to be 6-8 days. The lower bound is found to be 1.4-2.4 days. Both bounds depend on the global forecast system and vary seasonally. In addition, forecasts of the probability of an extreme power ramp event were found to possess a shorter limit of predictability (4.5-5.5 days). The upper bound to useful forecast range can only be extended by improving the global forecast system (outside the control of most users) or by changing the metric used in the probability forecast. Improved downscaling and microscale modelling of the wind farm response may act to decrease the lower bound to useful forecast range. The potential gain from such improvements have diminishing returns beyond the short-range (out to around 2 days).},
  file      = {My first review:Cannon16skilRngProbFrcst_MyReview_1.docx:Word 2007+;REVIEW copy:Cannon16skilRngProbFrcst_reviewCopy.pdf:PDF;Cannon16skilRngProbFrcst.html:reviews\\Cannon16skilRngProbFrcst\\Cannon16skilRngProbFrcst.html:URL},
}

@Article{Garcia15stndrdizVarsColin,
  author    = {Garc{\'\i}a, Jos{\'e} and Salmer{\'o}n, Rom{\'a}n and Garc{\'\i}a, Catalina and L{\'o}pez Mart{\'\i}n, Mar{\'\i}a del Mar},
  title     = {Standardization of variables and collinearity diagnostic in ridge regression},
  journal   = {International Statistical Review},
  year      = {2015},
  volume    = {84},
  number    = {2},
  pages     = {245--266},
  issn      = {1751-5823},
  note      = {10.1111/insr.12099},
  abstract  = {Ridge estimation (RE) is an alternative method to ordinary least squares when there exists a collinearity problem in a linear regression model. The variance inflator factor (VIF) is applied to test if the problem exists in the original model and is also necessary after applying the ridge estimate to check if the chosen value for parameter k has mitigated the collinearity problem. This paper shows that the application of the original data when working with the ridge estimate leads to non-monotone VIF values. Garcia et al. (2014) showed some problems with the traditional VIF used in RE. We propose an augmented VIF, VIFR(j,k), associated with RE, which is obtained by standardizing the data before augmenting the model. The VIFR(j,k) will coincide with the VIF associated with the ordinary least squares estimator when k = 0. The augmented VIF has the very desirable properties of being continuous, monotone in the ridge parameter and higher than one.},
  comment   = {Should you standardize variables for ridge regression?  I think this paper says that you have to, so that you can detect collinearity and select the right value for the ridge regression lambda ('k' in this paper).  But this paper is pretty hard to understand.},
  doi       = {10.1111/insr.12099},
  file      = {Garcia15stndrdizVarsColin.pdf:Garcia15stndrdizVarsColin.pdf:PDF},
  keywords  = {Collinearity, linear regression, variance inflator factor, ridge regression, standardization},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2017.02.02},
  url       = {http://dx.doi.org/10.1111/insr.12099},
}

@Article{Alet16caseBetterPVFrcst,
  author    = {Alet, Pierre-Jean and Efthymiou, Venizelos and Graditi, Giorgio and Juel, Mari and Nemac, Franko and Pierro, Marco and Rikos, Evangelos and Tselepis, Stathis and Yang, Guangya and Moser, David and others},
  title     = {The case for better PV forecasting},
  journal   = {Pv-tech Power},
  year      = {2016},
  volume    = {8},
  abstract  = {Rising levels of PV penetration mean increasingly sophisticated forecasting
technologies are needed to maintain grid stability and maximise the economic value of PV
systems. The Grid Integration working group of the European Technology and Innovation Platform
? Photovoltaics (ETIP PV) shares the results of its ongoing research into the advantages and
limitations of current forecasting technologies},
  comment   = {Reference used in GIZ Colombia solar part.},
  file      = {Alet16caseBetterPVFrcst.pdf:Alet16caseBetterPVFrcst.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.27},
}

@Article{Goldberg10extremeRisk,
  author    = {Goldberg, L. and Hayes, M.Y. and Menchero, J. and Mitra, I.},
  title     = {Extreme risk analysis},
  journal   = {The Journal of Performance Measurement},
  year      = {2010},
  volume    = {17},
  abstract  = {Risk analysis involves gaining deeper insight into the sources of risk and evaluating whether these risks accurately
reflect the views of the portfolio manager. In this paper we show how to extend standard volatility analytics to
shortfall, a measure of extreme risk. Using two examples, we show how shortfall provides a more complete and
intuitive picture of risk than value at risk. In two subsequent examples we illustrate the additional perspective
offered by analyzing shortfall and volatility in tandem.},
  comment   = {spinning reserves. Uses some kind of generalized correlation for scenario generation},
  file      = {Goldberg10extremeRisk.pdf:Goldberg10extremeRisk.pdf:PDF},
  groups    = {Test, doReadNonWPV_1},
  owner     = {scot},
  timestamp = {2010.12.14},
}

@Article{Bhat11quantRgrsnTrees,
  author    = {Bhat, HS and Kumar, N and Vaz, G},
  title     = {Quantile Regression Trees},
  journal   = {Submitted, not yet published},
  year      = {2011},
  abstract  = {Robust approaches to data mining form a crucial part of
data mining methods. We propose a quantile regression
tree method that is more robust to outliers than standard
regression trees that use a splitting criterion based on the
sum of squared errors. The splitting criterion we propose
is based on a tilted absolute value loss function that is
naturally associated with empirical quantiles. Least absolute
deviation regression trees are a subcase of this approach.
Since computational time is a major concern with such
methods, we outline an algorithm to reduce the number of
operations required to split a node. We also test quantile
trees on simulated and real data sets and show that the
criterion used can lead to more robust and more accurate
trees than the trees built using a least squares criterion.
Analyzing the residuals from the tests with both real and
simulated data leads to an understanding of when quantile
trees might be expected to outperform standard regression
trees.
Keywords: Regression Trees, Quantile
Estimation, Least Absolute Deviation.},
  comment   = {Regression tree per quantile. Nodes split not on entropy or RMSE, but on quantiles. Might be a way to define local linear neighborhoods? Compare with Meinshausen06qrForests

Submitted but not yet published.},
  file      = {Bhat11quantRgrsnTrees.pdf:Bhat11quantRgrsnTrees.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.03.30},
  url       = {http://faculty.ucmerced.edu/hbhat/publications.html},
}

@InProceedings{Hasager09satNrsWnd,
  author    = {Charlotte Bay Hasager and Alexis Mouche and Merete Bruun Christiansen and Poul Astrup and Morten Nielsen},
  title     = {Satellite winds in EU-Norsewind},
  booktitle = {European Wind Energy Conference and Exhibition (EWEC)},
  year      = {2009},
  abstract  = {Satellite-based wind mapping over the ocean will be contributed in the EU-Norsewind project (2008-2012) with the aim to produce a wind atlas for the Northern European Seas including the Baltic, North and Irish Seas. Furthermore, ground-based observations and atmospheric modeling are major parts of the project. In the present paper the satellite-based wind mapping methodology is described and the implications and perspectives of this part of work. Keywords: offshore wind resource, satellite.},
  comment   = {Data available for NORSEWind * Charlotte recommended this survey of sattelite data candidates for NORSEWInD Satellite * Sensor Microwave/Imager (SSM/I): -- 30 year time series but bad near coast -- passive (SAR/scatterometer are active) -- relevant for trend analysis -- no direction * SAR (five available; will use two) -- ANWSWRS ---- what Riso will use ---- imperfect agreement with NOGAPS system (meso model?) -- SOPRANO ---- what CLS will use ---- from ESA ---- ECMWF winds are the Bayesian prior ---- requires visual inspection SAR vs. Scatterometer * SAR spatial resolution much higher * SAR sample rate slow enough (and non-random) enough to cause diurnal probs (scat must not be non-random either, though) * 100's of SAR temporal samples vs. ~10K for scatterometer * but QuikSCAT died in 2009 *},
  file      = {Hasager09satNrsWnd.pdf:Hasager09satNrsWnd.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.06.15},
  url       = {http://www.norsewind.eu/public/publications.html},
}

@Article{Pinson12scenQualWind,
  author    = {P. Pinson and R. Girard},
  title     = {Evaluating the quality of scenarios of short-term wind power generation},
  journal   = {Applied Energy},
  year      = {2012},
  volume    = {96},
  number    = {0},
  pages     = {12--20},
  issn      = {0306-2619},
  note      = {<ce:title>Smart Grids</ce:title>},
  abstract  = {Scenarios of short-term wind power generation are becoming increasingly popular as input to multistage decision-making problems e.g. multivariate stochastic optimization and stochastic programming. The quality of these scenarios is intuitively expected to substantially impact the benefits from their use in decision-making. So far however, their verification is almost always focused on their marginal distributions for each individual lead time only, thus overlooking their temporal interdependence structure. The shortcomings of such an approach are discussed. Multivariate verification tools, as well as diagnostic approaches based on event-based verification are then presented. Their application to the evaluation of various sets of scenarios of short-term wind power generation demonstrates them as valuable discrimination tools.},
  comment   = {Can tell ensembles from statistical scenarios even though they have the same marginal distributions. Uses an energy score, but this was criticized in Pinson13DiscrimAbilityEnrgyScr

Related: Girard10trajEvalWind},
  doi       = {10.1016/j.apenergy.2011.11.004},
  file      = {Pinson12scenQualWind.pdf:Pinson12scenQualWind.pdf:PDF},
  groups    = {Test, doReadNonWPV_1},
  keywords  = {Renewable energy},
  owner     = {sotterson},
  timestamp = {2013.10.11},
  url       = {http://www.sciencedirect.com/science/article/pii/S0306261911006994},
}

@Article{Pedregosa11ScikitlearnMachLrnPython,
  author   = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  title    = {Scikit-learn: Machine Learning in {P}ython},
  journal  = {Journal of Machine Learning Research},
  year     = {2011},
  volume   = {12},
  pages    = {2825--2830},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algo-
rithms for medium-scale supervised and unsupervised problems. This package focuses on bring-
ing machine learning to non-specialists using a general-purpose high-level language. Emphasis is
put on ease of use, performance, documentation, and API consistency. It has minimal dependen-
cies and is distributed under the simplified BSD license, encouraging its use in both academic
and commercial settings. Source code, binaries, and documentation can be downloaded from
http://scikit-learn.sourceforge.net.
Keywords: Python, supervised learning, unsupervised learning, model selection},
  comment  = {Chapter on Covariance Estimation: Pedregosa16SckikitLearnUserCovEstChap},
  file     = {Pedregosa11ScikitlearnMachLrnPython.pdf:Pedregosa11ScikitlearnMachLrnPython.pdf:PDF},
  url      = {http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html},
}

@Article{Byrn16aucForProbFrcst,
  author        = {Byrne, Simon},
  title         = {A note on the use of empirical AUC for evaluating probabilistic forecasts},
  journal       = {Electron. J. Statist.},
  year          = {2016},
  volume        = {10},
  number        = {1},
  pages         = {380--393},
  __markedentry = {[Scott:1]},
  abstract      = {Scoring functions are used to evaluate and compare partially probabilistic forecasts. We investigate the use of rank-sum functions such as empirical Area Under the Curve (AUC), a widely used measure of classification performance, as a scoring function for the prediction of probabilities of a set of binary outcomes. It is shown that the AUC is not generally a proper scoring function, that is, under certain circumstances it is possible to improve on the expected AUC by modifying the quoted probabilities from their true values. However with some restrictions, or with certain modifications, it can be made proper. },
  comment       = {In general, the AUC isn't proper, but can be improved by adjusting the score.  

Under some constraints, it can be proper, though.
* for example, if the forecaster knows the number of positive events, extreme tail events can be ruled out.},
  doi           = {10.1214/16-EJS1109},
  file          = {:Byrn16aucForProbFrcst.pdf:PDF},
  fjournal      = {Electronic Journal of Statistics},
  publisher     = {The Institute of Mathematical Statistics and the Bernoulli Society},
  url           = {https://doi.org/10.1214/16-EJS1109},
}

@Article{Gneiting07strictPropScore,
  author    = {Gneiting, Tilmann and Raftery, Adrian E},
  title     = {Strictly Proper Scoring Rules, Prediction, and Estimation},
  journal   = {Journal of the American Statistical Association},
  year      = {2007},
  volume    = {102},
  number    = {477},
  pages     = {359--378},
  abstract  = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distributionF if he or she issues the probabilistic forecast F, rather than G F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage.},
  comment   = {Another probabilistic forecast scoring metric w/ huge num. of cites (680 in Oct. 2013). Explains CRPS (but Jonsson13elecPriceQR is more clear)

Is used by Cannon11quantRgrsnNNprecip

CRPS is the absolute error (average?) of a deterministic forecast: can use it to directly compare probabilistic and deterministic forecasts.

I have the tech report returned by google scholar, but not the final journal paper.

Is hacked for censored quantiles in Friederichs07censQRprecip

Also Hersbach00crpsDecomp},
  doi       = {10.1198/016214506000001437},
  eprint    = {http://amstat.tandfonline.com/doi/pdf/10.1198/016214506000001437},
  file      = {Tech Report:Gneiting07strictPropScore_TR.pdf:PDF},
  groups    = {Test, doReadWPV_1},
  owner     = {sotterson},
  timestamp = {2013.10.23},
}

@TechReport{Schwarz12unifSplnFrmwrk,
  author      = {Schwarz, Katsiaryna and Krivobokova, Tatyana},
  title       = {A unified framework for spline estimators},
  institution = {Courant Research Centre: Poverty, Equity and Growth-Discussion Papers},
  year        = {2012},
  abstract    = {sdfafsd
This article develops a unified framework to study the (asymptotic) properties of all
(periodic) spline based estimators, that is of regression, penalized and smoothing
splines. The explicit form of the periodic Demmler-Reinsch basis of general degree in
terms of exponential splines allows to derive the exact expression for the equivalent
kernel of all spline estimators simultaneously The corresponding bandwidth, which
drives the asymptotic behavior of spline estimators, is shown to be a function of both --
the number of knots and the smoothing parameter. A strategy for the optimal
bandwidth selection is discussed.

Key words and phrases: B-splines; Equivalent kernels; Euler-Frobenius polynomials;
Exponential splines; Demmler-Reinsch basis.},
  comment     = {Generalized, penalized splines, especially periodic w/ some kinda asymtopic Fourier trick. but still pick number knots by CV. Maybe good to come back to some time...

Note that this paper is a 2013 version. I couldn't figure out where it was published.},
  file        = {Schwarz12unifSplnFrmwrk.pdf:Schwarz12unifSplnFrmwrk.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2014.11.14},
  url         = {http://www.econstor.eu/handle/10419/90505},
}

@InProceedings{Vasko02estNumSegsPerm,
  author    = {Kari T. Vasko and Hannu T. T. Toivonen},
  title     = {Estimating the number of segments in time series data using permutation tests},
  booktitle = {International Conference on Data Mining (ICDM)},
  year      = {2002},
  pages     = {466--473},
  abstract  = {Segmentation is a popular technique for discovering structure in time series data. We address the largely open problem of estimating the number of segments that can be reliably discovered. We introduce a novel method for the problem, called Pete. Pete is based on permutation testing. The problem is an instance of model (dimension) selection. The proposed method analyzes the possible overfit of a model to the available data rather than uses a term for penalizing model complexity. In this respect the approach is more similar to cross-validation than regularization based techniques (e.g., AIC, BIC, MDL, MML). Further, the method produces a pvalue for each increase in the number of segments. This gives the user an overview of the statistical significance of the segmentations. We evaluate the performance of the proposed method using both synthetic and real time series data. The experiments show that permutation testing gives realistic results about the number of reliably identifiable segments and that it compares favorably with the Monte Carlo cross-validation (MCCV) and commonly used BIC criteria.},
  comment   = {Stopping criteria for time series clustering; use permutations to see if regression error reduction is b/c are segmenting noise * use for figuring out num. of regimes?},
  file      = {Vasko02estNumSegsPerm.pdf:Vasko02estNumSegsPerm.pdf:PDF;Vasko02estNumSegsPerm.pdf:Vasko02estNumSegsPerm.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.03.02},
}

@Misc{Wikipedia19sensitivAndSpecif,
  author       = {Wikipedia},
  title        = {Sensitivity and specificity},
  howpublished = {Wikipedia},
  year         = {2019},
  abstract     = {Sensitivity and specificity are statistical measures of the performance of a binary classification test, also known in statistics as a classification function, that are widely used in medicine:

* Sensitivity (also called the true positive rate, the recall, or probability of detection[1] in some fields) measures the proportion of actual positives that are correctly identified as such (e.g., the percentage of sick people who are correctly identified as having the condition).

* Specificity (also called the true negative rate) measures the proportion of actual negatives that are correctly identified as such (e.g., the percentage of healthy people who are correctly identified as not having the condition).},
  comment      = {Sensitivity and Specificity are two binary classification metrics with a lot of aliases

Sensitivity = proportion of actual positives correctly detected
a.k.a. true postive rate = recall = probability of detection

Specificity = proportion of actual negatives correctly detected
a.k.a. true negative rate},
  file         = {:Wikipedia19sensitivAndSpecif.pdf:PDF},
  url          = {https://en.wikipedia.org/wiki/Sensitivity_and_specificity},
}

@InProceedings{Hinneburg06onlineDFT,
  author    = {A. Hinneburg and D. Habich and M. Karnstedt},
  title     = {Analyzing Data Streams by Online {DFT}},
  booktitle = {International Workshop on Knowledge Discovery from Data Streams (IWKDDS)},
  year      = {2006},
  pages     = {67--76},
  abstract  = {Sensor data have become very huge and single measures are produced at high rates, resulting in streaming sensor data. In this paper, we present a new mining tool called Online DFT, which is particularly powerful for estimating the spectrum of a data stream. Unique features of our new method include its low update complexity with high-accuracy estimations for very long periods, and its ability of long-range forecasting based on our Online DFT. Furthermore, we describe some applications of our Online DFT.},
  comment   = {Detects periodicities online (useful for operational system). Careful attention to DFT boundaries. Can help with longrange forecast},
  file      = {Hinneburg06onlineDFT.pdf:Hinneburg06onlineDFT.pdf:PDF;Hinneburg06onlineDFT.pdf:Hinneburg06onlineDFT.pdf:PDF},
  location  = {Berlin, Germany},
  url       = {http://www.tu-ilmenau.de/fakia/Marcel-Karnstedt.8818.0.html},
}

@Article{Anagnostopoulos10streamCov,
  author    = {Anagnostopoulos, Christoforos and Adams, Niall M. and Hand, David J.},
  title     = {Streaming Covariance Selection with Applications to Adaptive Querying in Sensor Networks},
  journal   = {The Computer Journal},
  year      = {2010},
  abstract  = {Sensor networks can be naturally represented as graphical models, where the edge set encodes the presence of sparsity in the correlation structure between sensors. Such graphical representations can be valuable for information mining purposes as well as for optimizing bandwidth and battery usage with minimal loss of estimation accuracy. We use a computationally efficient technique for estimating sparse graphical models which fits a sparse linear regression locally at each node of the graph via the Lasso estimator. Using a recently suggested online, temporally adaptive implementation of the Lasso, we propose an algorithm for streaming graphical model selection over sensor networks. With battery consumption minimization applications in mind, we use this algorithm as the basis of an adaptive querying scheme. We discuss implementation issues in the context of environmental monitoring using sensor networks, where the objective is short-term forecasting of local wind direction. The algorithm is tested against real UK weather data and conclusions are drawn about certain tradeoffs inherent in decentralized sensor networks data analysis.},
  comment   = {adaptive lasso w/ missing feature handling},
  doi       = {10.1093/comjnl/bxp123},
  eprint    = {http://comjnl.oxfordjournals.org/content/early/2010/01/08/comjnl.bxp123.full.pdf+html},
  file      = {Anagnostopoulos10streamCov.pdf:Anagnostopoulos10streamCov.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.09.27},
  url       = {http://comjnl.oxfordjournals.org/content/early/2010/01/08/comjnl.bxp123.abstract},
}
???

@Article{Mitchell05covSpcTimeSepTest,
  author    = {Mitchell, Matthew W. and Genton, Marc G. and Gumpertz, Marcia L.},
  title     = {Testing for separability of space-time covariances},
  journal   = {Environmetrics},
  year      = {2005},
  volume    = {16},
  number    = {8},
  pages     = {819--831},
  issn      = {1099-095X},
  abstract  = {Separable space-time covariance models are often used for modeling in environmental sciences because of their computational benefits. Unfortunately, there are few formal statistical tests for separability. We adapt a likelihood ratio test based on multivariate repeated measures to the spatio-temporal context. We apply this test to an environmental monitoring data set. Copyright  2005 John Wiley & Sons, Ltd.},
  comment   = {Tests to see if cov matrix is really space/time separable, as in matrix normal models (e.g. Anderlucci15covPatMix).  And also as in precision matrices too, like in Tastu15spcTimeTrajGaussCpla ?},
  doi       = {10.1002/env.737},
  file      = {:papers\\Mitchell05covSpcTimeSepTest.pdf:PDF},
  keywords  = {elevated CO2, FACE, Kronecker product, separable space-time covariance},
  owner     = {sotterson},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2017.07.11},
  url       = {http://dx.doi.org/10.1002/env.737},
}

@Article{Sweeney11frcstCombo,
  author    = {Sweeney, Conor P and Lynch, Peter and Nolan, Paul},
  title     = {Reducing errors of wind speed forecasts by an optimal combination of post-processing methods},
  journal   = {Meteorological Applications},
  year      = {2011},
  abstract  = {Seven adaptive approaches to post-processing wind speed forecasts are discussed and compared. Forecasts of the wind speed over 48 h are run at horizontal resolutions of 7 and 3 km for a domain centred over Ireland. Forecast wind speeds over a 2 year period are compared to observed wind speeds at seven synoptic stations around Ireland and skill scores calculated. Two automatic methods for combining forecast streams are applied. The forecasts produced by the combined methods give bias and root mean squared errors that are better than the numerical weather prediction forecasts at all station locations. One of the combined forecast methods results in skill scores that are equal to or better than all of its component forecast streams. This method is straightforward to apply and should prove bene?cial in operational wind forecasting. KEY WORDS adaptive post-processing; numerical weather prediction; Kalman filter; artificial neural network},
  comment   = {Simple individual NWP MOS corrections get best result in average error-weighted mean, but maybe NN could have been done better. These techniques could be useful for future feature selection experiments, or for generating new ensembles for a probabilistic forecast.

* trying to correct 1-48 hour ahead COSMO NWP wind speed forecasts at 14 10m met stations.
* authors read and use the time trick in SalcedoSanz09frcstBankNN but did not use the 2 hidden layer NN structure found to be best in that paper (maybe b/c it wasn't really better?)
* is perhaps further advanced in: Courtney13frcstBayAvg (Sweeney is coauthor)

Individual corrections:

1. STB: rolling window bias correction (30 day sliding window, like most of the simple methods)
2. DRL: hour-dependent bias correction -- Compared to STB, DRL reduces bias but not RMSE (skill)!
3. LLS: linear least squared bias/scale correction
4. KAL: Kalman
-- Kalman doesn't beat sliding window LLS
5. MAV: mean/variance adjustment (a little like Jan's prob. forecast ensemble calibration)
-- seems like mean s/ have been subtracted before variance scaling!!
-- Mean/variance correction doesn't improve RMSE over even simple bias correction!
6. DIR: binned directional bias correction
-- Directional bias correction helps on certain hilly sites.
-- surprisingly good
7. ANN: standard neural nework
-- 6 inputs: speed/dir, temp, frcst hour (NWP hours ahead), cos/sin projection of time (S1,S2)
--- why these inputs? (see my comments in pdf)
--- time cos/sin projection from SalcedoSanz09frcstBankNN: that paper doesn't explain them either.
-- trained every day over a 30 day window
-- Interesting that direction wasn't projected to two components but time was (S1, S2)
-- ANN better than simple bias correction at most sites.

Individual correction results summary
* all methods reduce bias
* RMSE captures forecast skill, is a better way to measure than bias
* LLS and DIR are best at the same or more sites than ANN, and LLS has lowest LSE across all stations. This shouldn't happen, I think, so there must have been some training problems.

Combined forecasts

8. ANNCOM: inputs NWP at two spatial resultions w/ the 7 simple corrections (16 inputs)
-- 30 days of training
-- n hidden nodes shrunk from 2 to 1
9. MSECOM: simple weighted error-weighted average of same 16 inputs
-- only a 2 day window used Combined forecast results summary
* Simple linear least squares combo of the 7 methods is better than NN combo, although both are better than the single methods.
* no ANN feature selection performed
* I suspect that including things like direction in combining ANN would have improved its results. Newsletter also discusses this: Sweeney12cosmoPostProc},
  file      = {Sweeney11frcstCombo.pdf:Sweeney11frcstCombo.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2013.02.20},
  url       = {http://mathsci.ucd.ie/~plynch/Publications/MetApps01.pdf},
}

@Article{Nikolakakis11optWindSolMix,
  author                     = {Nikolakakis, Thomas and Fthenakis, Vasilis},
  title                      = {The optimum mix of electricity from wind- and solar-sources in conventional power systems: Evaluating the case for New York State},
  journal                    = {ENERGY POLICY},
  year                       = {2011},
  volume                     = {39},
  number                     = {11},
  pages                      = {6972--6980},
  month                      = nov,
  issn                       = {0301-4215},
  abstract                   = {{Several countries and states have set targets for substantially increasing renewable energy (RE) contributions in their electricity grids. As the potential for additional hydro-electricity is limited in the US most future RE penetration is envisioned to be in the form of wind and solar. Our simulations, based on hourly resource and load data, demonstrate the maximum penetration achievable in the grid managed by the New York Independent System Operator (NYISO), by wind- and solar-power independently, and when they are combined. By optimizing the synergy between these two intermittent resources, a maximum penetration of renewable-energy in the grid can be accomplished; this is shown for different scenarios of grid flexibility. For example, for an 80\% flexible grid, a total penetration of 30\% of wind and solar energy can be achieved in the NY state without adding storage and without having to dump more than 3\% energy, whereas if this was to be met by wind alone, 12\% of energy would have to be dumped. Considering that several US states and countries have high targets for renewable energy penetration, optimizing the mixture of RE to accomplish such goals is valuable for energy managing and planning. (C) 2011 Elsevier Ltd. All rights reserved.}},
  affiliation                = {{Fthenakis, V (Reprint Author), Columbia Univ, Dept Earth \& Environm Engn, Ctr Life Cycle Anal, New York, NY 10027 USA. Nikolakakis, Thomas; Fthenakis, Vasilis, Columbia Univ, Dept Earth \& Environm Engn, Ctr Life Cycle Anal, New York, NY 10027 USA. Fthenakis, Vasilis, Brookhaven Natl Lab, Sustainable Energy Technol Dept, Natl Photovolta Environm Res Ctr, Upton, NY 11973 USA.}},
  author-email               = {{vmf5\@columbia.edu}},
  comment                    = {New York State can provide 27\% of its power from wind and solar, with conventional backup and no storage.
* very conventional flexibility case:
-- 70\% conventional flexibility (30\% is always on),
-- 10\% limit to curtailment
-- wind alone: 18\% penetration.
-- PV alone: 12\%.
-- optimal combo: 27\%
* optimal PV/wind mix was e.g. 14.5GW PV and 11.1GW wind (9\% curtailment; 25\% penetration)
* With 100\% system flexibility, 20\% curtailment: ~69\% penetration


Compare with Hart and Jacobsen's 80\% C02 reduction scenario for California (Hart11genProfLrgVarRen in my bibtex) and IEEE intermittency issue.},
  doc-delivery-number        = {{862US}},
  doi                        = {{10.1016/j.enpol.2011.05.052}},
  file                       = {Nikolakakis11optWindSolMix.pdf:Nikolakakis11optWindSolMix.pdf:PDF},
  funding-acknowledgement    = {{New York State Energy Research and Development Authority (NYSERDA)}},
  funding-text               = {{This work was supported in part from the New York State Energy Research and Development Authority (NYSERDA).}},
  groups                     = {Read},
  journal-iso                = {{Energy Policy}},
  keywords                   = {{Solar; Wind; Grid integration}},
  keywords-plus              = {{RADIATION}},
  language                   = {{English}},
  location                   = {{THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND}},
  number-of-cited-references = {{27}},
  owner                      = {sotterson},
  publisher                  = {ELSEVIER SCI LTD},
  subject-category           = {{Energy \& Fuels; Environmental Sciences \& Ecology}},
  times-cited                = {{1}},
  timestamp                  = {2012.07.08},
  type                       = {{Article}},
  unique-id                  = {{ISI:000298120200028}},
  web-of-science-category    = {{Energy \& Fuels; Environmental Sciences; Environmental Studies}},
}

@Article{DeGiorgi11nwpStatWavltNN,
  author    = {Maria Grazia De Giorgi and Antonio Ficarella and Marco Tarantino},
  title     = {Assessment of the benefits of numerical weather predictions in wind power forecasting based on statistical methods},
  journal   = {Energy},
  year      = {2011},
  volume    = {36},
  number    = {7},
  pages     = {3968--3978},
  issn      = {0360-5442},
  abstract  = {Several forecast systems based on Artificial Neural Networks have been developed to predict power production of a wind farm located in a complex terrain, where geographical effects make wind speed predictions difficult) in different time horizons: 1,3,6,12 and 24 h. In the first system, the neural network has been used only as a statistic model based on time series of wind power; later it has been integrated with numerical weather predictions, by which an interesting improvement of the performance has been reached, especially with the longer time horizons. In particular, a sensitivity analysis has been carried out in order to find those numerical weather parameters with the best impact on the forecast. Then, after the implementation of forecast systems based on a single ANN, the two best prediction systems individuated through the sensitivity analysis, have been employed in a hybrid approach, made up of three different ANNs. Besides, a prediction system based on the wavelet decomposition technique has been also carried out in order to evaluate its contribute on the forecast performance in two time horizons (1 and 24 h). The error of the different forecast systems is investigated and the statistical distributions of the error are calculated and presented. Highlights * In a complex terrain a sensitivity analysis is necessary to choose the best NWP configuration. * Combining more ANNs for the same prediction can improve the performance. * Combining an ANN with the wavelet decomposition can improve the performance. Keywords Forecasting; Wind power; Artificial neural networks; Wavelet decomposition; Numerical weather predictions},
  comment   = {Interesting because:
* NN ouputs are integrated into NWP, increasing longer horizon forecast accuracy (Ewelina inverse operator?)
* Wavelets used, tested on day aheard (AGP Austrian TSO idea).},
  doi       = {10.1016/j.energy.2011.05.006},
  file      = {DeGiorgi11nwpStatWavltNN.pdf:DeGiorgi11nwpStatWavltNN.pdf:PDF},
  keywords  = {Forecasting},
  owner     = {sotterson},
  timestamp = {2013.03.13},
  url       = {http://www.sciencedirect.com/science/article/pii/S0360544211003215},
}

@Article{Zanakis98multiAttrDcsnCmpr,
  author    = {Zanakis, Stelios H and Solomon, Anthony and Wishart, Nicole and Dublish, Sandipa},
  title     = {Multi-attribute decision making: a simulation comparison of select methods},
  journal   = {European journal of operational research},
  year      = {1998},
  volume    = {107},
  number    = {3},
  pages     = {507--529},
  abstract  = {Several methods have been proposed for solving multi-attribute decision making problems (MADM). A major criticism
of MADM is that different techniques may yield different results when applied to the same problem. The problem
considered in this study consists of a decision matrix input of N criteria weights and ratings of L alternatives on each
criterion. The comparative performance of some methods has been investigated in a few, mostly field, studies. In this
simulation experiment we investigate the performance of eight methods: ELECTRE, TOPSIS, Multiplicative Exponential-
Weighting (MEW), Simple Additive Weighting (SAW), and four versions of AHP (original vs. geometric scale and right
eigenvector vs. mean transformation solution). Simulation parameters are the number of alternatives, criteria and their
distribution. The solutions are analyzed using twelve measures of similarity of performance. Similarities and differences in
the behavior of these methods are investigated. Dissimilarities in weights produced by these methods become stronger in
problems with few alternatives; however, the corresponding final rankings of the alternatives vary across methods more in
problems with many alternatives. Although less significant, the distribution of criterion weights affects the methods
differently. In general, all AHP versions behave similarly and closer to SAW than the other methods. ELECTRE is the least
similar to SAW (except for closer matching the top-ranked alternative), followed by MEW. TOPSIS behaves closer to AHP
and differently from ELECTRE and MEW, except for problems with few criteria. A similar rank-reversal experiment
produced the following performance order of methods: SAW and MEW (best), followed by TOPSIS, AHPs and ELECTRE.
It should be noted that the ELECTRE version used was adapted to the common MADM problem and therefore it did not
take advantage of the method’s capabilities in handling problems with ordinal or imprecise information. © 1998 Elsevier
Science B.V.
Keywords: Multiple criteria analysis; Decision theory; Utility theory; Simulation},
  comment   = {The main reference a lot of people use in predictive maintenance.  I thought it has sometime to do with the Wishart charts that Peter Deeskow (STEAG, and ModernWindABS) uses, but now I'm not sure.},
  file      = {:Zanakis98multiAttrDcsnCmpr.pdf:PDF},
  publisher = {Elsevier},
  url       = {https://www.researchgate.net/publication/215653478_Multi-attribute_decision_making_A_simulation_comparison_of_select_methods},
}

@Article{Mofrad10wavletShape,
  author    = {Mofrad, B. and Tehrani-Fard, A. and Zoroofi, A. and Akhlaghpoor, S. and Chen, Y.W.},
  title     = {A Novel Wavelet Based Multi-Scale Statistical Shape Model-Analysis for the Liver Application: Segmentation and Classification},
  journal   = {Current Medical Imaging Reviews},
  year      = {2010},
  volume    = {6},
  number    = {3},
  pages     = {145--155},
  abstract  = {Several methods have been proposed to construct Statistical Shape Model (SSM) to aim image analysis using computer in field Computer Aided Diagnosis (CAD), Computer Assisted Surgery (CAS), and other medical applications by providing a prior knowledge. The major challenge for liver shape model is a high variation in geometry such as size, shape and volume between livers. In this paper, we have presented a new technique for the automatic Multi-Scale Statistical Shape Model (MS-SSM) of three-dimensional (3-D) liver from volumetric segmented images data. The procedure included both building of Spherical Harmonics shape description and the Wavelet transform. Principal Component Analysis (PCA) was applied to corresponding landmarks on the tanning shapes for performing leave-one-out test. Validation metrics, for comparing performances of the MS-SSM method against SSM, were the Hausdorff distance measure and statistical parameter of Dice Similarity Coefficient (DSC). We evaluated the performance of our proposed method against to traditional method. The results confirmed that the proposed MS-SSM technique was successful, and more accurate for liver domain. We also examined robustness of the method in liver classification. In this research classification was performed on feature vector obtained from PCA using Support Vector Machine (SVM) and k-Nearest Neighbors (k-NN) classifiers. Diagnostic accuracy was determined by leave-one-out cross-validation method and Receiver Operating Characteristic (ROC) analysis for each observer. The results showed that our proposed method to be more accurate and robust for liver discrimination. Keywords: Statistical shape modeling; liver; wavelet transform; shape-base classification; PCA},
  comment   = {For lagged wind velocity basis? * I couldn't get the pdf via DTU},
  owner     = {scot},
  publisher = {Bentham Science Publishers},
  timestamp = {2010.09.06},
  url       = {http://www.ingentaconnect.com/content/ben/cmir/2010/00000006/00000003/art00003},
}

@InProceedings{Cofino02BNweathPred,
  author    = {Antonio S. Cofi{\~n}o and Rafael Cano and Carmen Sordo and Jos{\'e} M. Guti{\'e}rrez},
  title     = {{Bayes}ian networks for probabilistic weather prediction},
  booktitle = {European Conference on Artificial Intelligence (ECAI)},
  year      = {2002},
  pages     = {695--699},
  publisher = {Press},
  abstract  = {Several standard approaches have been introduced for meteorological time series prediction (analog techniques, neural networks, etc.). However, when dealing with multivariate spatially distributed time series (e.g., a network of meteorological stations over the Iberian peninsula) the above methods do not consider all the available information (they consider special independency assumptions to simplify the model). In this work, we introduce Bayesian Networks (BNs) in this framework to model the spatial and temporal dependencies among the different stations using a directed acyclic graph. This graph is learnt from the available databases and allows deriving a probabilistic model consistent with all the available information. Afterwards, the resulting model is combined with numerical atmospheric predictions which are given as evidence for the model. Efficient inference mechanisms provide the conditional distributions of the desired variables at a desired future time. We illustrate the efficiency of the proposed methodology by obtaining precipitation forecasts for 100 stations in the North basin of the Iberian peninsula during Winter 1999. We show how standard analog techniques are a special case of the proposed methodology when no spatial dependencies are considered in the model.},
  comment   = {Bayesian networks learn wind met mast spatio-temporal dependencies. Could use for probabilistic forecast feature selection since connects describe joint distribution. Also, the error distribution is built into the model.},
  file      = {Cofino02BNweathPred.pdf:Cofino02BNweathPred.pdf:PDF},
  groups    = {ErrDistProps, doReadWPV_2},
  owner     = {scot},
  timestamp = {2010.08.02},
}

@Article{Bengio04OutOfSmplExt,
  author    = {Bengio, Yoshua and Paiement, Jean-Fran{\c{c}}ois and Vincent, Pascal and Delalleau, Olivier and Le Roux, Nicolas and Ouimet, Marie},
  title     = {Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clustering},
  journal   = {Advances in neural information processing systems},
  year      = {2004},
  volume    = {16},
  pages     = {177--184},
  abstract  = {Several unsupervised learning algorithms based on an eigendecomposition
provide either an embedding or a clustering only for given training
points, with no straightforward extension for out-of-sample examples
short of recomputing eigenvectors. This paper provides a unified framework
for extending Local Linear Embedding (LLE), Isomap, Laplacian
Eigenmaps, Multi-Dimensional Scaling (for dimensionality reduction)
as well as for Spectral Clustering. This framework is based on seeing
these algorithms as learning eigenfunctions of a data-dependent kernel.
Numerical experiments show that the generalizations performed have a
level of error comparable to the variability of the embedding algorithms
due to the choice of training data.},
  comment   = {How to do MDS on unseen training data. Also for: LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering. Could also be a way to adapt these online (somehow)

* MDS extension in eq. (14)
 -- This is what's used in the isomap out-of-sample function in the Matlab Toolbox for Dimensionality Reduction
(Maaten09DimRedCmprTechNote).
 -- says there is an older algorithm that requires 'adding a dimension': Gower68addPtMultivar

Online adaptation: would need to figure out how to subtract an old point that was used in the original MDS or whatever eigenvalue calculation. Could this be done with Matthew Brand Mitsubishi incremental SVD idea?},
  file      = {Bengio04OutOfSmplExt.pdf:Bengio04OutOfSmplExt.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.06.26},
  url       = {http://books.google.com/books?hl=en&amp;lr=&amp;id=0F-9C7K8fQ8C&amp;oi=fnd&amp;pg=PA177&amp;dq=Out-of-sample+extensions+forLLE,+Isomap,+MDS,+Eigenmaps+and+spectral+clustering&amp;ots=THDtr0X93_&amp;sig=vOKWUzXLyFHOZ1-yZnGBmm8Fos4},
}

@Article{Gooding13cityPVinstallFactors,
  author   = {James Gooding and Holly Edwards and Jannik Giesekam and Rolf Crook},
  title    = {Solar City Indicator: A methodology to predict city level PV installed capacity by combining physical capacity and socio-economic factors},
  journal  = {Solar Energy},
  year     = {2013},
  volume   = {95},
  pages    = {325 - 335},
  issn     = {0038-092X},
  abstract = {Shifting to renewable sources of electricity is imperative in achieving global reductions in carbon emissions and ensuring future energy security. One technology, solar photovoltaics (PV), has begun to generate a noticeable contribution to the electricity mix in numerous countries. However, the upper limits of this contribution have not been explored in a way that combines both building-by-building solar resource appraisals with the city-scale socio-economic contexts that dictate PV uptake. This paper presents such a method, whereby a ‘Solar City Indicator’ is calculated and used to rank cities by their capacity to generate electricity from roof-mounted PV. Seven major UK cities were chosen for analysis based on available data; Dundee, Derby, Edinburgh, Glasgow, Leicester, Nottingham and Sheffield. The physical capacity of each city was established using a GIS-based methodology, exploiting digital surface models and LiDAR data, with distinct methodologies for large and small properties. Socio-economic factors (income, education, environmental consciousness, building stock and ownership) were chosen based on existing literature and correlation with current levels of PV installations. These factors were enumerated using data that was readily available across each city. Results show that Derby has the greatest potential of all the cities analysed, as it offers both good physical and socio-economic potential. In terms of physical capacity it was seen that over a 15year payback period there are two plateaus, showing a marked difference in viability between small and large PV arrays. It was found that both the physical and socio-economic potential of a city are strongly influenced by the nature of the local building stock. This study also identifies areas where policy needs to be focused in order to encourage uptake and highlights factors limiting maximum PV uptake. While this methodology has been demonstrated using UK cities, it is equally applicable to any country where city data is available.},
  doi      = {https://doi.org/10.1016/j.solener.2013.06.027},
  keywords = {Photovoltaics, Renewable energy, GIS, Potential quantification, Resource appraisal},
  url      = {http://www.sciencedirect.com/science/article/pii/S0038092X13002570},
}

@Article{Roy14perfPredPitchTurbineVar,
  author    = {Roy, Sanjoy},
  title     = {Performance prediction of active pitch-regulated wind turbine with short duration variations in source wind},
  journal   = {Applied Energy},
  year      = {2014},
  volume    = {114},
  pages     = {700--708},
  abstract  = {Short duration wind variations affect real time performance of active pitch-regulated wind turbines in two ways as evident from reported experimental and empirical studies. First the mean output power, which may be referred to as the short duration output power, differs significantly from the corresponding zero-turbulence value obtained with ideal source wind streamlines. Second, random variation of output around the mean value appears with a significant standard deviation; the normalised value of which is referred to as the short duration variability. In this paper, analytical interpretation of both metrics is presented under assumption of two-parameter Weibull statistics for short duration wind variations. Statistical estimates for the metrics are presented for conditions described by the well known IEC 61400-1 Standards. Finally the statistical estimation procedure is applied to a Vestas V90 3 MW zero-turbulence output curve as an illustrative application example.
Keywords

 Wind energy;
 Weibull statistics;
 Pitch-regulated turbines;
 Short duration power distortion

Highlights

* Uses turbulence intensity as a parametric measure of short duration wind variations.
* Derives statistical expression for the short duration output power curve for a WECS.
* Derives statistical expression for the short duration output power covariance for a WECS.
* Establishes algorithm for computation of short duration output power variability.
* Compares statistical estimates according to IEC 1400-1 with empirical data.},
  comment   = {Might explain why turbulence intensity is useful for wind power forecasting in physical terms. Might also be interesting for possible power estimation, as it desciribes the shor term variability of wind power, somehow (pdf ordered from subito).},
  file      = {Roy14perfPredPitchTurbineVar.pdf:Roy14perfPredPitchTurbineVar.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.11.18},
  url       = {http://www.sciencedirect.com/science/article/pii/S0306261913008295},
}

@InProceedings{Lange02windPowShrtTrmMetCond,
  author    = {Lange, Matthias and Heinemann, Detlev},
  title     = {Accuracy of short term wind power predictions depending on meteorological conditions},
  booktitle = {CD-Proc. of the 2002 Global Windpower Conference, Paris, France},
  year      = {2002},
  abstract  = {Short term wind power predictions should provide two types of information: The expected power output of wind turbines
and the expected uncertainty of this prediction. So far, the uncertainty is commonly given by annual averages such as the
root mean square error. But the prediction error depends on the complexity of the prevailing meteorological situation and,
therefore, should be newly assessed for each individual prediction. We investigate the impact of meteorological conditions
on the prediction accuracy. As a first approach we consider the wind speed and find that the uncertainty of the wind speed
prediction does only weakly depend on the magnitude of the predicted wind speed. Moreover, we derive a method to model
the uncertainty of a specific power prediction in terms of the power curve and the mean error of the underlying wind speed
prediction. Using an existing weather classification scheme we relate the prediction error of the wind speed to the overall
weather situation. While for a number of sites the prediction uncertainty is significantly lower in weather conditions dominated
by high pressure than in low pressure situations other sites do not show this effect.},
  comment   = {Giebel05windEnsemble says that this paper shows how to get uncertaintly information from the temporal development of lagged ensembles. Also, a try at predicting error with weather categories. It's not clear if they saw something with pressure (abstract is vague).},
  file      = {Lange02windPowShrtTrmMetCond.pdf:Lange02windPowShrtTrmMetCond.pdf:PDF},
  groups    = {Ensemble, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2014.07.16},
}

@InProceedings{Dobschinski09offsiteShortest,
  author    = {J. Dobschinski and B. Lange and A. Wessel},
  title     = {Integration of offsite wind speed measurements in shortest-term wind power prediction systems},
  booktitle = {International Workshop on Large Scale Integration of Wind Power into Power Systems as well as on Transmission Networks for Offshore Wind Farms},
  year      = {2009},
  abstract  = {Shortest-term wind power forecasts of an individual wind farm are normally based on numerical weather predictions (NWP) and respective power measurements of the recent past. In this study offsite wind measurements has been investigated to improve the shortestterm forecast of wind power and to setup an alternative forecast system if measured wind power from the predicted wind farm is temporal not available. Especially at high wind speeds the offsite wind measurement provide additional information for the forecast system, due to the fact that the constant power output of the wind farm running at nominal power provide no information for the forecast system. Wind speed measurements from the ISET wind measurement network are investigated with respect to additional information content concerning the forecast of a single wind farm. Based on a correlation analysis between offsite wind and wind farm power measurements depending on spatial distance and time lag an individual offsite wind measurement has been allocated for each wind farm. The corresponding wind measurement is included in the neural network based forecast system from the ISET. As expected the study reveals a minor improvement of the global forecast error but a significant one at higher wind speed levels. However the exclusive use of offsite wind measurements only in combination with NWP-models leads to a satisfying forecast quality and shows the ability to substitute forecasts based on recent power measurements in case of breakdown.},
  comment   = {Arne's correlation paper claiming that offsite observations are too far away to do much good.},
  file      = {Dobschinski09offsiteShortest.pdf:Dobschinski09offsiteShortest.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.09.28},
}

@Conference{Wessel09offsiteMastFrcst,
  author    = {A. Wessel and J. Dobschinski and B. Lange},
  title     = {Integration of offsite wind speed measurements in shortest-term wind power prediction systems},
  booktitle = {8\textsuperscript{th} International Workshop on Large Scale Integration of Wind Power into Power Systems as well as on Transmission Networks for Offshore Wind Farms},
  year      = {2009},
  month     = oct,
  abstract  = {Shortest-term wind power forecasts of an
individual wind farm are normally based on numerical weather
predictions (NWP) and respective power measurements of the
recent past. In this study offsite wind measurements has been
investigated to improve the shortest-term forecast of wind
power and to setup an alternative forecast system if measured
wind power from the predicted wind farm is temporal not
available. Especially at high wind speeds the offsite wind
measurement provide additional information for the forecast
system, due to the fact that the constant power output of the
wind farm running at nominal power provide no information
for the forecast system.
Wind speed measurements from the ISET wind
measurement network are investigated with respect to
additional information content concerning the forecast of a
single wind farm. Based on a correlation analysis between
offsite wind and wind farm power measurements depending on
spatial distance and time lag an individual offsite wind
measurement has been allocated for each wind farm. The
corresponding wind measurement is included in the neural
network based forecast system from the ISET.
As expected the study reveals a minor improvement of the
global forecast error but a significant one at higher wind speed
levels. However the exclusive use of offsite wind measurements
only in combination with NWP-models leads to a satisfying
forecast quality and shows the ability to substitute forecasts
based on recent power measurements in case of breakdown.
Index Terms?Short Term Forecast, offsite measurements},
  comment   = {Arne's paper on adding external masts to improve forecasting (I think this was for the all of Germany forecast). Single correlation approach yeilded improvements in rare high wind speeds but no much (any?) improvement overall. Also reduced bias. Also a reference for the IWES german met mast measurement network.

Not a great approach b/c it's univariate, and makes a linearlity assumption.},
  file      = {Wessel09offsiteMastFrcst.pdf:Wessel09offsiteMastFrcst.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2012.09.24},
}

@InProceedings{Grimit11balanceRampFrcst,
  author    = {Eric P. Grimit and K. Larson and J. Lerner and M. T. Stoelinga},
  title     = {Making energy balancing decisions based on very uncertain wind power ramp forecasts},
  booktitle = {American Meteorological Society Annual Meeting},
  year      = {2011},
  month     = jan,
  abstract  = {Short-range (0-6 h) forecasts of wind power ramp events have become an essential requirement for energy balancing authorities to efficiently manage the electric power system and meet NERC control performance standards. Yet predictions of ramp events have minimal skill and contain large uncertainties. Fledgling validation studies of state-of-art ramp event forecast guidance show very low hit rates, no matter the modeling techniques and ramp event definitions used, even when allowing for small timing errors. At great expense, balancing authorities are staffing wind forecast desks 24/7/365, structuring financial hedges, and adopting persistent wind curtailment strategies to mitigate or even eliminate the risk. To lower these costs and drive efficiency, better energy balancing decisions can be made by considering the actual ramp event risk in tandem with the external market signals and transmission constraints. Guidance on wind power ramp events is needed in probabilistic form in order to quantify the forecast uncertainty and afford the user the opportunity to take action only when the estimated risk is above the user's tolerance for it. Specific examples are shown of tuning a best-estimate forecast based on an underlying forecast ensemble of wind power ramps at a U.S. wind farm over a several month period. The tradeoffs between frequency bias, hit rate, false alarm rate, miss rate, and misclassification rate are examined by maximizing contingency table scores such as the equitable threat score and the Gerrity Skill Score. In the end, the metric to be optimized depends on each user's cost for missed events and tolerance for false alarms, which likely varies substantially by market and power system conditions.},
  comment   = {ramp forecasting, claims to be tunable to operators cost how? I asked for the paper.},
  groups    = {Test, Use, doReadWPV_2},
  owner     = {scot},
  timestamp = {2011.05.02},
  url       = {http://ams.confex.com/ams/91Annual/webprogram/Paper186594.html},
}

@Article{Pinson09probFrcstStatScenWind,
  author    = {P. Pinson and G. Papaefthymiou and B. Kl\"{o}ckl and H. Aa. Nielsen and H. Madsen},
  title     = {From probabilistic forecasts to statistical scenarios of short-term wind power production},
  journal   = {Wind Energy},
  year      = {2009},
  volume    = {12},
  number    = {1},
  pages     = {51--62},
  abstract  = {Short-term (up to 2-3 days ahead) probabilistic forecasts of wind power provide forecast users with a highly valuable information on the uncertainty of expected wind generation. Whatever the type of these probabilistic forecasts, they are produced on a per horizon basis, and hence do not inform on the development of the forecast uncertainty through forecast series. However, this additional informationmay be paramount for a large class of time-dependent and multi-stage decision-making problems e.g. optimal operation of combined wind-storage systems or multiple-market trading with different gate closures. This issue is addressed here by describing a method that permits the generation of statistical scenarios of short-term wind generation that accounts for both the interdependence structure of prediction errors and the predictive distributions of wind power production. The method is based on the conversion of series of prediction errors to a multivariate Gaussian random variable, the interdependence structure of which can then be summarized by a unique covariance matrix. Such matrix is recursively estimated in order to accommodate long-term variations in the prediction error characteristics. The quality and interest of the methodology are demonstrated with an application to the test case of a multi-MW wind farm over a period of more than two years.},
  comment   = {May be basis for new DTU ramp forecasting Preprint (attached) has easier to see colored graphs. I have the .tex file for the preprint too.
* Assumes that Gaussianizing RV's with a scalar transform makes their joint distribution a multi-variate Gaussian, which is NOT TRUE in general. See Wu10gaussMargNotJoint.
* Use true multivariate Gaussianization (see energytop.org)?
* Pierre says this is a copula approach. But I thought correlations were done rank space, and the correlations here are done after the uniform dist. has been converted to gaussian (what space is that?) . On the other hand, p. 19 and Eq. 10, p. l0 of Schmidt07copulasCope seems to indicate that you do indeed xfer to Gaussian before doing the correlation.
compared against NWP ensembles in Girard10trajEvalWind (ensembles seem better?)
* Used in: Soares17distGridMgmtPowFlw

* expanded to spatial covariance in Tastu15spcTimeTrajGaussCpla},
  doi       = {10.1002/we.284},
  file      = {Pinson09probFrcstStatScenWind.pdf:Pinson09probFrcstStatScenWind.pdf:PDF;Pre-print.  Easier to read colored graphs:Pinson09probFrcstStatScenWind_preprint.pdf:PDF},
  groups    = {Read, PointDerived, Test, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2009.04.17},
}

@Article{Zhou11conjFrcstMkts,
  author   = {Q. Zhou and L. Tesfatsion and C. C. Liu},
  title    = {Short-Term Congestion Forecasting in Wholesale Power Markets},
  journal  = {IEEE Transactions on Power Systems},
  year     = {2011},
  volume   = {26},
  number   = {4},
  pages    = {2185--2196},
  month    = nov,
  issn     = {0885-8950},
  abstract = {Short-term congestion forecasting is highly important for market participants in wholesale power markets that use locational marginal prices (LMPs) to manage congestion. Accurate congestion forecasting facilitates market traders in bidding and trading activities and assists market operators in system planning. This study proposes a new short-term forecasting algorithm for congestion, LMPs, and other power system variables based on the concept of system patterns - combinations of status flags for generating units and transmission lines. The advantage of this algorithm relative to standard statistical forecasting methods is that structural aspects underlying power market operations are exploited to reduce forecast error. The advantage relative to previously proposed structural forecasting methods is that data requirements are substantially reduced. Forecasting results based on a NYISO case study demonstrate the feasibility and accuracy of the proposed algorithm.},
  doi      = {10.1109/TPWRS.2011.2123118},
  file     = {:Zhou11conjFrcstMkts.pdf:PDF},
  keywords = {load forecasting, power markets, power system economics, pricing, statistical analysis, LMP, NYISO case study, forecast error reduction, locational marginal prices, power system variables, short-term congestion forecasting, standard statistical forecasting methods, wholesale power markets, Algorithm design and analysis, Forecasting, Power markets, Power system planning, Probabilistic logic, Congestion forecasting, convex hull algorithm, load partitioning, locational marginal price, price forecasting, system patterns, wholesale power market},
}

@Article{Couto14weathRegimeRamps,
  author    = {Couto, A. and Costa, P. and Rodrigues, L. and Lopes, V. and Estanqueiro, A.},
  title     = {Impact of Weather Regimes on the Wind Power Ramp Forecast in {Portugal}},
  journal   = {IEEE Transactions on Sustainable Energy},
  year      = {2014},
  note      = {Early Access},
  abstract  = {Short-term forecasting and diagnostic tools for severe changes of wind power production (power ramps) may provide reliable information for a secure power system operation at a small cost. Understanding the underlying role of the synoptic weather regimes (WRs) in triggering the wind power ramp events can be an added value to improve and complement the current forecast techniques. This work identifies and classifies the WRs over mainland Portugal associated with the occurrence of severe wind power ramps. The most representative WRs are identified on compressed surface level atmospheric data using principal component analysis by applying K-means clustering. The results show a strong association between some synoptic circulation patterns and step variations of the wind power production indicating the possibility to identify certain WRs that are prone to trigger severe wind power ramps, thus opening the possibility for future development of diagnostic warning systems for system operators{\textquoteright} use.},
  comment   = {LNEG's ramp thing for IRPWIND, which is pretty bogus: does crude PCA on weather field (questionable dimension choice algorithm), then clusters with KNN (num. cluster choice not explained). These "regimes" were apparently not informative enough to ID ramps, so they found sequences of them that were associated with their definition of a ramp. The test appears to have been done on the same training data. Ramps were detected w/ positive skill but the detection rate is not compared to simply running their ramp definition over the existing power forecast.

Criticisms I put in my 4/21/15 IRPWIND meeting notes:

1. Rationale for ramp definition?
 a. 50%: at low power, anything is a ramp; high power nothing is?
 b. Ramp definition doesn't include the length, mentioned for visual ramp selection
 c. How do thresholds map to TSO requirements?
2. Test on train?
3. Test on train suspicious b/c a lot of unexplained choices
 a. Visually hand-picked ramps
 b. num. PCA dimensions (but there was some kind of sensitivity analysis)
 c. num. KNN clusters
 d. Sequence choice
4. Not compared w/ running their ramp definition over power forecast
5. PCA breaks spatial field continuity (need multilinear or something)


Other comments

* Inputs were:
 - 70m wind "velocity",
 - instability (vertical temp gradient)
 - horizontal pressure gradient, I think

* Standard PCA vectorization breaks of spatial field continuity: Could try:
 - Demsar13spatialPCA
 - Huang08SimultTensSpcClust

* num. clusters and num. PCA dims also s/b chosen on separate data},
  doi       = {10.1109/TSTE.2014.2334062},
  file      = {Couto14weathRegimeRamps.pdf:Couto14weathRegimeRamps.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2014.11.03},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6863716},
}

@InProceedings{Fugon08dataMineFrcst,
  author      = {Fugon, Lionel and Juban, Jeremie and Kariniotakis, Georges},
  title       = {Data mining for wind power forecasting},
  booktitle   = {European Wind Energy Conference and Exhibition (EWEC)},
  year        = {2008},
  month       = apr,
  abstract    = {Short-term forecasting of wind energy production up to 2-3 days ahead is recognized as a major contribution for reliable large-scale wind power integration. Increasing the value of wind generation through the improvement of prediction systems performance is recognised as one of the priorities in wind energy research needs for the coming years. This paper aims to evaluate Data Mining type of models for wind power forecasting. Models that are examined include neural networks, support vector machines, the recently proposed regression trees approach, and others. Evaluation results are presented for several real wind farms.},
  comment     = {spinning reserves. another way to generate scenarios.},
  file        = {Fugon08dataMineFrcst.pdf:Fugon08dataMineFrcst.pdf:PDF},
  institution = {INRIA a CCSD electronic archive server based on P.A.O.L [http://hal.inria.fr/oai/oai.php] (France)},
  keywords    = {[SPI:ENERG] Engineering Sciences/Energetics, Wind power, forecasting, wind energy production, wind farm},
  location    = {http://www.scientificcommons.org/59435509},
  owner       = {scot},
  timestamp   = {2010.12.01},
  url         = {http://hal-ensmp.archives-ouvertes.fr/hal-00506101/en/},
}

@Article{Pinson08locLinAdaptOrthog,
  author    = {Pinson, Pierre and Nielsen, Henrik Aa. and Madsen, Henrik and Nielsen,, Torben S.},
  title     = {Local linear regression with adaptive orthogonal fitting for the wind power application},
  journal   = {Statistics and Computing},
  year      = {2008},
  volume    = {18},
  number    = {1},
  pages     = {59--71},
  issn      = {0960-3174},
  abstract  = {Short-term forecasting of wind generation requires a model of the function for the conversion of meteorological variables (mainly wind speed) to power production. Such a power curve is nonlinear and bounded, in addition to being nonstationary. Local linear regression is an appealing nonparametric approach for power curve estimation, for which the model coefficients can be tracked with recursive Least Squares (LS) methods. This may lead to an inaccurate estimate of the true power curve, owing to the assumption that a noise component is present on the response variable axis only. Therefore, this assumption is relaxed here, by describing a local linear regression with orthogonal fit. Local linear coefficients are defined as those which minimize a weighted Total Least Squares (TLS) criterion. An adaptive estimation method is introduced in order to accommodate nonstationarity. This has the additional benefit of lowering the computational costs of updating local coefficients every time new observations become available. The estimation method is based on tracking the left-most eigenvector of the augmented covariance matrix. A robustification of the estimation method is also proposed. Simulations on semi-artificial datasets (for which the true power curve is available) underline the properties of the proposed regression and related estimation methods. An important result is the significantly higher ability of local polynomial regression with orthogonal fit to accurately approximate the target regression, even though it may hardly be visible when calculating error criteria against corrupted data.},
  comment   = {way to do wind speed to power curves, maybe has some problems
* orthogonal regression unloved in HRensembleHR10finalrep
* true power curve model [eq (39)]: g(u) = exp(-tau2*exp(-tau1*(u)))},
  doi       = {10.1007/s11222-007-9038-7},
  file      = {Pinson08locLinAdaptOrthog.pdf:Pinson08locLinAdaptOrthog.pdf:PDF;Pinson08locLinAdaptOrthog.pdf:Pinson08locLinAdaptOrthog.pdf:PDF},
  location  = {Hingham, MA, USA},
  owner     = {sotterson},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2009.04.17},
}

@InProceedings{Juban07ProbWndFrcstFeatSelKDE,
  author    = {Juban, J{\'e}r{\'e}mie and Fugon, Lionel and Kariniotakis, Georges and others},
  title     = {Probabilistic short-term wind power forecasting based on kernel density estimators},
  booktitle = {Proceedings, European Wind Energy Conference and exhibition, EWEC 2007},
  year      = {2007},
  abstract  = {Short-term wind power forecasting tools have been developed
for some time. The majority of such tools usually provide
single-valued (spot) predictions. Such predictions are however
often not adequate when the aim is decision-making under uncertainty.
In that case there is a clear requirement by end-users
to have additional information on the uncertainty of the predictions
for performing efficiently functions such as reserves estimation,
unit commitment, trading in electricity markets, a.o.
In this paper, we propose a method for producing the complete
predictive probability density function (PDF) for each
time step of the prediction horizon based on the kernel density
estimation technique. The performance of the proposed
approach is demonstrated using real data from several wind
farms. Comparisons to state-of-the-art methods from both outside
and inside the wind power forecasting community are presented
illustrating the performances of the proposed method.},
  comment   = {Select short term wind power features with mutual information. Then does KDE quantile forecast. Features other than the usual speed/dir were tested.},
  file      = {Juban07ProbWndFrcstFeatSelKDE.pdf:Juban07ProbWndFrcstFeatSelKDE.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.10},
  url       = {http://hal.archives-ouvertes.fr/hal-00526011/},
}

@Article{Hardin13genRealisticCorrMat,
  author    = {Hardin, Johanna and Garcia, Stephan Ramon and Golan, David and others},
  title     = {A method for generating realistic correlation matrices},
  journal   = {The Annals of Applied Statistics},
  year      = {2013},
  volume    = {7},
  number    = {3},
  pages     = {1733--1762},
  abstract  = {Simulating sample correlation matrices is important in many areas of statistics. Approaches such as generating Gaussian data and finding their sample correlation matrix or generating random uniform [?1,1][?1,1] deviates as pairwise correlations both have drawbacks. We develop an algorithm for adding noise, in a highly controlled manner, to general correlation matrices. In many instances, our method yields results which are superior to those obtained by simply simulating Gaussian data. Moreover, we demonstrate how our general algorithm can be tailored to a number of different correlation models. Using our results with a few different applications, we show that simulating correlation matrices can help assess statistical methodology.},
  comment   = {How to generate random correlation matrices (positive semi-definite w/ 1's on the diagonal, if normalized).

Has R code.

Is used in: Hollenbach14missDatImpCopula

Compare with: Davies00randGenStblCorrMat},
  file      = {Hardin13genRealisticCorrMat.pdf:Hardin13genRealisticCorrMat.pdf:PDF},
  owner     = {sotterson},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2017.03.16},
  url       = {http://projecteuclid.org/euclid.aoas/1380804814},
}

@InProceedings{Willemain04scenarioBoot,
  author               = {Willemain, T. R.},
  title                = {Recent advances in scenario generation using the bootstrap},
  booktitle            = {IIE Annual Conference and Exhibition},
  year                 = {2004},
  pages                = {1843--1848},
  abstract             = {Simulation analysis is driven by input scenarios. It is desirable to have a fast, automatic way to generate an unlimited number of realistic scenarios. We reviewed recent research, including some results not yet published, showing that bootstrapping observed inputs can greatly improve estimates of simulation uncertainty, that it is possible to operationalize the notion of "good" scenarios in terms of matching the gold standard of the distribution of the "distance" between independent samples from the same data generating process, and that this idea can be used to adapt various time-series bootstrap methods for scenario generation in operational and financial realms. Conference code: 66321 Sponsors: Operations Concepts Inc.; H.B. Maynard and Co.},
  citeulike-article-id = {2749112},
  citeulike-linkout-0  = {http://www.scopus.com/record/display.url?view=extended&origin=resultslist&eid=2-s2.0-30044446981},
  comment              = {spinning reserves},
  file                 = {Willemain04scenarioBoot.pdf:Willemain04scenarioBoot.pdf:PDF},
  groups               = {Test, doReadNonWPV_2},
  keywords             = {bootstrap, simulation, simulationbootstrap, statistics},
  location             = {Department of Decision Sciences and Engineering Systems, Rensselaer Polytechnic Institute, Troy, NY 12180-3590, United States},
  owner                = {scot},
  posted-at            = {2008-05-03 16:12:07},
  timestamp            = {2010.11.24},
  url                  = {http://www.scopus.com/record/display.url?view=extended&origin=resultslist&eid=2-s2.0-30044446981},
}

@Article{Demirel02scenarioBtstrpBlk,
  author    = {Demirel, O.F. and Willemain, T.R.},
  title     = {Generation of simulation input scenarios using bootstrap methods},
  journal   = {Journal of the Operational Research Society},
  year      = {2002},
  volume    = {53},
  number    = {1},
  pages     = {69--78},
  issn      = {0160-5682},
  abstract  = {Simulation modellers frequently face a choice between fidelity and variety in their input scenarios. Using an historical trace provides only one realistic scenario. Using the input modelling facilities in commercial simulation software may provide any number of unrealistic scenarios. We ease this dilemma by developing a way to use the moving blocks bootstrap to convert a single trace into an unlimited number of realistic input scenarios. We do this by setting the bootstrap block size to make the bootstrap samples mimic independent realizations in terms of the distribution of distance between pairs of inputs. We measure distance using a new statistic computed from zero crossings. We estimate the best block size by scaling up an estimate computed by analysing subseries of the trace},
  comment   = {spinning reserves. scenario generation by bootstrap, preserving temporal correlation by picking blocks},
  owner     = {scot},
  publisher = {Palgrave Macmillan},
  timestamp = {2010.12.01},
}

@Book{Politis99subsampBook,
  title     = {Subsampling},
  publisher = {Springer Verlag},
  year      = {1999},
  author    = {Politis, D.N. and Romano, J.P. and Wolf, M.},
  abstract  = {Since Efron's profound paper on the bootstrap, an enormous amount of effort has been spent on the development of bootstrap, jacknife, and other resampling methods. The primary goal of these computer-intensive methods has been to provide statistical tools that work in complex situations without imposing unrealistic or unverifiable assumptions about the data generating mechanism. This book sets out to lay some of the foundations for subsampling methodology and related methods. Table of Contents: Bootstrap Sampling Distributions.- Subsampling in the IID Case.- Stationary Time Series.- Nonstationary Time Series.- Subsampling for Random Fields.- Subsampling Marked Point Processes.- Confidence Sets for General Parameters.- Extrapolation and Higher-Order Accuracy.- Subsampling with Unknown Rate.- Subsampling the Mean with Heavy Tails.- Subsampling the Autoregressive Root.- Choice of the Block Size.- Simulation Studies.- Subsampling Stock Returns.},
  comment   = {Subsampled bootstrapping (confidence intervals when can't boostrap w/ replacement) and other stuff
* Geyer06subSampBootStrap is a good review of the subsampling bootstrap, w/ small improvements
* how to pick subsample size may be explained in Simar10mofnBtstrp

* example of when can't bootstrap w/ replacement is K-NN MI, which Francois06permTestMutInf has found is biased by this (I have found the same thing in my ramp experiments).
* unknown convergence alg. may be covered in: Bertail99subsampBSunkCnvrg

* Berg10subSampBSpval shows how to use the technique for p-value hypothesis testing},
  owner     = {scot},
  timestamp = {2011.06.16},
  url       = {http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-98854-2?cm_mmc=Google-_-Book%20Search-_-Springer-_-0},
}

@Article{Bondell10nonCrossQR,
  author    = {Bondell, Howard D and Reich, Brian J and Wang, Huixia},
  title     = {Noncrossing quantile regression curve estimation},
  journal   = {Biometrika},
  year      = {2010},
  volume    = {97},
  number    = {4},
  pages     = {825--838},
  abstract  = {Since quantile regression curves are estimated individually, the quantile curves can cross, leading to an invalid distribution for the response. A simple constrained version of quantile regression is proposed to avoid the crossing problem for both linear and nonparametric quantile curves. A simulation study and a reanalysis of tropical cyclone intensity data shows the usefulness of the procedure. Asymptotic properties of the estimator are equivalent to the typical approach under standard conditions, and the proposed estimator reduces to the classical one if there is no crossing. The performance of the constrained estimator has shown significant improvement by adding smoothing and stability across the quantile levels.},
  comment   = {A change in quantile regression optimization to get rid of quantile crossing. Perhaps preferred by LopezLopez14altConfigQR.

Has R here: http://www4.stat.ncsu.edu/~bondell/software.html},
  file      = {Bondell10nonCrossQR.pdf:Bondell10nonCrossQR.pdf:PDF},
  publisher = {Biometrika Trust},
  url       = {http://biomet.oxfordjournals.org/content/97/4/825.abstract},
}

@InProceedings{Szabo13infoTheoEstTlbx,
  author    = {Zolt{{\'a}}n Szab{{\'o}}},
  title     = {Information Theoretical Estimators (ITE) Toolbox},
  booktitle = {NIPS},
  year      = {2013},
  abstract  = {Since the pioneering work of Shannon, entropy, mutual information, association, divergence measures and kernels
on distributions have found a broad range of applications in many areas of machine learning. Entropies provide a
natural notion to quantify the uncertainty of random variables, mutual information and association indices measure
the dependence among its arguments, divergences and kernels offer efficient tools to define the ?distance? and the
inner product of probability measures, respectively.
Recent advances in the field have shown that many of these quantities can be estimated nonparametrically in a
statistically consistent way. Nonparametric nature is highly important since our goal is to estimate functionals of
distributions and not the underlying distributions, densities. From practical point of view this means that the perfor-
mance of nonparametric methods is becoming superior to plug-in type approaches as the dimension is increasing.
For certain kernel based estimators it can also be proved that their efficiency is in fact dimension independent.
An other challenge one has to face with is invariance to monotone increasing transformations of the random vari-
ables. Indeed, for example in feature selection it is highly desirable to construct and apply estimators that do not
depend on the unit chosen, i.e., to achieve scale-invariance. As it has been shown recently, one can formulate con-
sistent estimators (i) possessing the dimensional independent and the scale-invariance properties simultaneously, (ii)
that are also robust to outliers.
There exist numerous relations among information theoretical quantities. One of the most well-known such relation
is that mutual information can be expressed in terms of Shannon entropy, or Kullback-Leibler divergence. From
estimation point of view, such relations (once identified) can be readily used to build meta estimators from base
ones.
Despite the large number of successful applications and the recent theoretical contributions, existing software pack-
ages in the domain either focus on discrete variables, or quite specialized applications/information theoretical esti-
mation methods. Such limitations of the available solutions highly encumber their reusage, restrict their extension
with the latest advances, and put restraints to collaborative development.
In order to cope with these challenges, we have recently released the ITE (information theoretical estimators) tool-
box. ITE},
  comment   = {Matlab Info theory toolbox by author who does distribution regression (Szabo16LrnDistRgrssn).  Has a quickread slide.

Conference Page:
http://mloss.org/workshop/nips13/

Matlab Central Link Exchange
https://www.mathworks.com/matlabcentral/linkexchange/links/3441-information-theoretical-estimators-ite-toolbox

Author homepage:
http://www.cmap.polytechnique.fr/~zoltan.szabo/},
  file      = {Paper:Szabo13infoTheoEstTlbx.pdf:PDF;Highlight slide:Szabo13infoTheoEstTlbx_highlight_slide.pdf:PDF},
  url       = {http://mloss.org/workshop/nips13/},
}

@Article{Portnoy97gaussLapQRspeedup,
  author    = {Portnoy, Stephen and Koenker, Roger and others},
  title     = {The {Gauss}ian hare and the Laplacian tortoise: computability of squared-error versus absolute-error estimators},
  journal   = {Statistical Science},
  year      = {1997},
  volume    = {12},
  number    = {4},
  pages     = {279--300},
  abstract  = {Since the time of Gauss, it has been generally accepted that
`2-methods of combining observations by minimizing sums of squared errors
have significant computational advantages over earlier `1-methods
based on minimization of absolute errors advocated by Boscovich,
Laplace and others. However, `1-methods are known to have signi-
cant robustness advantages over `2-methods in many applications, and
related quantile regression methods provide a useful, complementary
approach to classical least-squares estimation of statistical models. Combining
recent advances in interior point methods for solving linear programs
with a new statistical preprocessing approach for `1-type problems,
we obtain a 10- to 100-fold improvement in computational speeds
over current (simplex-based) `1-algorithms in large problems, demonstrating
that `1-methods can be made competitive with `2-methods in
terms of computational speed throughout the entire range of problem
sizes. Formal complexity results suggest that `1-regression can be made
faster than least-squares regression for n sufficiently large and p modest.
Key words and phrases: `1, L1, least absolute deviations, median, regression
quantiles, interior point, statistical preprocessing, linear programming,
simplex method, simultaneous confidence bands.},
  comment   = {A preprocessing step that speeds up quantile regression, according to Hunter00qrMMalg},
  file      = {Portnoy97gaussLapQRspeedup.pdf:Portnoy97gaussLapQRspeedup.pdf:PDF},
  owner     = {sotterson},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2014.07.03},
  url       = {http://projecteuclid.org/euclid.ss/1030037960},
}

@InProceedings{Huang08SimultTensSpcClust,
  author       = {Huang, Heng and Ding, Chris and Luo, Dijun and Li, Tao},
  title        = {Simultaneous tensor subspace selection and clustering: the equivalence of high order svd and k-means clustering},
  booktitle    = {Proceedings of the 14\textsuperscript{th} ACM SIGKDD international conference on Knowledge Discovery and Data mining},
  year         = {2008},
  pages        = {327--335},
  organization = {ACM},
  abstract     = {Singular Value Decomposition (SVD)/Principal Component
Analysis (PCA) have played a vital role in ?nding patterns
from many datasets. Recently tensor factorization has been
used for data mining and pattern recognition in high in-
dex/order data. High Order SVD (HOSVD) is a commonly
used tensor factorization method and has recently been used
in numerous applications like graphs, videos, social networks,
etc.
In this paper we prove that HOSVD does simultaneous
subspace selection (data compression) and K-means cluster-
ing widely used for unsupervised learning tasks. We show
how to utilize this new feature of HOSVD for clustering.
We demonstrate these new results using three real and large
datasets, two on face images datasets and one on hand-
written digits dataset. Using this new HOSVD clustering
feature we provide a dataset quality assessment on many
frequently used experimental datasets with expected noise
levels.
Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications-
Data Mining
General Terms
Algorithms},
  comment      = {Clustering tensor spaces w/ pca (svd) and K-means},
  file         = {Huang08SimultTensSpcClust.pdf:Huang08SimultTensSpcClust.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2014.11.05},
  url          = {http://dl.acm.org/citation.cfm?id=1401933},
}

@InProceedings{Madsen08toolsWindIntegrate,
  author    = {Henrik Madsen and Torben Skov Nielsen and Pierre Pinson},
  title     = {Tools for Integrating Large Scale Wind Power},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  year      = {2008},
  abstract  = {Slides outline
* History of WPPT (Wind Power Prediction Tool)
* Methods used for predicting the wind power
* Modelling approach
* The WPPT implementation
* Case study
* Further possibilities
* Some references
* The ANEMOS consortium
* The CENER-ENFOR consortium},
  comment   = {Composable blocks for adaptive wind predictions. For farm/project/region; days/hours/minutes ahead. Use NWP, power, fixed models

WPPT: wind power prediction tool

Adaptive/recursive, adjusts for
* changing population of turbines
* poorly modeled characteristics eg. dirty blades
* NWP model changes Misc notes
* Is composed of configurable blocks, hooked together depending upon the types of available data
-- several examples of how the blocks are hooked together
-- same blocks can be used for farm, project or region
-- for "online" they seem to get 15 minute data
* cut regulation cost by 38-39\% over persistence by using this system instead of persistence

PREDICTION BUILDING BLOCKS:

I. Wind power prediction for farms w/o online measurements
* based only on past powers, not wind
* Is linear regression of measured power from nearby farms that do have online measurements
* Two ways to update coeffs
1.) farms w/ online measurements: adaptive RLS
2.) farms w/ no measurements: coeffs are fixed based on distance, nominal power and utilization time

II. Power curve model
* appear to build turbine, farm and regional power curves
* different curves depending upon prediction horizion (NWP predicted winds have different props)
* not clear how farm or regional forecasts fit w/ individual turbine power estimates for non-online turbines

III. Dynamical Prediction Model (yet another one...)
* 3 kinds of terms: past measured powers, prediction based on wind, diurnal correction
* linear regression on past power outputs and predicted power (based wind forecasts, I think)
* 3 term weighted sinusoidal to correct for deviation of predictions due to diurnal variation
-- not clear how this is a function of variable wind or power since it's additive, w/ no dependence
-- does adaptiation handle it somehow?
-- are diurnal cycles naturally handled y 3 Fourier terms?
* is adaptive, somehow
* says num. terms dep on lookahead time but equ doesnt show it (slide 11)

IV. Upscaling model
* multiplicative func. of (wind,dir), not clear... 5-60 minute ahead prediction (NowCaster!)
* inputs are current power, forecasted wind speed, power "variability"

* really only last input? So is regressive?
* they know how to mix forecasted wind and past power into a single model

Combined forecasts
* feature selection: avoid using too many correlated forecasts
* 10-15\% improvement for adding more than one MET provider (tower or forecast?)

Uncertainty estimation
* quantile regression
* adaptive variance estimation
* ensemble-based corrected quantiles
* when added uncertainty, cost reduction over persistence went from 38 to 39\%},
  file      = {Madsen08toolsWindIntegrate.pdf:Madsen08toolsWindIntegrate.pdf:PDF;Madsen08toolsWindIntegrate.pdf:Madsen08toolsWindIntegrate.pdf:PDF},
  groups    = {IWFTM08, Read, PointDerived, Use, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2008.09.19},
  url       = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
}

@InProceedings{Yu12MultiWayGMMkernPLS,
  author    = {Yu, Jie},
  title     = {A Multiway {Gauss}ian Mixture Model Based Adaptive Kernel Partial Least Squares Regression Approach for Inferential Quality Predictions of Batch Processes},
  booktitle = {Advanced Control of Chemical Processes},
  year      = {2012},
  volume    = {8},
  number    = {1},
  pages     = {57--62},
  abstract  = {Soft sensor technique has become increasingly important to provide reliable on-line mea-
surements, facilitate advanced process control and improve product quality in process industries. The
conventional soft sensors are normally single-model based and thus may not be appropriate for processes
with shifting operating conditions or phases. In this study, a multiway Gaussian mixture model (MGMM)
based kernel partial least squares (PLS) method is proposed to handle multiple operating phases in
batch or semibatch processes. The measurement data are first projected onto high-dimensional kernel
feature space to account for the process nonlinearity. Then the multiway Gaussian mixture model is
estimated with multiple Gaussian clusters in the kernel space. Thus various localized PLS models can
be built within each Gaussian cluster to characterize the dynamics in the particular operating phase.
Using Bayesian inference strategy, the soft sensor models for all the test samples are adaptively selected
from the multiple localized kernel PLS models representing different phases and further used for online
quality predictions. The presented soft sensor method is applied to the multiphase penicillin fermentation
process and the computational results demonstrate that its performance is superior to the conventional
multiway kernel PLS model.},
  comment   = {Kernel PLS regression using high dim feature space multi-way GMM's. Learns different process regimes, much more accurate than single KPLS prediction. Good for regime detection, power curve learning, etc? Regimes could be e.g. wind direction, wind speed, curtailments, downregulating, etc.},
  file      = {Yu12MultiWayGMMkernPLS.pdf:Yu12MultiWayGMMkernPLS.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.01.30},
  url       = {http://www.nt.ntnu.no/users/skoge/prost/proceedings/adchem-2012/files/0086.pdf},
}

@Article{Manikandan16designTseriesANN,
  author    = {Narayanan Manikandan and Srinivasan Subha},
  title     = {Software Design Challenges in Time Series Prediction Systems Using Parallel Implementation of Artificial Neural Networks},
  journal   = {The Scientific World Journal},
  year      = {2016},
  volume    = {2016},
  pages     = {1--10},
  abstract  = {Software development life cycle has been characterized by destructive disconnects between activities like planning, analysis, design, and programming. Particularly software developed with prediction based results is always a big challenge for designers. Time series data forecasting like currency exchange, stock prices, and weather report are some of the areas where an extensive research is going on for the last three decades. In the initial days, the problems with financial analysis and prediction were solved by statistical models and methods. For the last two decades, a large number of Artificial Neural Networks based learning models have been proposed to solve the problems of financial data and get accurate results in prediction of the future trends and prices. This paper addressed some architectural design related issues for performance improvement through vectorising the strengths of multivariate econometric time series models and Artificial Neural Networks. It provides an adaptive approach for predicting exchange rates and it can be called hybrid methodology for predicting exchange rates. This framework is tested for finding the accuracy and performance of parallel algorithms used.},
  comment   = {Explains how Levenberg-Marquardt can be implemented in parallel.  Maybe this is how Matlab does it?

Seems like it's a divide the data by N algorithm that would not increase memory use.

From Mathworks05redMemLevMarq, basic LM memory use is npts*nWeightsAndBiases

If you split the data up, the total use would be 

Sum(k=1:Q/K)  Q/K*nWeightsAndBiases = nWeightsAndBiases * K*Q/K = the same thing.},
  doi       = {http://dx.doi.org/10.1155/2016/6709352},
  file      = {:Manikandan16designTseriesANN.pdf:PDF},
  publisher = {Hindawi Limited},
}

@Article{Jaus19navSolarIradMeasIss,
  author   = {Joachim Jaus},
  title    = {Navigating solar irradiance measurement issues},
  journal  = {Power and Energy Solutions},
  year     = {2019},
  abstract = {Solar irradiance measurements provide valuable information 
to select a site for the construction of a solar power plant, to 
calculate the efficiency of the plant operation, or to determine 
possibilities for improvements of the power plant setup. But 
only precise measurements can do the job well. Dr Jaus 
explores how to circumnavigate the potential pitfalls.},
  comment  = {Describes the hardware and measurment practices needed for solar plant siting and operation.

I was confused by the logic in Perez17grndCalDetSatIrrad regarding GHI: I used this paper to try to the understand the reasoning.   The relevant things I learned here were:

1. DNI is the easier to measure than DIF or GHI.  
2. The reason is that DIF and GHI must measure radiation fro the entire field, and so are vulnerable to glass dome and detector reflectance.  DNI has only a 2.5 degree range, and so has less problem w/ cosine reflection angle repsonse.
3. Yet global irrad is the thing you want b/c that's what PV needs
4. And yet, DNI measurements are especially susceptible to soiling by dust 

This all argues for using calculated GHI, supposing DIF or DNI is easier to measure (I think you need only 2 of three to get all three, see Kankiewicz14solarUncertRsrcTune).},
  file     = {:Jaus19navSolarIradMeasIss.pdf:PDF},
  url      = {http://www.pes.eu.com/solar/navigating-solar-irradiance-measurement-issues/5029/},
}

@Article{Elshurafa18lrnCrvPVbos,
  author   = {Amro M. Elshurafa and Shahad R. Albardi and Simona Bigerna and Carlo Andrea Bollino},
  title    = {Estimating the learning curve of solar PV balance–of–system for over 20 countries: Implications and policy recommendations},
  journal  = {Journal of Cleaner Production},
  year     = {2018},
  volume   = {196},
  pages    = {122 - 134},
  issn     = {0959-6526},
  abstract = {Solar photovoltaic systems installed on homes and commercial building rooftops are deemed central for a low-carbon future. As capital costs of photovoltaics continue to fall, its role towards making buildings more sustainable and environmentally-friendly will continue to grow. Capital costs of a photovoltaic system comprise the module and balance-of-system costs. The latter refers to everything-else needed to make the photovoltaic system functional including cables, mounts, labor, etc. While modules are priced internationally, the balance-of-system cost is country-specific. Price developments of modules, which have been thoroughly studied in literature, followed an 80% learning curve. Research on the balance-of-system learning curve however, has not been as extensive. In this paper, we estimate for the first time the learning curve of balance-of-system costs in photovoltaics for more than 20 countries via an extensive dataset. Our calculations yield a global learning curve for the balance-of-system of 89%, which corresponds to a progress ratio of 11% compared with 20% for modules. Understanding the rate at which capital costs of photovoltaics are falling with such detail will aid in more effective renewable energy policy planning and budgeting. Finally, some steps requiring no financial commitment but can bring down balance-of-system costs are discussed, which greatly contribute to a cleaner and more sustainable future.},
  comment  = {A potentially time varying estimate of the PV balance of system costs vs. country.  Interesting b/c that cost is separated out.  Problem: where get the forecasts of the conditioning variables?},
  doi      = {https://doi.org/10.1016/j.jclepro.2018.06.016},
  file     = {:Elshurafa18lrnCrvPVbos.pdf:PDF},
  keywords = {Solar photovoltaics, Learning curves, Balance of systems, Capital costs, Rooftop solar, Renewable energy policy},
  url      = {http://www.sciencedirect.com/science/article/pii/S0959652618316652},
}

@Book{Kleissl13SolarEnrgyFrcstBk,
  title     = {Solar Energy Forecasting and Resource Assessment},
  publisher = {Elsevier LTD, Oxford},
  year      = {2013},
  author    = {Kleissl, Jan},
  isbn      = {978-0-12-397177-7},
  abstract  = {Solar power is widely acknowledged to be the fastest-growing energy industry
in the world. As technological improvements steadily progress toward the
erasure of cost and efficiency barriers, two issues are coming to the forefront of
public discourse on solar energydvariability and reliability. Solar-project
developers and their financiers are increasingly scrutinizing the accuracy of
long-term resource projections; as well, grid operators? concerns about variable
short-term power generation are growing. These issues have made the field of
solar forecasting and resource assessment pivotally important, and to date,
there has been no comprehensive single text devoted to it. This volume aims to
become the authoritative work on solar forecasting and resource assessment,
incorporating contributions from internationally recognized researchers from
both industry and academia whose focus is on applying information from
underlying scientific fundamentals to practical industry needs, and on
emphasizing the latest technological developments driving this discipline
forward.
The audience for the book comprises scientists and engineers working in
the power-utility or renewable-energy industry and other, related energy
fields, as well as in atmospheric science and meteorology. Solar-energy
professionals are particularly targeted, including research scientists, project
developers, system operators, planners and engineers, and investors in and
financiers of solar-energy projects. This book is the only one dedicated to
the short-term forecast and assessment of solar-resource bankability and
variability, providing readers with a complete understanding of the state of
the art.
Chapters 2 and 3 address the semi-empirical and physically-based methods
developed for estimating surface solar-radiation resources using satellite
observations of clouds and atmospheric aerosols. Satellite solar resource esti-
mates are increasingly capable of replacing or at least complementing ground-
based observations for solar power prospecting. The financial risks to solar-
energy projects, the statistical analysis of temporal and spatial variations in
solar-radiation resources, and the impacts of resource variability on electrical-
power generation are presented in Chapters 4, 5, 6 and 7.
The ability to forecast solar resources for the range of time intervals
important for managing the electrical-power grid and its markets is an active
area of research and development. Chapter 8 provides an overview of solar-
forecasting methods and evaluation metrics. Chapter 9 describes short-term
solar-resource forecasts based on surface observations of clouds from sky
imagery. Chapters 10 and 11 describe hour-ahead forecasting methods based on
satellite data for grid operators in the United States and Europe.
Background, data assimilation, and case studies of Numerical weather
prediction (NWP) models applied to day-ahead solar forecasting are addressed
in Chapters 12, 13, and 14. Stochastic-learning methods for improving all types
of solar-resource forecasts are presented in Chapter 15.
My gratitude goes to all contributors and to my sponsors (California Public
Utilities Commission, California Energy Commission, Panasonic Corporation,
US Department of Energy) and undergraduate and doctoral students who
embrace the philosophy of lab-to-market research. May our joint work enable
seamless and economical integration of large amounts of solar power in the
electric grid.

The images in this book appear in black and white and are repeated in color
in the color plate section near the middle of the book.},
  date      = {2013-07-13},
  ean       = {9780123971777},
  file      = {Kleissl13SolarEnrgyFrcstBk.pdf:Kleissl13SolarEnrgyFrcstBk.pdf:PDF},
  owner     = {sotterson},
  pagetotal = {400},
  timestamp = {2017.04.18},
  url       = {http://www.ebook.de/de/product/20428667/jan_kleissl_solar_energy_forecasting_and_resource_assessment.html},
}

@Article{Jonsson13elecPriceQR,
  author    = {J{\'o}nsson, Tryggvi and Pinson, Pierre and Nielsen, Henrik Aa and Madsen, Henrik},
  title     = {Predictive densities for day-ahead electricity prices using time-adaptive quantile regression},
  journal   = {European Journal of Operational Research, Applied Energy},
  year      = {2013},
  abstract  = {Solving many of the problems actors in modern electricity markets are faced
on a daily basis requires scenarios of the day-ahead price. These scenarios are
generated using predictive densities for the same prices. A semi-parametric
methodology for generating such densities is presented, comprising a timeadaptive
quantile regression model for the 5-95\% quantiles and a description
of the distribution tails by exponential distribution. The forecasting skill of
the proposed model is compared to that of 4 benchmark approaches and the
well known GARCH model during a 3 year period. Whereas the benchmarks
are outperformed in terms of general forecasting skill the superiority of the
semi-parametric model over the GARCH model lies in the former?s ability to
generate reliable quantile estimates.
Keywords: Stochastic Processes, electricity},
  comment   = {A way to make quantile regression time adaptive.

Also explains CRPSS skill score, based on ratio of CRPS of model to CRPS of a reference (here of a time-invariant Gaussian defined price "climatology").

CRPS explanation is better than in Gneiting07strictPropScore

The place where this was published needs to be checked: The pdf says it was submitted to "Applied Energy" but the google scholar bibtex says},
  file      = {Jonsson13elecPriceQR.pdf:Jonsson13elecPriceQR.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2013.10.29},
  url       = {http://www2.compute.dtu.dk/~ppin/docs/jonssonetal13_spprobfore.pdf},
}

@Article{Hersbach00crpsDecomp,
  author    = {Hersbach, Hans},
  title     = {Decomposition of the continuous ranked probability score for ensemble prediction systems},
  year      = {2000},
  volume    = {15},
  number    = {5},
  pages     = {559--570},
  issn      = {1520-0434},
  abstract  = {Some time ago, the continuous ranked probability score (CRPS) was proposed as a new verification tool for (probabilistic) forecast systems. Its focus is on the entire permissible range of a certain (weather) parameter. The CRPS can be seen as a ranked probability score with an infinite number of classes, each of zero width. Alternatively, it can be interpreted as the integral of the Brier score over all possible threshold values for the parameter under consideration. For a deterministic forecast system the CRPS reduces to the mean absolute error.

In this paper it is shown that for an ensemble prediction system the CRPS can be decomposed into a reliability part and a resolution/uncertainty part, in a way that is similar to the decomposition of the Brier score. The reliability part of the CRPS is closely connected to the rank histogram of the ensemble, while the resolution/uncertainty part can be related to the average spread within the ensemble and the behavior of its outliers. The usefulness of such a decomposition is illustrated for the ensemble prediction system running at the European Centre for Medium-Range Weather Forecasts. The evaluation of the CRPS and its decomposition proposed in this paper can be extended to systems issuing continuous probability forecasts, by realizing that these can be interpreted as the limit of ensemble forecasts with an infinite number of members.},
  booktitle = {Weather and Forecasting},
  comment   = {Maybe the best ref. for CRPS. Explains discrete computation, how to compute for ensembles, decomposition... highly cited.

Decomposition is analogous to the decomposition of the Brier score. Should probably understand why both decompositions matter.

* the CRPS of a deterministic forecast is equal to the mean absolute error (MAE)
* it's a non-local score (see Brocker07properScore)
* multidimensional CRPS is some kind of weighted average
* in practice, I think I've found that it's insensitive to quantile crossover (this is the algorithm used in the R verification package that I ported to crpsDecomposition.m, for the Eweline project.  I've since written my own CRPS code which uses weighted quantile scores.  Also, Malte, wrote Quanti9leScoreDecom.m, based on Bentzien14quantScrDecomp)
* CRPS can be  interpreted as the integral of the Brier score over all possible threshold values},
  doi       = {10.1175/1520-0434%282000%29015%3C0559:DOTCRP%3E2.0.CO;2},
  file      = {Hersbach00crpsDecomp.pdf:Hersbach00crpsDecomp.pdf:PDF},
  groups    = {Test, CitaviImport1, doReadWPV_1},
  keywords  = {CPRSS},
  ncite     = {224},
  owner     = {sotterson},
  timestamp = {2013.09.26},
}

@Article{Politis03btstrpTimeSer,
  author    = {Politis, D.N.},
  title     = {The impact of bootstrap methods on time series analysis},
  journal   = {Statistical Science},
  year      = {2003},
  volume    = {18},
  number    = {2},
  pages     = {219--230},
  abstract  = {Sparked by Efron?s seminal paper, the decade of the 1980s was a period of active research on bootstrap methods for independent data -- mainly i.i.d. or regression set-ups. By contrast, in the 1990s much research was directed towards resampling dependent data, for example, time series and random ?elds. Consequently, the availability of valid nonparametric inference procedures based on resampling and/or subsampling has freed practitioners from the necessity of resorting to simplifying assumptions such as normality or linearity that may be misleading. Key words and phrases: Block bootstrap, con?dence intervals, linear models, resampling, large sample inference, nonparametric estimation, subsampling.},
  comment   = {Some kind of bootstrap subsampling equiv. to delete-d jacknife; ordinary bootstrap generally more accurate than more general subsample (except for median estimation) See p. 221-222 for discussion of when subsampling is more/less acccurate.},
  file      = {Politis03btstrpTimeSer.pdf:Politis03btstrpTimeSer.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2011.11.23},
  url       = {http://projecteuclid.org.globalproxy.cvt.dk/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.ss/1063994977},
}

@Article{Camargo15modelWindPVcmplmnt,
  author    = {Camargo, L Ramirez and Zink, R and Dorner, W},
  title     = {Spatiotemporal modeling for assessing complementarity of renewable energy sources in distributed energy systems},
  journal   = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  year      = {2015},
  volume    = {2},
  number    = {4},
  pages     = {147},
  abstract  = {Spatial assessments of the potential of renewable energy sources (RES) have become a valuable information basis for policy and
decision-making. These studies, however, do not explicitly consider the variability in time of RES such as solar energy or wind. Until
now, the focus is usually given to economic profitability based on yearly balances, which do not allow a comprehensive examination
of RES-technologies complementarity. Incrementing temporal resolution of energy output estimation will permit to plan the
aggregation of a diverse pool of RES plants i.e., to conceive a system as a virtual power plant (VPP). This paper presents a
spatiotemporal analysis methodology to estimate RES potential of municipalities. The methodology relies on a combination of open
source geographic information systems (GIS) processing tools and the in-memory array processing environment of Python and NumPy.
Beyond the typical identification of suitable locations to build power plants, it is possible to define which of them are the best for a
balanced local energy supply. A case study of a municipality, using spatial data with one square meter resolution and one hour temporal
resolution, shows strong complementarity of photovoltaic and wind power. Furthermore, it is shown that a detailed deployment strategy
of potential suitable locations for RES, calculated with modest computational requirements, can support municipalities to develop
VPPs and improve security of supply.},
  comment   = {Solar and wind complimentarity for virtual power plant running.  Useful for ReWP reserve power portfolio planning.},
  file      = {Camargo15modelWindPVcmplmnt.pdf:Camargo15modelWindPVcmplmnt.pdf:PDF},
  owner     = {sotterson},
  publisher = {Copernicus GmbH},
  timestamp = {2017.02.06},
  url       = {http://search.proquest.com/openview/b414ec24f42a0396ac8bd6cb1691e871/1?pq-origsite=gscholar&cbl=2037681},
}

@Article{Datta16Datta16hierNNGaussProcGeoStat,
  author    = {Datta, Abhirup and Banerjee, Sudipto and Finley, Andrew O and Gelfand, Alan E},
  title     = {Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets},
  journal   = {Journal of the American Statistical Association},
  year      = {2016},
  volume    = {111},
  number    = {514},
  pages     = {800--812},
  abstract  = {Spatial process models for analyzing geostatistical data entail computations that

become prohibitive as the number of spatial locations become large. This manuscript

develops a class of highly scalable Nearest Neighbor Gaussian Process (NNGP) models

to provide fully model-based inference for large geostatistical datasets. We establish

that the NNGP is a well-defined spatial process providing legitimate finite-dimensional

Gaussian densities with sparse precision matrices. We embed the NNGP as a sparsity-

inducing prior within a rich hierarchical modeling framework and outline how compu-

tationally efficient Markov chain Monte Carlo (MCMC) algorithms can be executed

without storing or decomposing large matrices. The floating point operations (flops)

per iteration of this algorithm is linear in the number of spatial locations, thereby

rendering substantial scalability. We illustrate the computational and inferential ben-

efits of the NNGP over competing methods using simulation studies and also analyze

forest biomass from a massive United States Forest Inventory dataset at a scale that

precludes alternative dimension-reducing methods.},
  comment   = {Considered to be "exciting" by a stats guru who wrote the STAN proabilistic programming package.  This one is good because it can model non-homogenous covariance matrices.  Also has C/C++ code, I think.
},
  file      = {:Datta16hierNNGaussProcGeoStat.pdf:PDF},
  publisher = {Taylor \& Francis},
}

@InProceedings{Hamad08spectClustIntro,
  author    = {Hamad, D. and Biela, P.},
  title     = {Introduction to spectral clustering},
  booktitle = {Information and Communication Technologies: From Theory to Applications, 2008. ICTTA 2008. 3\textsuperscript{rd} International Conference on},
  year      = {2008},
  pages     = {1--6},
  month     = apr,
  abstract  = {Spectral clustering methods are based on graph and matrix theories. Their principle is simple: given some data inputs, build similarity matrix, analyse the spectrum of its Laplacian matrix, and often get a perfect clustering from the eigenvectors analysis. This paper presents an introduction to spectral clustering methods and some applications in signal and image segmentation.},
  comment   = {Just a review of spectral clustering, like the Ng 2001 paper (in speakerClust.bib), but more comprehensive.},
  doi       = {10.1109/ICTTA.2008.4529994},
  file      = {Hamad08spectClustIntro.pdf:Hamad08spectClustIntro.pdf:PDF},
  keywords  = {Laplacian matrix;eigenvectors analysis;graph theory;matrix theory;similarity matrix;spectral clustering;eigenvalues and eigenfunctions;graph theory;matrix algebra;pattern clustering;},
  owner     = {sotterson},
  timestamp = {2011.11.16},
}

@InProceedings{Blau17NonRedundSpectDimRed,
  author    = {Blau, Yochai and Michaeli, Tomer},
  title     = {Non-Redundant Spectral Dimensionality Reduction},
  booktitle = {ECML PKDD. European Conference On Machine Learning \& Principles And Practice Of Knowledge Discovery In Databases},
  year      = {2017},
  abstract  = {Spectral dimensionality reduction algorithms are widely used
in numerous domains, including for recognition, segmentation, tracking
and visualization. However, despite their popularity, these algorithms
suffer from a major limitation known as the “repeated Eigen-directions”
phenomenon. That is, many of the embedding coordinates they produce
typically capture the same direction along the data manifold. This leads
to redundant and inefficient representations that do not reveal the true
intrinsic dimensionality of the data. In this paper, we propose a general
method for avoiding redundancy in spectral algorithms. Our approach
relies on replacing the orthogonality constraints underlying those meth-
ods by unpredictability constraints. Specifically, we require that each
embedding coordinate be unpredictable (in the statistical sense) from all
previous ones. We prove that these constraints necessarily prevent redun-
dancy, and provide a simple technique to incorporate them into existing
methods. As we illustrate on challenging high-dimensional scenarios, our
approach produces significantly more informative and compact represen-
tations, which improve visualization and classification tasks.},
  comment   = {Impressive spectral dimension reduction technique, does better than standard Ng dimred, ISOMAP and several others.  One claim to fame is that this techniques works better than others when used as a preprocessor for classification.

See also: Maaten08VisualizingDataUsingTSNE},
  file      = {:Blau17NonRedundSpectDimRed.pdf:PDF},
  url       = {http://ecmlpkdd2017.ijs.si/program.html},
}

@Article{OrtegaVazquez07spinResCostBen,
  author    = {Ortega-Vazquez, M. A. and Kirschen, D. S.},
  title     = {Optimizing the Spinning Reserve Requirements Using a Cost/Benefit Analysis},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2007},
  volume    = {22},
  number    = {1},
  pages     = {24--33},
  month     = feb,
  issn      = {0885-8950},
  abstract  = {Spinning reserve (SR) is one of the most important resources used by system operators to respond to unforeseen events such as generation outages and sudden load changes. While keeping large amounts of generation in reserve protects the power system against the generation deficits that might arise from different contingencies, and thus reduces the probability of having to resort to load shedding, this reserve provision is costly. Traditional unit commitment (UC) formulations use deterministic criteria, such as the capacity of the largest online generator to set the SR requirements. Other UC formulations adjust this requirement based on probabilistic criteria but require iterative processes or approximate calculations of the level of risk associated with the provision of reserve. This paper describes an offline method for setting the SR requirements based on the cost of its provision and the benefit derived from its availability},
  comment   = {Juan Mi's recommendation},
  doi       = {10.1109/TPWRS.2006.888951},
  file      = {Ortega-Vazquez07spinResCostBen.pdf:Ortega-Vazquez07spinResCostBen.pdf:PDF},
  groups    = {Use, doReadNonWPV_2},
  keywords  = {approximate calculations;cost-benefit analysis;generation outages;iterative process;load changes;load shedding;probabilistic criteria;spinning reserve requirements;system operators;unit commitment;cost-benefit analysis;iterative methods;power system economics;},
  owner     = {sotterson},
  timestamp = {2012.02.09},
}

@Article{Boydstun80comp3DmaxReach,
  author    = {Louis E. Boydstun and Thomas J. Armstrong and Fred L. Bookstein},
  title     = {A comparison of three dimensional maximum reach estimation techniques},
  journal   = {Journal of Biomechanics},
  year      = {1980},
  volume    = {13},
  number    = {8},
  pages     = {717--724},
  issn      = {0021-9290},
  abstract  = {Spline, periodic spline and spherical harmonic maximum reach estimation procedures are compared and evaluated based on ease of usage, estimation error and bias. The comparison indicates that each method provides comparable estimates based on percent variance accounted for (R2) and standard error. Spherical harmonics, however, are less biased and provide accurate estimates of the maximum reach sphere at the north and south poles, whereas splines and periodic splines are substantially biased in these regions. Three dimensional computer graphic depictions of reach data for the seated operator illustrate these biases.},
  comment   = {Spherical harmonics a bit better than tensor splines (regular and periodic) for modeling radius vs. theta/phi. Relevant to wind dir regression?

Problem: model how far a machine operator can reach (r) given theta and phi. Model it with splines, periodic splines and spherical harmonics. Looks like this function is fairly smooth.

Spline
* is "local": outliers don't spread influence, are contained to a few bases
 -- outliers can be ameliorated by either careful knot placement or by having a lot of knots
 -- This is different than, say, a Fourier basis, where the effect of each outlier is spread across the whole analysis window (but is localized in frequency)
* must pad the ends with m extra knots on each end (m is the order)
* Make 2D splines by tensor product of divided differences.

Periodic Spline
* Make periodic by simply copying knots across the ends, or here, by copying divided differences (since it's a tensor). See Fig 5. and the text
* But rather than copying knots would it be better to do some kind of constrained optimization?
* Periodicity in theta (circular) but not in phi (so can't be spherically smooth). Is there no such thing as a spherical spline?

Spherical harmonics
* spherical basis makes is possible to enforce continuity in both theta and phi
* bases are harmonics in theta and phi
* spherical basis functions have almost zero correlation.
* Irregular sampling might be better but they may not use it b/c of practical reasons.

They sample regularly.
* Spherical best for few parameters (10), All about the same for 16 parameters. 10 is smaller than I'd expect.

Related paper here: http://www.ingentaconnect.com/content/hfes/hfproc/1979/00000023/00000003/art00003},
  doi       = {DOI: 10.1016/0021-9290(80)90357-7},
  file      = {Boydstun80comp3DmaxReach.pdf:Boydstun80comp3DmaxReach.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  timestamp = {2010.08.11},
  url       = {http://www.sciencedirect.com/science/article/B6T82-4CDHTWY-H/2/9f6b9ae38a2c9595e08a9c91f77fdbf5},
}

@Article{Kagerer13splnLSQregrssnIntro,
  author    = {Kagerer, Kathrin},
  title     = {A short introduction to splines in least squares regression analysis},
  year      = {2013},
  abstract  = {Splines are an attractive way of flexibly modeling a regression curve since
their basis functions can be included like ordinary covariates in regression settings. An
overview of least squares regression using splines is presented including many graphical
illustrations and comprehensive examples. Starting from two bases that are widely
used for constructing splines, three different variants of splines are discussed: simple
regression splines, penalized splines and smoothing splines. Further, restrictions such
as monotonicity constraints are considered. The presented spline variants are illustrated
and compared in a bivariate and a multivariate example with well-known data
sets. A brief computational guide for practitioners using the open-source software R
is given.
Key words. B-spline, truncated power basis, derivative, monotonicity, penalty,
smoothing spline, R.},
  comment   = {A really clear intro to spline regression. Has R code at the end.
- b-spline vs. natural spline
- truncated basis
- penalized splines (p-splines)
- multivariate splines.

Montonicity
 - equations for increasing or decreasing
 - but these are constraint matrix equations that must be solved jointly w/ cost function
 - e.g. penalized spline closed form solution would turn into some other fancy optimization (I used Matlab's fmincon()
 - be careful when building the shape constraint matrix, C, in eq. (13).  See my comments in the pdf.

Spline order/degree (either penalized or unpenalized):
- not too important if have lots of knots
- several opinions based on monotonicity, convexity, and visual smoothness
- my conclusion: a good choiorder k=4, degree=k-1=3

Number of knots (penalized spline)
- A couple suggestions from Ruppert02numKntsPenSpln (and one other of his papers).
- one rule of thumb is: min(n/4,35);
 -- n=num. of training points
 -- from Ruppert02numKntsPenSpln
 (however, is this the same "penalized spline?"  See Ruppert02*
- another rule of thumb is between 20 and 40 knots
 -- but it seems like you can't have 20 knots if you have only 10 pts.?
 -- from 2003 Ruppert book, which I don't have, and one other paper I didn't look at

Matrix math/Matlab
* Eilers96FlexSmthBsplnPnlty has a linear knot matlab implementation * partial matrix math explanation
* Eilers10SplnsKntsPnlties partly explains how to do periodic regression
* Eilers03calTemp2Dpspline has analytical expression for 2D splines},
  file      = {Kagerer13splnLSQregrssnIntro.pdf:Kagerer13splnLSQregrssnIntro.pdf:PDF},
  owner     = {sotterson},
  publisher = {University of Regensburg, Faculty of Business, Economics and Management Information Systems},
  timestamp = {2014.10.30},
  url       = {http://epub.uni-regensburg.de/27968/},
}

@Book{Nolan15StableDistBootCh1,
  title     = {Stable Distributions - Models for Heavy Tailed Data},
  publisher = {Birkhauser},
  year      = {2015},
  author    = {J. P. Nolan},
  note      = {In progress, Chapter 1 online at academic2.american.edu/$\sim$jpnolan},
  abstract  = {Stable distributions are a rich class of probability distributions that allow skewness and heavy tails and have many intriguing mathematical properties. The class was characterized by Paul L?evy in his study of sums of independent identically distributed terms in the 1920?s.

The lack of closed formulas for densities and distribution functions for all but a few stable distributions (Gaussian, Cauchy and L?evy, see Figure 1.1), has been a major drawback to the use of stable distributions by practitioners. There are now reliable computer programs to compute stable densities, distribution functions and quantiles. With these programs, it is possible to use stable models in a variety of practical problems.

This book describes the basic facts about univariate and multivariate stable distributions, with an emphasis on practical applications. Part I focuses on univariate stable laws. This chapter describes basic properties of univariate stable distributions. Chapter 2 gives examples of stable laws arising in different problems. Chapter 3 gives proofs of the results in this chapter, as well as more technical details about stable distributions. Chapter 4 describes methods of fitting stable models to data. This structure is continued in Part II, which concerns multivariate stable laws. Chapters 5, 6, and 8 give basic facts about multivariate stable distributions, proofs and technical results, and estimation respectively. Part III is about stable regression, stable times series, and general stable processes. At the end of the book, Part IV describes related distributions and the appendices give tables of stable quantiles, modes and asymptotic standard deviations of maximum likelihood estimators of stable parameters.

Stable distributions have been proposed as a model for many types of physical and economic systems. There are several reasons for using a stable distribution to describe a system. The first is where there are solid theoretical reasons for expecting a non-Gaussian stable model, e.g. reflection off a rotating mirror yielding a Cauchy distribution, hitting times for a Brownian motion yielding a L'evy distribution, the gravitational field of stars yielding the Holtsmark distribution; see Feller (1971) and Uchaikin and Zolotarev (1999) for these and other examples. The second reason is the Generalized Central Limit Theorem which states that the only possible non-trivial limit of normalized sums of independent identically distributed terms is stable. It is argued that some observed quantities are the sum of many small terms - the price of a stock, the noise in a communication system, etc. and hence a stable model should be used to describe such systems. The third argument for
modeling with stable distributions is empirical: many large data sets exhibit heavy tails and skewness. The strong empirical evidence for these features combined with the Generalized Central Limit Theorem is used by many to justify the use of stable models. Examples in finance and economics are given in Mandelbrot (1963), Fama (1965), Samuelson (1967), Roll (1970), Embrechts et al. (1997), Rachev and Mittnik (2000), McCulloch (1996); in communication systems by Stuck and Kleiner (1974), Zolotarev (1986), and Nikias and Shao (1995). Such data sets are poorly described by a Gaussian model, but can be well described by a stable distribution. Several recent monographs focus on stable models: Zolotarev (1986), Uchaikin and Zolotarev (1999), Christoph and Wolf (1992), Samorodnitsky and Taqqu (1994), Janicki and Weron (1994), and Nikias and Shao (1995). The related topic of modeling with the extremes of data and heavy tailed distributions is discussed in Embrechts et al. (1997), Adler et al. (1998), and in Reiss and Thomas (2001).},
  comment   = {Introductory chapter to stable distributions (stable means the weighted sum of independent distributions has the same distribution shape but is shifted (and scaled?). Gaussians are one type of stable distribution but the focus here is on non-Gaussian, and heavy tailed (which I think means "extreme valued"). Axel used these to compute extreme values for some project, and he, I and Dominik had a meeting about them.  I think he used this to get extreme values from bounded quantile regression cdfs.  Author of this book also provides code.

* stable distributions are unbounded but so are Gaussians and people use them for bounded variables b/c they work well enough:
https://books.google.de/books?id=sv8jGSVFra8C&pg=PA112&lpg=PA112&dq=bounded+stable+distribution&source=bl&ots=Yvk3pyE1Tx&sig=1B3Ee29QQrBft7cyT3yBgygDxJg&hl=en&sa=X&redir_esc=y\#v=onepage&q=bounded%20stable%20distribution&f=false
* this stable fit algorithm works from empirical cdf so don't need to literally sample, like Axel did:
http://math.bu.edu/people/mveillet/html/alphastablepub.html\#14
* can also get the pdf, given an alph-stable dist:
http://math.bu.edu/people/mveillet/html/alphastablepub.html\#7 (useful if want to convolve).
* another kind of distribution that can be estimated from quantiles: Keelin11QuantParamDist
* alpha-stable dist. applications PhD Thesis, including regression: Frain09alphaDistAppsThesis

* TOC lists hazard functions as an application but that's in the unpublished Chapter 2 of this book.

* chapter has a bunch of formulas for sums of stable distributions. Maybe good for ReWP?
* truncated stable distribution here: Menn09smthTruncStblDist
* another intro w/ mention of trunc., est, from quantiles: Borak05stableDist
The author's stable distribution page has some nice illustrations and extensions e.g. to multivariate:
http://academic2.american.edu/~jpnolan/stable/stable.html

Other ways of estimating a distribution from quantiles
* general: Cook10distFromQuant
* Johnson System: George11estJohnsonDistBnd},
  file      = {Nolan15StableDistBootCh1.pdf:Nolan15StableDistBootCh1.pdf:PDF},
  location  = {Boston},
  url       = {http://academic2.american.edu/~jpnolan/stable/stable.html},
}

@Article{Carpenter17StanProbProgLang,
  author    = {Bob Carpenter and Andrew Gelman and Matthew D. Hoffman and Daniel Lee and Ben Goodrich and Michael Betancourt and Marcus Brubaker and Jiqiang Guo and Peter Li and Allen Riddell},
  title     = {Stan: A Probabilistic Programming Language},
  journal   = {Journal of Statistical Software},
  year      = {2017},
  volume    = {76},
  number    = {1},
  abstract  = {Stan is a probabilistic programming language for specifying statistical models. A Stan
program imperatively defines a log probability function over parameters conditioned on
specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference
for continuous-variable models through Markov chain Monte Carlo methods such as the
No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized
maximum likelihood estimates are calculated using optimization methods such as the
limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm.
Stan is also a platform for computing log densities and their gradients and Hessians,
which can be used in alternative algorithms such as variational Bayes, expectation propa-
gation, and marginal inference using approximate integration. To this end, Stan is set up
so that the densities, gradients, and Hessians, along with intermediate quantities of the
algorithm such as acceptance probabilities, are easily accessible.
Stan can be called from the command line using the cmdstan package, through R using
the rstan package, and through Python using the pystan package. All three interfaces sup-
port sampling and optimization-based inference with diagnostics and posterior analysis.
rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter
transforms, and specialized plotting.

Keywords: probabilistic program, Bayesian inference, algorithmic differentiation, Stan.},
  comment   = {probabilistic programming language that can do covariance/precision matrix estimation, with cholesky tricks too.

Is really liked by Andrew Gelman, that statistics guru who says it's vastly faster than others stuff.},
  doi       = {10.18637/jss.v076.i01},
  file      = {:Carpenter17StanProbProgLang.pdf:PDF},
  publisher = {Foundation for Open Access Statistic},
}

@Unpublished{Lockhart13collin6ways,
  author    = {Leland Lockhart},
  title     = {6~{W}ays to Address Collinearity in Regression Models},
  note      = {Blog post on Learn it daily.},
  month     = sep,
  year      = {2013},
  abstract  = {Standard linear regression methods are known to fail (or, at least, perform sub-optimally) in the presence of
highly correlated predictors. If the ultimate goal of the analysis is prediction (as opposed to interpretation of
specific predictor-outcome relationships), some additional processing may be needed in order to produce a
viable predictive model. In no particular order, we present six ways to deal with highly correlated data when
developing a linear regression model. It should be noted that the recommendations below apply specifically
to continuous outcome models, i.e., models in which the dependent variable is a real-valued number.},
  comment   = {How to deal with collinearity

1. Manual variable selection
- Use variance inflation factor
- Univariate correlations (but this doesn't catch multicollinarity)

2. Tree-based variable selection e.g. decision tree

3. Regression-based variable selection
- stepwise regression
- I suppose wrapper-based selection, too...

4. PCA

5. PLS

6. Shrinkage
- ridge regression
 -- a commenter mentioned "equity estimation," a constrained form of ridge regression.
- LASSO, etc.
- I think covariance shrinkage too?},
  file      = {Lockhart13collin6ways.pdf:Lockhart13collin6ways.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2014.07.06},
  url       = {http://learnitdaily.com/six-ways-to-address-collinearity-in-regression-models/},
}

@Book{Hastie09elemStatLrnBook,
  title     = {The Elements of Statistical Learning},
  publisher = {Springer},
  year      = {2009},
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome and Hastie, T and Friedman, J and Tibshirani, R},
  volume    = {2},
  number    = {1},
  edition   = {2},
  month     = jan,
  abstract  = {Statistical learning plays a key role in many areas of science, finance and
industry. Here are some examples of learning problems:
* Predict whether a patient, hospitalized due to a heart attack, will
have a second heart attack. The prediction is to be based on demographic,
diet and clinical measurements for that patient.
* Predict the price of a stock in 6 months from now, on the basis of
company performance measures and economic data.
* Identify the numbers in a handwritten ZIP code, from a digitized
image.
* Estimate the amount of glucose in the blood of a diabetic person,
from the infrared absorption spectrum of that person?s blood.
* Identify the risk factors for prostate cancer, based on clinical and
demographic variables.
The science of learning plays a key role in the fields of statistics, data
mining and artificial intelligence, intersecting with areas of engineering and
other disciplines.
This book is about learning from data. In a typical scenario, we have
an outcome measurement, usually quantitative (such as a stock price) or
categorical (such as heart attack/no heart attack), that we wish to predict
based on a set of features (such as diet and clinical measurements). We
have a training set of data, in which we observe the outcome and feature

measurements for a set of objects (such as people). Using this data we build
a prediction model, or learner, which will enable us to predict the outcome
for new unseen objects. A good learner is one that accurately predicts such
an outcome.
The examples above describe what is called the supervised learning problem.
It is called ?supervised? because of the presence of the outcome variable
to guide the learning process. In the unsupervised learning problem,
we observe only the features and have no measurements of the outcome.
Our task is rather to describe how the data are organized or clustered. We
devote most of this book to supervised learning; the unsupervised problem
is less developed in the literature, and is the focus of Chapter 14.
Here are some examples of real learning problems that are discussed in
this book.},
  comment   = {The classic statistical learning book that Henning and Olivier liked so much at DTU. Has R code!

* recommends k=5 or 10 for k-fold CV, but he's quoting some other paper
* relationship between ridge regression lamba and eigenvalues},
  file      = {Hastie09elemStatLrnBook_print10.pdf:Hastie09elemStatLrnBook_print10.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.07},
  url       = {https://web.stanford.edu/~hastie/Papers/ESLII.pdf},
}

@Article{Genton07spaceTimeCovMatrixApprox,
  author    = {Marc G. Genton},
  title     = {Separable approximations of space-time covariance matrices},
  journal   = {Environmetrics},
  year      = {2007},
  volume    = {18},
  number    = {7},
  pages     = {681--695},
  abstract  = {Statistical modeling of space-time data has often been based on separable covariance functions, that is, covariances that can be written as a product of a purely spatial covariance and a purely temporal covariance. The main reason is that the structure of separable covariances dramatically reduces the number of parameters in the covariance matrix and thus facilitates computational procedures for large space-time data sets. In this paper, we discuss separable approximations of nonseparable space-time covariance matrices. Specifically, we describe the nearest Kronecker product approximation, in the Frobenius norm, of a space-time covariance matrix. The algorithm is simple to implement and the solution preserves properties of the space-time covariance matrix, such as symmetry, positive definiteness, and other structures. The separable approximation allows for fast kriging of large space-time data sets. We present several illustrative examples based on an application to data of Irish wind speeds, showing that only small differences in prediction error arise while computational savings for large data sets can be obtained. Copyright ? 2007 John Wiley \& Sons, Ltd.},
  comment   = {Use for wind velocity basis function, upscaling, offsite observations?

See also: Porcu07covarStatSpaceTime, LI07nonParaAssessSpaceTimeCovar},
  doi       = {10.1002/env.854},
  owner     = {scotto},
  timestamp = {2008.12.27},
  url       = {http://www3.interscience.wiley.com/journal/114250866/abstract},
}

@Article{Feldmann14spatEnsPprocNonHomGR,
  author       = {Feldmann, Kira and Scheuerer, Michael and Thorarinsdottir, Thordis L.},
  title        = {Spatial Postprocessing of Ensemble Forecasts for Temperature Using Nonhomogeneous Gaussian Regression},
  journal      = {Monthly Weather Review},
  volume       = {143},
  number       = {3},
  pages        = {955--971},
  issn         = {0027-0644},
  abstract     = {Statistical postprocessing techniques are commonly used to improve the skill of ensembles from numerical weather forecasts. This paper considers spatial extensions of the well-established nonhomogeneous Gaussian regression ({NGR}) postprocessing technique for surface temperature and a recent modification thereof in which the local climatology is included in the regression model to permit locally adaptive postprocessing. In a comparative study employing 21-h forecasts from the Consortium for Small Scale Modelling ensemble predictive system over Germany ({COSMO}-{DE}), two approaches for modeling spatial forecast error correlations are considered: a parametric Gaussian random field model and the ensemble copula coupling ({ECC}) approach, which utilizes the spatial rank correlation structure of the raw ensemble. Additionally, the {NGR} methods are compared to both univariate and spatial versions of the ensemble Bayesian model averaging ({BMA}) postprocessing technique.},
  comment      = {Spatio-temporal ensemble calibration?  An example of nonhomogenious Gaussian Regression.},
  date         = {2014-10-29},
  doi          = {10.1175/MWR-D-14-00210.1},
  file         = {2014 arXiv paper:Feldmann14spatEnsPprocNonHomGR.pdf:PDF},
  owner        = {sotterson},
  shortjournal = {Mon. Wea. Rev.},
  timestamp    = {2016.11.10},
  url          = {http://journals.ametsoc.org/doi/abs/10.1175/MWR-D-14-00210.1},
  urldate      = {2016-11-10},
}

@Book{Harrell15rgrssnMdlStrtgyBook,
  title     = {Regression modeling strategies: with applications to linear models, logistic regression, and survival analysis},
  publisher = {Springer Science \& Business Media},
  year      = {2015},
  author    = {Harrell, Frank E},
  abstract  = {Statistics comprises among other areas study design, hypothesis testing, estimation, and prediction. This text aims at the last area, by presenting methods that enable an analyst to develop models that will make accurate predictions of responses for future observations. Prediction could be considered a superset of hypothesis testing and estimation, so the methods presented here will also assist the analyst in those areas. It is worth pausing to explain how this is so.},
  comment   = {Book corresponding to Harrell13rmsRpkg.  For model selection, says you must nest 10-fold cross-validation s/b repeated 50-100X, that bootstrap would require fewer model trainings (~300).

Actually, the author said the thing about CV in the blog post below, said that this book would explain why:
http://stats.stackexchange.com/questions/14516/understanding-bootstrapping-for-validation-and-model-selection
which pointed course for this book, here:
http://biostat.mc.vanderbilt.edu/wiki/Main/RmS

Model selection part of book might be worth reading, but I only have the slides.},
  doi       = {10.1007/978-3-319-19425-7},
  file      = {Slides:Harrell15rgrssnMdlStrtgyBook_slides.pdf:PDF},
  url       = {http://link.springer.com/book/10.1007%2F978-3-319-19425-7},
}

@InProceedings{Hertzler03classicalDistSDE,
  author      = {Hertzler, Greg},
  title       = {A Stochastic Differential Equation for Modeling the ?Classical? Probability Distributions},
  booktitle   = {47\textsuperscript{th} Conference},
  year        = {2003},
  number      = {57891},
  month       = feb,
  abstract    = {Stochastic differential equations are a flexible way to model continuous probability distributions. The most popular differential equations are for non-stationary Lognormal, non-stationary Normal and stationary Ornstein-Uhlenbeck distributions. The probability densities are known for these distributions and the assumptions behind the differential equations are well understood. Unfortunately, the assumptions do not fit most situations. In economics and finance, prices and quantities are usually stationary and positive. The Lognormal and Normal distributions are nonstationary and the Normal and Ornstein-Uhlenbeck distributions allow negative prices and quantities. This study derives a stochastic differential equation that includes most of the classical probability distributions as special cases and greatly expands the number distributions that can be used in models of stochastic dynamic systems.},
  comment     = {Which modeled pdfs are implied by which SDE formulations. An encyclopedia of beta-family distributions. A good reference for stochastic differential equations.},
  file        = {Hertzler03classicalDistSDE.pdf:Hertzler03classicalDistSDE.pdf:PDF},
  groups      = {PointDerived, doReadNonWPV_2},
  institution = {Australian Agricultural and Resource Economics Society},
  keywords    = {Research Methods/ Statistical Methods},
  location    = {Fremantle, Australia},
  owner       = {sotterson},
  timestamp   = {2014.02.06},
  url         = {http://EconPapers.repec.org/RePEc:ags:aare03:57891},
}

@Article{Pineda10scenarioRedRiskAvrs,
  author    = {Pineda, S. and Conejo, A.J.},
  title     = {Scenario reduction for risk-averse electricity trading},
  journal   = {Generation, Transmission Distribution, IET},
  year      = {2010},
  volume    = {4},
  number    = {6},
  pages     = {694--705},
  month     = jun,
  issn      = {1751-8687},
  abstract  = {Stochastic optimisation models used to identify risk-averse decisions in electricity futures markets are usually hard to solve because of the large number of scenarios representing the uncertain parameters involved. A novel scenario reduction technique is proposed to select those scenarios that, considering the risk aversion of the decision maker, best represent the original scenario set and make the optimisation problem tractable. Two case studies illustrate the performance of the proposed technique to reduce scenarios pertaining to both continuous and discrete uncertain parameters. The advantage of the proposed technique against the existing ones is apparent in highly risk-averse cases.},
  comment   = {Reducing stochastic programming scenarios explicitly considering risk (variance of return) sensitivity},
  doi       = {10.1049/iet-gtd.2009.0376},
  file      = {Pineda10scenarioRedRiskAvrs.pdf:Pineda10scenarioRedRiskAvrs.pdf:PDF},
  keywords  = {decision maker;electricity futures markets;risk aversion;risk-averse decisions;risk-averse electricity trading;scenario reduction technique;stochastic optimisation models;power markets;stochastic processes;},
  owner     = {scot},
  timestamp = {2010.11.22},
  url       = {http://ieeexplore.ieee.org.globalproxy.cvt.dk/search/srchabstract.jsp?tp=&arnumber=5473192&queryText%3DScenario+reduction+for+risk-averse+electricity+trading%26openedRefinements%3D*%26searchField%3DSearch+All},
}

@Conference{Bahramirad13pesProgDraft,
  author    = {Shay Bahramirad},
  title     = {Draft Program 5-31-13},
  booktitle = {2013 IEEE PES General Meeting -},
  year      = {2013},
  abstract  = {Stochastic optimization has been used for decades in the
power industry in mid-term hydrothermal scheduling},
  comment   = {A reference showing that power systems are already using stochastic optimization:

Session overview says that:
"Stochastic optimization has been used for decades in the power industry in mid-term hydrothermal scheduling"},
  file      = {Bahramirad13pesProgDraft.pdf:Bahramirad13pesProgDraft.pdf:PDF},
  groups    = {Use, doReadWPV_1},
  owner     = {sotterson},
  timestamp = {2013.10.02},
}

@Article{Dorrestijn13cnvctStochTimeCorr,
  author    = {Dorrestijn, J and Crommelin, DT and Biello, JA and B{\"o}ing, SJ},
  title     = {A data-driven multi-cloud model for stochastic parametrization of deep convection},
  journal   = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  year      = {2013},
  volume    = {371},
  number    = {1991},
  pages     = {20120374},
  abstract  = {Stochastic subgrid models have been proposed to capture the missing variability and
correct systematic medium term errors in general circulation models (GCMs). In
particular, the poor representation of subgrid scale deep convection is a persistent
problem which stochastic parameterizations are attempting to correct. In this paper
we construct such a subgrid model using data derived from large-eddy simulations
(LES) of deep convection. We use a data driven stochastic parametrization methodology
(Crommelin & Vanden-Eijnden 2008 and Dorrestijn et al. (submitted 2012)) to construct
a stochastic model describing a finite number of cloud states similar to the model of
Khouider et al. (2010). Our model emulates, in a computationally inexpensive manner,
the deep convection resolving LES. Transitions between the cloud states are modelled
with Markov chains. By conditioning the Markov chains on large-scale variables we obtain
a conditional Markov chain (CMC) which reproduces the time-evolution of the cloud
fractions. Furthermore, we show that the variability (standard deviation) and spatial
distribution of cloud types produced by the Markov chains becomes more faithful to the
LES data when local spatial coupling is introduced in the subgrid Markov chains. Such
spatially coupled Markov chains are equivalent to stochastic cellular automata (SCA).
Key words: conditional Markov chains, stochastic cellular automata, large-eddy simulation, climate,
variability.},
  comment   = {Calibration/stochastic models that preserve temporal correlations. In this case, convection models. Reference from Richarde Keane, in response to the question of how to do calibration w/o messing up temporal derivatives. Besides ensemble calibration, the grid aspect might be somehow useful for NWP grid upscaling or nodal forecasts, and maybe probabilsitic persistence (w/ the correct temporal dependencies).},
  file      = {Dorrestijn13cnvctStochTimeCorr.pdf:Dorrestijn13cnvctStochTimeCorr.pdf:PDF},
  owner     = {sotterson},
  publisher = {The Royal Society},
  timestamp = {2014.12.19},
  url       = {http://rsta.royalsocietypublishing.org/content/371/1991/20120374.short},
}

@Article{Safta14accelGridOptSDE,
  author    = {Safta, Cosmin and Chen, Richard L and Najm, Habib N and Pinar, Ali and others},
  title     = {Toward Using Surrogates to Accelerate Solution of Stochastic Electricity Grid Operations Problems},
  journal   = {arXiv preprint arXiv:1407.2232},
  year      = {2014},
  abstract  = {Stochastic unit commitment models typically handle
uncertainties in forecast demand by considering a finite number
of realizations from a stochastic process model for loads. Accurate
evaluations of expectations or higher moments for the quantities
of interest require a prohibitively large number of model evaluations.
In this paper we propose an alternative approach based
on using surrogate models valid over the range of the forecast
uncertainty. We consider surrogate models based on Polynomial
Chaos expansions, constructed using sparse quadrature methods.
Considering expected generation cost, we demonstrate the
approach can lead to several orders of magnitude reduction in
computational cost relative to using Monte Carlo sampling on
the original model, for a given target error threshold.

Index Terms: Stochastic Unit Commitment, Monte Carlo Sampling,
Polynomial Chaos Expansion},
  comment   = {These guys appear to use something like stochastic differential equations for unit commitment, etc. -- or plan to. They say for a deeper look into their representation, see ref [11], which is about SDE's. They say that this makes the solutions much faster.},
  file      = {Safta14accelGridOptSDE.pdf:Safta14accelGridOptSDE.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.13},
  url       = {http://arxiv.org/abs/1407.2232},
}

@Article{Manceau13techDiffusnPreannc,
  author    = {Delphine Manceau and Jehoshua Eliashberg and Vithala R. Rao and Meng Su},
  title     = {A Diffusion Model for Preannounced Products},
  journal   = {Customer Needs and Solutions},
  year      = {2013},
  volume    = {1},
  number    = {1},
  pages     = {77--89},
  month     = {dec},
  abstract  = {Strategic preannouncement of a new product
launch by a firm creates a pent-up demand (or consumers
committed to purchase prior to launch) for the new product.
The level of the pent-up demand depends, among other fac-
tors, on the timing and the reputation of the firm announcing
the new product; it is critical in shaping up the diffusion
process of the new product after launch. In this paper, we
develop a two-phase diffusion model that describes both the
impact of the announcement on consumers’ purchase com-
mitments and the diffusion process since launch starting with
a strictly positive number of new product orders. We illustrate
the empirical performance of the model with an old but classic
dataset that captures both advance purchase orders as well as
sales after launch for a new audio CD. We discuss how our
model can guide a firm’s decision on when to preannounce the
introduction of its new product or technology, relative to the
time of launch, so as to maximize the total benefit during the
planning horizon.

Keywords Preannounced products . Credibility of firms .
Diffusion and adoption processes},
  comment   = {Related to pent up EV demand, where people are holding on their old gas cars, waiting for EVs to get a little better?},
  doi       = {10.1007/s40547-013-0003-7},
  file      = {:Manceau13techDiffusnPreannc.pdf:PDF},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{Bo09structOutAssocRgrsn,
  author    = {Liefeng Bo and Sminchisescu, C.},
  title     = {Structured output-associative regression},
  booktitle = {Computer Vision and Pattern Recognition (CVPR)},
  year      = {2009},
  pages     = {2403--2410},
  month     = jun,
  abstract  = {Structured outputs such as multidimensional vectors or graphs are frequently encountered in real world pattern recognition applications such as computer vision, natural language processing or computational biology. This motivates the learning of functional dependencies between spaces with complex, interdependent inputs and outputs, as arising e.g. from images and their corresponding 3d scene representations. In this spirit, we propose a new structured learning method-Structured Output-Associative Regression (SOAR)-that models not only the input-dependency but also the self-dependency of outputs, in order to provide an output re-correlation mechanism that complements the (more standard) input-based regressive prediction. The model is simple but powerful, and, in principle, applicable in conjunction with any existing regression algorithms. SOAR can be kernelized to deal with non-linear problems and learning is efficient via primal/dual formulations not unlike ones used for kernel ridge regression or support vector regression. We demonstrate that the method outperforms weighted nearest neighbor and regression methods for the reconstruction of images of handwritten digits and for 3D human pose estimation from video in the HumanEva benchmark.},
  comment   = {Regression w/ output correlations taken into account. Possible uses * outputs are multiple plants or turbines * outputs could be single plant or turbine across time -- like ENFOR's forecast time series? -- use this model instead handling input and output autoregression with whitening},
  doi       = {10.1109/CVPR.2009.5206699},
  file      = {Bo09structOutAssocRgrsn.pdf:Bo09structOutAssocRgrsn.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {3D human pose estimation;HumanEva benchmark;handwritten digits;image reconstruction;input-based regressive prediction;kernel ridge regression;multidimensional vectors;nonlinear problems;primal/dual formulations;real world pattern recognition applications;structured learning method;structured output-associative regression;support vector regression;learning (artificial intelligence);pose estimation;regression analysis;video signal processing;},
  owner     = {scot},
  timestamp = {2010.08.31},
}

@Article{Liu09crvRegisClustFunc,
  author    = {Xueli Liu and Mark C.K. Yang},
  title     = {Simultaneous curve registration and clustering for functional data},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {2009},
  volume    = {53},
  number    = {4},
  pages     = {1361--1376},
  issn      = {0167-9473},
  abstract  = {Study of dynamic processes in many areas of science has led to the appearance of functional data sets. It is often the case that individual trajectories vary both in the amplitude space and in the time space. We develop a coherent clustering procedure that allows for temporal aligning. Under this framework, closed form solutions of an EM type learning algorithm are derived. The method can be applied to all types of curve data but is particularly useful when phase variation is present. We demonstrate the method by both simulation studies and an application to human growth curves.},
  comment   = {Use for regime or phase error clustering? Could also be used to find ensemble analogs.},
  doi       = {DOI: 10.1016/j.csda.2008.11.019},
  file      = {Liu09crvRegisClustFunc.pdf:Liu09crvRegisClustFunc.pdf:PDF},
  groups    = {doReadNonWPV_1},
  owner     = {scot},
  timestamp = {2010.11.08},
  url       = {http://www.sciencedirect.com/science/article/B6V8V-4V2NK6S-4/2/846422ded70f10bf686af860b482b8d5},
}

@Article{Bach11lrnSubModCnvx,
  author    = {Bach, F.},
  title     = {Learning with Submodular Functions: A Convex Optimization Perspective},
  journal   = {Arxiv preprint arXiv:1111.6453},
  year      = {2011},
  month     = nov,
  abstract  = {Submodular functions are relevant to machine learning for mainly two reasons: (1) some problems may be expressed directly as the optimization of submodular functions and (2) the Lovasz extension of submodular functions provides a useful set of regularization functions for supervised and unsupervised learning. In this paper, we present the theory of submodular functions from a convex analysis perspective, presenting tight links between certain polyhedra, combinatorial optimization and convex optimization problems. In particular, we show how submodular function minimization is equivalent to solving a wide variety of convex optimization problems. This allows the derivation of new effcient algorithms for approximate submodular function minimization with theoretical guarantees and good practical performance. By listing many examples of submodular functions, we review various applications to machine learning, such as clustering or subset selection, as well as a family of structured sparsity-inducing norms that can be derived and used from submodular functions.},
  comment   = {Convex optimization tutorial, w/ references (and tutorial on) convex optimization},
  file      = {Bach11lrnSubModCnvx.pdf:Bach11lrnSubModCnvx.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2012.08.13},
  url       = {http://arxiv.org/abs/1111.6453},
}

@TechReport{Akbarinia11sumRcrsProb,
  author      = {Akbarinia, Reza and Valduriez, Patrick and Verger, Guillaume},
  title       = {{SUM} Query Processing over Probabilistic Data},
  institution = {INRIA},
  year        = {2011},
  type        = {Rapport de recherche},
  number      = {RR-7629},
  month       = may,
  abstract    = {{SUM queries are crucial for many applications that need to deal with probabilistic data. In this report, we are interested in the queries, called ALL\_SUM, that return all possible sum values and their probabilities. In general, there is no efficient solution for the problem of evaluating ALL\_SUM queries. But, for many practical applications, where aggregate values are small integers or real numbers with small precision, it is possible to develop efficient solutions. In this report, based on a recursive approach, we propose a complete solution for this problem. We implemented our solution and conducted an extensive experimental evaluation over synthetic and real-world data sets; the results show its effectiveness.}},
  affiliation = {ZENITH - INRIA Sophia Antipolis , Laboratoire d'Informatique de Robotique et de Micro{\'e}lectronique de Montpellier - LIRMM},
  comment     = {A fast recursive summing procedure for ALL possible sums of probabilistic integers (handles floating point by quantization). Algorithm looks simple.

Maybe useful for probabilistic forecast aggregation.},
  file        = {Akbarinia11sumRcrsProb.pdf:Akbarinia11sumRcrsProb.pdf:PDF},
  groups      = {Upscaling (prob), doReadNonWPV_2},
  hal_id      = {inria-00596020},
  keywords    = {Probabilistic databases; query processing; aggregate queries; SUM queries},
  language    = {Anglais},
  owner       = {sotterson},
  pages       = {21},
  timestamp   = {2014.03.21},
  url         = {http://hal.inria.fr/inria-00596020},
}

@Article{Wood06LowRnkTensProSmthGAM,
  author    = {Wood, Simon N.},
  title     = {Low-Rank Scale-Invariant Tensor Product Smooths for Generalized Additive Mixed Models},
  journal   = {Biometrics},
  year      = {2006},
  volume    = {62},
  number    = {4},
  pages     = {1025--1036},
  issn      = {1541-0420},
  abstract  = {Summary A general method for constructing low-rank tensor product smooths for use as components of generalized additive models or generalized additive mixed models is presented. A penalized regression approach is adopted in which tensor product smooths of several variables are constructed from smooths of each variable separately, these marginal smooths being represented using a low-rank basis with an associated quadratic wiggliness penalty. The smooths offer several advantages: (i) they have one wiggliness penalty per covariate and are hence invariant to linear rescaling of covariates, making them useful when there is no natural way to scale covariates relative to each other; (ii) they have a useful tuneable range of smoothness, unlike single-penalty tensor product smooths that are scale invariant; (iii) the relatively low rank of the smooths means that they are computationally efficient; (iv) the penalties on the smooths are easily interpretable in terms of function shape; (v) the smooths can be generated completely automatically from any marginal smoothing bases and associated quadratic penalties, giving the modeler considerable flexibility to choose the basis penalty combination most appropriate to each modeling task; and (vi) the smooths can easily be written as components of a standard linear or generalized linear mixed model, allowing them to be used as components of the rich family of such models implemented in standard software, and to take advantage of the efficient and stable computational methods that have been developed for such models. A small simulation study shows that the methods can compare favorably with recently developed smoothing spline ANOVA methods.},
  comment   = {A way to do a wind velocity basis? Seems to be a nice tensor spline with convenient smoothing.

Intro in: Wood03ThinPltRgrsSpln
Computation updated in: Wood08FastSmthGAM
Something like it used here: Wood14GnrlzdGAMlrgDat

Nice presentation slides also attached},
  doi       = {10.1111/j.1541-0420.2006.00574.x},
  file      = {Paper:Wood06LowRnkTensProSmthGAM.pdf:PDF;Slides:Wood06LowRnkTensProSmthGAM_slides.pdf:PDF},
  keywords  = {Computationally efficient, Generalized additive mixed model, (GAMM), Mixed effect variable coefficient model, Multiple penalties, Penalized regression, Scale invariant, Smooth interaction, Smoothing penalty, Spline, SS-ANOVA, Tensor product smooth},
  owner     = {sotterson},
  publisher = {Blackwell Publishing Inc},
  timestamp = {2014.07.25},
}

@Article{Parker00calMagTheory,
  author    = {Parker, Robert L.},
  title     = {Calibration of the pass-through magnetometerI. Theory},
  journal   = {Geophysical Journal, International},
  year      = {2000},
  volume    = {142},
  pages     = {371--38313},
  month     = aug,
  abstract  = {Summary By studying a simple model of a pass-through magnetometer we show that there are circumstances in which misleading results might arise if the spatial sensitivity of the instrument is not properly corrected. For example, if the core sample is not correctly centred, or the magnetometer itself is misaligned, serious distortion can appear in the inferred inclination distribution.The possibility of such errors warrants a thorough study of laboratory instruments and, as a first step, we require a spatial calibration, that is, an estimate of the sensitivity of the various coils to samples placed anywhere in the sensing region. Only when this information is available for laboratory magnetometers will it be possible to calculate suitable corrections. The fact that laboratory magnetometers employ superconducting material makes inferring the response from the geometry of the coils impractical because the field from a specimen is modified inside the instrument by image currents flowing in the superconducting elements. To overcome this obstacle we treat a very general calibration problem.We show that the sensitivity of a particular coil as a function of position obeys Laplace's equation, and therefore the description in space of the sensitivity is mathematically exactly the same as modelling the geomagnetic field. A calibration experiment consists of several hundred measurements performed on a tiny dipole sample, systematically positioned throughout the sensing volume of the instrument. From such observations we aim to construct a harmonic interpolating function that represents the response in the measurement region. The natural geometry for the problem is that of a cylinder, so we work from the cylindrical harmonic expansion of an equivalent magnetic field.Cylindrical harmonic expansions take the form of an infinite set of unknown functions, not just a collection of coefficients as with spherical harmonics. To build a suitable interpolating function from them we appeal to the principles of spline interpolation by constructing a model that minimizes some measure of response complexity. We examine in detail two such measures. The first corresponds to magnetic field energy; the second is a more abstract norm that smoothes more heavily than the energy norm, and whose Gram matrix elements can be found without recourse to lengthy numerical procedures. The second norm promises to form the basis of a practical programme of calibration.},
  comment   = {cylindrical harmonics fit w/ spline-like Hilbert space smoothing
* maybe use for lagged wind velocity basis function?
* how cylindrical harmonics are harder than spherical harmonics since you must estimate a function as well as coeffs * says that reproducing kernel spaces are more modern but chose simpler trick here
* is this somehow related to support vector regression?},
  doi       = {10.1046/j.1365-246x.2000.00171.x},
  file      = {Parker00calMagTheory.pdf:Parker00calMagTheory.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.08.30},
}

@InProceedings{Morales09valReservWind,
  author    = {Morales, J.M. and Conejo, A.J. and P\'{e}rez-Ruiz, J.},
  title     = {Economic valuation of reserves in power systems with high penetration of wind power},
  booktitle = {Power Engineering Society General Meeting, IEEE)},
  year      = {2009},
  pages     = {1--1},
  month     = {26-30},
  abstract  = {Summary form only given. This paper proposes a methodology to determine the required level of spinning and non-spinning reserves in a power system with a high penetration of wind power. The computation of the required reserve levels and their costs is achieved through a stochastic programming market-clearing model spanning a daily time horizon. This model considers the network constraints and takes into account the cost of both the load shedding and the wind spillage. The methodology proposed is illustrated using an example and a realistic case study. Some conclusions are finally drawn.},
  comment   = {Seems to have also been published in IEEE transactions on power systems, 2009},
  doi       = {10.1109/PES.2009.5260229},
  file      = {Morales09valReservWind.pdf:Morales09valReservWind.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  issn      = {1944-9925},
  keywords  = {load shedding;market-clearing model;power systems;stochastic programming;wind power penetration;costing;load shedding;power generation economics;stochastic programming;wind power plants;},
  owner     = {scot},
  timestamp = {2010.07.05},
}

@Article{Kneib09varSelGeoaddRgrssn,
  author    = {Kneib, Thomas and Hothorn, Torsten and Tutz, Gerhard},
  title     = {Variable Selection and Model Choice in Geoadditive Regression Models},
  journal   = {Biometrics},
  year      = {2009},
  volume    = {65},
  number    = {2},
  pages     = {626--634},
  issn      = {1541-0420},
  abstract  = {Summary Model choice and variable selection are issues of major concern in practical regression analyses, arising in many biometric applications such as habitat suitability analyses, where the aim is to identify the influence of potentially many environmental conditions on certain species. We describe regression models for breeding bird communities that facilitate both model choice and variable selection, by a boosting algorithm that works within a class of geoadditive regression models comprising spatial effects, nonparametric effects of continuous covariates, interaction surfaces, and varying coefficients. The major modeling components are penalized splines and their bivariate tensor product extensions. All smooth model terms are represented as the sum of a parametric component and a smooth component with one degree of freedom to obtain a fair comparison between the model terms. A generic representation of the geoadditive model allows us to devise a general boosting algorithm that automatically performs model choice and variable selection.},
  comment   = {Describes the varying coefficient and 2\textsuperscript{nd} order interaction terms in Fenske12strctAddQRthesis. Actually the description isn't complete so you'll have to follow a few more references.},
  doi       = {10.1111/j.1541-0420.2008.01112.x},
  file      = {Kneib09varSelGeoaddRgrssn.pdf:Kneib09varSelGeoaddRgrssn.pdf:PDF},
  keywords  = {Bivariate smoothing, Boosting, Functional gradient, Penalized splines, Space-varying effects},
  owner     = {sotterson},
  publisher = {Blackwell Publishing Inc},
  timestamp = {2014.11.08},
}

@Article{Sohn08svmQR,
  author    = {Sohn, I and Kim, S and Hwang, C and Lee, JW and Shim, J},
  title     = {Support vector machine quantile regression for detecting differentially expressed genes in microarray analysis},
  journal   = {Methods of information in medicine},
  year      = {2008},
  volume    = {47},
  number    = {5},
  pages     = {459},
  abstract  = {Summary
Objectives: One of the main objectives of microarray
analysis is to identify genes differentially expressed
under two distinct experimental conditions. This task is
complicated by the noisiness of data and the large
number of genes that are examined. Fold change (FC)
based gene selection often misleads because error
variability for each gene is heterogeneous in different
intensity ranges. Several statistical methods have been
suggested, but some of them result in high false positive
rates because they make very strong parametric
assumptions.
Methods: We present support vector quantile regression
(SVMQR) using iterative reweighted least squares
(IRWLS) procedure based on the Newton method instead
of usual quadratic programming algorithms. This
procedure makes it possible to derive the generalized
approximate cross validation (GACV) method for
choosing the parameters which affect the performance
of SVMAR. We propose SVMQR based on a novel
method for identifying differentially expressed genes
with a small number of replicated microarrays.
Results: We applied SVMQR to both three biological dataset
and simulated dataset and showed that it
performed more reliably and consistently than FC-based
gene selection, Newton?s method based on the posterior
odds of change, or the nonparametric t-test variant implemented
in significance analysis of microarrays (SAM).
Conclusions: The SVMQR method was an exploratory
method for cDNA microarray experiments to identify
genes with different expression levels between two
types of samples (e.g., tumor versus normal tissue).
The SVMQR method performed well in the situation
where error variability for each gene was heterogeneous
in intensity ranges.
Keywords
cDNA microarray, support vector machine, support
vector machine quantile regression},
  comment   = {quantile regression with support vector machines. Was done in high dim. gene problem (seems that there are ~6K) and is gradient based, and solved with IRWLS since the usual quadratic programming is impossible. Suggested that it is better than MARS (I'm not sure they showed it here, but I didn't check).},
  file      = {Sohn08svmQR.pdf:Sohn08svmQR.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2014.06.24},
  url       = {http://www.schattauer.de/de/magazine/uebersicht/zeitschriften-a-z/methods/contents/archivestandard/manuscript/10343.html},
}

@Article{Schmidt19LCOelecStrgProj,
  author   = {Oliver Schmidt and Sylvain Melchior and Adam Hawkes and Iain Staffell},
  title    = {Projecting the Future Levelized Cost of Electricity Storage Technologies},
  journal  = {Joule},
  year     = {2019},
  volume   = {3},
  number   = {1},
  pages    = {81 - 100},
  issn     = {2542-4351},
  abstract = {Summary
The future role of stationary electricity storage is perceived as highly uncertain. One reason is that most studies into the future cost of storage technologies focus on investment cost. An appropriate cost assessment must be based on the application-specific lifetime cost of storing electricity. We determine the levelized cost of storage (LCOS) for 9 technologies in 12 power system applications from 2015 to 2050 based on projected investment cost reductions and current performance parameters. We find that LCOS will reduce by one-third to one-half by 2030 and 2050, respectively, across the modeled applications, with lithium ion likely to become most cost efficient for nearly all stationary applications from 2030. Investments in alternative technologies may prove futile unless significant performance improvements can retain competitiveness with lithium ion. These insights increase transparency around the future competitiveness of electricity storage technologies and can help guide research, policy, and investment activities to ensure cost-efficient deployment.},
  comment  = {LCOS forecast of stationary electricity storage over 2015-50 shows Li Ion on top.  Surprising.  References learning rate paper so must use one somehow.  Good for WattPlan Grid.},
  doi      = {https://doi.org/10.1016/j.joule.2018.12.008},
  file     = {:Schmidt19LCOelecStrgProj.pdf:PDF},
  keywords = {electrical energy storage, levelized cost of storage, future cost, power, system applications, electricity storage competitiveness},
  url      = {http://www.sciencedirect.com/science/article/pii/S254243511830583X},
}

@Article{Dette08quantRgrsNonCross,
  author    = {Dette, Holger and Volgushev, Stanislav},
  title     = {Non-crossing non-parametric estimates of quantile curves},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year      = {2008},
  volume    = {70},
  number    = {3},
  pages     = {609--627},
  issn      = {1467-9868},
  abstract  = {Summary: Since the introduction by Koenker and Bassett, quantile regression has become increasingly important in many applications. However, many non-parametric conditional quantile estimates yield crossing quantile curves (calculated for various p(0,1)). We propose a new non-parametric estimate of conditional quantiles that avoids this problem. The method uses an initial estimate of the conditional distribution function in the first step and solves the problem of inversion and monotonization with respect to p(0,1) simultaneously. It is demonstrated that the new estimates are asymptotically normally distributed with the same asymptotic bias and variance as quantile estimates that are obtained by inversion of a locally constant or locally linear smoothed conditional distribution function. The performance of the new procedure is illustrated by means of a simulation study and some comparisons with the currently available procedures which are similar in spirit with the method proposed are presented.},
  comment   = {How to avoid quantile regression crossover for linear and spline QR. Also has spline QR regularization

* points out that crossing problem gets worse when have lots of inputs!
* has nice comparison w/ arrangement uncrossing, says Chernozhukov10qrNoCross},
  doi       = {10.1111/j.1467-9868.2008.00651.x},
  file      = {Tech Report:Dette08quantRgrsNonCross_TR.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_1},
  keywords  = {Conditional distribution, Crossing quantile curves, Local linear estimate, Nadaraya, Watson estimate, Quantile estimation},
  owner     = {sotterson},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2013.10.23},
}

@Article{Johnstone12econValProbFrcstPortfolio,
  author        = {Johnstone, D. J.},
  title         = {Log-optimal economic evaluation of probability forecasts},
  journal       = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  year          = {2012},
  volume        = {175},
  number        = {3},
  pages         = {661-689},
  __markedentry = {[Scott:1]},
  abstract      = {Summary.  The commercial test of an expert's probability assessments is not that they are accurate in an abstract sense, but that they yield financial returns to decision makers. From this utilitarian standpoint, a model or forecaster is merely a font of cash pay-offs, like any other form of asset or security. The modern perspective in finance theory is that individual ‘securities’ (sources of cash pay-offs) must be valued in portfolio rather than of themselves. Applying portfolio methods to forecast evaluation, the theoretical worth of a forecast depends on its marginal contribution to the best available portfolio of securities. When considered within a log-optimal (maximum E[ log (wealth)]) portfolio, the value of an individual forecast (or forecaster) depends on both its expected cash pay-off and the covariance of its pay-off with those from all other available securities. In effect, portfolio theory rewards forecasters more for making accurate forecasts when other forecasters (or, more broadly, other sources of pay-offs) perform badly than when all or most forecasters do well. Conversely, the penalty for being wrong is reduced when other forecasters are right. This has the effect of promoting original thinking and unique (idiosyncratic) forecasting expertise. Herding by resort to industry standard models or routines is discouraged.},
  comment       = {How compute the comparitive value of probabilistic forecasts within a portfolio of forecasts.},
  doi           = {10.1111/j.1467-985X.2011.01011.x},
  eprint        = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-985X.2011.01011.x},
  file          = {:Johnstone12econValProbFrcstPortfolio.pdf:PDF},
  keywords      = {Economic forecast evaluation, Portfolio theory, Probability forecast},
  url           = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-985X.2011.01011.x},
}

@Article{Wang07autoRegrOrderLasso,
  author               = {Wang, Hansheng and Li, Guodong and Tsai, Chih-Ling},
  title                = {Regression coefficient and autoregressive order shrinkage and selection via the lasso},
  year                 = {2007},
  volume               = {69},
  number               = {1},
  month                = feb,
  pages                = {63--78},
  issn                 = {1467-9868},
  doi                  = {10.1111/j.1467-9868.2007.00577.x},
  abstract             = {Summary. The least absolute shrinkage and selection operator ('lasso') has been widely used in regression shrinkage and selection. We extend its application to the regression model with autoregressive errors. Two types of lasso estimators are carefully studied. The first is similar to the traditional lasso estimator with only two tuning parameters (one for regression coefficients and the other for autoregression coefficients). These tuning parameters can be easily calculated via a data-driven method, but the resulting lasso estimator may not be fully efficient. To overcome this limitation, we propose a second lasso estimator which uses different tuning parameters for each coefficient. We show that this modified lasso can produce the estimator as efficiently as the oracle. Moreover, we propose an algorithm for tuning parameter estimates to obtain the modified lasso estimator. Simulation studies demonstrate that the modified estimator is superior to the traditional estimator. One empirical example is also presented to illustrate the usefulness of lasso estimators. The extension of the lasso to the autoregression with exogenous variables model is briefly discussed.},
  citeulike-article-id = {1048319},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/j.1467-9868.2007.00577.x},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/bpl/rssb/2007/00000069/00000001/art00005},
  citeulike-linkout-2  = {http://www3.interscience.wiley.com/cgi-bin/abstract/118490748/ABSTRACT},
  file                 = {Wang07autoRegrOrderLasso.pdf:Wang07autoRegrOrderLasso.pdf:PDF;Wang07autoRegrOrderLasso.pdf:Wang07autoRegrOrderLasso.pdf:PDF},
  journal              = {Journal of the Royal Statistical Society, Series B},
  keywords             = {lasso},
  location             = {Peking University, Beijing, People's Republic of China; University of Hong Kong, People's Republic of China; University of California at Davis, USA},
  owner                = {sotterson},
  posted-at            = {2009-08-08 07:19:20},
  publisher            = {Blackwell Publishing},
  timestamp            = {2009.08.17},
}

@Article{Gneiting07frcstCalSharp,
  author    = {Gneiting, T. and Balabdaoui, F. and Raftery, A. E.},
  title     = {Probabilistic forecasts, calibration and sharpness},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year      = {2007},
  volume    = {69},
  number    = {2},
  pages     = {243--268},
  issn      = {1467-9868},
  abstract  = {Summary.? Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.

Keywords:

Cross-validation;
Density forecast;
Ensemble prediction system;
Ex post evaluation;
Forecast verification;
Model diagnostics;
Posterior predictive assessment;
Predictive distribution;
Prequential principle;
Probability integral transform;
Proper scoring rule},
  booktitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  comment   = {Highly cited, seems fundamental

Attached pdf is actually the pdf from citeseerx:
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.80.8772
but the abstract wording, at least, matches the Journal article abstract.},
  doi       = {10.1111/j.1467-9868.2007.00587.x},
  file      = {Tech Note, not Journal Paper:Gneiting07frcstCalSharp.pdf:PDF},
  groups    = {Ensemble, Test, CitaviImport1, doReadWPV_1},
  keywords  = {probabilistic forecast},
  ncite     = {302},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2013.09.26},
}

@Article{Reich12spatTempQR,
  author    = {Reich, Brian J.},
  title     = {Spatiotemporal quantile regression for detecting distributional changes in environmental processes},
  journal   = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  year      = {2012},
  volume    = {61},
  number    = {4},
  pages     = {535--553},
  issn      = {1467-9876},
  abstract  = {Summary.??? Climate change may lead to changes in several aspects of the distribution of climate variables, including changes in the mean, increased variability and severity of extreme events. We propose the use of spatiotemporal quantile regression as a flexible and interpretable method for simultaneously detecting changes in several features of the distribution of climate variables. The spatiotemporal quantile regression model assumes that each quantile level changes linearly in time, permitting straightforward inference on the time trend for each quantile level. Unlike classical quantile regression which uses model-free methods to analyse a single quantile or several quantiles separately, we take a model-based approach which jointly models all quantiles, and thus the entire response distribution. In the spatiotemporal quantile regression model, each spatial location has its own quantile function that evolves over time, and the quantile functions are smoothed spatially by using Gaussian process priors. We propose a basis expansion for the quantile function that permits a closed form for the likelihood and allows for residual correlation modelling via a Gaussian spatial copula. We illustrate the methods by using temperature data for the south-east USA from the years 1931???2009. For these data, borrowing information across space identifies more significant time trends than classical non-spatial quantile regression. We find a decreasing time trend for much of the spatial domain for monthly mean and maximum temperatures. For the lower quantiles of monthly minimum temperature, we find a decrease in Georgia and Florida, and an increase in Virginia and the Carolinas.},
  comment   = {Use for upscaling/aggregating, nodal forecasts, etc. They're looking for distributional weather changes so maybe good for regime detection too. Compare with quantile tree methods in Meinshausen06qrForests, Bhat11quantRgrsnTrees,... Use for IRPWIND?

Cites Oh11fastSmthQuantreg},
  doi       = {10.1111/j.1467-9876.2011.01025.x},
  groups    = {Ensemble, doReadWPV_2},
  keywords  = {Bayesian hierarchical model, Climate change, Non-Gaussian data, US temperature data, Warming hole},
  owner     = {sotterson},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2014.03.30},
}

@Article{Glasbey08spatioTempAutoRegressSolar,
  author    = {C. A. Glasbey and D. J. Allcroft},
  title     = {A spatiotemporal auto-regressive moving average model for solar radiation},
  year      = {2008},
  volume    = {57},
  number    = {3},
  pages     = {343--355},
  doi       = {10.1111/j.1467-9876.2007.00617.x},
  url       = {http://www3.interscience.wiley.com/journal/119396495/abstract},
  abstract  = {Summary.?To investigate the variability in energy output from a network of photovoltaic cells, solar radiation was recorded at 10 sites every 10 min in the Pentland Hills to the south of Edinburgh. We identify spatiotemporal auto-regressive moving average models as the most appropriate to address this problem. Although previously considered computationally prohibitive to work with, we show that by approximating using toroidal space and fitting by matching auto-correlations, calculations can be substantially reduced. We find that a first-order spatiotemporal auto-regressive (STAR(1)) process with a first-order neighbourhood structure and a Matern noise process provide an adequate fit to the data, and we demonstrate its use in simulating realizations of energy output.},
  groups    = {PV},
  journal   = {Journal of the Royal Statistical Society, Series C},
  owner     = {scotto},
  timestamp = {2008.12.27},
}

@Article{Lindgren11linkGaussMMRFandSDE,
  author    = {Lindgren, Finn and Rue, H{\aa}vard and Lindstr{\"o}m, Johan},
  title     = {An explicit link between Gaussian fields and Gaussian Markov random fields: the stochastic partial differential equation approach},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year      = {2011},
  volume    = {73},
  number    = {4},
  pages     = {423--498},
  issn      = {1467-9868},
  abstract  = {Summary.Continuously indexed Gaussian fields (GFs) are the most important ingredient in spatial statistical modelling and geostatistics. The specification through the covariance function gives an intuitive interpretation of the field properties. On the computational side, GFs are hampered with the big n problem, since the cost of factorizing dense matrices is cubic in the dimension. Although computational power today is at an all time high, this fact seems still to be a computational bottleneck in many applications. Along with GFs, there is the class of Gaussian Markov random fields (GMRFs) which are discretely indexed. The Markov property makes the precision matrix involved sparse, which enables the use of numerical algorithms for sparse matrices, that for fields in  only use the square root of the time required by general algorithms. The specification of a GMRF is through its full conditional distributions but its marginal properties are not transparent in such a parameterization. We show that, using an approximate stochastic weak solution to (linear) stochastic partial differential equations, we can, for some GFs in the Mat??rn class, provide an explicit link, for any triangulation of , between GFs and GMRFs, formulated as a basis function representation. The consequence is that we can take the best from the two worlds and do the modelling by using GFs but do the computations by using GMRFs. Perhaps more importantly, our approach generalizes to other covariance functions generated by SPDEs, including oscillating and non-stationary GFs, as well as GFs on manifolds. We illustrate our approach by analysing global temperature data with a non-stationary model defined on a sphere.

Keywords: Approximate Bayesian inference; Covariance functions; Gaussian fields; Gaussian
Markov random fields; Latent Gaussian models; Sparse matrices; Stochastic partial differential
equations},
  comment   = {Referenced by Tastu15spcTimeTrajGaussCpla as the link between  stochastic partial differential equations (SDE, SPDE?) and "some kind of precision matrix."

RELATED
* Rue02fitGausMarkovRndFlds
* Simpson12spatStatForgetCov},
  doi       = {10.1111/j.1467-9868.2011.00777.x},
  file      = {Lindgren11linkGaussMMRFandSDE.pdf:Lindgren11linkGaussMMRFandSDE.pdf:PDF},
  keywords  = {Approximate Bayesian inference, Covariance functions, Gaussian fields, Gaussian Markov random fields, Latent Gaussian models, Sparse matrices, Stochastic partial differential equations},
  owner     = {sotterson},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2017.06.15},
  url       = {http://dx.doi.org/10.1111/j.1467-9868.2011.00777.x},
}

@Article{Granger76frcstXfrmSeries,
  author    = {Granger, C.W.J. and Newbold, P.},
  title     = {Forecasting transformed series},
  journal   = {Journal of the Royal Statistical Society, Series B},
  year      = {1976},
  volume    = {38},
  number    = {2},
  pages     = {189--203},
  abstract  = {Suppose that a forecasting model is available for the process Xt but that interest centres on the instantaneous transformation yt = T(Xt). On the assumption that Xt is Gaussian and stationary, or can be reduced to stationarity by differencing, this paper examines the autocovariance structure of and methods for forecasting the transformed series. The development employs the Hermite polynomial expansion, thus allowing results to be derived for a very general class of instantaneous transformations. Keywords: TIME SERIES; FORECASTING; TRANSFORMED VARIABLES; NON-LINEAR FORECASTS; HERMITE POLYNOMIALS},
  comment   = {how to get confidence intervals and mean if have e.g. log transformed something you're forecasting (originally from Rolland Lowe, DTU)


See also:
Ahmed10empCompForecast
Fink09xformFAQs
Bremnes06compQuantileWind
Simonoff09transfRegrsn},
  file      = {Granger76frcstXfrmSeries.pdf:Granger76frcstXfrmSeries.pdf:PDF},
  owner     = {scot},
  publisher = {JSTOR},
  timestamp = {2011.05.16},
}

@Article{Buskirk15propenseLRvsRandFrst,
  author   = {Buskirk, Trent D. and Kolenikov, Stanislav},
  title    = {Finding Respondents in the Forest: A Comparison of Logistic Regression and Random Forest Models for Response Propensity Weighting and Stratification},
  journal  = {Survey Methods: Insights from the Field},
  year     = {2015},
  pages    = {17},
  issn     = {2296-4754},
  abstract = {Survey response rates for modern surveys using many different modes are trending downward leaving the potential for nonresponse biases

in estimates derived from using only the respondents. The reasons for nonresponse may be complex functions of known auxiliary variables or

unknown latent variables not measured by practitioners. The degree to which the propensity to respond is associated with survey outcomes

casts light on the overall potential for nonresponse biases for estimates of means and totals. The most common method for nonresponse

adjustments to compensate for the potential bias in estimates has been logistic and probit regression models. However, for more complex

nonresponse mechanisms that may be nonlinear or involve many interaction effects, these methods may fail to converge and thus fail to

generate nonresponse adjustments for the sampling weights. In this paper we compare these traditional techniques to a relatively new data

mining technique- random forests – under a simple and complex nonresponse propensity population model using both direct and propensity

stratification nonresponse adjustments. Random forests appear to offer marginal improvements for the complex response model over logistic

regression in direct propensity adjustment, but have some surprising results for propensity stratification across both response models.},
  file     = {:Buskirk15propenseLRvsRandFrst.pdf:PDF},
  urn      = {https://nbn-resolving.org/urn:nbn:de:0168-ssoar-427053},
}

@InProceedings{SantAnna11tsSymbSaxPerACA,
  author    = {Sant''Anna, A. and Wickstrom, N.},
  title     = {Symbolization of time-series: An evaluation of SAX, Persist, and {AC}A},
  booktitle = {Image and Signal Processing (CISP), 2011 4\textsuperscript{th} International Congress on},
  year      = {2011},
  volume    = {4},
  pages     = {2223--2228},
  abstract  = {Symbolization of time-series has successfully been used to extract temporal patterns from experimental data. Segmentation is an unavoidable step of the symbolization process, and it may be characterized on two domains: the amplitude and the temporal domain. These two groups of methods present advantages and disadvantages each. Can their performance be estimated a priori based on signal characteristics? This paper evaluates the performance of SAX, Persist and ACA on 47 different time-series, based on signal periodicity. Results show that SAX tends to perform best on random signals whereas ACA may outperform the other methods on highly periodic signals. However, results do not support that a most adequate method may be determined a priory.},
  comment   = {Hard to say but SAX maybe best on random signals; ACA best on very periodic (diurnal correction?) Is Sax not good for diurnal corrections, then?},
  doi       = {10.1109/CISP.2011.6100559},
  file      = {SantAnna11tsSymbSaxPerACA.pdf:SantAnna11tsSymbSaxPerACA.pdf:PDF},
  keywords  = {approximation theory;pattern clustering;signal processing;symbol manipulation;time series;ACA;Persist;SAX;aligned cluster analysis;amplitude domain;signal characteristics;signal periodicity;symbolic aggregate approximation;temporal domain;temporal pattern extraction;time-series symbolization;Approximation methods;Data mining;Databases;Educational institutions;Electrocardiography;Noise;Quantization},
  owner     = {sotterson},
  timestamp = {2013.03.15},
}

@Article{Meer18reviewProbPVfrcstNetDmd,
  author   = {D.W. van der Meer and J. Widén and J. Munkhammar},
  title    = {Review on probabilistic forecasting of photovoltaic power production and electricity consumption},
  journal  = {Renewable and Sustainable Energy Reviews},
  year     = {2018},
  volume   = {81},
  pages    = {1484 - 1512},
  issn     = {1364-0321},
  abstract = {tAccurate forecasting simultaneously becomes more important and more challenging due to the increasing penetration of photovoltaic (PV) systems in the built environment on the one hand, and the increasing stochastic nature of electricity consumption, e.g., through electric vehicles (EVs), on the other hand. Until recently, research has mainly focused on deterministic forecasting. However, such forecasts convey little information about the possible future state of a system and since a forecast is inherently erroneous, it is important to quantify this error. This paper therefore focuses on the recent advances in the area of probabilistic forecasting of solar power (PSPF) and load forecasting (PLF). The goal of a probabilistic forecast is to provide either a complete predictive density of the future state or to predict that the future state of a system will fall in an interval, defined by a confidence level. The aim of this paper is to analyze the state of the art and assess the different approaches in terms of their performance, but also to what extent these approaches can be generalized so that they not only perform best on the data set for which they were designed, but also on other data sets or different case studies. In addition, growing interest in net demand forecasting, i.e., demand less generation, is another important motivation to combine PSPF and PLF into one review paper and assess compatibility. One important finding is that there is no single preferred model that can be applied to any circumstance. In fact, a study has shown that the same model, with adapted parameters, applied to different case studies performed well but did not excel, when compared to models that were optimized for the specific task. Furthermore, there is need for standardization, in particular in terms of filtering night time data, normalizing results and performance metrics.},
  comment  = {Big review of PV generation and elecricity consumption.  Useful for net demand forecasting.},
  doi      = {https://doi.org/10.1016/j.rser.2017.05.212},
  file     = {:Meer18reviewProbPVfrcstNetDmd.pdf:PDF},
  keywords = {Probabilistic forecasting, Electricity consumption, Photovoltaic, Solar radiation, Irradiance, Prediction interval},
  url      = {http://www.sciencedirect.com/science/article/pii/S1364032117308523},
}

@Article{Cichocki14TensorNetworksBigOpt,
  author    = {Cichocki, Andrzej},
  title     = {Tensor Networks for Big Data Analytics and Large-Scale Optimization Problems},
  journal   = {arXiv preprint arXiv:1407.3124},
  year      = {2014},
  abstract  = {Tensor decompositions and tensor
networks are emerging and promising tools for data
analysis and data mining. In this paper we review
basic and emerging models and associated algorithms
for large-scale tensor networks, especially Tensor
Train (TT) decompositions using novel mathematical
and graphical representations. We discus the concept
of tensorization (i.e., creating very high-order tensors
from lower-order original data) and super compression
of data achieved via quantized tensor train (QTT)
networks. The main objective of this paper is to show
how tensor networks can be used to solve a wide
class of big data optimization problems (that are far
from tractable by classical numerical methods) by
applying tensorization and performing all operations
using relatively small size matrices and tensors and
applying iteratively optimized and approximative
tensor contractions.
Keywords: Tensor networks, tensor train (TT) decompositions,
matrix product states (MPS), matrix
product operators (MPO), basic tensor operations,
optimization problems for very large-scale problems:
generalized eigenvalue decomposition (GEVD),
PCA/SVD, canonical correlation analysis (CCA).},
  comment   = {Maybe a good tutorial on "quantized" tensor networks or trains, which allow representation of high dimensional data by low dimension tensors. Good for scenario tree reduction (is this like vines?) or high dimension optimization? Refers to several matlab toolboxes.

* table showing how it's related to deep learning
* Zhao14MultiNonLinPLSoverview is maybe a briefer version, a paper to read first?

Connections?
* multilinear, uncorrelated PCA ( Lu09UncorrMultiLinPCA)?
* vine copula decomposition e.g. Cooke10vinesArise
* representation learning (deep learning): Bengio13RepLrnRev},
  file      = {Cichocki14TensorNetworksBigOpt.pdf:Cichocki14TensorNetworksBigOpt.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.02.09},
  url       = {http://arxiv.org/abs/1407.3124},
}

@InProceedings{Althoff12OnlnTnsrFactPCA,
  author    = {Althoff, Alric},
  title     = {Online Tensor Factorization for Feature Selection in {EEG}},
  booktitle = {Presented at the 25\textsuperscript{th} annual UCSD Undergraduate Research Conference},
  year      = {2012},
  abstract  = {Tensor decompositions are a valuable tool in data analysis, but the computational
cost of standard tensor algorithms quickly becomes prohibitive, especially when considering
large and time-evolving data sets such as those found in signal processing
applications. In this work multilinear PCA, a common tensor analysis technique, will
be modified to enable the processing of large scale tensorial time-evolving data, such
as EEG, with much improved performance both in terms of memory and CPU time.},
  comment   = {Simple randomizing algorithm for speeding up tensor multilinear PCA (e.g. Lu09UncorrMultiLinPCA), but is online and adaptive (regime detection, adaptive feature selection, etc.},
  file      = {Althoff12OnlnTnsrFactPCA.pdf:Althoff12OnlnTnsrFactPCA.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.02.19},
  url       = {http://alricalthoff.com/sp12_honor_thesis_alric_althoff.pdf},
}

@Article{Donati18maxInfCoeffTl,
  author   = {Donati, Claudio and Albanese, Davide and Riccadonna, Samantha and Franceschi, Pietro},
  title    = {{A practical tool for maximal information coefficient analysis}},
  journal  = {GigaScience},
  year     = {2018},
  volume   = {7},
  number   = {4},
  month    = {04},
  issn     = {2047-217X},
  abstract = {{The ability of finding complex associations in large omics datasets, assessing their significance, and prioritizing them according to their strength can be of great help in the data exploration phase. Mutual information-based measures of association are particularly promising, in particular after the recent introduction of the TICe and MICe estimators, which combine computational efficiency with superior bias/variance properties. An open-source software implementation of these two measures providing a complete procedure to test their significance would be extremely useful.Here, we present MICtools, a comprehensive and effective pipeline that combines TICe and MICe into a multistep procedure that allows the identification of relationships of various degrees of complexity. MICtools calculates their strength assessing statistical significance using a permutation-based strategy. The performances of the proposed approach are assessed by an extensive investigation in synthetic datasets and an example of a potential application on a metagenomic dataset is also illustrated.We show that MICtools, combining TICe and MICe, is able to highlight associations that would not be captured by conventional strategies.}},
  comment  = {Feature ranking etc. with mutual info functions.  Is a two step info screening procedure that overcomes (they say) the limitations of the maximal information coeffecient (MIC) pointed out in Kinney14mutMaxInfoEquit.  This paper cites Kinney14mutMaxInfoEquit so maybe this approach as solved those problems. },
  doi      = {10.1093/gigascience/giy032},
  eprint   = {http://oup.prod.sis.lan/gigascience/article-pdf/7/4/giy032/24620151/giy032.pdf},
  file     = {:Donati18maxInfCoeffTl.pdf:PDF},
  url      = {https://dx.doi.org/10.1093/gigascience/giy032},
}

@InProceedings{Guo14WindPowCrvWRF,
  author       = {Guo, Zhenhai and Xiao, Xia},
  title        = {Wind Power Assessment Based on a WRF Wind Simulation with Developed Power Curve Modeling Methods},
  booktitle    = {Abstract and Applied Analysis},
  year         = {2014},
  volume       = {2014},
  organization = {Hindawi Publishing Corporation},
  abstract     = {The accurate assessment of wind power potential requires not only the detailed knowledge of the local wind resource but also an
equivalent power curve with good effect for a local wind farm. Although the probability distribution functions (pdfs) of the wind
speed are commonly used, their seemingly good performance for distribution may not always translate into an accurate assessment
of power generation. This paper contributes to the development of wind power assessment based on the wind speed simulation of
weather research and forecasting (WRF)and two improved power curvemodeling methods.These approaches are improvements on
the power curvemodeling that is originally fitted by the single layer feed-forward neural network (SLFN) in this paper; in addition,
a data quality check and outlier detection technique and the directional curve modelingmethod are adopted to effectively enhance
the original model performance. The proposed two methods, named WRF-SLFN-OD and WRF-SLFN-WD, are able to avoid the
interference from abnormal output and the directional effect of local wind speed during the power curve modeling process. The
data examined are from three stations in northern China; the simulation indicates that the two developed methods have strong
abilities to provide a more accurate assessment of the wind power potential compared with the original methods.},
  comment      = {A good practical paper on how to estimate an NWP (WRF) to power curve from data. Includes bad data elimination, which is nice, and directional dependence (although this is only binning). Might be good for ReWP or just the Availon project, where the first part is Quality Control.},
  file         = {Guo14WindPowCrvWRF.pdf:Guo14WindPowCrvWRF.pdf:PDF;Guo14WindPowCrvWRF.pdf:Guo14WindPowCrvWRF.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2014.11.08},
  url          = {http://www.hindawi.com/journals/aaa/2014/941648/abs/},
}

@Article{Kaufman05perSplnRgrsn,
  author    = {Cari G. Kaufman and Valerie Ventura and Robert E. Kass},
  title     = {Spline-based nonparametric regression for periodic functions and its application to directional tuning of neurons},
  journal   = {Statistics in Medicine},
  year      = {2005},
  volume    = {24},
  pages     = {2255--2265},
  abstract  = {The activity of neurons in the brain often varies systematically with some quantitative feature of a stimulus or action. A well-known example is the tendency of the firing rates of neurons in the primary motor cortex to vary with the direction of a subject's arm or wrist movement. When this movement is constrained to vary in only two dimensions, the direction of movement may be characterized by an angle, and the neuronal firing rate can be written as a function of this angle. Thefiring rate function has traditionally been fit with a cosine, but recent evidence suggests that departures from cosine tuning occur frequently. We report here a new nonparametric regression method for fitting periodic functions and demonstrate its application to the fitting of neuronal data. The method is an extension of Bayesian Adaptive Regression Splines (BARS) and applies both to normal and non-normal data, including Poisson data, which commonly arise in neuronal applications. We compare the new method to a periodic version of smoothing splines and some parametric alternatives and and the new method to be especially valuable when the smoothness of the periodic function varies unevenly across its domain. KEYWORDS: nonparametric regression, periodic functions, Bayesian Adaptive Regression Splines, smoothing splines, neuronal data},
  comment   = {How to build a periodic basis. Can solve w/ linear regression but they do it for Bayesian stuff. They also talk about a sin(theta), cos(theta) basis and reference papers discussing the weakness of this approach. Has R (I think, Wallstrom08bayesSplineR) and Matlab,

Says that circular smoothing splines are best for a "uniform" periodic function but otherwise, their CBARS spline algorithm is best. Circular smoothers seem to be 2\textsuperscript{nd} best overall. The cosine basis w/ identity link function is best on a function that looks like a cosine.

Angular reproducing kernel (I think this is what they call a cosine basis later).
* build a angular reproducing kernel that starts with sin/cos harmonics
* simplifies approximately to cos terms only (reduces the dimension by a factor of 2)
 - but they don't say how many terms (out of up to infinity) they use.
* compare w/ Pritchard10varQuantProbFrcst for wind power app
 - still uses sin/cos
 - stops at order 2 interaction

Matlab and R are here:
http://www.stat.cmu.edu/~kass/bars/bars.html},
  file      = {Kaufman05perSplnRgrsn.pdf:Kaufman05perSplnRgrsn.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.08.10},
  url       = {http://www.stat.cmu.edu/~kass/papers/},
}

@Article{Huang09adaptPartLinRgrsn,
  author    = {Zhensheng Huang and Riquan Zhang},
  title     = {Efficient estimation of adaptive varying-coefficient partially linear regression model},
  journal   = {Statistics \& Probability Letters},
  year      = {2009},
  volume    = {79},
  number    = {7},
  pages     = {943--952},
  issn      = {0167-7152},
  abstract  = {The adaptive varying-coefficient partially linear regression (AVCPLR) model is proposed by combining the nonparametric regression model and varying-coefficient regression model with different smoothing variables. It can be seen as a generalization of the varying-coefficient partially linear regression model, and it is also an example of a generalized structured model as defined by Mammen and Neilsen [Mammen, E., Nielsen, J.P., 2003. Generalised structured models. Biometrika 90, 551-566]. Based on the local linear technique and the marginal integrated method, the initial estimators of these unknown functions are obtained, each of which has big variance. To decrease the variances of these initial estimators, the one-step backfitting technique proposed by Linton [Linton, O.B., 1997. Efficient estimation of additive nonparametric regression models. Biometrika 82, 93-100] is used to obtain the efficient estimators of all unknown functions for the AVCPLR model, and their asymptotic normalities are studied. Two simulated examples are given to illustrate the AVCPLR model and the proposed estimation methodology.},
  comment   = {sorta like cp-var but more flexible, maybe better adaptation?},
  doi       = {DOI: 10.1016/j.spl.2008.11.022},
  file      = {Huang09adaptPartLinRgrsn.pdf:Huang09adaptPartLinRgrsn.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.09.27},
  url       = {http://www.sciencedirect.com/science/article/B6V1D-4V2NNWT-2/2/63365fbebd13780cb94ca4c5bbf691c6},
}

@InProceedings{Solin15StateSpaceStudentTprocR,
  author    = {Solin, Arno and S{\"a}rkk{\"a}, Simo},
  title     = {State Space Methods for Efficient Inference in Student-t Process Regression.},
  booktitle = {AISTATS},
  year      = {2015},
  abstract  = {The added flexibility of Student-t processes
(TPs) over Gaussian processes (GPs) robus-
tifies inference in outlier-contaminated noisy
data. The uncertainties are better accounted
for than in GP regression, because the pre-
dictive covariances explicitly depend on the
training observations. For an entangled
noise model, the canonical-form TP regres-
sion problem can be solved analytically, but
the naive TP and GP solutions share the
same cubic computational cost in the num-
ber of training observations. We show how
a large class of temporal TP regression mod-
els can be reformulated as state space mod-
els, and how a forward filtering and back-
ward smoothing recursion can be derived for
solving the inference analytically in linear
time complexity. This is a novel finding
that generalizes the previously known con-
nection between Gaussian process regression
and Kalman filtering to more general ellipti-
cal processes and non-Gaussian Bayesian fil-
tering. We derive this connection, demon-
strate the benefits of the approach with ex-
amples, and finally apply the method to em-
pirical data},
  comment   = {Like Gaussian Processes but with Student-T (better by itself) and a state-space time model (should be more computationally efficient).  Kind of like a stochastic differential equation.  Is a scenario generator.

Can handle missing data inherently.

Paper and Poster are attached.},
  file      = {:Solin15StateSpaceStudentTprocR.pdf:PDF;:Solin15StateSpaceStudentTprocR_poster.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.20},
  url       = {http://proceedings.mlr.press/v38/solin15.pdf},
}

@Article{Peters13StochConvectObsModel,
  author    = {Peters, Karsten and Jakob, Christian and Davies, Laura and Khouider, Boualem and Majda, Andrew J},
  title     = {Stochastic behavior of tropical convection in observations and a multicloud model},
  journal   = {Journal of the Atmospheric Sciences},
  year      = {2013},
  volume    = {70},
  number    = {11},
  pages     = {3556--3575},
  abstract  = {The aim for a more accurate representation of tropical convection in global circulation models is a longstanding
issue. Here, the relationships between large and convective scales in observations and a stochastic
multicloud model (SMCM) to ultimately support the design of a novel convection parameterization with
stochastic elements are investigated. Observations of tropical convection obtained at Darwin and Kwajalein
are used here. It is found that the variability of observed tropical convection generally decreases with increasing
large-scale forcing, implying a transition from stochastic to more deterministic behavior with increasing
forcing. Convection shows a more systematic relationship with measures related to large-scale
convergence compared to measures related to energetics (e.g., CAPE). Using the observations, the parameters
in the SMCM are adjusted. Then, the SMCM is forced with the time series of the observed large-scale
state and the simulated convective behavior is compared to that observed. It is found that the SMCM cloud
fields compare better with observations when using predictors related to convergence rather than energetics.
Furthermore, the underlying framework of the SMCM is able to reproduce the observed functional dependencies
of convective variability on the imposed large-scale state?an encouraging result on the road
toward a novel convection parameterization approach. However, establishing sound cause-and-effect relationships
between tropical convection and the large-scale environment remains problematic and warrants
further research.},
  comment   = {Maybe for calibration/stochastic models that preserve temporal correlations. In this case, convection models. Reference from Richarde Keane, in response to the question of how to do calibration w/o messing up temporal derivatives. Besides ensemble calibration, the grid aspect might be somehow useful for NWP grid upscaling or nodal forecasts, and maybe probabilsitic persistence (w/ the correct temporal dependencies).},
  doi       = {10.1175/JAS-D-13-031.1},
  file      = {Peters13StochConvectObsModel.pdf:Peters13StochConvectObsModel.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.12.19},
}

@Article{Zaytar16wthFrcstWeq2seqRNNlstm,
  author     = {Mohamed Akram Zaytar and Chaker El Amrani},
  title      = {Sequence to Sequence Weather Forecasting with Long Short-Term Memory Recurrent Neural Networks},
  journal    = {International Journal of Computer Applications},
  year       = {2016},
  volume     = {143},
  number     = {11},
  pages      = {7-11},
  month      = {Jun},
  issn       = {0975-8887},
  abstract   = {The aim of this paper is to present a deep neural network
architecture and use it in time series weather prediction. It
uses multi stacked LSTMs to map sequences of weather values
of the same length. The final goal is to produce two types
of models per city (for 9 cities in Morocco) to forecast
24 and 72 hours worth of weather data (for Temperature,
Humidity and Wind Speed). Approximately 15 years (2000-2015)
of hourly meteorological data was used to train the model.
The results show that LSTM based neural networks are
competitive with the traditional methods and can be considered
a better alternative to forecast general weather conditions.

General Terms
Machine Learning, Weather Forecasting, Pattern Recognition, Times Series

Keywords
Deep Learning, Sequence to Sequence Learning, Artificial Neural
Networks, Recurrent Neural Networks},
  address    = {New York, USA},
  comment    = {An RNN LSTM weather forecast.  Authors seem genuine but the journal might be fake (https://www.quora.com/Is-IJCA-a-fake-journal)},
  doi        = {10.5120/ijca2016910497},
  file       = {Zaytar16wthFrcstWeq2seqRNNlstm.pdf:Zaytar16wthFrcstWeq2seqRNNlstm.pdf:PDF},
  issue_date = {June 2016},
  numpages   = {5},
  publisher  = {Foundation of Computer Science (FCS), NY, USA},
  url        = {http://www.ijcaonline.org/archives/volume143/number11/25119-2016910497},
}

@InProceedings{Dalto15deepNNultraShrtWindSpdPred,
  author    = {M. Dalto and J. Matuško and M. Vašak},
  title     = {Deep neural networks for ultra-short-term wind forecasting},
  booktitle = {Proc. IEEE Int. Conf. Industrial Technology (ICIT)},
  year      = {2015},
  pages     = {1657--1663},
  month     = mar,
  abstract  = {The aim of this paper is to present input variable selection algorithm and deep neural networks application to ultra-short-term wind prediction. Shallow and deep neural networks coupled with input variable selection algorithm are compared on the ultra-short-term wind prediction task for a set of different locations. Results show that carefully selected deep neural networks outperform shallow ones. Input variable selection use reduces the neural network complexity and simplifies deep neural network training.},
  doi       = {10.1109/ICIT.2015.7125335},
  file      = {:Dalto15deepNNultraShrtWindSpdPred.pdf:PDF},
  keywords  = {neural nets, wind power, deep neural network training, input variable selection algorithm, ultrashort-term wind forecasting, ultrashort-term wind prediction, Artificial neural networks, Complexity theory, Input variables, Predictive models, Training, Wind forecasting},
}

@InProceedings{Latinne01limNtreesRandForest,
  author    = {Patrice Latinne and Olivier Debeir and Christine Decaestecker},
  title     = {Limiting the Number of Trees in Random Forests},
  booktitle = {Multiple Classifier Systems},
  year      = {2001},
  abstract  = {The aim of this paper is to propose a simple procedure that a priori determines a minimum number of classifiers to combine in order to obtain a prediction accuracy level similar to the one obtained with the combination of larger ensembles. The procedure is based on the McNemar non-parametric test of significance. Knowing a priori the minimum size of the classifier ensemble giving the best prediction accuracy, constitutes a gain for time and memory costs especially for huge data bases and real-time applications. Here we applied this procedure to four multiple classifier systems with C4.5 decision tree (Breiman’s Bagging, Ho’s Random subspaces, their combination we labeled ’Bagfs’, and Breiman’s Random forests) and five large benchmark data bases. It is worth noticing that the proposed procedure may easily be extended to other base learning algorithms than a decision tree as well. The experimental results showed that it is possible to limit significantly the number of trees. We also showed that the minimum number of trees required for obtaining the best prediction accuracy may vary from one classifier combination method to another.},
  comment   = {Biau16randFrstGuideTour kind-of recommends this as a way to pick # trees.  Supposed to be simple.

Note that there's no penalty for picking too many trees, just for not picking enough:  Biau16randFrstGuideTour},
  file      = {:Latinne01limNtreesRandForest.pdf:PDF},
  url       = {https://link.springer.com/chapter/10.1007/3-540-48219-9_18},
}

@Article{Yeniay02plsrNcmpsCmpr,
  author   = {Yeniay, {\"O}zg{\"u}r and G{\"o}kta\c{s}, Atill},
  title    = {A comparison of partial least squares regression with other prediction methods},
  journal  = {Hacettepe Journal of Mathematics and Statistics},
  year     = {2002},
  volume   = {31},
  number   = {99},
  pages    = {99--101},
  abstract = {The aim of this study is to compare popular regression methods with the
partial least squares method. The paper presents a theoretical point of view,
as well as an application on a real data set. It is found that partial least
squares regression yields somewhat better results in terms of the predictive
ability of models obtained when compared to the other prediction methods.
Key Words: Ordinary least squares, Ridge regression, Principal component regression,
Partial least squares.},
  comment  = {A decent intro to PLSR and also advocates pickingnum. of components w/ cross-validation RMSE. Also, concludes that when data is collinear or has poor fit, PLSR is somewhat better than OLS, principal comps regression and ridge regression. It has the highest predictability with the lowest factor. This paper is also an OK PLSR tutorial.

* also explains how to do multi-output PLSR},
  file     = {Yeniay02plsrNcmpsCmpr.pdf:Yeniay02plsrNcmpsCmpr.pdf:PDF},
  url      = {http://www.hjms.hacettepe.edu.tr/uploads/ce7bdb8f-5f89-4f03-9822-a20e2ee35d03.pdf},
}

@Article{Nykvist19,
  author   = {Bj{\:o}rn Nykvist and Frances Sprei and M{\aa}ns Nilsson},
  title    = {Assessing the progress toward lower priced long range battery electric vehicles},
  journal  = {Energy Policy},
  year     = {2019},
  volume   = {124},
  pages    = {144 - 155},
  issn     = {0301-4215},
  abstract = {The aim of this study is to show the progress in attributes and prices of battery electric vehicles (BEV) and to analyse in which market segments long range BEV can be produced at comparable cost to conventional cars. We assess 48 models available to consumers since 1997, collecting data on attributes, weight and vehicle prices. We also provide an analysis of recent progress in battery pack costs. Based on this data, the share of BEV vehicle price that is related to the battery pack is modelled. To illustrate future progress we estimate when it is possible to produce a BEV with a 200 mile range in a given price percentile. We find that the price percentile where the price of a BEV is comparable to conventional cars changes in a nonlinear way when battery pack costs fall below 200–250 USD/kWh. Furthermore, we show that battery pack costs of 150 USD/kWh could imply that production costs of a BEV with a 200 mile range are cost competitive for almost 50% of the US car market segments by 2020. Finally, the most critical conditions for this development are discussed and assessed by sensitivity analysis applying conservative values to our model.},
  comment  = {A learning-rate based price forecast for long range electric cars.  Quite a bit about learning rates, actually.},
  doi      = {https://doi.org/10.1016/j.enpol.2018.09.035},
  file     = {:Nykvist19bevPriceFrcst.pdf:PDF},
  keywords = {Battery electric vehicle, Production, Attributes, Range, Market segments},
  url      = {http://www.sciencedirect.com/science/article/pii/S0301421518306487},
}

@Article{Feil05procTransKalmanSeg,
  author       = {Feil, Balazs \& Abonyi, Janos \& Nemeth, Sandor \& Arva, Peter},
  title        = {Monitoring Process Transitions by {Kalman} Filtering and Time-Series Segmentation},
  year         = {2005},
  volume       = {29},
  number       = {6},
  month        = jan,
  pages        = {1419--1427},
  doi          = {10.1016/j.compchemeng.2005.02.014},
  url          = {http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6TFT-4FNDS1X-3&_user=10&_rdoc=1&_fmt=&_orig=search&_sort=d&view=c&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=ae73346782f51eb713eed1204aefa813},
  abstract     = {The analysis of historical process data of technological systems plays important role in process monitoring, modelling and control. Time-series segmentation algorithms are often used to detect homogenous periods of operation-based on input?output process data. However, historical process data alone may not be sufficient for the monitoring of complex processes. This paper incorporates the first-principle model of the process into the segmentation algorithm. The key idea is to use a model-based non-linear state-estimation algorithm to detect the changes in the correlation among the state-variables. The homogeneity of the time-series segments is measured using a PCA similarity factor calculated from the covariance matrices given by the state-estimation algorithm. The whole approach is applied to the monitoring of an industrial high-density polyethylene plant.},
  file         = {Feil05procTransKalmanSeg.pdf:Feil05procTransKalmanSeg.pdf:PDF;Feil05procTransKalmanSeg.pdf:Feil05procTransKalmanSeg.pdf:PDF},
  journal      = {Computers and Chemical Engineering},
  lang         = { e},
  organization = {Folyamatm?rn?ki Int?zeti Tansz?k},
  owner        = {sotterson},
  refereed     = { Refereed},
  sortkey      = { 10},
  timestamp    = {2009.03.09},
}

@Article{Oczeretko07uterineSynch,
  author    = {Edward Oczeretko and Agnieszka Kitlas and Marta Borowska and Jolanta Swiatecka and Tadeusz Laudanski},
  title     = {Uterine Contractility: Visualization of Synchronization Measures in Two Simultaneously Recorded Signals},
  journal   = {Annals of the New York Academy of Sciences. Issue Reproductive Biomechanics},
  year      = {2007},
  volume    = {1101},
  pages     = {1749--6632},
  abstract  = {The analysis of the uterine contraction signals in nonpregnant states gives information about physiological changes during the menstrual cycle. Spontaneous uterine activity was recorded directly by a dual microtip catheter. The device consisted of two ultra-miniature pressure sensors. One sensor was placed in the fundus, the other in the cervix. It was important to identify time delays between contractions in two topographic locations, which may be of potential diagnostic significance in various pathologies: dysmenorrhea, endometriosis, and fecundity disorders. In this study the following synchronization measures2014the cross-correlation, the semblance, the mutual information2014were used to visualize the time delay changes over time. These measures were computed in a moving window with a width corresponding to approximately two or three contractions. As a result, the running synchronization functions were obtained. The running synchronization functions visualize changes in the propagation of the two simultaneously recorded signals. The propagation\% parameter assessed from these functions allows for quantitative description of synchronization. Finally, we illustrate the use of running synchronization functions to investigate the effect of treatment with tamoxifen on primary dysmenorrhea.},
  comment   = {Synchonization detection using correlation, semblance and mutual information * intersting because they detect synchronization using semblance -- used in geophysics but I've never heard of it * rest of article not very interesting: no conclusions},
  doi       = {10.1196/annals.1389.007},
  file      = {Oczeretko07uterineSynch.pdf:Oczeretko07uterineSynch.pdf:PDF;Oczeretko07uterineSynch.pdf:Oczeretko07uterineSynch.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2008.10.10},
  url       = {http://www3.interscience.wiley.com/journal/117985484/abstract},
}

@Article{Shrestha06expAdaboostRT,
  author    = {Shrestha, D and Solomatine, D},
  title     = {Experiments with AdaBoost. {RT}, an improved boosting scheme for regression},
  journal   = {Neural computation},
  year      = {2006},
  volume    = {18},
  number    = {7},
  pages     = {1678--1710},
  abstract  = {The application of boosting technique to regression problems has received
relatively little attention in contrast to research aimed at classification
problems. This letter describes a new boosting algorithm, AdaBoost.
RT, for regression problems. Its idea is in filtering out the examples
with the relative estimation error that is higher than the preset threshold
value, and then following the AdaBoost procedure. Thus, it requires
selecting the suboptimal value of the error threshold to demarcate examples
as poorly or well predicted. Some experimental results using the M5
model tree as a weak learning machine for several benchmark data sets
are reported. The results are compared to other boosting methods, bagging,
artificial neural networks, and a single M5 model tree. The preliminary
empirical comparisons show higher performance of AdaBoost.RT
for most of the considered data sets.},
  comment   = {A continuous boosting technique},
  file      = {Shrestha06expAdaboostRT.pdf:Shrestha06expAdaboostRT.pdf:PDF},
  owner     = {sotterson},
  publisher = {MIT Press},
  timestamp = {2014.07.24},
  url       = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6796101},
}

@TechReport{Calvet12abcBandwidth,
  author      = {Calvet, C and Czellar, Veronika},
  title       = {Accurate Methods for Approximate {Bayes}ian Computation Filtering},
  institution = {Technical Report, HEC Paris},
  year        = {2012},
  abstract    = {The Approximate Bayesian Computation (ABC) filter extends the particle
filtering methodology to general state-space models in which the density
of the observation conditional on the state is intractable. We provide an exact
upper bound for the mean squared error of the ABCfilter and show that
under appropriate bandwidth and kernel specifications, ABCconverges to the
target distribution as the number of particles goes to infinity. The optimal
convergence rate decreases with the dimension of the observation space but is
invariant to the complexity of the state space. We also show that the usual
adaptive bandwidth used in the ABCliterature leads to an inconsistent filter.
We develop a plug-in rule for the bandwidth and demonstrate the good accuracy
of the resulting filter on a multifractal asset pricing model with investor
learning. Despite the recent criticism of ABCmethods, we find that under
appropriate kernel and bandwidth specifications, ABC filtering is a very powerful
tool for model selection when the likelihood function is unavailable in
closed-form.

Keywords: Bandwidth; Kernel density estimation; Likelihood estimation;
Model selection; Particle filter; State-space model.},
  comment     = {A plugin bandwidth formula that helps out Approximate Bayesian Computation (ABC).

There's an R implementation of ABC, somewhere in this bibtex file.},
  file        = {Calvet12abcBandwidth.pdf:Calvet12abcBandwidth.pdf:PDF},
  groups      = {Ensemble, PointDerived, Test, doReadNonWPV_2},
  owner       = {sotterson},
  timestamp   = {2013.10.04},
}

@Article{Kang08clustRegressDirMix,
  author    = {Changku Kang and Subhashis Ghosal},
  title     = {Clusterwise Regression Using Dirichlet Mixtures},
  journal   = {Platinum Jubilee volume of Indian Statistical Institute},
  year      = {2008},
  abstract  = {The article describes a method of estimating nonparametric regression function through Bayesian clustering. The basic working assumption in the underlying method is that the population is a union of several hidden subpopulations in each of which a different linear regression is in force and the overall nonlinear regression function arises as a result of superposition of these linear regression functions. A Bayesian clustering technique based on Dirichlet mixture process is used to identify clusters which correspond to samples from these hidden subpopulations. The clusters are formed automatically within a Markov chain Monter-Carlo scheme arising from a Dirichlet mixture process prior for the density of the regressor variable. The number of components in the mixing distribution is thus treated as unknown allowing considerable flexibility in modeling. Within each cluster, we estimate model parameters by the standard least square method or some of its variations. Automatic model averaging takes care of the uncertainty in classifying a new observation to the obtained clusters. As opposed to most commonly used nonparametric regression estimates which break up the sample locally, our method splits the sample into a number of subgroups not depending on the dimension of the regressor variable. Thus our method avoids the curse of dimensionality problem. Through extensive simulations, we compare the performance of our proposed method with that of commonly used nonparametric regression techniques. We conclude that when the model assumption holds and the subpopulation are not highly overlapping, our method has smaller estimation error particularly if the dimension is relatively large.},
  comment   = {Like 2007 paper but includes num. mixtures determination. Not clear if this was ever actually published as comments here: http://209.85.173.132/search?q=cache:x0KmMhL1uksJ:www4.stat.ncsu.edu/~sghosal/papers.html+CLUSTERWISE+REGRESSION+USING+DIRICHLET+MIXTURES&hl=en&ct=clnk&cd=2&gl=us&client=firefox-a don't match the WSPC - Proceedings notation in the pdf},
  file      = {Kang08clustRegressDirMix.pdf:Kang08clustRegressDirMix.pdf:PDF;Kang08clustRegressDirMix.pdf:Kang08clustRegressDirMix.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.12.12},
}

@Article{Pinson04OonlinePredRiskWind,
  author    = {P. Pinson and G. Kariniotakis},
  title     = {On-line assessment of prediction risk for wind power production forecasts},
  journal   = {Wind Energy},
  year      = {2004},
  volume    = {7},
  number    = {2},
  pages     = {119--132},
  abstract  = {The article introduces a new methodology for assessing on-line the prediction risk of shortterm wind power forecasts.The first stage of this methodology consists in computing confidence intervals with a confidence level defined by the end-user.The resampling approach is used, which, in contrast to existing methods for wind forecasting, does not need to make a restrictive hypothesis on the distribution of the errors.To account for the non-linearity of the power curve and the cut-off effect, the errors are classified using appropriate fuzzy sets. The confidence intervals are then fine-tuned to reduce their width in the case of stable weather conditions. For this purpose an appropriate index, called the 'meteo-risk index' (MRI), is defined reflecting the spread of the available numerical weather predictions. A linear relation between that index and the resulting prediction error is shown. The second part of the methodology is to use the MRI itself as a preventive on-line tool to derive signals for the operator on the meteorological risk, i.e. the probabilities of the occurrence of high prediction errors depending on the weather stability. Evaluation results of this methodology over a 1 year period on the case study of Ireland are given, where the output of several wind farms is predicted using a dynamic fuzzy neural network-based model.The proposed methodology is generic and can be applied to all kinds of wind power prediction models.},
  comment   = {Wind power forecast confidence level predictor w/ bootstrapping and fuzzy conditional dependence.  This is a crude probabilistic wind power forecast with stuff potentially useful for copula conditional covariance matrixes and for quantile regression:  could project copula inputs onto speed/power basis functions, resulting in more meaningful covariance matrices (somehow?) and could use NPW lag MRI as a quantile regression input.

Bootstrapping for confidence intervals, conditional upon deterministically forecasted power and wind speed, latest NWP horizon

* First, sequentially bin into fuzzy forecasted speed/power categories
   1. Two  speed bins: above and below high wind speed cutoff
       - above cutoff, forecast error can only be negative
       - neg. error proportional to forecasted power e.g. if forecast is zero, and in cutoff, error is zero
    2. Three power bins
       - low power (zero true power); 
       - mid power (linear curve); 
       - high power (saturated or cutoff)
* Get empirical quantiles for each speed/power category by sampling sliding window of past errors
* I guess a CI is computed for each category, and then the final CI is a fuzzy set weighted result of each of them
* A different one for each NWP lookahead time
* Point forecast came from a fuzzy neural network

Adjustment of bootstrapped intervals w/ time-lagged NWP ensemble spread

* between forecast ages (NWP start times), compute RMS difference of forecasts over a time window (I think)
* "meteo risk index" (MRI): average over the forecast differences 
   - I didn't bother to understand this exactly
   - they call this 'stability' but it's not the NWP stability parameter
* predict true confidence level with a linear fit of bootstrapped confidence intervals vs. MRI
   - in eq. 6, is e0 the conf interval width or something?
   - explanation not totally clear
   - only allow narrowing of confidence level, not widening
* in tests, could narrow CI 65% of time, mean narrow was 11% -- pretty good
* Anyway, MRI would be a good probabilistic forecast feature
   

See also Pinson10condPredInt

},
  doi       = {10.1002/we.114)},
  file      = {Pinson04OonlinePredRiskWind.pdf:Pinson04OonlinePredRiskWind.pdf:PDF;Pinson04OonlinePredRiskWind.pdf:Pinson04OonlinePredRiskWind.pdf:PDF},
  groups    = {DOE-PNL09, PointDerived, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2009.03.03},
  url       = {http://www3.interscience.wiley.com/journal/108565883/abstract},
}

@InProceedings{Dobschinski14howGdFrcstVrblty,
  author    = {Jan Dobschinski},
  title     = {How good is my forecast? Comparability of wind power forecast errros},
  booktitle = {Proceedings of the 13\textsuperscript{th} International Workshop on Large-Scale Integration of Wind Power into Power Systems as well as on Transmission Networks for Offshore Wind Power Plants},
  year      = {2014},
  month     = nov,
  abstract  = {The assessment of wind power forecasting accuracy
is often not trivial. When discussing the forecast quality you
have also to discuss the specifics of the considered wind farm
or portfolio forecast. E.g. which forecast horizon is
considered? Which time period is investigated? What is the
size of the analyzed wind farm and/or the spatial distribution
of the portfolio? How is the forecast influenced by the local
orography? How much and which weather forecasts are used
and combined? Has also other data like online power
measurements been used? Within this study a way is proposed
that allows a standardized assessment of wind power
forecasting accuracy. This means that, for example, a fair
comparison of forecasts covering different time periods and
having different aggregation levels would be possible.
Keywords- wind power forecasting; numerical weather
prediction; forecastability},
  comment   = {Jan's paper showing that more varible wind power plant power means that the plant will be hard to forecast.},
  file      = {Dobschinski14howGdFrcstVrblty.pdf:Dobschinski14howGdFrcstVrblty.pdf:PDF},
  location  = {Berlin},
  url       = {http://www.researchgate.net/profile/Jan_Dobschinski/publications},
}

@InProceedings{Trigeorgis16adieuFeatsSpchEmotRecog,
  author    = {G. Trigeorgis and F. Ringeval and R. Brueckner and E. Marchi and M. A. Nicolaou and B. Schuller and S. Zafeiriou},
  title     = {Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network},
  booktitle = {Proc. Speech and Signal Processing (ICASSP) 2016 IEEE Int. Conf. Acoustics},
  year      = {2016},
  pages     = {5200--5204},
  month     = mar,
  abstract  = {The automatic recognition of spontaneous emotions from speech is a challenging task. On the one hand, acoustic features need to be robust enough to capture the emotional content for various styles of speaking, and while on the other, machine learning algorithms need to be insensitive to outliers while being able to model the context. Whereas the latter has been tackled by the use of Long Short-Term Memory (LSTM) networks, the former is still under very active investigations, even though more than a decade of research has provided a large set of acoustic descriptors. In this paper, we propose a solution to the problem of `context-aware' emotional relevant feature extraction, by combining Convolutional Neural Networks (CNNs) with LSTM networks, in order to automatically learn the best representation of the speech signal directly from the raw time representation. In this novel work on the so-called end-to-end speech emotion recognition, we show that the use of the proposed topology significantly outperforms the traditional approaches based on signal processing techniques for the prediction of spontaneous and natural emotions on the RECOLA database.},
  comment   = {Speech emotion features learned from raw time series using deep learning.},
  doi       = {10.1109/ICASSP.2016.7472669},
  file      = {Trigeorgis16adieuFeatsSpchEmotRecog.pdf:Trigeorgis16adieuFeatsSpchEmotRecog.pdf:PDF},
  keywords  = {emotion recognition, learning (artificial intelligence), recurrent neural nets, speech recognition, CNN, LSTM networks, RECOLA database, acoustic features, deep convolutional recurrent network, end-to-end speech emotion recognition, feature extraction, long short-term memory neworks, machine learning algorithms, raw time representation, Acoustics, Convolution, Emotion recognition, Feature extraction, Neural networks, Speech, Speech recognition, CNN, LSTM, deep learning, emotion recognition, end-to-end learning, raw waveform},
  owner     = {sotterson},
  timestamp = {2017.02.01},
}

@TechReport{Cosguner18dynPriceDynBass,
  author   = {Cosguner, Koray and Seetharaman, PB Seethu and Wu, Chunhua},
  title    = {Dynamic Pricing for New Products: Utility-Based Generalization of the Bass Diffusion Model},
  year     = {2018},
  month    = aug,
  abstract = {The Bass Model (BM) has an excellent track record in the realm of new product sales forecasting. However, 
its use for optimal dynamic pricing or advertising is relatively limited. This is because the Generalized Bass Model 
(GBM), which extends the BM to handle marketing variables, uses only percentage changes in marketing variables 
and not the actual values of the marketing variables. This restricts the normative use of the GBM, for example, to 
deriving the optimal price path for a new product, conditional on an assumed launch price, but not the launch price 
itself. In this paper, we propose a utility-based generalization of the BM which can yield normative prescriptions 
regarding both the introductory price, as well as the price path after launch, for the new product. We propose two 
versions of our proposed diffusion model, namely the Bass-Gumbel Diffusion Model (BGDM) and the Bass-Logit 
Diffusion Model (BLDM). Using empirical data from three product categories, we show that our proposed diffusion 
models handily outperform the GBM and BM in forecasting new product sales both in sample and out of sample, with 
the BLDM outperforming the BGDM. We derive optimal pricing policies for a new product that are implied by the 
BLDM under various ranges of model parameters. We explain how managers can use our proposed diffusion model 
to derive optimal marketing policies in a computationally convenient manner without having to explicitly solve a 
dynamic optimization problem. 

 

Keywords: Dynamic Pricing, Optimal Pricing, New Products, Diffusion Model, Bass Model (BM), Generalized Bass 
Model (GBM), New Product Sales Forecasting. },
  comment  = {A flexible Bass diffusion model that can be used w/o dynamic optimzation.  Says that Generalized Bass Model (GBM, Bass94bassWithoutDecVars) is severly deficient


Seems related to: 
Kapur12bassDiffuseSDE
Dong17resPVdeployFrcst


},
  file     = {:Cosguner18dynPriceDynBass.pdf:PDF},
  url      = {https://www.researchgate.net/profile/Koray_Cosguner/publication/328736172_Dynamic_Pricing_for_New_Products_Utility-Based_Generalization_of_the_Bass_Diffusion_Model/links/5be0711a92851c6b27aa085e/Dynamic-Pricing-for-New-Products-Utility-Based-Generalization-of-the-Bass-Diffusion-Model.pdf},
}

@Article{Bertotti16bassDiffusCorrInhomog,
  author   = {M.L. Bertotti and J. Brunner and G. Modanese},
  title    = {The Bass diffusion model on networks with correlations and inhomogeneous advertising},
  journal  = {Chaos, Solitons \& Fractals},
  year     = {2016},
  volume   = {90},
  pages    = {55 - 63},
  issn     = {0960-0779},
  note     = {Challenges in Data Science},
  abstract = {The Bass model, which is an effective forecasting tool for innovation diffusion based on large collections of empirical data, assumes an homogeneous diffusion process. We introduce a network structure into this model and we investigate numerically the dynamics in the case of networks with link density P(k)=c/kγ, where k=1,…,N. The resulting curve of the total adoptions in time is qualitatively similar to the homogeneous Bass curve corresponding to a case with the same average number of connections. The peak of the adoptions, however, tends to occur earlier, particularly when γ and N are large (i.e., when there are few hubs with a large maximum number of connections). Most interestingly, the adoption curve of the hubs anticipates the total adoption curve in a predictable way, with peak times which can be, for instance when N=100, between 10\% and 60\% of the total adoptions peak. This may allow to monitor the hubs for forecasting purposes. We also consider the case of networks with assortative and disassortative correlations and a case of inhomogeneous advertising where the publicity terms are “targeted” on the hubs while maintaining their total cost constant.},
  doi      = {https://doi.org/10.1016/j.chaos.2016.02.039},
  keywords = {Innovation diffusion, Bass equation, scale-free networks, correlated networks},
  url      = {http://www.sciencedirect.com/science/article/pii/S0960077916300686},
}

@InBook{Dixon06bootstrpRsmpl,
  title     = {Bootstrap Resampling},
  publisher = {John Wiley \& Sons, Ltd},
  year      = {2006},
  author    = {Dixon, Philip M.},
  isbn      = {9780470057339},
  abstract  = {The bootstrap is a resampling method for statistical inference. It is commonly used to estimate confidence intervals, but it can also be used to estimate bias and variance of an estimator or calibrate hypothesis tests. This article will illustrate bootstrap concepts using a simple example, describe different types of bootstraps and some of their theoretical and practical properties, discuss computation and other details, and indicate extensions that are especially appropriate for environmetric data. The methods will be illustrated using data on heavy metal concentrations in groundwater and magnesium concentration in blood.},
  booktitle = {Encyclopedia of Environmetrics},
  comment   = {Explains several bootstrap confidence interval techniques. Mentions accelerated bootstrap used in Matlab stats toolboxes bootci()},
  doi       = {10.1002/9780470057339.vab028},
  file      = {Dixon06bootstrpRsmpl.pdf:Dixon06bootstrpRsmpl.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2011.11.27},
}

@Article{Todter12genIgnCRPS,
  author        = {T{\"o}dter, Julian and Ahrens, Bodo},
  title         = {Generalization of the Ignorance Score: Continuous Ranked Version and Its Decomposition.},
  journal       = {Monthly Weather Review},
  year          = {2012},
  volume        = {140},
  number        = {6},
  __markedentry = {[Scott:1]},
  abstract      = {The Brier score (BS) and its generalizations to the multicategory ranked probability score (RPS) and to the continuous ranked probability score (CRPS) are the prominent verification measures for probabilistic forecasts. Particularly, their decompositions into measures quantifying the reliability, resolution, and uncertainty of the forecasts are attractive. Information theory sets up the natural framework for forecast verification. Recently, it has been shown that the BS is a second-order approximation of the information-based ignorance score (IGN), which also contains easily interpretable components and can also be generalized to a ranked version (RIGN). Here, the IGN, its generalizations, and decompositions are systematically discussed in analogy to the variants of the BS. Additionally, a continuous ranked IGN (CRIGN) is introduced in analogy to the CRPS. The applicability and usefulness of the conceptually appealing CRIGN are illustrated, together with an algorithm to evaluate its components reliability, resolution, and uncertainty for ensemble-generated forecasts.},
  comment       = {Another decomposition of CRPS. Matlab available here:
https://github.com/itszootime/emulatorization-api/blob/master/matlab/validation/crps_ign.m},
  doi           = {10.1175/MWR-D-11-00266.1},
  file          = {Todter12genIgnCRPS.pdf:Todter12genIgnCRPS.pdf:PDF},
  groups        = {Test, doReadWPV_1},
  owner         = {sotterson},
  timestamp     = {2014-02-28},
}

@Article{Jewson04problemBrierScore,
  author      = {Stephen Jewson},
  title       = {The problem with the Brier score},
  abstract    = {The Brier score is frequently used by meteorologists to measure the skill of binary probabilistic forecasts. We show, however, that in simple idealised cases it gives counterintuitive results. We advocate the use of an alternative measure that has a more compelling intuitive justification.},
  comment     = {Proposes banning the Brier Score and replacing it with a log likelihood score.  I'm not sure I buy the intuitive argument given here, although Benedetti10scoreFrcstVerifRare is more convincing.

A binary probability forecast predicting zero probability is very wrong when there are ever any positive true outcomes.  But the Brier score is insensitive to the location (within the predicted density) of the prediction; if the true probability is 0.1 and there are two forecasts of f1=0 and f2=0.25, the Brier Score says that f1 is better because it's close to 0.1.  The author says that, intuitively, f2 is better.  

Anyway, it's proposed to replace the Brier Score with the log likelihood score, which gives an infinite penalty whenever a 0 or 1 is forecasted -- so it won't let you ever rule out a positive or negative event.

The copy I downloaded seems to have been updated in 2018, but there's noting on arXiv saying that.

Also see Benedetti10scoreFrcstVerifRare, which cites this paper

Jewson03likSkillFrcst wants to  replace CRPS with a likelyhood score too.},
  date        = {2004-01-10},
  eprint      = {http://arxiv.org/abs/physics/0401046v1},
  eprintclass = {physics.ao-ph},
  eprinttype  = {arXiv},
  file        = {:Jewson04problemBrierScore.pdf:PDF},
  keywords    = {physics.ao-ph},
  url         = {https://arxiv.org/abs/physics/0401046},
}

@Article{Oh11fastSmthQuantreg,
  author    = {Oh, Hee-Seok and Lee, Thomas C. M. and Nychka, Douglas W.},
  title     = {Fast Nonparametric Quantile Regression With Arbitrary Smoothing Methods},
  journal   = {Journal of Computational and Graphical Statistics},
  year      = {2011},
  volume    = {20},
  number    = {2},
  pages     = {510--526},
  abstract  = {The calculation of nonparametric quantile regression curve estimates is often computationally intensive, as typically an expensive nonlinear optimization problem is involved. This article proposes a fast and easy-to-implement method for computing such estimates. The main idea is to approximate the costly nonlinear optimization by a sequence of well-studied penalized least squares-type nonparametric mean regression estimation problems. The new method can be paired with different nonparametric smoothing methods and can also be applied to higher dimensional settings. Therefore, it provides a unified framework for computing different types of nonparametric quantile regression estimates, and it also greatly broadens the scope of the applicability of quantile regression methodology. This wide applicability and the practical performance of the proposed method are illustrated with smoothing spline and wavelet curve estimators, for both uni- and bivariate settings. Results from numerical experiments suggest that estimates obtained from the proposed method are superior to many competitors. This article has supplementary material online.},
  comment   = {If it's really fast, could maybe use this for an agglomerative quantile regression algorithm. Has R code

cited in Reich12spatTempQR

Supplementary Materials Section says:
* appendix that contains technical details and proofs (appendix.pdf), and
* R codes that implement the proposed methodology (Rcodes.zip).},
  doi       = {10.1198/jcgs.2010.10063},
  eprint    = {http://amstat.tandfonline.com/doi/pdf/10.1198/jcgs.2010.10063},
  file      = {Oh11fastSmthQuantreg.pdf:Oh11fastSmthQuantreg.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.03.30},
}

@Article{Hart12co2reduceRenew,
  author    = {Hart, Elaine K. and Jacobson, Mark Z.},
  title     = {The carbon abatement potential of high penetration intermittent renewables},
  journal   = {Energy Environ. Sci.},
  year      = {2012},
  volume    = {5},
  pages     = {6592--6601},
  abstract  = {The carbon abatement potentials of wind turbines{,} photovoltaics{,} and concentrating solar power plants were investigated using dispatch simulations over California with 2005-06 meteorological and load data. A parameterization of the simulation results is presented that provides approximations of both low-penetration carbon abatement rates and maximum carbon abatement potentials based on the temporal characteristics of the resource and the load. The results suggest that shallow carbon emissions reductions (up to 20\% of the base case) can be achieved most efficiently with geothermal power and demand reductions via energy efficiency or conservation. Deep emissions reductions (up to 89\% for this closed system){,} however{,} may require the build-out of very large fleets of intermittent renewables and improved power system flexibility{,} communications{,} and controls. At very high penetrations{,} combining wind and solar power improved renewable portfolio performance over individual build-out scenarios by reducing curtailment{,} suggesting that further reductions may be met by importing uncorrelated out-of-state renewable power. The results also suggest that 90-100\% carbon emission reductions will rely on the development of demand response and energy storage facilities with power capacities of at least 65\% of peak demand and energy capacities large enough to accommodate seasonal energy storage.},
  comment   = {Most cost-efficient way to cut C02 emissions is by increasing percentages (in California).

< 20\%: geothermal + efficiency
< 89\%: large fleet of mixed solar and wind, some of it from out of state (uncorrelated)
90-100\%: storage + demand response with > 65\% peak demand and enough capacity for seasonal storage.

I've also evernoted this paper.},
  doi       = {10.1039/C2EE03490E},
  file      = {Hart12co2reduceRenew.pdf:Hart12co2reduceRenew.pdf:PDF},
  issue     = {5},
  owner     = {sotterson},
  publisher = {The Royal Society of Chemistry},
  timestamp = {2012.09.14},
}

@Book{May11revVarSelNN,
  title     = {Review of input variable selection methods for artificial neural networks},
  publisher = {INTECH Open Access Publisher},
  year      = {2011},
  author    = {May, Robert and Dandy, Graeme and Maier, Holger},
  abstract  = {The choice of input variables is a fundamental, and yet crucial consideration in identifying the
optimal functional form of statistical models. The task of selecting input variables is common
to the development of all statistical models, and is largely dependent on the discovery of
relationships within the available data to identify suitable predictors of the model output.
In the case of parametric, or semi-parametric empirical models, the difficulty of the input
variable selection task is somewhat alleviated by the a priori assumption of the functional
form of the model, which is based on some physical interpretation of the underlying system
or process being modelled. However, in the case of artificial neural networks (ANNs), and
other similarly data-driven statistical modelling approaches, there is no such assumption
made regarding the structure of the model. Instead, the input variables are selected from
the available data, and the model is developed subsequently. The difficulty of selecting input
variables arises due to (i) the number of available variables, which may be very large; (ii)
correlations between potential input variables, which creates redundancy; and (iii) variables
that have little or no predictive power.

Variable subset selection has been a longstanding issue in fields of applied statistics dealing
with inference and linear regression (Miller, 1984), and the advent of ANN models has only
served to create new challenges in this field. The non-linearity, inherent complexity and
non-parametric nature of ANN regression make it difficult to apply many existing analytical
variable selection methods. The difficulty of selecting input variables is further exacerbated
during ANN development, since the task of selecting inputs is often delegated to the ANN
during the learning phase of development. A popular notion is that an ANN is adequately
capable of identifying redundant and noise variables during training, and that the trained
network will use only the salient input variables. ANN architectures can be built with
arbitrary flexibility and can be successfully trained using any combination of input variables
(assuming they are good predictors). Consequently, allowances are often made for a large
number of input variables, with the belief that the ability to incorporate such flexibility and
redundancy creates a more robust model. Such pragmatism is perhaps symptomatic of the
popularisation of ANN models through machine learning, rather than statistical learning
theory. ANN models are too often developed without due consideration given to the
effect that the choice of input variables has on model complexity, learning difficulty, and
performance of the subsequently trained ANN.

Recently, ANN modellers have become increasingly aware of the need to undertake input
variable selection (IVS), and a myriad of methods employed to undertake the IVS task
are described within reported ANN applications?some more suited to ANN development
than others. This review therefore serves to provide some guidance to ANN modellers, by
highlighting some of the key issues surrounding variable selection within the context of ANN
development, and survey some the alternative strategies that can be adopted within a general
framework, and provide some examples with discussion on the benefits and disadvantges in
each case.},
  comment   = {Big catalog of ways to select pattern reconition inputs -- wrappers, embedded, filters -- but with a focus on neural nets.  Also talks about dimension reduction.  Interesting part might be partial mutual information feat, genetic algorithms, single value regression, and MI/ICA theoretical justifications.

Interesting parts
* Redundant inputs increase number of local optima
* num. training samps increase exponentially with input dim (Scott 92), MLP esp. vulnerable b/c num. weights grow fast
* some NN's can be trained to ignore irrelevant inputs but not RBF and GRNN (Nadaraya-Watson)

Desirable features for filter algorithms (and wrappers, I suppose)
* max rel.
* min redundancy
* min red - max Rel (mRMR, like in Peng* in my PhD)

Stopping criterion (when have I picked enough features?)
* Mallows' Cp: not clear
* AIC/BIC
* Vapnik-Chernovenkis (VC) dimension
* Bootstrap significance test

Feature testing search
* can't do exhaustive except for tiny problems
* Forward search: doesn't consider all combos, likely to hit local optimum
* Step-wise regression: looks for significant coeffs (I think), can go backwards and reconsider past selections
* Backward elimination: must build the biggest model first
* Heuristic Genetic/evolutionary search: surprisingly positive about it, says good for avoiding local minima
* Vector Quantization (VQ)

Wrappers
* Single variable regression (SVR)
 - similar to what I think of as a wrapper but works only trains single-variable models
 - uses GRNN, which doesn't have model selection problem (but it does if you consider bandwidth)
 - remove variable if MSE worse than a 95% boostrap confidence interval.
 -- BETTER THAN MY CROSS-VALIDATION APPROACH?
 -- bootstrap is input shuffling, not training of CV sets, seems like it would be more CPU friendly
* Genetic algorithm ANN (GA-NN)
 - is fast
* doesn't mention my CV set idea

Embedded
* RFE (recursive feature elimination:) remove features duing training based on weights
* Evolutionary ANN:
 - NN objective function includes complexity penalty
 - can find more global optimima
 - like GA-NN except includes optimization of weight params
 - can have binary feature in/out variable

Filters
* linear corr
* Partial corr.  Almost like step-wise except
 - feature decision not based on regression coeff
 - forward only: no backwards
 * Box-Jenkins:
 - a way of selecting ARMA lags, applied to NN's.
 - NN-ARX model better than ARMA
 - but may not be optimal
* Mutual info
 -  min error at max MI (Torkkola 2003 reference)
 - basically the Ding and Peng mRMR idea in my PhD, I think
* Partial Mutual Information ( May08varSelpartMutInfo may explain the PMI better)
 - at each step, pick feature that has most MI that is not in already-selected features
 - stepwise sum of PMI is an optimal estimate of full MI according to MI chain rule
 (eq. 18)
 - but must estimate Y|(selected features) so they train a kernel regressor for some reason
 -- why not just do an ANN?
 -- why not do GRNN if is faster and has less model selection, as mentioned elsewhere?
 - also estimate C|(selected features), which seems pointless (see my notes in May08varSelpartMutInfo)
 - use Bootstrap termination criteria but this is not the best
 -- computationally expensive (but they use KDE MI est)
 -- May 2009b has cheaper, more accurate termination, and also Fernando 2009
 - optimality is approximately assured (problem is Y|(selected features), I think)
 - perhaps better explained in
 - does not use Kraskov MI but kernel density estimator, so is probably computationally expensive, not accurate
 - can this be done just w/ Kraskov multidim estimator?  (no regression training in middle)
 -- Frenzel07partMutInfo seems to have done this (and my notes on it say it's more computationall efficient)

 -- Can also be done w/ Kraskov-like partial/conditional MI: Vlachos10nonUnifStSpcMI (same as Frenzel*?)
 - explanation for MI
 - also see Vergara14revFeatSelMutInfo
 - also see Darudi13partMutInfoFeatSel


Dim Reduction
* PCA
* ICA: better for nonlinear problems since making independence but it's not explained how to pick components


This article recommended here:
http://stats.stackexchange.com/questions/35089/feature-selection-using-mutual-information-in-matlab},
  file      = {May11revVarSelNN.pdf:May11revVarSelNN.pdf:PDF},
  url       = {http://cdn.intechopen.com/pdfs/14882/InTech-Review_of_input_variable_selection_methods_for_artificial_neural_networks.pdf},
}

@Article{Prendergast16quantLorenzGINI,
  author    = {Prendergast, Luke A. and Staudte, Robert G.},
  title     = {Quantile versions of the Lorenz curve},
  journal   = {Electron. J. Statist.},
  year      = {2016},
  volume    = {10},
  number    = {2},
  pages     = {1896--1926},
  abstract  = {The classical Lorenz curve is often used to depict inequality in
a population of incomes, and the associated Gini coefficient is relied upon to
make comparisons between different countries and other groups. The sam-
ple estimates of these moment-based concepts are sensitive to outliers and
so we investigate the extent to which quantile-based versions can capture
income inequality and lead to robust procedures. Distribution-free interval
estimates of the associated coefficients of inequality are obtained, as well
as sample sizes required to estimate them to a given accuracy. Convexity,
transference and robustness of the measures are examined and illustrated.
Keywords and phra},
  comment   = {Calculating GINI coefficients from anonymized summary stats, like quantiles, interval means, etc.  

See Simpler: Tille12histoLorenzGini, Lyon16advtgsGrpMnLorenzGINI},
  doi       = {10.1214/16-EJS1154},
  file      = {:Prendergast16quantLorenzGINI.pdf:PDF},
  fjournal  = {Electronic Journal of Statistics},
  publisher = {The Institute of Mathematical Statistics and the Bernoulli Society},
  url       = {https://doi.org/10.1214/16-EJS1154},
}

@Article{Kang15elecCustClustExpertDR,
  author   = {Kang, Jimyung and Lee, Jee-Hyong},
  title    = {Electricity Customer Clustering Following Experts’ Principle for Demand Response Applications},
  journal  = {Energies},
  year     = {2015},
  volume   = {8},
  number   = {10},
  pages    = {12242--12265},
  issn     = {1996-1073},
  abstract = {The clustering of electricity customers might have an effective meaning if, and only if, it is verified by domain experts. Most of the previous studies on customer clustering, however, do not consider real applications, but only the structure of clusters. Therefore, there is no guarantee that the clustering results are applicable to real domains.  In other words, the results might not coincide with those of domain experts. In this paper, we focus on formulating clusters that are applicable to real applications based on domain expert knowledge. More specifically, we try to define a distance between customers that generates clusters that are applicable to demand response applications. First, the k-sliding distance, which is a new distance between two electricity customers, is proposed for customer clustering. The effect of k-sliding distance is verified by expert knowledge. Second, a genetic programming framework is proposed to automatically determine a more improved distance measure. The distance measure generated by our framework can be considered as a reflection of the clustering principles of domain experts. The results of the genetic programming demonstrate the possibility of deriving clustering principles.},
  comment  = {What I like about this one, is that it explains what experts are looking for in load profile clustering.},
  doi      = {10.3390/en81012242},
  file     = {:Kang15elecCustClustExpertDR.pdf:PDF},
  url      = {http://www.mdpi.com/1996-1073/8/10/12242},
}

@Article{Granata16intrinsDimGraphEst,
  author    = {Granata, Daniele and Carnevale, Vincenzo},
  title     = {Accurate Estimation of the Intrinsic Dimension Using Graph Distances: Unraveling the Geometric Complexity of Datasets},
  journal   = {Scientific Reports},
  year      = {2016},
  volume    = {6},
  abstract  = {The collective behavior of a large number of degrees of freedom can be often described by a handful of variables. This observation justifies the use of dimensionality reduction approaches to model complex systems and motivates the search for a small set of relevant ?collective? variables. Here, we analyze this issue by focusing on the optimal number of variable needed to capture the salient features of a generic dataset and develop a novel estimator for the intrinsic dimension (ID). By approximating geodesics with minimum distance paths on a graph, we analyze the distribution of pairwise distances around the maximum and exploit its dependency on the dimensionality to obtain an ID estimate. We show that the estimator does not depend on the shape of the intrinsic manifold and is highly accurate, even for exceedingly small sample sizes. We apply the method to several relevant datasets from image recognition databases and protein multiple sequence alignments and discuss possible interpretations for the estimated dimension in light of the correlations among input variables and of the information content of the dataset.},
  comment   = {Intrinsic dimension (number of variables on the manifold are needed to represent a set of data, I think), can be estimated with graph distances.  This can guide data compression, say, with ISOMAP.},
  doi       = {10.1038/srep31377},
  file      = {Granata16intrinsDimGraphEst.pdf:Granata16intrinsDimGraphEst.pdf:PDF},
  owner     = {sotterson},
  publisher = {Nature Publishing Group},
  timestamp = {2017.02.03},
  url       = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4980871/},
}

@InProceedings{Lemke09dynFrcstComboDiv,
  author    = {Lemke, C. and Riedel, S. and Gabrys, B.},
  title     = {Dynamic combination of forecasts generated by diversification procedures applied to forecasting of airline cancellations},
  booktitle = {Computational Intelligence for Financial Engineering (CIFEr)},
  year      = {2009},
  pages     = {85--91},
  month     = mar,
  abstract  = {The combination of forecasts is a well established procedure for improving forecast performance and decreasing the risk of selecting an inferior model out of an existing pool of models. Work in this area mainly focuses on combining several functionally different models, but some publications also deal with combining forecasts with the same functional approach. In the latter case, individual forecasts are generated by diversifying one or more model parameters or, if dealing with hierarchical data, by using forecasts from different levels. This work looks at multi-dimensional data from airline industry, with the aim of improving the forecast of cancellation rates for bookings. Three different methods are employed for the generation of individual forecasts. Forecast combinations are usually implemented in a more or less static structure, either including all available forecasts or trimming a fixed percentage of the worst performing models. For a big number of individual forecasts, this procedure can become inefficient. In this paper, a dynamic approach of pooling and trimming is applied to the generated forecasts for airline cancellation data.},
  comment   = {Switches forecast combinations in realtime, sort of like switching regression theory in: Riedel09poolMultiFrcstCombo

So, it's a kind of dynamic feature selection for ensemble forecasts.},
  doi       = {10.1109/CIFER.2009.4937507},
  file      = {Lemke09dynFrcstComboDiv.pdf:Lemke09dynFrcstComboDiv.pdf:PDF},
  groups    = {Ensemble, doReadNonWPV_1},
  keywords  = {Forecast combination;airline booking cancellation rate;airline cancellation forecasting;airline industry;airline revenue management;multidimensional data;time series forecasting;trimming approach;variance-based pooling approach;forecasting theory;time series;travel industry;},
  owner     = {scot},
  timestamp = {2010.07.01},
}

@Article{Martins03cmplxStepDerivCSD,
  author    = {Martins, Joaquim RRA and Sturdza, Peter and Alonso, Juan J},
  title     = {The complex-step derivative approximation},
  journal   = {ACM Transactions on Mathematical Software (TOMS)},
  year      = {2003},
  volume    = {29},
  number    = {3},
  pages     = {245--262},
  abstract  = {The complex-step derivative approximation and its application to numerical algorithms are presented.
Improvements to the basic method are suggested that further increase its accuracy and
robustness and unveil the connection to algorithmic differentiation theory. A general procedure
for the implementation of the complex-step method is described in detail and a script is developed
that automates its implementation. Automatic implementations of the complex-step method for
Fortran and C/C++ are presented and compared to existing algorithmic differentiation tools. The
complex-step method is tested in two large multidisciplinary solvers and the resulting sensitivities
are compared to results given by finite differences. The resulting sensitivities are shown to be as
accurate as the analyses. Accuracy, robustness, ease of implementation and maintainability make
these complex-step derivative approximation tools very attractive options for sensitivity analysis.
Categories and Subject Descriptors: G.1.4 [Numerical Analysis]: Quadrature and Numerical Differentiation;
I.1.2 [Symbolic and Algebraic Manipulation]: Algorithms?analysis of algorithms
General Terms: Algorithms, Performance
Additional KeyWords and Phrases: Automatic differentiation, forward mode, complex-step derivative
approximation, overloading, gradients, sensitivities},
  comment   = {Complex step differentiation accuracy analysis, and relationship to algorithmic differentiation theory. Conclusion: it's extrememely accurate. Has Matlab.

Original paper on modern CSD algorthm: Squire98cmplxDerivsCSD

Matlab library (almost not needed since CSD is so simple but there are some corner cases): Shampine07AccNumerDiffMatlab},
  file      = {Martins03cmplxStepDerivCSD.pdf:Martins03cmplxStepDerivCSD.pdf:PDF},
  owner     = {sotterson},
  publisher = {ACM},
  timestamp = {2014.07.02},
  url       = {http://dl.acm.org/citation.cfm?id=838251},
}

@Article{Bessa13renVarPowSys,
  author    = {Bessa, Ricardo and Moreira, Carlos and Silva, Bernardo and Matos, Manuel},
  title     = {Handling renewable energy variability and uncertainty in power systems operation},
  journal   = {WENE},
  year      = {2013},
  issn      = {2041-8396},
  abstract  = {The concerns about global warming (greenhouse-gas emissions), scarcity of fossil fuels reserves, and primary energy independence of regions or countries have led to a dramatic increase of renewable energy sources (RES) penetration in electric power systems, mainly wind and solar power. This created new challenges associated with the variability and uncertainty of these sources. Handling these two characteristics is a key issue that includes technological, regulatory, and computational aspects. Advanced tools for handling RES maximize the resultant benefits and keep the reliability indices at the required level. Recent advances in forecasting and management algorithms provided means to manage RES. Forecasts of renewable generation for the next hours/days play a crucial role in the management tools and protocols of the system operator. These forecasts are used as input for setting reserve requirements and performing the unit commitment (UC) and economic dispatch (ED) processes. Probabilistic forecasts are being included in the management tools, enabling a move from deterministic to stochastic methods, which conduct to robust solutions. On the technological side, advances to increase mid-merit and base-load generation flexibility should be a priority. The use of storage devices to mitigate uncertainty and variability is particularly valuable for isolated power system, whereas in interconnected systems, economic criteria might be a barrier to invest in new storage facilities. The possibility of sending active and reactive control set points to RES power plants offers more flexibility. Furthermore, the emergence of the smart grid concept and the increasing share of controllable loads contribute with flexibility to increase the RES penetration levels.},
  booktitle = {Wiley Interdisciplinary Reviews: Energy and Environment},
  comment   = {Bessa writes good papers but I can't find the PDF!

Try later?},
  doi       = {10.1002/wene.76},
  groups    = {Use, CitaviImport1, doReadNonWPV_1},
  keywords  = {variability, uncertainty},
  owner     = {sotterson},
  timestamp = {2013.09.26},
}

@Article{Broecker12rawEnsQntScrCRPS,
  author    = {Br{\"o}cker, Jochen},
  title     = {Evaluating raw ensembles with the continuous ranked probability score},
  journal   = {Quarterly Journal of the Royal Meteorological Society},
  year      = {2012},
  volume    = {138},
  number    = {667},
  pages     = {1611--1617},
  abstract  = {The continuous ranked probability score (CRPS) is a frequently used scoring rule. In contrast with many other scoring rules, the CRPS evaluates cumulative distribution functions. An ensemble of forecasts can easily be converted into a piecewise constant cumulative distribution function with steps at the ensemble members. This renders the CRPS a convenient scoring rule for the evaluation of ?raw? ensembles, obviating the need for sophisticated ensemble model output statistics or dressing methods prior to evaluation. In this article, a relation between the CRPS score and the quantile score is established. The evaluation of ?raw? ensembles using the CRPS is discussed in this light. It is shown that latent in this evaluation is an interpretation of the ensemble as quantiles but with non-uniform levels. This needs to be taken into account if the ensemble is evaluated further, for example with rank histograms.},
  comment   = {A way to evaluate wind speed ensembles against measured power without a power curve? At least avoids calibration or dressing.

* CRPS is a weighted sum of quantile scores (kinda like in Gneiting11compFrcstWgtQntSc)
  - but he doesn't really say what the weights are

* a CRPS-optimal rank histogram is not actually flat!

Generates fake data for test

an EMOS use: Schuhen12

I actually didn't quite understand this paper after a quick skim. Should really read it.},
  doi       = {10.1002/qj.1891/full},
  file      = {Broecker12rawEnsQntScrCRPS.pdf:Broecker12rawEnsQntScrCRPS.pdf:PDF},
  groups    = {Read, Ensemble, Test, doReadNonWPV_1},
  ncite     = {1},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.05.26},
}

@Article{Lorenz11RegionalPVpowFrcst,
  author    = {Lorenz, Elke and Scheidsteger, Thomas and Hurka, Johannes and Heinemann, Detlev and Kurz, Christian},
  title     = {Regional {PV} power prediction for improved grid integration},
  journal   = {Progress in Photovoltaics: Research and Applications},
  year      = {2011},
  volume    = {19},
  number    = {7},
  pages     = {757--771},
  abstract  = {The contribution of power production from PV systems to the electricity supply is constantly increasing. An efficient use of
the fluctuating solar power production will highly benefit from forecast information on the expected power production, as a
basis for management of the electricity grids and trading on the energy market. We present and evaluate the regional PV
power prediction system of University of Oldenburg and Meteocontrol GmbH providing forecasts of up to 2 days ahead
with hourly resolution. The proposed approach is based on forecasts of the global model of the European Centre for
Medium-Range Forecasts (ECMWF). It includes a post-processing procedure to derive optimised, site-specific irradiance
forecasts and explicit physical modelling steps to convert the predicted irradiances to PV power. Finally, regional power
forecasts are derived by up-scaling from a representative set of PV systems. The investigation of proper up-scaling is a
special focus of this paper. We introduce a modified up-scaling approach, modelling the spatial distribution of the nominal
power with a resolution of 1818. The operational PV power prediction system is evaluated in comparison to the modified
up-scaling approach for the control areas of the two German transmission system operators ?transpower? and ?50 Hertz? for
the period 2.7.2009?30.4.2010. rmse values of the operational forecasts are in the range of 4?5% with respect to the
nominal power for intra-day and day-ahead forecast horizons. Further improvement is achieved with the modified upscaling
approach.
KEYWORDS
PV power prediction; grid integration; irradiance prediction; PV simulation},
  comment   = {How to do PV upscaling for horizons up to two days ahead.},
  doi       = {10.1002/pip.1033/abstract},
  file      = {Lorenz11RegionalPVpowFrcst.pdf:Lorenz11RegionalPVpowFrcst.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2015.02.19},
}

@InCollection{LeCun98effBkprp,
  author    = {LeCun, Yann and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  title     = {Efficient backprop},
  booktitle = {Neural networks: Tricks of the trade},
  publisher = {Springer},
  year      = {1998},
  pages     = {9--50},
  abstract  = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most ?classical? second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
  comment   = {Classic tips for better NN training, as well as how to pick layer transfer functions.


Recommended here by Matteo De Felice, here:
http://metaoptimize.com/qa/questions/9055/role-of-transfer-functions-in-neural-network
(commenters say it's useful and easy to read)},
  file      = {LeCun98effBkprp.pdf:LeCun98effBkprp.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2013.06.10},
}

@Article{Yan10copulaRpkg,
  author    = {Yan, J. and Kojadinovic, I.},
  title     = {Modeling Multivariate Distributions with Continuous Margins Using the copula R Package},
  journal   = {Journal of Statistical Software},
  year      = {2010},
  volume    = {34},
  number    = {9},
  month     = may,
  abstract  = {The copula-based modeling of multivariate distributions with continuous margins is presented as a succession of rank-based tests: a multivariate test of randomness followed by a test of mutual independence and a series of goodness-of-fit tests. All the tests under consideration are based on the empirical copula, which is a nonparametric rank-based estimator of the true unknown copula. The principles of the tests are recalled and their implementation in the copula R package is briefy described. Their use in the construction of a copula model from data is thoroughly illustrated on real insurance and financial data. Keywords: goodness of fit, multivariate independence, pseudo-observations, rank-based tests, serial independence.},
  comment   = {How to use the R package for copulas. Also, why time series should be pre-whitened

* say GARCH is a good whitener},
  file      = {Yan10copulaRpkg.pdf:Yan10copulaRpkg.pdf:PDF},
  owner     = {scot},
  publisher = {American Statistical Association},
  timestamp = {2010.12.20},
}

@InProceedings{Vogt18multiDistLrnAnomDet,
  author    = {Stephan Vogt and Scott Otterson and Volker Berkhout},
  title     = {Multi-task distribution learning approach to anomaly detection of operational states of wind turbine},
  booktitle = {WindEurope},
  year      = {2018},
  address   = {Hamburg, DE},
  abstract  = {The detection of abnormal operation modes is of fundamental importance for both 
operational management and predictive maintenance of wind turbines. Anomaly detection 
approaches in this context should consider the additional information content that probabilistic 
models can provide. Instead of binary anomaly classification, the probabilistic information is 
necessary for proper decision making and risk assessment. Common models, such as quantile 
and distribution regression can provide probabilistic information. While they are appropriate in 
predicting the cumulative distribution function, they struggle to accurately describe the 
probability of an event to occur. In this article we present a new, multi-task learning based 
approach for a continuous distribution regression with deep neural networks. Using real-world 
data from an offshore wind turbine, we show that with this model we can better reflect the 
probability of observed events than with conventional methods. While the predicted 
cumulative distribution function has a similar quality and no significant differences are visible 
in the continuous ranked probability score, the probability density function will be substantially 
smoother. This is also reflected in a significantly lower ignorance score. },
  comment   = {Single output NN distribution regression with random thresholds during training.  This is what I called dr1nn in the IRPWIND final report for WP 82.3 (2018).  Although, I implemented it with the Matlab NN toolbox and Stephan later reimplimented it for this paper with what I would guess were his own tools.

Multitask intro: Caruana97multiTaskLearninng},
  file      = {:Vogt18multiDistLrnAnomDet.pdf:PDF},
  groups    = {Scott:1},
}

@Article{Hao11visMultiDimSaX,
  author    = {Hao, M. and Marwah, M. and Janetzko, H. and Sharma, R. and Keim, D. A. and Dayal, U. and Patnaik, D. and Ramakrishnan, N.},
  title     = {Visualizing frequent patterns in large multivariate time series},
  year      = {2011},
  abstract  = {The detection of previously unknown, frequently occurring patterns in time series, often called motifs, has been
recognized as an important task. However, it is difficult to discover and visualize these motifs as their numbers
increase, especially in large multivariate time series. To find frequent motifs, we use several temporal data
mining and event encoding techniques to cluster and convert a multivariate time series to a sequence of events.
Then we quantify the efficiency of the discovered motifs by linking them with a performance metric. To
visualize frequent patterns in a large time series with potentially hundreds of nested motifs on a single display,
we introduce three novel visual analytics methods: (1) motif layout, using colored rectangles for visualizing the
occurrences and hierarchical relationships of motifs in a multivariate time series, (2) motif distortion, for
enlarging or shrinking motifs as appropriate for easy analysis and (3) motif merging, to combine a number of
identical adjacent motif instances without cluttering the display. Analysts can interactively optimize the degree
of distortion and merging to get the best possible view. A specific motif (e.g., the most efficient or least efficient
motif) can be quickly detected from a large time series for further investigation. We have applied these methods
to two real-world data sets: data center cooling and oil well production. The results provide important new
insights into the recurring patterns.
Keywords: recurring patterns, time series, multivariate data, motifs, distortion, merging},
  comment   = {Multidimensional SaX and visualization.

Use for analog ensemble forecast or conditional point forecast.},
  doi       = {10.1117/12.872169},
  file      = {Hao11visMultiDimSaX.pdf:Hao11visMultiDimSaX.pdf:PDF},
  groups    = {Ensemble, PointDerived, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2013.03.15},
  url       = { + http://dx.doi.org/10.1117/12.872169},
}

@InProceedings{Milligan10opRsrvWindPowCmpr,
  author    = {Milligan, Michael and Donohoo, Pearl and Lew, Debra and Ela, Erik and Kirby, Brendan and Holttinen, Hannele and Lannoye, Eamonn and Flynn, Damian and O?Malley, Mark and Miller, Nicholas and others},
  title     = {Operating reserves and wind power integration: an international comparison},
  booktitle = {Proc. 9\textsuperscript{th} International Workshop on large-scale integration of wind power into power systems},
  year      = {2010},
  pages     = {18--29},
  abstract  = {The determination of additional operating reserves in power systems with high wind penetration is attracting a significant amount of attention and research. Wind integration analysis over the past several years has shown that the level of operating reserve that is induced by wind is not a constant function of the installed capacity. Observations and analysis of actual wind plant operating data has shown that wind does not change its output fast enough to be considered as a contingency event. However, the variability that wind adds to the system does require the activation or deactivation of additional operating reserves. This paper provides a high-level international comparison of methods and key results from both operating practice and integration analysis, based on the work in International Energy Agency IEA WIND Task 25 on Large-scale Wind Integration. The paper concludes with an assessment of the common themes and important differences, along with recent emerging trends.
Index Terms--operating reserves, power system operation, power system reliability, power systems, wind power generation},
  comment   = {Comparison of different methods for allocating reserve power, including in the Dutch frequency domain method. Dynamic reserve allocation is found to be appropriate when wind power is concerned.

I think there was one other NREL paper on this topic too.},
  file      = {Milligan10opRsrvWindPowCmpr.pdf:Milligan10opRsrvWindPowCmpr.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.01.13},
  url       = {http://www.nrel.gov/docs/fy11osti/49019.pdf},
}

@TechReport{Crampes18flexElecMktsRamp,
  author      = {Crampes, Claude and Renault, J{\'e}r{\^o}me and others},
  title       = {Supply flexibility in electricity markets},
  institution = {Toulouse School of Economics (TSE)},
  year        = {2018},
  abstract    = {The development of non-dispatchable renewable sources of energy requires more flexible
reliable thermal equipment to match residual demand. We analyze the advantages of
delaying production decisions to benefit from more precise information on states of the
world, at the expense of higher production costs in a two-period framework where two
technologies with different flexibility characteristics are available. We determine first-
best production levels ex ante and ex post, that is, when demand is still random and
is known with certainty respectively. We then show that, under perfect competition,
first best can be implemented indifferently either by means of ex post state-contingent
markets or by means of a day-ahead market followed by adjustment markets. By
contrast, when the industry is imperfectly competitive, the two market designs are not
equivalent.

JEL codes: C72, D24, D47, L23, L94
Key words: flexibility, electricity, production costs, day-ahead market, real-time
market, perfect competition, imperfect competition},
  comment     = {Says that day ahead markets are not necessary with renewable energy under certain circumstances.  Also has optimization (I think) approach that uses a swinging doors-like ramp and regulation definition.},
  file        = {:Crampes18flexElecMktsRamp.pdf:PDF},
  url         = {https://www.tse-fr.eu/sites/default/files/TSE/documents/doc/wp/2018/wp_tse_964.pdf},
}

@Article{Lei07predWindPowPhaseRecons,
  author    = {Dong Lei and Wang Lijie and Hu Shi and Gao Shuang and Liao Xiaozhong},
  title     = {Prediction of Wind Power Generation based on Chaotic Phase Space Reconstruction Models},
  journal   = {Power Electronics and Drive Systems (PEDS)},
  year      = {2007},
  pages     = {744--748},
  month     = nov,
  abstract  = {The development of wind generation has rapidly progressed over the last decade, but it must be integrated into power grids and electric utility systems. However, it cannot be dispatched like conventional generators because the power generated by the wind changes rapidly because of the continuous fluctuation of wind speed and direction. So it is very important to predict the wind power generation. This paper discusses why the wind power generation can be predicted in short-term, and how to setup the construction of an ANN (artificial neural network) prediction model of wind power based on chaotic time series. The analysis of modeling with low dimensions nonlinear dynamics indicates that time series of wind power generation have chaotic characteristics, and wind power can be predicted in short-term. Phase space reconstruction method can be used for ANN model design. The data from the wind farm located in the Saihanba China are used for this study.},
  comment   = {phase space w/ neural net
* short term so good for 1/2 hour or NowCaster?},
  doi       = {10.1109/PEDS.2007.4487786},
  file      = {Lei07predWindPowPhaseRecons.pdf:Lei07predWindPowPhaseRecons.pdf:PDF;Lei07predWindPowPhaseRecons.pdf:Lei07predWindPowPhaseRecons.pdf:PDF},
  keywords  = {chaos, load forecasting, neural nets, nonlinear dynamical systems, phase space methods, power system simulation, time series, wind power plantsANN prediction model, artificial neural network, chaotic phase space reconstruction models, chaotic time series, electric utility systems, nonlinear dynamics, power grids, wind farm, wind power generation},
  owner     = {sotterson},
  timestamp = {2009.02.12},
  url       = {http://ieeexplore.ieee.org/Xplore/login.jsp?url=/iel5/4472734/4487657/04487786.pdf?arnumber=4487786},
}

@Article{Methaprayoon05neuralWindFrcstUncert,
  author    = {Methaprayoon, K. and Lee, W.J. and Yingvivatanapong, C. and Liao, J.},
  title     = {An integration of ANN wind power estimation into UC considering the forecasting uncertainty},
  journal   = {Industrial and Commercial Power Systems Technical Conference, IEEE},
  year      = {2005},
  pages     = {116--124},
  month     = may,
  abstract  = {The development of wind generation has rapidly progressed over the last decade. With the advance in wind turbine technologies, wind energy has become competitive with other fuel-based generation resources. The fluctuation of wind, however, makes it difficult to optimize the use of wind power generation. Current practice ignores the possible available capacity of the wind generation during the unit commitment scheduling. This may cause operation issues and waste usable capacity when the installation of the wind generation increases. An accurate wind capacity forecasting is essential for efficient wind energy and capacity dispatching. To ensure the system reliability, one also has to consider the forecast uncertainty when integrating the wind capacity into generation planning. This paper discusses the development of an artificial neural network based wind forecast model with the consideration of wind generation uncertainty by using probabilistic concept of confidence interval. The data from a wind farm located in the Southern Oklahoma is used for this study},
  comment   = {Confidence interval wind forecasting w/ neural net. Eric is reading this.},
  doi       = {10.1109/ICPS.2005.1436364},
  file      = {Methaprayoon05neuralWindFrcstUncert.pdf:Methaprayoon05neuralWindFrcstUncert.pdf:PDF;Methaprayoon05neuralWindFrcstUncert.pdf:Methaprayoon05neuralWindFrcstUncert.pdf:PDF},
  groups    = {DOE-PNL09, PointDerived, Use, doReadWPV_2},
  keywords  = {fuel, load forecasting, neural nets, optimisation, power generation dispatch, power generation planning, power generation reliability, power generation scheduling, wind power plants, wind turbinesANN, artificial neural network, forecasting uncertainty, fuel, generation planning, optimization, reliability, wind energy, wind power estimation, wind power generation, wind turbine},
  owner     = {sotterson},
  timestamp = {2009.03.03},
}

@Article{Astolfi08timeVarConnect,
  author    = {Astolfi, L. and Cincotti, F. and Mattia, D. and De Vico Fallani, F. and Tocci, A. and Colosimo, A. and Salinari, S. and Marciani, M.G. and Hesse, W. and Witte, H. and Ursino, M. and Zavaglia, M. and Babiloni, F.},
  title     = {Tracking the Time-Varying Cortical Connectivity Patterns by Adaptive Multivariate Estimators},
  journal   = {Biomedical Engineering, IEEE Transactions on},
  year      = {2008},
  volume    = {55},
  number    = {3},
  pages     = {902--913},
  month     = mar,
  issn      = {0018-9294},
  abstract  = {The directed transfer function (DTF) and the partial directed coherence (PDC) are frequency-domain estimators that are able to describe interactions between cortical areas in terms of the concept of Granger causality. However, the classical estimation of these methods is based on the multivariate autoregressive modelling (MVAR) of time series, which requires the stationarity of the signals. In this way, transient pathways of information transfer remains hidden. The objective of this study is to test a time-varying multivariate method for the estimation of rapidly changing connectivity relationships between cortical areas of the human brain, based on DTF/PDC and on the use of adaptive MVAR modelling (AMVAR) and to apply it to a set of real high resolution EEG data. This approach will allow the observation of rapidly changing influences between the cortical areas during the execution of a task. The simulation results indicated that time-varying DTF and PDC are able to estimate correctly the imposed connectivity patterns under reasonable operative conditions of signal-to-noise ratio (SNR) ad number of trials. An SNR of Ave and a number of trials of at least 20 provide a good accuracy in the estimation. After testing the method by the simulation study, we provide an application to the cortical estimations obtained from high resolution EEG data recorded from a group of healthy subject during a combined foot-lips movement and present the time-varying connectivity patterns resulting from the application of both DTF and PDC. Two different cortical networks were detected with tqhe proposed methods, one constant across the task and the other evolving during the preparation of the joint movement.},
  comment   = {Run adaptive MVAR, derive freq-band-dependency causal graph from the coeffs
* Interested in tracking frequency band dependence of multichan biomed signals
-- fMRI, EEG, MEG
* adaptive MVAR provides the dep. estimate

Causality is derived by two methods
1. Directed transfer function (DTF)
* a non-zero in the MVAR transfer function freq. bin
* later determine direction w/ direction of prediction error reduction (Granger)
2. Partial directed coherence (PDC)
* partial correlation in frequency land
* calculate it on a per-FFT-bin basis
* worked better than DTF},
  doi       = {10.1109/TBME.2007.905419},
  file      = {Astolfi08timeVarConnect.pdf:Astolfi08timeVarConnect.pdf:PDF},
  groups    = {Read},
  keywords  = {EEG data;Granger causality;adaptive multivariate estimators;directed transfer function;electroencephalography;foot-lips movement;human brain;information transfer pathway;multivariate autoregressive modelling;partial directed coherence;time series;time varying cortical connectivity patterns;adaptive estimation;electroencephalography;neurophysiology;time series;Adult;Algorithms;Brain Mapping;Electroencephalography;Evoked Potentials;Female;Humans;Male;Motor Cortex;Movement;Multivariate Analysis;Nerve Net;Neural Pathways;Pattern Recognition, Automated;},
  owner     = {scotto},
  timestamp = {2010.08.01},
}

@InProceedings{Chen15higgsBoostTr,
  author    = {Chen, Tianqi and He, Tong},
  title     = {Higgs boson discovery with boosted trees},
  booktitle = {NIPS 2014 Workshop on High-energy Physics and Machine Learning},
  year      = {2015},
  pages     = {69--80},
  abstract  = {The discovery of the Higgs boson is remarkable for its importance in modern Physics
research. The next step for physicists is to discover more about the Higgs boson from the
data of the Large Hadron Collider (LHC). A fundamental and challenging task is to extract
the signal of Higgs boson from background noises. The machine learning technique is one
important component in solving this problem.
In this paper, we propose to solve the Higgs boson classification problem with a gradient
boosting approach. Our model learns ensemble of boosted trees that makes careful tradeoff
between classification error and model complexity. Physical meaningful features are further
extracted to improve the classification accuracy. Our final solution obtained an AMS of
3.71885 on the private leaderboard, making us the top 2% in the Higgs boson challenge.

Keywords: Higgs Boson, Machine Learning, Gradient Boosting},
  comment   = {Recommended friendly intro to Boosted Trees, like in H2O, I think.  A detailed technical report is also attached.

More technical boosted trees paper is: Chen16xgboostSclbl

General blog post
https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/

Video: Gradient Boosted Machine Learning (Trevor Hastie)
https://youtu.be/wPqtzj5VZus},
  file      = {Paper:Chen15higgsBoostTr.pdf:PDF;Detailed Tech Report:Chen15higgsBoostTr_TR.pdf:PDF},
  url       = {http://proceedings.mlr.press/v42/chen14.pdf},
}

@TechReport{Sigrin16mktDmdDgenDoc,
  author      = {Benjamin Sigrin and Michael Gleason and Robert Preus and Ian Baring-Gould and Robert Margolis},
  title       = {The Distributed Generation Market Demand Model (dGen): Documentation},
  institution = {NREL (National Renewable Energy Laboratory)},
  year        = {2016},
  number      = {NREL/TP-6A20-65231},
  abstract    = {The Distributed Generation Market Demand (dGen) model is a geospatially rich, bottom-up, 
market-penetration model that simulates the potential adoption of distributed energy resources 
(DERs) for residential, commercial, and industrial entities in the continental United States 
through 2050. The National Renewable Energy Laboratory (NREL) developed dGen to analyze 
the key factors that will affect future market demand for distributed solar, wind, storage, and 
other DER technologies in the United States within a single modeling platform. The dGen model 
builds on, extends, and provides significant advances (Table ES-1) over NREL’s SolarDS model 
(Denholm et al. 2009), which is now deprecated. 
Currently, dGen simulates the adoption of distributed solar (the dSolar module) and distributed 
wind (the dWind module), as described in detail in Appendices A and B, respectively. The dGen 
team will add modules in FY16 for behind-the-meter storage (dStorage) as well as a module for 
evaluating distributed geothermal systems (dGeo), such as ground-source heat pumps and 
geothermal direct use. The model is also configured to link with utility scale capacity expansion 
models maintained and applied at NREL (see Appendix C). All technologies modeled within the 
dGen framework leverage a database of highly resolved geospatial information (Figure ES-1), 
along with algorithms for modeling DER economics, customer decision-making, and diffusion of 
technology over time.},
  comment     = {Divides US into 200m x 200m squares and classifies selects some as agents of some energy user category, who will adopt DER or not.  Adoption somehow controlled by adaptive Bass model that changes its coeffs each year.  Detailed land use.  Does both Wind and Solar.

Assumptions 
* most important adoption factors are location, energy consumption and cost: No demographics, but somehow does have population increase
* no econ factors like GINI
* imitation is captured by Bass, not by spatial relationships

Gavin Novotny wrote:

NREL “generic adoption forecast”
•	https://www.nrel.gov/analysis/dgen/
•	“geospatially rich, bottom-up, market-penetration model”
•	Used to assist Maine state legislature in forecasting DPV by 2021
•	Successor to “SolarDS” model

Data Sources
* EIA RECS: Yearly energy consumption by building category in US.
                   Latest update as of May 2019 was in 2018
* EIA CBECS:   Yearly energy consumption by building category in US.
                         Latest update as of May 2019 was in 2012
* ORNL: day/night occupancy for classifying residential
* HSIP: commercial buildings, large spaces and building types
* OpenEI Utility-Rage Database
* load profiles: a couple synthesized for commercial buildings.  I missed where residentials came from


 
See also: 
Dong17resPVdeployFrcst
Gagnon17netMtrAdoptPV
Cole18impactSunShot2030 (uses it)
},
  file        = {:Sigrin16mktDmdDgenDoc.pdf:PDF},
}

@TechReport{Corbus2011eastWindTrans,
  author      = {Corbus, D. and others},
  title       = {Eastern wind integration and transmission study (2011 revision)},
  institution = {National Renewable Energy Laboratory},
  year        = {2011},
  number      = {NREL/TP-550-47078},
  abstract    = {The Eastern Wind Integration and Transmission Study (EWITS) is the culmination of an effort that spanned two and one-half years. The study team began by modeling wind resources in a large part of the Eastern Interconnection and finished by conducting a detailed wind integration study and top-down transmission analysis. The study resulted in information that can be used to guide future work. A number of other studies have already examined similar wind integration issues, but the breadth and depth of the analysis in EWITS is unique. EWITS builds on the work of previous integration studies, which looked at considerably smaller geographic footprints, focused almost exclusively
on wind integration, and did not include transmission. EWITS took the next step by
expanding the study area and including conceptual transmission overlays.
Just a few years ago, 5\% wind energy penetration was a lofty goal, and to some the idea of integrating 20\% wind by 2024 might seem a bit optimistic. And yet, we know from the European experience here some countries have already reached wind energy penetrations of 10\% or higher in a short period of time that change can occur rapidly
and that planning for that change is critically important. Because building transmission
capacity takes much longer than installing wind plants, there is a sense of urgency to studying transmission. It is already starting to limit wind growth in certain areas.
The goal of the EWITS team was not to further any specific agenda or regional vision of the future, but to be as objective as possible while conducting a technical study of future high-penetration wind scenarios. To help guide the EWITS work, the U.S. Department of Energy (DOE) National Renewable Energy Laboratory (NREL) convened a Technical Review Committee (TRC) composed of regional electric reliability council representatives, expert reviewers, transmission planners, utility administrators, and wind industry representatives. Over a period of 14 months while the study was in progress, the TRC held 6 full-day meetings along with numerous Webinars and conference calls to review study progress; comment on study inputs, methods, and assumptions; assist with collecting data; and review drafts of the study report.
Planning for the expansion of the electrical grid is a process that requires an immense amount of study, dialogue among regional organizations, development of technical methodologies, and communication and coordination among a multitude of important stakeholders. Keeping abreast of the changes is challenging because there are so many different developments, ideas, and viewpoints. It is my hope that the EWITS results will be helpful to all those involved in the planning of the future electrical grid and form a foundation for future studies.},
  comment     = {30\% wind is easily economical for East coast. Balancing ramps are driven by load, not wind.},
  file        = {Tech Report:Corbus2011eastWindTrans.pdf:PDF;Executive Overview:Corbus2011eastWindTransExecOv.pdf:PDF},
  location    = {Golden, CO, USA},
  owner       = {sotterson},
  timestamp   = {2012.03.03},
  url         = {http://www.nrel.gov/wind/systemsintegration/ewits.html},
}

@Book{Lange05physShortWindPredBook,
  title     = {Physical Approach to Short-Term Wind Power Prediction},
  publisher = {Springer},
  year      = {2006},
  author    = {Matthias Lange and Ulrich Focken},
  abstract  = {The effective integration of wind energy into the overall electricity supply is a technical and economical challenge because the availability of wind power is determined by fluctuating meteorological conditions. This book offers an approach to the ultimate goal of the short-term prediction of the power output of winds farms. Starting from basic aspects of atmospheric fluid dynamics, the authors discuss the structure of winds fields, the available forecast systems and the handling of the intrinsic, weather-dependent uncertainties in the regional prediction of the power generated by wind turbines. This book addresses scientists and engineers working in wind energy related R and D and industry, as well as graduate students and nonspecialists researchers in the fields of atmospheric physics and meteorology.},
  comment   = {good chapter: Overview of Wind Power Prediction Systems},
  owner     = {sotterson},
  timestamp = {2008.07.03},
  url       = {http://www.springer.com/engineering/power+engineering/book/978-3-540-25662-5},
}

@InProceedings{Alessandrini15AppAnlgEnsSolPowFrcst,
  author    = {Stefano Alessandrini and L. delle Monache and T. Brummet and S. E. Haupt and G. Wiener},
  title     = {An Application of an Analog Ensemble for Short-Term Solar Power Forecasting},
  booktitle = {95th American Meteorological Society Annual Meeting},
  year      = {2015},
  month     = jan,
  abstract  = {The efficient integration of solar power in the energy market is limited by its natural variability and predictability. A cost-effective utilization of solar energy over a grid strongly depends on the accuracy and reliability of the solar power forecasts available to the Transmission System Operators (TSOs). In several countries the legislation requires solar power producers to pay penalties proportional to the errors of day-ahead energy forecasts, which makes the accuracy of such predictions a determining factor for producers to increase their revenues. To this end, probabilistic predictions can provide accurate deterministic forecasts along with a quantification of their uncertainty, as well as a reliable estimate of the probability to overcome a certain production threshold.
The analog ensemble (AnEn) technique has been developed by NCAR (Delle Monache et.al 2011, 2013) and it has been extensively tested for the probabilistic prediction of both meteorological variables and wind power. We will present a novel application of AnEn to solar power forecasting (from photovoltaic panels). The AnEn is based on an historical set of deterministic predictions and observations of the quantity to be predicted. For each forecast lead time and location, the ensemble prediction of a given variable is constituted by a set of measurements of the past (i.e., 1-hour averages of solar power). These measurements are those concurrent to past deterministic predictions for the same lead time and location, chosen based on their similarity to the current forecast. The meteorological variables used to identify the past forecast similar to the current one are called analog predictors. The variable to be predicted, the predictand, is the 1-hour average of the produced solar power. One of the advantages of applying AnEn to solar power predictions is that a radiation to power conversion curve specific for each production unit is not necessary as such conversion is built-in the AnEn approach.
Data from eight solar power production units are used to test the AnEn for solar power forecasting. These units are located in the Sacramento metropolitan area, in California. One-hour average power data are available for an 8-month period. Also, in the proximity of the production units hourly averaged measurements of 2-m air temperature (T2M), global horizontal irradiation (GHI) and direct normal irradiation (DNI) are available.
The deterministic predictions used to generate the AnEn are those from NCAR's DICast system, which is available for the whole period covered by solar power measurements. Every DICast run starts at 1200 UTC and it includes 72 hourly lead times. The forecast time series of GHI, cloud cover (CC), DNI, and T2M have been computed at the solar farm locations and used as analog predictors for the AnEn metric computation. The hourly average azimuth angle (AZ) and solar elevation angle (EL) have been computed separately and then added to the set of analog predictors. The latter allow to define the sun position and to take into account possible obstacle's shadows (e.g., buildings or mountains).
The ?analogs? are selected from a ?training period? defined by the first 5-months of the entire data set. The remaining part (3 months) of the dataset is used to estimate the AnEn performance. To mimic real-time operations, for each forecast the training goes from the start of the period up to one day before the date the forecast was issued.
An in-depth analysis of important attributes of probabilistic predictions generated with the AnEn will be presented. These attributes include statistical consistency, reliability, resolution, sharpness, and the spread-skill relationship. The AnEn provides reliable, sharp, and statistical consistent probabilistic solar power predictions, at a fraction of the real-time computational cost of traditional ensemble methods.},
  comment   = {Reference from Rafael for GIZ Colombia.  See also: Alessandrini15anlogEnsPrbSolPowFrcst},
  owner     = {sotterson},
  timestamp = {2017.04.27},
  url       = {https://ams.confex.com/ams/95Annual/webprogram/Paper268578.html},
}

@TechReport{Kristov14elecDS021st,
  author      = {Lorenzo Kristov and Paul De Martini},
  title       = {21\textsuperscript{st} Century Electric Distribution System Operations},
  institution = {Caltech Resnick Institute},
  year        = {2014},
  type        = {White Paper},
  month       = may,
  abstract    = {The electric industry has gone through dramatic changes over the past two decades. The restructuring program begun in the early 1990s opened up the wholesale markets with numerous competitive generation companies transacting over open-access transmission systems. New entities called independent system operators (ISOs) and regional transmission organizations (RTOs) arose as wholesale market operators and transmission service providers for the majority of customers in the US. In other locales, traditional utility balancing authorities evolved to provide similar functions for their area of responsibility. Many states also opened retail electricity supply to competition, allowing customers greater choice over the sources of energy and creating new business opportunities for retail providers. Concurrently, over 40 states enacted policies to promote renewable supply portfolios and a similar number have net energy metering tariffs which have helped to spur adoption of distribution energy technologies. All of this has occurred during a period of increasing customer service and reliability expectations. Combined, these factors are creating a need to transition our electric system into a 21\textsuperscript{st} century power system that continues to be reliable and cost effective while incorporating more sustainable energy sources.},
  comment     = {Possibly how CAISO may organize its DSO's. Note that DSO's are responsible for net load forecasts. They don't mention probabilistic forecasts for DSO's, though.},
  file        = {Kristov14elecDS021st.pdf:Kristov14elecDS021st.pdf:PDF},
  groups      = {Use, doReadWPV_1},
  location    = {California, USA},
  owner       = {sotterson},
  timestamp   = {2014.07.07},
  url         = {http://smart.caltech.edu/publications-0.shtml},
}

@Article{Huang15extrmLrnMachGap,
  author    = {Guang-Bin Huang},
  title     = {What are Extreme Learning Machines? Filling the Gap Between Frank Rosenblatt?s Dream and John von Neumann?s Puzzle},
  journal   = {Cogn Comput},
  year      = {2015},
  volume    = {7},
  pages     = {263-278},
  abstract  = {The emergent machine learning technique?
extreme learning machines (ELMs)?has become a hot area
of research over the past years, which is attributed to the
growing research activities and significant contributions
made by numerous researchers around the world. Recently, it
has come to our attention that a number of misplaced notions
and misunderstandings are being dissipated on the relation-
ships between ELM and some earlier works. This paper
wishes to clarify that (1) ELM theories manage to address the
open problem which has puzzled the neural networks, ma-
chine learning and neuroscience communities for 60 years:
whether hidden nodes/neurons need to be tuned in learning,
and proved that in contrast to the common knowledge and
conventional neural network learning tenets, hidden nodes/
neurons do not need to be iteratively tuned in wide types of
neural networks and learning models (Fourier series, biolo-
gical learning, etc.). Unlike ELM theories, none of those
earlier works provides theoretical foundations on feedfor-
ward neural networks with random hidden nodes; (2) ELM is
proposed for both generalized single-hidden-layer feedfor-
ward network and multi-hidden-layer feedforward networks
(including biological neural networks); (3) homogeneous
architecture-based ELM is proposed for feature learning,
clustering, regression and (binary/multi-class) classification.
(4) Compared to ELM, SVM and LS-SVM tend to provide
suboptimal solutions, and SVM and LS-SVM do not con-
sider feature representations in hidden layers of multi-hid-
den-layer feedforward networks either.

Keywords Extreme learning machine,  Random vector
functional link, QuickNet, Radial basis function network, Feedforward neural network, Randomness},
  comment   = {Extreme learning machines have universal approximation properties like 3 layer MLPs (similarly, in the limit) .  Authors say you can avoid time spent on network tuning and get the same performance.  Regularization is done by Tchiconnov matrix,  which can be ridge regression or whatever) on the output of the random layer.

ELM and deep learning
* and can also do deep learning, it is claimed
* not clear what the algorithm would be for selecting layers or how to tune them.
* says it does feature learning, not clear how

Node types
* Lots of random bases are possible (a good list is given).
1. Sigmoid
2. Fourier
3. Hardlimit
4. Gaussian
5. Multi-quadrics
6. Wavelet
7. a bunch of complex functions},
  doi       = {10.1007/s12559-015-9333-0},
  file      = {Huang15extrmLrnMachGap.pdf:Huang15extrmLrnMachGap.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2017.01.04},
}

@Article{Bikora18densFrcstDmdEconMdls,
  author   = {Can Bikcora and Lennart Verheijen and Siep Weiland},
  title    = {Density forecasting of daily electricity demand with ARMA-GARCH, CAViaR, and CARE econometric models},
  journal  = {Sustainable Energy, Grids and Networks},
  year     = {2018},
  volume   = {13},
  pages    = {148 - 156},
  issn     = {2352-4677},
  abstract = {The emerging need for risk-aware operational decisions on power systems calls for the development of accurate probabilistic load forecasting methods. To serve this purpose, various celebrated modeling approaches are applied from the field of economics where uncertainty forecasting has been a longstanding fundamental area of research. In particular, this paper proposes the use of ARMA-GARCH conditional mean–variance model in day-ahead forecasting and evaluates the CAViaR quantile regression model and the CARE expectile regression model as alternatives, with all of them incorporating exogenous inputs. In addition to the conventional quasi-maximum likelihood estimation (QMLE) of the ARMA-GARCH model, a special emphasis is put on least-squares (LS) based iterative and nonlinear estimation schemes. Empirical results are generated based on low-voltage side currents collected from transformers in the Netherlands, with the forecasts being assessed probabilistically via the continuous ranked probability score. Performance comparisons demonstrated improved results with the ARMA-GARCH model in relation to the others. Moreover, its estimation by means of the proposed iterative LS estimation method achieved the best forecast performance in a short runtime, thereby proven to be attractive for practical deployment.},
  comment  = {Demand forecast w/ econ models like ARMA and others I've never heard of.  Says Mean-variance models are preferable to quantile and expectile regression models, which I find hard to believe.},
  doi      = {https://doi.org/10.1016/j.segan.2018.01.001},
  file     = {paper:Bikora18densFrcstDmdEconMdls.pdf:PDF},
  keywords = {Conditional mean–variance models, Density forecasting, Expectile regression, Quantile regression, Short-term load forecasting},
  url      = {http://www.sciencedirect.com/science/article/pii/S2352467716300595},
}

@Article{Zhu12shrtWindFrcstOp,
  author    = {Zhu, Xinxin and Genton, Marc G},
  title     = {Short-Term Wind Speed Forecasting for Power System Operations},
  journal   = {International Statistical Review},
  year      = {2012},
  volume    = {80},
  number    = {1},
  pages     = {2--23},
  note      = {pdf from: http://www.stat.tamu.edu/~genton/2012.ZG.ISR.pdf},
  abstract  = {The emphasis on renewable energy and concerns about the environment have led to large-scale
wind energy penetration worldwide. However, there are also significant challenges associated with
the use of wind energy due to the intermittent and unstable nature of wind. High-quality short-term
wind speed forecasting is critical to reliable and secure power system operations. This article begins
with an overview of the current status of worldwide wind power developments and future trends. It
then reviews some statistical short-term wind speed forecasting models, including traditional time
series approaches and more advanced space???ime statistical models. It also discusses the evaluation
of forecast accuracy, in particular, the need for realistic loss functions. New challenges in wind speed
forecasting regarding ramp events and offshore wind farms are also presented.
Key words: Evaluation; forecasting; loss function; ramp event; space???ime model; statistical model;
time series model; wind speed; wind power.},
  comment   = {Loss functions, including probabilistic loss functions, spatio-temporal models, and regime switching. Also, periodic models. Useful for Eweline, etc.},
  doi       = {10.1111/j.1751-5823.2011.00168.x/abstract},
  file      = {Zhu12shrtWindFrcstOp.pdf:Zhu12shrtWindFrcstOp.pdf:PDF},
  groups    = {ErrDistProps, doReadWPV_2},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2013.05.17},
}

@Article{Zhang03bayesNNnewt,
  author    = {Li Zhang and Luh, P.B. and Kasiviswanathan, K.},
  title     = {Energy clearing price prediction and confidence interval estimation with cascaded neural networks},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2003},
  volume    = {18},
  number    = {1},
  pages     = {99--105},
  issn      = {0885-8950},
  abstract  = {The energy market clearing prices (MCPs) in deregulated power markets are volatile. Good MCP prediction and its confidence interval estimation will help utilities and independent power producers submit effective bids with low risks. MCP prediction, however, is difficult since bidding strategies used by participants are complicated and various uncertainties interact in an intricate way. Furthermore, MCP predictors usually have a cascaded structure, as several key input factors need to be predicted first. Cascaded structures are widely used, however, they have not been adequately investigated. This paper analyzes the uncertainties involved in a cascaded neural-network (NN) structure for MCP prediction, and develops the prediction distribution under the Bayesian framework. A computationally efficient algorithm to evaluate the confidence intervals by using the memoryless Quasi-Newton method is also developed. Testing results on a classroom problem and on New England MCP prediction show that the method is computationally efficient and provides accurate prediction and confidence coverage. The scheme is generic, and can be applied to various networks, such as multilayer perceptrons and radial basis function networks.},
  comment   = {Maybe good for Bayesian Model Averaging of an ensemble of point forecasts (coming from NN).

Referenced in Guan12loadFrcstWvltNNpreFilt(?), where it is said that this paper:

Starts with a prior distribution of the NN's weights, and then optimized weights are determined by maximizing the posterior distribution based on historical data. Through Taylor series expansion, the prediction distribution conditioned on a new input and weights was derived and approximated as a Gaussian distribution. The method in this paper uses a quasi-newtwon method.

Wright99bayesNN is same idea but is less computationally efficient},
  doi       = {10.1109/TPWRS.2002.807062},
  file      = {Zhang03bayesNNnewt.pdf:Zhang03bayesNNnewt.pdf:PDF},
  groups    = {Ensemble, Use, doReadNonWPV_1},
  keywords  = {Bayes methods;cascade networks;neural nets;power markets;power system analysis computing;power system economics;statistical analysis;tariffs;Bayesian framework;bidding strategies;cascaded neural networks;confidence coverage;confidence interval estimation;deregulated power markets;energy market clearing price prediction;independent power producers;memoryless Quasi-Newton method;multilayer perceptrons;prediction accuracy;prediction distribution;radial basis function networks;utilities;Bayesian methods;Helium;Measurement uncertainty;Multilayer perceptrons;Neural networks;Power markets;Power system management;Radial basis function networks;Risk management;Testing},
  owner     = {sotterson},
  timestamp = {2013.04.10},
}

@Article{Nott12enKFisABC,
  author    = {Nott, David J and Marshall, Lucy and Ngoc, Tran Minh},
  title     = {The ensemble {Kalman} filter is an ABC algorithm},
  journal   = {Statistics and Computing},
  year      = {2012},
  volume    = {22},
  number    = {6},
  pages     = {1273--1276},
  abstract  = {The ensemble Kalman filter is the method of choice for many difficult high-dimensional filtering problems in meteorology, oceanography, hydrology and other fields. In this note we show that a common variant of the ensemble Kalman filter is an approximate Bayesian computation (ABC) algorithm. This is of interest for a number of reasons. First, the ensemble Kalman filter is an example of an ABC algorithm that predates the development of ABC algorithms. Second, the ensemble Kalman filter is used for very high-dimensional problems, whereas ABC methods are normally applied only in very low-dimensional problems. Third, recent state of the art extensions of the ensemble Kalman filter can also be understood within the ABC framework.},
  comment   = {Maybe ABC methods can be used for high dim problems, since a kind of ensemble Kalman filter is, and since it's an ABC. Other insights too.

Note that other ABC methods avoid dimensionality problems w/ feature selection or dimension reduction (in this bibtex file). Maybe that's that the EnKF is doing too?},
  file      = {Nott12enKFisABC.pdf:Nott12enKFisABC.pdf:PDF},
  groups    = {Ensemble, PointDerived, Test, doReadNonWPV_2},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2013.10.04},
  url       = {http://link.springer.com/article/10.1007%2Fs11222-011-9300-x},
}

@TechReport{Giebel05windEnsemble,
  author      = {Gregor Giebel and Jake Badger and Lars Landberg and Henrik Aalborg Nielsen and Torben Skov Nielsen and Henrik Madsen and Kai Sattler and Henrik Feddersen and Henrik Vedel and John T{\o}fting and Lars Kruse and Lars Voulund},
  title       = {Wind Power Prediction using Ensembles},
  institution = {Ris{\o} National Laboratory},
  year        = {2005},
  number      = {Ris{\o}-R-1527(EN)},
  month       = sep,
  abstract    = {The Ensemble project investigated the use of meteorological
ensemble fore-casts for the prognosis of uncertainty of the
forecasts, and found a good method to make use of ensemble
forecasts. This method was then tried based on ensembles from
ECMWF in form of a demo application for both the Nysted
offshore wind farm and the whole Jutland/Funen area. The utilities
used these forecasts for maintenance planning, fuel consumption
estimates and over-the-weekend trading on the Leipzig power
exchange. Other notable scientific results include the better
accuracy of forecasts made up from a simple superposition of two
NWP provider (in our case, DMI and DWD), an investigation of the
merits of a parameterisation of the turbulent kinetic energy within
the delivered wind speed forecasts, and the finding that a ?na?ve?
downscaling of each of the coarse ECMWF ensemble members
with higher resolution HIRLAM did not improve the error scores or
the result space enough to warrant the computational effort.},
  comment     = {* instantanious time-lagged ensemble spread doesn't predict uncertainty (ref 6(
 -- but apparenty their temporal development does give a clue about ensembles papers (refs 2,3)
 ---- Pinson09skillEnsWndPow may be an update of ref 2
 ---- ref 3: Lange02windPowShrtTrmMetCond
* NWP errors don't depend upon wind speed!
 -- but do depend upon location in power curve
* Interesting KNN forecasting idea (ref 11)
 -- look back through history for K=10 days with forecasts like the one that has been forecasted
 -- use (the actually measured?) wind speeds during those days to build an empirical quantile forecast
 -- I guess it worked, but I can't find the paper.
 -- this is a kind of KNN regime learning
* tried TKE (turbulent kinetic energy) as a forecast input
 -- several types
 -- one of them slightly improved a point forecast
 -- but they didn't check to see if it could improve a quantile forecast (maybe I should try this?)

Ensembles --> power
 * first learn a logarithmic transform of power
 -- separation of power and speed makes it possible to always cover full power range on test, if it wasn't in train
 -- do it this way so that it's not biased (I gather from the Conclusions on p. 39, although I missed this in the text)
 * Predict transformed power with nonlinear local function
 -- a function of NWP u,v and horizon
 -- horizon dependence is affected only by speed and dir
 -- trained on ensembles? Doesn't say.
 -- power curve is a deterministic fit (local linear and local constant)
 -- trained by S+ (and R?) function LFLM
* Foreach ensemble, predict the power
* At each time, calc raw pow probs from ranks of ensemble probs
 -- means that you can't predict quantiles outside of (25-75%)!
 -- is badly calibrated, as shown in QQ plots
 -- but can use the QQ plots with measurement quantiles to adjust the raw probabilities
* Predict quantiles from raw power probs
 -- a spline fit of the raw power prob, predicting a log transformed version of the adjusted raw probs
 ---- does this keep it bounded or something?
 -- horizon has a multiplicative affected on quantile prob. (and is modeled w/ a local 2\textsuperscript{nd} order poly)
* Adaptation
 -- they say it's really important to adapt, or at least regularly recalibrate
* autocorrelation is ignored in this method
* NWP: HIRLAM and COSMODE, I think

Use of forecasts
* for Elsam (Danish pow Co.?) control room, they're used in both planning and operation phase
* in demo, find that users just use the point forecast (ensemble mean), not the ensemble
* users aren't able to judge quality of forecasts b/c don't have enough data
* for some power plants need at least a 7 day ahead forecast!
* Theoretical uses
 -- Electricity trade over weekend
 -- Power station failure
 -- Fuel demand predictions


RESULTS
* the usual reliability, sharpness, and resolution plots
* Spread/skill relationship: they do find one
 -- the forecast interquantile range does predict the pt. forecast MAE (pt. forecast is the ensemble median)
 -- I'm not sure if it exists below 24 hours (they "disregard" this range)
* get slightly better results using NWP grid point offset in the direction of offshore winds.
* Multi-model: avg. better than either NWP by itself, as usual
*},
  file        = {Giebel05windEnsemble.pdf:Giebel05windEnsemble.pdf:PDF},
  groups      = {Read, Ensemble, doReadWPV_1, doReadNonWPV_1, doReadNonWPV_2},
  owner       = {scotto},
  timestamp   = {2008.07.04},
  url         = {http://www.risoe.dtu.dk/knowledge_base/publications/reports/ris-r-1527.aspx?sc_lang=en},
}

@TechReport{Arellano-Valle11mutInfoMultiVar,
  author      = {Reinaldo B. Arellano-Valle and Javier E. Contreras-Reyes and Marc G. Genton},
  title       = {{Shannon} Entropy and Mutual Information for Multivariate Skew-Elliptical Distributions, with Application to Monitoring Network Design},
  institution = {KingAbdullah University of Science and Technology (KAUST)},
  year        = {2011},
  type        = {IAMCS Pre-Print Paper Series},
  number      = {2011-207},
  abstract    = {The entropy and mutual information index are important concepts developed by Shannon (1948) in the context of information theory. They have been widely studied in the case of the multivariate normal distribution. We first extend these tools to the full symmetric class of multivariate elliptical distributions, and then to the more flexible families of multivariate skew-elliptical distributions. We study in detail the cases of the multivariate skew-normal and skew-t distributions. We implement our findings to the application of the optimal design of an ozone monitoring station network in Santiago de Chile.},
  comment     = {Mutual info expressions for several distributions, including Normal distribution. Says they're Shannon but then seem to be differential!

* expression for differential multivariate entropy agrees w/ Ahmed89entropyMultiVar, has dimension dependence (p.7)
* mutual informatoin doesn't have dim dependence

Note: these are differential vs. shannon entropies and MI's.

Using Wikipedia as a crutch, it defines the entropy found here as differential: http://en.wikipedia.org/wiki/Multivariate_normal_distribution\#Entropy and even cites Ahmed as the source.},
  file        = {Arellano-Valle11mutInfoMultiVar.pdf:Arellano-Valle11mutInfoMultiVar.pdf:PDF},
  owner       = {scotto},
  timestamp   = {2011.05.21},
  url         = {http://iamcs.tamu.edu/research_sub.php?tab_sub=research&cms_id=8},
}

@InCollection{HernandezLobato13gaussProceCondTS,
  author    = {Hern\'{a}ndez-Lobato, Jos\'{e} Miguel and Lloyd, James R and Hern\'{a}ndez-Lobato, Daniel},
  title     = {Gaussian Process Conditional Copulas with Applications to Financial Time Series},
  booktitle = {Advances in Neural Information Processing Systems 26},
  publisher = {Curran Associates, Inc.},
  year      = {2013},
  editor    = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  pages     = {1736--1744},
  abstract  = {The estimation of dependencies between multiple variables is a central problem in the analysis of financial time series. A common approach is to express these dependencies in terms of a copula function. Typically the copula function is assumed to be constant but this may be innacurate when there are covariates that could have a large influence on the dependence structure of the data. To account for this, a Bayesian framework for the estimation of conditional copulas is proposed. In this framework the parameters of a copula are non-linearly related to some arbitrary conditioning variables. We evaluate the ability of our method to predict time-varying dependencies on several equities and currencies and observe consistent performance gains compared to static copula models and other time-varying copula methods.},
  comment   = {How to compute a conditional Gaussian Process (with slides).  Does not look as simple as with a Normal Copula.
  Related to (same as?): Wilson10copulaProc },
  file      = {Paper:HernandezLobato13gaussProceCondTS.pdf:PDF;Slides:HernandezLobato13gaussProceCondTS_Slides.pdf:PDF},
  url       = {http://papers.nips.cc/paper/5084-gaussian-process-conditional-copulas-with-applications-to-financial-time-series.pdf},
}

@Article{Wang13extrmQRpowTranf,
  author    = {Huixia Judy Wang and Deyuan Li},
  title     = {Estimation of Extreme Conditional Quantiles Through Power Transformation},
  journal   = {Journal of the American Statistical Association},
  year      = {2013},
  volume    = {108},
  number    = {503},
  pages     = {1062--1074},
  abstract  = {The estimation of extreme conditional quantiles is an important issue in numerous disciplines. Quantile regression (QR) provides a natural way to capture the covariate effects at different tails of the response distribution. However, without any distributional assumptions, estimation from conventional QR is often unstable at the tails, especially for heavy-tailed distributions due to data sparsity. In this article, we develop a new three-stage estimation procedure that integrates QR and extreme value theory by estimating intermediate conditional quantiles using QR and extrapolating these estimates to tails based on extreme value theory. Using the power-transformed QR, the proposed method allows more flexibility than existing methods that rely on the linearity of quantiles on the original scale, while extending the applicability of parametric models to borrow information across covariates without resorting to nonparametric smoothing. In addition, we propose a test procedure to assess the commonality of extreme value index, which could be useful for obtaining more efficient estimation by sharing information across covariates. We establish the asymptotic properties of the proposed method and demonstrate its value through simulation study and the analysis of a medical cost data. Supplementary materials for this article are available online. [I've pasted that on the end of the attached pdf]

KEY WORDS: Box-Cox power transformation; Extreme value; Heavy-tailed distribution; High quantile; Quantile regression.},
  comment   = {A way to do linear quantile regression without assuming linearity in the extreme conditional distribution tails. Maybe a way to handle boundedness of power curves? Also has a test for tail heaviness dependence upon QR inputs. So, a kind of feature selection.},
  doi       = {10.1080/01621459.2013.820134},
  eprint    = {http://dx.doi.org/10.1080/01621459.2013.820134},
  file      = {Wang13extrmQRpowTranf.pdf:Wang13extrmQRpowTranf.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.04.20},
}

@InProceedings{Laparra09pcaGauss,
  author    = {Laparra, V. and Camps-Valls, G. and Malo, J.},
  title     = {{PCA Gauss}ianization for image processing},
  booktitle = {Image Processing (ICIP)},
  year      = {2009},
  pages     = {3985--3988},
  month     = {7-10},
  abstract  = {The estimation of high-dimensional probability density functions (PDFs) is not an easy task for many image processing applications. The linear models assumed by widely used transforms are often quite restrictive to describe the PDF of natural images. In fact, additional non-linear processing is needed to overcome the limitations of the model. On the contrary, the class of techniques collectively known as projection pursuit, which solve the high-dimensional problem by sequential univariate solutions, may be applied to very general PDFs (e.g. iterative Gaussianization procedures). However, the associated computational cost has prevented their extensive use in image processing. In this work, we propose a fast alternative to iterative Gaussianization methods that makes it suitable for image processing while ensuring its theoretical convergence. Method performance is successfully illustrated in image synthesis and classification problems.},
  comment   = {fast gaussianization? has matlab here: http://www.uv.es/vista/vistavalencia/software/GPCA/GPCA.htm},
  doi       = {10.1109/ICIP.2009.5413808},
  file      = {Laparra09pcaGauss.pdf:Laparra09pcaGauss.pdf:PDF},
  issn      = {1522-4880},
  keywords  = {PCA Gaussianization;high-dimensional probability density function estimation;image classification problems;image processing;image synthesis problem;iterative Gaussianization methods;linear models;nonlinear processing;principal component analysis;sequential univariate solutions;Gaussian processes;image classification;iterative methods;principal component analysis;probability;},
  owner     = {scot},
  timestamp = {2010.08.02},
}

@Article{Fan16estLrgCovPrecMatrix,
  author    = {Fan, Jianqing and Liao, Yuan and Liu, Han},
  title     = {An overview of the estimation of large covariance and precision matrices},
  journal   = {The Econometrics Journal},
  year      = {2016},
  volume    = {19},
  number    = {1},
  pages     = {C1--C32},
  issn      = {1368-423X},
  abstract  = {The estimation of large covariance and precision matrices is fundamental in modern multivariate analysis. However, problems arise from the statistical analysis of large panel economic and financial data. The covariance matrix reveals marginal correlations between variables, while the precision matrix encodes conditional correlations between pairs of variables given the remaining variables. In this paper, we provide a selective review of several recent developments on the estimation of large covariance and precision matrices. We focus on two general approaches: a rank-based method and a factor-model-based method. Theories and applications of both approaches are presented. These methods are expected to be widely applicable to the analysis of economic and financial data.
   Keywords: High-dimensionality, graphical model, approximate factor model, principal components, sparse matrix, low-rank matrix, thresholding, heavy-tailed, elliptical distribution, rank based methods
},
  comment   = {Maybe explains precision matrix and other stuff....
  Also has a chapter on estimating cov matrices for heavy-tailed elliptical distributions (Student T?)

pdf came from: https://arxiv.org/pdf/1504.02995

This paper is cited by other papers that might be quite interesting:

https://scholar.google.com/scholar?cites=16271339680434877919&as_sdt=2005&sciodt=0,5&hl=en

e.g. this thesis:

https://dspace.mit.edu/handle/1721.1/105001

which builds graphical models, compares to graph lasso, which might be useful for turbine root cause finding.},
  doi       = {10.1111/ectj.12061},
  file      = {Fan16estLrgCovPrecMatrix.pdf:Fan16estLrgCovPrecMatrix.pdf:PDF},
  keywords  = {Approximate factor model, Elliptical distribution, Graphical model, Heavy-tailed, High-dimensionality, Low-rank matrix, Principal components, Rank-based methods, Sparse matrix, Thresholding},
  owner     = {sotterson},
  timestamp = {2017.06.14},
  url       = {http://dx.doi.org/10.1111/ectj.12061},
}

@Article{Pinheiro96unCnstrParamVarCovMat,
  author    = {Pinheiro, Jos{\'e} C. and Bates, Douglas M.},
  title     = {Unconstrained parametrizations for variance-covariance matrices},
  journal   = {Statistics and Computing},
  year      = {1996},
  volume    = {6},
  number    = {3},
  pages     = {289--296},
  issn      = {1573-1375},
  abstract  = {The estimation of variance-covariance matrices through optimization of an objective function, such as a log-likelihood function, is usually a difficult numerical problem. Since the estimates should be positive semi-definite matrices, we must use constrained optimization, or employ a parametrization that enforces this condition. We describe here five different parametrizations for variance-covariance matrices that ensure positive definiteness, thus leaving the estimation problem unconstrained. We compare the parametrizations based on their computational efficiency and statistical interpretability. The results described here are particularly useful in maximum likelihood and restricted maximum likelihood estimation in linear and non-linear mixed-effects models, but are also applicable to other areas of statistics.},
  comment   = {parameteriztions of covariance matrix that ensure pos semi-definiteness, as a cov. mat must have.  Could be in the middle of a covariance learning optimization routine, if the paramerizations aren't to restrictiive.  Avoids constraints, as the parameterization makes them unnecessary.},
  doi       = {10.1007/BF00140873},
  file      = {Pinheiro96unCnstrParamVarCovMat.pdf:Pinheiro96unCnstrParamVarCovMat.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.13},
  url       = {http://dx.doi.org/10.1007/BF00140873},
}

@Article{Wiesenthal12enrgyTechLrnPolicy,
  author   = {Wiesenthal, T and Dowling, P and Morbee, J and Thiel, C and Schade, B and Russ, P and Simoes, S and Peteves, S and Schoots, K and Londo, M and others},
  title    = {Technology learning curves for energy policy support},
  journal  = {JRC scientific and policy reports},
  year     = {2012},
  volume   = {332},
  pages    = {617-629},
  issn     = {1464-4096},
  abstract = {The European Commission's Joint Research Centre and the Energy Research Centre of the Netherlands (ECN) organised an expert workshop on 'Learning Curves for Policy Support' in Amsterdam on 8 March 2012. It aimed to assess the challenges in the application of the two-factor learning curve, or alternative solutions in supporting policy decision making in the framework of the European Strategic Energy Technology Plan, and explored options for improvement. The workshop gathered distinguished experts in the field of scientific research on learning curves and policy researchers from the European Commission and ECN to assess the challenges in the application of the two-factor-learning curve, or alternative solutions in supporting policy decision making, and to provide options for improvement. This paper forms the summary of outcomes from the workshop. Due to the very different nature of the One-Factor-Learning concept and the Two-Factor-Learning concept, these are discussed in separate parts. In each of these parts the context and the methodology are introduced, methodological and data challenges are described and the problems associated with the application of the concept in models is discussed.},
  comment  = {Some kinda two factor learning curve for tech. learning.  wasn't that MIT prof already using tech learning neural nets or something?},
  doi      = {10.1111/bju.12315},
  file     = {:Wiesenthal12enrgyTechLrnPolicy.pdf:PDF},
  subtitle = {Learning curves for urological procedures},
  url      = {https://ec.europa.eu/jrc/en/publication/eur-scientific-and-technical-research-reports/technology-learning-curves-energy-policy-support},
}

@Article{TautzWeinert16scadaWindTurbCondMon,
  author    = {Tautz-Weinert, Jannis and Watson, Simon J},
  title     = {Using SCADA Data for Wind Turbine Condition Monitoring--},
  journal   = {IET Renewable Power Generation},
  year      = {2016},
  abstract  = {The ever increasing size of wind turbines and the move to build them offshore have accelerated the need for optimised maintenance strategies in order to reduce operating costs. Predictive maintenance requires detailed information on the condition of turbines. Due to the high costs of dedicated condition monitoring systems based on mainly vibration measurements, the use of data from the turbine supervisory control and data acquisition (SCADA) system is appealing. This review discusses recent research using SCADA data for failure detection and condition monitoring (CM), focussing on approaches which have already proved their ability to detect anomalies in data from real turbines. Approaches are categorised as (i) trending, (ii) clustering, (iii) normal behaviour modelling, (iv) damage modelling and (v) assessment of alarms and expert systems. Potential for future research on the use of SCADA data for advanced turbine CM is discussed.},
  comment   = {ModernWindABS.  New but I don't know if it's a good article.},
  doi       = {10.1049/iet-rpg.2016.0248},
  file      = {TautzWeinert16scadaWindTurbCondMon.pdf:TautzWeinert16scadaWindTurbCondMon.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2016.12.19},
  url       = {10.1049/iet-rpg.2016.0248},
}

@InProceedings{Florita12frcstErrFreqDist,
  author    = {Florita, A and Hodge, BM and Milligan, M},
  title     = {Wind power forecasting error frequency analyses for operational power system studies},
  booktitle = {11\textsuperscript{th} International Workshop on Large-Scale Integration of Wind Power in Power Systems Proceedings, Lisbon, Portugal},
  year      = {2012},
  url       = {http://www.nrel.gov/docs/fy12osti/56086.pdf},
  abstract  = {The examination of wind power forecasting errors is crucial for optimal unit commitment and economic dispatch of power systems with significant wind power penetrations. This scheduling process includes both renewable and nonrenewable generators, and the incorporation of wind power forecasts will become increasingly important as wind fleets constitute a larger portion of generation portfolios. This research considers the Western Wind and Solar Integration Study database of wind power forecasts and numerical actualizations. This database comprises more than 30,000 locations spread throughout the western United States, with a total wind power capacity of 960 GW. Error analyses for individual sites and for specific balancing areas are performed using the database, quantifying the fit to theoretical distributions through goodness-of-fit metrics. Insights into wind-power forecasting error distributions are established for various levels of temporal and spatial resolution, contrasts made among the frequency distribution alternatives, and recommendations put forth for harnessing the results. Empirical data are used to produce more realistic site-level forecasts than previously employed, such that higher resolution operational studies are possible. This research feeds into a larger work of renewable integration through the links wind power forecasting has with various operational issues, such as stochastic unit commitment and flexible reserve level determination. Keywords?wind forecasting, error},
  file      = {Florita12frcstErrFreqDist.pdf:Florita12frcstErrFreqDist.pdf:PDF},
  groups    = {Test, ErrDistProps, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2013.02.18},
}

@InBook{Perera14machLrnTechRES,
  pages     = {81--96},
  title     = {Machine Learning Techniques for Supporting Renewable Energy Generation and Integration: A Survey},
  publisher = {Springer International Publishing},
  year      = {2014},
  author    = {Perera, Kasun S. and Aung, Zeyar and Woon, Wei Lee},
  editor    = {Woon, Wei Lee and Aung, Zeyar and Madnick, Stuart},
  address   = {Cham},
  isbn      = {978-3-319-13290-7},
  abstract  = {The extraction of energy from renewable sources is rapidly growing. The current pace of technological development makes it commercially viable to harness energy from sun, wind, geothermal and many other renewable sources. Because of the negative effects on the environment and the economy, conventional energy sources like natural gas, crude oil and coal are coming under political and economic pressure. Thus, they require a better mix of energy sources with a higher percentage of renewable energy sources. Harnessing energy from renewable sources range from small scale (e.g., a single household) to large scale (e.g., power plants producing several MWs to a few GWs providing energy to an entire city). An inherent characteristic common to all renewable power plants is that power generation is dependent on environmental parameters and thus cannot be fully controlled or planned for in advance. In a power grid, it is necessary to predict the amount of power that will be generated in the future, including those from the renewable sources, as fluctuations in capacity and/or quality can have negative impacts on the physical health of the entire grid as well as the quality of life of its users. As renewable power plants continue to expand, it will also be necessary to determine their optimal sizes, locations and configurations. In addition, management of the smart grid, in which the renewable energy plants are integrated, is also a challenging problem. In this paper we provide a survey on different machine learning techniques used to address the above issues related to renewable energy generation and integration.

Keywords

Renewable energy Smart grids Machine learning},
  booktitle = {Data Analytics for Renewable Energy Integration: Second ECML PKDD Workshop, DARE 2014, Nancy, France, September 19, 2014, Revised Selected Papers},
  doi       = {10.1007/978-3-319-13290-7_7},
  file      = {Perera14machLrnTechRES.pdf:Perera14machLrnTechRES.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.26},
  url       = {http://dx.doi.org/10.1007/978-3-319-13290-7_7},
}

@Article{Feo08interDepWeakCoupleDet,
  author    = {Oscar De Feo and Cristian Carmeli},
  title     = {Estimating interdependences in networks of weakly coupled deterministic systems},
  journal   = {Physical Review E},
  year      = {2008},
  volume    = {77},
  number    = {2},
  pages     = {026711},
  abstract  = {The extraction of information from measured data about the interactions taking place in a network of systems is a key topic in modern applied sciences. This topic has been traditionally addressed by considering bivariate time series, providing methods which are sometimes difficult to extend to multivariate data, the limiting factor being the computational complexity. Here, we present a computationally viable method based on black-box modeling which, while theoretically applicable only when a deterministic hypothesis about the processes behind the recordings is plausible, proves to work also when this assumption is severely affected. Conceptually, the method is very simple and is composed of three independent steps: in the first step a state-space reconstruction is performed separately on each measured signal; in the second step, a local model, i.e., a nonlinear dynamical system, is fitted separately on each reconstructed measured signal; afterward, a linear model of the dynamical interactions is obtained by cross-relating the reconstructed measured variables to the dynamics unexplained by the local models. The method is successfully validated on numerically generated data. An assessment of its sensitivity to data length and modeling and measurement noise intensity, and of its applicability to large-scale systems, is also provided.},
  comment   = {Like partial directed coherence or partial mutual information but does it work on shorter/noisier/weaker data?
* hard to say since it's not compared to the alternatives},
  doi       = {10.1103/PhysRevE.77.026711},
  eid       = {026711},
  file      = {Feo08interDepWeakCoupleDet.pdf:Feo08interDepWeakCoupleDet.pdf:PDF;Feo08interDepWeakCoupleDet.pdf:Feo08interDepWeakCoupleDet.pdf:PDF},
  keywords  = {nonlinear dynamical systems; time series},
  numpages  = {15},
  owner     = {sotterson},
  publisher = {APS},
  timestamp = {2009.03.16},
  url       = {http://link.aps.org/abstract/PRE/v77/e026711},
}

@Article{Pelland13pvSolarFrcstStateOfArt,
  author    = {Pelland, Sophie and Remund, Jan and Kleissl, J and Oozeki, Takashi and De Brabandere, K},
  title     = {Photovoltaic and solar forecasting: state of the art},
  journal   = {IEA PVPS, Task},
  year      = {2013},
  volume    = {14},
  note      = {ISBN 978-3-906042-13-8},
  abstract  = {The field of solar and photovoltaic (PV) forecasting is rapidly evolving. The current report provides a
snapshot of the state of the art of this dynamic research area, focusing on solar and PV forecasts for
time horizons ranging from a few minutes ahead to several days ahead. Diverse resources are used to
generate solar and PV forecasts, ranging from measured weather and PV system data to satellite and
sky imagery observations of clouds, to numerical weather prediction (NWP) models which form the
basis of modern weather forecasting. The usefulness of these resources varies depending on the
forecast horizon considered: very short?term forecasts (0 to 6 hours ahead) perform best when they
make use of measured data, while numerical weather prediction models become essential for
forecast horizons beyond approximately six hours. The best approaches make use of both data and
NWP models. Examples of this strategy include the use of NWP model outputs in stochastic learning
models, or the use of measured data for post?processing NWP models to correct systematic
deviations between NWP model outputs and measured data.
Benchmarking efforts have been conducted to compare the accuracy of various solar and PV forecast
models against common datasets. Such benchmarking is critical to assessing forecast accuracy, since
this accuracy depends on numerous factors, such as local climate, forecast horizon and whether
forecasts apply to a single point or cover a wide geographic area. In the latter case, which is often the
main interest of electric system operators, higher accuracies can be achieved since random errors at
distant locations tend to be largely uncorrelated and to partially cancel out.},
  comment   = {Later state of the art for PV than Kleissl10solarFrcstStateOfArt. Just read this one?},
  file      = {Pelland13pvSolarFrcstStateOfArt.pdf:Pelland13pvSolarFrcstStateOfArt.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.02.19},
  url       = {http://uc-ciee.org/all-documents/a/451/113/nested},
}

@PhdThesis{Ng10kernCopulaProcThesis,
  author    = {Ng, Eddie KH},
  title     = {Kernel-based copula processes},
  school    = {University of Toronto},
  year      = {2010},
  type      = {phdthesis},
  abstract  = {The field of time-series analysis has made important contributions to a wide spectrum of ap-

plications such as tide-level studies in hydrology, natural resource prospecting in geo-statistics,

speech recognition, weather forecasting, financial trading, and economic forecasts and analy-

sis. Nevertheless, the analysis of the non-Gaussian and non-stationary features of time-series

remains challenging for the current state-of-art models.

This thesis proposes an innovative framework that leverages the theory of copula, combined

with a probabilistic framework from the machine learning community, to produce a versatile

tool for multiple time-series analysis. I coined this new model Kernel-based Copula Processes

(KCPs). Under the new proposed framework, various idiosyncracies can be modeled compactly

via a kernel function for each individual time-series, and long-range dependency can be cap-

tured by a copula function. The copula function separates the marginal behavior and serial

dependency structures, thus allowing them to be modeled separately and with much greater

flexibility. Moreover, the codependent structure of a large number of time-series with poten-

tially vastly different characteristics can be captured in a compact and elegant fashion through

the notion of a binding copula. This feature allows a highly heterogeneous model to be built,

breaking free from the homogeneous limitation of most conventional models. The KCPs have

demonstrated superior predictive power when used to forecast a multitude of data sets from

meteorological and financial areas. Finally, the versatility of the KCP model is exemplified

when it was successfully applied to non-trivial classification problems unaltered},
  comment   = {Copulas with, I think, kernel dependencies.  Can do spatial and temporal dependence.  Has relation to stochastic differential equations (SDEs)},
  file      = {Ng10kernCopulaProcThesis.pdf:Ng10kernCopulaProcThesis.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.22},
  url       = {http://utstat.utoronto.ca/sjaimung/papers/Phd_Thesis_Eddie_KH_Ng.pdf},
}

@InBook{Jaimungal09KernCopulaPhD,
  pages     = {628--643},
  title     = {Kernel-Based Copula Processes},
  publisher = {Springer Berlin Heidelberg},
  year      = {2009},
  author    = {Jaimungal, Sebastian and Ng, Eddie K. H.},
  editor    = {Buntine, Wray and Grobelnik, Marko and Mladeni{\'{c}}, Dunja and Shawe-Taylor, John},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-04180-8},
  abstract  = {The field of time-series analysis has made important contributions to a wide spectrum of ap-

plications such as tide-level studies in hydrology, natural resource prospecting in geo-statistics,

speech recognition, weather forecasting, financial trading, and economic forecasts and analy-

sis. Nevertheless, the analysis of the non-Gaussian and non-stationary features of time-series

remains challenging for the current state-of-art models.

This thesis proposes an innovative framework that leverages the theory of copula, combined

with a probabilistic framework from the machine learning community, to produce a versatile

tool for multiple time-series analysis. I coined this new model Kernel-based Copula Processes

(KCPs). Under the new proposed framework, various idiosyncracies can be modeled compactly

via a kernel function for each individual time-series, and long-range dependency can be cap-

tured by a copula function. The copula function separates the marginal behavior and serial

dependency structures, thus allowing them to be modeled separately and with much greater

flexibility. Moreover, the codependent structure of a large number of time-series with poten-

tially vastly different characteristics can be captured in a compact and elegant fashion through

the notion of a binding copula. This feature allows a highly heterogeneous model to be built,

breaking free from the homogeneous limitation of most conventional models. The KCPs have

demonstrated superior predictive power when used to forecast a multitude of data sets from

meteorological and financial areas. Finally, the versatility of the KCP model is exemplified

when it was successfully applied to non-trivial classification problems unaltered.},
  booktitle = {Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2009, Bled, Slovenia, September 7-11, 2009, Proceedings, Part I},
  doi       = {10.1007/978-3-642-04180-8_58},
  file      = {PhD Thesis:Jaimungal09KernCopulaPhD.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.06},
  url       = {http://dx.doi.org/10.1007/978-3-642-04180-8_58},
}

@Book{Rogers83diffusInnovBk,
  title     = {Diffusion of innovations},
  publisher = {The Free Press},
  year      = {1983},
  author    = {Everett M. Rogers},
  abstract  = {THE FIRST EDITION OF THIS BOOK, Dijflsion 0flnn0vati0ns, was
published in 1962. At the time, there were 405 publications about this
topic available. The second edition and revision, Communication of
Innovations: A Cross-Cultuml Approach (CO-authored with F. Floyd
Shoemaker), was published in 1971, nine years later. By then the
number of diffusion publications had almost quadmpled t0 about
1,500, ofwhich approximately 1,200 were empirical research reports,
and the other 300 were bibliographies, syntheses, theoretical writings,
and 0ther types of nonempirical publications.
At present, 12 years later, the total number of diffusion publica-
tions has more than doubled again, t0 3,085. And the number of em-
pirical diffusion research reports has increased from 1,500 t0 2,297. I
think there is almost n0 0ther field of behavior science research that
represents more effort by more scholars in more nations.
Because of this vast increase in the foundation of diffusion
research on Which the present book is based, it is both (1) a revision of
the theoretical frarnework, and the research evidence supporting this
model ofdiffusion, and (2) a new intellectual venture, in the sense that
new concepts and new theoretical Viewpoints are introduced. I
estimate that this book represents about equally (1) a continuity with
my two previous books on diffusion, and (2) differences and im-
provements in the basic framework. So the reader can regard the pres-
ent book as the third volume in a three-volume set on the diffusion of
innovations. The stream of diffusion scholarship over the past forty
years or so represents both similarities and differences, continuities
and discontinuities, and so must my three books, each published ap-
proximately a decade apart. By n0 means, however, d0 I seek only t0
synthesize the important findings from past research; I also strive
herein t0 criticize this work (including my own), and t0 Iay out direc-
tions for the future that are different from the recent past.
I have titled the present book Difl’usion oflnnovations t0 identify
it with the forty-year sequential tradition ofdiffusion studies marked
by my 1962 book of the same title. In any event, most people refer t0
4. Persuasion (and diffusion) researchers often give allegiance t0
the View that their dependent variable is to change attitudes rather
than overt behavior. Diffusion researchers have been more oriented t0
the dependent variable of adoption (a decision t0 use and implement a
new idea), than t0 actual implemenlation itself (or t0 studying the con-
sequences of innovation . , ,
Most past d1ffus10n studies have been based upon a linear model
of communication, defined as the process by which messages are
transferred from a source t0 a receiver. Such a one-way View ofhuman
cornrnunication describes certain types of communication; many
kinds of diffusion d0 indeed consist of one individual, such as a
change agent, informing a potential adopter about a new idea. But
other types of diffusion are more accurately described by a con-
vergence model, in which cornrnunication is defined as a process in
which the participants create and share information with one another
to reach a mutual understanding (Rogers and Kincaid, 1981, p. 63).
In the present book we seek to show the improved understanding
that can often be achieved by conceptualizing certain kinds of diffu-
sion in light ofthis convergence model. This emphasis on diffusion as
information exchange among participants in a cornrnunication proc-
ess is found particularly in our Chapter 8 on diffusion networks.
Conceptually, the present book makes use of the important con-
cepts of uncertainty and information. Uncertainty is the degree t0
which a nurnber of alternatives are perceived with respect to the occur-
rence of an event and the relative probabilities of these alternatives.
Uncertainty implies a lack ofpredictability ofthe future. lt motivates
an individual to seek information. Information is a difference in
matter-energy that affects uncertainty in a situation where a choice ex-
ists among a set of alternatives (Rogers and Kincaid, 1981, p. 64). The
concept of information is a favorite in the field of cornrnunication
research, and in fact the field really began to grow as an intellectual
enterprise once Claude Shannon and Warren Weaver (1949) had pro-
posed a theory of communication that was organized around the no-
tion ofinformation. , _ ‚
ne 1nd of uncertainty 1s generated by an innovation, defined as
an idea, practice, or object that is perceived as new by an individual or
another unit of adoption. An innovation presents an individual or an
organization with a new alternative or alternatives, with new rneans of
solving problems. But the probabilities of the new alternatives being
superior to previous practice are not exactly known by the individual
problem solvers. Thus, they are motivated to seek further information

xix

Preface
about the innovation in order to cope with the uncertainty that it
creates.
So the present book is cast in a theoretical framework involving
the concepts of information and uncertainty. Information about in-
novations is often sought from near-peers, especially information
abouttheirsubjectiveevaluationsoftheinnovation.Thisinformation
exchange about a new idea occurs through a convergence process in-
volving interpersonal networks. The diffusion ofinnovations, thus, is
essentially a social process in which subjectively perceived informa-
tion about a new idea is cornrnunicated.
The general field ofcommunication research has not been charac-
terized by much "weed pulling," that is, the criticism ofour scientific
activities so that the field's findings and fallacies can be publicly
understood (Siebold, 1979). Thus, it may be a healthy turn of events
that, beginning in the early 1970s, criticisrns of the diffusion frarne-
work began t0 appear. Ofcourse, it would be a mistake to becorne so
fond of weed pulling that the entire garden is destroyed (Reardon,
1981, p. 261).
Throughout the present book we seek t0 represent a healthily criti-
cal stance. We d0 notjust need rnore-of-the-same diffusion research.
The challenge for diffusion scholars ofthe future is t0 move beyond
the proven methods and models ofthe past, to recognize their short-
comings and limitations, and to broaden their conceptions ofthe dif-
fusion ofinnovations. We offer this book as one step toward this goal.},
  comment   = {Related to work Clean Energy Research would like me to do.  Is the 1983 version.  There's a newer version from 2010.  Is cited by Dong17resPVdeployFrcst as one review reference for diffusion of innovation models.

See also:  
Dong17resPVdeployFrcst which uses the Bose Diffusion equation
Gagnon17netMtrAdoptPV which uses diffusion equations, I think},
  file      = {:Rogers83diffusInnovBk.pdf:PDF},
  url       = {http://www.experience-capitalization.net/bitstream/handle/123456789/83/diffusion-of-innovations.pdf?sequence=1&isAllowed=y},
}

@Article{Bandourian02cmprParamIncDist,
  author    = {Bandourian, Ripsy and McDonald, James and Turley, Robert S},
  title     = {A comparison of parametric models of income distribution across countries and over time},
  year      = {2002},
  abstract  = {The five-parameter generalized beta distribution and ten of its special cases are considered as 
models fore the size distribution of income. The models are fit to income data for 23 countries 
and various years—a total of 82 data sets. Of the models considered, the Weibull, Dagum and 
generalized beta of the second kind are best fitting of the models with two, three and four 
parameters for 62 percent, 84 percent, and 96 percent of the data sets, respectively. Increasing 
inequality with respect to pre-tax income is observed in most of the countries considered. },
  comment   = {Weibull (2 param) and Dagum (3 param) are clearly the best fitting across countries and time, Dagum better than Weibull.  Generalized Beta (4 param) must be better.

Useful for filling out expected value tails of zip code based demo data (neeed to calc Gini coeff)},
  file      = {:Bandourian02cmprParamIncDist.pdf:PDF},
  publisher = {Luxembourg income study working paper},
  url       = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=324900},
}

@TechReport{Rogers11frcstingNorthAmTransmsn,
  author      = {Rogers, J. and Porter, K.},
  title       = {Central Wind Forecasting Programs in {North America} by Regional Transmission Organizations and Electric Utilities: Revised Edition},
  institution = {National Renewable Energy Laboratory (NREL), Golden, CO.},
  year        = {2011},
  abstract    = {The following table addresses the implementation of central wind power forecasting by electric utilities and regional transmission organizations in North America. The first part of the table focuses on electric utilities and regional transmission organizations that have central wind power forecasting in place; the second part focuses on electric utilities and regional transmission organizations that plan to adopt central wind power forecasting in 2010. Table entries are organized as follows:
* What the record peak demand is for the electric utility or regional transmission organization.
* What total generating capacity is available.
* What amount of wind capacity has been installed.
* What area is served by the electric utility or regional transmission organization
* When the wind power forecast was put into operation, or will be in operation.
* What wind power forecast vendor and model is used.
* What wind power forecast tools and techniques are used.
* What applications the wind power forecasts are used for.
* How wind power forecasts are paid for.
* What data is required, by wind turbine and wind project.
* How the wind power forecast is conducted.
* Whether a ramp forecast is prepared.
* How the wind power forecast performed.
Caution should be used in the interpretation of the performance data. The data encompasses different time periods, and can vary significantly based on location, season, weather regime, geographic distribution of the wind projects, potential wind curtailment, wind turbine availability, and other factors.},
  comment     = {Giant table of N. American forecasting programs Mentions alarms being sent out on a network, but not ramp alarms. But ramp forecasts are mentioned.},
  file        = {Rogers11frcstingNorthAmTransmsn.pdf:Rogers11frcstingNorthAmTransmsn.pdf:PDF},
  groups      = {Use, doReadWPV_2},
  owner       = {scot},
  timestamp   = {2011.05.02},
}

@Article{Liang14StochModelOptMicroGridSurv,
  author    = {Liang, Hao and Zhuang, Weihua},
  title     = {Stochastic Modeling and Optimization in a Microgrid: A Survey},
  journal   = {Energies},
  year      = {2014},
  volume    = {7},
  number    = {4},
  pages     = {2027--2050},
  abstract  = {The future smart grid is expected to be an interconnected network of small-scale
and self-contained microgrids, in addition to a large-scale electric power backbone. By
utilizing microsources, such as renewable energy sources and combined heat and power
plants, microgrids can supply electrical and heat loads in local areas in an economic
and environment friendly way. To better adopt the intermittent and weather-dependent
renewable power generation, energy storage devices, such as batteries, heat buffers and
plug-in electric vehicles (PEVs) with vehicle-to-grid systems can be integrated in microgrids.
However, significant technical challenges arise in the planning, operation and control of
microgrids, due to the randomness in renewable power generation, the buffering effect
of energy storage devices and the high mobility of PEVs. The two-way communication
functionalities of the future smart grid provide an opportunity to address these challenges,
by offering the communication links for microgrid status information collection. However,
how to utilize stochastic modeling and optimization tools for efficient, reliable and economic
planning, operation and control of microgrids remains an open issue. In this paper, we
investigate the key features of microgrids and provide a comprehensive literature survey on
the stochastic modeling and optimization tools for a microgrid. Future research directions
are also identified.
Keywords: microgrid; smart grid; stochastic modeling; stochastic optimization},
  comment   = {Uses stochastic differential equations, I think describing gas prices, to do optimization.},
  file      = {Liang14StochModelOptMicroGridSurv.pdf:Liang14StochModelOptMicroGridSurv.pdf:PDF},
  owner     = {sotterson},
  publisher = {Multidisciplinary Digital Publishing Institute},
  timestamp = {2014.11.13},
  url       = {http://www.mdpi.com/1996-1073/7/4/2027/pdf},
}

@Article{Sensfuss08meritOrderDE,
  author   = {Frank Sensfu\ss and Mario Ragwitz and Massimo Genoese},
  title    = {The merit-order effect: A detailed analysis of the price effect of renewable electricity generation on spot market prices in Germany},
  journal  = {Energy Policy},
  year     = {2008},
  volume   = {36},
  number   = {8},
  pages    = {3086 - 3094},
  issn     = {0301-4215},
  abstract = {The German feed-in support of electricity generation from renewable energy sources has led to high growth rates of the supported technologies. Critics state that the costs for consumers are too high. An important aspect to be considered in the discussion is the price effect created by renewable electricity generation. This paper seeks to analyse the impact of privileged renewable electricity generation on the electricity market in Germany. The central aspect to be analysed is the impact of renewable electricity generation on spot market prices. The results generated by an agent-based simulation platform indicate that the financial volume of the price reduction is considerable. In the short run, this gives rise to a distributional effect which creates savings for the demand side by reducing generator profits. In the case of the year 2006, the volume of the merit-order effect exceeds the volume of the net support payments for renewable electricity generation which have to be paid by consumers. },
  comment  = {It's old but it has a simple explanation of the German day ahead market merit order effect, in which renewables with zero marginal cost can drive prices below zero (which increasingly happens these days).  This occurs even for perfect renewable forecasts,},
  doi      = {http://dx.doi.org/10.1016/j.enpol.2008.03.035},
  file     = {Sensfuss08meritOrderDE.pdf:Sensfuss08meritOrderDE.pdf:PDF},
  keywords = {Renewable energy, Electricity market, Agent-based simulation },
  url      = {http://www.sciencedirect.com/science/article/pii/S0301421508001717},
}

@Book{Rasmussen06GaussProcBook,
  title     = {Gaussian Processes for Machine Learning},
  publisher = {The MIT Press},
  year      = {2006},
  author    = {Carl Edward Rasmussen and Christopher K. I. Williams},
  isbn      = {0-262-18253-X},
  abstract  = {The goal of building systems that can adapt to their environments and learn
from their experience has attracted researchers from many fields, including com-
puter science, engineering, mathematics, physics, neuroscience, and cognitive
science. Out of this research has come a wide variety of learning techniques that
have the potential to transform many scientific and industrial fields. Recently,
several research communities have converged on a common set of issues sur-
rounding supervised, unsupervised, and reinforcement learning problems. The
MIT Press series on Adaptive Computation and Machine Learning seeks to
unify the many diverse strands of machine learning research and to foster high
quality research and innovative applications.
One of the most active directions in machine learning has been the de-
velopment of practical Bayesian methods for challenging learning problems.
Gaussian Processes for Machine Learning presents one of the most important
Bayesian machine learning approaches based on a particularly effective method
for placing a prior distribution over the space of functions. Carl Edward Ras-
mussen and Chris Williams are two of the pioneers in this area, and their book
describes the mathematical foundations and practical application of Gaussian
processes in regression and classification tasks. They also show how Gaussian
processes can be interpreted as a Bayesian version of the well-known support
vector machine methods. Students and researchers who study this book will be
able to apply Gaussian process methods in creative ways to solve a wide range
of problems in science and engineering.},
  comment   = {Book on Gaussian Processes.  I also bought the physical book.

P. 200 has the formulas for conditional and marginal multivariate normal distributions (Wang06imageProcE161crsNts has the derivation)},
  date      = {2006-01-11},
  ean       = {9780262182539},
  file      = {Rasmussen06GaussProcBook.pdf:Rasmussen06GaussProcBook.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.03.16},
  url       = {http://www.ebook.de/de/product/5192092/rasmussen_gaussian_processes_for_machine_learning.html},
}

@InProceedings{Shi15confLSTMprecipNow,
  author    = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-Kin and Woo, Wang-chun},
  title     = {Convolutional LSTM network: A machine learning approach for precipitation nowcasting},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2015},
  pages     = {802--810},
  abstract  = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
  comment   = {Uses and LSTM RNN to nowcast rain using satellite measurements.  Say benefit of CNN is that it's not fully connected, not that it's convoultional (I think).

52 cites on March 2017

Use for regime detection, front tracking, ...},
  file      = {shi15confLSTMprecipNow.pdf:shi15confLSTMprecipNow.pdf:PDF},
  url       = {http://papers.nips.cc/paper/5955-convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting},
}

@InProceedings{Suzuki10suffDimRedSqMutInf,
  author    = {Suzuki, T. and Sugiyama, M.},
  title     = {Sufficient dimension reduction via squared-loss mutual information estimation},
  booktitle = {Artificial Intelligence and Statistics (AISTAT)},
  year      = {2010},
  pages     = {804--811},
  abstract  = {The goal of sufficient dimension reduction in supervised learning is to find the lowdimensional subspace of input features that is 'sufficient' for predicting output values. In this paper, we propose a novel sufficient dimension reduction method using a squared loss variant of mutual information as a dependency measure. We utilize an analytic approximator of squared-loss mutual information based on density ratio estimation, which is shown to possess suitable convergence properties. We then develop a natural gradient algorithm for sufficient subspace search. Numerical experiments show that the proposed method compares favorably with existing dimension reduction approaches.},
  comment   = {Dimension reduction preserving max squared mutual information between input and output; has Matlab
* dim reduction limited to linear; best output dim assumed known (why no MI stopping criteria?)
-- IDEA: since there's already a kernel basis function in the middle of the algorithm, could that kernel somehow be used to do non-linear dim reduction?
* something like PLS, but has squared mutual information criteria (want output cond. indep of input, when given the dim reduced version)

* squared MI: f-divergence between joint dist. and marg. dist product (independence, w/ squared loss; normal MI is log loss
* several complicated approximations I didn't follow
* kernel estimate of density ratios (a kernel basis function)
-- makes problem tractable
-- different kernels for different purposes: continuous regression, classification, "structured" output (what is that?)
-- kernel and regularization params chosen via cross-validation (looking for max MI) Tests
* regression on artificial data:
-- doesn't say what the dims were, either input or output
-- only 100 samples (keeps the kernel computable?)
-- clearly better than the other methods
* classification
-- some real data benchmarks, I think
-- only 200 training samples
-- maybe better than other techniques, although no so much on the "pima" data (why?

Feature selection, preserving regression value via new mutual info estimator
* usable for high dim featsel for featsel?
* better than lasso, shrinkage, etc?
* matlab runs kinda slow, though
* examples are for classification, not continuous valued regression

* matlab here: http://sugiyama-www.cs.titech.ac.jp/~sugi/software/LSDR/index.html

* uses Suzuki09mutInfoEstRatio, I think},
  file      = {Suzuki10suffDimRedSqMutInf.pdf:Suzuki10suffDimRedSqMutInf.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  timestamp = {2010.08.31},
  url       = {http://jmlr.csail.mit.edu/proceedings/papers/v9/suzuki10a/suzuki10a.pdf},
}

@InProceedings{Pardoe10BoostRgrsnXfer,
  author    = {Pardoe, David and Stone, Peter},
  title     = {Boosting for regression transfer},
  booktitle = {Proceedings of the 27\textsuperscript{th} international conference on Machine learning (ICML-10)},
  year      = {2010},
  pages     = {863--870},
  abstract  = {The goal of transfer learning is to improve
the learning of a new target concept given
knowledge of related source concept(s). We
introduce the first boosting-based algorithms
for transfer learning that apply to regression
tasks. First, we describe two existing classification
transfer algorithms, ExpBoost and
TrAdaBoost, and show how they can be modified
for regression. We then introduce extensions
of these algorithms that improve performance
significantly on controlled experiments
in a wide range of test domains.},
  comment   = {Could use for adaptive point forecasting, maybe especially when the NWP model changes. Also, could be a way to make Fenske11idRiskBoostAddQR. Another way: Pan08xfrLrnDimRed},
  file      = {Pardoe10BoostRgrsnXfer.pdf:Pardoe10BoostRgrsnXfer.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.07.24},
  url       = {http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_PardoeS10.pdf},
}

@Book{Ullrich09frcstHdgMkt,
  title     = {Forecasting and Hedging in the Foreign Exchange Markets},
  publisher = {Springer},
  year      = {2009},
  author    = {Ullrich, Christian},
  volume    = {623},
  abstract  = {The growing complexity of many real world problems is one of the biggest challenges of our time. The area of international finance is one prominent example where decision making is often fraud to mistakes, and tasks such as forecasting, trading and hedging exchange rates seem to be too difficult to expect correct or at least adequate decisions. From the high complexity of the foreign exchange market and related decision problems, the author derives the necessity to use tools from Machine Learning and Artificial Intelligence, e.g. Support Vector Machines, and to combine such methods with sophisticated financial modelling techniques. The suitability of this combination of ideas is demonstrated by an empirical study and by simulation.

Content Level ? Professional/practitioner

Keywords ? Artificial Intelligence - Computational Finance - Decision-making - Foreign Exchange Market Complexity - Machine Learning

Related subjects ? Artificial Intelligence - Business Information Systems - Finance & Banking - Financial Economics - Quantitative Finance},
  comment   = {Says that, in high dimensions, p-Gaussians "lead to better generalization than Gaussians" also more computation b/c of fractional kernel
(p. 103, as viewed on Google books)

He calls them "exotic"!

p-Gaussians: Francois05locKernHiDim},
  owner     = {sotterson},
  timestamp = {2014.03.28},
  url       = {http://www.springer.com/economics/financial+economics/book/978-3-642-00494-0?token=gbgen&wt_mc=Google-_-Book+Search-_-Springer-_-EN&otherVersion=978-3-642-00495-7},
}

@Article{Cristaldi12simplePVmodel,
  author   = {L. Cristaldi and M. Faifer and M. Rossi and F. Ponci},
  title    = {A Simple Photovoltaic Panel Model: Characterization Procedure and Evaluation of the Role of Environmental Measurements},
  journal  = {IEEE Transactions on Instrumentation and Measurement},
  year     = {2012},
  volume   = {61},
  number   = {10},
  pages    = {2632-2641},
  month    = {Oct},
  issn     = {0018-9456},
  abstract = {The growing number of installed photovoltaic (PV) plants is making the simulation of their behavior and their effects on the power network more and more relevant. In this context, an accurate yet simple model of the panels is beneficial for evaluating the power production as well as the system efficiency in off-line and on-line analysis. In the preliminary design stages of large plants, this kind of model can support design and decision making, allowing for the simulation of the entire plant and the testing of various power architectures. During the operation, such models may support monitoring, diagnostic, and control functions. In this paper, a model of PV panel, suitable for the aforementioned applications, is presented together with a simple procedure for the identification of its parameters. The critical issues related to the measurement and the estimation of the required environmental quantities are analyzed together with their main metrological requirements. Finally, the experimental validation of the proposed model and algorithms is presented using as a case study the estimation of the energy production of a domestic solar plant.},
  comment  = {Simple model for PV outout, based on electrical circuit and physics but  looks like it's easy to estimate the paramters of from data.},
  doi      = {10.1109/TIM.2012.2199196},
  file     = {Cristaldi12simplePVmodel.pdf:Cristaldi12simplePVmodel.pdf:PDF},
  keywords = {decision making;photovoltaic cells;solar cells;PV panel;characterization evaluation;characterization procedure;decision making;domestic solar plant;energy production estimation;environmental measurements;environmental quantities;installed photovoltaic plants;metrological requirements;off-line analysis;on-line analysis;photovoltaic panel model;power architectures;power production;Estimation;Integrated circuit modeling;Mathematical model;Meteorology;Resistance;Solar radiation;Temperature measurement;Maintenance;PV power systems;modeling;photovoltaic (PV) cell measurements;solar energy},
}

@InProceedings{Liu12tsoLargeWind,
  author       = {Liu, Zongyu and van der Sluis, Lou and Winter, Wilhelm and Paeschke, Helmut and Becker, Raik and Weber, Christoph and Eickmann, Jonas and Schroeders, Christian and Oldewurtel, Frauke and Roald, Line and others},
  title        = {Challenges, experiences and possible solutions in transmission system operation with large wind integration},
  booktitle    = {11th International Workshop on Large-Scale Integration of Wind Power into Power Systems},
  year         = {2012},
  organization = {11th International Workshop on Large-Scale Integration of Wind Power into Power Systems},
  abstract     = {The growing share of electricity generated from 
intermittent renewable energy sources as well as increasing 
market-based cross-border flows and related physical flows 
are leading to rising uncertainties in transmission network 
operation. This paper presents the current and possible future 
operational challenges from TSOs’ experiences, and provides 
a comprehensive overview of expected features that a toolbox 
should contain for the stable operation of the transmission 
network. In addition, it outlines the research vision of the 
UMBRELLA project, namely better use of renewable energy 
forecasts, optimization of transmission system operation, and 
risk-based security assessment, as the possible solution to 
tackle the aforementioned challenges. In the end, a brief 
introduction to the UMBRELLA project has been 
incorporated.  
Keywords: System operation, wind integration, renewable 
energy forecasts, optimization of transmission system operation, 
risk-based security assessment, TSO experiences},
  file         = {:Liu12tsoLargeWind.pdf:PDF},
}

@Article{Winters60exponMvAvgSalesFrcst,
  author    = {Winters, Peter R},
  title     = {Forecasting sales by exponentially weighted moving averages},
  journal   = {Management Science},
  year      = {1960},
  volume    = {6},
  number    = {3},
  pages     = {324--342},
  abstract  = {The growing use of computers for mechanized inventory control and production planning has brought with it the need for explicit forecasts of sales and usage for individual products and materials. These forecasts must be made on a routine basis for thousands of products, so that they must be made quickly, and, both in terms of computing time and information storage, cheaply; they should be responsive to changing conditions. The paper presents a method of forecasting sales which has these desirable characteristics, and which in terms of ability to forecast compares favorably with other, more traditional methods. Several models of the exponential forecasting system are presented, along with several examples of application.},
  comment   = {I think this is one of the original papers for the "Holt-Winters" forecast correction algorithms used by Trygvii at DTU to remove forecast bias (and scale?) errors (e.g. in Jonsson13spotFrcstWind).

See also: Meyer02naiveFrcstMethR
See also: Taylor07frcstSprMktPriceQR for a comparison w/ exponential weighting},
  doi       = {10.1287/mnsc.6.3.324},
  file      = {Winters60exponMvAvgSalesFrcst.pdf:Winters60exponMvAvgSalesFrcst.pdf:PDF},
  owner     = {sotterson},
  publisher = {INFORMS},
  timestamp = {2015.03.10},
}

@PhdThesis{Kraskov04syncIndepMeasMIphd,
  author      = {Kraskov, Alexander},
  title       = {Synchronization and Interdependence Measures and their Applications to the Electroencephalogram of Epilepsy Patients and Clustering of Data},
  year        = {2004},
  abstract    = {The history of the very interesting phenomenon of synchronization started in the seven-
teenth century when the famous Dutch researcher Christiaan Huygens observed perfect
agreement between the oscillating motions of two clocks hanging from a common sup-
port [41, 89]. The word ?synchronization? came to many contemporary languages from
ancient Greece. The etymology of this word is very simple, it consists of two parts ???
(syn = common) and ??o?o? (chronos = time). In a direct translation the verb ?synchro-
nize? means ?to happen at the same time? or ?to agree in time?. This translation can be
taken as a first approximation to the definition of synchronization because it contains one
of the main features, namely coincidence in time, i.e., synchronous motion. One can argue,
that this condition is too weak, especially, if coincidence happens very rarely. In our opin-
ion the repeated coincidence over long time, i.e., lasting synchronous motion, is usually a
consequence of synchronization. Therefore, in this thesis we will use synchronization and
synchronous motion as synonymous.
Later, in the beginning of the twentieth century, synchronization phenomena were studied
by W.H. Eccles and J.H. Vincent in the context of electrical and radio engineering de-
velopment. In their experiments the adjustment of the frequencies of two coupled triode
generators with initially different frequencies was demonstrated. A few years later E. Ap-
pleton [11] and B. van der Pol extended the experiments of Eccles and Vincent and also
undertook theoretical investigations of synchronization phenomena. Van der Pol derived
his famous equation, the first example of a non-linear self-oscillating system [124]. More-
over, van der Pol together with van der Mark proposed an electrical model of the human
heart consisting of three coupled relaxation oscillators [125].
A new stage of synchronization studies started some decades after the discovery of deter-
ministic chaos [71]. In the early 1980s, the notion of synchronization was extended to the
case of interacting chaotic oscillators [31, 86, 4, 84]. Deterministic chaos is characterized
by sensitivity to initial conditions, i.e., trajectories starting from very close points diverge
exponentially. Therefore, synchronization between chaotic oscillators was not expected.
However, extensive experimental investigations have proven its existence. Prominent ex-
amples include electronics [87, 39, 83], laser dynamics [28, 105, 119], solid state physics
[85], plasma physics [97], communication [20, 45] and chaos control [91, 107].
The simplest form of synchronization occurs if the states of systems exactly coincide in
time. This type of synchronization is usually referred to as identical synchronization. It can
be observed if the coupling strength between identical systems is high enough [31, 86]. In
this case coincidence means that the two states are identical. One can easily extend the no-
tion of coincidence to a more complicated functional relation. This leads us to the concept
of generalized synchronization, which was introduced for unidirectionally coupled systems
in Refs. [4, 106]. Quite often the most reliable information about interacting systems is
contained in the phases of each system. An entrainment of these phases is fundamental for
phase synchronization. For chaotic oscillators it was first described in Refs. [101, 88, 80].
The variety of synchronization concepts spurred the development of many different ap-
proaches aiming at a quantification of the degree of synchronization between two systems
or rather between two time series measured from the respective systems. Mutual infor-
mation is one of them [37, 21]. It is zero if and only if two random variables are strictly
independent. This distinctive feature singles out mutual information among other mea-
sures. Different estimators for mutual information were proposed in the literature but all of
them have significant systematical errors. This problem has motivated us to develop two
new families of estimators with a minimal bias. These estimators will be introduced in the
first original part of this thesis.
Topological approaches to quantify generalized synchronization include the method of mu-
tual false nearest neighbors [106] and the index based on non-linear mutual predictions
[108] as well as more recent measures like the non-linear interdependencies [13] and syn-
chronization likelihood [116]. Different ways to quantify phase synchronization have been
proposed in [120, 72]. In this context, the notion of a phase is very important and different
approaches for its extraction from time series have been developed. Two of the most im-
portant techniques use the Hilbert transform [101] and the wavelet transform [57]. In the
literature a theoretical comparison of these two methods was still missing. This compari-
son along with an extended discussion about the ambiguity of phase definition constitutes
the second original part of this thesis.
A challenging application for measures of synchronization is the study of neuronal dynam-
ics, since synchronization phenomena have been increasingly recognized as a key feature
for establishing the communication between different regions of the brain [126, 29, 127].
On the other hand synchronization plays an important role for pathological processes such
as Parkinson?s disease or epilepsy. A unique window to neuronal dynamics is given by
electroencephalographic (EEG) recordings from epilepsy patients undergoing pre-surgical
diagnostics [69]. To yield sufficient information for an unequivocal localization of the
seizure-generating structure (epileptic focus) in the brain, sometimes multichannel record-
ings using intracranial monitoring techniques are acquired. In this case the EEG is recorded
directly from the surface of the brain and from specific structures within the brain [26].
The investigation of these recordings by means of linear and non-linear time series analy-
sis techniques can help to further understand the spatio-temporal dynamics of the epileptic
brain (see, e.g., [62]). In particular, synchronization and de-synchronization phenomena
play an important role in the epileptic process. This motivated us to carry out a com-
prehensive comparison of the different measures of synchronization with respect to their
capability to detect the side of the epileptic focus. To test the degree to which the ob-
tained results are specifically related to synchronization phenomena we applied a bivariate
surrogate data analysis. This study constitutes the third original part of this thesis.
All measures of phase synchronization and nonlinear interdependence test for similarities
between two systems using only one dimensional times series, whereas mutual information
can be applied to objects of any dimension. This feature puts mutual information between
solely bivariate approaches and multivariate approaches, like e.g., independent component
analysis (ICA) [42]. In application to the analysis of multichannel EEG recordings multi-
variate methods can be used to derive different kinds of spatial and temporal information.
For example, grouping the different channels for a more precise localization of the epilep-
tic focus or classification of the intervals preceding an epileptic seizure and the intervals
far away from any seizure activity can be of great value in epilepsy research. An attempt
to retrieve this type of information can be undertaken with the help of clustering methods
[27, 43]. In the last original part of this thesis we propose a new method for a hierarchi-
cal clustering of data based on the grouping property of mutual information. We show
two examples of its application to data from genetics (mitochondrial DNA sequences of
mammals) and cardiology (electrical activity of the heart of a pregnant woman).
This thesis is organized as follows: First, in Chapter 2 an introduction to synchronization
and its different notions is given. In Chapter 3 different approaches to quantify synchro-
nization phenomena are presented. Along with traditional methods such as linear cross-
correlation and coherence functions (Sec. 3.1), nonlinear approaches with information the-
oretical background, namely mutual information (Sec. 3.2) and transfer entropy (Sec. 3.3)
and methods developed in the framework of nonlinear time series analysis (Secs. 3.4 and
3.5) are introduced. In Sec. 3.2.2 new estimators for mutual information and transfer en-
tropy are presented. Sections 3.4.3 and 3.4.4 contain a comparative study of different phase
extraction methods. In Chapter 4 all measures of synchronization and interdependence in-
troduced in Chapter 3 are applied to electroencephalographic time series measured from
the brain of epilepsy patients (Section 4.1). In particular, the localization of the epileptic
focus is addressed in Section 4.2. The bivariate surrogate data techniques are described
and applied in Sec. 4.2.2. In Chapter 5 a new algorithm for hierarchical clustering based
on mutual information is presented. In Section 5.1 the algorithm is formulated and in Sec-
tion 5.3 two its applications are discussed. Finally, the conclusions of this thesis are drawn
in Chapter 6.},
  comment     = {* Section 3.2.2 explains in more detail the KNN mutual information estimator in Kraskov04EstMutInfKNN.
* Perhaps Victor02binlessInfo is even better about the KNN entropy estimator derivation.


This is Kraskov's PhD thesiz},
  file        = {Kraskov04syncIndepMeasMIphd.pdf:Kraskov04syncIndepMeasMIphd.pdf:PDF},
  institution = {Universit{\"a}t Wuppertal, Fakult{\"a}t f{\"u}r Mathematik und Naturwissenschaften{\guillemotright} Physik{\guillemotright} Dissertationen},
  url         = {http://elpub.bib.uni-wuppertal.de/servlets/DerivateServlet/Derivate-976/dc04f1.pdf},
}

@TechReport{Hoyt17usApartmentDemandFrcst,
  author      = {Hoyt},
  title       = {U.S. Apartment Demand – A Forward Look},
  institution = {Hoyt Advisory Services, Dinn Focused Marketing, Inc. and Whitegate Real Estate Advisors, LLC},
  year        = {2017},
  abstract    = {The housing bubble fallout of 2007-2010 resulted in a paradigm shift in the U.S. among many 
households.  Disillusioned by the bursting of the house price bubble that destroyed equity, many 
former home owners continue to rent today.  Younger households, seeking more mobility and often 
saddled with student loans, postpone home ownership or choose to have the flexibility of renting.  
Demographic shifts also affect home ownership and the result has been a declining home ownership 
rate and corresponding increase in the percentage of households that rent.  Some of this shift came 
about in the same housing units, as owned units became part of the rental inventory and today some 
one-third of all rental units are single-family units.  },
  comment     = {Forecasts US rental housing demand out to 2030.  Methodology is described in Appendix 5.  May be useful for DER adoption forecasts.

Also includes population growth forecast, age forecasts, and economic forecasts.

Lots of maps w/ per state, even per-city data.

mentions that data was aggregated or disaggregated across space but doesn't say much about, at least on p. 26

Private data more accurate than census surveys; sometimes the census data is better, so they combine them.  p. 26.},
  file        = {:Hoyt17usApartmentDemandFrcst.pdf:PDF},
}

@Article{Phan14stochOptPowFlow2stage,
  author    = {Phan, Dzung and Ghosh, Soumyadip},
  title     = {Two-stage stochastic optimization for optimal power flow under renewable generation uncertainty},
  year      = {2014},
  volume    = {24},
  number    = {1},
  pages     = {2},
  url       = {http://dl.acm.org/citation.cfm?id=2553084},
  abstract  = {The idea that Sebastian Wildenhues would like to collaborate on},
  file      = {Phan14stochOptPowFlow2stage.pdf:Phan14stochOptPowFlow2stage.pdf:PDF},
  journal   = {ACM Transactions on Modeling and Computer Simulation (TOMACS)},
  owner     = {sotterson},
  publisher = {ACM},
  timestamp = {2014.07.09},
}

@Conference{Wang09windCommitDisp,
  author    = {Wang, J. and Botterud, A. and Miranda, V. and Monteiro, C. and Sheble,G.},
  title     = {Impact of wind power forecasting on unit commitment and dispatch},
  booktitle = {International Workshop on Large-Scale Integration of Wind Power into Power Systems},
  year      = {2009},
  abstract  = {The impact of wind power forecasting on unit commitment and dispatch is investigated in this paper. We present two unit commitment methods to address the variability and intermittency of wind power. The uncertainty in wind power forecasting is captured by a number of scenarios in the stochastic unit commitment approach, while a point forecast of wind power output is used in the deterministic alternative. Several cases with different wind power forecasts and reserve requirements are simulated. The preliminary results show that the quality of wind power forecasting has a great impact on unit commitment and dispatch. The stochastic method shows its value in terms of relatively lower dispatch cost. However, the dispatch results are also sensitive to the level of reserve requirement. Our results so far indicate that a deterministic method combined with an increased reserve requirement can produce results that are comparable to the stochastic case. Index Terms?Wind power, forecasting, electricity markets, unit commitment, dispatch, stochastic optimization.},
  comment   = {Wind forecast errors impact unit commitment and dispatch. Shown w/ maybe too-simple simulation I'm not sure if the simulation was detailed enough to draw conclusions about stochastic vs. deterministic algorithms (they seem equivalent, here) but it's clear that wind power forecasts errors are expensive. Ortega-Vazquez10windOpCost might be a better reference.
* compare stochastic and deterministic commitment algorithms
* find that wind power forecast errors greatly increase cost
* cost doesn't include emissions, so it sometimes looks like burning a lot of reserve fuel is worth it to avoid load curtailment costs.
* wind curtailment doesn't happen, though it's allowed (no transmission constraints in sim, etc.)
* May want to use the WILMAR model for simulations w/ Pierre's stuff
* spinning reserves are a fixed percentage, not probabilistically adjusted (and there's no reserve market)
* startup costs simpler than other methods
* At dispatch it becomes always deterministic but how?
 -- Plant ramping constraints still mean that you must think into the future
 -- instantaneous, realized wide. power isn't enough unless assume persistence over plant ramp time
* future loads assumed to be known (no forecast)
* argues that stochastic scenarios s/ already incorporate reserve uncertainty, but..
 -- they still include fixed reserves in their stochastic sims
 -- there's a study [10], which shows stoch. is better than determ when include fixed reserve (but is it w/o fixed reserve?)
* thermal power plant constraints discussed in refs 12 and 13
* stoch and determ cost about the same, but
 -- determ is TEST ON TRAIN!
 -- Stochastic allocates more reserve units for same fixed reserve pct (S1 vs. D2), p. 6
 -- for same 10\% fixed reseve, stoch has much less load curtail cost than determ (D2/S1 in table 6)},
  file      = {Wang09windCommitDisp.pdf:Wang09windCommitDisp.pdf:PDF},
  groups    = {Read, Ensemble, Use, doReadWPV_2},
  owner     = {scot},
  timestamp = {2011.04.18},
}

@MastersThesis{Martin17intrdyMktThesis,
  author   = {Henry Martin},
  title    = {A Limit Order Book Model for the German Intraday Electricity Market},
  school   = {Technical University Munich},
  year     = {2017},
  abstract = {The importance of the German intraday electricity market increased significantly
over the past years, however there is limited research about intraday market models
in Germany. In this thesis we examine the possibility of applying limit order book
market models designed for stock markets to the German intraday electricity market.
We find that the general approach may be suitable for modeling characteristics like
liquidity, but that the specific models are a poor fit. Therefore we propose new models
that are tailored to our application.

We proceed by preparing the EPEX SPOT M7 order flow data, a data set containing
all orders submitted to the German intraday electricity market. We present prepro-
cessing steps and decision rules necessary to infer the identity of individual orders,
information that is not directly available. Based on these rules, we implement a
historical market simulator that reconstructs the complete state of the market for
every point in time. The historic market simulator itself can be used as a simple,
limit order book based market model. It allows modeling the influence of trade size
and trading time on the available market price. Thus, this model can, when limited
to small trade sizes, represent important dimensions of liquidity. We find that the
historical EPEX SPOT order flow simulator can reproduce independent transaction
records with a near-exact accuracy.

The order flow is then used to create a stochastic market model. First, we introduce a
horizon dependent power law model for the order submission intensity, which provides
a very good fit to empirical data. We show that trading size is strongly clustered
around integers, and propose a mixture model that can reproduce the preference of
traders for rounded values. Order cancellation arrival intensity is indirectly modeled
by a distribution of order lifetimes. We show that order lifetime does not follow
the exponential distribution used for stock markets so we instead model it using a
log-logistic model.

Finally, we simulate the market model for the last two hours of trading for one month
of unseen test data. For most of the variables, the simulated outcome matches the
time independent distributions of the empirical data. The fit is especially close for
the order arrival rate, order prices, the mid-price, the price at the best ask and the
price of the best bid.},
  comment  = {Henry's Masters Thesis},
}

@Article{Olsson10modelBlncWindSDE,
  author    = {Magnus Olsson and Magnus Perninge and Lennart S{\"o}der},
  title     = {Modeling real-time balancing power demands in wind power systems using stochastic differential equations},
  journal   = {Electric Power Systems Research},
  year      = {2010},
  volume    = {80},
  number    = {8},
  pages     = {966--974},
  issn      = {0378-7796},
  abstract  = {The inclusion of wind power into power systems has a significant impact on the demand for real-time balancing power due to the stochastic nature of wind power production. The overall aim of this paper is to present probabilistic models of the impact of large-scale integration of wind power on the continuous demand in \{MW\} for real-time balancing power. This is important not only for system operators, but also for producers and consumers since they in most systems through various market solutions provide balancing power. Since there can occur situations where the wind power variations cancel out other types of deviations in the system, models on an hourly basis are not sufficient. Therefore the developed model is in continuous time and is based on stochastic differential equations (SDE). The model can be used within an analytical framework or in Monte Carlo simulations.

Keywords

 Stochastic differential equations;
 Real-time balancing;
 Wind power},
  comment   = {pdf ordered from subito

11 google scholar hits to look up:
http://scholar.google.com/scholar?cites=18411538589704765251&as_sdt=2005&sciodt=0,5&hl=en},
  doi       = {10.1016/j.epsr.2010.01.004},
  file      = {Olsson10modelBlncWindSDE.pdf:Olsson10modelBlncWindSDE.pdf:PDF},
  keywords  = {Stochastic differential equations},
  owner     = {sotterson},
  timestamp = {2014.11.13},
  url       = {http://www.sciencedirect.com/science/article/pii/S0378779610000246},
}

@Article{Girard12spatTempErrProp,
  author    = {Girard, Robin and Allard, Denis},
  title     = {Spatio-temporal propagation of wind power prediction errors},
  journal   = {Wind Energy},
  year      = {2012},
  month     = aug,
  abstract  = {The increasing concentration of wind farms in some parts of the world calls for a new descriptive framework of power fluctuation that can summarize spatio-temporal characteristics of the wind power production process. In the mean time, this high number of measurement devices has great potential for informing or alerting about upcoming front or phase errors. In this paper, we shed light on the spatio-temporal characteristics of wind power forecast errors. We justify and use two strategies for obtaining the main direction and the speed of propagation. The first of these relies on an analysis of the local maxima of the correlation structure, and the second builds upon a planar wave modeling. The connection is made with meteorological parameters. Our analysis is presented so as to be easily reproducible in other cases.},
  comment   = {Spatio-temporal correlation of large number of observations + meteorological params + 2 strategies yield the main direction and propagation of wind speed f forecast errors},
  doi       = {10.1002/we.1527},
  file      = {:Girard12spatTempErrProp.pdf:PDF},
  groups    = {ErrDistProps, doReadNonWPV_1},
  publisher = {Wiley Blackwell (John Wiley \& Sons)},
}

@InProceedings{Bessa10rsrvCompProbDetFrcst,
  author    = {Bessa, Ricardo J. and Matos, Manuel A.},
  title     = {Comparison of probabilistic and deterministic approaches for setting operating reserve in systems with high penetration of wind power},
  booktitle = {Power Generation, Transmission, Distribution and Energy Conversion (MedPower 2010), 7\textsuperscript{th} Mediterranean Conference and Exhibition on},
  year      = {2010},
  pages     = {1--9},
  month     = nov,
  abstract  = {The increasing levels of wind power penetration motivated a revisitation of methods for setting operating reserve requirements for the next and current day. System Operators (SO) are now moving from deterministic intro probabilistic approaches, and including wind power forecasts in their decision-making problems. In this manuscript, a probabilistic approach that evaluates the consequences of setting each possible reserve level through a set of risk indices is compared with frequently used deterministic rules and a probabilistic rule where wind power uncertainty is described by a Gaussian distribution. The comparison is performed over a period of five months for a realistic power system, using real load and wind power generation data. Results highlight the limitations of deterministic rules, challenge the Gaussian assumption and illustrate the usefulness of risk indices derived from the probabilistic forecast and using a full probabilistic methodology.},
  comment   = {Markus Speckman' papers on German Primary/Secondary/Tertiary res. rqts.},
  doi       = {10.1049/cp.2010.0867},
  file      = {Bessa10rsrvCompProbDetFrcst.pdf:Bessa10rsrvCompProbDetFrcst.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2012.02.09},
}

@Article{Papaefthymiou09copulaPowUncert,
  author    = {Papaefthymiou, G. and Kurowicka, D.},
  title     = {Using Copulas for Modeling Stochastic Dependence in Power System Uncertainty Analysis},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2009},
  volume    = {24},
  number    = {1},
  pages     = {40--49},
  month     = feb,
  issn      = {0885-8950},
  abstract  = {The increasing penetration of renewable generation in power systems necessitates the modeling of this stochastic system infeed in operation and planning studies. The system analysis leads to multivariate uncertainty analysis problems, involving non-Normal correlated random variables. In this context, the modeling of stochastic dependence is paramount for obtaining accurate results; it corresponds to the concurrent behavior of the random variables, having a major impact to the aggregate uncertainty (in problems where the random variables correspond to spatially spread stochastic infeeds) or their evolution in time (in problems where the random variables correspond to infeeds over specific time-periods). In order to investigate, measure and model stochastic dependence, one should transform all different random variables to a common domain, the rank/uniform domain, by applying the cumulative distribution function transformation. In this domain, special functions, copulae, can be used for modeling dependence. In this contribution the basic theory concerning the use of these functions for dependence modeling is presented and focus is given on a basic function, the Normal copula. The case study shows the application of the technique for the study of the large-scale integration of wind power in the Netherlands.},
  comment   = {Effect of new wind power on Netherlands net load distribution. Uses copulas.
* they use normal copula:
-- they explain how this misses extreme event tail correlation.
-- but the still use it
* ensuring positive-definiteness
-- hypersphere decomp.
-- spectral decomp
-- results about the same for the above
-- didn't try Cholesky (as in Morales10)
-- would this have worked?
* analytic relationship between linear and rank correlation (for bivariates)
* compare with Lu14pairCopulaSpatWindPow
* used in Wang16evalStochMethScen (I think but year is off by one)

Data
* wind speed measurments
  - 10m --> 80m
  - generic power curve (so way jaggier than a farm would be)
* Load
  - growth extrapolated to 2020
  - Somehow, it's a time series b/c they calculated correlation with wind power

Copula Marginals
* not from QR forecat but just empirical distribution
* Papaefthymiou08spatioTempModel does QR instead
  - needed there b/c that's a forecast, while this is just power production?


Interesting
* problem w/ modeling a wind park w/ a single turbine
* found bad wind correlations:
-- onshore not correlated with demand
-- wind resources correlated across the country
* include this in Spinning reserves literature review.},
  doi       = {10.1109/TPWRS.2008.2004728},
  file      = {Papaefthymiou09copulaPowUncert.pdf:Papaefthymiou09copulaPowUncert.pdf:PDF},
  keywords  = {cumulative distribution function transformation;large-scale integration;multivariate uncertainty analysis;power generation uncertainty;power system operation;power system planning;power system renewable generation;stochastic system modeling;wind power;large scale integration;power generation planning;stochastic processes;wind power;wind power plants;},
  owner     = {scot},
  timestamp = {2010.11.24},
}

@InProceedings{Moller13windFrcstSDE,
  author    = {Jan Kloppenborg M{\o}ller and Pierre Pinson and Henrik Madsen},
  title     = {Probabilistic Forecasts of Wind Power Generation by Stochastic Differential Equation Models},
  booktitle = {59\textsuperscript{th} ISI World Statistics Congress},
  year      = {2013},
  month     = aug,
  abstract  = {The increasing production of renewable energy, and in particular wind energy, introduces
highly volatile sources of energy in the total production. This implies that methods
for reliable probabilistic forecasts of future wind power production are essential.
Today there exist numerous methods and tools for providing point forecasts of wind
power generation. However, for efficient and safe regulation, and for harvesting optimal
trading strategies reliable information on the uncertainty is also needed. In this paper
we focus on forecasts on the 1-48 hour horizon. It is well-known that the form of the
conditional density for the wind power production is highly dependent on the level of
predicted wind power in addition to the prediction horizon. This paper describes a new
approach for wind power forecasting based on state dependent stochastic differential
equations (SDEs). Specifically we will use a logistic type stochastic differential equation
to account for the natural restrictions (wind power cannot exceed installed capacity
and cannot be below zero). The SDE is driven by a widely used point predictor for wind
power forecast, and the SDE formulation allows us to calculate both state dependent
conditional uncertainties as well as correlation structures. Evaluation and optimization
of the model is obtained by evaluating the likelihood of a 48-dimensional random vector
when accounting for the correlation structure defined by the SDE-formulation. We
explore the correlation parameters and skewness of the model and input-variables (prediction
horizon and point predictions), by a non-parametric (spline based) model for the
parameters.
Keywords: nonlinear forecasting, state space model, stochastic differential equations,
wind power prediction.},
  comment   = {SDE's used for wind power forecasting using SDE's.

It's done for solar in: Iversen13solarFrcstSDE},
  file      = {Moller13windFrcstSDE.pdf:Moller13windFrcstSDE.pdf:PDF},
  location  = {Hong Kong, China},
  owner     = {sotterson},
  timestamp = {2014.02.04},
  url       = {http://www.statistics.gov.hk/wsc/STS019-P5-A.pdf},
}

@MastersThesis{Tietz13modelDynWkTurbSprngMss,
  author      = {Sascha Tietz},
  title       = {Modeling of dynamic wake-effects and turbine reactions for providing a highly time resolved estimate of the available active power of a wind farm},
  year        = {2013},
  abstract    = {The Increasing shares of fluctuating, renewable energies on the German electricity
production, result in a higher demand of operating reserve for balancing forcasting
Deviations. Likewise, Conventional power Capacities are being driven out of the
market, examined did the renewables have to succed in providing operating reserve.
Wind turbines are particularily suited for this task, due to Their almost controllability.
Neverheless, for operating reserve commission by wind power plants, Their available
active power needs to be Known with a high temporal resolution. A model for providing
a highly time-resolved estimate of the available active power of a curtailed wind farm
is developed. Two efficient algorithms for modeling the dynamic turbine reactions are
developed for this purpose. Deviations from the measurements are systematically
Analyzed. The wake-effects within the wind farm are incorporated using on extended
version of the Jensen-wake-model, wind direction fluctuations Incorporating stocastic.
The dynamic propagation of wake-effects through the wind farm is calculated is based
on Taylor's hypothesis. . The temporal resolution of the wake-model is set accor ding to
the highest, still relevant turbulence-frequency
keywords: Physics, Master thesis, wind turbine, wind farm, wake effect, dy- namic
model turbine, operating reserve},
  comment     = {Possible power estimation using dynamic spring mass turbine model. Done by Fraunhofer IWES master's student.

Data and homedir is here: K:\Organisation\Diplomanden\Arbeiten\Sascha_Tietz},
  file        = {Thesis ("English"):Tietz13modelDynWkTurbSprngMss_trans.pdf:PDF;Thesis (German):Tietz13modelDynWkTurbSprngMss.pdf:PDF},
  institution = {Geor-August-Universit{\"a}t,},
  location    = {G{\"o}ttingen, Germany},
  owner       = {sotterson},
  timestamp   = {2014.11.14},
}

@Article{Wu14solarFrcstShrtTermHybrid,
  author    = {Wu, Yuan-Kang and Chen, Chao-Rong and Abdul Rahman, Hasimah},
  title     = {A Novel Hybrid Model for Short-Term Forecasting in {PV} Power Generation},
  journal   = {International Journal of Photoenergy},
  year      = {2014},
  volume    = {2014},
  abstract  = {The increasing use of solar power as a source of electricity has led to increased interest in forecasting its power output over shorttime
horizons. Short-term forecasts are needed for operational planning, switching sources, programming backup, reserve usage,
and peak load matching. However, the output of a photovoltaic (PV) system is influenced by irradiation, cloud cover, and other
weather conditions. These factors make it difficult to conduct short-term PV output forecasting. In this paper, an experimental
database of solar power output, solar irradiance, air, and module temperature data has been utilized. It includes data from the
Green Energy Office Building in Malaysia, the Taichung Thermal Plant of Taipower, and National Penghu University. Based on
the historical PV power and weather data provided in the experiment, all factors that influence photovoltaic-generated energy are
discussed.Moreover, five types of forecasting modules were developed and utilized to predict the one-hour-ahead PV output.They
include the ARIMA, SVM, ANN, ANFIS, and the combination models using GA algorithm. Forecasting results show the high
precision and efficiency of this combination model.Therefore, the proposed model is suitable for ensuring the stable operation of
a photovoltaic generation system.},
  comment   = {A bunch of hour-ahead PV forecast algorithms, combined with a genetic algorithm. Could these also be boosted weak learners?},
  file      = {Wu14solarFrcstShrtTermHybrid.pdf:Wu14solarFrcstShrtTermHybrid.pdf:PDF},
  owner     = {sotterson},
  publisher = {Hindawi Publishing Corporation},
  timestamp = {2015.02.19},
  url       = {http://www.hindawi.com/journals/ijp/2014/569249/abs/},
}

@TechReport{Ferreira11rampFrcstSurv,
  author      = {Ferreira, C. and Gama, J. and Matias, L. and Botterud, A. and Wang, J.},
  title       = {A survey on wind power ramp forecasting},
  institution = {Argonne National Laboratory (ANL)},
  year        = {2011},
  number      = {ANL/DIS-10-13},
  abstract    = {The increasing use of wind power as a source of electricity poses new challenges with regard to both power production and load balance in the electricity grid. This new source of energy is volatile and highly variable. The only way to integrate such power into the grid is to develop reliable and accurate wind power forecasting systems. Electricity generated from wind power can be highly variable at several different timescales: sub-hourly, hourly, daily, and seasonally. Wind energy, like other electricity sources, must be scheduled. Although wind power forecasting methods are used, the ability to predict wind plant output remains relatively low for short-term operation. Because instantaneous electrical generation and consumption must remain in balance to maintain grid stability, wind power?s variability can present substantial challenges when large amounts of wind power are incorporated into a grid system. A critical issue is ramp events, which are sudden and large changes (increases or decreases) in wind power. This report presents an overview of current ramp definitions and state-of-the-art approaches in ramp event forecasting.},
  comment     = {Good overview: Ramp definitions, Ramp forecast performance metrics and algorithms, and features

Definitions
* no standard definition, but...
* often a threshold on \% of installed capacity over some time duration
-- \% of capacity is criticized for being inaccurate, but it's there all the time
* 4 definitions mentioned (p. 4-6)

Ramp causes
* upward - intense low pressure (cyclones)
- low level jets
- thunderstorms
- gusts (isn't that kind of generic?)
* downward
- rapid pressure gradient slackening
- passage of local pressure couplet (adjacent high/low areas that stick together)
- wind turbine cutoff

Forecast Accuracy Evaluation Metrics
* Point forecast metrics (some of which can be adapted to probabilistic forecasts)
-- kappa coefficient
-- hit/miss stats (precision/recall, Fscore)
-- Critical Success Index (CSI)
-- Bias score
-- KSS (Hanssen \& Kuipers Skill Score), not great for infrequent events like ramps
-- EDS: Extreme dependency score
-- OR (odds ratio)
-- ORSS (odds ratio skill score), maybe the best?
-- note: ORSSS, KSS and bias cover the whole hit/miss detection contingency table (good?)
* Probabilistic forecast metrics
-- BS (Brier Score)
-- BSS (Brier skill socre)
-- RPS (ranked probability score)
-- Reliability diagrams (not so much a metric but a display)
-- MSE (not appropriate for ramp forecast b/c heavily weighs large errors. WHY IS THAT BAD?)
-- MAE (HOW MEASURE THIS? See referenced paper?)
-- standard deviation of absolute error (why a good measure? MAE could be huge but constant)

Economic Value of Ramp Forecasts
* PCAP (partial cost of advance purchase). Some kind of measure of ancillary cost improvement due to ramp forecasting. I don't get it, though.
* Potter/Grimmit found for PCAP intervals w/ different optimal strategies
-- in one of them, a probabislitic ramp forecast was best
-- in the other, a deterministic system was best
-- how would a system know which forecast strategy to pick (read orig. paper)

Ramp Forecasting Models (I'll just list them)
* Greaves et al. 2009
* Zheng and Kusiak, 2009
* Bossavy et al. 2010
* Zack et al. 2010
* WEPROG
* Pyle 2010
* why isn't 3TIER's system mentioned?},
  file        = {Ferreira11rampFrcstSurv.pdf:Ferreira11rampFrcstSurv.pdf:PDF},
  groups      = {Test, Use, doReadWPV_2},
  owner       = {scot},
  timestamp   = {2011.04.18},
  url         = {http://www.dis.anl.gov/projects/windpowerforecasting.html},
}

@Article{Ahrens08infoSkillProbFrcst,
  author    = {Ahrens, B. and Walser, A.},
  title     = {Information-Based Skill Scores for Probabilistic Forecasts},
  journal   = {Monthly weather review},
  year      = {2008},
  volume    = {136},
  number    = {1},
  pages     = {351--362},
  abstract  = {The information content, that is, the predictive capability, of a forecast system is often quantified with skill scores. This paper introduces two ranked mutual information skill (RMIS) scores, RMISO and RMISY, for the evaluation of probabilistic forecasts. These scores are based on the concept of mutual information of random variables as developed in information theory. Like the ranked probability skill score (RPSS)? another and often applied skill score?the new scores compare cumulative probabilities for multiple event thresholds. The RMISO quantifies the fraction of information in the observational data that is explained by the forecasts. The RMISY quantifies the amount of useful information in the forecasts. Like the RPSS, the new scores are biased, but they can be debiased with a simple and robust method. This and additional promising characteristics of the scores are discussed with ensemble forecast assessment experiments.},
  comment   = {Info-based score for ensemble forecasts, possibly discrete. So, can use MI for ramp forecast scoring.

Brier Score
* just RMS diff
* Should only be used for binary, 2 category forecasts

Ranked Probability Score (RPS)
* RMS diff between forecast and climatological c.d.f.'s
* Can do multi-categorical variables, which could be quantile levels

* Better than Brier
* is scored against climatology
* (note: for ramps, climatology is the same all year. Some ramp paper I read)

Ignorance
* Have an ignorance score for each forecast output (small for certain forecasts, doesn't measure error, though)
* not good for multi-category stuff

Self Information and why Kraskov et. al normalize MI when clustering

* H(X) == I(X,X) is called self-information
* MI(X1,X1) <= max(H(X1),H(X2))
* they do some kind of normalization by num. of categories or intervals

* Refs Miller 1955; Paninski 2003; Schuermann 2004 all unbias entropy (but are they multi-dim?)

Ranked Mutual Information Skill Score
* says MI is a distrance metric, satisfies triangle inequality (Kraskov08MIChierClustMutInf says it doesn't)
* Is Kraskov talking about multidimensional, while this is is only pairs of scalars?
* A couple references about mutual information bias correction
* note that entropy depends on num. of classs Ensemble forercasts
* rain events forecasted by ensembles. Probs proportional to num. enembles predicting it (like our ramp forecast)
* generally accepted that larger ensembles of skillful forecasts explain a larger fraction of observation entropy
* with increasing ensemble size, the number of potentially populated prob. classes increases.},
  file      = {Ahrens08infoSkillProbFrcst.pdf:Ahrens08infoSkillProbFrcst.pdf:PDF},
  groups    = {Read, Ensemble, Test, doReadWPV_2},
  owner     = {scot},
  publisher = {American Meteorological Society},
  timestamp = {2011.05.26},
  url       = {http://adsabs.harvard.edu/abs/2008MWRv..136..352A},
}

@TechReport{unknown15delivDownRegWind,
  author      = {Unknown},
  title       = {Delivery of downward aFRR by wind farms},
  institution = {Windvision, Enercon, Eneco and Elia},
  year        = {2015},
  month       = oct,
  abstract    = {The installed capacity of intermittent renewable energy resources (PV, wind,...) increased significantly
over the last years. A further increase is expected in order to achieve the European climate and energy
goals for 2020, reaching up to an expected installed capacity of over 8000 MW in Belgium by the end of
2019. The intermittent nature and limited predictability of these renewables represent additional
challenges to balance the grid.
The sustainable integration of high volumes of intermittent renewables in the system requires these
resources to provide flexibility to balance the generated and consumed electricity.
In the Belgian system the Balancing Responsible Parties are responsible for balancing their portfolio on
a fifteen minute level
1
. In second instance Elia is responsible to resolve any residual imbalance in the
system by means of activation of flexibility offered by market parties or by activation of pre-contracted
reserve capacity.
In Belgium a large share of the pre-contracted reserve capacity for Frequency Containment Reserves
(FCR, former primary reserves) and Automatic Frequency Restoration Reserves (aFRR, former
secondary reserves) is provided by gas-fired power plants (CCGTs).
The market conditions for these CCGT units declined over the last years as these types of units often
fall out of the North-West European (NWE) merit order, with, as a result, high must run costs for these
units to provide FCR and aFRR. Therefore it is important to investigate whether other resources, that
are running under such market conditions (i.e. with less running gas units), could ensure the provision of
these types of reserves.
Windvision (Owner), Eneco (BRP), Enercon (Supplier of windfarms) and Elia (TSO) teamed up to
perform a pilot project investigating the technical capability of wind farms to provide aFRR capacity. In
order to gain as much experience as possible, it was decided to physically test the delivery of downward
aFRR on a real wind farm. The pilot project took place on the Estinnes wind farm of WindVision. This
wind farm has a nominal power of 81 MW and consists of 10 E-126 Enercon wind turbines of 7,5 MW
each and one wind turbine of 6 MW. The existing systems of the wind farm were modified in order to
allow aFRR delivery. Eneco is the BRP (Balancing Responsible Party) for the wind farm and performed
the daily nomination of the available aFRR capacity on the wind farm.
The focus of the pilot project was mainly on the provision of downward aFRR capacity, as the provision
of upward aFRR capacity would require continuous de-rating of the wind farm (significant loss of green
certificates for the power producer, and finally a high cost of the service for the system).},
  comment     = {Report on using wind farms for negative power reserve. I should read for ReWP},
  file        = {unknown15delivDownRegWind.pdf:unknown15delivDownRegWind.pdf:PDF},
  url         = {http://www.elia.be/~/media/files/Elia/users-group/task-force-balancing/Downward_aFRR_windfarms_EN_2015.pdf},
}

@Article{Pajares08featSelTimeSeriesForecast,
  author    = {Pajares, R.G. and Benitez, J.M. and Palmero, G.S.},
  title     = {Feature Selection for Time Series Forecasting: A Case Study},
  year      = {2008},
  month     = sep,
  pages     = {555--560},
  doi       = {10.1109/HIS.2008.95},
  abstract  = {The integration of feature selection techniques within the modeling process of a time series forecaster can improve dealing with some usual important problems in this type of tasks, such as noise reduction, the curse of dimensionality and reducing the complexity of both the problem and the solution. In this paper we show how a convenient combination of feature selection procedures with soft computing techniques can be used to solve satisfactorily a real world problem. The problem is a rather hard one and consists of forecasting the amount of incoming calls for an emergency call center, so that the center managers can make a better resource planning.},
  file      = {Pajares08featSelTimeSeriesForecast.pdf:Pajares08featSelTimeSeriesForecast.pdf:PDF;Pajares08featSelTimeSeriesForecast.pdf:Pajares08featSelTimeSeriesForecast.pdf:PDF},
  journal   = {Hybrid Intelligent Systems (HIS)},
  keywords  = {feature extraction, forecasting theory, time seriesemergency call center, feature selection techniques, noise reduction, resource planning, soft computing techniques, time series forecasting},
  owner     = {sotterson},
  timestamp = {2009.01.08},
}

@InProceedings{Hodge12windLoadFrcstErrDist,
  author    = {Hodge, Bri-Mathias and Florita, Anthony and Orwig, Kirsten and Lew, Debra and Milligan, Michael},
  title     = {A comparison of wind power and load forecasting error distributions},
  booktitle = {Proc. 2012 World Renewable Energy Forum},
  year      = {2012},
  abstract  = {The introduction of large amounts of variable
uncertain power sources, such as wind power, into
electricity grid presents a number of challenges
system operations. One issue involves the uncertainty
associated with scheduling power that wind will supply
future timeframes.  However, this is not an entirely
challenge; load is also variable and uncertain, and
strongly influenced by weather patterns.  In this work
make a comparison between the day-ahead forecasting
errors encountered in wind power forecasting and
forecasting.  The study examines the distribution of errors
from operational forecasting systems in two different
Independent System Operator (ISO) regions for both
wind power and load forecasts at the day-ahead
timeframe.  The day-ahead timescale is critical in power
system operations because it serves the unit commitment
function for slow-starting conventional generators.},
  comment   = {The shape of day-ahead wind power  forecasting errors is similar to those of day-ahead load forecasts, so maybe they are correlated, and therefore should be done jointly, if probabilistic.},
  file      = {Hodge12windLoadFrcstErrDist.pdf:Hodge12windLoadFrcstErrDist.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.03.03},
  url       = {https://pdfs.semanticscholar.org/e561/b18450e6c7efd3af8e4a04384e4104c2dd63.pdf},
}

@Article{Braun18priceSenseFeatsDEmkt,
  author    = {Braun, Sebastian M. and Brunner, Christoph},
  title     = {Price Sensitivity of Hourly Day-ahead and Quarter-hourly Intraday Auctions in Germany},
  journal   = {Zeitschrift für Energiewirtschaft},
  year      = {2018},
  pages     = {1},
  month     = apr,
  issn      = {1866-2765},
  abstract  = {The introduction of the quarter-hourly intraday auction in 2014 for the German market confirms a tendency towards short-term energy markets. The reason for the new market was the need to trade shorter periods than just hours a day-ahead to minimize open positions in the more volatile continuous intraday trading. The increased production capacity of solar power boosted this requirement for new short-term power products. The quarter-hourly market shows a distinctive zigzag price formation. We identify two influencing factors: first, the solar residual that combines the trading of solar power ramps around midday as well as the gradients of consumption and thermal power plant ramps throughout the course of the day, and second, a characteristic two stage market design with higher liquidity for the hourly than for the quarter-hourly auction. Therefore, demand, solar generation and inflexible ramps of thermal power plants are hedged at the hourly day-ahead auction and use the quarter-hourly auction only to balance the remaining differences. To prove this argument the price sensitivities of the hourly day-ahead and quarter-hourly intraday auctions in Germany are compared based on actual bid and ask curves from 2015 and 2016. Finally, the development of an adequate design of future spot markets is discussed.ZusammenfassungDie Einführung der viertelstündlichen Intraday-Auktion im Jahre 2014 für den deutschen Strommarkt untermauert die Bedeutung der kurzfristigen Energiemärkte. Der Grund für die Einführung war der Bedarf, bereits vortägig, viertelstündliche Mengen auszugleichen und offene Positionen im volatilen kontinuierlichen Intraday-Handel zu minimieren. Die gestiegene Stromproduktion aus Solarenergie verstärkte die Nachfrage nach neuen kurzfristigen Produkten. Der Viertelstundenmarkt zeigt eine ausgeprägte Zickzack-Preisbildung. Wir identifizieren zwei Einflussfaktoren: erstens, den Handel von Solarrampen um die Mittagszeit, sowie die Gradienten von Nachfrage und thermischer Erzeugung im Laufe des Tages und zweitens, ein charakteristisches zweistufiges Marktdesign mit höherer Liquidität an den stündlichen als auf der viertelstündlichen Auktion. Die unterschiedliche Liquidität lässt darauf schließen, dass bei der stündlichen Day-Ahead-Auktion die solare Erzeugung und die unflexiblen Rampen thermischer Kraftwerke abgesichert und die viertelstündliche Auktion genutzt, um die verbleibenden Differenzen auszugleichen. Um diese Aussage argumentativ zu verifizieren, werden die Preissensitivitäten der stündlichen Day-Ahead- und viertelstündlichen Intraday-Auktionen in Deutschland anhand der tatsächlichen Angebots- und Nachfragekurven von 2015 und 2016 verglichen. Abschließend wird die Entwicklung einer adäquaten Gestaltung zukünftiger Spotmärkte diskutiert.},
  comment   = {Another measure of liquidity paper on German market.  ID's features that affect the market.},
  date      = {2018-04-18},
  doi       = {10.1007/s12398-018-0228-0},
  publisher = {Springer},
  url       = {http://dx.doi.org/10.1007/s12398-018-0228-0},
}

@Patent{Brummel16monEnrgProgSiePtnt,
  nationality = {European Union},
  number      = {EP 2 706 422 B1},
  year        = {2016},
  yearfiled   = {2012},
  author      = {Brummel, Hans-Gerd and Heesche, Kai and Pfeifer, Uwe and Sterzing, Volkmar},
  title       = {Method for computer-implemented monitoring of the operation of a technical system, in particular an electrical energy production assembly},
  language    = {German},
  assignee    = {Siemens Aktiengesellschaft},
  address     = {80333 München (DE)},
  type        = {patenteu},
  day         = {27},
  dayfiled    = {11},
  month       = jul,
  monthfiled  = sep,
  abstract    = {The invention relates to a method and a
Device for computerized monitoring of the
Operating a technical system, in particular a
electric power generation plant. Further, concerns
the invention is a corresponding technical system
and a computer program product.},
  comment     = {Siemens' patent, from Peter Deeskow of STEAG.  He thinks it's important.

The paper they cite: Juricic06faultDetGaussProc
Related, since authors are from Siemens? Lang08neuralCloundMon},
  file        = {:Brummel16monEnrgProgSiePtnt.pdf:PDF},
}

@Article{Barski14qntHdgShrtfllCtl,
  author    = {Barski, Micha{\l}},
  title     = {On the shortfall risk control-a refinement of the quantile hedging method},
  journal   = {arXiv preprint arXiv:1402.3725},
  year      = {2014},
  abstract  = {The issue of constructing a risk minimizing hedge with additional constraints on the
shortfall risk is examined. Several classical risk minimizing problems have been adapted to
the new setting and solved. The existence and specific forms of optimal strategies in a general
semimartingale market model with the use of conditional statistical tests have been proven.
The quantile hedging method applied in [4] and [5] as well as the classical Neyman-Pearson
lemma have been generalized. Optimal hedging strategies with shortfall constraints in the
Black-Scholes and exponential Poisson model have been explicitly determined.},
  comment   = {How to hedge using quantiles with contraints on shortfall. Maybe related to ReWP interday and intraday offer stochastic optimization? Note that traditional quantile hedging -- while optimal in the long term -- can put you out of business in the short-term via shortfalls. This technique is designed to avoid that, but I'm not sure if it's exactly related to power trading.},
  file      = {Barski14qntHdgShrtfllCtl.pdf:Barski14qntHdgShrtfllCtl.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.01.26},
  url       = {http://arxiv.org/abs/1402.3725},
}

@Article{Fink09xformFAQs,
  author    = {Fink, Edward L},
  title     = {The FAQs on data transformation},
  journal   = {Communication Monographs},
  year      = {2009},
  volume    = {76},
  number    = {4},
  pages     = {379--397},
  abstract  = {The job of the data analyst, strangely enough, is to find random error (Winer, 1968).
When all systematic variability has been removed from data, the leftover*or residual
or disturbance or error*will be random, without pattern; the analyst knows that the
analysis is complete when random error has been found.1 To find random error, data
often need to be transformed nonlinearly, and this issue is the topic of this paper.
Data transformation has a long history (e.g., Box & Cox, 1964) and was already in
review essays in the 1960s (Kruskal, 1968). Tukey?s (1977) volume on exploratory
data analysis, with its emphasis on data transformation, created new interests in these
techniques; Cohen (1990) stated that ??John Tukey?s (1977) Exploratory Data Analysis
is an inspiring account of how to effect graphic and numeric analyses of the data at
hand so as to understand them?? (p. 1310, emphasis in original). Yet to some,
exploratory data analysis (EDA), and the data transformations that are an integral
part of EDA, appear illegitimate or novel or exotic.
...},
  comment   = {When and when to pre-transform data. Especially, has a couple hints for double bounded linearizing transforms e.g. for wind and solar power.

* several transforms for bounded outputs
* The actual regression relation that results when you do the input (and/or output) transform. For both single input multiple inputs.

* See also: Simonoff09transfRegrsn

I asked the author if the the transform was invertible.  He said yes:
https://mail.google.com/mail/u/0/#search/from%3Afink/14411c7a438f6e24

-------------------------------------------------------------------------------------------
Edward L. Fink <elf@umd.edu>
	
2/8/14
	
to me
I'm not sure what you mean by "inverse" here. If you want to "reverse code" the data, you can do it lots of ways:

1.  Instead of

{[(Max - X)**LAMBDA] - [(X - MIN)**LAMBDA]} ,   <1>

you can reverse the order:

{[(X - MIN)**LAMBDA] - [(MAX - X)**LAMBDA]},    <2>

which is the same as multiplying <1>

2.  Or you can change LAMBDA from whatever value it is to (-1)*LAMBDA.  <3>

3.  Or you can subtract the value from <1> above from a large value.

If by "inverse" you mean something else, let me know. Perhaps I don't understand your question.

Ed


Scott Otterson wrote:

    Hello Dr. Fink,

    I found your monograph:

    Fink, E. L.
    The FAQs on data transformation
    /Communication Monographs, Taylor & Francis, /*2009*/, 76/, 379-397
},
  doi       = {doi:org/10.1080/03637750903310352},
  file      = {Fink09xformFAQs.pdf:Fink09xformFAQs.pdf:PDF},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2014.02.07},
  url       = {http://academic.csuohio.edu/kneuendorf/c63111/Fink09.pdf},
}

@InProceedings{Pezeshki16dcnstrLadderNet,
  author    = {Mohammad Pezeshki and Linxi Fan and Philemon Brakel and Aaron Courville and Yoshua Bengio},
  title     = {Deconstructing the Ladder Network Architecture},
  booktitle = {Proc. 33rd International Conference on Machine Learning,},
  year      = {2016},
  pages     = {2368--2376},
  abstract  = {The Ladder Network is a recent new approach to semi-supervised learning that turned out to be very successful. While showing impressive performance, the Ladder Network has many components intertwined, whose contributions are not obvious in such a complex architecture. This paper presents an extensive experimental investigation of variants of the Ladder Network in which we replaced or removed individual components to learn about their relative importance. For semi-supervised tasks, we conclude that the most important contribution is made by the lateral connections, followed by the application of noise, and the choice of what we refer to as the ?combinator function?. As the number of labeled training examples increases, the lateral connections and the reconstruction criterion become less important, with most of the generalization improvement coming from the injection of noise in each layer. Finally, we introduce a combinator function that reduces test error rates on Permutation-Invariant MNIST to 0.57% for the supervised setting, and to 0.97% and 1.0% for semi-supervised settings with 1000 and 100 labeled examples, respectively.},
  comment   = {Finds out what about ladder network yields the performance improvement.  Also enhances it.  Supplementary materials included in attached pdf.

Latest ladder net paper before this one: Rasmus15semiSupLadder
Also see: Valpola15neurPCA2deepUnsupLrnBk

Note that there was also a 2015 ArXiv paper with the same title.},
  file      = {Pezeshki16dcnstrLadderNet.pdf:Pezeshki16dcnstrLadderNet.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.02.01},
  url       = {http://jmlr.org/proceedings/papers/v48/pezeshki16.html},
}

@Article{Zhang17rampSpatTempWindP,
  author   = {Jie Zhang and Mingjian Cui and Bri-Mathias Hodge and Anthony Florita and Jeffrey Freedman},
  title    = {Ramp forecasting performance from improved short-term wind power forecasting over multiple spatial and temporal scales},
  journal  = {Energy},
  year     = {2017},
  volume   = {122},
  pages    = {528 - 541},
  issn     = {0360-5442},
  abstract = {The large variability and uncertainty in wind power generation present a concern to power system operators, especially given the increasing amounts of wind power being integrated into the electric power system. Large ramps, one of the biggest concerns, can significantly influence system economics and reliability. The Wind Forecast Improvement Project (WFIP) was to improve the accuracy of forecasts and to evaluate the economic benefits of these improvements to grid operators. This paper evaluates the ramp forecasting accuracy gained by improving the performance of short-term wind power forecasting. This study focuses on the WFIP southern study region, which encompasses most of the Electric Reliability Council of Texas (ERCOT) territory, to compare the experimental WFIP forecasts to the existing short-term wind power forecasts (used at ERCOT) at multiple spatial and temporal scales. The study employs four significant wind power ramping definitions according to the power change magnitude, direction, and duration. The optimized swinging door algorithm is adopted to extract ramp events from actual and forecasted wind power time series. The results show that the experimental WFIP forecasts improve the accuracy of the wind power ramp forecasting. This improvement can result in substantial costs savings and power system reliability enhancements.},
  doi      = {https://doi.org/10.1016/j.energy.2017.01.104},
  file     = {:Zhang17rampSpatTempWindP.pdf:PDF},
  keywords = {Wind forecasting, Grid integration, Ramp forecasting, ERCOT, Optimized swinging door algorithm},
  url      = {http://www.sciencedirect.com/science/article/pii/S0360544217301111},
}

@TechReport{Nardi08autoRegrLASSO,
  author      = {Yuval Nardi and Alessandro Rinaldo},
  title       = {Autoregressive Process Modeling via the Lasso Procedure},
  institution = {Carnegie Mellon Department of Statistics},
  year        = {2008},
  number      = {862},
  month       = may,
  abstract    = {The Lasso is a popular model selection and estimation procedure for linear models that enjoys nice theoretical properties. In this paper, we study the Lasso estimator for ftting autoregressive time series models. We adopt a double asymptotic framework where the maximal lag may increase with the sample size. We derive theoretical results establishing various types of consistency. In particular, we derive conditions under which the Lasso estimator for the autoregressive coeffcients is model selection consistent, estimation consistent and prediction consistent. Simulation study results are reported.},
  comment     = {Selects AR lags with lasso. Interesting in that maxlag increases w/ train set size.},
  file        = {Nardi08autoRegrLASSO.pdf:Nardi08autoRegrLASSO.pdf:PDF;Nardi08autoRegrLASSO.pdf:Nardi08autoRegrLASSO.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2009.08.17},
  url         = {http://www.stat.cmu.edu/tr/},
}

@Article{Tibshirani05fusedLasso,
  author              = {Tibshirani, Robert and Saunders, Michael and Saharon Rosset and Zhu, Ji and Knight, Keith},
  title               = {Sparsity and Smoothness via the Fused Lasso},
  journal             = {Journal of the Royal Statistical Society, Series B},
  year                = {2005},
  volume              = {67},
  number              = {1},
  pages               = {91--108},
  issn                = {1369-7412},
  abstract            = {The lasso penalizes a least squares regression by the sum of the absolute values ( $L_1-norm$ ) of the coefficients. The form of this penalty encourages sparse solutions (with many coefficients equal to 0). We propose the 'fused lasso', a generalization that is designed for problems with features that can be ordered in some meaningful way. The fused lasso penalizes the $L_1-norm$ of both the coefficients and their successive differences. Thus it encourages sparsity of the coefficients and also sparsity of their differences-i.e. local constancy of the coefficient profile. The fused lasso is especially useful when the number of features p is much greater than N, the sample size. The technique is also extended to the 'hinge' loss function that underlies the support vector classifier. We illustrate the methods on examples from protein mass spectroscopy and gene expression data.},
  comment             = {selection for ordered coeffs; did this work for haar wavelet coeff selection?? Some other paper said that wavelets in general aren't good when remove coeffs and then inverse transform, but they had a solution to this: the analytic wavelet (I have the paper in this bib file somewhere).},
  copyright           = {Copyright ?? 2005 Royal Statistical Society},
  file                = {Tibshirani05fusedLasso.pdf:Tibshirani05fusedLasso.pdf:PDF},
  jstor_articletype   = {primary_article},
  jstor_formatteddate = {2005},
  owner               = {scotto},
  publisher           = {Blackwell Publishing for the Royal Statistical Society},
  timestamp           = {2010.08.16},
  url                 = {http://www.jstor.org/stable/3647602},
}

@Article{Scarrott12ExtrmValThrshUncrtReview,
  author    = {Scarrott, Carl and MacDonald, Anna},
  title     = {A Review of Extreme Value Threshold Estimation And Uncertainty Quantification},
  journal   = {REVSTAT--Statistical Journal},
  year      = {2012},
  volume    = {10},
  number    = {1},
  pages     = {33--60},
  abstract  = {The last decade has seen development of a plethora of approaches for threshold estimation
in extreme value applications. From a statistical perspective, the threshold is
loosely defined such that the population tail can be well approximated by an extreme
value model (e.g., the generalised Pareto distribution), obtaining a balance between
the bias due to the asymptotic tail approximation and parameter estimation uncertainty
due to the inherent sparsity of threshold excess data. This paper reviews recent
advances and some traditional approaches, focusing on those that provide quantification
of the associated uncertainty on inferences (e.g., return level estimation).
Key-Words: extreme value threshold selection;},
  comment   = {Maybe a nice review to start out with. Might also be a quality metric. I assume that is relevant to extreme quantile REGRESSION.},
  file      = {Scarrott12ExtrmValThrshUncrtReview.pdf:Scarrott12ExtrmValThrshUncrtReview.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.04.17},
  url       = {https://www.ine.pt/revstat/pdf/rs120102.pdf},
}

@Article{Ding08tseriesMineRepDist,
  author    = {Ding, Hui and Trajcevski, Goce and Scheuermann, Peter and Wang, Xiaoyue and Keogh, Eamonn},
  title     = {Querying and mining of time series data: experimental comparison of representations and distance measures},
  journal   = {Proceedings of the VLDB Endowment},
  year      = {2008},
  volume    = {1},
  number    = {2},
  pages     = {1542--1552},
  abstract  = {The last decade has witnessed a tremendous growths of interests in applications that deal with querying and mining of time series data. Numerous representation methods for dimensionality reduction and similarity measures geared towards time series have been introduced. Each individual work introducing a particular method has made specific claims and, aside from the occasional theoretical justifications, provided quantitative experimental observations. However, for the most part, the comparative aspects of these experiments were too narrowly focused on demonstrating the benefits of the proposed methods over some of the previously introduced ones. In order to provide a comprehensive validation, we conducted an extensive set of time series experiments re-implementing 8 different representation methods and 9 similarity measures and their variants, and testing their effectiveness on 38 time series data sets from a wide variety of application domains. In this paper, we give an overview of these different techniques and present our comparative experimental findings regarding their effectiveness. Our experiments have provided both a unified validation of some of the existing achievements, and in some cases, suggested that certain claims in the literature may be unduly optimistic.},
  comment   = {Highly cited...},
  file      = {Ding08tseriesMineRepDist.pdf:Ding08tseriesMineRepDist.pdf:PDF},
  owner     = {sotterson},
  publisher = {VLDB Endowment},
  timestamp = {2014.04.24},
  url       = {http://dl.acm.org/citation.cfm?id=1454226},
}

@Book{Cherubini11dynCopulaFinanceBk,
  title     = {Dynamic Copula methods in finance},
  publisher = {John Wiley \& Sons},
  year      = {2011},
  author    = {Cherubini, Umberto and Mulinacci, Sabrina and Gobbi, Fabio and Romagnoli, Silvia},
  volume    = {625},
  isbn      = {978-0470683071},
  abstract  = {The latest tools and techniques for pricing and risk management

This book introduces readers to the use of copula functions to represent the dynamics of financial assets and risk factors, integrated temporal and cross-section applications. The first part of the book will briefly introduce the standard the theory of copula functions, before examining the link between copulas and Markov processes. It will then introduce new techniques to design Markov processes that are suited to represent the dynamics of market risk factors and their co-movement, providing techniques to both estimate and simulate such dynamics. The second part of the book will show readers how to apply these methods to the evaluation of pricing of multivariate derivative contracts in the equity and credit markets. It will then move on to explore the applications of joint temporal and cross-section aggregation to the problem of risk integration.},
  date      = {2011-11-11},
  ean       = {9780470683071},
  file      = {:Cherubini11dynCopulaFinanceBk.pdf:PDF},
  pagetotal = {274},
  url       = {http://www.ebook.de/de/product/14834484/umberto_cherubini_sabrina_mulinacci_fabio_gobbi_dynamic_copula_methods_in_finance.html},
}

@Article{Hocke08gapfillLombScarglePer,
  author    = {Hocke, K. and K?mpfer, N.},
  title     = {Gap filling and noise reduction of unevenly sampled data by means of the Lomb-Scargle periodogram},
  journal   = {Atmospheric Chemistry and Physics Discussions},
  year      = {2008},
  volume    = {8},
  number    = {2},
  pages     = {4603--4623},
  issn      = {1680-7367},
  abstract  = {The Lomb-Scargle periodogram is widely used for the estimation of the power spectral density of unevenly sampled data. A small extension of the algorithm of the Lomb- Scargle periodogram permits the estimation of the phases of the spectral components. 5 The amplitude and phase information is sufficient for the construction of a complex Fourier spectrum. The inverse Fourier transform can be applied to this Fourier spectrum and provides an evenly sampled series (Scargle, 1989). We are testing the proposed reconstruction method by means of artificial time series and real observations of mesospheric ozone, having data gaps and noise. For data gap filling and noise 10 reduction, it is necessary to modify the Fourier spectrum before the inverse Fourier transform is done. The modification can be easily performed by selection of the relevant spectral components which are above a given confidence limit or within a certain frequency range. Examples with time series of lower mesospheric ozone show that the reconstruction method can reproduce steep ozone gradients around sunrise and 15 sunset and superposed planetary wave-like oscillations observed by a ground-based microwave radiometer at Payerne.},
  comment   = {Fills gaps in unevenly sampled data, can extend periodicity. Comes with Matlab Matlab * see ref 15 * I've already downloaded it /Users/sotterson/NowCaster/PowerSight/gapfill},
  file      = {Hocke08gapfillLombScarglePer.pdf:Hocke08gapfillLombScarglePer.pdf:PDF;Hocke08gapfillLombScarglePer.pdf:Hocke08gapfillLombScarglePer.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.03.13},
  url       = {http://www.atmos-chem-phys-discuss.net/8/4603/2008/},
}

@Article{Pinson08regimeSwitchWind,
  author    = {P. Pinson and L. Christensen and H. Madsen and P. E. S{\o}rensen and M. H. Donovan and L. E. Jensen},
  title     = {Regime-switching modelling of the fluctuations of offshore wind generation},
  journal   = {Journal of Wind Engineering and Industrial Aerodynamics},
  year      = {2008},
  volume    = {96},
  number    = {12},
  pages     = {2327--2347},
  month     = dec,
  abstract  = {The magnitude of power fluctuations at large offshore wind farms has a significant impact on the control and management strategies of their power output. If focusing on the minute scale, it looks like different regimes yield different behaviours of the wind power output. The use of statistical regime-switching models is thus investigated. Regime-switching approaches relying on observable (i.e. based on recent wind power production) or non-observable (i.e. a hidden Markov chain) regime sequences are considered. The former approach is based on either self-exciting threshold autoregressive (SETAR) or smooth transition autoregressive (STAR) models, while Markov-switching autoregressive (MSAR) models comprise the kernel of the latter one. The particularities of these models are presented, as well as methods for the estimation of their parameters. The competing approaches are evaluated on a one-step ahead forecasting exercise with time-series of power production averaged at a 1, 5, and 10-min rate, at the Horns Rev and Nysted offshore wind farms in Denmark. For the former wind farm, the one-step ahead root mean square error (RMSE) is contained between 0.8\% and 5\% of installed capacity, while it goes from 0.6\% to 3.9\% of installed capacity for the case of Nysted. It is shown that the regime-switching approach based on MSAR models significantly outperforms those based on observable regime sequences. The reduction in one-step ahead RMSE ranges from 19\% to 32\% depending on the wind farm and time resolution considered. The presented results clearly demonstrate that the magnitude of fluctuations of offshore wind power cannot be considered as simply influenced by the generation level only.},
  comment   = {describes regime switch tuning in Tastu10spatTempErr},
  doi       = {10.1016/j.jweia.2008.03.010},
  file      = {Pinson06regimeSwitchWind.pdf:Pinson08regimeSwitchWind.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.04.17},
  url       = {http://www2.imm.dtu.dk/pubdb/p.php?5069},
}

@Article{Mathworks05redMemLevMarq,
  author   = {MathWorks},
  title    = {Reduced Memory Levenberg-Marquardt (trainlm)},
  journal  = {Matlab Neural Network Toolbox},
  year     = {2005},
  abstract = {The main drawback of the Levenberg-Marquardt algorithm is that it requires the storage of some matrices that
can be quite large for certain problems. The size of the Jacobian matrix is , where Q is the number of
training sets and n is the number of weights and biases in the network. It turns out that this matrix does not have
to be computed and stored as a whole. For example, if we were to divide the Jacobian into two equal
submatrices we could compute the approximate Hessian matrix as follows},
  comment  = {The reduced memory Levenberg-Marquardt algorithm can reduce memory down to the size of an approximation to the Hessian matrix, at the cost of "significant" computational overhead.  Is this like Wilamowski99redMemLevMarq1hidLayer?

The algorithm requires that  a huge Jacobian be calculated, of size

Q x n;   

Q = # of training points
n = # weights and biases in the net

In Matlab, if set mem_reduc ('reduction' after Matlab 2016a or so) to 2, then cut the memory in half, and so on...

But min memory required will have size n X n, for the Hessian.},
  file     = {:Mathworks05redMemLevMarq.pdf:PDF},
  url      = {http://matlab.izmiran.ru/help/toolbox/nnet/backpr13.html},
}

@Article{Hagan94nnTrnLevMarq,
  author   = {M. T. Hagan and M. B. Menhaj},
  title    = {Training feedforward networks with the Marquardt algorithm},
  journal  = {IEEE Transaction on Neural Networks},
  year     = {1994},
  volume   = {5},
  number   = {6},
  pages    = {989--993},
  month    = nov,
  issn     = {1045-9227},
  abstract = {The Marquardt algorithm for nonlinear least squares is presented and is incorporated into the backpropagation algorithm for training feedforward neural networks. The algorithm is tested on several function approximation problems, and is compared with a conjugate gradient algorithm and a variable learning rate algorithm. It is found that the Marquardt algorithm is much more efficient than either of the other techniques when the network contains no more than a few hundred weights},
  comment  = {Reference for the neural network Levenberg-Marquardt algorithm.

Cited here:
http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=6410134},
  doi      = {10.1109/72.329697},
  file     = {paper:Hagan94nnTrnLevMarq.pdf:PDF},
  keywords = {backpropagation, feedforward neural nets, function approximation, least squares approximations, Marquardt algorithm, backpropagation, feedforward network training, feedforward neural networks, function approximation, learning, nonlinear least squares, Acceleration, Approximation algorithms, Backpropagation algorithms, Convergence, Feedforward neural networks, Function approximation, Least squares approximation, Least squares methods, Neural networks, Testing},
}

@TECHREPORT{Unknown15PilotProjectDelivery,
  Author                   = {Unknown},
  Title                    = {Pilot Project: Delivery Of Downward Secondary Control By Wind Farms},
  Institution              = {WindVision, elia, Eneco and Enercon},
  Year                     = {2015},
  Month                    = oct,
  Abstract                 = {The massive integration of renewables, like solar and wind generation, in the electricity grid represents a challenge for keeping the balance between power consumption and generation. In the future it is expected that there will be fewer conventional units running during periods with high renewables infeed. Today these units deliver the majority of balancing services to the TSO. Hence, a further diversification of resources delivering balancing services to the grid is required.

This pilot project therefore investigates the technical capability of wind farms to provide downward secondary control (Automatic Frequency Restoration Reserves) to the electricity grid. This will facilitate the further integration of renewables in the grid and as such contribute to the fulfilment of the European 2020 energy and climate targets.

The results of the pilot are very promising for a future participation of wind farms in the secondary control market in Belgium. The capability of wind farms to perform fast regulation of their production, with relative high accuracy, has been demonstrated. The pilot project has identified some technical and market related aspects that must be further investigated to facilitate the commercial participation of wind farms in this market in the future.

In the framework of this project, the Enercon turbines of the Estinnes wind farm of WindVision technically contributed, for a period of about 2 months, to the delivery of secondary control power to the Belgian grid by continuously changing the active power output of the turbines according to a set-point defined by Elia. Eneco, being the balancing responsible party of the wind farm, provided nominations for the available secondary control capacity on the wind farm to Elia. The project set-up was such that there was no interference with the balancing market.},
  File                     = {Unknown15windNegRsrvPilot.pdf:Unknown15windNegRsrvPilot.pdf:PDF},
  URL                      = {http://www.elia.be/en/users-group/ad-hoc-taskforce-balancing/studies-publications/R2-wind%20study}
}

@Article{Hunt01shortTermWindPredWavlet,
  author    = {Hunt, K. and Nason, G.P.},
  title     = {Wind speed modelling and short-term prediction using wavelets},
  journal   = {Wind Engineering},
  year      = {2001},
  volume    = {25},
  pages     = {55--61},
  abstract  = {The mathematical method of wavelets is explained and used to predict wind condi-
tions using short-term data collected at a site and referred to long term data from,
say, a meteorological station. We model the response time-series in terms of a multi-
scale wavelet decomposition of the explanatory time-series. Preliminary results of this
method, using hourly 10 minute averaged data from six locations in the British Isles,
allow comparison with a linear regression method in terms of prediction errors over 21
days.},
  comment   = {PCA of wavelet comps of offsite measurements used to predict wind
* prediction time is several days ahead
* related to Nason 1999 tech report
* sites are 136 km apart
* used 10 minute data
* direction important
* only 2 PC's! + direction in regressors.
* works better than linear regression},
  file      = {Hunt01shortTermWindPredWavlet.pdf:Hunt01shortTermWindPredWavlet.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.08.21},
  url       = {http://www2.stats.bris.ac.uk/~magpn/Research/papers/wsmstp.pdf},
}

@TechReport{Monahan86GenerateMatrixTdist,
  author      = {Monahan, John F},
  title       = {Generating the Matrix t Distribution},
  institution = {NC State Department of Statistics},
  year        = {1986},
  abstract    = {The matrix t distribution is a generalization of the multivariate Student t

distribution, since the marginal distribution of rows or columns are of that

form, as are some conditional distributions of columns. Moreover, Zellner

(1971) uses the name Generalized Student t distribution. The distribution

arises as the marginal posterior distribution of the matrix of regression

coefficients in a multivariate normal regression model. Further details can be

found in Zellner (1971, Chapter 8 and Appendix B.5), Dickey (1967), Geisser

(1965) and Tiao and Zellner (1965).

Since the generation follows a different characterization, the distribution

theory is discussed in Section II. The method of computation is given in

Section III. The first Appendix contains some details in two lemmas. The

code for generating the distribution is given in the second Appendix.},
  file        = {:papers\\Monahan86GenerateMatrixTdist.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2017.08.22},
  url         = {http://www.stat.ncsu.edu/information/library/mimeo.php},
}

@Article{Cardellach03gpsrBlnOcnWnd,
  author    = {Cardellach, E. and Ruffini, G. and Pino, D. and Rius, A. and Komjathy, A. and Garrison, J.L.},
  title     = {Mediterranean balloon experiment: Ocean wind speed sensing from the stratosphere, using {GPS} reflections},
  journal   = {Remote Sensing of Environment},
  year      = {2003},
  volume    = {88},
  number    = {3},
  pages     = {351--362},
  abstract  = {The MEditerranean Balloon EXperiment (MEBEX), conducted in August 99 from the middle?up stratosphere, was designed to assess the wind retrieval sensitivity of Global Navigation Satellite Systems Reflections (GNSSR) technology from high altitudes. Global Positioning System reflected signals (GPSR) collected at altitudes around 37 km with a dedicated receiver have been inverted to mean square slopes (MSS) of the sea surface and wind speeds. The theoretical tool to interpret the geophysical parameters was a bistatic model, which also depends on geometrical parameters. The results have been analyzed in terms of internal consistency, repeatability and geometry-dependent performance. In addition, wind velocities have been compared to independent measurements by QuikSCAT, TOPEX, ERS/RA and a Radio Sonde, with an agreement better than 2 m/s. A Numerical Weather Prediction Model (NWPM, the MM5 mesoscale forecast model) has also been used for comparison with varying results during the experiment. The conclusion of this study confirms the capability of high altitude GPSR/Delay-map receivers with low gain antennas to infer surface winds. D 2003 Elsevier Inc. All rights reserved. Keywords: Global Positioning System (GPS); Scatterometry; Oceanography; Bistatic radar},
  comment   = {37 km high wandering balloon demonstrates idea of getting wind speed from GPS satellite reflections. Also has old list of "known" wind speed. * Does high alititude (37 km) make measurements more or less accurate? Answer could be found by looking up the details of the inversion equation. *Nearly stationary receiver means low sensitivity to wind speed, and zero sensitivity to wind direction. But maybe there's a way: see p.359 * Sensitivity at higher wind speeds could be a problem for wind power; cut-in is about about 3.5 m/s and the flat spot is at about 14 m/s, so area of reduced glistening change w.r.t. wind speed is at exactly the spot where wind power changes the most w.r.t. wind speed. * Amount of averaging required is no problem (1 min): could average 10X or more and still get usable wind speed * Would CTS style snip alignment might help with some of the correlation alignment problems? * Table 2: Sources for "known" wind speed, which are (an obsolete) list of things to compare with. * Cross satellite repeatability seems OK. * But satellite reflection path arcs rarely overlapped when using up to six satellites, and the delay between one and the next was up to an hour. An argument for using more satellites... * Was biased w.r.t QuikSCAT. Could scatterometer calibration help GPSR accuracy? Some paper 2010-2012 paper I read did get better results by calibrating with radio buoys (I think).},
  file      = {Cardellach03gpsrBlnOcnWnd.pdf:Cardellach03gpsrBlnOcnWnd.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2012.07.09},
}

@Misc{White01normalEquNotes,
  author       = {Robert E. White},
  title        = {Lecture 11: Normal Equation and Polynomial Approximation},
  howpublished = {Lecture notes},
  year         = {2001},
  abstract     = {The model has the form of Ax = d where A is an mxn matrix with m larger than n,
that is, there are more rows or equations than the unknowns in the x vector. Since there
are no exact solutions, we try to find x so that the residual vector in r = d - Ax is as
"small" as possible.},
  comment      = {Course notes: The author also has a couple books, and publishes the corresponding matlab:

Elements of Matrix Modeling and Computing with MATLAB
http://www4.ncsu.edu/eos/users/w/white/www/book/emactoc.htm

Computational Mathematics: Models, Methods and Analysis with MATLAB and MPI
http://www4.ncsu.edu/eos/users/w/white/www/book/cma.htm},
  file         = {White01normalEquNotes.pdf:White01normalEquNotes.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2015.02.04},
  url          = {http://www4.ncsu.edu/eos/users/w/white/www/white/mamac/Lecture%2011.pdf.},
}

@Article{Hamill97relDiagsMultiCatProb,
  author    = {Hamill, Thomas M},
  title     = {Reliability diagrams for multicategory probabilistic forecasts},
  journal   = {Weather and forecasting},
  year      = {1997},
  volume    = {12},
  number    = {4},
  pages     = {736--741},
  abstract  = {The most common method of verifying multicategory probabilistic forecasts such as are used in probabilistic
quantitative precipitation forecasting is through the use of the ranked probability score. This single number
description of forecast accuracy can never capture the multidimensional nature of forecast quality and does not
inform the forecaster about the sources of forecast deficiencies. A new type of reliability diagram is developed
here and applied to probabilistic quantitative precipitation forecasts from a university contest. This diagram is
shown to potentially be useful in helping the forecaster to correct some errors in assigning the categorical
probabilities.},
  comment   = {Calls the IWES kind of reliability diagram a "multicategory reliability diagram," or MCRD. Says it's better than binary thresholded reliability diagrams.},
  doi       = {10.1175/1520-0434(1997)012<0736:RDFMPF>2.0.CO;2},
  file      = {Hamill97relDiagsMultiCatProb.pdf:Hamill97relDiagsMultiCatProb.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.12.08},
}

@Article{Hofert13samplingMultivarTdist,
  author    = {Hofert, Marius},
  title     = {On Sampling from the Multivariate t Distribution.},
  journal   = {R Journal},
  year      = {2013},
  volume    = {5},
  number    = {2},
  abstract  = {The multivariate normal and the multivariate t distributions belong to the most widely used
multivariate distributions in statistics, quantitative risk management, and insurance. In contrast to
the multivariate normal distribution, the parameterization of the multivariate t distribution does
not correspond to its moments. This, paired with a non-standard implementation in the R package
mvtnorm, provides traps for working with the multivariate t distribution. In this paper, common
traps are clarified and corresponding recent changes to mvtnorm are presented.},
  comment   = {May have some good tips for generating samples given a trained t distribution model.},
  file      = {:papers\\Hofert13samplingMultivarTdist.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.08.22},
  url       = {https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiLpcWygOvVAhUlCpoKHYmEANcQFggmMAA&url=https%3A%2F%2Fjournal.r-project.org%2Farchive%2F2013-2%2Fhofert.pdf&usg=AFQjCNEw3lPgSPvoe-y410et6_-xASnhbw},
}

@InCollection{Haupt14windPowFrcstNCARchapter,
  author    = {Haupt, Sue Ellen and Mahoney, William P and Parks, Keith},
  title     = {Wind power forecasting},
  booktitle = {Weather Matters for Energy},
  publisher = {Springer},
  year      = {2014},
  pages     = {295--318},
  abstract  = {The National Center for Atmospheric Research (NCAR) has configured a Wind Power
Forecasting System for Xcel Energy that integrates high resolution and ensemble modeling with
artificial intelligence methods. This state-of-the-science forecasting system includes specific
technologies for short-term detection of wind power ramps, including a Variational Doppler
Radar Analysis System and an expert system. This chapter describes this forecasting system and
how wind power forecasting can significantly improve grid integration by improving reliability
in a manner that can minimize costs.  Errors in forecasts become opportunity costs in the energy
market; thus, more accurate forecasts have the potential to save substantial amounts of money for
the utilities and their ratepayers.  As renewable energy expands, it becomes more important to
provide high quality forecasts so that renewable energy can carve out its place in the energy mix},
  comment   = {Review of NCAR system.  Discusses costs of error, shows some NWP pheneomena that it catches.},
  file      = {Haupt14windPowFrcstNCARchapter.pdf:Haupt14windPowFrcstNCARchapter.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.03.21},
  url       = {http://link.springer.com/chapter/10.1007/978-1-4614-9221-4_14},
}

@Article{Moehrlen12tradeStrtgyRESeeg2012,
  author    = {M{\"o}hrlen, Corinna and Pahlow, Markus and J{\o}rgensen, Jess U},
  title     = {Investigation of various trading strategies for wind and solar power developed for the new {EEG} 2012 law},
  journal   = {Zeitschrift fur Energiewirtschaft},
  year      = {2012},
  volume    = {36},
  number    = {1},
  pages     = {9},
  abstract  = {The new EEG 2012 law opens up for more parties to participate in the trading of wind and solar power, because of the bonus system that now compensates everybody for all market relevant costs, not only the Transmission System Operators. Therefore it can be expected, that the trading of renewable energies by private parties will increase. One of the central questions to be answered is how efficient does a balance responsible party have to be to stay competitive also with a small pool. The quantification of balance costs for different trading strategies is however complex and non-trivial. We propose a methodology in this study that accounts for this fact. Additionally, we analyse and show the requirement and the monetary value of Intra-Day trading for the handling of wind and solar power. The trading strategies proposed in this article make use of an uncertainty band around the forecasts used in the Intra-Day in order to avoid double trading and thereby reducing the total balancing volume and the associated costs.},
  comment   = {Argument that should use probabilistic forecasts on DE intraday markets after EEG 2012.

* TSO's have small trading costs, but later they want to avoid double trading (b/c of costs or b/c of risk of buy high, sell low?)
* "spot market" 24 hrs ahead
* intraday "shorter times," but I found that there were bids w/ horizons over 30 hrs
* No incentive or rqt. to increase EEG wind plant operational efficiency, (since get fixed price, but wouldn't you still want to sell more?)
* tests done w/ WEPROG 75 member ensembles made at 48 hr. horizon.
 -- says ensembles provide decent error coverage up to 18 hrs ahead (no test, though. Seems that QR would be better then?)
 -- the prob forecasts can be used to improve point forecasts
* their upscaling algorithm used an Ensemble Kalman Filter (see the paper)
 -- this iEnKF has 3D feedback from measurements at different locations
 -- sounds like it's a wind speed correcter, not a power correcter
* Must use ensemble forecasts for good extreme event performance. Stat methods (QR?) don't work.

Market properties
* lose money by trading wind energy on intraday market
* reason: more expensive to buy intraday error correction power than to trade on a bias-free day-ahead forecast
 -- so DA forecast lack of bias is important!
 -- Matthias Stark complained about the bias in our forecast, said it should be MAE, but that's not necessary
 -- should probably do adaptive bias correction on DA RMSE-optimized forecasts, at the least
* claim that multiple trades somehow activates extra control power (WHY?)
* Balancing power prices are extremely difficult to forecast

Market simulation
* they simulated trading for all of DE
* simulated up to 13 hours ahead in intraday trading
* did hourly intraday (not 15 min periods)
* did six combos of trading w/ different data
* didn't simulate reserve activation cost. This seems wrong.

Trading algorithm:
* only fix the error that the quantiles say is outside of confidence interval
* only fix it enough to bring it just inside the interval (I think)
* Spread-based uncertainty estimation
 -- estimation from correlation of ensemble spread and forecast error
 -- not quantile regression, so should be able to do better than this
* seems like intraday forecast correction equation (6) is wrong, is missing sum of prev. corrections
* don't trade if forecast error estimated to be < 2% of installed cap.
* price forecast:
 -- didn't use one
 -- couldn't predict price from error volumes, especially failed to predict large errors.
 -- note that Pinson07tradeWindProbFrcst assumes you _can_ forecast this but doesn't prove/demonstrate that this is true

RESULTS
* alg makes the least number of corrections, and they are small
* seems to work (I didn't read this too carefully)},
  file      = {Moehrlen12tradeStrtgyRESeeg2012.pdf:Moehrlen12tradeStrtgyRESeeg2012.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2015.04.30},
  url       = {http://download.weprog.com/WEPROG_Trading_strategies_EEG2012_ZEFE_71-2012-01_en.pdf},
}

@Article{Salvador05lrnAnomDet,
  author    = {Salvador,, Stan and Chan,, Philip},
  title     = {Learning States and Rules for Detecting Anomalies in Time Series},
  year      = {2005},
  volume    = {23},
  number    = {3},
  pages     = {241--255},
  issn      = {0924-669X},
  doi       = {10.1007/s10489-005-4610-3},
  abstract  = {The normal operation of a device can be characterized in different temporal states. To identify these states, we introduce a segmentation algorithm called Gecko that can determine a reasonable number of segments using our proposed L method. We then use the RIPPER classification algorithm to describe these states in logical rules. Finally, transitional logic between the states is added to create a finite state automaton. Our empirical results, on data obtained from the NASA shuttle program, indicate that the Gecko segmentation algorithm is comparable to a human expert in identifying states, and our L method performs better than the existing permutation tests method when determining the number of segments to return in segmentation algorithms. Empirical results have also shown that our overall system can track normal behavior and detect anomalies.},
  file      = {Salvador05lrnAnomDet.pdf:Salvador05lrnAnomDet.pdf:PDF;Salvador05lrnAnomDet.pdf:Salvador05lrnAnomDet.pdf:PDF},
  journal   = {Applied Intelligence},
  location  = {Hingham, MA, USA},
  owner     = {sotterson},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2009.03.09},
}

@Article{Ghosh03nortaCorrRVdimNoGaussCpla,
  author    = {Ghosh, Soumyadip and Henderson, Shane G},
  title     = {Behavior of the NORTA method for correlated random vector generation as the dimension increases},
  journal   = {ACM Transactions on Modeling and Computer Simulation (TOMACS)},
  year      = {2003},
  volume    = {13},
  number    = {3},
  pages     = {276--294},
  abstract  = {The NORTA method is a fast general-purpose method for generating samples of a random vector
with given marginal distributions and given correlation matrix. It is known that there exist
marginal distributions and correlation matrices that the NORTA method cannot match, even
though a random vector with the prescribed qualities exists. In this case we say that the correlation
matrix is NORTA defective (for the given marginals). We investigate this problem as the dimension
of the random vector increases. Simulation results show that the problem rapidly becomes acute,
in the sense that an increasingly large proportion of correlation matrices are NORTA defective.
Simulation results also show that if one is willing to settle for a correlation matrix that is ?close?
to the desired one, then NORTA performs well with increasing dimension. As part of our analysis
we develop a method for sampling symmetric positive definite correlation matrices uniformly (in
the Lebesgue measure sense) from the set of all such matrices. This procedure can be used more
generally for sampling uniformly from the space of all symmetric positive definite matrices with
diagonal elements fixed at positive values.},
  comment   = {Proof that can't express all patterns of Pearson corr and uniform margins with a Gaussian copula.

Owen13monteCarloBook says:

Ghosh and Henderson (2003) look into the phenomenon in which a desired
Pearson correlation matrix, in combination with uniform margins, is unattain-
able via the Gaussian copula. A well studied example from Li and Hammond
(1975) has d = 3, F1 = F2 = F3 = U(0, 1) and

Corr(X) =

1 ?0.4 0.2
?0.4 1 0.8

0.2 0.8 1

.

They show that Gaussian copula sampling cannot produce such a distribution.
That such a distribution exists, and can be sampled by some other method, was
proved by Ghosh and Henderson (2002). The failure to exist via the Gaussian
copula is most interesting when the distribution exists.},
  file      = {:papers\\Ghosh03nortaCorrRVdimNoGaussCpla.pdf:PDF},
  owner     = {sotterson},
  publisher = {ACM},
  timestamp = {2017.07.05},
  url       = {https://ecommons.cornell.edu/bitstream/handle/1813/9249/TR001372.pdf?sequence=1},
}

@InProceedings{Theis10ensCosmoDE,
  author    = {Theis, S and Gebhardt, C and Buchhold, M and Ben Bouall{\`e}gue, Z and Ohl, R and Paulat, M and Peralta, C},
  title     = {Developing an Ensemble Prediction System based on COSMO-DE},
  booktitle = {10\textsuperscript{th} EMS Annual Meeting, 10\textsuperscript{th} European Conference on Applications of Meteorology (ECAM) Abstracts,},
  year      = {2010},
  volume    = {1},
  location  = {Z{\"u}rich, Switzerland},
  pages     = {768},
  url       = {http://adsabs.harvard.edu/abs/2010ems..confE.768T},
  abstract  = {The numerical weather prediction model COSMO-DE is a configuration of the COSMO model with a horizontal
grid size of 2.8 km. It has been running operationally at DWD since 2007, it covers the area of Germany and
produces forecasts with a lead time of 0-21 hours. The model COSMO-DE is convection-permitting, which means
that it does without a parametrisation of deep convection and simulates deep convection explicitly. One aim is an
improved forecast of convective heavy rain events.
Convection-permitting models are in operational use at several weather services, but currently not in ensemble
mode. It is expected that an ensemble system could reveal the advantages of a convection-permitting model even
better. The probabilistic approach is necessary, because the explicit simulation of convective processes for more
than a few hours cannot be viewed as a deterministic forecast anymore. This is due to the chaotic behaviour and
short life cycle of the processes which are simulated explicitly now.
In the framework of the project COSMO-DE-EPS, DWD is developing and implementing an ensemble prediction
system (EPS) for the model COSMO-DE. The project COSMO-DE-EPS comprises the generation of ensemble
members, as well as the verification and visualization of the ensemble forecasts and also statistical postprocessing.
A pre-operational mode of the EPS with 20 ensemble members is foreseen to start in 2010. Operational use is
envisaged to start in 2012, after an upgrade to 40 members and inclusion of statistical postprocessing.
The presentation introduces the project COSMO-DE-EPS and describes the design of the ensemble as it is planned
for the pre-operational mode. In particular, the currently implemented method for the generation of ensemble
members will be explained and discussed. The method includes variations of initial conditions, lateral boundary
conditions, and model physics. At present, pragmatic methods are applied which resemble the basic ideas of a
multi-model approach. For the variation of initial conditions and lateral boundary conditions, forecasts of different
global models are used, and for the variation of model physics, different configurations of the COSMO-DE
are used. Verification and investigation of the ensemble forecasts give an estimate about the current quality
characteristics of the ensemble and how the variations affect the forecasts. The main focus is precipitation. For
statistical postprocessing, a logistic regression for probabilities of precipitation is under development.},
  file      = {Theis10ensCosmoDE.pdf:Theis10ensCosmoDE.pdf:PDF},
  groups    = {Ensemble, CitaviImport1, doReadNonWPV_1},
  keywords  = {COSMO-DE-EPS, DWD, EPS},
  ncite     = {0},
  owner     = {sotterson},
  timestamp = {2013.09.26},
}

@TechReport{Cibulka12frcstNeedsCAISO,
  author      = {Cibulka, Lloyd and Brown, Merwin and Miller, Larry and Von Meier, Alexandra},
  title       = {User Requirements {And} Research Needs For Renewable Generation Forecasting Tools That Will Meet The Needs Of The Caiso {And} Utilities For 2020},
  institution = {California Energy Commission},
  year        = {2012},
  type        = {Task 2 White Paper Report},
  number      = {CEC-XXX-2012-XXX},
  month       = sep,
  abstract    = {The objectives of this white paper were to: assess the current state of the art in renewable
generation (solar and wind) forecasting tools, including their various components, reliability,
shortcomings, and the current research efforts to improve these tools; determine the specific
requirements of the California end-users of the technology, including the CAISO and the
California utilities, for effective management of renewable resources as California strives to
achieve 33\% renewable penetration by 2020 in accordance with the state?s Renewable Portfolio
Standard (RPS); and determine the renewable forecasting technology gaps that will exist by
2020, and identify the research efforts necessary to address those gaps. This white paper is
intended to provide information that will help target future solicitations for research toward
applications that will help California better reach its near-term renewable energy goals.
Keywords: California Energy Commission, renewable energy, forecasting, RPS, Renewable
Portfolio Standard, solar generation, wind generation, renewable penetration.},
  comment     = {Uses and needs for probabilistic forecasts at CAISO. Speakers might be worthwhile tracking down.

CAISO
* uses a probabilistic ramp forecast tool designed by PNNL.
* wants site-specific -- not regional -- probabistic forecasts

DOE
* research focus is on
-- decision making w/ probabilistic forecasts: "Translate probabilistic information into actionable decision-making (probabilistic forecasting) for operators."
--- probabilistic spatial and temporal aggregation},
  file        = {Cibulka12frcstNeedsCAISO.pdf:Cibulka12frcstNeedsCAISO.pdf:PDF},
  groups      = {Use, doReadWPV_2},
  location    = {California, USA},
  owner       = {sotterson},
  timestamp   = {2013.10.02},
}

@InBook{Glavitsch91optPowFlwAlgsBookCh,
  chapter   = {Optimal Power Flow Algorithms},
  title     = {Analysis and Control System Techniques for Electric Power Systems},
  publisher = {Academic Press Inc.},
  year      = {1991},
  author    = {Hans Glavitsch and Rainer Bacher},
  volume    = {41},
  abstract  = {The ordinary power flow or load flow problem is stated by specifying the loads
in megawatts and megavars to be supplied at certain nodes or busbars of a
transmission system and by the generated powers and the voltage magnitudes
at the remaining nodes of this system together with a complete topological
description of the system including its impedances. The objective is to determine
the complex nodal voltages from which all other quantities like line flows,
currents and losses can be derived. The model of the transmission system is
given in complex quantities since an alternating current system is assumed to
generate and supply the powers and loads.
In mathematical terms the problem can be reduced to a set of nonlinear
equations where the real and imaginary components of the nodal voltages are
the variables. The number of equations equals twice the number of nodes. The
nonlinearities can roughly be classiffied being of a quadratic nature. Gradient and relaxation techniques are the only methods for the solution of these
systems.
The result of a power flow problem tells the operator or a planner of a
system in which way the lines in the system are loaded, what the voltages
at the various buses are, how much of the generated power is lost and where
limits are exceeded.
The power flow problem is one of the basic problems in which both load
powers and generator powers are given or fixed. Today, this basic problem can
be efficiently handled on the computer for practically any size system.},
  comment   = {An apparently clear book chapter on optimal power flow (but a bit old).
See also: Zhang13RobustoptPowFlwVaR},
  file      = {Glavitsch91optPowFlwAlgsBookCh.pdf:Glavitsch91optPowFlwAlgsBookCh.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.26},
  url       = {http://people.ee.ethz.ch/~bacher/publications/},
}

@Article{Makarov09OperWindCA,
  author    = {Makarov, Y.V. and Loutan, C. and Jian Ma and de Mello, P.},
  title     = {Operational Impacts of Wind Generation on California Power Systems},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2009},
  volume    = {24},
  number    = {2},
  pages     = {1039--1050},
  month     = may,
  issn      = {0885-8950},
  abstract  = {The paper analyzes the impact of integrating wind generation on the regulation and load following requirements of the California Independent System Operator (CAISO). These requirements are simulated and compared for the study cases with and without wind generation impacts included into the study for the years 2006 and 2010. Regulation and load following models were built based on hour-ahead and five-minute ahead load and wind generation forecasts. In 2006, the CAISO system peaked at 50 270 MW. Wind generation (at the installed capacity of 2600 MW) had limited impact on the requirement of load following and regulation in the CAISO Balancing Authority. However, in 2010 (with an expected installed capacity of approximately 6700 MW), this impact will significantly increase. The results provide very useful information for the CAISO to adjust its scheduling and real-time dispatch systems to reliably accommodate future wind generation additions within the CAISO Balancing Authority.},
  comment   = {How to set ramp requirements w/ swinging door (separates regulation from load following)
* ref 29 seems like the original algo ref.
-- orig ref for swingdoor in Bristol09swingDoorTrend
* swinging door also mentioned in:
"Integration of Renewable Resources: Technical Appendices for California ISO Renewable Integration Studies", Version 1 www.caiso.com/282d/282d85c9391b0.pdf
},
  doi       = {10.1109/TPWRS.2009.2016364},
  file      = {Makarov09OperWindCA.pdf:Makarov09OperWindCA.pdf:PDF},
  groups    = {Ensemble, Use, doReadWPV_2},
  keywords  = {CAISO Balancing Authority;California Independent System Operator;load following models;operational impacts;power 2600 MW;power 50270 MW;power systems;realtime dispatch systems;regulation following models;scheduling systems;wind generation forecasts;load forecasting;power generation dispatch;power generation scheduling;wind power plants;},
  owner     = {scot},
  timestamp = {2011.06.20},
  url       = {www.caiso.com/282d/282d85c9391b0.pdf},
}

@InProceedings{Shenoy15StochOptPowMktRgrssn,
  author    = {Shenoy, Saahil and Gorinevsky, Dimitry},
  title     = {Stochastic Optimization of Power Market Forecast Using Non-Parametric Regression Models},
  booktitle = {{IEEE} POWER \& ENERGY SOCIETY GENERAL MEETING},
  year      = {2015},
  month     = jul,
  abstract  = {The paper considers stochastic optimization of
the electricity procurement in the day-ahead power market.
The novelty is in addressing the random errors of time
series forecasting of electrical power loads and prices in the
procurement. This problem is currently important because
of the increased random variability in the power grid that
is caused by growing integration of renewable generation.
This paper presents a methodology for stochastic optimization
using data-driven models. We consider non-parametric models
of multivariate distributions based on multiple quantile regressions,
built from historical data sets. The statistics, such
as cost expectation, required for the stochastic optimization
are computed numerically using these models. Applying the
methodology to utility data shows that 2\% improvement of the
costs is feasible.},
  comment   = {Day ahead power market bidding optimization using trained forecast error distributions. Saved 2\%. Maybe good for German TSO trading (intraday?)?

Forecasting method is here: Shenoy15stochOptLdFrcstRgrssn (I think)},
  file      = {Shenoy15StochOptPowMktRgrssn.pdf:Shenoy15StochOptPowMktRgrssn.pdf:PDF},
  location  = {Denver, CO},
  owner     = {sotterson},
  timestamp = {2015.04.17},
  url       = {http://web.stanford.edu/~gorin/papers/PES15_QRopt.pdf},
}

@Article{Selsnick05dualTreeCmplxWvlts,
  author    = {Selesnick, I.W. and Baraniuk, R.G. and Kingsbury, N.C.},
  title     = {The dual-tree complex wavelet transform},
  journal   = {Signal Processing Magazine, IEEE},
  year      = {2005},
  volume    = {22},
  number    = {6},
  pages     = {123--151},
  month     = nov,
  issn      = {1053-5888},
  abstract  = {The paper discusses the theory behind the dual-tree transform, shows how complex wavelets with good properties can be designed, and illustrates a range of applications in signal and image processing. The authors use the complex number symbol C in CWT to avoid confusion with the often-used acronym CWT for the (different) continuous wavelet transform. The four fundamentals, intertwined shortcomings of wavelet transform and some solutions are also discussed. Several methods for filter design are described for dual-tree CWT that demonstrates with relatively short filters, an effective invertible approximately analytic wavelet transform can indeed be implemented using the dual-tree approach.},
  comment   = {Wavelet probs -- esp. multidimensional -- reduced by making them complex, a basis for many new multidimensional wavelet approaches. A good overview paper, authors say.},
  doi       = {10.1109/MSP.2005.1550194},
  file      = {Selsnick05dualTreeCmplxWvlts.pdf:Selsnick05dualTreeCmplxWvlts.pdf:PDF},
  keywords  = { dual-tree complex wavelet transform; filter design; image processing; signal processing; channel bank filters; image processing; wavelet transforms;},
  owner     = {scotto},
  timestamp = {2010.08.14},
}

@Electronic{Rockafellar14superQuantRgrssn,
  author    = {Rockafellar, R Terry and Royset, Johannes O and Miranda, SI},
  year      = {2014},
  title     = {Superquantile regression with applications to buffered reliability, uncertainty quantification, and conditional value-at-risk},
  url       = {http://www.sciencedirect.com/science/article/pii/S0377221713008692},
  abstract  = {The paper presents a generalized regression technique centered on a superquantile (also called conditional
value-at-risk) that is consistent with that coherent measure of risk and yields more conservatively
fitted curves than classical least-squares and quantile regression. In contrast to other generalized regression
techniques that approximate conditional superquantiles by various combinations of conditional
quantiles, we directly and in perfect analog to classical regression obtain superquantile regression functions
as optimal solutions of certain error minimization problems. We show the existence and possible
uniqueness of regression functions, discuss the stability of regression functions under perturbations
and approximation of the underlying data, and propose an extension of the coefficient of determination
R-squared for assessing the goodness of fit. The paper presents two numerical methods for solving the
error minimization problems and illustrates the methodology in several numerical examples in the areas
of uncertainty quantification, reliability engineering, and financial risk management.

Keywords:
Generalized regression
Superquantiles
Conditional value-at-risk
Uncertainty quantification
Buffered failure probability
Stochastic programming},
  comment   = {Prediction the average cost in the upper tail of a distribution. Useful for tractable optimization, is additive, and conservative. Seems relevant to regulation power determination and stochastic optimization. Also maybe aggregation since superquantiles are "some kind of" additive. Good for value at risk problems.

Could be use for estimating reserve requirements, which are a contional value at risk problem: SaezGallego14probRsrvRqtsDet

 Slides are also attached.},
  file      = {Paper:Rockafellar14superQuantRgrssn.pdf:PDF;Slides:Rockafellar14superQuantRgrssn_slides.pdf:PDF},
  journal   = {European Journal of Operational Research},
  number    = {1},
  owner     = {sotterson},
  pages     = {140--154},
  publisher = {Elsevier},
  timestamp = {2014.11.07},
  volume    = {234},
}

@Article{Ghaderi17DeepLrnSpatTempFrcstWindPow,
  author      = {Amir Ghaderi and Borhan M. Sanandaji and Faezeh Ghaderi},
  title       = {Deep Forecast: Deep Learning-based Spatio-Temporal Forecasting},
  journal     = {ICML Time Series Workshop},
  year        = {2017},
  abstract    = {The paper presents a spatio-temporal wind speed forecasting algorithm using Deep Learning (DL)and in particular, Recurrent Neural Networks(RNNs). Motivated by recent advances in renewable energy integration and smart grids, we apply our proposed algorithm for wind speed forecasting. Renewable energy resources (wind and solar)are random in nature and, thus, their integration is facilitated with accurate short-term forecasts. In our proposed framework, we model the spatiotemporal information by a graph whose nodes are data generating entities and its edges basically model how these nodes are interacting with each other. One of the main contributions of our work is the fact that we obtain forecasts of all nodes of the graph at the same time based on one framework. Results of a case study on recorded time series data from a collection of wind mills in the north-east of the U.S. show that the proposed DL-based forecasting algorithm significantly improves the short-term forecasts compared to a set of widely-used benchmarks models.},
  comment     = {Deep learning spatio temporal wind power forecast, has TensorFlow code

code
https://github.com/amirstar/Deep-Forecast},
  date        = {2017-07-24},
  eprint      = {1707.08110v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Ghaderi17DeepLrnSpatTempFrcstWindPow.pdf:PDF},
  keywords    = {cs.LG},
  url         = {http://arxiv.org/pdf/1707.08110v1},
}

@Article{Skartveit98hourlydiffusefraction,
  author    = {Arvid Skartveit and Jan Asle Olseth and Marit Elisabet Tuft},
  title     = {An hourly diffuse fraction model with correction for variability and surface albedo},
  journal   = {Solar Energy},
  year      = {1998},
  volume    = {63},
  number    = {3},
  pages     = {173 - 183},
  issn      = {0038-092X},
  abstract  = {The paper presents an improved version of a previously published model for the diffuse fraction of hourly global irradiance. In addition to hourly solar elevation and clearness index, an hour-to-hour variability index and regional surface albedo are included among the input parameters. Moreover, to prevent excessively high normal incidence beam irradiances at very low solar elevations, the model does not allow a solar elevation dependent maximum beam transmittance to be exceeded. This new model is tuned to 32 years of data from Bergen, Norway. Moreover, a test against independent data from four European stations showed that the model performs better than the models of Erbs et al. (1982), Maxwell (1987)and Perez et al. (1992). },
  comment   = {Part of IWES PV model.  See also Saint-Drenan14commentsGenPVpow},
  doi       = {https://doi.org/10.1016/S0038-092X(98)00067-X},
  file      = {Skartveit98hourlydiffusefraction.pdf:Skartveit98hourlydiffusefraction.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.30},
  url       = {http://www.sciencedirect.com/science/article/pii/S0038092X9800067X},
}

@Article{Barthelmie07wakeModelMeasLargeWind,
  author    = {R.J. Barthelmie and O. Rathmann and S.T. Frandsen and K. Hansen and E. Politis and J. Prospathopoulos and K. Rados and D. Cabez??n and W. Schlez and J. Phillips and A. Neubert and J.G. Schepers and S.P. van der Pijl},
  title     = {Modelling and measurements of wakes in large wind farms},
  year      = {2007},
  volume    = {75},
  doi       = {10.1088/1742-6596/75/1/012049},
  abstract  = {The paper presents research conducted in the Flow workpackage of the EU funded UPWIND project which focuses on improving models of flow within and downwind of large wind farms in complex terrain and offshore. The main activity is modelling the behaviour of wind turbine wakes in order to improve power output predictions},
  file      = {Barthelmie07wakeModelMeasLargeWind.pdf:Barthelmie07wakeModelMeasLargeWind.pdf:PDF},
  journal   = {Journal of Physics: Conference},
  owner     = {sotterson},
  timestamp = {2008.08.28},
}

@InCollection{Stolojescu10wvltPredTS,
  author    = {Stolojescu, Cristina and Railean, Ion and Lenca, Sorin Moga1 Philippe and Isar, Alexandru},
  title     = {A wavelet based prediction method for time series},
  booktitle = {Stochastic Modeling Techniques and Data Analysis International Conference},
  year      = {2010},
  month     = jun,
  abstract  = {The paper proposes a wavelet-based forecasting method for time series. We used the multi-resolution decomposition of the signal implemented using trous wavelet transform. We combined the Stationary Wavelet Transform (SWT) with four prediction methodologies: Artificial Neural Networks, ARIMA, Linear regression and Random walk. These techniques were applied to two types of real data series: WiMAX network traffic and financial. We proved that the best results are obtained using ANN combined with the wavelet transform. Also, we compared the results using various types of mother wavelets. It is shown that Haar and Reverse biorthogonal 1 give the best results. Keywords: time series, Stationary Wavelet Transform, forecasting.},
  comment   = {Stationary wavelet transform + several additional forecasting methods e.g. NN: Haar (Daubechies1) and Reverse biorthogonal 1 wavlets improve performance on this data. Not a ton of explanation... Also has (not super) slides.},
  file      = {Paper:Stolojescu10wvltPredTS.pdf:PDF;Slides:Stolojescu10wvltPredTS_slides.pdf:PDF},
  location  = {Chania, Crete},
  owner     = {sotterson},
  timestamp = {2013.03.16},
}

@Article{Stoica04modelSelInfo,
  author    = {Stoica, P. and Selen, Y.},
  title     = {Model-order selection: a review of information criterion rules},
  journal   = {Signal Processing Magazine, IEEE},
  year      = {2004},
  volume    = {21},
  number    = {4},
  pages     = {36--47},
  month     = jul,
  issn      = {1053-5888},
  abstract  = {The parametric (or model-based) methods of signal processing often require not only the estimation of a vector of real-valued parameters but also the selection of one or several integer-valued parameters that are equally important for the specification of a data model. Examples of these integer-valued parameters of the model include the orders of an autoregressive moving average model, the number of sinusoidal components in a sinusoids-in-noise signal, and the number of source signals impinging on a sensor array. In each of these cases, the integer-valued parameters determine the dimension of the parameter vector of the data model, and they must be estimated from the data.},
  comment   = {BIC/AIC/GIC},
  doi       = {10.1109/MSP.2004.1311138},
  file      = {Stoica04modelSelInfo.pdf:Stoica04modelSelInfo.pdf:PDF;Stoica04modelSelInfo.pdf:Stoica04modelSelInfo.pdf:PDF},
  keywords  = { Bayes methods, array signal processing, autoregressive moving average processes, information theory, maximum likelihood estimation autoregressive moving average model, data model, integer-valued parameters, maximum likelihood parameter estimation, real-valued parameters, sensor array, signal processing, sinusoids-in-noise signal},
  owner     = {sotterson},
  timestamp = {2009.06.18},
}

@Article{Vanston08tipsTechAdopt,
  author   = {Vanston, LK},
  title    = {Practical tips for forecasting new technology adoption},
  journal  = {Telektronikk},
  year     = {2008},
  volume   = {3},
  number   = {4},
  pages    = {179--189},
  abstract = {The pattern by which new technology is adopted is reasonably well-understood and, assuming there
is data, there are mathematical models and methods to help forecast. However, many of the most
strategic forecasts involve not much data and lots of uncertainty. There are ‘big’ methods – alternate
scenarios, for example – to address such issues, but sometimes the practitioner needs to make a
good forecast quickly and with few resources. In the process, the same issues often come up. Some
examples: When will a new technology be introduced? Will anyone adopt it? If so, how many and how
quickly? What are the factors of success? How long before the technology is obsolete? Which of two
or more technologies will win? Each of these questions gives rise to other questions, the answers to
which enable a good forecast. Or, in some cases, the answers lead to the conclusion that a definitive
forecast would be premature. This paper provides examples, philosophy, and practical advice for
addressing these questions so that quick – and dirty – forecasts are long-lasting and beautiful.},
  comment  = {Read first?},
  file     = {:Vanston08tipsTechAdopt.pdf:PDF},
  url      = {http://tfi.com/pubs/forecasting-tips.pdf},
}

@Article{Eilers03calTemp2Dpspline,
  author    = {Eilers, Paul HC and Marx, Brian D},
  title     = {Multivariate calibration with temperature interaction using two-dimensional penalized signal regression},
  journal   = {Chemometrics and intelligent laboratory systems},
  year      = {2003},
  volume    = {66},
  number    = {2},
  pages     = {159--174},
  abstract  = {The Penalized Signal Regression (PSR) approach to multivariate calibration (MVC) assumes a smooth vector of coefficients
for weighting a spectrum to predict the unknown concentration of a chemical component. B-splines and roughness penalties,
based on differences, are used to estimate the coefficients. In this paper, we extend PSR to incorporate a covariate like
temperature. A smooth surface on the wavelength?temperature domain is estimated, using tensor products of B-splines and
penalties along the two dimensions. A slice of this surface gives the vector of weights at an arbitrary temperature. We present
the theory and apply multi-dimensional PSR to a published data set, showing good performance. We also introduce and apply a
simplification based on a varying-coefficient model (VCM).

Keywords: Calibration transfer; Multivariate calibration; P-splines; Signal regression; Stability; Tensor products; Varying-coefficient models},
  comment   = {Has an analytical expression for the optimal 2D penalized spline. Requires 2 or 3 D grid search of smoothing coeffs.

* Eilers10SplnsKntsPnlties and Kagerer13splnLSQregrssnIntro are nice penalized spline intros.},
  file      = {Eilers03calTemp2Dpspline.pdf:Eilers03calTemp2Dpspline.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2015.04.15},
  url       = {http://www.sciencedirect.com/science/article/pii/S0169743903000297},
}

@Article{Meyer12cnstrPenSpln,
  author    = {Meyer, Mary C.},
  title     = {Constrained penalized splines},
  journal   = {Canadian Journal of Statistics},
  year      = {2012},
  volume    = {40},
  number    = {1},
  pages     = {190--206},
  issn      = {1708-945X},
  abstract  = {The penalized spline is a popular method for function estimation when the assumption of smoothness is valid. In this paper, methods for estimation and inference are proposed using penalized splines under additional constraints of shape, such as monotonicity or convexity. The constrained penalized spline estimator is shown to have the same convergence rates as the corresponding unconstrained penalized spline, although in practice the squared error loss is typically smaller for the constrained versions. The penalty parameter may be chosen with generalized cross-validation, which also provides a method for determining if the shape restrictions hold. The method is not a formal hypothesis test, but is shown to have nice large-sample properties, and simulations show that it compares well with existing tests for monotonicity. Extensions to the partial linear model, the generalized regression model, and the varying coefficient model are given, and examples demonstrate the utility of the methods. The Canadian Journal of Statistics 40: 190206; 2012 2012 Statistical Society of Canada},
  comment   = {Montonicity, concavity, etc. constraints of penalized splines, don ewith cone programming.  Not as comprehensive as Pya15shapeCnstrAddMdl but has R code here:

http://www.stat.colostate.edu/~meyer/penspl.htm},
  doi       = {10.1002/cjs.10137},
  file      = {Meyer12cnstrPenSpln.pdf:Meyer12cnstrPenSpln.pdf:PDF},
  keywords  = {Cone projection, convex regression, function estimation, model selection, monotone regression, MSC 2010: Primary 62G08, secondary 62G05},
  publisher = {John Wiley \& Sons, Inc.},
}

@Article{Atger99ensSkill,
  author    = {Atger, F.},
  title     = {The skill of ensemble prediction systems},
  year      = {1999},
  volume    = {127},
  number    = {9},
  pages     = {1941--1953},
  issn      = {0027-0644},
  abstract  = {The performance of ensemble prediction systems (EPSs) is investigated by examining the probability distribution
of 500-hPa geopotential height over Europe. The probability score (or half Brier score) is used to evaluate the
quality of probabilistic forecasts of a single binary event. The skill of an EPS is assessed by comparing its
performance, in terms of the probability score, to the performance of a reference probabilistic forecast. The reference
forecast is based on the control forecast of the system under consideration, using model error statistics to estimate
a probability distribution. A decomposition of the skill score is applied in order to distinguish between the two
main aspects of the forecast performance: reliability and resolution. The contribution of the ensemble mean and
the ensemble spread to the performance of an EPS is evaluated by comparing the skill score to the skill score of
a probabilistic forecast based on the EPS mean, using model error statistics to estimate a probability distribution.
The performance of the European Centre for Medium-Range Weather Forecasts (ECMWF) EPS is reviewed.
The system is skillful (with respect to the reference forecast) from 196 h onward. There is some skill from
148 h in terms of reliability. The performance comes mainly from the contribution of the ensemble mean. The
contribution of the ensemble spread is slightly negative, but becomes positive after a calibration of the EPS
standard deviation. The calibration improves predominantly the reliability contribution to the skill score. The
calibrated EPS is skillful from 172 h onward.
The impact of ensemble size on the performance of an EPS is also investigated. The skill score of the ECMWF
EPS decreases steadily with reducing numbers of ensemble members and the resolution is particularly affected.
The impact is mainly due to the ensemble spread contributing negatively to the skill. The ensemble mean
contribution to the skill decreases marginally when reducing the ensemble size up to 11 members.
The performance of the U.S. National Centers for Environmental Prediction (NCEP) EPS is also reviewed.
The NCEP EPS has a lower skill score (vs a reference forecast based on its control forecast) than the ECMWF
EPS especially in terms of reliability. This is mainly due to the smaller spread of the NCEP EPS contributing
negatively to the skill. On the other hand, the NCEP and ECMWF ensemble means contribute similarly to the
skill. As a consequence, the performance of the two systems in terms of resolution is comparable.
The performance of a poor man?s EPS, consisting of the forecasts of different NWP centers, is discussed.
The poor man?s EPS is more skillful than either the ECMWF EPS or the NCEP EPS up to 1144 h, despite a
negative contribution of the spread to the skill score. The higher skill of the poor man?s EPS is mainly due to
a better resolution.},
  booktitle = {Monthly Weather Review},
  comment   = {Effect of ensemble size of forecast skill and resolution, also an application of skill score decomposition (which distinguished between reliability and resolution).

Highly cited, but maybe there's something better in an overview paper?},
  doi       = {10.1175/1520-0493(1999)127%3C1941:TSOEPS%3E2.0.CO;2},
  file      = {Atger99ensSkill.pdf:Atger99ensSkill.pdf:PDF},
  groups    = {Ensemble, Test, CitaviImport1, doReadNonWPV_2},
  keywords  = {skill score, resolution, ensemble forecast, reliability, Brier skill score},
  ncite     = {129},
  owner     = {sotterson},
  timestamp = {2013.09.26},
}

@InProceedings{Houle09sharedKNNdimCurs,
  author    = {Houle, Michael E and Kriegel, Hans-Peter and Kr{\"o}ger, Peer and Schubert, Erich and Zimek, Arthur},
  title     = {Can shared-neighbor distances defeat the curse of dimensionality?},
  booktitle = {Proceedings of the 22\textsuperscript{nd} International Conference on Scientific and Statistical Database Management},
  year      = {2009},
  abstract  = {The performance of similarity measures for search, indexing, and data mining applications tends to degrade rapidly as the dimensionality of the data increases. The effects of the so-called ?curse of dimensionality? have been studied by researchers for data sets generated according to a single data distribution. In this paper, we study the effects of this phenomenon on different similarity measures for multiply-distributed data. In particular, we assess the performance of shared-neighbor similarity measures, which are secondary similarity measures based on the rankings of data objects induced by some primary distance measure. We find that rank-based similarity measures can result in more stable performance than their associated primary distance measures.},
  comment   = {Shared neighbhor distance seems to help w/ high dim but I downloaded this b/c it has equations for common ways of converting a similarity metric into a distance (eqs. 3-5)},
  doi       = {10.1007/978-3-642-13818-8_34},
  file      = {Houle09sharedKNNdimCurs.pdf:Houle09sharedKNNdimCurs.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.03.28},
}

@Article{Wharton12atmStabWindPow,
  author    = {Wharton, Sonia and Lundquist, Julie K},
  title     = {Atmospheric stability affects wind turbine power collection},
  journal   = {Environmental Research Letters},
  year      = {2012},
  volume    = {7},
  number    = {1},
  pages     = {014005},
  abstract  = {The power generated by a wind turbine largely depends on the wind speed. During time periods with identical hub-height wind speeds but different shapes to the wind profile, a turbine will produce different amounts of power. This variability may be induced by atmospheric stability, which affects profiles of mean wind speed, direction and turbulence across the rotor disk. Our letter examines turbine power generation data, segregated by atmospheric stability, in order to investigate power performance dependences at a West Coast North American wind farm. The dependence of power on stability is clear, regardless of whether time periods are segregated by three-dimensional turbulence, turbulence intensity or wind shear. The power generated at a given wind speed is higher under stable conditions and lower under strongly convective conditions: average power output differences approach 15\%. Wind energy resource assessment and day ahead power forecasting could benefit from increased accuracy if atmospheric stability impacts were measured and appropriately incorporated in power forecasts, e.g., through the generation of power curves based on a range of turbulence regimes.
Keywords: wind turbines, wind power, atmospheric stability, wind shear, turbulence},
  comment   = {Turbulence parameters can predict power output variances of up to 15\%. Discovered by binning power production by the normal stuff plus turbulence (three-dimensional turbulence, turbulence intensity or wind shear. )

Advocates accurately measuring stability and incorporating them into power forecasts.},
  file      = {Wharton12atmStabWindPow.pdf:Wharton12atmStabWindPow.pdf:PDF},
  owner     = {sotterson},
  publisher = {IOP Publishing},
  timestamp = {2014.11.02},
  url       = {http://iopscience.iop.org/1748-9326/7/1/014005},
}

@Article{Li03windRecurrMLP,
  author    = {Li, S.},
  title     = {Wind power prediction using recurrent multilayer perceptron neural networks},
  year      = {2003},
  volume    = {4},
  month     = jul,
  pages     = {2325--2330},
  doi       = {10.1109/PES.2003.1270992},
  abstract  = {The power generated by the wind changes rapidly because of the continuous fluctuation of wind speed and direction. It is important for the power industry to have the capability to predict the power produced by the wind for the power management and control. In this paper, temporal characteristics of wind power generation are studied and recurrent multilayer perceptron (RMLP) neural networks are used to predict the power. The extended Kalman filter based backpropagation through time algorithm is used to train the RMLP networks. The paper demonstrates the RMLP network solution for the power prediction, and show that the RMLPs can be used to predict the wind power in changing wind conditions.},
  file      = {Li03windRecurrMLP.pdf:Li03windRecurrMLP.pdf:PDF;Li03windRecurrMLP.pdf:Li03windRecurrMLP.pdf:PDF},
  journal   = {Power Engineering Society General Meeting, IEEE},
  keywords  = { Kalman filters, electricity supply industry, multilayer perceptrons, power control, power markets, recurrent neural nets, wind power, wind power plants Kalman filter, neural networks, power control, power industry, power management, recurrent multilayer perceptron, wind power generation, wind power prediction},
  owner     = {scotto},
  timestamp = {2008.07.06},
}

@InProceedings{Louie10evalProbWindPow,
  author    = {Louie, Henry},
  title     = {Evaluation of probabilistic models of wind plant power output characteristics},
  booktitle = {Probabilistic Methods Applied to Power Systems (PMAPS), 2010 IEEE 11\textsuperscript{th} International Conference on},
  year      = {2010},
  pages     = {442--447},
  abstract  = {The power output by weather-driven renewable resources such as wind energy conversion systems can be appropriately described as being stochastic. To manage these resources, probabilistic models of wind power are being increasingly employed by power system stakeholders in applications such as stochastic unit-commitment programs and wind power forecast systems. This paper evaluates probabilistic models-specifically the probability density functions-of aggregate wind plant power output and conditional and unconditional variations of aggregate wind plant power output. The parameters of the models are fit to historical aggregate wind plant power data from three large North American systems. Parametric and non-parametric evaluations of the suitability of the models are performed in the form of $\chi$\textsuperscript{2} goodness-of-fit tests and through the inspection of probability plots and histograms. It is shown that Beta distributions are appropriate models for the aggregate power output and Laplace distributions are appropriate models for wind power variability. Conditional wind power variation follows a generalized extreme value distribution.},
  comment   = {Wind power distributions (over time) for large aggregated systems (BPA, ERCOT, MISO). For conditioning on bins of wind power, the GEV best for hour-ahead "conditional"; beta best for long-term stats (unconditional dist); Laplace best for modelling "variability", uncondition 1-hour persistence forecast error; Normal, Gamma and Weibull don't work
* eval done w/ chi-square test
* normal fails partly b/c wind speeds aren't independent, making central limit theorem fail.

Meanings of the types of forecasts

Unconditional
* fit a distribution to the one hour normalized power average
* Best fitting: Gaussian
 -- a surprise since Gaussians would produce negative wind power and power > 1 ??

Variability
* do an hour ahead persistence forecast
* fit an unconditional distribution
* Best fitting: Laplace

Conditional (hour ahead persistence forecast)
* do an hour ahead persistence forecast
* bin current wind power like: AlAwami09statCharWindPowFrcst
* fit a distribution of persistence errors for forecasts associated w/ each current bin
* best fitting dist: GEV
* NOTE: AlAwami09statCharWindPowFrcst fournd that beta fit better for low power bins
 -- shouldn't these guys have made the same check?
 -- their goodness of fit criteria (chi-squared) was more "principled" than in AlAwami*},
  doi       = {10.1109/PMAPS.2010.5528963},
  file      = {Louie10evalProbWindPow.pdf:Louie10evalProbWindPow.pdf:PDF},
  groups    = {Read, ErrDistProps, doReadWPV_1},
  owner     = {sotterson},
  timestamp = {2014.07.18},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5528963},
}

@Article{Kleissl14curStateArtSolFrcst,
  author    = {Jan Kleissl},
  title     = {Current State of the Art in Solar Forecasting},
  journal   = {Final Report. California Renewable Energy Forecasting, Resource Data and Mapping. University of California, Appendix A.},
  year      = {2014},
  abstract  = {The power output delivered from a photovoltaic module highly depends on the amount of irradiance,
which reaches the solar cells. Many factors determine the ideal output or optimum yield in a photovoltaic
module. However, the environment is one of the contributing parameters which directly affect the
photovoltaic performance. The authors review and evaluate key contributions to the understanding,
performance effects, and mitigation of power loss due to soiling on a solar panel. Electrical characteristics
of PV (Voltage and current) are discussed with respect to shading due to soiling. Shading due to soiling is
divided in two categories, namely, soft shading such as air pollution, and hard shading which occurs
when a solid such as accumulated dust blocks the sunlight. The result shows that soft shading affects the
current provided by the PV module, but the voltage remains the same. In hard shading, the performance
of the PV module depends on whether some cells are shaded or all cells of the PV module are shaded. If
some cells are shaded, then as long as the unshaded cells receive solar irradiance, there will be some
output although there will be a decrease in the voltage output of the PV module. This study also present a
few cleaning method to prevent from dust accumulation on the surface of solar arrays.},
  comment   = {CPV production is highly correlated to DNI. DNI is impacted by phenomena
that are very difficult to forecast such as cirrus clouds, wild fires, dust storms, and episodic air
pollution events which can reduced DNI by up to 30% on otherwise cloud-free days. Water
vapor, which is also an important determinant of DNI, is t},
  file      = {Kleissl14curStateArtSolFrcst.pdf:Kleissl14curStateArtSolFrcst.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.29},
  url       = {http://uc-ciee.org/all-documents/a/451/113/nested},
}

@InProceedings{Marzban08quantRgrsn,
  author    = {Matheny, A. and Goldgof, D.B.},
  title     = {Quantile Regression},
  booktitle = {19\textsuperscript{th} Conference on Probability and Statistics},
  year      = {2008},
  abstract  = {The prediction from most regression models - be it multiple regression, neural networks, trees, etc. -
is a point estimate of the conditional mean of a response (i.e., quantity being predicted), given a set of
predictors. However, the conditional mean measures only the ?center? of the conditional distribution of
the response. A more complete summary of the conditional distribution is provided by its quantiles. The
0.5 quantile (i.e., the median) can serve as a measure of the center, and the 0.9 quantile marks the value
of the response below which resides 90quantile and the 0.05 quantile serves as the 90thereby conveying
uncertainty. Quantiles arise naturally in environmental sciences. For example, one may desire to know
the lowest level (e.g., 0.1 quantile) of a river, given the amount of snowpack; or the highest temperature
(e.g., the 0.9 quantile), given cloud cover. Recent advances in computing allow the development
of regression models for predicting a given quantile of the conditional distribution, both parametrically
and nonparametrically. The general approach is called Quantile Regression, but the methodology (of
conditional quantile estimation) applies to any statistical model, be it multiple regression, support vector
machines, or random forests. In this talk, the principles of quantile regression are reviewed and the
methodology is illustrated through several examples. The technique and the examples display many of
the features common in both machine learning and statistics.},
  comment   = {Friendly overview of QR with lots of meteorological example plots. Might be good as an inspiration for a tutorial.},
  file      = {Marzban08quantRgrsn.pdf:Marzban08quantRgrsn.pdf:PDF},
  groups    = {Read, PointDerived, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2013.10.23},
}

@Article{Kibria06matrixTdistAppInf,
  author    = {B.M. Golam Kibria},
  title     = {The matrix-t distribution and its applications in predictive inference},
  journal   = {Journal of Multivariate Analysis},
  year      = {2006},
  volume    = {97},
  number    = {3},
  pages     = {785 - 795},
  issn      = {0047-259X},
  abstract  = {The predictive distributions of the future responses and regression matrix under the multivariate elliptically contoured distributions are derived using structural approach. The predictive distributions are obtained as matrix-t which are identical to those obtained under matrix normal and matrix-t distributions. This gives inference robustness with respect to departures from the reference case of independent sampling from the matrix normal or dependent but uncorrelated sampling from matrix-t distributions. Some successful applications of matrix-t distribution in the field of spatial prediction have been addressed.},
  comment   = {Might be good for parsimoniously modeling time dependence of a multivariate signal the way that the matrix normal distribution is.  Also, maybe good for mixtures of t-distributions the way matrix normal distribution is.},
  doi       = {http://dx.doi.org/10.1016/j.jmva.2005.08.001},
  file      = {:papers\\Kibria06matrixTdistAppInf.pdf:PDF},
  keywords  = {Elliptically contoured distribution, Matrix-, Multivariate linear model, Predictive distribution, Robustness, Spatial prediction, Tolerance region},
  owner     = {sotterson},
  timestamp = {2017.08.22},
  url       = {http://www.sciencedirect.com/science/article/pii/S0047259X05001260},
}

@Patent{Wegerich13predCondMonSmartSigPtnt,
  nationality = {European Union},
  number      = {EP 2 015 186 B1},
  year        = {2013},
  yearfiled   = {2002},
  author      = {Wegerich, Stephan W. and Wolosewicz, Andre and Pipke, R. M.},
  title       = {Diagnostic systems and methods for predictive condition monitoring},
  language    = {English},
  assignee    = {Smartsignal Corporation},
  address     = {Lisle, IL 60532 (US)},
  type        = {patenteu},
  day         = {13},
  dayfiled    = {11},
  month       = mar,
  monthfiled  = jan,
  abstract    = {The present invention relates generally to the field of early detection and diagnosis of incipient machine failure
or process upset. More particularly, the invention is directed to model-based monitoring of processes and machines,
and experience-based diagnostics.},
  comment     = {GE (smartsignal) patent.  Peter of STEAG considers this his strongest competitor.},
  file        = {:Wegerich13predCondMonSmartSigPtnt.pdf:PDF},
}

@Article{Heim07spaceVaryCoeffs3D,
  author    = {Heim, S and Fahrmeir, L and Eilers, Paul HC and Marx, Brian D},
  title     = {{3D} space-varying coefficient models with application to diffusion tensor imaging},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {2007},
  volume    = {51},
  number    = {12},
  pages     = {6212--6228},
  abstract  = {The present methodological development and the primary application ?eld origi-
nate from di?usion tensor imaging (DTI), a powerful nuclear magnetic resonance
technique which enables the quanti?cation of microscopical tissue properties. The
current analysis framework of separate voxelwise regressions is reformulated as a 3d
space-varying coe?cient model (SVCM) for the entire set of di?usion tensor images
recorded on a 3d voxel grid. The SVCM uni?es the three-step cascade of standard
data processing (voxelwise regression, smoothing, interpolation) into one framework
based on B-spline basis functions. Thereby strength is borrowed from spatially cor-
related voxels to gain a regularization e?ect right at the estimation stage. Two
SVCM variants are conceptualized: a full tensor product approach and a sequential
approximation, rendering the SVCM numerically and computationally feasible even
for the huge dimension of the joint model in a realistic setup. A simulation study
shows that both approaches outperform the standard method of voxelwise regression with subsequent regularization. Application of the fast sequential method to real DTI data demonstrates the inherent ability to increase the grid resolution by
evaluating the incorporated basis functions at intermediate points. The resulting
continuous regularized tensor ?eld may serve as basis for multiple applications, yet,
ameloriation of local adaptivity is desirable.
Key words: Di?usion tensor, Brain imaging, P-splines, Varying coe?cient model},
  comment   = {3D varying coeff model via 3d tensor psplines},
  file      = {Heim07spaceVaryCoeffs3D.pdf:Heim07spaceVaryCoeffs3D.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.11.09},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167947307000035},
}

@Article{LopezLopez14altConfigQRnoCross,
  author    = {L{\'o}pez L{\'o}pez, P and Verkade, JS and Weerts, AH and Solomatine, DP},
  title     = {Alternative configurations of quantile regression for estimating predictive uncertainty in water level forecasts for the upper Severn River: a comparison},
  journal   = {Hydrology and Earth System Sciences Discussions, 11, 2014},
  year      = {2014},
  abstract  = {The present study comprises an inter-comparison of different con?gurations of a sta-
tistical post-processor that is used to estimate predictive hydrological uncertainty. It
builds on earlier work by Weerts et al. (2011, herinafter referred to as WWV2011),
5 who used the Quantile Regression technique to estimate predictive hydrological un-
certainty using a deterministic water level forecast as a predictor. The various con?g-
urations are designed to address two issues with the WWV2011 implementation: (i)
quantile crossing, which causes non-strictly rising cumulative predictive distributions,
and (ii) the use of linear quantile models to describe joint distributions that may not be
10 strictly linear. Thus, four con?gurations were built: (i) the ?as is? implementation used
by WWV2011, (ii) a con?guration that implements a non-crossing quantile technique,
(iii) a con?guration where quantile models are built in Normal space after application
of the Normal Quantile Transform, and (iv) a con?guration that builds quantile model
separately on separate domains of the predictor. Using each, four re-forecasting series
15 of water levels at four teen stations in the Upper Severn River were established. The
quality of these four series was inter-compared using a set of graphical and numerical
veri?cation metrics. Intercomparison showed that reliability and sharpness vary across
con?gurations, but in none of the con?gurations do these two forecast quality aspects
improve simultaneously. Further analysis shows that skills in terms of Brier Skill Score,
20 mean Continuous Ranked Probability Skill Score and Relative Operating Characteristic
Score is very similar across the four con?gurations.},
  comment   = {Four quantile methods for reducing linear QR crossing. Perhaps they prefer Bondell10nonCrossQR but they say that no method is always better.},
  file      = {LopezLopez14altConfigQRnoCross.pdf:LopezLopez14altConfigQRnoCross.pdf:PDF},
  publisher = {European Geosciences Union},
  url       = {http://repository.tudelft.nl/assets/uuid:1929caca-bbf1-4344-ba1b-e01ed34e7976/Verkade_2014.pdf},
}

@Article{Pascual13extrmWindWvlts,
  author    = {A. Pascual and M.L. Mart{\'i}n and F. Valero and M.Y. Luna and A. Morata},
  title     = {Wintertime connections between extreme wind patterns in {Spain} and large-scale geopotential height field},
  journal   = {Atmospheric Research},
  year      = {2013},
  volume    = {122},
  number    = {0},
  pages     = {213--228},
  issn      = {0169-8095},
  abstract  = {The present study is focused on the study of the variability and the most significant wind speed patterns in Spain during the winter season analyzing as well connections between the wind speed field and the geopotential height at 1000Pa over an Atlantic area. The daily wind speed variability is investigated by means of principal components using wind speed observations. Five main modes of variation, accounting 66\% of the variance of the original data, have been identified, highlighting their differences in the Spanish wind speed behavior. Connections between the wind speeds and the large-scale atmospheric field were underlined by means of composite maps. Composite maps were built up to give an averaged atmospheric circulation associated with extreme wind speed variability in Spain. Moreover, the principal component analysis was also applied to the geopotential heights, providing relationships between the large-scale atmospheric modes and the observational local wind speeds. Such relationships are shown in terms of the cumulated frequency values of wind speed associated with the extreme scores of the obtained large-scale atmospheric modes, showing those large-scale atmospheric patterns more dominant in the wind field in Spain.},
  comment   = {For regime clustering/detection? Something like the work on convection cells I saw at DTU Risoe.

Could be an input to a probabilistic forecast, since it has something to do with "extreme scores"},
  doi       = {10.1016/j.atmosres.2012.10.033},
  file      = {Pascual13extrmWindWvlts.pdf:Pascual13extrmWindWvlts.pdf:PDF},
  groups    = {PointDerived, doReadWPV_2},
  keywords  = {Daily mean wind speed},
  owner     = {sotterson},
  timestamp = {2013.03.13},
  url       = {http://www.sciencedirect.com/science/article/pii/S0169809512003870},
}

@InProceedings{Chen15stockPredRNNlstm,
  author    = {K. Chen and Y. Zhou and F. Dai},
  title     = {A LSTM-based method for stock returns prediction: A case study of {China} stock market},
  booktitle = {Proc. IEEE Int. Conf. Big Data (Big Data)},
  year      = {2015},
  pages     = {2823--2824},
  month     = oct,
  abstract  = {The presented paper modeled and predicted China stock returns using LSTM. The historical data of China stock market were transformed into 30-days-long sequences with 10 learning features and 3-day earning rate labeling. The model was fitted by training on 900000 sequences and tested using the other 311361 sequences. Compared with random prediction method, our LSTM model improved the accuracy of stock returns prediction from 14.3\% to 27.2\%. The efforts demonstrated the power of LSTM in stock market prediction in China, which is mechanical yet much more unpredictable.},
  doi       = {10.1109/BigData.2015.7364089},
  file      = {Chen15stockPredRNNlstm.pdf:Chen15stockPredRNNlstm.pdf:PDF},
  keywords  = {financial data processing, learning (artificial intelligence), stock markets, China stock market, LSTM-based method, long short-term memory, machine learning algorithms, stock returns prediction, Computational modeling, Computer architecture, Indexes, Predictive models, Recurrent neural networks, Stock markets, Training},
  owner     = {sotterson},
  timestamp = {2017.03.08},
}

@Article{Jayram08aggProbStrms,
  author     = {Jayram, T. S. and McGregor, Andrew and Muthukrishnan, S. and Vee, Erik},
  title      = {Estimating Statistical Aggregates on Probabilistic Data Streams},
  journal    = {ACM Trans. Database Syst.},
  year       = {2008},
  volume     = {33},
  number     = {4},
  pages      = {261--2630},
  month      = dec,
  issn       = {0362-5915},
  abstract   = {The probabilistic stream model was introduced by Jayram et al. [2007]. It is a generalization of the data stream model that is suited to handling probabilistic data, where each item of the stream represents a probability distribution over a set of possible events. Therefore, a probabilistic stream determines a distribution over a potentially exponential number of classical deterministic streams, where each item is deterministically one of the domain values.

We present algorithms for computing commonly used aggregates on a probabilistic stream. We present the first one pass streaming algorithms for estimating the expected mean of a probabilistic stream. Next, we consider the problem of estimating frequency moments for probabilistic data. We propose a general approach to obtain unbiased estimators working over probabilistic data by utilizing unbiased estimators designed for standard streams. Applying this approach, we extend a classical data stream algorithm to obtain a one-pass algorithm for estimating F2, the second frequency moment. We present the first known streaming algorithms for estimating F0, the number of distinct items on probabilistic streams. Our work also gives an efficient one-pass algorithm for estimating the median, and a two-pass algorithm for estimating the range.},
  acmid      = {1412338},
  articleno  = {26},
  comment    = {The epsilon-approximate median can be used to approximate any quantile of the probabilsitic aggregate. There is also a SUM operator, which when combined, can somehow compute the quantile of the sum? Several other aggregates also discussed.

The idea was to use this for probabilistic upscaling, somehow.},
  doi        = {10.1145/1412331.1412338},
  file       = {Jayram08aggProbStrms.pdf:Jayram08aggProbStrms.pdf:PDF},
  groups     = {Upscaling (prob), doReadNonWPV_1},
  issue_date = {November 2008},
  keywords   = {OLAP, Probabilistic streams, frequency moments, mean, median},
  location   = {New York, NY, USA},
  numpages   = {30},
  owner      = {sotterson},
  publisher  = {ACM},
  timestamp  = {2014.03.21},
}

@Article{Roulston02frcstInfoEval,
  author    = {Roulston, M. S. and Smith, L. A.},
  title     = {Evaluating probabilistic forecasts using information theory},
  year      = {2002},
  volume    = {130},
  number    = {6},
  pages     = {1653--1660},
  issn      = {0027-0644},
  abstract  = {The problem of assessing the quality of an operational forecasting system that produces probabilistic forecasts is addressed using information theory. A measure of the quality of the forecasting scheme, based on the amount of a data compression it allows, is outlined. This measure, called ignorance, is a logarithmic scoring rule that is a modified version of relative entropy and can be calculated for real forecasts and realizations. It is equivalent to the expected returns that would be obtained by placing bets proportional to the forecast probabilities. Like the cost-loss score, ignorance is not equivalent to the Brier score, but, unlike cost-loss scores, ignorance easily generalizes beyond binary decision scenarios. The use of the skill score is illustrated by evaluating the ECMWF ensemble forecasts for temperature at London's Heathrow airport.},
  booktitle = {Monthly Weather Review},
  comment   = {Info theory eval of forecasted prob. dist; is equivalent to placing bets based on density (why not just calculate that number then?). Generalizes to non-binary decisions, so is possibly useful. Maybe the "equivalent to" is a binary decision?

Criticizes Brier score b/c it has the same value for pairs of the ignorance score, and also because it would lose money of used for betting, unlike w/ ignorance.

Proposed expected ignorance could be an alternative to CRPS

Highly cited.

Maybe worthwhile for optimizing day ahead and hour ahead forecast bidding?

Ignorance score was better at showing a difference in QR and DR in: Vogt18multiDistLrnAnomDet},
  file      = {Roulston02frcstInfoEval.pdf:Roulston02frcstInfoEval.pdf:PDF},
  groups    = {Test, CitaviImport1, doReadNonWPV_1},
  keywords  = {probabilistic forecast},
  ncite     = {176},
  owner     = {sotterson},
  timestamp = {2013.09.26},
}

@Article{Saul03globLocLLE,
  author    = {Saul, Lawrence K and Roweis, Sam T},
  title     = {Think globally, fit locally: unsupervised learning of low dimensional manifolds},
  journal   = {The Journal of Machine Learning Research},
  year      = {2003},
  volume    = {4},
  pages     = {119--155},
  abstract  = {The problem of dimensionality reduction arises in many fields of information processing, including machine learning, data compression, scientific visualization, pattern recognition, and neural computation. Here we describe locally linear embedding (LLE), an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data. The data, assumed to be sampled from an underlying manifold, are mapped into a single global coordinate system of lower dimensionality. The mapping is derived from the symmetries of locally linear reconstructions, and the actual computation of the embedding reduces to a sparse eigenvalue problem. Notably, the optimizations in LLE---though capable of generating highly nonlinear embeddings---are simple to implement, and they do not involve local minima. In this paper, we describe the implementation of the algorithm in detail and discuss several extensions that enhance its performance. We present results of the algorithm applied to data sampled from known manifolds, as well as to collections of images of faces, lips, and handwritten digits. These examples are used to provide extensive illustrations of the algorithm's performance---both successes and failures---and to relate the algorithm to previous and ongoing work in nonlinear dimensionality reduction.},
  comment   = {Local linear dimension reduction paper with over a thousand cites. Uses KNN somehow. Has Matlab

The Matlab Toolbox for Dimensionality Reduction:
http://www.mathworks.com/matlabcentral/linkexchange/links/1626-the-matlab-toolbox-for-dimensionality-reduction},
  file      = {Saul03globLocLLE.pdf:Saul03globLocLLE.pdf:PDF},
  owner     = {sotterson},
  publisher = {JMLR. org},
  timestamp = {2014.04.21},
  url       = {http://dl.acm.org/citation.cfm?id=945372},
}

@Article{Wong14extrmValRgrssnPkOvThrsh,
  author    = {Wong, Tong Siu Tung and Li, Wai Keung},
  title     = {Extreme values identification in regression using a peaks-over-threshold approach},
  journal   = {Journal of Applied Statistics},
  year      = {2014},
  volume    = {42},
  number    = {3},
  pages     = {566--576},
  month     = nov,
  issn      = {0266-4763},
  abstract  = {The problem of heavy tail in regression models is studied. It is proposed that regression models are estimated by a standard procedure and a statistical check for heavy tail using residuals is conducted as a tool for regression diagnostic. Using the peaks-over-threshold approach, the generalized Pareto distribution quantifies the degree of heavy tail by the extreme value index. The number of excesses is determined by means of an innovative threshold model which partitions the random sample into extreme values and ordinary values. The overall decision on a significant heavy tail is justified by both a statistical test and a quantile?quantile plot. The usefulness of the approach includes justification of goodness of fit of the estimated regression model and quantification of the occurrence of extremal events. The proposed methodology is supplemented by surface ozone level in the city center of Leeds.
The problem of heavy tail in regression models is studied. It is proposed that regression models are estimated by a standard procedure and a statistical check for heavy tail using residuals is conducted as a tool for regression diagnostic. Using the peaks-over-threshold approach, the generalized Pareto distribution quantifies the degree of heavy tail by the extreme value index. The number of excesses is determined by means of an innovative threshold model which partitions the random sample into extreme values and ordinary values. The overall decision on a significant heavy tail is justified by both a statistical test and a quantile?quantile plot. The usefulness of the approach includes justification of goodness of fit of the estimated regression model and quantification of the occurrence of extremal events. The proposed methodology is supplemented by surface ozone level in the city center of Leeds.},
  booktitle = {Journal of Applied Statistics},
  comment   = {doi: 10.1080/02664763.2014.978843
Review:
Both a quality of fit method and a way to estimate heavy tails in quantile regression. Some kind of sampling approach that makes it data driven.},
  doi       = {10.1080/02664763.2014.978843},
  file      = {Wong14extrmValRgrssnPkOvThrsh.pdf:Wong14extrmValRgrssnPkOvThrsh.pdf:PDF},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2015.04.17},
}

@Article{Benedetti10scoreFrcstVerifRare,
  author        = {Benedetti, Riccardo},
  title         = {Scoring rules for forecast verification},
  journal       = {Monthly Weather Review},
  year          = {2010},
  volume        = {138},
  number        = {1},
  pages         = {203--211},
  __markedentry = {[Scott:1]},
  abstract      = {The problem of probabilistic forecast verification is approached from a theoretical point of view starting from three basic desiderata: additivity, exclusive dependence on physical observations (“locality”), and strictly proper behavior. By imposing such requirements and only using elementary mathematics, a univocal measure of forecast goodness is demonstrated to exist. This measure is the logarithmic score, based on the relative entropy between the observed occurrence frequencies and the predicted probabilities for the forecast events. Information theory is then used as a guide to choose the scoring-scale offset for obtaining meaningful and fair skill scores. Finally the Brier score is assessed and, for single-event forecasts, its equivalence to the second-order approximation of the logarithmic score is shown.

The large part of the presented results are far from being new or original, nevertheless their use still meets with some resistance in the weather forecast community. This paper aims at providing a clear presentation of the main arguments for using the logarithmic score.
Keywords: Forecast verification; Probability forecast/models; Quality assurance/controls },
  comment       = {
* Properties considered essential for a scoring rule (desiderata)
   1. Additivity:  the score of a new forecast result adds (averages, I think) onto the total score of the past results, with the same weight, I think
   2. Locality: The score depends only on the predicted probability of the actual outcome -- the rest of the distribution doesn't matter.  This means that, the Brier Score and CRPS are non-local.
   3. Propriety: the score is "proper," meaning that the forecaster gets the best score only if he forecasts the probability he believes to be correct.

* The three desiderata ==> are only satisfied b a logarithmic scoring rule
   - for both the single event and the sequence of events.
* Skill scores show the relationship between a "zero level" entropy and forecast/truth cross-entropy
   - there's a climatology entropy
   - there's also a max ignorance entropy, where the prob of an event is 1/m, where there are m possibilties




For more about Brier Score problems: Jewson04problemBrierScore},
  file          = {:Benedetti10scoreFrcstVerifRare.pdf:PDF},
  url           = {https://journals.ametsoc.org/doi/full/10.1175/2009MWR2945.1},
}

@Article{Andric96polarWvlt,
  author    = {O. Andric and C. Johnston and N. Erdol},
  title     = {Wavelets in polar coordinates},
  journal   = {Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {1996},
  volume    = {3},
  pages     = {1507--1510},
  abstract  = {The problems encountered in the development and implementation of two-dimensional orthonormal wavelet bases and their filter banks in polar coordinates are addressed. These wavelets and filter banks have possible applications in processing signals that are collected by sensors working in the polar coordinate system, such as biomedical and radar generated signals. Wavelet bases are developed in the convenient and familiar surrounding of the rectangular plane, and the theory is transported to polar plane. Corresponding filter banks are developed and the implementation of wavelet analysis in the polar plane is discussed. Examples are provided.},
  comment   = {R/theta wavelet, accomodates dialation in theta. Can use for wind spd/dir basis function? It think this would smooth on diagonals of R and theta, unlike a tensor product of 1-D wavelets, which would only smooth along the R or theta axis. No citations of this paper that I can find, however},
  doi       = {10.1109/ICASSP.1996.543949},
  file      = {Andric96polarWvlt.pdf:Andric96polarWvlt.pdf:PDF},
  isbn      = {0-7803-3192-3},
  location  = {Los Alamitos, CA, USA},
  owner     = {scotto},
  publisher = {IEEE Computer Society},
  timestamp = {2010.08.15},
}

@InProceedings{Rahim08dynThermPowLines,
  author    = {Azlan Abdul Rahim and Mohd Fahmi Hashim},
  title     = {Pilot Implementation Of The Dynamic Thermal Rating Technology In Tnb Transmission Lines},
  booktitle = {Conference of Electric Power Supply Industry},
  year      = {2008},
  abstract  = {The project objective is to determine the dynamic thermal current rating or ampacity of a transmission line in TNB, based on the actual measured service and weather conditions. This is achieved by means of a Dynamic Thermal Current Rating (DTCR) system. The study lines are the 275kV transmission lines. The system comprises of 2 laser distance measurement (LDM) sensors for measuring the conductor ground clearance and a weather station for measuring the wind speed, wind direction, ambient temperature, relative humidity and solar radiation. The data is transmitted to the on-line DTCR system central processor located at National Load Despatch Centre (NLDC) via GSM communication. The DTCR system central processor comprises of the EPRI DTCR V4.0 software for calculating the conductor dynamic thermal current rating},
  comment   = {laser imaged sag + weather station + sofware ==> 157\% higher thermal rating that standard static rating},
  file      = {Rahim08dynThermPowLines.pdf:Rahim08dynThermPowLines.pdf:PDF;Rahim08dynThermPowLines.pdf:Rahim08dynThermPowLines.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.02.23},
  url       = {http://www.cepsi2008.org/},
}

@Article{Vassiliou10linDistLagRconv,
  author    = {Vassiliou, E.E. and Demetriou, .C.},
  title     = {A linearly distributed lag estimator with r-convex coefficients},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {2010},
  volume    = {54},
  number    = {11},
  pages     = {2836--2849},
  abstract  = {The purpose of linearly distributed lag models is to estimate, from time series data, values of the dependent variable by incorporating prior information of the independent variable. A least-squares calculation is proposed for estimating the lag coefficients subject to the condition that the rth differences of the coefficients are non-negative, where r is a prescribed positive integer. Such priors do not assume any parameterization of the coefficients, and in several cases they provide such an accurate representation of the prior knowledge, so as to compare favorably to established methods. In particular, the choice of the prior knowledge parameter r gives the lag coefficients interesting special features such as monotonicity, convexity, convexity/concavity, etc. The proposed estimation problem is a strictly convex quadratic programming calculation, where each of the constraint functions depends on r+1 adjacent lag coefficients multiplied by the binomial numbers with alternating signs that arise in the expansion of the rth power of (1-1). The most distinctive feature of this calculation is the Toeplitz structure of the constraint coefficient matrix, which allows the development of a special active set method that is faster than general quadratic programming algorithms. Most of this efficiency is due to reducing the equality constrained minimization calculations, which occur during the quadratic programming iterations, to unconstrained minimization ones that depend on much fewer variables. Some examples with real and simulated data are presented in order to illustrate this approach.},
  comment   = {Distributed lags that are forced to be montonic, convex, concave, etc. - important for some applications but not for my wind stuff right now - related work by Demetriou on montonic optimization might be relevant to wind power spinning reserve cost estimator},
  file      = {Vassiliou10linDistLagRconv.pdf:Vassiliou10linDistLagRconv.pdf:PDF},
  keywords  = { Almon polynomial Approximation Consumption Difference Distributed lag model Least squares r-convexity Regression Quadratic programming Time series Toeplitz matrix},
  owner     = {scot},
  timestamp = {2010.08.05},
  url       = {http://econpapers.repec.org/RePEc:eee:csdana:v:54:y:2010:i:11:p:2836-2849},
}

@Article{Trevor04LAR,
  author    = {Bradley Efron Trevor and Trevor Hastie and Lain Johnstone and Robert Tibshirani},
  title     = {Least Angle Regression},
  journal   = {Annals of Statistics},
  year      = {2004},
  volume    = {32},
  number    = {2},
  pages     = {407--499},
  abstract  = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
  comment   = {The least angle regression (LAR) algorithm for solving the Lasso. Much more efficient than straight Lasso.

* Trevor03LARtechNote.pdf: text seems identical to journal paper, graphs clearer, no discussion
-- source: http://www-stat.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf (alghough date in pdf says 2003)
-- matlab code has vanilla implementation which uses eq. num.'s in this paper: http://www.mathworks.com/matlabcentral/fileexchange/23186 * Trevor04LARjournal.pdf: journal paper, with 40 or so pages of discussion after the main article but the graphs are hard to read},
  file      = {Tech Note, nice graphs, no dicussion:Trevor03LARtechNote.pdf:PDF;Journal Paper w/ Dicussions, bad graphs:Trevor04LAR.pdf:PDF;Trevor04LARjournal.pdf:Trevor04LARjournal.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.08.13},
}

@TechReport{Barquin11optimateTR,
  author      = {Juli{\'a}n Barqu{\'i}n and Luis Rouco and Enrique Rivero (Comillas)},
  title       = {Optimate: An {O}pen {P}latform to {T}est {I}ntegration in new {MA}rke{T} designs of massive intermittent {E}nergy sources dispersed in several regional power market},
  institution = {{REE}},
  year        = {2011},
  number      = {D2.2+D3.3.1+D3.3.2+D3.2},
  abstract    = {The purpose of this document is to identify the attributes of the structure and functioning of existing electricity markets and mechanisms in Europe in order to assess the appropriateness of the meta-model representation in OPTIMATE, to describe their detailed designs which reflect often sustainable issues currently taken into account in each country, and to identify rationales for possible evolution},
  comment     = {Juan Mi's recommended description of European energy markets. Markus Speckman (IWES) says it's already obsolete but still has good information.

Has a big table of how reserves, etc. are set, some of which are probabilistic e.g. Tertiariy reserve in Italy},
  file        = {Barquin11optimateTR.pdf:Barquin11optimateTR.pdf:PDF},
  groups      = {Use, doReadWPV_1},
  owner       = {sotterson},
  timestamp   = {2012.02.09},
}

@Article{Cannon11quantRgrsnNNprecip,
  author    = {Alex J. Cannon},
  title     = {Quantile regression neural networks: Implementation in R and application to precipitation downscaling},
  journal   = {Computers \& Geosciences},
  year      = {2011},
  volume    = {37},
  number    = {9},
  pages     = {1277--1284},
  issn      = {0098-3004},
  abstract  = {The qrnn package for R implements the quantile regression neural network, which is an artificial neural network extension of linear quantile regression. The model formulation follows from previous work on the estimation of censored regression quantiles. The result is a nonparametric, nonlinear model suitable for making probabilistic predictions of mixed discrete-continuous variables like precipitation amounts, wind speeds, or pollutant concentrations, as well as continuous variables. A differentiable approximation to the quantile regression error function is adopted so that gradient-based optimization algorithms can be used to estimate model parameters. Weight penalty and bootstrap aggregation methods are used to avoid overfitting. For convenience, functions for quantile-based probability density, cumulative distribution, and inverse cumulative distribution functions are also provided. Package functions are demonstrated on a simple precipitation downscaling task.},
  comment   = {Single quantile NN regression R package, with focus on censored regression. Several training tricks. Would need to add two-sided censoring to use for wind.

* can express arbitrary density function, doesn't assume a parametric one like a Gaussian
* monotonicity of QR means that it's good for censored regression (I didn't quite get this point)
* QRNN is normal linear regression if replace tanh() middle layer node w/ linear function.
* Several other flexible nonlinear QR mentioned too (p. 1283)
* non-differentiable () cost func. screws up normal NN training.

-- censoring within QR handled by "ramp" cost function
-- the usual tilted abs. value QR cost function
-- result is premature convergence
-- What did Andre (Baier11mqrnn) do?
* Solutions to non-differentiability
-- Huber norm smoothing of both cost fucntions
---- replace p(u) or r(u) w/ Huber Norm approx,
---- does L2 norm in the middle (so smooth where the L1 norm discontinuity occurs), and L1 norm outside.
-- Huber epsilon tuned by searching for min err function
---- Optimal? Instead chose by literally counting the quantile samples?
---- Output must be mapped to unit variance, zero mean so can used fixed eps. schedule

Two Regularization Approaches:
1. weight penalty (but this decreases nonlinearity).
2. bagging (bootstrapped ens. avg.) w/ special censoring treatment.

Tail treatment would have to be changed for wind power (needs to be two sided)
* For non-censoring, assumes upper/lower tail exponential decrease
-- not good for wind power, which has a lot of mass at 0 and 1 normalized power.
-- Is left and right censoring possible?
-- Bremnes04windLocQR doesn't censor, just preproc's input and then undoes the result (better?)

Optimization tests (not testing generalization on unseen data)
* R package benchmark algorithms: quantreg, rq and crq
* censored data tests use QVSS (Friederichs07censQRprecip), a hacked version of Gneiting07strictPropScore
* Does 5 restarts, just like our WPMS NN
* Save lowest error run, inviting overtraining (Does Razavi11newFormNN NN slope trick help here?)
* RESULT: Uncensored optimization about the same as baseline; Censored works better

Test to see if bagging helped:
* added 2 extra nodes to induce diversity by overtraining.
* The idea improves performance slightly.
* Choice for Huber espsion not tuned.

FORECASTING TEST
* cross-validation for nHidNode picking, and bagging to get an esemble average.
* different net (num. of hidden nodes) for each quantile. How avoid crossing?
* calibration and sharpness aren't specifically tested against the banchmarks but the quantile match to climatology is.

CROSSOVER
* no mention if it's a problem here but no explicity treatment, since separate NN for each quantile
* Mentioned ways to avoid crossover (not done), including one in R quantreg package (Chernozhukov10qrNoCross)
* Could have mentioned Dette08quantRgrsNonCross and others.
* Baier11mqrnn handles crossover by doing all taus at the same time inside a single neural net

RESULTS:
Binary precip. forecast: better than benchmark; quantile comparison: slightly better than benchmark

SOFTWARE
Homepage w/ more references
qrnn: R package for the quantile regression neural network model
http://www.eos.ubc.ca/~acannon/qrnn/},
  doi       = {10.1016/j.cageo.2010.07.005},
  file      = {Cannon11quantRgrsnNNprecip.pdf:Cannon11quantRgrsnNNprecip.pdf:PDF},
  groups    = {Read},
  keywords  = {Probabilistic},
  owner     = {sotterson},
  timestamp = {2013.10.17},
  url       = {http://www.sciencedirect.com/science/article/pii/S009830041000292X},
}

@Article{Belkin18modernMachLrnBiasVar,
  author   = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  title    = {Reconciling modern machine learning and the bias-variance trade-off},
  journal  = {arXiv preprint arXiv:1812.11118},
  year     = {2018},
  abstract = {The question of generalization in machine learning—how algorithms are able to learn predictors
from a training sample to make accurate predictions out-of-sample—is revisited in light of the recent
breakthroughs in modern machine learning technology. The classical approach to understanding gener-
alization is based on bias-variance trade-offs, where model complexity is carefully calibrated so that the
fit on the training sample reflects performance out-of-sample. However, it is now common practice to fit
highly complex models like deep neural networks to data with (nearly) zero training error, and yet these
interpolating predictors are observed to have good out-of-sample accuracy even for noisy data. How
can the classical understanding of generalization be reconciled with these observations from modern
machine learning practice?
In this paper, we bridge the two regimes by exhibiting a new “double descent” risk curve that extends
the traditional U-shaped bias-variance curve beyond the point of interpolation. Specifically, the curve
shows that as soon as the model complexity is high enough to achieve interpolation on the training
sample—a point that we call the “interpolation threshold”—the risk of suitably chosen interpolating
predictors from these models can, in fact, be decreasing as the model complexity increases, often below
the risk achieved using non-interpolating models. The double descent risk curve is demonstrated for a
broad range of models, including neural networks and random forests, and a mechanism for producing
this behavior is posited.},
  comment  = {How modern machine learning techniques get zero error on train, yet generalize well on new data.  Method is to way "over train"!  Also, nice plots

Old school ML
1. complex models e.g with number of parameters >= number of training points, will memorize ("interpolate"  in this paper) the training data unless heavily regularized
2. Interpolating the training data is bad because you will get bad performance on unseen test data

New school ML
* Anddrew Ng's talk said that it was regularization of super-complex deep networks that allowed good out of sample performance (Ng16nutsBoltsDeepLrn).  
* But there's more to it that that: Interpolation is actually good (double descent risk curve in first figure):
  - You actually want even MORE complexity thant is needed to Interpolate
  - just enough complexity to interpolate is a worst case, but the test error drops as you go further than that
  - you may get lower test error than you got in the Old School "sweet spot" between a too simple model, and the model just complex enough to interpolate
* interpolation threshold: the minimum number of model parameters at which the training data is memorized
   - LESSON of this paper: now keep making the model even more complex, and you may get lower error on test data
   - how far to go is not clear to me, but maybe you can again do CV on a chunk of the training data?

Why over-parameterized, super complex models work
* Occam's rule: simplest thing that fits the data is the best (references to a couple ML papers that say this in math)
* complex function classes contain functions with smaller norm that perfectly fit the data
* as you increase complexity, you decrease the fucntion norm (I think, or this is possible somehow, I don't understand this, actually)

Examples
* Random Fourier Features Neural nets
   - N random complex exponential projections of input vector (each called a "feature")
   - prediction is a linear weighting of these features
   - weight training
      -- the resulting function is the one that interpolates the training data w/ minimal functional norm
      -- I don't know how they do that -- some kinda ridge?  Maybe RFF references would explain?
   - can approximate a reproducing kernel Hilber Space (RHKS), like used by SVM
   - was originally used instead of kernels b/c if have a large # of training points, the model is still small, unlike a kernel which increases w/ npts * npts
   - Figure 2 shows the error peak at the interpolation threshold, and how the error decreases when get way more complex than that
* 2 layer NN, trained w/ SGD
   - SGD seems to be an example of a common NN training approach producing small norm functions: Refs [14, 16] say this, it seems  
   - Figure 3 clearly shows the double descent curve, w/ an error peak at the interpolation threshold, where npts == nParams, exactly
   - on MNIST classifiation problem, which as only 4K points
* Random Forests
   - Figure 4 shows the interpolation point on MNIST (10K points), w/ the following MSE drop afterwards
   - this is also true of L2 boosted trees (so works for XGBOOST?)

Why has nobody noticed this before?
1. classical work was done on linear settings -- not complex enough
2. classical work was in regularization, which PREVENTS interpolation, so you'd never see the double convergence curve
3. RFF models originally approximated kernels when there were too many points to make a kernel
    -- they did this to reduce computation
    -- making the model super complex is the opposite of what they were thinking
4. the interpolation peak is hard to see for NNs, especially when regularized 
    (Figure 9 of Appendix C is an example where they worked hard to make it visible)
5. NN training was typicall stopped by an early stopping critereon, which hid the benefits of super-training a super-complex model

Nice example plots in Appendices for many different algorithms (could put in talk).

Also: Wyner17adaboostRandFrstInterp has more plots for decision trees, and a slightly different argument},
  file     = {:Belkin18modernMachLrnBiasVar.pdf:PDF},
  url      = {https://arxiv.org/abs/1812.11118},
}

@Article{Biau16randFrstGuideTour,
  author   = {Biau, G{\'e}rard and Scornet, Erwan},
  title    = {A random forest guided tour},
  journal  = {TEST},
  year     = {2016},
  volume   = {25},
  number   = {2},
  pages    = {197--227},
  month    = {Jun},
  issn     = {1863-8260},
  abstract = {The random forest algorithm, proposed by L. Breiman in 2001, has been extremely successful as a general-purpose classification and regression method. The approach, which combines several randomized decision trees and aggregates their predictions by averaging, has shown excellent performance in settings where the number of variables is much larger than the number of observations. Moreover, it is versatile enough to be applied to large-scale problems, is easily adapted to various ad hoc learning tasks, and returns measures of variable importance. The present article reviews the most recent theoretical and methodological developments for random forests. Emphasis is placed on the mathematical forces driving the algorithm, with special attention given to the selection of parameters, the resampling mechanism, and variable importance measures. This review is intended to provide non-experts easy access to the main ideas.},
  comment  = {Good overview of the first Breiman, 2001 random forest algorithm.  How
it works, how to tune tweakable parameters, and why the algorithm works
(I skipped this last theoretical part, but I might want to go back to it
if I ever reread and try to comprehend Wyner17adaboostRandFrstInterp)

* Tutorial recommended by Olson18sensRndFrstProbKern
* Compare tuning advice here with Tang18whenRandFrstFail
* Expansion of this paper's ideas: Arlot16commentsRndFrstGuidTour

Basic Random Forest Idea: Bagging (bootstrap-aggregating)
* Boostrapping: Many decision trees are trained on different data (bootstrapped) w/ randomly chosen parmeters
  + can be with or without replacement
* Aggregating: decisions are either averaged (regression) or majority voted (classification)

Decision trees used in Random Forest ensemble
* Splitting
  + every node is split at each training step
  + candidate input variables for each split are chosen randomly, with the number chosen being a tweakable parameter, mtry
  + Split Critereon
    - Regression: CART Squared Error split critereon
      * for each input variable, find a splitting threshold which minimizes the RMSE if the prediction is made by separately averaging y values on either side of the split
      * split on the variable which reduces the RMSE the most at its optimal splitting threshold
    - Classification: CART Gini impurity split critereon
      * I didn't quite understand the equation given.
      * Vincent17giniPurity has a (relvant?) explanation
* Stopping
  + stop growing trees when node has <= nodesize points

Tuning of apparently most important parameters
* M: # trees
  + bigger M ==> more stability, more computation
  + No penalty for picking too many trees, only one for not picking enough [says one ref]
  + Latinne01limNtreesRandForest can pick M w/ smaller size, no loss in perf
* nodesize: max # points in leaf nodes
  + R library default for nodesize
    - classification: 1
    - regression: 5
  + Kruppa13cnsmrRiskProbRFnodesize can pick nodesize w/ simple alg
* mtry: # dimensions to test at each split
  + R library default for mtry
    - classification: mtry=sqrt(p); p=input dimension (recommended value in ref)
    - regression: mtry=p/3 (R package default)
  + [some refs] say make as large as computationally possible
  + but I think I read that for probabilistic forecasts, it must be tuned
  + Forest-RK [Bernard08forest-RKmtry] randomizes mtry, avoiding the choice.
    - this has been further improved in Bernard12dynRandForest
* a_n: number of bootstrap samples used to train each tree
  + R library defaults for a_n: a_n == total # training points
     Using the default, I guess you'd want to sample with replacement,
     then, so there would be at least some randomness in the training set
      (although note that Breiman04consistRandForest says that
      bootstrapping not that important on his simple empirical tests)
  +  no other advice than this, it seems

    Tutorial recommended by Olson18sensRndFrstProbKern

    Error estimation There's soine kind of out-of-bag error estimate
    that is run internally which composes a disjoint test and train set
    (for each tree?).  This part is not too clear.  But it's explained
    in Kruppa13cnsmrRiskProbRFnodesize
},
  day      = {01},
  doi      = {10.1007/s11749-016-0481-7},
  file     = {:Biau16randFrstGuideTour.pdf:PDF},
  url      = {https://doi.org/10.1007/s11749-016-0481-7},
}

@Article{Yang14knnClustPowCurv,
  author    = {Yang, Hejun and Hu, Bo and Yin, Lei and Chen, Ya and Liao, Qinglong},
  title     = {Clustering-interpolation Method and Its Application to Wind Turbine Generator Curve},
  journal   = {Elektronika ir Elektrotechnika},
  year      = {2014},
  volume    = {20},
  number    = {8},
  pages     = {13--19},
  abstract  = {The real-time operating wind turbine power curve
(WPC) of a wind turbine generator (WTG) is not completely
identical to a WPC provided by the manufacturer because of
various factors. In order to obtain an accurate WPC model that
can consider various factors, this paper improves a bisecting
k-means clustering algorithm. The improved clustering
algorithm is used for partitioning the measured data into a
certain number of groups, which can be expressed in their
centroids. The interpolation method based on the polynomial is
carried out for modelling a WPC of WTG. The modelled WPC is
applied to the reliability analysis of the generating systems with
a wind farm. The results show that the accuracy of the linear
interpolation is higher than that of quadratic interpolation and
cubic spline interpolation when there are a relatively large
number of clusters.
Index Terms?Wind turbine power curve, bisecting k-means
clustering, interpolation, wind farm, reliability analysis},
  comment   = {Generate power curve (directional, I think) w/ top-down KNN clustering and interpolation. Beats cubic spline. I haven't read this yet but should.},
  file      = {Yang14knnClustPowCurv.pdf:Yang14knnClustPowCurv.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.02},
  url       = {http://158.129.0.15/index.php/elt/article/view/5195},
}

@InProceedings{Bae10dimRedMDSinterp,
  author       = {Bae, Seung-Hee and Choi, Jong Youl and Qiu, Judy and Fox, Geoffrey C},
  title        = {Dimension reduction and visualization of large high-dimensional data via interpolation},
  booktitle    = {Proceedings of the 19\textsuperscript{th} ACM International Symposium on High Performance Distributed Computing},
  year         = {2010},
  pages        = {203--214},
  organization = {ACM},
  abstract     = {The recent explosion of publicly available biology gene sequences
and chemical compounds offers an unprecedented
opportunity for data mining. To make data analysis feasible
for such vast volume and high-dimensional scientific data, we
apply high performance dimension reduction algorithms. It
facilitates the investigation of unknown structures in a three
dimensional visualization. Among the known dimension reduction
algorithms, we utilize the multidimensional scaling
and generative topographic mapping algorithms to configure
the given high-dimensional data into the target dimension.
However, both algorithms require large physical memory as
well as computational resources. Thus, the authors propose
an interpolated approach to utilizing the mapping of only a
subset of the given data. This approach effectively reduces
computational complexity. With minor trade-off of approximation,
interpolation method makes it possible to process
millions of data points with modest amounts of computation
and memory requirement. Since huge amount of data are
dealt, we represent how to parallelize proposed interpolation
algorithms, as well. For the evaluation of the interpolated
MDS by STRESS criteria, it is necessary to compute symmetric
all pairwise computation with only subset of required
data per process, so we also propose a simple but efficient
parallel mechanism for the symmetric all pairwise computation
when only a subset of data is available to each process.
Our experimental results illustrate that the quality of interpolated
mapping results are comparable to the mapping
results of original algorithm only. In parallel performance aspect,
those interpolation methods are well parallelized with
high efficiency. With the proposed interpolation method, we
construct a configuration of two-million out-of-sample data
into the target dimension, and the number of out-of-sample
data can be increased further.
Categories and Subject Descriptors
I.5 [Pattern Recognition]: Miscellaneous},
  comment      = {Dim reduction, or out-of-sample solution for either classical or non-classical MDS by computing full MDS on a subset and then interopolting the rest of the out-of-sample point from their KNN training set neighbors (they use k=2!). Itertative algorithm is very simple, can be paralleized to the point level, and is pretty accurate with small traiing set. Is much less computationally and memory intensive. The also demonstrate the technique on the GTM EM clustering technique, which I didn't read much about.},
  file         = {Bae10dimRedMDSinterp.pdf:Bae10dimRedMDSinterp.pdf:PDF},
  groups       = {Read},
  owner        = {sotterson},
  timestamp    = {2014.06.27},
  url          = {http://dl.acm.org/citation.cfm?id=1851501},
}

@Misc{Aleo19pvPanelsLowLight,
  author   = {Aleo},
  title    = {All solar panels are not equal - how low light behavior greatly impacts yields},
  year     = {2019},
  abstract = {The rise of the importance of solar panels energy yields has prompted the use of a new term within our industry and that you may have seen inside presentations or datasheets – “low light behavior”. In this post, we would like to address this issue to give our readership a better understanding of this phenomenon.},
  comment  = {Shows how low level light design of commercial panels can greatly affect annual yields.  Nice graphs.

See also Grunow04weakLightPV},
  file     = {:Aleo19pvPanelsLowLight.pdf:PDF},
  url      = {https://www.aleo-solar.com/all-solar-panels-are-not-equal/},
}

@TechReport{Jorgensen08aesoFR,
  author      = {Jess U. J{\o}rgensen and Corinna M\"{o}hrlen},
  title       = {{AESO} Wind Power Forecasting Pilot Project Final Report},
  institution = {WEPROG},
  year        = {2008},
  month       = may,
  abstract    = {The scope of the final report for the AESO wind forecasting pilot project has been to provide detailed answers on a number of questions that have been discussed within the project and that are relevant for the future handling of wind power in Alberta. The analysis of the results achieved in the pilot project showed us that there is an unusual challenge and opportunity at hand. The opportunity being the wind resource and the challenge being how to sustainably integrate an increasing installed capacity. In this executive summary, we will briefly discuss our results and findings in the project and provide a recommendation for a future wind power forecasting solution for Alberta. Finally, we will discuss to which wind power penetration level the described solution can scale.},
  comment     = {Subjective advice on Alberta forecasting and power bidding. Descriptions of causes of ramps and forecast errors. Also see earlier tech report (attached).

Notes from Tech Report
* lot's of stuff: I mostly looked at the ramp part.
* how to hedge against ramps in a power market
* measured uncertainty vs. lookahead time
* advice on best kind of ensemble members for ramp forecasting (ensembles trained for low avg. error will tend to miss ramps (p. 24)) Also see final report},
  file        = {Final Report:Jorgensen08aesoFR.pdf:PDF;Tech Report:Jorgensen08aesoFrcstTR.pdf:PDF},
  location    = {Ebberup, Denmark},
  owner       = {scot},
  timestamp   = {2011.04.11},
}

@InProceedings{Fernando05inputSelNNpmi,
  author       = {Fernando, TMKG and Maier, HR and Dandy, GC and May, R},
  title        = {Efficient selection of inputs for artificial neural network models},
  booktitle    = {Proc. of MODSIM 2005 International Congress on Modelling and Simulation: Modelling and Simulation Society of Australia and New Zealand},
  year         = {2005},
  pages        = {1806--1812},
  organization = {Citeseer},
  abstract     = {The selection of an appropriate subset of variables
from a set of measured potential input variables
for inclusion as inputs to model the system under
investigation is a vital step in model development.
This is particularly important in data driven
techniques, such as artificial neural networks
(ANNs) and fuzzy systems, as the performance of
the final model is heavily dependent on the input
variables used to develop the model. Selection of
the best set of input variables is essential to being
able to model the system under consideration
reliably. When the available data set is high
dimensional, it is necessary to select a subset of
the potential input variables to reduce the number
of free parameters in the model in order to obtain
good generalization with finite data. The correct
choice of model inputs is also important for
improving computational efficiency. However,
the topic of input selection is a difficult one. Real
systems are generally complex and mostly
associated with nonlinear processes.
Consequently, the dependencies between output
and input variables, as well as conditional
dependencies between variables, are difficult to
measure.
Mutual information (MI) has been used
successfully to measure the dependence between
output and input variables. In contrast to the
linear correlation coefficient, which often forms
the basis of empirical input variable selection
approaches, mutual information is capable of
measuring dependencies based on both linear and
nonlinear relationships, making it well suited for
use with complex nonlinear systems. Partial
mutual information (PMI) has been proposed in
recent years as a means of measuring conditional
dependencies between output and input variables
(Sharma, 2000). It is a robust technique for
selecting input variables for multivariate,
nonlinear, complex natural systems, such as
hydrological processes. The PMI approach is a
stepwise input variable selection algorithm.
Consequently, it is necessary to have a reliable
technique to indicate whether a selected candidate

variable is significant or not. The original
algorithm proposed by Sharma (2000) used the
bootstrap method with 100 bootstraps to obtain
the 95th percentile confidence limit for the PMI.
However, as pointed out by Chernick (1999),
about 5,000 bootstraps are needed for simple
problems and about 10,000 bootstraps for more
complicated problems in order to estimate the
required confidence intervals reliably. Use of
such a large number of bootstraps as the stopping
criterion for the PMI algorithm would decrease
the computational efficiency of the algorithm
significantly, probably to the point of
impracticality for most realistic problems.
The focus of this study is to introduce an
alternative stopping criterion for PMI algorithm
implementation, which is both robust and
computationally efficient. As part of the proposed
method, significant PMI scores are treated as
outliers in the computed PMI scores. A robust
outlier detection technique, the Hampel identifier
(Davies and Gather, 1993), is used to evaluate the
significance of selected candidate inputs. The
reliability of the new technique is first
investigated using two nonlinear data series where
dependencies of attributes were known a priori.
The new technique consistently selects the correct
inputs, while being computationally efficient.
The modified PMI algorithm is then applied to
select inputs to forecast salinity in the River
Murray at Murray Bridge, South Australia, which
are used to develop an ANN model. The results
obtained in this study are compared with those
obtained in three previous studies which
developed ANN models for the same case study.
The proposed PMI algorithm identifies only 11
inputs as significant from 1323 candidate inputs.
The resulting ANN model has the smallest
number of inputs when compared with the models
developed in previous studies for this case study,
with no loss in predictive performance.

Keywords: Artificial neural networks; input identification; mutual information; outliers; robust statistics},
  comment      = {An early partial mutual information feature selection method.  Jan might have used (matlab\Utilities\Utilities_jd\Informationstheorie) it but it seems like there are now better methods.  See May11revVarSelNN and many others.

Darudi13partMutInfoFeatSel is also a kind of PMI hack.},
  file         = {Fernando05inputSelNNpmi.pdf:Fernando05inputSelNNpmi.pdf:PDF},
  url          = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.439.6723&rep=rep1&type=pdf},
}

@InProceedings{Ferrari13datAprxPVmaint,
  author    = {S. Ferrari and M. Lazzaroni and V. Piuri and A. Salman and L. Cristaldi and M. Faifer},
  title     = {A data approximation based approach to photovoltaic systems maintenance},
  booktitle = {2013 IEEE Workshop on Environmental Energy and Structural Monitoring Systems},
  year      = {2013},
  pages     = {1-6},
  month     = {Sept},
  abstract  = {The solar panel, which transforms the energy carried by the light in electricity, is a reliable component of a photovoltaic (PV) system, but its efficiency depends on several factors, such as its orientation, its working temperature, and its tidiness. Since maintenance is an expensive activity, a careful evaluation of the degradation of the panel and the resulting production loss has to be carried out. Besides, an accurate estimation of the potential production with respect to the weather condition requires expensive instruments and skilled operators. In this paper, we propose an alternative approach based on the prediction of the potential production based on a public weather station in the nearby of the considered plant. Several computational intelligence paradigms as well as several prediction setups are here challenged and compared.},
  comment   = {Models PV output e used the Support Vector Regression (SVR) [13], the Feed-forward Neural network
(FFN) [14], and the k-Nearest Neighbor (k-NN) predictor [15].

Insert in Colombia slides as ref. for AI models.  Could follow up on refs to snip out the bits on KNN, etc.},
  doi       = {10.1109/EESMS.2013.6661694},
  file      = {Ferrari13datAprxPVmaint.pdf:Ferrari13datAprxPVmaint.pdf:PDF},
  keywords  = {approximation theory;maintenance engineering;solar cells;PV system;computational intelligence paradigms;data approximation based approach;panel degradation;photovoltaic systems maintenance;potential production;production loss;public weather station;solar panel;weather condition;working temperature;Approximation methods;Maintenance engineering;Neurons;Predictive models;Production;Solar radiation;Temperature measurement},
}

@Article{Bessa15spatTempSolPowFrcst,
  author    = {R. J. Bessa and A. Trindade and V. Miranda},
  title     = {Spatial-Temporal Solar Power Forecasting for Smart Grids},
  journal   = IEEE_J_IINF,
  year      = {2015},
  volume    = {11},
  number    = {1},
  pages     = {232--241},
  month     = feb,
  issn      = {1551-3203},
  abstract  = {The solar power penetration in distribution grids is growing fast during the last years, particularly at the low-voltage (LV) level, which introduces new challenges when operating distribution grids. Across the world, distribution system operators (DSO) are developing the smart grid concept, and one key tool for this new paradigm is solar power forecasting. This paper presents a new spatial-temporal forecasting method based on the vector autoregression framework, which combines observations of solar generation collected by smart meters and distribution transformer controllers. The scope is 6-h-ahead forecasts at the residential solar photovoltaic and medium-voltage (MV)/LV substation levels. This framework has been tested in the smart grid pilot of {\'{E}}vora, Portugal, and using data from 44 microgeneration units and 10 MV/LV substations. A benchmark comparison was made with the autoregressive forecasting model (AR-univariate model) leading to an improvement on average between 8\% and 10\%.

Index Terms?Distribution network, forecasting, smart grid,
smart metering, solar power, spatial?temporal.},
  doi       = {10.1109/TII.2014.2365703},
  file      = {Bessa15spatTempSolPowFrcst.pdf:Bessa15spatTempSolPowFrcst.pdf:PDF},
  keywords  = {autoregressive processes, load forecasting, photovoltaic power systems, power distribution control, power generation control, power transformers, smart meters, smart power grids, solar power stations, substations, vectors, {\'{E}}vora, Portugal, autoregressive forecasting model, distribution grids, distribution system operators, distribution transformer controllers, low-voltage level, medium-voltage substation levels, residential solar photovoltaic, smart grid pilot, smart meters, solar power penetration, spatial-temporal solar power forecasting, vector autoregression framework, Forecasting, Mathematical model, Predictive models, Reactive power, Smart grids, Substations, Vectors, Distribution network, Solar power, distribution network, forecasting, smart grid, smart metering, solar power, spatial--temporal, spatialtemporal},
  owner     = {sotterson},
  timestamp = {2017.04.13},
}

@InProceedings{AlAwami09statCharWindPowFrcst,
  author    = {Al-Awami, A.T. and El-Sharkawi, M.A.},
  title     = {Statistical characterization of wind power output for a given wind power forecast},
  booktitle = {North American Power Symposium (NAPS), 2009},
  year      = {2009},
  pages     = {1--4},
  abstract  = {The stochastic nature of wind power output makes the integration of high penetration of wind into the power grid a real challenge. In this work, the uncertainty associated with the wind power output for a given wind power forecast is modeled using conditional probability density functions (pdf). Two pdf functions are considered: Beta and extreme value. Simulation results show that, in general, the proposed extreme value distribution outperforms Beta distribution at data bins of high wind power forecast whereas Beta is usually better at low to moderate wind forecast.},
  comment   = {Extreme value distritubution fits forecasts better at high forecasted power (>54% norm. power). Conditional wind pow density forecast by binning point forecast (persistence) and fitting beta or extreme value dist to measurements in each bin. This is a bit like my spline vs. linear QR forecast.

* binning something like Louie10evalProbWindPow, which also evaluated extreme value dist, beta, etc.
* Extreme value is clearly better in some high forecasted power bins but beta is better in the rest.
* Measured by a crude binned version of the quantile score,
 -- Louie10evalProbWindPow picked GEV all the time based on chi-squared
* EV dist. is mostly better above 54% normalized forecast power.},
  doi       = {10.1109/NAPS.2009.5484044},
  file      = {AlAwami09statCharWindPowFrcst.pdf:AlAwami09statCharWindPowFrcst.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2015.04.07},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5484044},
}

@Article{Hutter15beyondManTunHypParam,
  author    = {Hutter, Frank and L{\"u}cke, J{\"o}rg and Schmidt-Thieme, Lars},
  title     = {Beyond Manual Tuning of Hyperparameters},
  journal   = {KI-K{\"u}nstliche Intelligenz},
  year      = {2015},
  volume    = {29},
  number    = {4},
  pages     = {329--337},
  abstract  = {The success of hand-crafted machine learning systems in many applications raises the question of making machine learning algorithms more autonomous, i.e., to reduce the requirement of expert input to a minimum. We discuss two strategies towards this goal: (1) automated optimization of hyperparameters (including mechanisms for feature selection, preprocessing, model selection, etc) and (2) the development of algorithms with reduced sets of hyperparameters. Since many research directions (e.g., deep learning), show a tendency towards increasingly complex algorithms with more and more hyperparamters, the demand for both of these strategies continuously increases. We review recent hyperparameter optimization methods and discuss data-driven approaches to avoid the introduction of hyperparameters using unsupervised learning. We end in discussing how these complementary strategies can work hand-in-hand, representing a very promising approach towards autonomous machine learning.
Keywords
Hyperparameter optimization Automatic machine learning Autonomous learning Deep learning},
  comment   = {Yeah hyper parameters are a pain.  Also includes deep learning and feature selection.  Maybe worth reading because I need an update on featsel and need to learn more about deep learning.},
  doi       = {10.1007/s13218-015-0381-0},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2016.06.18},
}

@Article{Bengio13repLrnOvrvw,
  author    = {Bengio, Y. and Courville, A. and Vincent, P.5},
  title     = {Representation Learning: A Review and New Perspectives},
  journal   = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year      = {2013},
  volume    = {35},
  number    = {8},
  pages     = {1798--1828},
  abstract  = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
  comment   = {Hints on what kind of features to learn in deep learning, mentions how PCA fits in.

see also tensor decomp, which seems similar: Cichocki14TensorNetworksBigOpt

Another good deep learn overview: Schmidhuber15deepLrnOvrvw},
  doi       = {10.1109/TPAMI.2013.50},
  file      = {2014 ArXiv version:Bengio13RepLrnRev.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.01},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6472238},
}

@InCollection{Feurer15autoMLautoSkLrn,
  author    = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
  title     = {Efficient and Robust Automated Machine Learning},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year      = {2015},
  editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages     = {2962--2970},
  abstract  = {The success of machine learning in a broad range of applications has led to an
ever-growing demand for machine learning systems that can be used off the shelf
by non-experts. To be effective in practice, such systems need to automatically
choose a good algorithm and feature preprocessing steps for a new dataset at hand,
and also set their respective hyperparameters. Recent work has started to tackle this
automated machine learning (AutoML) problem with the help of efficient Bayesian
optimization methods. Building on this, we introduce a robust new AutoML system
based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and
4 data preprocessing methods, giving rise to a structured hypothesis space with
110 hyperparameters). This system, which we dub AUTO-SKLEARN, improves on
existing AutoML methods by automatically taking into account past performance
on similar datasets, and by constructing ensembles from the models evaluated
during the optimization. Our system won the first phase of the ongoing ChaLearn
AutoML challenge, and our comprehensive analysis on over 100 diverse datasets
shows that it substantially outperforms the previous state of the art in AutoML. We
also demonstrate the performance gains due to each of our contributions and derive
insights into the effectiveness of the individual components of AUTO-SKLEARN.},
  comment   = {Automatic tuning of ML Python AUTO-SKLEARN algorithm parameters, also algorithm selection, ensemble building (I think).  Here they describing wining a some automl challenge.

At ECML 2017, I talked to Frank Hutter (leader in the field and one of the authors) and he said that someone told him that the regression functions in this package weren't that great, and he said that not much time had been spent on them.

Still, this is the paper he recommended as the closest thing to automatic machine learning.},
  file      = {:Feurer15autoMLautoSkLrn.pdf:PDF},
  url       = {http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf},
}

@Article{Weigel08modelCombo,
  author    = {Weigel, AP and Liniger, MA and Appenzeller, C.},
  title     = {Can multi-model combination really enhance the prediction skill of probabilistic ensemble forecasts?},
  journal   = {Quarterly Journal of the Royal Meteorological Society},
  year      = {2008},
  volume    = {134},
  number    = {630},
  pages     = {241--260},
  abstract  = {The success of multi-model ensemble combination has been demonstrated in many studies. Given that a multi-model contains information from all participating models, including the less skilful ones, the question remains as to why, and under what conditions, a multi-model can outperform the best participating single model. It is the aim of this paper to resolve this apparent paradox. The study is based on a synthetic forecast generator, allowing the generation of perfectly-calibrated single-model ensembles of any size and skill. Additionally, the degree of ensemble under-dispersion (or overconfidence) can be prescribed. Multi-model ensembles are then constructed from both weighted and unweighted averages of these single-model ensembles. Applying this toy model, we carry out systematic model-combination experiments. We evaluate how multi-model performance depends on the skill and overconfidence of the participating single models. It turns out that multi-model ensembles can indeed locally outperform a ?best-model? approach, but only if the single-model ensembles are overconfident. The reason is that multi-model combination reduces overconfidence, i.e. ensemble spread is widened while average ensemble-mean error is reduced. This implies a net gain in prediction skill, because probabilistic skill scores penalize overconfidence. Under these conditions, even the addition of an objectively-poor model can improve multi-model skill. It seems that simple ensemble inflation methods cannot yield the same skill improvement. Using seasonal near-surface temperature forecasts from the DEMETER dataset, we show that the conclusions drawn from the toy-model experiments hold equally in a real multi-model ensemble prediction system. KEY WORDS DEMETER; inflation; probabilistic verification; seasonal predictions; toy model; under-dispersion},
  comment   = {Model combination works when they are overconfident. Seems fundamental to ensemble forecasting feature selection, etc.},
  file      = {Weigel08modelCombo.pdf:Weigel08modelCombo.pdf:PDF},
  groups    = {Ensemble, doReadWPV_2},
  owner     = {scot},
  publisher = {Wiley Online Library},
  timestamp = {2011.05.13},
}

@Article{Yang07SurvPowSensNet,
  author    = {Yi Yang and Lambert, F. and Divan, D.},
  title     = {A Survey on Technologies for Implementing Sensor Networks for Power Delivery S},
  journal   = {Power Engineering Society General Meeting, IEEE},
  year      = {2007},
  pages     = {1--8},
  month     = jun,
  issn      = {1932-5517},
  abstract  = {The task of monitoring asset status and optimizing asset utilization for the T\&D industry, given millions of assets and hundreds of thousands of miles of power lines distributed geographically over millions of square miles, seems particularly challenging if not impossible. Given the traditionally high cost of sensing and communications, the grid has minimal 'smarts' with much of the intelligence located at major substations. Dramatic reductions in sensor, computing and communications costs, coupled with significant performance enhancements has raised the possibility of realizing widely and massively distributed sensor networks (SNs) to monitor utility asset status. Under NEETRAC funding, a survey was conducted to review existing sensor technologies and products, and to estimate the possibility of extending these to realize distributed SNs. Possible applications for such SNs were also explored, as was the issue of cost point at which such networks would become commercially viable. This paper provides an overview of the highlights from the detailed survey that was conducted, and identifies 'gaps' in currently available sensor technologies, both from a performance and cost point.},
  comment   = {overview of powerline sag and direct thermal measurement: overheadn and underground; cost vs. benefit comparison Sag measurement and accuracy * GPS: 20cm * Inclinometer: Precise angle measurement is required. * Resistive wires: Sensitive to external disturbance * Tension measurement: measures avg. cond temp * Sagometer:Around ?2 inches1 * Laser Distance Measurement: Around ?2 inches Conductor temp measurement * FODTS * IR-based},
  doi       = {10.1109/PES.2007.386289},
  file      = {Yang07SurvPowSensNet.pdf:Yang07SurvPowSensNet.pdf:PDF;Yang07SurvPowSensNet.pdf:Yang07SurvPowSensNet.pdf:PDF},
  groups    = {Read},
  keywords  = {distributed sensors, electricity supply industry, power system measurementT\&,D industry, asset utilization optimization, distributed sensor network, power delivery system, sensor network, utility asset status monitoring},
  owner     = {sotterson},
  timestamp = {2009.02.23},
}

@Article{Gonzalez17rvwQuantifLrn,
  author     = {Gonz\'{a}lez, Pablo and Casta\~{n}o, Alberto and Chawla, Nitesh V. and Coz, Juan Jos{\'e} Del},
  title      = {A Review on Quantification Learning},
  journal    = {ACM Comput. Surv.},
  year       = {2017},
  volume     = {50},
  number     = {5},
  pages      = {74:1--74:40},
  month      = sep,
  issn       = {0360-0300},
  abstract   = {The task of quantification consists in providing an aggregate estimation (e.g., the class distribution in a classification problem) for unseen test sets, applying a model that is trained using a training set with a different data distribution. Several real-world applications demand this kind of method that does not require predictions for individual examples and just focuses on obtaining accurate estimates at an aggregate level. During the past few years, several quantification methods have been proposed from different perspectives and with different goals. This article presents a unified review of the main approaches with the aim of serving as an introductory tutorial for newcomers in the field.},
  acmid      = {3117807},
  address    = {New York, NY, USA},
  articleno  = {74},
  comment    = {Hyper-relevant to WattPlan Grid per-consumer logistic regression.  Predicting and aggregate of some population which is predicted at the individual level using some ML model.  But the ML model was trained on a dataset with different properties than now.  This is about how to handle that, and it apparently involves not predicting the individuals.},
  doi        = {10.1145/3117807},
  file       = {:Gonzalez17rvwQuantifLrn.pdf:PDF},
  issue_date = {November 2017},
  keywords   = {Class distribution estimation, prevalence estimation, quantification},
  numpages   = {40},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3117807},
}

@Article{Tadeuchi07quickOnlineFeatSel,
  author    = {Tadeuchi, Y. and Oshima, R. and Nishida, K. and Yamauchi, K. and Omori, T.},
  title     = {Quick online feature selection method for regression - A feature selection method inspired by human behavior},
  year      = {2007},
  month     = oct,
  pages     = {1895--1900},
  doi       = {10.1109/ICSMC.2007.4414117},
  abstract  = {The task of variable selection is essential to improving the ability of machine learning systems to generalize. Although there are many conventional variable selection methods, almost all of them need to prepare and learn a large number of samples in advance because they are based on offline learning. This property is not suitable for online learning systems. To overcome this inconvenience, we propose a quick online variable selection method inspired by human problem solving behaviors. The proposed method tries to generate several variable set candidates in a speculative manner using a filter method and evaluates them using a wrapper method. The method can also function in concept-drifting environments, where relevant variable sets are changing. The experimental results show that the new method yields appropriate variable sets from a small number of samples.},
  file      = {Tadeuchi07quickOnlineFeatSel.pdf:Tadeuchi07quickOnlineFeatSel.pdf:PDF;Tadeuchi07quickOnlineFeatSel.pdf:Tadeuchi07quickOnlineFeatSel.pdf:PDF},
  journal   = {Systems, Man and Cybernetics, IEEE},
  keywords  = {learning (artificial intelligence), regression analysisconcept-drifting environments, human problem solving behaviors, machine learning systems, quick online feature selection method, regression, variable selection},
  owner     = {sotterson},
  timestamp = {2009.01.08},
}

@Misc{Wikepedia19techAdoptLfCycl,
  author   = {Wikipedia},
  title    = {Technology adoption life cycle},
  month    = apr,
  year     = {2019},
  abstract = {The technology adoption lifecycle is a sociological model that describes the adoption or acceptance of a new product or innovation, according to the demographic and psychological characteristics of defined adopter groups. The process of adoption over time is typically illustrated as a classical normal distribution or "bell curve". The model indicates that the first group of people to use a new product is called "innovators", followed by "early adopters". Next come the early majority and late majority, and the last group to eventually adopt a product are called "Laggards" or "phobics." For example, a phobic may only use a cloud service when it is the only remaining method of performing a required task, but the phobic may not have an in-depth technical knowledge of how to use the service.},
  comment  = {Basic idea came from U of Iowa researchers, who split the population in to segments who were more to less eager to adopt a product.  They were able to find sociological categories for them:


* innovators – had larger farms, were more educated, more prosperous and more risk-oriented 
* early adopters – younger, more educated, tended to be community leaders, less prosperous 
* early majority – more conservative but open to new ideas, active in community and influence to neighbours 
* late majority – older, less educated, fairly conservative and less socially active 
* laggards – very conservative, had small farms and capital, oldest and least educated

Organized in counts of adoption vs. time, the approximated this as a normal density.  Integrate it to get total adoption and you get a normal cdf.

Advancements on this:
* discontinous adoption
* s-curve instead of normal
* disruption
* network graph of neighbors/friends: more on close edges ==> more likelly to adopt},
  file     = {:Wikepedia19techAdoptLfCycl.pdf:PDF},
  url      = {https://en.wikipedia.org/wiki/Technology_adoption_life_cycle},
}

@InProceedings{Hering08spaceTimeWindFrcst,
  author    = {Amanda S. Hering and Marc G. Genton},
  title     = {Powering Up with Space-Time Wind Forecasting},
  booktitle = {American Wind Energy Association (AWEA)},
  year      = {2008},
  abstract  = {The technology to harvest electricity from wind energy is now advanced enough to make entire cities powered by it a reality. High-quality shortterm forecasts of wind speed are vital to making this a more reliable energy source. Gneiting et al. (2006) have introduced an accurate and sharp model for forecasting the average wind speed two hours ahead based on both spatial and temporal information; however, this model is split into nonunique regimes based on the wind direction at an off-site location. This work both generalizes and improves upon this model by treating wind direction as a circular variable and including it in the model. It is robust in many experiments, such as predicting at new locations and under rotations of the wind directions. We compare this with the more common approach of modeling wind speeds and directions in the Cartesian space and use a skew-t distribution for the errors. The quality of the predictions from all of these models can be more realistically assessed with a loss measure that depends upon the power curve relating wind speed to power output. This proposed loss measure yields more insight into the true worth of each model's predictions.},
  comment   = {Related slides at: Hering08spaceTimeWindFrcstSlides},
  file      = {Hering08spaceTimeWindFrcst.pdf:Hering08spaceTimeWindFrcst.pdf:PDF;Hering08spaceTimeWindFrcstSlides.pdf:Hering08spaceTimeWindFrcstSlides.pdf:PDF},
  groups    = {ErrDistProps, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2009.03.04},
}

@Article{Hering10spaceTimeWindFrcst,
  author    = {Hering, A.S. and Genton, M.G.},
  title     = {Powering up with space-time wind forecasting},
  journal   = {Journal of the American Statistical Association},
  year      = {2010},
  volume    = {105},
  number    = {489},
  pages     = {92--104},
  abstract  = {The technology to harvest electricity from wind energy is now advanced enough to make entire cities powered by it a reality. High-quality, short-term forecasts of wind speed are vital to making this a more reliable energy source. Gneiting et al. (2006) have introduced a model for the average wind speed two hours ahead based on both spatial and temporal information. The forecasts produced by this model are accurate, and subject to accuracy, the predictive distribution is sharp, that is, highly concentrated around its center. However, this model is split into nonunique regimes based on the wind direction at an offsite location. This paper both generalizes and improves upon this model by treating wind direction as a circular variable and including it in the model. It is robust in many experiments, such as predicting wind at other locations. We compare this with the more common approach of modeling wind speeds and directions in the Cartesian space and use a skew-t distribution for the errors. The quality of the predictions from all of these models can be more realistically assessed with a loss measure that depends upon the power curve relating wind speed to power output. This proposed loss measure yields more insight into the true value of each model?s predictions.},
  comment   = {Older versions: Hering08spaceTimeWindFrcst, Hering08spaceTimeWindFrcstSlides I couldn't download this at DTU},
  doi       = {10.1198/jasa.2009.ap08117},
  owner     = {scotto},
  publisher = {ASA},
  timestamp = {2011.11.07},
}

@Misc{Geyer06subSampBootStrap,
  author       = {Charles J. Geyer},
  title        = {5601~{N}otes: The Subsampling Bootstrap},
  howpublished = {Course Notes, Stat 5601, University of Minnesota},
  month        = apr,
  year         = {2006},
  abstract     = {The term "bootstrap" was coined by Efron (1979). He described both the nonparametric and parametric bootstrap. In particular, his nonparametric bootstrap is the procedure of resampling with replacement from the original sample at the same sample size, which is by far the most commonly used bootstrap procedure. It wasn't long before people experimented with resampling at different sample sizes. But the key discovery in that area came later. Politis and Romano (1994) described resampling without replacement from the original sample at smaller than the original sample size. This is different enough from Efron's idea that in their book (Politis, Romano, and Wolf, 1999) they don't call it 'bootstrap' but just plain 'subsampling'. Whatever you call it, here's why it is such an important innovation. * Politis and Romano's subsampling bootstrap takes samples without replacement of size b from the original sample of size n, generally with b  n (read "b much less than n"). Such samples are themselves samples of size b from the true unknown distribution F of the original sample. * Efron's nonparametric bootstrap takes samples with replacement of size n from the original sample of size n (both sample sizes the same). Such samples are samples of size n from the empirical distribution b Fn associated with the original sample. Each of these procedures does the Wrong Thing. * The Right Thing is samples of the right size n from the right distribution F. * The Politis and Romano thing is samples of the wrong size b  n from the right distribution F. * The Efron thing is samples of the right size n from the wrong distribution b Fn. Both Wrong Things are wrong. We would like to do the Right Thing but we can't. (More precisely, we have exactly one such sample, the original data, and can't get more. Scientists may get more data, but that's of no interest to us statisticians.) So which Wrong Thing do we want to do? Both have pluses and minuses. The Efron procedure is older, more widely used, and familiar to more people. It is also easier to use, at least in simple situations. But the Politis and Romano procedure has the great virtue of working in situations where the Efron bootstrap fails. The two main classes of such situations are presented in the following sections.},
  comment      = {Subsampled bootstrapping: How to get confidence intervals when boostrapping w/ replacement won't work. (a very nice writeup). R code attached.

Note, my notes are on the 2006 version, which Geyer corrected in 2013. See end of this entry for our email exchange.

The usual Bootstrap:
- with replacement
- wrong dist. (b/c w/ replacement)?
- sample is same size (right size)
- won't work for non-stationary time series
- fails for simple problems like the max (or is it the convergence rate of the max?)
- fails for Kraskov mutual info estimation (for this, see Francois06permTestMutInf) Subsampling Bootstrap:?
- WITHOUT replacement
- right dist? (no replacement)
- SMALLER size: wrong size
- fine for stationary time series (w/ contiguous subsamples)
- still assumes stationarity (how bad if it's not stationary?)

Basic idea of subsampling bootstrap:
The full sample is considered a sample of the true distribution, which means that parameters estimated from it have some error, and the error has some distribution. This error decreases as n, the full sample size decreases. Since we don't know the true distribution, we don't know the parameter error distribution either, but sub-sampling bootstrap estimates it by estimating the parameters from subsamples of size b, from the original distribution of size n. The error distribution at size n is then extraplated from the subsample estimates by estimating the "rate" at which the error distribution approaches some asymptotic distribution in relation to the sample size.

Double sided example seems to be here: http://www.stat.umn.edu/geyer/old/5601/examp/subtoot.html

* note that on the bottom example, he doesn't do a sliding window but instead selects nboot random samples of size nsub, w/o replacement

* how to pick subsample size may be explained in Simar10mofnBtstrp

* This is an expansion and maybe a small improvement of the methods in the book: Politis, D. N.; Romano, J. P., and Wolf, M. (1999). Subsampling. Springer Verlag. (bibtex key Politis99subsampBook)
* can potentially a huge num. of subsamples for each size, but Berg10subSampBSpval says you don't need to use all of them.
* example of when can't bootstrap w/ replacement is K-NN MI, which Francois06permTestMutInf has found is biased by this (I have found the same thing in my ramp experiments).
* these notes may have a bug on p. 16 -- I sent an email to Geyer on Nov. 22, 2011
* Bertail99subsampBSunkCnvrg is related, seems to be the log() trick method used to estimate rate (that paper also talks about confidence intervals for multivariate estimates).
* Bickel08choiceOfMsubsampBtstrp also uses Geyer's idea of log spaced subsample tests, but he's trying to pick the subsample size, not determine the rate. The ideas could be combined, it seems.
* Parts of Berg10subSampBSpval are a little more clear vs. the last couple pages of Geyer (this paper)
* final subsample size is vague and arbitrary: Politis01asympSubsamp may be a better alternative

Bootstrapping is extended to regression in: Fox08btstrpRgrsnBookChap

--- email about 2013 corrections ----------------

>>> Charles Geyer <charlie\@stat.umn.edu> 5/15/2013 6:02 PM >>>

I know this is very old. I was just searching for 5601 in Gmail and saw this that I had missed when you sent it. You are indeed correct. I think I have fixed it correctly in the attached which will be used in 5601 this fall (I haven't taught it for five years, have been teaching master's level theory instead). Thanks (belatedly).


On Tue, Nov 22, 2011 at 6:29 AM, Scott Otterson <scott.otterson\@iwes.fraunhofer.de> wrote:

Dr. Geyer,
Hello from a fellow UW PhD alum, as well as a UMN BSEE graduate.
First, thanks for posting your Stat 5601 course notes, particularly those on the subsampling bootstrap. They were the clearest introduction I've found on the topic.
There's one thing I can't understand, though, and it might be a bug: On the bottom of p. 16 of the notes, the R code is as follows:

> theta.star <- theta.star[, m]
> crit.val <- sort(z.star)[(nboot + 1) * alpha]
> theta.hat - c(0, crit.val)/n^bet
First, theta.star isn't used in this code segment, and second, z.star is used, even though it's not set. But looking back to p. 13, z.star is an intermediate value inside of a loop; if the notes were generated using some kind of literate programming technique, where the code was incorporated into the document, then z.star would remain corresponding to the last b value in the loop (b=135). However, the code on p. 16 is using b=90.
Am I right about that? And shouldn't z.star for b=90 be used?
Thanks,
Scott
Dr. Scott Otterson
Abteilung Energiemeteorologie und Netzintegration
Fraunhofer IWES
Tel: +49 (0)561 7294-252
E-Mail: scott.otterson\@fraunhofer.iwes.de},
  file         = {Corrected version, 2013:Geyer13subSampBootStrap_corrected.pdf:PDF;Original, with my notes Geyer's error:Geyer06subSampBootStrap.pdf:PDF},
  groups       = {Read},
  owner        = {scot},
  timestamp    = {2011.06.08},
}

@TechReport{Treyz07housingPriceElasticityFrcst,
  author      = {Frederick Treyz and George Treyz and Nicolas Mata},
  title       = {Predicting Housing Price Changes in States and MSAs Using Area Specific Price Elasticities},
  institution = {REMI},
  year        = {2007},
  abstract    = {The three most important aspects of understanding housing markets are: Location, 
location, and location. As housing markets across the U.S. decline, state and local 
differences in housing price responses are just as evident as during the boom. These 
differences have wide-ranging implications in terms of regional economic growth and 
responses to changes in policies. 

Areas with relatively constrained land available for new development experience large 
fluctuations in housing prices in response to changing income and population pressures. 
Regions with more development potential can add or lose population and income with 
relatively modest housing price responses. This means that states with relatively high 
housing price responses tend to see economic growth mitigated as the cost of living 
increases, reducing the willingness of people to live in the state. 

This paper presents new state and MSA-specific housing price elasticities in the REMI 
Policy Insight Version 9.5 model. In section II, we discuss state and MSA differences in 
housing price changes, and present basic data. Section III shows the housing price 
equations, and section IV shows the estimation results. Section V presents REMI Policy 
Insight simulation results. },
  comment     = {Froecasts area-specific housing prices out to 2050. Explains price sensitivity regression method and a lot of other stuff.  Price sensitivty inputs might help might help with adoption probability estimation.},
  file        = {:Treyz07housingPriceElasticityFrcst.pdf:PDF},
  url         = {https://www.remi.com/topics-and-studies/predicting-housing-price-changes-in-states-and-msas-using-area-specific-price-elasticities/},
}

@Article{Chen01decompBasisPurst,
  author              = {Chen, Scott Shaobing and Donoho, David L. and Saunders, Michael A.},
  title               = {Atomic Decomposition by Basis Pursuit},
  journal             = {SIAM Review},
  year                = {2001},
  volume              = {43},
  number              = {1},
  pages               = {129--159},
  issn                = {0036-1445},
  abstract            = {The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries-stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). Basis pursuit (BP) is a principle for decomposing a signal into an "optimal" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, abstract harmonic analysis, total variation denoising, and multiscale edge denoising. BP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear and quadratic programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugategradient solver.},
  comment             = {Highly cited early paper on finding best sparse basis, related to lasso and wavelets. Use for wind dir/spd relationship? has matlab},
  copyright           = {Copyright ?? 2001 Society for Industrial and Applied Mathematics},
  file                = {Chen01decompBasisPurst.pdf:Chen01decompBasisPurst.pdf:PDF},
  jstor_articletype   = {primary_article},
  jstor_formatteddate = {Mar., 2001},
  owner               = {scotto},
  publisher           = {Society for Industrial and Applied Mathematics},
  timestamp           = {2010.08.16},
  url                 = {http://www.jstor.org/stable/3649687},
}

@Article{Kiesel15econAnl15minIntradayPrice,
  author    = {Kiesel, Ruediger and Paraschiv, Florentina},
  title     = {Econometric analysis of 15-minute intraday electricity prices},
  journal   = {Available at SSRN},
  year      = {2015},
  abstract  = {The trading activity in the German intraday electricity market has increased significantly over the last years. This is partially due to an increasing share of renewable energy, wind and photovoltaic, which requires power generators to balance out the forecasting errors in their production. We investigate the bidding behaviour in the intraday market by looking at both last prices and continuous bidding, in the context of a fundamental model. A unique data set of 15-minute intraday prices and intraday-updated forecasts of wind and photovoltaic has been employed and price bids are modelled by prior information on fundamentals. We show that intraday prices adjust asymmetrically to both forecasting errors in renewables and to the volume of trades dependent on the threshold variable demand quote, which reflects the expected demand covered by the planned traditional capacity in the day-ahead market. The location of the threshold can be used by market participants to adjust their bids accordingly, given the latest updates in the wind and photovoltaic forecasting errors and the forecasts of the control area},
  comment   = {German intraday 15 minute price behavior.  Shows 15 min. sawtooth shape due to day ahead market product length of one hour.

>>> Henry Martin 11/4/2016 2:20 pm >>>
Hi Scott, Hi Volker,
here is the graph I told both of you about. It shows the dependency of the 15min intraday price to solar ramping. Notice how the price spikes occur in the first 15 minutes when solar feed in is on the rise, and in the last 15 minutes when it is decreasing. It is also interesting to see, that the amplitude of the price spikes depends on the slope of the solar generation. Reason is, that energy on the day ahead market can only be traded hourly, which means that if generation is increasing, generators sold to much energy in the first half of the hour and to few in the second half.

See you soon,
Henry},
  file      = {Kiesel15econAnl15minIntradayPrice.pdf:Kiesel15econAnl15minIntradayPrice.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2016.11.21},
  url       = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2671379},
}

@Article{Szmit12holtWinterMultiPeriod,
  author    = {Szmit, Maciej and Szmit, Anna},
  title     = {Usage of modified Holt-Winters method in the anomaly detection of network traffic: Case studies},
  journal   = {Journal of Computer Networks and Communications},
  year      = {2012},
  volume    = {2012},
  number    = {2012},
  abstract  = {The traditional Holt-Winters method is used, among others, in behavioural analysis of network traffic for development of adaptive models for various types of traffic in sample computer networks. This paper is devoted to the application of extended versions of these models for development of predicted templates and intruder detection.},
  comment   = {Adaptive Holt-Winters for multiple periodicity, here daily and weekly. Could be useful for adaptive price forecasting.},
  doi       = {10.1155/2012/192913},
  file      = {Szmit12holtWinterMultiPeriod.pdf:Szmit12holtWinterMultiPeriod.pdf:PDF},
  publisher = {Hindawi Publishing Corporation},
  url       = {http://www.hindawi.com/journals/jcnc/2012/192913/},
}

@InProceedings{Juricic06faultDetGaussProc,
  author       = {Juricic, Dj and Kocijan, J},
  title        = {Fault detection based on Gaussian process model},
  booktitle    = {Proceedings of the 5th Vienna Symposium on Mathematical Modeling (MathMod)},
  year         = {2006},
  organization = {Vienna, Austria},
  abstract     = {The traditional model-based fault detection and isolation (FDI) rely on tacit assumption
that the validity of process model is out of question for current process data. However, it happens in
practice that a process might end up in regions for which the underlying model is not validated. This
can result in false alarms. To avoid the risk, a validity index is suggested as a measure of confidence
assigned to the detection results. This measure is based on estimated distance of the current process
data from data employed in the learning set. If the former is close to the latter, the detector output will
be assigned high confidence. It is shown in this paper that the idea can be realized in a rather natural
way when Gaussian Process models are used to describe the input-output behavior of nonlinear dynamic
systems. This is a relatively recent modelling approach, which is probably for the first time adopted to
the FDI problem domain. For a particular case of sensor faults it results in a statistical test in the form
of a weighted sum of prediction errors. The effectiveness of the test is demonstrated on a simulated pH
process.},
  comment      = {Fault detect with confidence intervals.  Semi-probabilistic since based on GP's.  Seems to be what's patented in: Brummel16monEnrgProgSiePtnt},
  file         = {:Juricic06faultDetGaussProc.pdf:PDF},
}

@Article{Edelenbosch18socTechLrnRate,
  author    = {O Y Edelenbosch and David L McCollum and Hazel Pettifor and Charlie Wilson and Detlef P van Vuuren},
  title     = {Interactions between social learning and technological learning in electric vehicle futures},
  journal   = {Environmental Research Letters},
  year      = {2018},
  volume    = {13},
  number    = {12},
  pages     = {124004},
  month     = {nov},
  abstract  = {The transition to electric vehicles is an important strategy for reducing greenhouse gas emissions from
passenger cars. Modelling future pathways helps identify critical drivers and uncertainties. Global
integrated assessment models (IAMs)have been used extensively to analyse climate mitigation policy.
IAMs emphasise technological change processes but are largely silent on important social and behavioural
dimensions to future technological transitions. Here, we develop a novel conceptual framing and
empirical evidence base on social learning processes relevant for vehicle adoption. We then implement
this formulation of social learning in IMAGE, a widely-used global IAM. We apply this new modelling
approach to analyse how technological learning and social learning interact to influence electric vehicle
transition dynamics. We find that technological learning and social learning processes can be mutually
reinforcing. Increased electric vehicle market shares can induce technological learning which reduces
technology costs while social learning stimulates diffusion from early adopters to more risk-averse
adopter groups. In this way, both types of learning process interact to stimulate each other. In the absence
of social learning, however, the perceived risks of electric vehicle adoption among later-adopting groups
remains prohibitively high. In the absence of technological learning, electric vehicles remain relatively
expensive and therefore is only an attractive choice for early adopters. This first-of-its-kind model
formulation of both social and technological learning is a significant contribution to improving the
behavioural realism of global IAMs. Applying this new modelling approach emphasises the importance of
market heterogeneity, real-world consumer decision-making, and social dynamics as well as technology
parameters, to understand climate mitigation potentials.},
  comment   = {Jointly models technological learning rate (product cost w/ market share, I think) and social learning (diffusion to/from early adopters).  Would be useful for longer horizon, large scale forecasts.},
  doi       = {10.1088/1748-9326/aae948},
  file      = {:Edelenbosch18socTechLrnRate.pdf:PDF},
  publisher = {{IOP} Publishing},
}

@Article{Broecker08ensToPDF,
  author    = {Br{\"o}cker, Jochen and Smith, Leonard A},
  title     = {From ensemble forecasts to predictive distribution functions},
  journal   = {Tellus A},
  year      = {2008},
  volume    = {60},
  number    = {4},
  pages     = {663--678},
  abstract  = {The translation of an ensemble of model runs into a probability distribution is a common task in model-based prediction. Common methods for such ensemble interpretations proceed as if verification and ensemble were draws from the same underlying distribution, an assumption not viable for most, if any, real world ensembles. An alternative is to consider an ensemble as merely a source of information rather than the possible scenarios of reality. This approach, which looks for maps between ensembles and probabilistic distributions, is investigated and extended. Common methods are revisited, and an improvement to standard kernel dressing, called ?affine kernel dressing? (AKD), is introduced. AKD assumes an affine mapping between ensemble and verification, typically not acting on individual ensemble members but on the entire ensemble as a whole, the parameters of this mapping are determined in parallel with the other dressing parameters, including a weight assigned to the unconditioned (climatological) distribution. These amendments to standard kernel dressing, albeit simple, can improve performance significantly and are shown to be appropriate for both overdispersive and underdispersive ensembles, unlike standard kernel dressing which exacerbates over dispersion. Studies are presented using operational numerical weather predictions for two locations and data from the Lorenz63 system, demonstrating both effectiveness given operational constraints and statistical significance given a large sample.},
  comment   = {Simple improved kernel dressing. Could be a kernel dressing baseline. Somebody at Eweline IFP in Nov. 2014 said this preserved temporal stats.},
  file      = {Broecker08ensToPDF.pdf:Broecker08ensToPDF.pdf:PDF},
  groups    = {Ensemble, CitaviImport1, doReadNonWPV_1},
  ncite     = {37},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2013.09.26},
  url       = {http://www.tellusa.net/index.php/tellusa/article/view/15387},
}

@TechReport{UKONS08holtWintersARIMAreg,
  author      = {UK Office of National Statistics},
  title       = {From Holt-Winters to ARIMA Modelling: Measuring the Impact on Forecasting Errors for Components of Quarterly Estimates of Public Service Output},
  institution = {UK Centre for the Measurement of Government Activity},
  year        = {2008},
  month       = aug,
  abstract    = {The UK Centre for the Measurement of Government Activity
(UKCeMGA) was launched in July 2005 in order to take forward the
recommendations from the Atkinson Review to improve the measure
of Government Output and Productivity.
The division has been established to strengthen the capability of ONS
to publish authoritative and coherent measures of the output and
productivity of government-funded services in the UK National
Accounts. One of the key objectives of the work programme is to
ensure that the measures of key government services in the UK
National Accounts are fit for purpose.
UKCeMGA delivers output data to National Accounts on a quarterly
basis for seven different public services, namely Adult Social Care,
Children?s Social Care, Civil and Family Courts, Education, the Fire and
Rescue Services, Health Care and Social Security Administration.

Forecasting techniques are used extensively throughout UKCeMGA for
data deliveries to National Accounts in order to extend time series
beyond the most recent period for which data are available. This is
often carried out because a time lag can occur in some datasets,
meaning that some data are unavailable, and data that are available
are sometimes incomplete. For example, data for the current financial
or calendar year will often not be available until the following year. To
overcome this we forecast these missing data points. Actual data
content makes up just 36 per cent of the GDP(O) estimate of
Government output, even three months after the end of the relevant
quarter. Clearly good forecasting is essential if accurate GDP(O)
estimates are to be produced.},
  comment     = {For forecasting, Holt-winters with bias, trend and additive and/or seasonal adjustment terms. This is compared to ARIMA, also with seasonal components and with regression against a vector exogenous inputs (ARIMAreg wins I guess). Simple explanation of models and parameter selection in the appendices.},
  file        = {UKONS08holtWintersARIMAreg.pdf:UKONS08holtWintersARIMAreg.pdf:PDF},
  url         = {http://www.ons.gov.uk/ons/guide-method/ukcemga/ukcemga-publications/publications/archive/from-holt-winters-to-arima-modelling--measuring-the-impact-on-forecasting-errors-for-components-of-quarterly-estimates-of-public-service-output.pdf},
}

@Article{Hothorn14CondTransfMdls,
  author    = {Hothorn, Torsten and Kneib, Thomas and B{\"u}hlmann, Peter},
  title     = {Conditional transformation models},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year      = {2014},
  volume    = {76},
  number    = {1},
  pages     = {3--27},
  abstract  = {The ultimate goal of regression analysis is to obtain information about the conditional distribution of a response given a set of explanatory variables. This goal is, however, seldom achieved because most established regression models estimate only the conditional mean as a function of the explanatory variables and assume that higher moments are not affected by the regressors. The underlying reason for such a restriction is the assumption of additivity of signal and noise. We propose to relax this common assumption in the framework of transformation models. The novel class of semiparametric regression models proposed herein allows transformation functions to depend on explanatory variables. These transformation functions are estimated by regularized optimization of scoring rules for probabilistic forecasts, e.g. the continuous ranked probability score. The corresponding estimated conditional distribution functions are consistent. Conditional transformation models are potentially useful for describing possible heteroscedasticity, comparing spatially varying distributions, identifying extreme events, deriving prediction intervals and selecting variables beyond mean regression effects. An empirical investigation based on a heteroscedastic varying-coefficient simulation model demonstrates that semiparametric estimation of conditional distribution functions can be more beneficial than kernel-based non-parametric approaches or parametric generalized additive models for location, scale and shape.

Keywords:

 Boosting;
 Conditional distribution function;
 Conditional quantile function;
 Continuous ranked probability score;
 Prediction intervals;
 Structured additive regression},
  comment   = {For conditional distribution estimation, compares boosted additive models (Fenske12strctAddQRthesis Schmid07BoostAddPspline) with boosted models that assume an output distribution (like Mayr12genAddModelsHiDimBoost) with straight GAMLS and with kernel techniques. Has R.

Kernel techniques don't looks so good and boosted additive models seem about the same as the ones that assume a distribution, once varying terms as in Fenske12strctAddQRthesis are added.

Both journal and longer arXiv paper attached.

R is in the experimental ctm package: http://r-forge.r-project.org/projects/ctm/
(could be a good place to get more example boosting code)},
  doi       = {10.1111/rssb.12017/full},
  file      = {Journal paper:Hothorn14CondTransfMdls.pdf:PDF;Extended Version on ArXiv:Hothorn14CondTransfMdls_longer.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.08.01},
}

@InProceedings{Cheung11smartDsptchRES,
  author    = {K. W. Cheung and R. Rios-Zalapa},
  title     = {Smart dispatch for large grid operations with integrated renewable resources},
  booktitle = {Proc. ISGT 2011},
  year      = {2011},
  pages     = {1-7},
  month     = jan,
  abstract  = {The uncertainty of generation required to maintain system balancing has been growing significantly due to the penetration of renewable energy resources such as wind power and the impact of demand response. To deal with such uncertainty, RTO's require more look-ahead and forecasting capabilities beyond real-time. In this paper a new dispatch system called Generation Control Application (GCA) is proposed to address the challenges posed by renewable energy integration. With dynamic and robust dispatch algorithm and flexible system configuration, the proposed system will provide adequate system ramping capability to cope with uncertain intermittent resources while maintaining system reliability in large grid operations.},
  comment   = {Does a kind of probabilistic net demand forecast (I think) and handles everything so there is adequate ramping ability.

1st paper is the IEEE paper with the math.  Also attached is a friendlier white paper.},
  doi       = {10.1109/ISGT.2011.5759143},
  file      = {IEEE paper:Cheung11smartDsptchRES.pdf:PDF;White paper from somewher, almost the same but friendlier, less math:Cheung11smartDsptchRES_whitepaper.pdf:PDF},
  keywords  = {power generation dispatch, power generation reliability, power grids, renewable energy sources, GCA, adequate system ramping capability, demand response, dynamic dispatch algorithm, flexible system configuration, generation control application, integrated renewable resources, large-grid operations, renewable energy integration, robust dispatch algorithm, smart dispatch, system reliability, wind power, Predictive models, Real time systems, Renewable energy resources, Robustness, Uncertainty, Wind forecasting, Wind power generation, Economic dispatch, operation uncertainty, renewable energy integration, restructured power systems, smart grid},
  owner     = {sotterson},
  timestamp = {2017.03.11},
}

@InProceedings{Becker16spatTempWndFrcstErrGrid,
  author    = {Raik Becker and Christoph Weber},
  title     = {Spatio-Temporal Modelling of Wind Power Forecast Errors in Transmission Grids},
  booktitle = {Wind Integration Workshop},
  year      = {2016},
  abstract  = {The uncertainty of wind power forecasting puts
pressure on the daily operational planning of transmission
system operators (TSOs). The inertia of meteorologically re-
lated forecast systems leads to wind power forecasting errors
that propagate in space and time. Dealing with that and
accounting for likely deviations from wind power forecasts
can facilitate the integration of wind power into transmission
grids. This paper presents shortly an approach for modelling
the spatio-temporal propagation of wind power forecasting
errors in transmission grids. Therefore, the interdependency
of wind power forecasting errors at different sites and points
in time is modeled with so-called R-vine copulae or pair-copula
constructions (PCCs). The method is applied to a case study
of two grid nodes in the German transmission grid.},
  comment   = {Modeled errors w/ vine copulas, and then did scenario generation!
  Somehow picked pairwise relationships for day ahead grid node forecasting (pairs across time, with another pair tied at the zero-horizon across two nodes, but how tie more than two grid nodes?).   Only has a handwavy graph of forecasted scenarios, with no performance metric.  I'm not sure if this really worked.

References 9=11 (all in my Scenario gen. DTU review?) are worth reading.

Could use for ReWP.  Pick set of farms based on correlation, then pick quantile of power to sell from their sum using scenario generation.

Did this guy email to ask for my DTU scenario generation literature review???

* compare with Lu14pairCopulaSpatWindPow},
  date      = {2016-11-17},
  file      = {Becker16spatTempWndFrcstErrGrid.pdf:Becker16spatTempWndFrcstErrGrid.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2016.11.18},
  url       = {http://windintegrationworkshop.org/vienna2016},
}

@InProceedings{Otterson16stochTrdTSO,
  author    = {Scott Otterson and Dominik Jost and Malte Jansen and Malte Siefert},
  title     = {Stochastic trading across time under German TSO constraints},
  booktitle = {15th International Workshop on Large-Scale Integration of Wind Power into Power Systems},
  year      = {2016},
  address   = {Vienna},
  abstract  = {The uncertainty of wind power forecasts generally decreases with forecast horizon, making it possible to profitably sell large amounts of this power on a day-ahead interday market, and then to make small corrections closer to delivery time by trading on a short-horizon intraday market. Such multistage intraday buy/sell decisions are often made using deterministic power forecasts, even though probabilistic forecasts describe the highly relevant uncertainty vs. time relationship. This is especially true for German transmission system operators (TSOs), who are allowed to consider only the expected value -- i.e. a point forecast -- when selling on the interday market, and who have other restrictions on the intraday market.
Simulations suggest that certain German TSOs, or direct marketers which have smaller installed capacities, may benefit on the intraday market by trading while using probabilistic forecasts. A much larger benefit may also accrue with the use of a price forecast, also developed for this work. One limitation this study is that the market model used may have the effect of smoothing out price volatility and illiquidity.  A next step is to re-evaluate probabilistic trading on a market model which simulates such illiquidity.
Index Terms -- wind power, probabilistic forecasting, price forecasting, electricity markets, grid integration},
  comment   = {My Wind Integration Workshop (Vienna, 2016) paper.},
  date      = {November},
  file      = {Otterson16stochTrdTSO.docx:Otterson16stochTrdTSO.docx:Word 2007+},
  owner     = {sotterson},
  timestamp = {2016.11.18},
  url       = {http://windintegrationworkshop.org/vienna2016},
}

@Article{Lengyel19geoCmplxDfsnInnov,
  author      = {Balázs Lengyel and Riccardo Di Clemente and János Kertész and Marta C. González},
  title       = {The role of geography in the complex diffusion of innovations},
  journal     = {arXiv},
  year        = {2019},
  number      = {arXiv:1804.01349 [physics.soc-ph]},
  month       = feb,
  abstract    = {The urban-rural divide is increasing in modern societies and is influencing economic and social inequality, and political radicalization. These observed trends are deeply rooted in the spatial patterns of society, and require geographical extensions in network models of social influence. improved understanding of the ways in which new technologies and new ideas diffuse across locations can provide us with new insights into the spread of information, technological progress and economic development. In this work, we uncover the role of geography in the spatial diffusion of innovations via a unique dataset from iWiW, which is an online social network (OSN) in Hungary. We analyze its entire life cycle from 2002 to 2012, reaching up to 300 million friendships at the peak of its popularity and the subsequent disappearance after its failed competition with international OSN platforms. By applying a complex diffusion model on the empirical spatial social network, we observe that the connections of individuals with similar tendency to adopt, or assortative mixing, is crucial in spatial spreading. However, predicting the likelihood of adoption is the aim of diffusion models and a priori labeling of individuals in these models would be a paradox. Alternatively, we reveal the scaling laws in the number of innovators and early adopters in towns as a function of their population and gravity laws on the distance of ties. These empirical features need to be included in models of complex contagion.},
  date        = {2018-04-04},
  eprint      = {http://arxiv.org/abs/1804.01349v2},
  eprintclass = {physics.soc-ph},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1804.01349v2:PDF;:Lengyel19geoCmplxDfsnInnov.pdf:PDF},
  keywords    = {physics.soc-ph, cs.CY, cs.SI},
}

@InProceedings{Siefert14probWindFrcstCOSMODE_EPS,
  author       = {Malte Siefert and Jan Dobschinski and Scott Otterson and Thomas Kanefendt and Kristina Lundgren and Dominique Ernst and Mathias Zirkelbach and Anja Berkmann-Dick},
  title        = {Probabilistic wind power forecasts based on the COSMO-DE-EPS weather model},
  booktitle    = {13\textsuperscript{th} International Workshop on Large-Scale Integration of Wind Power into Power Systems as well as on Transmission Networks for Offshore Wind Power Plants},
  year         = {2014},
  month        = nov,
  organization = {Energynautics GmbH, Germany},
  abstract     = {The usability of weather ensemble systems and ensemble wind power forecasts for the energy industry are studied in the project EWeLiNE. Intermediate results are presented for single wind parks of the current COSMO-DE-EPS weather ensemble of the German Weather Service. We address forecasts for two applications of transmission system operators: quantile forecasts for the marketing of renewable energies and scenario forecasts for load flow calculations. The weather model is the COSMO-DE-EPS which is currently optimized with respect to application for wind power and PV forecasts. While the raw wind power ensemble forecast is under-dispersive and has to be calibrated, quantiles calculated from the ensemble are well calibrated. For scenario forecasts based on the ensemble we suggest a calibration which can handle the nonlinearity of wind power. For short term forecast of a few hours not all temporal properties are contained in the weather model. For this range we study a power ensemble which is generated by a stochastic differential equation.
Keywords-probabilistic wind power forecast; ensemble forecast; quantile forecast; scenario forecast; stochastic differential equation},
  comment      = {1\textsuperscript{st} Eweline attempts at ensemble frcst. Malte's COSMO-DE ensemble work for Eweline. I have not read this yet.},
  file         = {Siefert14probWindFrcstCOSMODE_EPS.docx:Siefert14probWindFrcstCOSMODE_EPS.docx:Word 2007+},
  location     = {Berlin},
  owner        = {sotterson},
  timestamp    = {2014.10.07},
  url          = {http://www.windintegrationworkshop.org/},
}

@Book{Brown74admisDistScores,
  title     = {Admissible scoring systems for continuous distributions},
  publisher = {Rand Corporation},
  year      = {1974},
  author    = {Brown, Thomas A.},
  abstract  = {The use of admissible scoring systems as a measure of
probabilistic forecas?ts is becoming increasingly wellknown
in ithose cases 1N"here the forecast is a discrete
distribution over a finite number of alternatives (e.g.:
Will it rain or not? Will Dewey, Truman, Wallace, or
Thurmond be elected? Will the Rams or the Vikings win the
game?) ? ~rhe defining property of an admissible scoring
system is that any individual perceives himself as maximizing
his expected score by reporting his true subjective
distribution. That is to say, if you want to beat the
system thE~ best way to do it is to be honest.
Most serious forE~casts which are made in the real
world seem to be forecasts of quantities (e.g.: What will
be the tot.al U.S. wheat production during 1974? How many
tanks will there bm .in the Egyptian Army on July 1, 1975?
What will be the Dow-~Jones average on January 2, 1976?)
rather than choices bE~tween a finite number of alternatives.
In such cc:tses as this,, it seems much more natural to ask
hhe forecaster to specify a continuous probability distribution
which represents his expectations rather than trying
to re-cast a basically continuous process into a discrete
one. But how can we construct an admissible scoring system
for a cont:inuous distribution? There are three basic
approaches which seem to work:
(1) We can regard the continuous distribution as the
limit of a discrete one, and derive a continuous
-2-
admissible :scoring system as the limit of a
sequence or d~screte ones.
(2) We can crea?te continuous admissible scoring
systems by exp~oiting the Schwartz inequality,
or by using ether well-known inequalities of
mathematical analysis.
(3) We can postulate a collection of possible bets
on a continuous variable, and construct an
admissible scoring system as the net pay-off to
a forecaster who takes all bets (and only those
bets) which appear favorable on the basss of
his reported distribution. This is an exact
analogue to the "gambling house" construction
method which may be used to discover discrete
admissible scoring systems.
Of the three techniques, I tend to prefer the third
because i i: gives greai:er insight into what actually lies
behind an admissible scoring system, and suggests ways to
tailor the~ scoring system to accomplish one's goals more
effectively in a given si~uation. But let us discuss each
of the me?t:hods in turn.},
  comment   = {Seems fundamental, but maybe the scoring systems are alread settled enough that the fundamentals aren't needed.

But this could be useful more devising an NN prob. forecast scoring algorithm?},
  file      = {Brown74admisDistScores.pdf:Brown74admisDistScores.pdf:PDF},
  groups    = {Test, CitaviImport1, doReadWPV_2},
  ncite     = {9},
  owner     = {sotterson},
  timestamp = {2013.09.26},
}

@Article{Bowden05inputNeuralWater,
  author    = {Gavin J. Bowden and Graeme C. Dandy and Holger R. Maier},
  title     = {Input determination for neural network models in water resources applications. Part 1: background and methodology},
  journal   = {Journal of Hydrology},
  year      = {2005},
  volume    = {301},
  number    = {1-4},
  pages     = {75--92},
  abstract  = {The use of artificial neural network (ANN) models in water resources applications has grown considerably over the last decade. However, an important step in the ANN modelling methodology that has received little attention is the selection of appropriate model inputs. This article is the first in a two-part series published in this issue and addresses the lack of a suitable input determination methodology for ANN models in water resources applications. The current state of input determination is reviewed and two input determination methodologies are presented. The first method is a model-free approach, which utilises a measure of the mutual information criterion to characterise the dependence between a potentialmodel input and the output variable.Tofacilitate the calculation of dependence in the case ofmultiple inputs, a partialmeasure of themutual information criterion is used. In the second method, a selforganizing map(SOM)is used to reduce the dimensionality of the input space and obtain independent inputs.To determine which inputs have a significant relationship with the output (dependent) variable, a hybrid genetic algorithm and general regression neural network (GAGRNN) is used.Both input determination techniques are tested on a number of synthetic data sets,where the dependence attributes were known a priori. In the second paper of the series, the input determinationmethodology is applied to a real-world case study in order to determine suitable model inputs for forecasting salinity in the River Murray, South Australia, 14 days in advance.},
  comment   = {Partial information feature selection. I got the code for an implementation from Martin Hanel: /Users/sotterson/proj/regime/contrib/pmi His paper is: Hanel08runoffPartialMutInfoNeural},
  doi       = {10.1016/j.jhydrol.2004.06.021},
  file      = {Bowden05inputNeuralWater.pdf:Bowden05inputNeuralWater.pdf:PDF;Bowden05inputNeuralWater.pdf:Bowden05inputNeuralWater.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.03.26},
}

@Conference{Papakonstantinou14uncertMktOffers,
  author    = {A. Papakonstantinou and P. Pinson},
  title     = {Towards electricity markets accommodating uncertain offers},
  booktitle = {International Workshop on Large-Scale Integration of Wind Power and Transmission Networks},
  year      = {2014},
  location  = {Berlin},
  url       = {http://pierrepinson.com/?page_id=778},
  abstract  = {The use of renewable energy sources of energy and
in particular wind and solar has been on the rise over the last
decades with plans to increase it even more. Such developments
introduce significant challenges in existing power systems and
can result in high electricity prices and costly infrastructure
investments. In this paper we propose a new electricity market
mechanism whereby the uncertain and dynamic nature of wind
power and other stochastic sources is embedded in the market
mechanism itself, by modelling producers' bids as probabilistic
estimates. An extension on the bilevel programming formulation
of an electricity market, based on the Continuous Ranked
Probability Score (CRPS) reduces the impact of poor estimates
for both the stochastic producers and the system operator. We
introduce a simulation setting which first demonstrates that
impact and then proceed to illustrate the main features our
market setup and compare it with a conventional electricity
market and a standard bilevel setup.},
  file      = {Papakonstantinou14uncertMktOffers.pdf:Papakonstantinou14uncertMktOffers.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.12.05},
}

@Article{Matheny95surfHarmShape,
  author    = {Matheny, A. and Goldgof, D.B.},
  title     = {The use of three- and four-dimensional surface harmonics for rigid and nonrigid shape recovery and representation},
  journal   = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year      = {1995},
  volume    = {17},
  number    = {10},
  pages     = {967--981},
  month     = oct,
  issn      = {0162-8828},
  abstract  = {The use of spherical harmonics for rigid and nonrigid shape representation is well known. This paper extends the method to surface harmonics defined on domains other than the sphere and to four-dimensional spherical harmonics. These harmonics enable us to represent shapes which cannot be represented as a global function in spherical coordinates, but can be in other coordinate systems. Prolate and oblate spheroidal harmonics and cylindrical harmonics are examples of surface harmonics which we find useful. Nonrigid shapes are represented as functions of space and time either by including the time-dependence as a separate factor or by using four-dimensional spherical harmonics. This paper compares the errors of fitting various surface harmonics to an assortment of synthetic and real data samples, both rigid and nonrigid. In all cases we use a linear least-squares approach to find the best fit to given range data. It is found that for some shapes there is a variation among geometries in the number of harmonics functions needed to achieve a desired accuracy. In particular, it was found that four-dimensional spherical harmonics provide an improved model of the motion of the left ventricle of the heart},
  comment   = {good paper on cylindrical and spherical harmonics: shows you what they can do. for lagged wind velocity estimation??},
  doi       = {10.1109/34.464561},
  file      = {Matheny95surfHarmShape.pdf:Matheny95surfHarmShape.pdf:PDF},
  keywords  = {3D surface harmonics;4D surface harmonics;cylindrical harmonics;four-dimensional surface harmonics;heart ventricle;linear least-squares approach;nonrigid shape recovery;oblate spheroidal harmonics;prolate spheroidal harmonics;real data samples;rigid shape recovery;shape representation;spherical harmonics;surface fitting;surface harmonics;synthetic data samples;three-dimensional surface harmonics;time-dependence;computer vision;image reconstruction;image representation;least squares approximations;medical image processing;surface fitting;},
  owner     = {scotto},
  timestamp = {2010.08.19},
}

@Article{Kwan14regressnInterpInvCovMat,
  author    = {Kwan, Clarence CY},
  title     = {A regression-based interpretation of the inverse of the sample covariance matrix},
  journal   = {Spreadsheets in Education (eJSiE)},
  year      = {2014},
  volume    = {7},
  number    = {1},
  pages     = {3},
  abstract  = {The usefulness of covariance and correlation matrices is well-known in various academic fields. Matrix inversion, if required in an analytical setting, tends to mask the intuition in interpreting the corresponding empirical or experimental results. Drawing on the finance literature in mean-variance portfolio analysis, this paper presents pedagogically a regression-based interpretation of the inverse of the sample covariance matrix. Microsoft ExcelTM plays an important pedagogic role in this paper. The availability of various Excel functions and computational tools for numerical illustrations provides flexibility for instructors in the delivery of the corresponding analytical materials.},
  comment   = {Derives the inverse covariance (precision) matrix in terms of linear regression coefficients,  variances and coefficient of determination (R^2).  See eq. 8.   Does not quite get to partial correlation.  Also expresses a minimum risk in terms of covariance matrix and/or linear regression.  Seems to be implicitly making smoe kind of uncorrelated errors assumption which I can't quite follow.

Some facts:

* Precision matrix diagonals: large for high variance, unpredictable variables (jth variable, diag j,j)
* Regression coeffs: shrunken when j is a high variance, unpredictable variable.  If it's a high variance, predictable variable it's not shrunk so much.
* Things that make a covariance matrix uninvertible (so no precision matrix)
  - A constant variable
  - a perfectly linear relationship between variables.

},
  file      = {Kwan14regressnInterpInvCovMat.pdf:Kwan14regressnInterpInvCovMat.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.15},
  url       = {http://epublications.bond.edu.au/ejsie/vol7/iss1/3/?utm_source=epublications.bond.edu.au%2Fejsie%2Fvol7%2Fiss1%2F3&utm_medium=PDF&utm_campaign=PDFCoverPages},
}

@Article{Roulston03medFrctVal,
  author    = {M.S. Roulston and D.T. Kaplan and J. Hardenberg and L.A. Smith},
  title     = {Using medium-range weather forcasts to improve the value of wind energy production},
  year      = {2003},
  volume    = {28},
  pages     = {585--602},
  abstract  = {The value of different strategies for consolidating the information in European Centre for Medium Range Weather Forecasting (ECMWF) forecasts to wind energy generators is investigated. Simulating the performance of generators using the different strategies in the context of a simplified electricity market revealed that ECMWF forecasts in production decisions improved the performance of generators at lead times of up to 6 days. Basing half-hourly production decisions on a production forecast generated by condtioning the climate on the ECMWF operational ensemble forecast yields the best results of all the strategies tested. ? 2002 Elsevier Science Ltd. All rights reserved.},
  file      = {Roulston03medFrctVal.pdf:Roulston03medFrctVal.pdf:PDF;Roulston03medFrctVal.pdf:Roulston03medFrctVal.pdf:PDF},
  groups    = {Ensemble, Use, doReadNonWPV_1},
  journal   = {Renewable Energy},
  owner     = {sotterson},
  timestamp = {2009.03.04},
}

@Article{Martin15varWindLenTime,
  author   = {Clara M St. Martin and Julie K Lundquist and Mark A Handschy},
  title    = {Variability of interconnected wind plants: correlation length and its dependence on variability time scale},
  journal  = {Environmental Research Letters},
  year     = {2015},
  volume   = {10},
  number   = {4},
  pages    = {044004},
  abstract = {The variability in wind-generated electricity complicates the integration of this electricity into the electrical grid. This challenge steepens as the percentage of renewably-generated electricity on the grid grows, but variability can be reduced by exploiting geographic diversity: correlations between wind farms decrease as the separation between wind farms increases. But how far is far enough to reduce variability? Grid management requires balancing production on various timescales, and so consideration of correlations reflective of those timescales can guide the appropriate spatial scales of geographic diversity grid integration. To answer ???how far is far enough,??? we investigate the universal behavior of geographic diversity by exploring wind-speed correlations using three extensive datasets spanning continents, durations and time resolution. First, one year of five-minute wind power generation data from 29 wind farms span 1270 km across Southeastern Australia (Australian Energy Market Operator). Second, 45 years of hourly 10 m wind-speeds from 117 stations span 5000 km across Canada (National Climate Data Archive of Environment Canada). Finally, four years of five-minute wind-speeds from 14 meteorological towers span 350 km of the Northwestern US (Bonneville Power Administration). After removing diurnal cycles and seasonal trends from all datasets, we investigate dependence of correlation length on time scale by digitally high-pass filtering the data on 0.25???2000 h timescales and calculating correlations between sites for each high-pass filter cut-off. Correlations fall to zero with increasing station separation distance, but the characteristic correlation length varies with the high-pass filter applied: the higher the cut-off frequency, the smaller the station separation required to achieve de-correlation. Remarkable similarities between these three datasets reveal behavior that, if universal, could be particularly useful for grid management. For high-pass filter time constants shorter than about ????=??38 h, all datasets exhibit a correlation length ##IMG## [http://ej.iop.org/images/1748-9326/10/4/044004/erl511511ieqn1.gif] {$\xi $} that falls at least as fast as ##IMG## [http://ej.iop.org/images/1748-9326/10/4/044004/erl511511ieqn2.gif] {${{\tau }^{-1}}$} . Since the inter-site separation needed for statistical independence falls for shorter time scales, higher-rate fluctuations can be effectively smoothed by aggregating wind plants over areas smaller than otherwise estimated.},
  comment  = {Interesting because decorrelation length depends upon high pass filter (reserve power time scale).  For ReWP and reserve power allocation?

* rapid wind-speed or power variations become uncorrelated at smaller spatial separations
than do slow variations.

Has a great table of differen spatio-temporal correlation equations found in the literature.

Compare with Bandi16SpectrumWindPow},
  file     = {Martin15varWindLenTime.pdf:Martin15varWindLenTime.pdf:PDF},
  url      = {http://stacks.iop.org/1748-9326/10/i=4/a=044004},
}

@Article{Suh16GaussCopulaVarMixDat,
  author    = {Suh, Suwon and Choi, Seungjin},
  title     = {Gaussian Copula Variational Autoencoders for Mixed Data},
  journal   = {arXiv preprint arXiv:1604.04960},
  year      = {2016},
  abstract  = {The variational autoencoder (VAE) is a generative model with continuous latent
variables where a pair of probabilistic encoder (bottom-up) and decoder (top-
down) is jointly learned by stochastic gradient variational Bayes. We first elabo-
rate Gaussian VAE, approximating the local covariance matrix of the decoder as an
outer product of the principal direction at a position determined by a sample drawn
from Gaussian distribution. We show that this model, referred to as VAE-ROC,
better captures the data manifold, compared to the standard Gaussian VAE where
independent multivariate Gaussian was used to model the decoder. Then we ex-
tend the VAE-ROC to handle mixed categorical and continuous data. To this end,
we employ Gaussian copula to model the local dependency in mixed categorical
and continuous data, leading to Gaussian copula variational autoencoder (GC-
VAE). As in VAE-ROC, we use the rank-one approximation for the covariance in
the Gaussian copula, to capture the local dependency structure in the mixed data.
Experiments on various datasets demonstrate the useful behaviour of VAE-ROC
and GCVAE, compared to the standard VAE.},
  comment   = {A mixture of Gaussian copulas that is both an autoencoder and a copula that can handle dependencies across a mix of continuous and discrete variables.  I think the autoencoder part could constrain dependency (good for high dims) and the mixture part could be good for different weather conditions for upscaling.  Finally the ability to handle discrete and continuous variables could be good for fault detection.},
  file      = {Suh16GaussCopulaVarMixDat.pdf:Suh16GaussCopulaVarMixDat.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.07},
}

@Article{Fan13featSelScrnHiDimVarCoeff,
  author    = {Fan, Jianqing and Ma, Yunbei and Dai, Wei},
  title     = {Nonparametric independence screening in sparse ultra-high dimensional varying coefficient models},
  journal   = {Journal of the American Statistical Association},
  year      = {2013},
  number    = {just-accepted},
  abstract  = {The varying coefficient model is an important class of nonparametric statistical model, which allows us to examine how the effects of covariates vary with exposure variables. When the number of covariates is large, the issue of variable selection arises. In this article, we propose and investigate marginal nonparametric screening methods to screen variables in sparse ultra-high-dimensional varying coefficient models. The proposed nonparametric independence screening (NIS) selects variables by ranking a measure of the nonparametric marginal contributions of each covariate given the exposure variable. The sure independent screening property is established under some mild technical conditions when the dimensionality is of nonpolynomial order, and the dimensionality reduction of NIS is quantified. To enhance the practical utility and finite sample performance, two data-driven iterative NIS (INIS) methods are proposed for selecting thresholding parameters and variables: conditional permutation and greedy methods, resulting in conditional-INIS and greedy-INIS. The effectiveness and flexibility of the proposed methods are further illustrated by simulation studies and real data applications.

Keywords: Sure independence screening; Variable selection; Sparsity; Conditional permutation; False posi-
tive rates},
  comment   = {Good for boosting? Or at least a coarse pre-feat sel for it. There is a journal paper but the one attached now is ArXiv

Authors both from Princeton so maybe it's a good paper?},
  doi       = {10.1080/01621459.2013.879828},
  file      = {ArXiv paper:Fan13featSelScrnHiDimVarCoeff.pdf:PDF},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2014.12.07},
}

@Article{Fan08varCoeffStatMeth,
  author    = {Fan, Jianqing and Zhang, Wenyang},
  title     = {Statistical methods with varying coefficient models},
  journal   = {Statistics and its Interface},
  year      = {2008},
  volume    = {1},
  number    = {1},
  pages     = {179},
  abstract  = {The varying coefficient models are very important tool to explore the dynamic pattern in many scientific areas, such as economics, finance, politics, epidemiology, medical science, ecology and so on. They are natural extensions of classical parametric models with good interpretability and are becoming more and more popular in data analysis. Thanks to their flexibility and interpretability, in the past ten years, the varying coefficient models have experienced deep and exciting developments on methodological, theoretical and applied sides. This paper gives a selective overview on the major methodological and theoretical developments on the varying coefficient models.
Keywords: Varying coefficient models, local linear modelling, bandwidth selection, cross-validation, confidence band, hypothesis test, semivarying coefficient models, exponential family, generalized varying coefficient models, local maximum likelihood, nonlinear time series, longitudinal data analysis, Cox models, local partial likelihood},
  comment   = {A friendly-sounding overview of varying coefficient models.},
  file      = {Fan08varCoeffStatMeth.pdf:Fan08varCoeffStatMeth.pdf:PDF},
  owner     = {sotterson},
  publisher = {NIH Public Access},
  timestamp = {2014.11.09},
  url       = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2575822/},
}

@Article{Rohrig2006appWindPredTools,
  author    = {Rohrig, K. and Lange, B.},
  title     = {Application of wind power prediction tools for power system operations},
  year      = {2006},
  month     = jun,
  doi       = {10.1109/PES.2006.1709159},
  abstract  = {The wide use of wind energy in Germany results in a lot of new power system operation problems corresponding especially to the stochastic character of the wind speed and to the not controllable production of energy. The significant amount of installed wind power in the German power system (currently more than 17 GW) make the traditional scheduling of the power generation for the next day very unsure. Consequently the costs of the power system operation are high because of a large scale provision of spinning reserve power coming from the traditional power plants. The decisive rule in the decreasing of these costs plays the exactness of the wind energy transformation modelling process which starts with the forecast of wind speed. In Germany since more than ten years the knowledge how to solve this problem is available. Based on more than 100 representative wind farm power measurements all over Germany very exact models for the determination of the current and expected wind power are developed. The models are in operation at the control stations of the Transmission System Operators.},
  file      = {Rohrig2006appWindPredTools.pdf:Rohrig2006appWindPredTools.pdf:PDF;Rohrig2006appWindPredTools.pdf:Rohrig2006appWindPredTools.pdf:PDF},
  journal   = {Power Engineering Society General Meeting, IEEE},
  keywords  = {power generation scheduling, power measurement, wind power, wind power plants Germany, Germany power system, Transmission System Operators, power generation scheduling, power system operations, spinning reserve power, stochastic character, wind energy, wind farm power measurements, wind power prediction tools, wind speed forecasting},
  owner     = {sotterson},
  timestamp = {2008.07.03},
}

@TechReport{HRensembleHR10finalrep,
  author      = {WEPROG},
  title       = {High Resolution Ensemble for Horns Rev - HRensembleHR -},
  institution = {WEPROG},
  year        = {2010},
  abstract    = {The wind farm output of an offshore-farm such as Horns Rev changes between nearly constant output to highly variable power output. A balance responsible will therefore benefit from knowing the variability of a wind farm in advance. Some understanding of the observed variability and the corresponding forecast error on offshore wind farms had been gathered in the past few years, while a large fraction (about 60\%) of the error still lacked understanding and required further intense research. This was the outset at the beginning of the HREnsemble project. It was found that there are obvious sources, which cause planetary boundary layer eddies to dominate the wind power production, such as cold air flowing over a warm sea surface. The spurious nature of power can however also appear under reversed conditions, which is more complex to explain, as this usually takes place under very specific weather conditions, which mostly do not occur often enough within a reasonable time frame of for example a year to draw realistic conclusions. Although ensemble prediction systems in moderate resolution provide an advantage in comparison to deterministic single forecasts especially because of the possibility to determine confidence intervals. However, these intervals are not always related to the mesoscale generated variability of the wind farm output. To produce a realistic variability of the wind offshore and to make use of the advantages from an ensemble of forecasts, the numerical weather prediction model resolution of the ensemble needs to approach the horizontal extend of the wind farm. To study these interactions further, the consortium has combined developments in ocean, weather and wind power prediction. In fact, beside the project?s focus on improving the wind power prediction, the prediction of meso-scale generated variability on the wind farm power output was studies intensively.},
  comment     = {Offshore ramp prediction, power curves, etc.
* use for Horns Rev and maybe Nysted?
* orthog power curve unloved
-- orthogonal overestimates full load hours (top of p. 87)
-- for ramps, this may be because it operated on an ensemble mean instead of the ensembles? (p. 89)},
  file        = {HRensembleHR10finalrep.pdf:HRensembleHR10finalrep.pdf:PDF},
  groups      = {Ensemble, doReadNonWPV_1},
  owner       = {scot},
  timestamp   = {2011.04.11},
  url         = {http://www.weprog.com/publications},
}

@Article{Morrissey10windPowDens,
  author    = {Morrissey, Mark L. and Cook, Werner E. and Greene, J. Scott},
  title     = {An Improved Method for Estimating the Wind Power Density Distribution Function},
  journal   = {Journal of Atmospheric and Oceanic Technology},
  year      = {2010},
  volume    = {27},
  number    = {7},
  pages     = {1153--1164},
  month     = mar,
  issn      = {0739-0572},
  abstract  = {The wind power density (WPD) distribution curve is essential for wind power assessment and wind turbine
engineering. The usual practice of estimating this curve from wind speed data is to first estimate the wind
speed probability density function (PDF) using a nonparametric or parametric method. The density function
is then multiplied by one-half the wind speed cubed times the air density. Unfortunately, this means that
minor errors in the estimation of the wind speed PDF can result in large errors in the WPD distribution curve
because the cubic term in the WPD function magnifies the error. To avoid this problem, this paper presents
a new method of estimating the WPD distribution curve through a direct estimation of the curve using a
Gauss Hermitian(?) expansion. It is demonstrated that the proposed method provides a much more reliable estimate
of the WPD distribution curve.},
  booktitle = {Journal of Atmospheric and Oceanic Technology},
  comment   = {doi: 10.1175/2010JTECHA1390.1
Review:
Direct estimation of wind power pdf from wind speed values is better than estimating the wind speed pdf and then applying a power curve. Possibly a way to do BMA for ensembles, or just for ordinary point forecasts.},
  doi       = {10.1175/2010JTECHA1390.1},
  file      = {Morrissey10windPowDens.pdf:Morrissey10windPowDens.pdf:PDF},
  groups    = {Ensemble, PointDerived, doReadWPV_2},
  owner     = {sotterson},
  publisher = {American Meteorological Society},
  timestamp = {2013.09.27},
}

@InProceedings{Hu15ContinRBMdeepNNWindSpdFrcstHK,
  author       = {Hu, Yanxing and Liu, James NK and You, Jane and Chan, Pak Wai},
  title        = {Continuous RBM Based Deep Neural Network for Wind Speed Forecasting in Hong Kong},
  booktitle    = {Proceedings of the International Conference on Image Processing, Computer Vision, and Pattern Recognition (IPCV)},
  year         = {2015},
  pages        = {368},
  organization = {The Steering Committee of The World Congress in Computer Science, Computer Engineering and Applied Computing (WorldComp)},
  abstract     = {The wind speed forecasting in Hong Kong is more
difficult than in other places in the same latitude for two reasons:
the great affect from the urbanization of Hong Kong in the long
term, and the very high wind speeds brought by the tropical
cyclones. Therefore, prediction model with higher learning ability
is in need for the wind speed forecast in Hong Kong. In this
paper, we try to employ the Deep Neural Network (DNN) to
solve the time series problem of wind speed forecasting in Hong
Kong since it is believed that Neural Network (NN) with deep
architectures can provide higher learning ability than shallow NN
model. Especially, in our paper, we use the continuous Restricted
Boltzmann Machine (CRBM) to build the network architecture
of the DNN. The CRBM is the continuous valued version of the
classical binary valued Restricted Boltzmann Machine (RBM).
Compared with the Stacked Auto-Encoder (SAE) model applied
in our previous study, this CRBM model is more generative, and
therefore more suitable for simulating the data in wind speed
domain.
In our research, we employ the DNN to process the massive
wind speed data involving millions of hourly records provided
by The Hong Kong Observatory (HKO)1. The results show that
the applied approach is able to provide a better features space
for computational models in wind speed data domain, and this
approach is also a new potential tool for the feature fusion of
continuous valued time series problems.
Keywords—Deep Neural Network, Continuous Restricted Boltz-
mann Machine, Wind Speed Forecasting, Feature Representation},
  file         = {:Hu15ContinRBMdeepNNWindSpdFrcstHK.pdf:PDF},
}

@TechReport{Allen09quantRgrsMinRisk,
  author      = {David E. Allen and Abhay Kumar Singh},
  title       = {Minimizing Loss at Times of Financial Crisis: Quantile Regression as a Tool for Portfolio Investment Decisions},
  institution = {School of Accounting, Finance and Economics, Edith Cowan University},
  year        = {2009},
  number      = {Working Paper 0912},
  month       = oct,
  abstract    = {The worldwide impact of the Global Financial Crisis on stock markets, investors and fund
managers has lead to a renewed interest in tools for robust risk management. Quantile
regression is a suitable candidate and deserves the interest of financial decision makers given
its remarkable capabilities for capturing and explaining the behaviour of financial return
series more effectively than the ordinary least squares regression methods which are the
standard tool. In this paper we present quantile regression estimation as an attractive
additional investment tool, which is more efficient than Ordinary Least Square in analyzing
information across the quantiles of a distribution. This translates into the more accurate
calibration of asset pricing models and subsequent informational gains in portfolio
formation. We present empirical evidence of the effectiveness of quantile regression based
techniques as applied across the quantiles of return distributions to derive information for
portfolio formation. We show, via stocks in Dow Jones Industrial Index, that at times of
financial setbacks such as the Global Financial Crisis, a portfolio of stocks formed using
quantile regression in the context of the Fama-French three factor model, performs better
than the one formed using traditional OLS.
Keywords: Factor models; Portfolio optimization; Quantile regression},
  comment     = {Using probabilistic forecasts to minimize risk in a financial application. It is said to work better because the full distribution is used. I think the tails. Related to spinning reserves, wind farm clustering, etc.

Lots of papers like this can be found in google scholar},
  file        = {Allen09quantRgrsMinRisk.pdf:Allen09quantRgrsMinRisk.pdf:PDF},
  groups      = {Use, doReadNonWPV_2},
  owner       = {sotterson},
  timestamp   = {2013.10.22},
}

@TechReport{Zervos16renewGlblStatRep,
  author      = {Arthouros Zervos},
  title       = {Renewables 2016 Global Status Report: REN21. ISBN 978-3-9818107-0-7},
  institution = {REN21 Secretariat. Paris},
  year        = {2016},
  abstract    = {The year 2015 was an extraordinary one for renewable energy. High-profile agreements were made by G7 and G20 governments
to accelerate access to renewable energy and to advance energy efficiency. The United Nations General Assembly adopted a
dedicated Sustainable Development Goal on Sustainable Energy for All (SDG 7). Despite a dramatic decline in global fossil fuel
prices, the world saw the largest global capacity additions from renewables to date. However, continuing fossil fuel subsidies and
low fossil fuel prices did slow growth in the heating and cooling sector, in particular.
Precedent-setting commitments to renewable energy were made by regional, state and local governments as well as by the private
sector. Global investment in renewables reached a new high, with investment in developing countries surpassing that of industrialised
countries. The year culminated with the United Nations Framework Convention on Climate Change?s (UNFCCC) 21st Conference of
the Parties (COP21) in Paris, where 195 countries agreed to limit global warming to well below 2 degrees Celsius.
Renewables are now cost-competitive with fossil fuels in many markets and are established around the world as mainstream
sources of energy. Renewable power generating capacity saw its largest increase ever. Modern renewable heat capacity also
continued to rise, and renewables use expanded in the transport sector. Distributed renewable energy is advancing rapidly to close
the gap between the energy haves and have-nots.
However, in order to increase energy access while at the same time meeting the target of limiting global temperature increase to
2 degrees Celsius, remaining fossil fuel reserves will have to be kept in the ground, and both renewable energy and energy efficiency
will have to be scaled up dramatically.
Similar to the renewable energy field itself, the Renewables Global Status Report is the sum of many parts. At its heart is a multi-
stakeholder network that collectively shares its insight and knowledge. These experts engage in the GSR process, giving their time,
contributing data and providing comment. Today the network stands at 700 renewable energy, energy access and energy efficiency
experts.
On behalf of the REN21 Secretariat, I would like to thank all those who have contributed to the successful production of this year?s
report. These include primary lead author Janet L. Sawin, lead authoring team members Kristen M. Seyboth and Freyr Sverrisson,
the section authors, GSR project manager, Rana Adib and the entire team at the REN21 Secretariat, under the leadership of REN21?s
Executive Secretary Christine Lins.
This year?s report clearly demonstrates the enormous potential of renewables. However, to accelerate the transition to a healthier,
more secure and climate-safe future, we need to build a smarter, more flexible system that maximises the use of variable sources of
renewable energy and that accommodates both centralised and decentralised as well as community-based generation.},
  comment     = {Says that Colombia's renewable energy goals are:

Colombia penetration targets
Electricity (grid-connected) k 3.5% by 2015; 6.5% by 2020
Electricity (off-grid) k 20% by 2015; 30% by 2020

So, by 2020, they are just about ready to need forecasting, according to:
http://www.evernote.com/l/AA1BJb_Z2o5DBqwmnLpB6TH9Q0woSl4jYEw/},
  file        = {Zervos16renewGlblStatRep.pdf:Zervos16renewGlblStatRep.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2017.03.26},
  url         = {http://www.ren21.net/status-of-renewables/global-status-report/},
}

@Misc{Veillette10AlphaStableDistMatlab,
  author       = {Mark Veillette},
  title        = {Alpha Stable distributions in MATLAB},
  howpublished = {Web Page},
  year         = {2010},
  abstract     = {The?following?gives?a?brief?introduction?to?the?Levy?alpha?stable?distribtuion?and?some
MATLAB?functions?I've?written?pertaining?to?this?distribtuion.?There?are?several?issues
which?make?dealing?with?these?distribtuions?difficult,?including?infinite?means?and?variances,
and?the?fact?that?the?pdf?or?cdf?cannot?be?written?in?closed?form.
MATLAB?does?not?currently?have?built?in?support?for?this?distribution,?but?there?is?third
party?software?which?has?several?well?developed?methods.?This?software?is?not?free?(in
general).?Information?can?be?found?here.
Alpha?stable?distributions?arise?naturally?in?the?study?of?heavy?tailed?distribtuions,?and?have
found?applications?in?economics?and?physics?as?models?of?rare,?but?extreme?events?(such?as
earthquakes?or?stock?market?crashes).?I?expect?these?models?to?grow?in?popularity?since?many
believe?that?some?of?our?recent?financial?troubles?happend?because?analysts?limited
themselves?to?Gaussian?models?(which?do?not?have?heavy?tails).
We'll?start?with?some?background?about?alpha?stable?distributions,?and?then?go?through?the
functions?I've?written.?An?excellent?introduction?to?these?distributions?is?[ST].
Download?these?functions?here},
  comment      = {How to estimate alpha-stable distributions in Matlab (a superset of Cauchy, which are good for wind power error: Hodge11frcstErrDistTime). Axel used this for his extreme value quantile extrapolation but noted that it was not bounded like real wind power. See my OneNote notes from that meeting.

Can also estimate Cauchy from quantiles: Cook10distFromQuant
Is this better than Johnson? George11estJohnsonDistBnd

Seems to use the Stable Toolbox from Matlab Central:
http://de.mathworks.com/products/connections/product_detail/product_35730.html},
  file         = {Veillette10AlphaStableDistMatlab.pdf:Veillette10AlphaStableDistMatlab.pdf:PDF},
  url          = {http://math.bu.edu/people/mveillet/html/alphastablepub.html},
}

@Article{Bengio09lrnDeepArchAI,
  author   = {Yoshua Bengio},
  title    = {Learning Deep Architectures for AI},
  journal  = {Foundations and Trends® in Machine Learning},
  year     = {2009},
  volume   = {2},
  number   = {1},
  pages    = {1-127},
  issn     = {1935-8237},
  abstract = {Theoretical results suggest that in order to learn the kind of com-
plicated functions that can represent high-level abstractions (e.g., in
vision, language, and other AI-level tasks), one may need deep architec-
tures. Deep architectures are composed of multiple levels of non-linear
operations, such as in neural nets with many hidden layers or in com-
plicated propositional formulae re-using many sub-formulae. Searching
the parameter space of deep architectures is a difficult task, but learning
algorithms such as those for Deep Belief Networks have recently been
proposed to tackle this problem with notable success, beating the state-
of-the-art in certain areas. This monograph discusses the motivations
and principles regarding learning algorithms for deep architectures, in
particular those exploiting as building blocks unsupervised learning of
single-layer models such as Restricted Boltzmann Machines, used to
construct deeper models such as Deep Belief Networks.},
  comment  = {Section 2 is said to be an informal explanation for why deep NN's can be better than shallow ones.

Other explanations:
  Pascanu2013numRespRgnsDeepNNpieceLin
  Montufar2014numLinRegnDeepNN

Guy who said it was an informal explanation: http://neuralnetworksanddeeplearning.com/chap5.html},
  doi      = {10.1561/2200000006},
  file     = {:Bengio09lrnDeepArchAI.pdf:PDF},
  url      = {http://dx.doi.org/10.1561/2200000006},
}

@Article{Zanobetti00addDistLagMort,
  author    = {Zanobetti, A and Wand, MP and Schwartz, J and Ryan, LM},
  title     = {Generalized additive distributed lag models: quantifying mortality displacement},
  journal   = {Biostatistics},
  year      = {2000},
  volume    = {3},
  pages     = {279--92},
  abstract  = {There are a number of applied settings where a response is measured repeatedly over time, and the impact of a stimulus at one time is distributed over several subsequent response measures. In the motivating application the stimulus is an air pollutant such as airborne particulate matter and the response is mortality. However, several other variables (e.g. daily temperature) impact the response in a possibly non-linear fashion. To quantify the effect of the stimulus in the presence of covariate data we combine two established regression techniques: generalized additive models and distributed lag models. Generalized additive models extend multiple linear regression by allowing for continuous covariates to be modeled as smooth, but otherwise unspecified, functions. Distributed lag models aim to relate the outcome variable to lagged values of a time-dependent predictor in a parsimonious fashion. The resultant, which we call generalized additive distributed lag models, are seen to effectively quantify the so-called 'mortality displacement effect' in environmental epidemiology, as illustrated through air pollution/mortality data from Milan, Italy.},
  comment   = {Distributed spline lag model w/ L2 penalty (ridge regression)
* good explanation of another Least-Squarable lag technique (w/ regularization)

* estimating spline lag coeffs adds p+K parameters over instantaneous linear regression
-- K is the num. of knots; p is the order of the spline
-- generally fewer params than literal lag blowout
-- num. of knots controls complexity of spline (here, curviness of lag shape along time)
-- is indep of max lag, so you can conceivably make a good model w/ long lag and few knots
-- BUT correlation input matrix is even bigger than literal lag blowout!
-- and MUCH bigger than in polynomial regression
-- so computation is heavy even though there are fewer parameters
-- see end of this review for explanation
* says that splines get too wiggly w/o the L2 penalty
* would L1 penalty (LASSO) be even better?
* some kind of penalty improved on this approach: Harezlak07funcRgrsnPenalties
* better yet: covariance matrix shinkage as hinted at in graphical model paper
-- LASSO somewhat worse that shrinkage on BN's: Kramer09gaussGraphModelsLasso

-- this was for correlation graphs, not lagged regression, but maybe the idea is the same
-- the shrinkage model: Schafer05shrinkCov and Opgen-Rhein07rankShrink

----- this is in R library: corpcor, esp. a linear regression model
----- it shrinks the correlation matrix, which will be a little BIGGER with splined lags than w/ literal blowout but maybe that's OK?

How to use splines w/ shrinkage instead of L2 penalty used in this paper min:

| | y-X*T*theta | + L2 penalty (just above eq. 4) |

just set L2 penalty to zero (lambda=0) and then have regular least squares problem.

Can reconstruct the beta's (delayed regression coeffs if delays are blown out) using (eq 4), setting lambda=0 again.

NUMBER OF PARAMETERS TO ESTIMATE
* max lag is q
* polynomial order is p
* K is the number of spline knots
literal blowout: q+1
polynomial: p [McDowell04polyDistLag]
spline: p+K+1

SIZE OF REGRESSION PROBLEM AFTER LAG EXPANSION
* in num. of columns after lag-expanding one varible
literal blowout: q+1
polynomial: p +1 [McDowell04polyDistLag]
spline: K + q + 2

Dimensions after spline transform before linear regression (from just above eq. 4). Includes one column at zero lag, which isn't "extra." (num. samples is T (T is also a matrix in this paper, so the notation is confusing):
X [dim(T-q, q+2)] * T [dim(q+2, p+2+K)] ==> dim(X*T) == (T-q, K+q+2)

num. columns is bigger than max lag, so this is even bigger than literal blowout, even though num. of parameters is fewer (K+p instead of q for literal blowout)},
  file      = {Zanobetti00addDistLagMort.pdf:Zanobetti00addDistLagMort.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  timestamp = {2010.08.04},
  url       = {http://www.ncbi.nlm.nih.gov/pubmed/12933509},
}

@Misc{Sjostrand05matlabLassoLarsSpca,
  author    = {K. Sj{\"{o}}strand},
  title     = {Matlab implementation of {LASSO, LARS,} the elastic net and {SPCA}},
  month     = jun,
  year      = {2005},
  note      = {Version 2.0},
  abstract  = {There are a number of interesting variable selection methods available beside the regular forward selection and stepwise selection methods. Such approaches include {LASSO} (Least Absolute Shrinkage and Selection Operator), least angle regression (LARS) and elastic net (LARS-EN) regression. There also exists a method for calculating principal components with sparse loadings. This software package contains Matlab implementations of these functions. The standard implementations of these functions are available as add-on packages in {S-}Plus and R.},
  comment   = {Matlab code for LARS, LASSO and SPCA See: http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3897/zip/imm3897.zip},
  keywords  = {{LASSO,} {LARS,} {SPCA,} Matlab, Elastic Net, Sparse, Sparsity, Variable selection},
  location  = {Richard Petersens Plads, Building 321, {DK-}2800 Kgs. Lyngby},
  owner     = {sotterson},
  publisher = {Informatics and Mathematical Modelling, Technical University of Denmark, {DTU}},
  timestamp = {2009.09.08},
  url       = {http://www2.imm.dtu.dk/pubdb/p.php?3897},
}

@Article{StatisticsSolutions17KendallAndSpearman,
  author    = {StatisticsSolutions},
  title     = {Kendall?s Tau and Spearman?s Rank Correlation Coefficient},
  journal   = {Statistics Solutions Blog},
  year      = {2017},
  abstract  = {There are two accepted measures of non-parametric rank correlations: Kendall?s tau and Spearman?s (rho) rank correlation coefficient.},
  comment   = {Compares Kendall's and Spearman's.  Spearman's is more widely used but maybe Kendall's is better? They are very similar and "invariably lead to the same inferences."

Advantages of using Kendall?s tau over Spearman's rho
* Distribution of Kendall?s tau has better statistical properties.
 - p-values more accurate for smaller sample sizes
 - "insensitive to error"
 - usually smaller values (good?)
* The interpretation of Kendall?s tau in terms of the probabilities of observing the agreeable (concordant) and non-agreeable (discordant) pairs is very direct.

},
  file      = {:papers\\StatisticsSolutions17KendallAndSpearman.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.07.05},
  url       = {http://www.statisticssolutions.com/kendalls-tau-and-spearmans-rank-correlation-coefficient/},
}

@Article{Porcu07covarStatSpaceTime,
  author    = {E. Porcu and J. Mateu and M. Bevilacqua},
  title     = {Covariance functions that are stationary or nonstationary in space and stationary in time},
  journal   = {Statistica Neerlandica},
  year      = {2007},
  volume    = {61},
  pages     = {358--382},
  abstract  = {There is a great demand for statistical modelling of phenomena that evolve in both space and time, and thus, there is a growing literature on covariance function models for spatio-temporal processes. Although several nonseparable space2013time covariance models are available in the literature, very few of them can be used for spatially anisotropic data. In this paper, we propose a new class of stationary nonseparable covariance functions that can be used for both geometrically and zonally anistropic data. In addition, we show some desirable mathematical features of this class. Another important aspect, only partially covered by the literature, is that of spatial nonstationarity. We show a very simple criteria allowing for the construction of space2013time covariance functions that are nonseparable, nonstationary in space and stationary in time. Part of the theoretical results proposed in the paper will then be used for the analysis of Irish wind speed data as in HASLETT and RAFTERY (Applied Statistics, 38, 1989, 1).},
  comment   = {Use for wind velocity basis function, upscaling, offsite observations?
See also Genton07spaceTimeCovMatrixApprox, LI07nonParaAssessSpaceTimeCovar},
  doi       = {10.1111/j.1467-9574.2007.00364.x},
  owner     = {scotto},
  timestamp = {2008.12.27},
  url       = {http://www3.interscience.wiley.com/journal/118480976/abstract?CRETRY=1&SRETRY=0},
}

@Article{Ghosh02cbDistMargCovNoGaussCpla,
  author    = {Ghosh, Soumyadip and Henderson, Shane G},
  title     = {Chessboard distributions and random vectors with specified marginals and covariance matrix},
  journal   = {Operations Research},
  year      = {2002},
  volume    = {50},
  number    = {5},
  pages     = {820--834},
  abstract  = {There is a growing need for the ability to specify and generate correlated random variables as primitive

inputs to stochastic models. Motivated by this need, several authors have explored the generation of

random vectors with specied marginals, together with a specied covariance matrix, through the use

of a transformation of a multivariate normal random vector.

A covariance matrix is said to be feasible for a given set of marginal distributions if a random vector

exists with these characteristics. We develop a computational approach for establishing whether a given

covariance matrix is feasible for a given set of marginals. The approach is used to rigorously establish

that there are sets of marginals with feasible covariance matrix that the normal transformation technique

referred to above cannot match.

An important feature of our analysis is that we show that for almost any covariance matrix (in a

certain precise sense), our computational procedure either explicitly provides a construction of a random

vector with the required properties, or establishes that no such random vector exists.

We also provide two new methodologies that may be used to deal with the situation where one cannot

exactly match the desired marginals and covariance matrix using the normal transformation technique.

The new methodologies possess certain advantages over other approaches that have been suggested in

the past.},
  comment   = {Shows that a checkerboard dist can exist and can be reproduced.  Matters because it's used in XX to prove that a desired Pearson correlation matrix, in combination with uniform margins, is unattainable via the Gaussian copula.},
  file      = {:papers\\Ghosh02cbDistMargCovNoGaussCpla.pdf:PDF},
  owner     = {sotterson},
  publisher = {INFORMS},
  timestamp = {2017.07.05},
  url       = {http://www.academia.edu/download/43767639/Chessboard_Distributions_and_Random_Vect20160315-1498-wghlxj.pdf},
}

@Article{Cormode10histoWvltProb,
  author    = {Cormode, G. and Garofalakis, Minos},
  title     = {Histograms and Wavelets on Probabilistic Data},
  journal   = {Knowledge and Data Engineering, IEEE Transactions on},
  year      = {2010},
  volume    = {22},
  number    = {8},
  pages     = {1142--1157},
  month     = aug,
  issn      = {1041-4347},
  abstract  = {There is a growing realization that uncertain information is a first-class citizen in modern database management. As such, we need techniques to correctly and efficiently process uncertain data in database systems. In particular, data reduction techniques that can produce concise, accurate synopses of large probabilistic relations are crucial. Similar to their deterministic relation counterparts, such compact probabilistic data synopses can form the foundation for human understanding and interactive data exploration, probabilistic query planning and optimization, and fast approximate query processing in probabilistic database systems. In this paper, we introduce definitions and algorithms for building histogram- and Haar wavelet-based synopses on probabilistic data. The core problem is to choose a set of histogram bucket boundaries or wavelet coefficients to optimize the accuracy of the approximate representation of a collection of probabilistic tuples under a given error metric. For a variety of different error metrics, we devise efficient algorithms that construct optimal or near optimal size B histogram and wavelet synopses. This requires careful analysis of the structure of the probability distributions, and novel extensions of known dynamic-programming-based techniques for the deterministic domain. Our experiments show that this approach clearly outperforms simple ideas, such as building summaries for samples drawn from the data distribution, while taking equal or less time.

Index Terms?Histograms, wavelets, probabilistic data.},
  comment   = {Several aggregations of probabilistic data, including (I think) one which is the probability of a sum being > X (very close to a quantile). Bins and histograms reduce the combinatorial problem to a managable size.

See also: Lave13solarVarWvlt},
  doi       = {10.1109/TKDE.2010.66},
  file      = {Cormode10histoWvltProb.pdf:Cormode10histoWvltProb.pdf:PDF},
  groups    = {Upscaling (prob), doReadNonWPV_1},
  keywords  = {Haar transforms;data reduction;database management systems;dynamic programming;probability;wavelet transforms;Haar wavelet-based synopses;data reduction technique;database management;dynamic-programming;histogram-based synopses;probabilistic data;probability distribution;Histograms;probabilistic data.;wavelets},
  owner     = {sotterson},
  timestamp = {2014.03.21},
}

@Article{Wyner17adaboostRandFrstInterp,
  author    = {Wyner, Abraham J and Olson, Matthew and Bleich, Justin and Mease, David},
  title     = {Explaining the success of adaboost and random forests as interpolating classifiers},
  journal   = {The Journal of Machine Learning Research},
  year      = {2017},
  volume    = {18},
  number    = {1},
  pages     = {1558--1590},
  abstract  = {There is a large literature explaining why AdaBoost is a successful classifier. The literature
on AdaBoost focuses on classifier margins and boosting’s interpretation as the optimiza-
tion of an exponential likelihood function. These existing explanations, however, have been
pointed out to be incomplete. A random forest is another popular ensemble method for
which there is substantially less explanation in the literature. We introduce a novel per-
spective on AdaBoost and random forests that proposes that the two algorithms work for
similar reasons. While both classifiers achieve similar predictive accuracy, random forests
cannot be conceived as a direct optimization procedure. Rather, random forests is a self-
averaging, interpolating algorithm which creates what we denote as a “spiked-smooth”
classifier, and we view AdaBoost in the same light. We conjecture that both AdaBoost and
random forests succeed because of this mechanism. We provide a number of examples to
support this explanation. In the process, we question the conventional wisdom that sug-
gests that boosting algorithms for classification require regularization or early stopping and
should be limited to low complexity classes of learners, such as decision stumps. We con-
clude that boosting should be used like random forests: with large decision trees, without
regularization or early stopping.
Keywords: AdaBoost, random forests, tree-ensembles, overfitting, classification},
  comment   = {How adaboost and random forests can grow model complexity to get increasingly lower training error yet do a good job of generalization.  In this regard, random forests are at least as good as adaboosted trees, if not better.  Has some nice graphs to borrow for a talk.

For random forests, the answer seems to be that an ensemble of deep deep trees, each of which "interpolates" (memorizes) its training data, has very tight decision bounds.  Yet the bootstrapping orients these boundaries in random directions across the ensemble, so that they cancel when averaging or taking the majority.  The parts that don't average are the narrow regions around the training points.  There are decent pictures of this, that could go into a talk.

There is a different but related argument made for adaboosted trees but I didn't bother to understand it because, even for random forests, I was confused by the part of the reasoning thatwas dependent upon Bayesian error, and also about what was noise and what wasn't.

Anyway, the author says that random forests are just as good as adaboosted trees, if not better.

Also: Belkin18modernMachLrnBiasVar},
  file      = {:Wyner17adaboostRandFrstInterp.pdf:PDF},
  publisher = {JMLR. org},
  url       = {http://www.jmlr.org/papers/volume18/15-240/15-240.pdf},
}

@InProceedings{Gill01interpIntrctHierGLM,
  author    = {Gill, Jeff},
  title     = {Interpreting interactions and interaction hierarchies in generalized linear models: Issues and applications},
  booktitle = {Annual Meeting of the American Political Science Association, San Francisco},
  year      = {2001},
  abstract  = {There is substantial confusion in political science and related literatures about the meaning and interpreta-
tion of interaction effects in models with non-interval, non-normal outcome variables. Often these terms are
casually thrown into the model specification without observing that their presence fundamentally changes the
interpretation of the resulting coefficients. This article addresses this lack of clarity and rigor by explaining
the conditional nature of reported coefficients and their standard errors in models with interactions, defining
the necessarily different interpretation of interactions in generalized linear models, and introducing useful hi-
erarchies of interaction effects in these specifications. Unfortunately, there is little written about specifying
the uncertainty of these interaction effects through reported standard errors, and much of which is currently
reported is actually misleading. This article fills a gap in the methodological literature by providing a general
analytical method for correctly calculating coefficient standard errors in models with second-order or higher
interactions and complex interaction specifications. The methodology is demonstrated with applications to
current work in political science. These examples demonstrate the utility of interaction hierarchy specifications
in generalized linear models by providing analyses of data from comparative politics, judicial decision-making,
and education public policy.},
  comment   = {The meaning of feature interaction terms and hierarchies in generalized linear models. Perhaps a fancy companion ot: Simonoff09transfRegrsn},
  file      = {Gill01interpIntrctHierGLM.pdf:Gill01interpIntrctHierGLM.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.02.17},
  url       = {http://jgill.wustl.edu/papers/interactions3.pdf},
}

@Book{Committee10weathMatters,
  title     = {When Weather Matters: Science and Service to Meet Critical Societal Needs},
  publisher = {Natl Academy Pr},
  year      = {2010},
  author    = {Committee on Progress and Priorities of US Weather Research and Research-to-Operations Activities and National Research Council},
  isbn      = {0309152496},
  abstract  = {There's a chunk in the Summary about renewable energy, starting on p. 12. Could copy it here.},
  comment   = {Book on weather prediction research needs, partly relevant to wind power * talks about importance of wind power ramp prediction, and cost (Milligan and Kirby 2008) * also weather research for wind power siting * maybe should read in Summary about renewable energy, starting on p. 12.},
  file      = {Committee10weathMatters.pdf:Committee10weathMatters.pdf:PDF},
  owner     = {scot},
  timestamp = {2011.05.05},
}

@Article{Gagnon17retailTariffPVdply,
  author   = {Pieter Gagnon and Wesley J. Cole and Bethany Frew and Robert Margolis},
  title    = {The impact of retail electricity tariff evolution on solar photovoltaic deployment},
  journal  = {The Electricity Journal},
  year     = {2017},
  volume   = {30},
  number   = {9},
  pages    = {22 - 28},
  issn     = {1040-6190},
  note     = {Energy Policy Institute's Seventh Annual Energy Policy Research Conference},
  abstract = {This analysis explores the impact that the evolution of retail electricity tariffs can have on the deployment of solar photovoltaics. It suggests that ignoring the evolution of tariffs resulted in up to a 36% higher prediction of the capacity of distributed PV in 2050, compared to scenarios that represented tariff evolution. Critically, the evolution of tariffs had a negligible impact on the total generation from PV—both utility-scale and distributed—in the scenarios that were examined.},
  doi      = {https://doi.org/10.1016/j.tej.2017.10.003},
  file     = {:Gagnon17retailTariffPVdply.pdf:PDF},
  keywords = {Photovoltaics, Solar, Tariffs, Rates, Evolution, Distributed, Adoption, Projection, DGen, ReEDS},
  url      = {http://www.sciencedirect.com/science/article/pii/S104061901730249X},
}

@Article{Demsar13spatialPCA,
  author    = {Dem{\v{s}}ar, Ur{\v{s}}ka and Harris, Paul and Brunsdon, Chris and Fotheringham, A Stewart and McLoone, Sean},
  title     = {Principal component analysis on spatial data: an overview},
  journal   = {Annals of the Association of American Geographers},
  year      = {2013},
  volume    = {103},
  number    = {1},
  pages     = {106--128},
  abstract  = {This article considers critically how one of the oldest and most widely applied statistical methods, principal
components analysis (PCA), is employed with spatial data. We ?rst provide a brief guide to how PCA works:
This includes robust and compositional PCA variants, links to factor analysis, latent variable modeling, and
multilevel PCA. We then present two different approaches to using PCA with spatial data. First we look at
the nonspatial approach, which avoids challenges posed by spatial data by using a standard PCA on attribute
space only. Within this approach we identify four main methodologies, which we de?ne as (1) PCA applied
to spatial objects, (2) PCA applied to raster data, (3) atmospheric science PCA, and (4) PCA on ?ows. In
the second approach, we look at PCA adapted for effects in geographical space by looking at PCA methods
adapted for ?rst-order nonstationary effects (spatial heterogeneity) and second-order stationary effects (spatial
autocorrelation). We also describe how PCA can be used to investigate multiple scales of spatial autocorrelation.
Furthermore, we attempt to disambiguate a terminology confusion by clarifying which methods are specifically
termed "spatial PCA" in the literature and how this term has different meanings in different areas. Finally, we
look at a further three variations of PCA that have not been used in a spatial context but show considerable
potential in this respect: simple PCA, sparse PCA, and multilinear PCA. Key Words: dimensionality reduction,
multivariate statistics, principal components analysis, spatial analysis and mathematical modeling, spatial data.},
  comment   = {Overview of how to do spatial PCA. Could be an improvement to Couto14weathRegimeRamps

Uncorr multilin PCA is related: Lu09UncorrMultiLinPCA},
  doi       = {10.1080/00045608.2012.689236},
  file      = {Demsar13spatialPCA.pdf:Demsar13spatialPCA.pdf:PDF},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2014.11.05},
}

@Article{Fraley11probFrcstR,
  author    = {Fraley, Chris and Raftery, Adrian E and Gneiting, Tilmann and Sloughter, JM and Berrocal, Veronica J},
  title     = {Probabilistic weather forecasting in R},
  journal   = {The R Journal},
  year      = {2011},
  volume    = {3},
  pages     = {55--63},
  abstract  = {This article describes two R packages
for probabilistic weather forecasting, ensembleBMA,
which offers ensemble postprocessing
via Bayesian model averaging (BMA), and Prob-
ForecastGOP, which implements the geostatistical
output perturbation (GOP) method. BMA
forecasting models use mixture distributions, in
which each component corresponds to an ensemble
member, and the form of the component
distribution depends on the weather parameter
(temperature, quantitative precipitation or wind
speed). The model parameters are estimated
from training data. The GOP technique uses geostatistical
methods to produce probabilistic forecasts
of entire weather fields for temperature or
pressure, based on a single numerical forecast on
a spatial grid. Both packages include functions
for evaluating predictive performance, in addition
to model fitting and forecasting.},
  comment   = {R code for probabilistic forecasting. One algorithm is BMA; the other generates its own ensemble from variogram realizations. Also includes validation code.},
  file      = {Fraley11probFrcstR.pdf:Fraley11probFrcstR.pdf:PDF},
  groups    = {Ensemble, Test, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.08},
}

@InProceedings{Mahler09sp500lagLasso,
  author    = {Nicolas Mahler},
  title     = {Modeling the S\&P 500 Index using the {Kalman} Filter and the LagLasso},
  booktitle = {Advances in Machine Learning for Computational Finance, International Workshop},
  year      = {2009},
  month     = jul,
  abstract  = {This article introduces a method to predict upward and downward monthly variations of the S\&P 500 index by using a pool of macro-economic and financial explicative variables. The method is based on the combination of a denoising step, performed by Kalman filtering, with a variable selection step, performed by a Lasso-type procedure. In particular, we propose an implementation of the Lasso method called LagLasso which includes selection of lags for individual factors. We provide promising backtesting results of the prediction model based on a naive trading rule.},
  comment   = {Use lasso to select predictor lag for stock uptick/downtick (use for wind ramps?)
* selects one lag per variable using lasso
* is predicting upward/downard S\&P 500 movement, so good for RAMP FORECASTING?
* needs Kalman filtering to detrend (economics, but this might make sense anyway)
* contrasted to a LARS approach, which selects a block of lags},
  file      = {Mahler09sp500lagLasso.pdf:Mahler09sp500lagLasso.pdf:PDF;Mahler09sp500lagLasso.pdf:Mahler09sp500lagLasso.pdf:PDF},
  location  = {London, UK},
  owner     = {sotterson},
  timestamp = {2009.08.13},
  url       = {http://web.me.com/davidrh/AMLCF09/Schedule.html},
}

@Article{Horowitz05nonParQRadd,
  author    = {Horowitz, Joel L and Lee, Sokbae},
  title     = {Nonparametric Estimation of an Additive Quantile Regression Model},
  journal   = {Journal of the American Statistical Association},
  year      = {2005},
  volume    = {100},
  number    = {472},
  pages     = {1238--1249},
  abstract  = {This article is concerned with estimating the additive components of a nonparametric additive quantile regression model. We develop an estimator that is asymptotically normally distributed with a rate of convergence in probability of na^r/(2r+1) when the additive components are r-times continuously differentiable for some r ^2. This result holds regardless of the dimension of the covariates, and thus the new estimator has no curse of dimensionality. In addition, the estimator has an oracle property and is easily extended to a generalized additive quantile regression model with a link function. The numerical performance and usefulness of the estimator are illustrated by Monte Carlo experiments and an empirical example.},
  comment   = {QR as a sum of nonlinear functions of individual inputs (a kind of dimension reduction). Has R.
* lists a test for nonlinearity (p.7)
* can't test for additivity, but can look at suggestive plots
* implemented in R w/ splines and quantreg package (actually, it's now in R quantreg package, and includes new algorithms for model selection)
* note that there are many papers before (in time) and after this paper.},
  doi       = {10.1198/016214505000000583},
  eprint    = {http://www.tandfonline.com/doi/pdf/10.1198/016214505000000583},
  file      = {Horowitz05nonParQRadd.pdf:Horowitz05nonParQRadd.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.24},
}

@Article{Alexandrov12mdrnTrndExtrct,
  author    = {Alexandrov, Theodore and Bianconcini, Silvia and Dagum, Estela Bee and Maass, Peter and McElroy, Tucker S},
  title     = {A review of some modern approaches to the problem of trend extraction},
  journal   = {Econometric Reviews},
  year      = {2012},
  volume    = {31},
  number    = {6},
  pages     = {593--624},
  abstract  = {This article presents a review of some modern approaches to trend extraction for
one-dimensional time series, which is one of the major tasks of time series analysis. The
trend of a time series is usually defined as a smooth additive component which contains
information about the time series global change, and we discuss this and other definitions
of the trend. We do not aim to review all the novel approaches, but rather to observe the
problem from different viewpoints and from different areas of expertise. The article contributes to
understanding the concept of a trend and the problem of its extraction. We present an overview
of advantages and disadvantages of the approaches under consideration, which are: the model-
based approach (MBA), nonparametric linear filtering, singular spectrum analysis (SSA), and
wavelets. The MBA assumes the specification of a stochastic time series model, which is usually
either an autoregressive integrated moving average (ARIMA) model or a state space model. The
nonparametric filtering methods do not require specification of model and are popular because of
their simplicity in application. We discuss the Henderson, LOESS, and Hodrick–Prescott filters
and their versions derived by exploiting the Reproducing Kernel Hilbert Space methodology.
In addition to these prominent approaches, we consider SSA and wavelet methods. SSA is
widespread in the geosciences; its algorithm is similar to that of principal components analysis,
but SSA is applied to time series. Wavelet methods are the de facto standard for denoising in
signal procession, and recent works revealed their potential in trend analysis.

Keywords Model-based approach; Nonparametric linear filtering; Singular spectrum analysis;
Time series; Trend; Wavelets.},
  comment   = {An interesting paper found by Jan Vibann (sp?) of TenneT.  Seems more about denoising than trend detection but is that actually the same thing and I don't know it?},
  file      = {:Alexandrov12mdrnTrndExtrct.pdf:PDF},
  publisher = {Taylor \& Francis},
}

@TechReport{Cripps03varCovarSelRegress,
  author      = {Edward Cripps and Chris Carter and Robert Kohn},
  title       = {Variable selection and covariance selection in multivariate regression models.},
  institution = {Statistical and Applied Mathematical Sciences Institute},
  year        = {2003},
  number      = {2003-14},
  month       = dec,
  abstract    = {This article provides a general framework for Bayesian variable selection and covariance selection in a multivariate regression model with Gaussian errors. By variable selection we mean allowing certain regression coefficients to be zero. By covariance selection we mean allowing certain elements of the inverse covariance matrix to be zero. We estimate all the model parameters by model averaging using a Markov chain Monte Carlo simulation method. The methodology is illustrated by applying it to four real data sets. The effectiveness of variable selection and covariance selection in estimating the multivariate regression model is assessed by using four loss functions and four simulated data sets. Each of the simulated data sets is based on parameter estimates obtained from a corresponding real data set.},
  comment     = {Bayesian linear regression variable and covariance matrix selection; partial regression; missing features; MCMC},
  file        = {Cripps03varCovarSelRegress.pdf:Cripps03varCovarSelRegress.pdf:PDF;Cripps03varCovarSelRegress.pdf:Cripps03varCovarSelRegress.pdf:PDF},
  location    = {PO Box 14006 Research Triangle Park, NC 27709-4006 www.samsi.info},
  owner       = {sotterson},
  timestamp   = {2009.02.18},
  url         = {http://www.samsi.info/TR/tr2003-14.pdf},
}

@Article{Sovacool08nukeGHGs,
  author    = {Sovacool, B.K.},
  title     = {Valuing the greenhouse gas emissions from nuclear power: A critical survey},
  journal   = {Energy Policy},
  year      = {2008},
  volume    = {36},
  number    = {8},
  pages     = {2950--2963},
  abstract  = {This article screens 103 lifecycle studies of greenhouse gas-equivalent emissions for nuclear power plants to identify a subset of the most current, original, and transparent studies. It begins by briefly detailing the separate components of the nuclear fuel cycle before explaining the methodology of the survey and exploring the variance of lifecycle estimates. It calculates that while the range of emissions for nuclear energy over the lifetime of a plant, reported from quali?ed studies examined, is from 1.4 g of carbon dioxide equivalent per kWh (g CO 2 e/kWh) to 288 g CO 2 e/kWh, the mean value is 66 g CO 2 e/kWh. The article then explains some of the factors responsible for the disparity in lifecycle estimates, in particular identifying errors in both the lowest estimates (not comprehensive) and the highest estimates (failure to consider co-products). It should be noted that nuclear power is not directly emitting greenhouse gas emissions, but rather that lifecycle emissions occur through plant construction, operation, uranium mining and milling, and plant decommissioning.},
  comment   = {Current nuclear lifecycle GHGs are about 7X that of offshore wind.},
  file      = {Sovacool08nukeGHGs.pdf:Sovacool08nukeGHGs.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2012.02.09},
}

@Misc{Cochrane18crossValTimeSer,
  author       = {Courtney Cochrane},
  title        = {Time Series Nested Cross-Validation},
  howpublished = {Medium: Towards Data Science},
  month        = may,
  year         = {2018},
  note         = {Blog Post},
  abstract     = {This blog post discusses the pitfalls of using traditional cross-validation 
with time series data. Specifically, we address 1) splitting a time series 
without causing data leakage, 2) using nested cross-validation to obtain an 
unbiased estimate of error on an independent test set, and 3) cross- 
validation with datasets that contain multiple time series.},
  comment      = {Cross validation that avoids anti-causal train/dev/test splits and also the choice of an arbitrary test period.  The most interesting idea is to assume that the time series of individuals in the test are independent (which seems like a big assumption).  Also, they pick test data from (almost) all times in the data but they way they do it is to use just  a tiny bit for train and then discard the tail ends of the data (e.g Figure 2).  This would exaggerate the error since the CV error average would include models which have used far less data than is available in the training set.  

Note that Bergmeir18crossValARfrcst doesn't worry about test/train causality,.

The explanations in this article are at best incomplete, so it would be a good idea to look at its three references.

The poulation-informed ideas are in the author's masters thesis:
populationInformedPredictSecondHalf() and populationInformedWPForwardChaining()
https://github.com/cocochrane/MastersThesis/blob/master/DataCollection.ipynb

},
  file         = {:Cochrane18crossValTimeSer.pdf:PDF},
  url          = {https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9},
}

@Book{Moler08NmrclCompMatlab,
  title     = {Numerical Computing with MATLAB: Revised Reprint},
  publisher = {Siam},
  year      = {2008},
  author    = {Moler, Cleve B},
  abstract  = {This book is an introduction to two subjects: Matlab and numerical computing.
This first chapter introduces Matlab by presenting several programs that investigate
elementary, but interesting, mathematical problems. If you already have
some experience programming in another language, we hope that you can see how
Matlab works by simply studying these programs.
If you want a more comprehensive introduction, there are many resources
available. You can select the Help tab in the toolstrip atop the Matlab command
window, then select Documentation, MATLAB and Getting Started.
A MathWorks Web site, MATLAB Tutorials and Learning Resources [11], offers
a number of introductory videos and a PDF manual entitled Getting Started with
MATLAB.
An introduction to MATLAB through a collection of mathematical and computational
projects is provided by Moler?s free online Experiments with MATLAB
[6].
A list of over 1500 Matlab-based books by other authors and publishers, in
several languages, is available at [12]. Three introductions to Matlab are of particular
interest here: a relatively short primer by Sigmon and Davis [9], a mediumsized,
mathematically oriented text by Higham and Higham [3], and a large, comprehensive
manual by Hanselman and Littlefield [2].},
  comment   = {Book on Matlab, linear algebra, etc. includes lots of matlab code

TOPICS
 Linear Equations (45 pages)
 Interpolation (27 pages)
 Zeros and Roots (25 pages)
 Least Squares (27 pages)
 Quadrature (21 pages)
 Ordinary Differential Equations (53 pages)
 Fourier Analysis (23 pages)
 Random Numbers (15 pages)
 Eigenvalues and Singular Values (39 pages)
 Partial Differential Equations (21 pages)


* Matlab code for this book available here
 http://www.mathworks.com/moler/chapters.html
* updated in 2013 (I downloaded this in 2015, so these pdfs must include those updates)
* Moler has another very basic book too:
 http://www.mathworks.com/moler/exm/chapters.html (also has lots of matlab)},
  file      = {Moler08NmrclCompMatlab.pdf:Moler08NmrclCompMatlab.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.02.04},
  url       = {http://www.mathworks.com/moler/chapters.html},
}

@InBook{Cios07featSelExtractMeth,
  chapter   = {Feature Extraction and Selection Methods},
  pages     = {133--233},
  title     = {Data Mining},
  publisher = {Springer US},
  year      = {2007},
  author    = {Krzysztof J. Cios and Roman W. Swiniarski and Witold Pedrycz and Lukasz A. Kurgan},
  abstract  = {This Chapter describes both supervised and unsupervised feature extraction methods. These include dimensionality reduction and feature extraction via unsupervised Principal Component Analysis, unsupervised Independent Component Analysis, and supervised Fisher?s linear discriminant analysis. The first two methods are linear transformations that optimally reduce dimensionality, in terms of the number of features, of the original unsupervised dataset. The Fisher?s method also implements a linear transformation that optimally converts supervised datasets into a new space that includes fewer features, which are more suitable for classification. While the above methods are mainly used with numerical (time-independent) data, we also describe two groups of methods for preprocessing of time-series data. These include Fourier transform and Wavelets and their two-dimensional versions. We also discuss Zernike moments and Singular Value Decomposition. The second part of the Chapter describes a wide variety of feature selection methods. The design of these methods is based on two components, namely, selection criteria and search methods.},
  comment   = {Review of ideas for feature reduction, generation and selection includes wavelets and MCMC techniques},
  file      = {Cios07featSelExtractMeth.pdf:Cios07featSelExtractMeth.pdf:PDF;Cios07featSelExtractMeth.pdf:Cios07featSelExtractMeth.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.01.29},
}

@InCollection{Comin14techDiffusMeasCausConseq,
  author    = {Comin, Diego and Mestieri, Marti},
  title     = {Technology diffusion: Measurement, causes, and consequences},
  booktitle = {Handbook of economic growth},
  publisher = {Elsevier},
  year      = {2014},
  volume    = {2},
  pages     = {565--622},
  abstract  = {This chapter discusses different approaches pursued to explore three broad questions
related to technology diffusion: what general patterns characterize the diffusion of tech-
nologies, and how have they changed over time; what are the key drivers of technology,
and what are the macroeconomic consequences of technology. We prioritize in our dis-
cussion unified approaches to these three questions that are based on direct measures of
technology.},
  comment   = {Go back and finish Comin06techDiffuse5facts, which also introduces the DB, if something is unclear or undefined here.},
  doi       = {10.1016/b978-0-444-53540-5.00002-1},
  file      = {:Comin14techDiffusMeasCausConseq.pdf:PDF},
  issn      = {1574-0684},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780444535405000021},
}

@InBook{Ramsay09registrAlignFDA,
  chapter     = {Chapter 8},
  pages       = {117--130},
  title       = {Registration: Aligning Features for Samples of Curves},
  publisher   = {Springer New York},
  year        = {2009},
  author      = {Ramsay, James and Hooker, Giles and Graves, Spencer and Ramsay, J.O and Hooker, Giles and Graves, Spencer},
  editor      = {Gentleman, Robert and Hornik, Kurt and Parmigiani, Giovanni},
  series      = {Use R},
  isbn        = {978-0-387-98185-7},
  abstract    = {This chapter presents two methods for separating phase variation from amplitude variation in functional data: landmark and continuous registration. We mentioned this problem in Section 1.1.1. We saw in the height acceleration curves in Figure 1.2 that the age of the pubertal growth spurt varies from girl to girl; this is phase variation. In addition, the intensity of the pubertal growth spurt also varies; this is amplitude variation. Landmark registration aligns features that are visible in all curves by estimating a strictly increasing nonlinear transformation of time that takes all the times of a given feature into a common value. Continuous registration uses the entire curve rather than specified features and can provide a more complete curve alignment. The chapter also describes a decomposition technique that permits the expression of the amount of phase variation in a sample of functional variation as a proportion of total variation.},
  affiliation = {2748, Howe Street Ottawa ON K2B 6W9 Canada},
  booktitle   = {Functional Data Analysis with R and MATLAB},
  comment     = {Smooth and align waveforms using splines. Includes regression with these.
* can enforce monotonic increase when smoothing
* can require time to smoothly and monotically increase during alignment
* can align based on derivatives (of smoothed waveforms)
* during alignment, can separate original RMS error into amplitude and time delay components (use for forecast phase error detection?)

Could possibly use to search for analog ensembles (another, probably faster way, might be the iSAX techniques)},
  doi         = {10.1007/978-0-387-98185-7_8},
  file        = {Ramsay09registrAlignFDA.pdf:Ramsay09registrAlignFDA.pdf:PDF},
  groups      = {Read, Ensemble, doReadNonWPV_2},
  keyword     = {Statistics},
  owner       = {scot},
  timestamp   = {2010.11.08},
}

@InCollection{Gjelsvik10stochProgHydro,
  author    = {Gjelsvik, Anders and Mo, Birger and Haugstad, Arne},
  title     = {Long-and medium-term operations planning and stochastic modelling in hydro-dominated power systems based on stochastic dual dynamic programming},
  booktitle = {Handbook of Power Systems I},
  publisher = {Springer},
  year      = {2010},
  pages     = {33--55},
  abstract  = {This chapter reviews how stochastic dual dynamic programming (SDDP)
has been applied to hydropower scheduling in the Nordic countries. The SDDP
method, developed in Brazil, makes it possible to optimize multi-reservoir hydro
systems with a detailed representation. Two applications are described: (1) A model
intended for the system of a single power company, with the power price as an
exogenous stochastic variable. In this case the standard SDDP algorithm has been
extended; it is combined with ordinary stochastic dynamic programming. (2) A
global model for a large system (possibly many countries) where the power price
is an internal (endogenous) variable. The main focus is on (1). The modelling of
the stochastic variables is discussed. Setting up proper stochastic models for inflow
and price is quite a challenge, especially in the case of (2) above. This is an area
where further work would be useful. Long computing time may in some cases be
a consideration. In particular, the local model has been used by utilities with good
results.
Keywords Energy economics, Hydro scheduling, Stochastic programming},
  comment   = {Stochastic programming has been used in the Nordic countries (and apparently Brasil) to plan hydro systems.},
  file      = {Gjelsvik10stochProgHydro.pdf:Gjelsvik10stochProgHydro.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.02},
}

@InBook{Sauer05reacVoltPwr,
  chapter   = {Ch. 2: Reactive Power And Voltage Control Issues In Electric Power Systems},
  pages     = {11--24},
  title     = {Applied Mathematics for Restructured Electric Power Systems},
  publisher = {Springer},
  year      = {2005},
  author    = {Sauer, P.},
  editor    = {Joe Chow and Felix Wu and James Momoh},
  abstract  = {This chapter was prepared primarily for "non power" engineers to introduce reactive power and voltage control concepts, and to identify several issues that remain as research challenges in this area. It begins with ba- sic definitions and information on reactive power, and then focuses on problems that have been known for many years and have surfaced re- cently in power system operations. These problems are described in the framework of traditional security analysis used in control centers throughout the world. The research challenges identify areas where collaboration between power systems engineers and applied mathemati- cians could yield major advances in power system reliability. Keywords: Voltage collapse, Maximum power transfer, Var reserves, reactive power, voltage control, security analysis, power transfer capability.},
  comment   = {What reactive power is. First half is OK; second half is unclear, but maybe corresponding slides (attached) are better.},
  doi       = {DOI: 10.1007/0-387-23471-3_1},
  file      = {Sauer05reacVoltPwr.pdf:Sauer05reacVoltPwr.pdf:PDF;Sauer05reacVoltPwrSlides.pptx:Sauer05reacVoltPwrSlides.pptx:PowerPoint},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2012.02.24},
  url       = {http://www.springerlink.com/content/n36l524623334610/},
}

@Article{Fridman17deepLrnSelfDrvCarClass,
  author    = {Lex Fridman},
  title     = {6.S094: Deep Learning for Self-Driving Cars (Class)},
  journal   = {Massachusetts Institute of Technology},
  year      = {2017},
  abstract  = {This class is an introduction to the practice of deep learning through the applied theme of building a self-driving car. It is open to beginners and is designed for those who are new to machine learning, but it can also benefit advanced researchers in the field looking for a practical overview of deep learning methods and their application.

Interested in the class? Here are some things you could do:

Register an account on the site to stay up-to-date. The material for the course is free and open to the public.
Follow the DeepTraffic and the DeepTesla tutorials, checkout the DeepTraffic leaderboard.
Design and evaluate neural networks for the DeepTraffic Game and the DeepTesla Simulation simulations all in the browser.
Watch the lectures and guest talks below.

Lecture Slides and Videos:
* Material marked in red indicates links that are not yet active but will soon be.

Lecture 1: Introduction to Deep Learning and Self-Driving Cars
[ Slides ] - [ Lecture Video ]
Lecture 2: Deep Reinforcement Learning for Motion Planning
[ Slides ] - [ Lecture Video ]
Lecture 3: Convolutional Neural Networks for End-to-End Learning of the Driving Task
[ Slides ] - [ Lecture Video ]
Lecture 4: Recurrent Neural Networks for Steering through Time
[ Slides ] - [ Lecture Video ]
Lecture 5: Deep Learning for Human-Centered Semi-Autonomous Vehicles
[ Slides ] - [ Lecture Video ]},
  comment   = {MIT class on deep learning w/ self driving cars.  Slides and videos for some talks.  Nice slides in the recurrent NN},
  file      = {Slides:Fridman17deepLrnSelfDrvCarClass.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.03.10},
  url       = {http://selfdrivingcars.mit.edu/},
}

@Article{Andersson00tensPLSnwayToolbox,
  author    = {Claus A Andersson and Rasmus Bro},
  title     = {The N-way Toolbox for \{MATLAB\}},
  journal   = {Chemometrics and Intelligent Laboratory Systems},
  year      = {2000},
  volume    = {52},
  number    = {1},
  pages     = {1--4},
  issn      = {0169-7439},
  abstract  = {This communication describes a free toolbox for MATLAB for analysis of multiway data. The toolbox is called The N-way Toolbox for MATLAB and is available on the internet at http://www.models.kvl.dk/source/. This communication is by no means an attempt to summarize or review the extensive work done in multiway data analysis but is intended solely for informing the reader of the existence, functionality, and applicability of the N-way Toolbox for MATLAB.

Keywords: N-way Toolbox; MATLAB; Multiway data analysis},
  comment   = {A matlab toolbox for doing tensor PLS and other stuff.

some of it is here? http://www.models.life.ku.dk/algorithms

or maybe here? http://www.models.kvl.dk/source/.

Maybe some of the Matlab tensor toolboxes written since this paper would be helpful.},
  doi       = {10.1016/S0169-7439(00)00071-X},
  file      = {Andersson00tensPLSnwayToolbox.pdf:Andersson00tensPLSnwayToolbox.pdf:PDF},
  keywords  = {N-way Toolbox},
  owner     = {sotterson},
  timestamp = {2015.02.09},
  url       = {http://www.sciencedirect.com/science/article/pii/S016974390000071X},
}

@Article{Weber10AdqtMktDsngEur,
  author    = {Christoph Weber},
  title     = {Adequate intraday market design to enable the integration of wind energy into the {Europe}an power systems},
  journal   = {Energy Policy},
  year      = {2010},
  volume    = {38},
  number    = {7},
  pages     = {3155--3163},
  issn      = {0301-4215},
  note      = {Large-scale wind power in electricity markets with Regular Papers},
  abstract  = {This contribution analyses the European electricity markets with respect to their aptitude to absorb large amounts of wind energy. Thereby in a first step the market designs of the major European power markets in France, Germany, Scandinavia, Spain and \{UK\} are reviewed, with a particular focus on liquidity in the spot and intraday markets. Then some key features of the short-term adjustments required by wind energy are discussed and the necessity of sufficient liquidity in intraday markets is highlighted. For the example of the German market subsequently the discrepancy between the physical short-term adjustment needs and the traded volumes on the intraday market is analyzed. This leads to an evaluation of proposals for improving the liquidity on the short-term market, including the use of continuous spot trading like in \{UK\} or the use of intraday auctions like in Spain.},
  comment   = {Says that intraday market not used much in Europe to satisfy day-ahead forecast errors (according cite in Zugno13tradeWindGenMrktQs)},
  doi       = {10.1016/j.enpol.2009.07.040},
  file      = {Weber10AdqtMktDsngEur.pdf:Weber10AdqtMktDsngEur.pdf:PDF},
  keywords  = {Wind integration},
  owner     = {sotterson},
  timestamp = {2015.03.05},
  url       = {http://www.sciencedirect.com/science/article/pii/S0301421509005564},
}

@TechReport{Jonsson13mktScenGen,
  author      = {Tryggvi J{\'o}nsson},
  title       = {Generation of Market Scenarios},
  institution = {The OSR Nordic Project},
  year        = {2013},
  month       = aug,
  abstract    = {This document describes parts of the work done in work package 6 (WP6) in the
OSRNordic research project. Previously, stochastic programming (SP) models have
been developed for allocating reserve capacity in a system with signi?cant wind power
penetration. These models are thoroughly described in Chen et al. (2012). The aim of
WP6 is to introduce market dynamics to the previously developed models.
The objective of including market dynamics to a model like the ones Chen et al.
(2012) describes is twofold. For one, the aim must be to better accommodate financial
risk of the system which then better addresses the operational risk in the power system.
This is done by introducing some form for a risk criteria (and attitude) in the models.
Secondly, the objective can be to let the models better reflect the circumstances at the
Nordic power market by adopting the models accordingly. In this regard, there are
some key discrepancies between the market environment in the Nordic countries and
some or all of the previously developed models.

... (this was just a chunk of the the introduction)},
  comment     = {Uses the CPRS Skill Score (CPRSS) to evaluate scenarios. From the OSR Nordic project, lots of market stuff.

TODO get reference to CRPSS (Jonsson 2012 G, I think)},
  file        = {Jonsson13mktScenGen.pdf:Jonsson13mktScenGen.pdf:PDF},
  groups      = {Test, doReadNonWPV_1},
  owner       = {Scott},
  timestamp   = {2013.10.28},
}

@Article{King04pvArrayPerf,
  author    = {D.L. King and W. E. Boyson and J.A. Kratochvil},
  title     = {Photovoltaic Array Performance Model},
  journal   = {Sandia Labs Report: SAND Report 2004-3535},
  year      = {2004},
  abstract  = {This document summarizes the equations and applications associated with the photovoltaic array
performance model developed at Sandia National Laboratories over the last twelve years.
Electrical, thermal, and optical characteristics for photovoltaic modules are included in the
model, and the model is designed to use hourly solar resource and meteorological data.  The
versatility and accuracy of the model has been validated for flat-plate modules (all technologies)
and for concentrator modules, as well as for large arrays of modules.  Applications include
system design and sizing, ?translation? of field performance measurements to standard reporting
conditions, system performance optimization, and real-time comparison of measured versus
expected system performance.},
  comment   = {How IWES finds the maximum power point.  Referred to by Saint-Drenan14commentsGenPVpow},
  file      = {King04pvArrayPerf.pdf:King04pvArrayPerf.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.30},
  url       = {http://prod.sandia.gov/techlib/access-control.cgi/2004/043535.pdf},
}

@TechReport{Hochloff14GermanPowerMarket,
  author      = {Patrick Hochloff and Malte Jansen and Dominik Jost and Dr. Stefan Bofinger},
  title       = {German Power Market: Introduction to markets, products and processes},
  institution = {Fraunhofer Institute Of Wind Energy And Energy System Technology},
  year        = {2014},
  type        = {Tech Report},
  abstract    = {This document will give the reader an introduction to the German power market. The aim is to provide a comprehensive overview over the market rules and products at the German wholesale and ancillary service markets.
The wholesale power market is introduced in chapter 3. The basic principles of electricity trading on derivative and spot markets are similar to other commodity markets. Commodity trading is typical for metals, coal, agricultural products, crude oil and natural gas. The purpose of derivative market is to hedge risks against unforeseen price developments. The seller of a commodity agrees upfront to pay a price for a specified good at a specified delivery date and the seller agrees to deliver the good at the specified time. These deals can be closed at exchanges or bilaterally between the buyer and the seller, the so-called over-the-counter trade (OTC). The European Energy Exchange (EEX) operates the power derivative market for the market area Germany. An introduction to the derivative markets is given in chapter 3.2 and to OTC trading in chapter 3.3.
A closer matching of supply and demand occurs on short-term spot markets that are operated by EPEX SPOT for the German market area. Spot markets establish power prices for every hour and even for every quarter of an hour. Chapter 3.1 introduces to the EPEX SPOT market.
However forecast of generation and consumption are not perfect but supply and demand must be in balance every time to keep a constant frequency. For this reason the TSOs procure ancillary services and especially frequency control reserve. Control reserve is an extra market that enables market participants to provide services. This market is explained in chapter 4. On the other hand, imbalances of a market participant are charged with a balancing energy price. The organisational framework of market participants and the charging of imbalances are introduced in chapter 1.},
  comment     = {Overview of how German power market works in 2014

Day ahead
* last bid is placed at noon;
* time covered by the bidding starts 12 hours after close
* last if time covered is 36 hours after close
* i.e. the possible range covered by the noon bids is 00:00-24 of the next day.},
  file        = {Hochloff14GermanPowerMarket.pdf:Hochloff14GermanPowerMarket.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2014.10.14},
}

@Book{Pineda12intrnlEnrgyMktEU,
  title     = {Creating the Internal Energy Market in Europe},
  publisher = {European Wind Energy Association},
  year      = {2012},
  author    = {Iv{\'a}n Pineda and Paul Wilczek},
  editor    = {Zo{\"o} Casey},
  isbn      = {978-2-930670-01-0},
  abstract  = {This EWEA report serves two main purposes:

* Contribute to the debate on the completion of an
Internal Energy Market (IEM) by 2014 and provide
views on present electricity market integration ap-
proaches and the development of a future flexible
power system with a large scale uptake of wind
power.

* Place the current regulatory frameworks for wind
power integration in the context of developing a
single EU market for energy. To this end, it com-
pares the impact of wind energy deployment with
the major obstacles blocking and integrated inter-
nal market.},
  comment   = {EU plan for energy market modernization.  Big report.},
  file      = {Pineda12intrnlEnrgyMktEU.pdf:Pineda12intrnlEnrgyMktEU.pdf:PDF},
  journal   = {European Wind Energy Association},
  owner     = {sotterson},
  timestamp = {2017.04.13},
  url       = {http://www.ewea.org/uploads/tx_err/Internal_energy_market.pdf},
}

@Article{MathWorks17simDetVarCopula,
  author    = {MathWorks},
  title     = {Simulating Dependent Random Variables Using Copulas (version R2017a)},
  journal   = {Matlab Statistics and Machine Learning Toolbox Documentation},
  year      = {2017},
  abstract  = {This example shows how to use copulas to generate data from multivariate distributions when there are complicated relationships among the variables, or when the individual variables are from different distributions.

MATLAB? is an ideal tool for running simulations that incorporate random inputs or noise. Statistics and Machine Learning Toolbox? provides functions to create sequences of random data according to many common univariate distributions. The Toolbox also includes a few functions to generate random data from multivariate distributions, such as the multivariate normal and multivariate t. However, there is no built-in way to generate multivariate distributions for all marginal distribtions, or in cases where the individual variables are from different distributions.

Recently, copulas have become popular in simulation models. Copulas are functions that describe dependencies among variables, and provide a way to create distributions to model correlated multivariate data. Using a copula, a data analyst can construct a multivariate distribution by specifying marginal univariate distributions, and choosing a particular copula to provide a correlation structure between variables. Bivariate distributions, as well as distributions in higher dimensions, are possible. In this example, we discuss how to use copulas to generate dependent multivariate random data in MATLAB, using Statistics and Machine Learning Toolbox.},
  comment   = {How to do dependent RV simulation using copulas.  Has equations for converting linear correlation to Spearmans Rank Correlation or Kendal's Tau (but which Kendal's "tied pair" is this? There are 3, see energytop.org)},
  file      = {:MathWorks17simDetVarCopula.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.20},
  url       = {https://www.mathworks.com/help/stats/examples/simulating-dependent-random-variables-using-copulas.html},
}

@Misc{CribariNeto09betaRgrssnR,
  author    = {Cribari-Neto, Francisco and Zeileis, Achim},
  title     = {Beta regression in R},
  year      = {2009},
  abstract  = {This introduction to the R package betareg is a (slightly) modified version of Cribari-
Neto and Zeileis (2010), published in the Journal of Statistical Software. A follow-up paper
with various extensions is Gruun, Kosmidis, and Zeileis (2012) ? a slightly modified version
of which is also provided within the package as vignette("betareg-ext", package =
"betareg")
The class of beta regression models is commonly used by practitioners to model variables
that assume values in the standard unit interval (0, 1). It is based on the assumption
that the dependent variable is beta-distributed and that its mean is related to a set of
regressors through a linear predictor with unknown coefficients and a link function. The
model also includes a precision parameter which may be constant or depend on a (potentially
different) set of regressors through a link function as well. This approach naturally
incorporates features such as heteroskedasticity or skewness which are commonly observed
in data taking values in the standard unit interval, such as rates or proportions. This paper
describes the betareg package which provides the class of beta regressions in the R system
for statistical computing. The underlying theory is briefly outlined, the implementation
discussed and illustrated in various replication exercises.
Keywords: beta regression, rates, proportions, R.},
  comment   = {Quick beta regression info in R. SEe the two papers it implements for more detail:},
  file      = {2014 version:CribariNeto14betaRgrsnInR.pdf:PDF;2009 verson:CribariNeto09betaRgrssnR.pdf:PDF},
  owner     = {sotterson},
  publisher = {Department of Statistics and Mathematics x, WU Vienna University of Economics and Business},
  timestamp = {2014.07.18},
  url       = {http://epub.wu.ac.at/726/},
}

@Article{Kleiber02strucchange,
  author    = {Kleiber, C. and Hornik, K. and Leisch, F. and Zeileis, A.},
  title     = {strucchange: An {R} Package for Testing for Structural Change in Linear Regression Models},
  journal   = {Journal of Statistical Software},
  year      = {2002},
  volume    = {7},
  number    = {02},
  abstract  = {This introduction to the R package strucchange is a (slightly) modified version of Zeileis, Leisch, Hornik, and Kleiber (2002), which reviews tests for structural change in linear regression models from the generalized uctuation test framework as well as from the F test (Chow test) framework. Since Zeileis et al. (2002) various extensions were added to the package, in particular related to breakpoint estimation (also know as \dating", discussed in Zeileis, Kleiber, Kramer, and Hornik 2003) and to structural change tests in other parametric models (Zeileis 2006). A more unifying view of the underlying theory is presented in Zeileis (2005) and Zeileis, Shah, and Patnaik (2010). Here, we focus on the linear regression model and introduce a unified approach for implementing tests from the uctuation test and F test framework for this model, illustrating how this approach has been realized in strucchange. Enhancing the standard significance test approach the package contains methods to fit, plot and test empirical uctuation processes (like CUSUM, MOSUM and estimates-based processes) and to compute, plot and test sequences of F statistics with the supF, aveF and expF test. Thus, it makes powerful tools available to display information about structural changes in regression relationships and to},
  comment   = {tests for changes in linear regressions. One option is online. * based on thresholds for expected errors with significance testing. * This was taken from the 12/2010 R package documentation.},
  file      = {Kleiber02strucchange.pdf:Kleiber02strucchange.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  publisher = {American Statistical Association},
  timestamp = {2011.02.28},
  url       = {http://cran.r-project.org/web/packages/strucchange/index.html},
}

@Book{Owen13monteCarloBook,
  title     = {Monte Carlo theory, methods and examples},
  year      = {2013},
  author    = {Art B. Owen},
  abstract  = {This is a book about the Monte Carlo method. The core idea of Monte Carlo is
to learn about a system by simulating it with random sampling. That approach
is powerful, flexible and very direct. It is often the simplest way to solve a
problem, and sometimes the only feasible way.
The Monte Carlo method is used in almost every quantitative subject of
study: physical sciences, engineering, statistics, finance, and computing, includ-
ing machine learning and graphics. Monte Carlo is even applied in some areas,
like music theory, that are not always thought of as quantitative.
The Monte Carlo method is both interesting and useful. In this book we will
look at the ideas behind Monte Carlo sampling and relate them to each other.
We will look at the mathematical properties of the methods to understand when
and why they work. We will also look at some important practical details that
we need to know in order to get reliable answers to serious problems. Often the
best way to see the ideas and practical details is through an example, and so
worked examples are included throughout.},
  comment   = {Many nice things on markov processes, including copulas, matrix normal distribution, ...

* info on matrix normal distribution (used in e.g. Anderlucci15covPatMix)
* Example of a multivariate distribution you can't model with a normal copula (p. 50, reference to Ghosh03nortaCorrRVdimNoGaussCpla).

Matrix Normal Distribution
* Useful for a random matrix where both rows and columns are correlated
* Matrix normal dist is really just a normal distribution
   - covariance matrix decomposed into the elementwise product of two separate covariance matrices.
   - Cov matrices are constrained by matrix c, or is trace(gamma) and alternative constraint?
* Is an "enormous simplification" in # coeffs over a full multivariate normal distribution
  - For an (md X md) covariance matrix,  # coeffs reduction is on the order of:
      (md)^2 -->  m^2 + d^2
  - Anderlucci15covPatMix has table of other options for coefficient size reduction (in a mixture)
  - compare with precision matrix decomp in: Tastu15spcTimeTrajGaussCpla
* How to sample this distribution is explained.  Pretty simple, A,B can be chosen by Cholesky decomp: Wikipedia17matrixNormDist},
  file      = {:papers\\Owen13monteCarloBook.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.07.05},
  url       = {http://statweb.stanford.edu/~owen/mc/},
}

@Misc{Karpathy16deepReinfLrnPongPix,
  author       = {Andrej Karpathy},
  title        = {Deep Reinforcement Learning: Pong from Pixels},
  howpublished = {Andrej Karpathy blog},
  month        = may,
  year         = {2016},
  abstract     = {This is a long overdue blog post on Reinforcement Learning (RL). RL is hot! You may have noticed that computers can now automatically learn to play ATARI games (from raw game pixels!), they are beating world champions at Go, simulated quadrupeds are learning to run and leap, and robots are learning how to perform complex manipulation tasks that defy explicit programming. It turns out that all of these advances fall under the umbrella of RL research. I also became interested in RL myself over the last ~year: I worked through Richard Sutton?s book, read through David Silver?s course, watched John Schulmann?s lectures, wrote an RL library in Javascript, over the summer interned at DeepMind working in the DeepRL group, and most recently pitched in a little with the design/development of OpenAI Gym, a new RL benchmarking toolkit. So I?ve certainly been on this funwagon for at least a year but until now I haven?t gotten around to writing up a short post on why RL is a big deal, what it?s about, how it all developed and where it might be going.},
  comment      = {Prefers policy gradient to DQ learning.

Recommends also viewing
  John Schulman 1-3(?): Deep Reinforcement learning:
  https://www.youtube.com/watch?v=ptaih9ksnjo},
  file         = {Blog Post:Karpathy16deepReinfLrnPongPix.pdf:PDF},
  owner        = {sotterson},
}

@Article{Cai16strctHiDimCovPrecAdapt,
  author    = {Cai, T. Tony and Ren, Zhao and Zhou, Harrison H.},
  title     = {Estimating structured high-dimensional covariance and precision matrices: Optimal rates and adaptive estimation},
  journal   = {Electron. J. Statist.},
  year      = {2016},
  volume    = {10},
  number    = {1},
  pages     = {1--59},
  abstract  = {This is an expository paper that reviews recent developments on optimal estimation of structured high-dimensional covariance and precision matrices. Minimax rates of convergence for estimating several classes of structured covariance and precision matrices, including bandable, Toeplitz, sparse, and sparse spiked covariance matrices as well as sparse precision matrices, are given under the spectral norm loss. Data-driven adaptive procedures for estimating various classes of matrices are presented. Some key technical tools including large deviation results and minimax lower bound arguments that are used in the theoretical analyses are discussed. In addition, estimation under other losses and a few related problems such as Gaussian graphical models, sparse principal component analysis, factor models, and hypothesis testing on the covariance structure are considered. Some open problems on estimating high-dimensional covariance and precision matrices and their functionals are also discussed},
  comment   = {Adaptive covaraince/precision matrix review paper.},
  doi       = {10.1214/15-EJS1081},
  file      = {Cai16strctHiDimCovPrecAdapt.pdf:Cai16strctHiDimCovPrecAdapt.pdf:PDF},
  fjournal  = {Electronic Journal of Statistics},
  publisher = {The Institute of Mathematical Statistics and the Bernoulli Society},
  url       = {http://dx.doi.org/10.1214/15-EJS1081},
}
,
  timestamp    = {2017.01.17},
  url          = {http://karpathy.github.io/2016/05/31/rl/},
}

@Article{Friedrich00extMdlEqsSDE,
  author    = {R. Friedrich and S. Siegert and J. Peinke and St. L??ck and M. Siefert and M. Lindemann and J. Raethjen and G. Deuschl and G. Pfister},
  title     = {Extracting model equations from experimental data},
  journal   = {Physics Letters A},
  year      = {2000},
  volume    = {271},
  number    = {3},
  pages     = {217--222},
  issn      = {0375-9601},
  abstract  = {This letter wants to present a general data-driven method for formulating suitable model equations for nonlinear complex systems. The method is validated in a quantitative way by its application to experimentally found data of a chaotic electric circuit. Furthermore, the results of an analysis of tremor data from patients suffering from Parkinson?s disease, from essential tremor, and from normal subjects with physiological tremor are presented, discussed and compared. They allow a distinction between the different forms of tremor.},
  comment   = {Langevin SDE coeffs estimated by estimating 1\textsuperscript{st} difference distribution -- useful for RES scenario generation and maybe optimization. But is a Gaussian dist required? How do you integrate it and correct for calibration errors?

Idea is that a system's state future state space can be predicted by a recursive application of Markov transtion proabilities. The transition probabilities are in derivative space, and must be integrated to get the actual values being predicted (power production of wind, solar, ...). Henrik Madsen, of DTU, says that the few num. of SDE parameters make optimization easy, but I don't know how.

Estimating state trajectory parameters in eq (1) from Markov conditional distributions, with time step, tau
.
1.) Pick nhbd within x+delX. These occur at time ti
2.) For each ti, find next state, X(ti+tau). (next state set is xtau)
3.) Estimate conditional dist. of xtau (assumed Gaussian)
4.) Get trajectory mean velocity, g(x), and velocity fluctuations, h(x) from mean and variance in 3.)

RESULTS
Visually show that the states of an electrical circuit are predicted, and that the state space of 3 kinds of human muscle tremors are different, given measurements of 20 individuals.

QUESTIONS
1.) Is a Gaussian necessary?
2.) Can it have any distribution for which mean and variance are defined? Or can it have any other dist?
3.) How do you integrate it and correct for calibration errors?

>>> Malte Siefert 2/4/2014 11:18 AM >>>

This is one of the original work for the estimation of a Langevin equation from data with the method I mentioned. In the mean time there are more papers on this topic.

Malte},
  doi       = {10.1016/S0375-9601(00)00334-0},
  file      = {Friedrich00extMdlEqsSDE.pdf:Friedrich00extMdlEqsSDE.pdf:PDF},
  groups    = {Read, PointDerived, Use, doReadNonWPV_2},
  owner     = {sotterson},
  timestamp = {2014.02.04},
  url       = {http://www.sciencedirect.com/science/article/pii/S0375960100003340},
}

@Article{Saied07dynRateTrans,
  author    = {Mohamed M. Saied},
  title     = {Assessing the dynamic rating of overhead transmission lines},
  journal   = {European Transactions on Electrical Power},
  year      = {2007},
  volume    = {17},
  number    = {5},
  pages     = {526--536},
  abstract  = {This paper addresses the achievable advantage of the dynamic rating of overhead power lines based on the continuous monitoring of the ambient temperature and wind speed. Starting from their measured profiles on a typical summer day, the corresponding possible current loading of the line could be determined over the 24 hours. It is then compared with the conventional line rating calculated using the maximum daily temperature and the typically assumed value of 0.6 m/s for the wind speed. The additional daily transmitted energy resulting from the dynamic line rating (DLR) could be found. The procedure is applied to lines of different conductor radii. Results indicate an achievable increase in the daily transmitted energy of about 40\%.},
  comment   = {Theoretically for one day in Kuwait, could gain 40\% in capacity due to dynamic rating (hot and windy) * not experimental, kind of crappy * looked at one day only (and curve fit it for some reason) * plugged measured ambinent wind/temp into some heat balance equations * Kuwaiti lines are short, so thermally limited (not limited by stability) * assume 100C max allowable temp (isn't this a little hgh, if annealing starts as low as 93C)?},
  doi       = {10.1002/etep.151},
  file      = {Saied07dynRateTrans.pdf:Saied07dynRateTrans.pdf:PDF;Saied07dynRateTrans.pdf:Saied07dynRateTrans.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.02.23},
}

@Article{Garaud11ensCalSel,
  author      = {Garaud, Damien and Mallet, Vivien},
  title       = {{Automatic calibration of an ensemble for uncertainty estimation and probabilistic forecast: Application to air quality}},
  journal     = {Journal of Geophysical Research},
  year        = {2011},
  volume      = {116},
  number      = {D19304},
  month       = oct,
  abstract    = {{This paper addresses the problem of calibrating an ensemble for uncertainty estimation. The calibration method involves (1) a large, automatically generated ensemble, (2) an ensemble score such as the variance of a rank histogram, and (3) the selection based on a combinatorial algorithm of a sub-ensemble that minimizes the ensemble score. The ensemble scores are the Brier score (for probabilistic forecasts), or derived from the rank histogram or the reliability diagram. These scores allow us to measure the quality of an uncertainty estimation, and the reliability and the resolution of an ensemble. The ensemble is generated on the Polyphemus modeling platform so that the uncertainties in the models' formulation and their input data can be taken into account. A 101-member ensemble of ground-ozone simulations is generated with full chemistry-transport models run across Europe during the year 2001. This ensemble is evaluated with the aforementioned scores. Several ensemble calibrations are carried out with the different ensemble scores. The calibration makes it possible to build 20- to 30-member ensembles which greatly improves the ensemble scores. The calibrations essentially improve the reliability, while the resolution remains unchanged. The spatial validity of the uncertainty maps is ensured by cross validation. The impact of the number of observations and observation errors is also addressed. Finally, the calibrated ensembles are able to produce accurate probabilistic forecasts and to forecast the uncertainties, even though these uncertainties are found to be strongly time-dependent.}},
  affiliation = {Centre d'Enseignement et de Recherche en Environnement Atmosph{\'e}rique - CEREA , CLIME - INRIA Rocquencourt},
  audience    = {internationale },
  comment     = {Combinatoric ensemble selection for probabilistic forecasting.

Helpful for adaboost?},
  doi         = {10.1029/2011JD015780},
  file        = {Garaud11ensCalSel.pdf:Garaud11ensCalSel.pdf:PDF},
  groups      = {Ensemble, doReadNonWPV_2},
  hal_id      = {hal-00655771},
  keywords    = {Air quality ; calibration ; ensemble ; photochemistry ; uncertainty},
  language    = {Anglais},
  owner       = {sotterson},
  publisher   = {American Geophysical Union},
  timestamp   = {2014.01.29},
  url         = {http://hal.inria.fr/hal-00655771},
}

@Article{Bouffard04probSpinRes,
  author    = {Bouffard, F. and Galiana, F.D.},
  title     = {An electricity market with a probabilistic spinning reserve criterion},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2004},
  volume    = {19},
  number    = {1},
  pages     = {300--307},
  month     = feb,
  issn      = {0885-8950},
  abstract  = {This paper addresses the problem of reliability-constrained market-clearing in pool-based electricity markets with unit commitment. In general, probabilistic reliability criteria that implicitly set the reserve requirement are defined by the loss-of-load probability and by the expected load not served. As the computation of such metrics is complicated by their nonlinear and combinatorial nature, we introduce the notion of hybrid metrics based on the probabilities of loss-of-load due to single and double generation outages only. The reliability-constrained market-clearing problem can then be formulated as a mixed-integer linear program and solved with large-scale commercial solvers. Numerical tests with data from the IEEE Reliability Test System indicate that the new method is computationally efficient and produces market-clearing results with the desired probabilistic characteristics.},
  comment   = {Juan Mi's recommendation. Mixed integer optimization in reserve markets, combined performance metric},
  doi       = {10.1109/TPWRS.2003.818587},
  file      = {Bouffard04probSpinRes.pdf:Bouffard04probSpinRes.pdf:PDF},
  groups    = {Test, Use, doReadNonWPV_2},
  keywords  = { IEEE reliability test system; computational complexity; double generation outages; expected load not served; hybrid metrics; large-scale commercial solvers; loss-of-load probability; mixed integer-linear programming; pool-based electricity market; probabilistic spinning reserve criterion; reliability-constrained market-clearing problem; scheduling; single generation outages; unit commitment; computational complexity; integer programming; linear programming; power markets; power system reliability; probability;},
  owner     = {sotterson},
  timestamp = {2012.02.09},
}

@InProceedings{Forbes11pvWindRelTenneT,
  author    = {Kevin Forbes and Marco Stampini and Ernest Zampelli},
  title     = {Do Higher {PV} Solar and Wind Energy Penetration Levels Pose a Challenge to Electric Power Reliability? Evidence from the TenneT Transmission System in {Germany}},
  booktitle = {4\textsuperscript{th} International Workshop on Empirical Methods in Energy Economics},
  year      = {2011},
  abstract  = {This paper addresses the reliability challenges facing electric power operations posed by
higher wind and PV solar energy penetration levels. While both of these energy sources are
renewable, they have the disadvantage of not being fully dispatchable. As a result, system
operators manage these energy resources by forecasting production levels. The accuracy of
the forecasts is critical since the stability of the system requires that the amount of power
generation in a control area match exactly, on a near-instantaneous basis, the system load, net
of losses and interchange with other control areas. The analysis focuses on the TenneT
transmission control area in Germany (formerly EON Netz) for the period 1 March 2010
through 30April 2011. Over this period, wind energy and PV solar energy accounted for
approximately 16 and 11 percent, respectively, of vertical load during day light hours.},
  comment   = {Paper saying that PV and wind power forecasting errors are likely to screw up TennetT's reliabilty in Germany. Seems relevant to Eweline.

Also, shows relative size of wind, pv and load forecast errors (I used this graph in the risk management workshop in Berlin in 2013). But this is a strange graph:
1. ) only during daylight hours
2. ) Not the usual RMSE/capacity, but RMSE / mean power produced or consumed by the thing being forecasted

Have both paper and slides.},
  file      = {Paper:Forbes11pvWindRelTenneT.pdf:PDF;Slides:Forbes11pvWindRelTenneT_Slides.pdf:PDF},
  groups    = {Test, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2013.06.19},
  url       = {http://www.cox.smu.edu/web/maguire-energy-center/full-program},
}

@Article{Xiao14genCorrRVjohnson,
  author   = {Xiao, Qing},
  title    = {Generating correlated random vector by Johnson system},
  journal  = {Journal of Data Science},
  year     = {2014},
  volume   = {12},
  number   = {2},
  abstract = {This paper aims to generate multivariate random vector with prescribed
correlation matrix by Johnson system. The probability weighted moment
(PWM) is employed to assess the parameters of Johnson system. By equat-
ing the first four PWMs of Johnson system with those of the target distri-
bution, a system of equations solved for the parameters is established. With
suitable initial values, solutions to the equations are obtained by the New-
ton iteration procedure. To allow for the generation of random vector with
prescribed correlation matrix, approaches to accommodate the dependency
are put forward. For the four transformation models of Johnson system,
nine cases are addressed. Analytical formulae are derived to determine the
equivalent correlation coefficient in the standard normal space for six cases,
the rest three ones are handled by an interpolation method. Finally, several
numerical examples are given out to check the proposed method.

Key words: correlation coefficient, Johnson system, normal transformation,
probability weighted moment.},
  comment  = {Given a multivariate RV, generate more samples w/ same correlation and marginal distributions. Uses Johnson system and includes handy analytical expression for Johnson parameters v.s. moments. Also model selection. This seems to be to be kind of like a copula but with a Johson transform instead of a rank transform. Could be good for ReWP wind farm clustering (if this can be turned into a sum problem, like a Gaussian can). The analytical expressions might be good for something too.

My main Johnson article: George11estJohnsonDistBnd},
  file     = {Xiao14genCorrRVjohnson.pdf:Xiao14genCorrRVjohnson.pdf:PDF},
  url      = {http://www.jds-online.com/files/1.JDS-1222_new.pdf},
}

@Article{Morales10windPrice,
  author    = {Juan M. Morales and Antonio J. Conejo and Juan P\'{e}rez-Ruiz},
  title     = {Simulating the Impact of Wind Production on Locational Marginal Prices},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2010},
  abstract  = {This paper analyzes the impact of wind production on the locational marginal prices of a fully competitive pool-based electricity market. Wind productions are modeled as negative loads and characterized by historical data records. The analysis pertains to the structural relationship between wind production and locational marginal prices, and disregards the collateral effect of strategic offering. The study presented allows a statistical characterization of locational marginal prices as a function of the statistical data of the wind plants and the structure of the considered electric energy system.},
  comment   = {Relevant to optimal spinning reserves project SUBMITTED TO IEEE TRANSACTIONS ON POWER SYSTEMS
LMP based on historical records, I think. Somehow, strategic offering is disregarded -- do TSO's need to do this too? Anyway, it's not forecasting, it seems.},
  file      = {Morales10windPrice.pdf:Morales10windPrice.pdf:PDF},
  groups    = {Use, doReadNonWPV_1},
  owner     = {scot},
  timestamp = {2010.07.05},
}

@Article{Zhao16nnSkewTcopulaPortfolioOpt,
  author    = {Yang Zhao and Charalampos Stasinakis and Georgios Sermpinis and Yukun Shi},
  title     = {Neural Network Copula Portfolio Optimization for Exchange Traded Funds},
  journal   = {{SSRN} Electronic Journal},
  year      = {2016},
  abstract  = {This paper attempts to investigate if adopting accurate forecasts from Neural Network (NN) models can 
lead to statistical and economically significant benefits in portfolio management decisions. In order to 
achieve that, three NNs, namely the Multi-Layer Perceptron (MLP), Recurrent Neural Network (RNN) 
and the Psi Sigma Network (PSN), are applied to the task of forecasting the daily returns of three 
Exchange Traded Funds (ETFs). The statistical and trading performance of the NNs is benchmarked 
with the traditional Auto-Regressive Moving Average (ARMA) models. Next, a novel dynamic 
asymmetric copula model (NNC) is introduced in order to capture the dependence structure across ETF 
returns. Based on the above, weekly re-balanced portfolios are obtained and compared by using the 
traditional mean-variance and the mean-CVaR portfolio optimization approach. In terms of the results, 
PSN outperforms all models in statistical and trading terms. Additionally, the asymmetric skewed t 
copula statistically outperforms symmetric copulas when it comes to modelling ETF returns dependence. 
The proposed NNC model leads to significant improvements in the portfolio optimization process, while 
forecasting covariance accounting for asymmetric dependence between the ETFs also improves the 
performance of obtained portfolios. 

 

 

Keywords 
Copulas, Neural Networks, Portfolio Optimization, ETF },
  comment   = {NN's produce forecasts and they are combined with a skew-T copula.   These are better than T-copulas because they can model asymmetry (and have better performance).  I don't know about robustness or dimensionality, though...

Also, a Psi Sigma Network (PSN) is found to be a better forecasting network thatn MLP's or RNN's.  
},
  doi       = {10.2139/ssrn.2877966},
  file      = {:Zhao16nnSkewTcopulaPortfolioOpt.pdf:PDF},
  publisher = {Elsevier {BV}},
}

@Article{Haywood97multiStepFreqDomain,
  author    = {J. Haywood and G. Tunnicliffe Wilson},
  title     = {Fitting Time Series Models by Minimizing Multistep-ahead Errors: a Frequency Domain Approach},
  journal   = {Journal of the Royal Statistical Society, Series B},
  year      = {1997},
  volume    = {59},
  number    = {1},
  pages     = {237--254},
  abstract  = {This paper brings together two topics in the estimation of time series forecasting models: the use of the multistep-ahead error sum of squares as a criterion to be minimized and frequency domain methods for carrying out this minimization. The methods are developed for the wide class of time series models having a spectrum which is linear in unknown coef?cients. This includes the IMA(1, 1) model for which the common exponentially weigh- ted moving average predictor is optimal, besides more general structural models for series exhibiting trends and seasonality. The method is extended to include the Box-Jenkins `air line' model. The value of the multistep criterion is that it provides protection against using an incorrectly speci?ed model. The value of frequency domain estimation is that the iteratively reweighted least squares scheme for ?tting generalized linear models is readily extended to construct the parameter estimates and their standard errors. It also yields insight into the loss of ef?ciency when the model is correct and the robustness of the criterion against an incorrect model. A simple example is used to illustrate the method, and a real example demonstrates the extension to seasonal models. The discussion considers a diagnostic test statistic for indicating an incorrect model.},
  comment   = {Training a multi-step ahead model in freq domain, if model params depend linearly in freq domain. A better linear regression?},
  doi       = {10.1111/1467-9868.00066},
  file      = {Haywood97multiStepFreqDomain.pdf:Haywood97multiStepFreqDomain.pdf:PDF;Haywood97multiStepFreqDomain.pdf:Haywood97multiStepFreqDomain.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.09.29},
  url       = {http://www3.interscience.wiley.com.offcampus.lib.washington.edu/search/allsearch?mode=viewselected&product=journal&ID=119163084&view_selected.x=73&view_selected.y=6&view_selected=view_selected},
}

@Article{Schafer09locLinRgrsSelModel,
  author    = {Schafer, ChadM. and Doksum, KjellA.},
  title     = {Selecting local models in multiple regression by maximizing power},
  journal   = {Metrika},
  year      = {2009},
  volume    = {69},
  number    = {2-3},
  pages     = {283--304},
  issn      = {0026-1335},
  abstract  = {This paper considers multiple regression
 procedures for analyzing the relationship between a
 response variable and a vector of d covariates in a
 nonpara- metric setting where tuning parameters need to
 be selected. We introduce an approach which handles the
 dilemma that with high dimensional data the sparsity of
 data in regions of the sample space makes estimation of
 nonparametric curves and surfaces virtually
 impossible. This is accomplished by abandoning the goal
 of trying to esti- mate true underlying curves and
 instead estimating measures of dependence that can
 determine important relationships between
 variables. These dependence measures are based on local
 parametric ?ts on subsets of the covariate space that
 vary in both dimen- sion and size within each
 dimension. The subset which maximizes a signal to noise
 ratio is chosen, where the signal is a local estimate of
 a dependence parameter which depends on the subset
 dimension and size, and the noise is an estimate of the
 stan- dard error (SE) of the estimated signal. This
 approach of choosing the window size to maximize a signal
 to noise ratio lifts the curse of dimensionality because
 for regions with sparsity of data the SE is very
 large. It corresponds to asymptotically maximizing the
 probability of correctly ?nding nonspurious relationships
 between covariates and a response or, more precisely,
 maximizing asymptotic power among a class of asymp- totic
 level ? t-tests indexed by subsets of the covariate
 space. Subsets that achieve this goal are called
 features. We investigate the properties of speci?c
 procedures based on the preceding ideas using asymptotic
 theory and Monte Carlo simulations and ?nd that within a
 selected dimension, the volume of the optimally selected
 subset does not tend to zero as n ? ? unless the volume
 of the subset of the covariate space where the response
 depends on the covariate vector tends to zero. Keywords
 Testing ? Ef?cacy ? Signal to noise ? Curse of
 dimensionality ? Local linear regression ? Bandwidth
 selection},
  comment   = {Feature selection for local linear models (different for each local linear model) via statistical power, I think. Might be analagous to what you'd do for a local linear quantile regression model?},
  doi       = {10.1007/s00184-008-0218-z},
  file      = {Schafer09locLinRgrsSelModel.pdf:Schafer09locLinRgrsSelModel.pdf:PDF},
  keywords  = {Testing; Efficacy; Signal to noise; Curse of dimensionality; Local linear regression; Bandwidth selection},
  language  = {English},
  owner     = {sotterson},
  publisher = {Springer-Verlag},
  timestamp = {2014.03.31},
}

@Article{Lim14multiModelEnsRain,
  author    = {Lim, Yaeji and Jo, Seongil and Lee, Jaeyong and Oh, Hee-Seok and Lee, Sang-Goo and Park, Yongtae and Kang, Hyun-Suk},
  title     = {Multimodel ensemble forecasting of rainfall over East {Asia}: regularized regression approach},
  journal   = {International Journal of Climatology},
  year      = {2014},
  issn      = {1097-0088},
  abstract  = {This paper considers the problem of predicting the rainfall over East Asia from multimode outputs. For this purpose, we propose a new multimode ensemble method based on regularized regression approach, which consists of two steps, the pre-processing step and the ensemble step. In the pre-processing step, we improve prediction from each model output using regularized regression, and in the ensemble step, we apply regularization-based regression method to combine the result from the pre-processing step. The main benefits of the proposed method are that it improves prediction accuracy, and it is capable of solving the singularity problem so that it can integrate many climate variables from multimode outputs for a better prediction. The proposed method is applied to monthly outputs from nine general circulation models (GCMs) on boreal summer (June, July, and August) over 20 years (1983???2002). The prediction ability of the proposed ensemble forecast is compared with the observations and the outputs (prediction) from each GCM. The results show that the proposed method is capable of improving forecast accuracy by adjusting each model before combining.},
  comment   = {Maybe another way to do multimodel wind power forecasting -- the twist is that it's with ensmbles too. I belive it also cites Oh11fastSmthQuantreg (a quantile regression paper) so maybe that's interesting too. But I can't find the paper.},
  doi       = {10.1002/joc.3938},
  keywords  = {climate change, regularized regression, precipitation, prediction},
  owner     = {sotterson},
  publisher = {John Wiley \& Sons, Ltd},
  timestamp = {2014.03.30},
}

@Article{Fisher92regrsnModAngular,
  author              = {Fisher, N. I. and Lee, A. J.},
  title               = {Regression Models for an Angular Response},
  year                = {1992},
  volume              = {48},
  number              = {3},
  pages               = {665--677},
  issn                = {0006-341X},
  url                 = {http://www.jstor.org/stable/2532334},
  abstract            = {This paper considers the problem of regressing an angular response variate on a set of linear explanatory variables. A general class of models is proposed in which the mean direction and dispersion of a von Mises variate are related to the explanatory variables by general link functions. Appropriate regression diagnostics, estimation and testing procedures are developed for fitting the models. The meaning of "correlation" between an angular and a linear variable is clarified, and leads to a general notion of multiple correlation associated with the regression model. The methods are applied to a set of data arising from a study of movements of intertidal gastropods.},
  copyright           = {Copyright ? 1992 International Biometric Society},
  file                = {Fisher92regrsnModAngular.pdf:Fisher92regrsnModAngular.pdf:PDF;Fisher92regrsnModAngular.pdf:Fisher92regrsnModAngular.pdf:PDF},
  journal             = {Biometrics},
  jstor_formatteddate = {Sep., 1992},
  owner               = {sotterson},
  publisher           = {International Biometric Society},
  timestamp           = {2009.05.08},
}

@Article{Sorensen07pwrFluctLrgWind,
  author    = {Sorensen, P. and Cutululis, N.A. and Vigueras-Rodriguez, A. and Jensen, L.E. and Hjerrild, J. and Donovan, M.H. and Madsen, H.},
  title     = {Power Fluctuations From Large Wind Farms},
  year      = {2007},
  volume    = {22},
  number    = {3},
  month     = aug,
  pages     = {958--965},
  issn      = {0885-8950},
  doi       = {10.1109/TPWRS.2007.901615},
  abstract  = {This paper deals with power fluctuations from wind farms. The time range in focus is between one minute and up to a couple of hours. In this time range, substantial power fluctuations have been observed during unstable weather conditions. A wind power fluctuation model is described, and measured time series from the first large offshore wind farm, Horns Rev in Denmark, are compared to simulated time series. The comparison between measured and simulated time series focuses on the ramping characteristics of the wind farm at different power levels and on the need for system generation reserves due to the fluctuations. The comparison shows a reasonable agreement between simulations and measurements, although there is still room for improvement of the simulation model.},
  file      = {Sorensen07pwrFluctLrgWind.pdf:Sorensen07pwrFluctLrgWind.pdf:PDF},
  journal   = {Power Systems, IEEE Transactions on},
  keywords  = {Denmark;Horns Rev;power fluctuations;system generation reserves;time series;wind farms;wind power;power system control;time series;wind power plants;},
  owner     = {sotterson},
  timestamp = {2012.08.03},
}

@InProceedings{Vogt15HybridPhysMLrgnFrcst,
  author    = {Stephan Vogt and Jan Dobschinski and Thomas Kanefendt and Scott Otterson and Yves-Marie Saint-Drenan},
  title     = {A Hybrid Physical and Machine Learning Based Forecast of Regional Wind Power},
  booktitle = {14th Wind Integration Workshop},
  year      = {2015},
  month     = oct,
  abstract  = {This Paper deals with the forecast of several wind
power plants in a spatial domain such as Germany. Machine
learning algorithms, in this case artificial neural networks, can
be trained given historical power data and numerical weather
predictions. These models are used with current weather
forecasts to predict the future power feed-in of single wind
power plants.
Instead of machine learning models, a physical model can
predict future feed-ins. This is often inferior because it misses
information about the site and the power plant implicitly
contained in the power measurements. On the other hand,
physical models can forecast when no historical time series
is available.
If power forecasts are required for areas, there are often some
plants for which measurements are not available. Therefore,
it is meaningful to combine machine learning and physical
modeling. Two methods are described that combine the advan-
tages of both approaches. The first method is a very simple
averaging of the time series from each model. The other is
a new algorithm, that averages both models depending upon
the spatial arrangement of the wind turbines in the forecasting
area. The procedures are tested for accuracy of prediction of
the feed-in into German wind power production in the year
2014.
Index Terms?regional wind power forecast, hybrid physical
and machine learning model, artificial neural network},
  comment   = {Stephan's paper on the mix of NWP grid and reference farm upscaling. There is, of course, nothing "physical" about it.

Possible power curve learning improvements:
* Cao12crvDerivParaPenSpln
* Stephen13windSpdDirEMmix
* see energytop.org <<Power curve learning>>
* could Johnson System distributions be used for the power curve? Torrez93JohnsonDistFitNeuron
* or use stable distribution regression, replacing the sum and power curve? Frain09alphaDistAppsThesis},
  file      = {Vogt15HybridPhysMLrgnFrcst.pdf:Vogt15HybridPhysMLrgnFrcst.pdf:PDF},
  location  = {Brussels},
  url       = {http://www.windintegrationworkshop.org/brussels2015/},
}

@Article{Kargin08curveFrcstFuncAR,
  author    = {V. Kargin and A. Onatski},
  title     = {Curve forecasting by functional autoregression},
  journal   = {Journal of Multivariate Analysis},
  year      = {2008},
  volume    = {99},
  number    = {10},
  pages     = {2508 - 2526},
  issn      = {0047-259X},
  abstract  = {This paper deals with the prediction of curve-valued autoregression processes. It develops a novel technique, predictive factor decomposition, for the estimation of the autoregression operator. The technique is based on finding a reduced-rank approximation to the autoregression operator that minimizes the expected squared norm of the prediction error. Implementing this idea, we relate the operator approximation problem to the singular value decomposition of a combination of cross-covariance and covariance operators. We develop an estimation method based on regularization of the empirical counterpart of this singular value decomposition, prove its consistency and evaluate convergence rates. The method is illustrated by an example of the term structure of the Eurodollar futures rates. In the sample corresponding to the period of normal growth, the predictive factor technique outperforms the principal components method and performs on a par with custom-designed prediction methods.},
  doi       = {http://dx.doi.org/10.1016/j.jmva.2008.03.001},
  file      = {:papers\\Kargin08curveFrcstFuncAR.pdf:PDF},
  keywords  = {62H25, 60G25, 91B84, Functional data analysis, Dimension reduction, Reduced-rank regression, Principal component, Singular value decomposition, Predictive factor, Term structure, Interest rates},
  owner     = {sotterson},
  timestamp = {2017.07.04},
  url       = {http://www.sciencedirect.com/science/article/pii/S0047259X08000961},
}

@Article{Svoboda97schedRampConstr,
  author    = {Svoboda, A.J. and Chung-Li Tseng and Chao-An Li and Johnson, R.B.},
  title     = {Short-term resource scheduling with ramp constraints [power generation scheduling]},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {1997},
  volume    = {12},
  number    = {1},
  pages     = {77--83},
  month     = feb,
  issn      = {0885-8950},
  abstract  = {This paper describes a Lagrangian relaxation-based method to solve the short-term resource scheduling (STRS) problem with ramp constraints. Instead of discretizing the generation levels, the ramp rate constraints are relaxed with the system demand constraints using Lagrange multipliers. Three kinds of ramp constraints, startup, operating and shutdown ramp constraints are considered. The proposed method has been applied to solve the hydro-thermal generation scheduling problem at PG amp;E. An example alone with numerical results is also presented},
  comment   = {Scheduling w/ ramp constraints. Ramps significantly constrain unit commitment
* Shows that it has been thought of, so ramps are important
* Sum of ramping capability for available units must be greater than demand ramp
* this is an old paper so ramps must have come from demand changes, not wind
* are demand ramps slower than wind ramps?

Three kinds of ramp constraints:
1.) startup: time to get to min power capability
2.) normal operational
3.) shutdown: time to drop from min power capability to zero

Actions taken based on ramps (complicated but...)
* turn a unit on or off
* set the amount of power the unit is producing

Optimization is done w/ dynamic programming and Lagrangian stuff.

* Also constrains min offtime, etc. (I think...)
* Didn't read this part},
  doi       = {10.1109/59.574926},
  file      = {Svoboda97schedRampConstr.pdf:Svoboda97schedRampConstr.pdf:PDF},
  groups    = {Read},
  keywords  = {Lagrange multipliers;Lagrangian relaxation method;hydro-thermal generation scheduling;operating ramp;power system demand constraints;ramp rate constraints;short-term power generation scheduling;short-term resource scheduling;shutdown ramp;startup ramp;hydrothermal power systems;optimisation;power system planning;scheduling;},
  owner     = {scot},
  timestamp = {2011.05.26},
}

@Article{Perez04satelliteIrradTerrain,
  author   = {Richard Perez and Pierre Ineichen and Marek Kmiecik and Kathleen Moore and David Renne and Ray George},
  title    = {Producing satellite-derived irradiances in complex arid terrain},
  journal  = {Solar Energy},
  year     = {2004},
  volume   = {77},
  number   = {4},
  pages    = {367 - 371},
  issn     = {0038-092X},
  note     = {The American Solar Energy Society's Solar 2003 Special Issue},
  abstract = {This paper describes a methodology to correct satellite-derived irradiances over complex terrain for models that use the visible satellite channel as main input for cloud index determination. Complex terrain is characterized by high reflectance surface and or the juxtaposition of high and low reflectance surfaces (e.g., desert plains and forested ridges). The correction consists of (1) climate dependent post-model clear sky calibration and (2) singularity identification and removal.},
  comment  = {Describes SolarAnywhere Irrac calcs, according to Kubiniec19solarSatelliteTuneGrndTR.  The baseline for the improvements in this paper is Perez02newMdllSatelliteIrrad

Irradiance estimate is mapped from min to of the range of satellite measurements, with low measurements indicating exposed ground and high values indicating highly reflective clouds.  There is a lot of normalization of this signal by time of day/year, the location specifics, ground measurements for snow, etc.

This paper is about fixes to problems the algorithms in Perez02newMdllSatelliteIrrad have iwht high ground reflectance, namely:

1. ground with very high specularity reults in small but significant underestimates of DNI.  Somehow, specular irradiance isn't measured correctly but is correct during some hours of the day.  Why this isn't explained.  

Solution: in clear, arid areas, the lower bound for each pixel is defined by a low empirical quantile over a month.
Validation: test at one ground measurment site shows improvement

2. bright areas next to dark areas e.g. forests on a ridge, have big errors when there are satellite nav. errors.  This makes the lower bound of dynamic range too low for the bright areas.

Solution:  detect holes in lower bound by visible spectrum 2D edge detection in space and rough vs. smooth difference in time.  Fill the holes by the neighborhood average, a little like a Photoshop acne remover.  It's not totally clear that this fix is only for problem 2 but anyway, it's enforcing smoothness.  They may also add the IR channel to this algorithm

Validation: no test b/c no ground instruments at the areas where this occurs.

Ideas for improvements
* really use quantiles (instead of counts) to measure low bound of dynamic range.
* do 3D smoothness enforcement (2D space + time), something like would be done for video processing
* add IR channel and do a 4D smoother
},
  doi      = {https://doi.org/10.1016/j.solener.2003.12.016},
  file     = {:Perez04satelliteIrradTerrain.pdf:PDF},
  url      = {http://www.sciencedirect.com/science/article/pii/S0038092X03004687},
}

@Article{Bacher09pvFrcstOnlineClrSky,
  author    = {Bacher, Peder and Madsen, Henrik and Nielsen, Henrik Aalborg},
  title     = {Online short-term solar power forecasting},
  journal   = {Solar Energy},
  year      = {2009},
  volume    = {83},
  number    = {10},
  pages     = {1772--1783},
  abstract  = {This paper describes a new approach to online forecasting of power production from PV systems. The method is suited to online forecasting in many applications and in this paper it is used to predict hourly values of solar power for horizons of up to 36 h. The data used is 15-min observations of solar power from 21 PV systems located on rooftops in a small village in Denmark. The suggested method is a two-stage method where first a statistical normalization of the solar power is obtained using a clear sky model. The clear sky model is found using statistical smoothing techniques. Then forecasts of the normalized solar power are calculated using adaptive linear time series models. Both autoregressive (AR) and AR with exogenous input (ARX) models are evaluated, where the latter takes numerical weather predictions (NWPs) as input. The results indicate that for forecasts up to 2 h ahead the most important input is the available observations of solar power, while for longer horizons NWPs are the most important input. A root mean square error improvement of around 35% is achieved by the ARX model compared to a proposed reference model.
Keywords

 Solar power;
 Prediction;
 Forecasting;
 Time series;
 Photovoltaic;
 Numerical weather predictions;
 Clear sky model;
 Quantile regression;
 Recursive least squares},
  comment   = {PV forecasting in two steps: 1. clear sky normalization 2. followed by ARX (0-2 hrs) or AR (2-36 hrs). Just like w/ wind, power is most important input or 0-2hrs, and after that, it's NWP. Highly cited so maybe a good paper to read for the normalization.},
  file      = {:Bacher09pvFrcstOnlineClrSky.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2015.02.19},
  url       = {http://www.sciencedirect.com/science/article/pii/S0038092X09001364},
}

@Article{Sanchez2006shortWindPred,
  author    = {Ismael S{\'a}nchez},
  title     = {Short-term prediction of wind energy production},
  year      = {2006},
  volume    = {22},
  number    = {1},
  pages     = {43--56},
  url       = {http://www.sciencedirect.com/science/article/B6V92-4HDP6VM-1/2/0f5ac6ae7950992b63e41d2dfaa2d49a},
  abstract  = {This paper describes a statistical forecasting system for the short-term prediction (up to 48 h ahead) of the wind energy production of a wind farm. The main feature of the proposed prediction system is its adaptability. The need for an adaptive prediction system is twofold. First, it has to deal with highly nonlinear relationships between the variables involved. Second, the prediction system would generate predictions for alternative wind farms, as it is made by the system operator for efficient network integration. This flexibility is attained through (i) the use of alternative models based on different assumptions about the variables involved; (ii) the adaptive estimation of their parameters using different recursive techniques; and (iii) using an on-line adaptive forecast combination scheme to obtain the final prediction. The described procedure is currently implemented in SIPRE??LICO, a wind energy prediction tool that is part of the on-line management of the Spanish Peninsular system operation.},
  file      = {Sanchez2006shortWindPred.pdf:Sanchez2006shortWindPred.pdf:PDF;Sanchez2006shortWindPred.pdf:Sanchez2006shortWindPred.pdf:PDF},
  journal   = {International Journal of Forecasting},
  owner     = {sotterson},
  timestamp = {2008.07.03},
}

@Article{Mann11smoothDerivInfo,
  author    = {B.P. Mann and F.A. Khasawneh and R. Fales},
  title     = {Using information to generate derivative coordinates from noisy time series},
  journal   = {Communications in Nonlinear Science and Numerical Simulation},
  year      = {2011},
  volume    = {16},
  number    = {8},
  pages     = {2999--3004},
  issn      = {1007-5704},
  abstract  = {This paper describes an approach for recovering a signal, along with the derivatives of the signal, from a noisy time series. To mimic an experimental setting, noise was superimposed onto a deterministic time series. Data smoothing was then used to successfully recover the derivative coordinates; however, the appropriate level of data smoothing must be determined. To investigate the level of smoothing, an information theoretic is applied to show a loss of information occurs for increased levels of noise; conversely, we have shown data smoothing can recover information by removing noise. An approximate criterion is then developed to balance the notion of information recovery through data smoothing with the observation that nearly negligible information changes occur for a sufficiently smoothed time series.},
  comment   = {Mutual information determines how much to smooth to get both a waveform value and its derivative

* Somewhat hacky, but easy to implement, maybe it works?

Possible uses?
* spline coeffs for lagged velocity basis or something?
-- derivative can smooth across time, or something
* something with wind power ramps},
  doi       = {DOI: 10.1016/j.cnsns.2010.11.011},
  file      = {Mann11smoothDerivInfo.pdf:Mann11smoothDerivInfo.pdf:PDF},
  groups    = {Read},
  keywords  = {Information theoretic},
  owner     = {scot},
  timestamp = {2011.06.07},
  url       = {http://www.sciencedirect.com/science/article/pii/S1007570410006003},
}

@Article{Gao06angularRgrsnSeason,
  author    = {Fei Gao and Kee-Seng Chia and Ingela Krantz and Per Nordin and David Machin},
  title     = {On the application of the von Mises distribution and angular regression methods to investigate the seasonality of disease onset},
  year      = {2006},
  volume    = {25},
  number    = {9},
  pages     = {1593--1618},
  doi       = {10.1002/sim.2463},
  abstract  = {This paper describes an approach to summarize the data arising from studies investigating the pattern of disease onset within a calendar year. Such data have been traditionally summarized into monthly counts summated over the complete years studied and patterns often examined by use of Pearson's 2 tests with 11 degrees of freedom. This test and others commonly used in practice are reviewed. As an alternative, we suggest that by first representing the date of onset for an individual as a point on a unit circle that the von Mises distribution with a single peak may provide a useful description of such data. Further an extension to angular regression including covariates, analogous to that used routinely in other areas of clinical research, potentially allows a more systematic and detailed investigation of possible seasonal patterns in patient subgroups. The methodology is applied to examples from the date of onset of primary angle-closure glaucoma and date of diagnosis of acute lymphoblastic leukaemia and examines in both situations how the peak onset varies with covariates. Difficulties associated with convergence to the maximum likelihood estimates of the associated parameters are described. Finally, we emphasize the need for individualized (rather than grouped) patient data to be available for study, a clear specification of the particular onset time studied, and suggest that further case studies are required to evaluate the approach.},
  file      = {Gao06angularRgrsnSeason.pdf:Gao06angularRgrsnSeason.pdf:PDF;Gao06angularRgrsnSeason.pdf:Gao06angularRgrsnSeason.pdf:PDF},
  journal   = {Statistics in Medicine},
  keywords  = {seasonality ? von Mises distribution ? angular regression ? acute primary angle-closure glaucoma ? acute lymphoblastic leukaemia ? leptospirosis},
  owner     = {sotterson},
  timestamp = {2009.05.08},
}

@Conference{J.Dobschinski08estPredIntvEns,
  author    = {J.Dobschinski and A.Wessel and B.Lange and K.Rohrig and L. v. Bremen and Y.M.Saint-Drenan},
  title     = {Estimation of Wind Power Prediction Intervals Using Stochastic Methods and Artificial Intelligence Model Ensembles},
  booktitle = {DEWEK},
  year      = {2008},
  abstract  = {This paper describes different methods to estimate the uncertainty of wind power forecasts in terms of prediction
intervals. The single methods and an ensemble average model have been applied to shortest-term wind power
forecasts (forecast horizon = 1, 2, 4 \& 8 h) of 62 spatially distributed wind farms in Germany to obtain intervals
with a nominal reliability of 90, 95 and 98 \%. Furthermore the ISET online model was used to calculate the
prediction interval for the total wind power generation of Germany with a reliability exceeding 99 \%. The skill of
the resultant intervals is investigated with regard to reliability and sharpness. It was found that the skill depends
on the quality of the underlying wind power forecast.},
  comment   = {Jan's suggestion for scoring probabilistic forecasts.},
  file      = {J.Dobschinski08estPredIntvEns.pdf:J.Dobschinski08estPredIntvEns.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.08.06},
}

@InProceedings{Yip08dynLineRateWind,
  author    = {Yip, H.T. and An, C. and Aten, M. and Ferris, R.},
  title     = {Dynamic Line Rating Protection for Wind Farm Connections},
  booktitle = {Developments in Power System Protection (DPSP)},
  year      = {2008},
  pages     = {693--697},
  month     = mar,
  abstract  = {This paper describes how dynamic line rating is applied for load management and protection of a 132 kV line between Skegness and Boston (North East of England) thereby enabling a larger penetration of wind generation. The rating of the line is calculated dynamically from local weather measurements to co-ordinate allowed generation automatically. As a back-up system, in case for some reason the wind farm power output is not reduced on command by the control system, it is proposed that a relay will initiate tripping of wind generators.},
  comment   = {Dynamic line rating avoids some wind turbine shutdowns? Read.},
  file      = {Yip08dynLineRateWind.pdf:Yip08dynLineRateWind.pdf:PDF;Yip08dynLineRateWind.pdf:Yip08dynLineRateWind.pdf:PDF},
  issn      = {0537-9989},
  keywords  = {power generation protection, wind power plantscload management, cload protection, clocal weather measurements, cwind generation, dynamic line rating protection, voltage 132 kV, wind farm connections},
  owner     = {sotterson},
  timestamp = {2009.04.09},
}

@InProceedings{Tung14randFrstQRhighDim,
  author       = {Tung, Nguyen Thanh and Huang, Joshua Zhexue and Khan, Imran and Li, Mark Junjie and Williams, Graham},
  title        = {Extensions to quantile regression forests for very high-dimensional data},
  booktitle    = {Pacific-Asia Conference on Knowledge Discovery and Data Mining},
  year         = {2014},
  pages        = {247--258},
  organization = {Springer},
  abstract     = {This paper describes new extensions to the state-of-the-art regression random forests Quantile Regression Forests (QRF) for applications to high-dimensional data with thousands of features. We propose a new subspace sampling method that randomly samples a subset of features from two separate feature sets, one containing important features and the other one containing less important features. The two feature sets partition the input data based on the importance measures of features. The partition is generated by using feature permutation to produce raw importance feature scores first and then applying p-value assessment to separate important features from the less important ones. The new subspace sampling method enables to generate trees from bagged sample data with smaller regression errors. For point regression, we choose the prediction value of Y from the range between two quantiles Q 0.05 and Q 0.95 instead of the conditional mean used in regression random forests. Our experiment results have shown that random forests with these extensions outperformed regression random forests and quantile regression forests in reduction of root mean square residuals.
Keywords: Regression Random Forests, Quantile Regression Forests,
Data Mining, High Dimensional Data},
  comment      = {Random Forest QR where per-node splitting feature samples both important and unimportant features.  Supposed to be better for high dimensions.},
  file         = {:Tung14randFrstQRhighDim.pdf:PDF},
  url          = {https://link.springer.com/chapter/10.1007/978-3-319-06605-9_21},
}

@InProceedings{Kankiewicz14solarUncertRsrcTune,
  author    = {Kankiewicz, Adam and Kirkland, Washington and Dise, John and Wu, Elynn and Perez, Richard},
  title     = {Solar 2014: Reducing solar project uncertainty with an optimized resource assessment tuning methodology},
  booktitle = {Proc. ASES Annu. Conf.},
  year      = {2014},
  pages     = {1--6},
  abstract  = {This paper describes procedures suitable for the optimal 
combination of ground-based and satellite-based 
irradiance data to reduce the overall uncertainty of the 
solar resource assessment. It presents a case study 
depicting the application of these approaches, and 
illustrates the benefits of proper solar resource tuning 
methods to ensure the production of a robust solar 
resource dataset.},
  comment   = {The tuning method used in Alfi16uncertSatteliteGrnd, which is what CPR SolarAnywhere uses in July 2019. 

* Satellite irradiance methods are more accurate than using ground measurements when the distance from ground instrument to site is > 25km.
* Mean Bias Error is -4.63\% over 12 months; -4.18\% during clear sky
* use only clear sky ground measurements to correct satellite measurements
  - only calculate when ground meas are within 20\% of solarAnywhere "clear sky GHI data"  
    - not clear what that means
  - must be careful to not overcorrect in cloudy areas
* somehow KSI goodness of fit is used to correct satellite GHI to match ground: both distribution and bias
* seasonal sliding window subtracts and sinusoidal bias
  - apparently has only 1 harmonic i.e. it's a pure sin
  - is sliding window binning something?  Not explained
* fixed GHI is used to reestimate DHI and DNI 
   - method preserving GHI/DHI ratio doesn't work
   - some other method (not explained) does work
* wind and temp are also corrected
  - NDFD and NARR (some kind of temp meas) are corrected w/ ground meas
  - seems like they just bias it -- hard to tell from graph if you could do better

POSSIBLE IMPROVEMENTS
* use modern nonlinear technique: gaussian process, NN, ....
* time of year has at least 2 harmonics (look again at Pritchard wind quantile persistence model)
* same thing for wind and temp corrections
* not clear what correction is applied during cloudy weather, or if it works linearly
* P90 comes from quantile regression; either sampling QR determined distributions and summing, or using them directly, if actually care about short time horizions?
* corrections should look for trends in remaining error (so can see Satellite or instrument degradation)




One of 3 papers describing CPR's solar assessment tuning.  Sent by Patrick.
},
  file      = {:Kankiewicz14solarUncertRsrcTune.pdf:PDF},
  url       = {https://www.cleanpower.com/resource/reducing-uncertainty-with-resource-assessment-tuning/},
}

@Article{Huang04realtimeMarket,
  author    = {Huang, J. and Yalla, P. and Yong, T.},
  title     = {New real time market applications at the California independent system operator (CAISO)},
  journal   = {Power Systems Conference and Exposition},
  year      = {2004},
  volume    = {3},
  pages     = {1228--1233},
  month     = oct,
  abstract  = {This paper describes the operation of the new realtime energy market at CAISO. The new real-time market utilizes the security constrained unit commitment (SCUC) and security constrained economic dispatch (SCED) programs to optimally determine resource commitments and dispatch schedules while observing resource and system real-time constraints [AI Cohen, et al., 1999]. The SCED calculates three prices: the dispatch interval ex-post market clearing price (MCP) and two weighted-average prices based on the dispatch interval ex-post MCP. This paper describes the scheduling activities of the CAISO new realtime market followed by the major scheduling applications. The scheduling market timeline and scheduling application interfaces are also described.},
  comment   = {Describes CAISO realtime transmission, generation and load markets; has lookahead and update times},
  doi       = {10.1109/PSCE.2004.1397599},
  file      = {Huang04realtimeMarket.pdf:Huang04realtimeMarket.pdf:PDF;Huang04realtimeMarket.pdf:Huang04realtimeMarket.pdf:PDF},
  keywords  = { energy management systems, load forecasting, power generation dispatch, power generation economics, power generation scheduling, power markets, pricing California independent system operator, automatic mitigation, energy management system, energy market, load predictor, market clearing price, real time market, resource dispatch, scheduling, security constrained economic dispatch, security constrained unit commitment},
  owner     = {sotterson},
  timestamp = {2009.02.24},
}

@Article{Potter06vShortWindFrcstTas,
  author    = {Potter, C.W. and Negnevitsky, M.},
  title     = {Very short-term wind forecasting for Tasmanian power generation},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2006},
  volume    = {21},
  number    = {2},
  pages     = {965--972},
  month     = may,
  issn      = {0885-8950},
  abstract  = {This paper describes very short-term wind prediction for power generation, utilizing a case study from Tasmania, Australia. Windpower presently is the fastest growing power generation sector in the world. However, windpower is intermittent. To be able to trade efficiently, make the best use of transmission line capability, and address concerns with system frequency in a re-regulated system, accurate very short-term forecasts are essential. The research introduces a novel approach-the application of an adaptive neuro-fuzzy inference system to forecasting a wind time series. Over the very short-term forecast interval, both windspeed and wind direction are important parameters. To be able to be gain the most from a forecast on this time scale, the turbines must be directed toward on oncoming wind. For this reason, this paper forecasts wind vectors, rather than windspeed or power output.},
  comment   = {Supposed to have ramp constraints but I don't see them. Zheng09rampDataMine says this has ramp rate constraint (ref [4] in that paper) but I don't see it.},
  doi       = {10.1109/TPWRS.2006.873421},
  file      = {Potter06vShortWindFrcstTas.pdf:Potter06vShortWindFrcstTas.pdf:PDF},
  groups    = {Read},
  keywords  = { Tasmanian power generation; adaptive neurofuzzy inference system; short-term wind forecasting; transmission line capability; wind power; wind time series; fuzzy neural nets; load forecasting; power engineering computing; power transmission; time series; wind power plants;},
  owner     = {scot},
  timestamp = {2011.05.26},
}

@InProceedings{Shenoy15stochOptLdFrcstRgrssn,
  author    = {Shenoy, Saahil and Gorinevsky, Dimitry and Boyd, Stephen},
  title     = {Non-Parametric Regression Modeling for Stochastic Optimization of Power Grid Load Forecast},
  booktitle = {American Control Conference},
  year      = {2015},
  month     = jun,
  abstract  = {This paper develops a method for building nonparametric
stochastic models of multivariate distributions from
large data sets. The motivation is stochastic optimization based
on time series forecasting models. The proposed non-parametric
stochastic modeling approach is based on multiple quantile
regressions with inter-quantile smoothing. The models are built
using ADMM optimization approach scalable to large datasets.
As an application example, the paper considers forecasting of
the loads in the electrical power grid. The forecasted load is used
for the electricity procurement in the day-ahead power market.
The stochastic optimization trades the costs of advance and
spot procurements of the electricity. This problem is currently
important because the random variability in the grid power
load increases with integration of renewable generation.},
  comment   = {Probabiistlic load forecasting for day ahead. Used in Shenoy15StochOptPowMktRgrssn (I think). Note that coauthor is S. Boyd, of my Stanford optimization class.

This was also Dominik's paper. Quantile regression and quadratic programming (a subset of convex programming, the class I was taking from Boyd, one of the authors). The interquantile smoothing must also help with crossover.

This could be an important paper b/c it's all things: intrerday market, load forecasting, stochastic optimization, and quantile smoothing.},
  file      = {Shenoy15stochOptLdFrcstRgrssn.pdf:Shenoy15stochOptLdFrcstRgrssn.pdf:PDF},
  location  = {Chicago, IL},
  owner     = {sotterson},
  timestamp = {2015.04.17},
  url       = {http://web.stanford.edu/~gorin/papers/ACC15_Quantile.pdf},
}

@Article{Kato08modelAngleObs,
  author    = {Kato, Shogo and Shimizu, Kunio},
  title     = {Dependent models for observations which include angular ones},
  journal   = {Journal of Statistical Planning and Inference},
  year      = {2008},
  volume    = {138},
  number    = {11},
  pages     = {3538--3549},
  issn      = {0378-3758},
  abstract  = {This paper discusses some stochastic models for dependence of observations which include angular ones. First, we provide a theorem which constructs four-dimensional distributions with specified bivariate marginals on certain manifolds such as two tori, cylinders or discs. Some properties of the submodel of the proposed models are investigated. The theorem is also applicable to the construction of a related Markov process, models for incomplete observations, and distributions with specified marginals on the disc. Second, two maximum entropy distributions on the cylinder are discussed. The circular marginal of each model is distributed as the generalized von Mises distribution which represents a symmetric or asymmetric, unimodal or bimodal shape. The proposed cylindrical model is applied to two data sets.},
  comment   = {use for lagged wind velocity basis? The author has a related PhD thesis here: http://kosmos.lib.keio.ac.jp/primo_library/libweb/action/search.do?fn=search&vid=KEIO&vl(freeText0)=Statistical+models+for+data+which+include+angular+observations+%20%E5%8A%A0%E8%97%A4%2C+%E6%98%87%E5%90%BE%202007},
  file      = {Kato08modelAngleObs.pdf:Kato08modelAngleObs.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.12.22},
}
???

@Article{Rue02fitGausMarkovRndFlds,
  author    = {Rue, H{\aa}vard and Tjelmeland, H{\aa}kon},
  title     = {Fitting Gaussian Markov Random Fields to Gaussian Fields},
  journal   = {Scandinavian Journal of Statistics},
  year      = {2002},
  volume    = {29},
  number    = {1},
  pages     = {31--49},
  issn      = {1467-9469},
  abstract  = {This paper discusses the following task often encountered in building Bayesian spatial models: construct a homogeneous Gaussian Markov random field (GMRF) on a lattice with correlation properties either as present in some observed data, or consistent with prior knowledge. The Markov property is essential in designing computationally efficient Markov chain Monte Carlo algorithms to analyse such models. We argue that we can restate both tasks as that of fitting a GMRF to a prescribed stationary Gaussian field on a lattice when both local and global properties are important. We demonstrate that using the Kullback???Leibler discrepancy often fails for this task, giving severely undesirable behaviour of the correlation function for lags outside the neighbourhood. We propose a new criterion that resolves this difficulty, and demonstrate that GMRFs with small neighbourhoods can approximate Gaussian fields surprisingly well even with long correlation lengths. Finally, we discuss implications of our findings for likelihood based inference for general Markov random fields when global properties are also important.},
  comment   = {Explains why increasing autocorrelation in a covariance matrix (e.g. temporal, as in  Tastu15spcTimeTrajGaussCpla)   is INCREASING precision matrix values. (See eq. 2)

* also a Gaussian Random Markov Field defineds a neighborhood structure of values in the precision matrix, which has zero values outside of a dependency neighborhood.

RELATED

* Simpson12spatStatForgetCov
* Lindgren11linkGaussMMRFandSDE
* Tastu15spcTimeTrajGaussCpla},
  doi       = {10.1111/1467-9469.00058},
  file      = {Rue02fitGausMarkovRndFlds.pdf:Rue02fitGausMarkovRndFlds.pdf:PDF},
  keywords  = {conditional autoregressive models, empirical Bayes, Gaussian fields, Gaussian Markov random field, Kullback-Leibler discrepancy, Markov chain Monte Carlo, maximum likelihood},
  owner     = {sotterson},
  publisher = {Blackwell Publishers Ltd.},
  timestamp = {2017.06.15},
  url       = {http://dx.doi.org/10.1111/1467-9469.00058},
}

@Article{Zhou12prbFrcstMkts,
  author    = {Zhou, Z. and Botterud, A. and Wang, J. and Bessa, R.J. and Keko, H. and Sumaili, J. and Miranda, V.},
  title     = {Application of probabilistic wind power forecasting in electricity markets},
  journal   = {Wind Energy},
  year      = {2012},
  volume    = {16},
  number    = {3},
  issn      = {1099-1824},
  abstract  = {This paper discusses the potential use of probabilistic wind power forecasting in electricity markets, with focus on the scheduling and dispatch decisions of the system operator. We apply probabilistic kernel density forecasting with a quantile-copula estimator to forecast the probability density function, from which forecasting quantiles and scenarios with temporal dependency of errors are derived. We show how the probabilistic forecasts can be used to schedule energy and operating reserves to accommodate the wind power forecast uncertainty. We simulate the operation of a two-settlement electricity market with clearing of day-ahead and real-time markets for energy and operating reserves. At the day-ahead stage, a deterministic point forecast is input to the commitment and dispatch procedure. Then a probabilistic forecast is used to adjust the commitment status of fast-starting units closer to real time, on the basis of either dynamic operating reserves or stochastic unit commitment. Finally, the real-time dispatch is based on the realized availability of wind power. To evaluate the model in a large-scale real-world setting, we take the power system in Illinois as a test case and compare different scheduling strategies. The results show better performance for dynamic compared with fixed operating reserve requirements. Furthermore, although there are differences in the detailed dispatch results, dynamic operating reserves and stochastic unit commitment give similar results in terms of cost. Overall, we find that probabilistic forecasts can contribute to improve the performance of the power system, both in terms of cost and reliability.},
  comment   = {Useful for TSO's in Ewelina? See also: Zhou13probFrcstElecMkts},
  doi       = {10.1002/we.1496},
  file      = {Zhou12prbFrcstMkts.pdf:Zhou12prbFrcstMkts.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  keywords  = {wind power forecasting, probabilistic forecasts, stochastic unit commitment, electricity system operation, dynamic operating reserves},
  owner     = {sotterson},
  publisher = {John Wiley \& Sons, Ltd},
  timestamp = {2013.03.13},
}

@Article{Henriot14windIntradayCentral,
  author    = {Henriot, Arthur and others},
  title     = {Market design with centralized wind power management: handling low-predictability in intraday markets},
  journal   = {The Energy Journal},
  year      = {2014},
  volume    = {35},
  number    = {1},
  pages     = {99--117},
  abstract  = {This paper evaluates the benefits for an agent managing the wind power production within a given
power system to trade in the intraday electricity markets, in a context of massive penetration of
intermittent renewables. Using a simple analytical model we find out that there are situations when it
will be costly for this agent to adjust its positions in intraday markets. A first key factor is of course
the technical flexibility of the power system: if highly flexible units provide energy at very low prices
in real-time there is no point in participating into intraday markets. Besides, we identify the way wind
production forecast errors evolve constitutes another essential, although less obvious, key-factor. Both
the value of the standard error and the correlation between forecasts errors at different gate closures
will determine the strategy of the wind power manager. Policy implications of our results are the
following: low liquidity in intraday markets will be unavoidable for given sets of technical
parameters, it will also be inefficient in some cases to set discrete auctions in intraday markets, and
compelling players to adjust their position in intraday markets will then generate additional costs.

Keywords: Market design, intraday markets, wind forecasts, large-scale renewables, intermittency},
  comment   = {When it makes sense for TSO's or BRP (balance responsible parties) to trade same electricity (ref. from VonSelasinsky14intradayMktDE).  Also considers the effect of the way wind power forecast errors evolve.},
  file      = {Henriot14windIntradayCentral.pdf:Henriot14windIntradayCentral.pdf:PDF},
  publisher = {International Association for Energy Economics},
  url       = {http://www.faee.fr/files/file/aee/seminaires/Divers/Prix%20FAEE%202012_Article_Henriot.pdf},
}

@InProceedings{Hassan13nnEnsbl,
  author    = {Hassan, Saima and Khosravi, Abbas and Jaafar, Jafreezal},
  title     = {Neural network ensemble: Evaluation of aggregation algorithms in electricity demand forecasting},
  booktitle = {Neural Networks (IJCNN), The 2013 International Joint Conference on},
  year      = {2013},
  pages     = {1--6},
  abstract  = {This paper examines and analyzes different aggregation algorithms to improve accuracy of forecasts obtained using neural network (NN) ensembles. These algorithms include equal-weights combination of Best NN models, combination of trimmed forecasts, and Bayesian Model Averaging (BMA). The predictive performance of these algorithms are evaluated using Australian electricity demand data. The output of the aggregation algorithms of NN ensembles are compared with a Naive approach. Mean absolute percentage error is applied as the performance index for assessing the quality of aggregated forecasts. Through comprehensive simulations, it is found that the aggregation algorithms can significantly improve the forecasting accuracies. The BMA algorithm also demonstrates the best performance amongst aggregation algorithms investigated in this study.},
  comment   = {Adaboost-like continuous forecasting},
  doi       = {10.1109/IJCNN.2013.6707005},
  file      = {Hassan13nnEnsbl.pdf:Hassan13nnEnsbl.pdf:PDF},
  issn      = {2161-4393},
  owner     = {sotterson},
  timestamp = {2014.01.21},
}

@Article{Almeida08openTechInvoDevCntries,
  author    = {Almeida, Rita and Fernandes, Ana Margarida},
  title     = {Openness and technological innovations in developing countries: evidence from firm-level surveys},
  journal   = {The Journal of Development Studies},
  year      = {2008},
  volume    = {44},
  number    = {5},
  pages     = {701--727},
  abstract  = {This paper examines international technology transfers using firm-level data across 43 
developing countries. Our findings show that exporting and importing activities are important 
channels for the transfer of technology. Majority foreign-owned firms are less likely to engage 
in technological innovations than minority foreign-owned firms or domestic firms. We interpret 
this finding as evidence that the technology transferred from multinational parents to majority-
owned subsidiaries is more mature than that transferred to minority-owned subsidiaries. Our 
findings also suggest that foreign-owned subsidiaries rely mostly on the direct transfer of 
technology from their parents and that firms that import intermediate inputs are more likely to 
acquire new technology from their machinery suppliers. },
  comment   = {A linear, probably logistic regression adoption somewhat like WattPlan Grid.  But is instantaneous, not over time, it seems.},
  file      = {:Almeida08openTechInvoDevCntries.pdf:PDF},
  publisher = {Taylor \& Francis},
  url       = {https://www.tandfonline.com/doi/abs/10.1080/00220380802009217},
}

@Article{Leith04elecForecastGaussProc,
  author    = {Leith, D.J. and Heidl, M. and Ringwood, J.V.},
  title     = {{Gauss}ian process prior models for electrical load forecasting},
  year      = {2004},
  month     = sep,
  pages     = {112--117},
  url       = {http://ieeexplore.ieee.org.offcampus.lib.washington.edu/search/srchabstract.jsp?arnumber=1378672&isnumber=30086&punumber=9484&k2dockey=1378672@ieeecnfs&query=((gaussian+process+prior+models+for+electrical+load+forecasting)%3Cin%3Emetadata)&pos=0&access=no},
  abstract  = {This paper examines models based on Gaussian process (GP) priors for electrical load forecasting. This methodology is seen to encompass a number of popular forecasting methods, such as basic structural models (BSMs) and seasonal auto-regressive intergrated (SARI) as special cases. The GP forecasting models are shown to have some desirable properties and their performance is examined on weekly and yearly Irish load data.},
  file      = {Leith04elecForecastGaussProc.pdf:Leith04elecForecastGaussProc.pdf:PDF;Leith04elecForecastGaussProc.pdf:Leith04elecForecastGaussProc.pdf:PDF},
  journal   = {Probabilistic Methods Applied to Power Systems (PMAPS)},
  keywords  = { Gaussian channels, autoregressive processes, load forecasting Gaussian process, basic structural models, electrical load forecasting, electricity demand, seasonal auto-regressive intergrated},
  owner     = {scotto},
  timestamp = {2008.10.04},
}

@TechReport{Tu13supervsdFctrFrcst,
  author      = {Tu, Yundong and Lee, Tae-Hwy},
  title       = {Forecasting using supervised factor models},
  institution = {Working paper, University of California, Riverside},
  year        = {2013},
  abstract    = {This paper examines the theoretical and empirical properties of a supervised factor
model based on combining forecasts using principal components (CFPC), in comparison
with two other supervised factor models (partial least squares regression, PLS, and
principal covariate regression, PCovR) and with the unsupervised principal component
regression, PCR. The supervision refers to training the predictors for a variable to
forecast. We compare the performance of the three supervised factor models and the
unsupervised factor model in forecasting of U.S. CPI in?ation. The main ?nding is
that the predictive ability of the supervised factor models is much better than the
unsupervised factor model. The computation of the factors can be doubly supervised
together with variable selection, which can further improve the forecasting performance
of the supervised factor models. Among the three supervised factor models, the CFPC
best performs and is also most stable. While PCovR also performs well and is stable,
the performance of PLS is less stable over di?erent out-of-sample forecasting periods.
The e?ect of supervision gets even larger as forecast horizon increases. Supervision
helps to reduce the number of factors and lags needed in modelling economic structure,
achieving more parsimony.
Key Words: Supervised factor model; Variable selection; Combining forecasts; Princi-
pal components; Supervision matrix; Fixed point; Principal covariate regression; Par-
tial least squares.},
  comment     = {Simple linear dimension reduction that works better for forecasting than PCA and PLS, as well as PCovR. Even better if also do feature selection first. Could be a way to do multi-nwp forecasts, reference forecast upscaling, or high dim quantile regression.

Compares 3 ideas:
PCR: PCA before linear regression
* almost always beat by the the "linear factor" techniques below
* best at short horizons; mysteriously fails relative to others at long horizons

PLS: partial least squares way
* dimension reduction in the NIPALS
* says Kelly12threePssRgrssn is a subset of PLS
* overall, the worst technique except PCR

PCovR: principal covariate regression
* PCA on X
* forecast done to minimize a weighted combo of two error terms

CFPC: combining forecasts using principal components
* Instead of doing PCA on inputs, does PCA on vector of forecasts produced with each scalar input by itself.
* Does PCA on those forecasts
* PCA comps used to make forecasts
* Same result as PCR when X is full rank
 -- not very true for upscaling with reference forecasts, where many ref forecasts are very similar.
* has some proofs that the method works in general

Variable selection
* done before doing the dimension reduction (on the order of 131 to 30 variables on an economic problem)
* compared hard thresholding vi t-statistic and LARS
* Tries LARS because it's better on correlated predictors (group effect), ranks their joint advantage instead of marginal advantage, and is as fast as OLS.
* But later finds that hard thresholding often works better anyway (p. 19)

Parsimony
* Supervised factor models are more parsimonious than PCR (small num. of factors are selected).
* num. dimensions set by BIC but I could also use Minka, since its PCA.

Performance vs. horizon
Superiority over PCR is seen most at long forecast horizons. This is with a weird target that averages by the horizon length, yet somehow, PCR is unable to take advantage of this.

EXTENSIONS FOR POINT FORECASTING

Says it could be extended by: Tu12MdlAvgPartEff

EXTENSIONS FOR QUANTILE FORECASTING

Finds that the approach in Kelly12threePssRgrssn (a subset of PLS) doesn't work as well as this algorithm on unseen data but it, CFPC, and PCovR are all better than PCA. Since Giglio13riskPartialQR used Kelly12, maybe the approach in this paper would be better for high dimensional quantile regression.

Splines: If could extend eq 17 on p. 1 to a K dimensional spline basis (one for each x_i, then this could be a way to to multivariate QR instead of the one in: Giglio13riskPartialQR
Seems possible since QR's would still be linear.},
  file        = {Tu13supervsdFctrFrcst.pdf:Tu13supervsdFctrFrcst.pdf:PDF},
  groups      = {Read},
  owner       = {sotterson},
  timestamp   = {2014.06.17},
}

@Article{Pascanu2013numRespRgnsDeepNNpieceLin,
  author      = {Razvan Pascanu and Guido Montufar and Yoshua Bengio},
  title       = {On the number of response regions of deep feed forward networks with piece-wise linear activations},
  abstract    = {This paper explores the complexity of deep feedforward networks with linear pre-synaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piecewise linear functions based on computational geometry. We look at a deep rectifier multi-layer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime, when the number of inputs stays constant, if the shallow model has $kn$ hidden units and $n_0$ inputs, then the number of linear regions is $O(k^{n_0}n^{n_0})$. For a $k$ layer model with $n$ hidden units on each layer it is $\Omega(\left\lfloor {n}/{n_0}\right\rfloor^{k-1}n^{n_0})$. The number $\left\lfloor{n}/{n_0}\right\rfloor^{k-1}$ grows faster than $k^{n_0}$ when $n$ tends to infinity or when $k$ tends to infinity and $n \geq 2n_0$. Additionally, even when $k$ is small, if we restrict $n$ to be $2n_0$, we can show that a deep model has considerably more linear regions that a shallow one. We consider this as a first step towards understanding the complexity of these models and specifically towards providing suitable mathematical tools for future analysis.

Keywords: Deep learning, artificial neural network, rectifier unit, hyperplane arrangement, representational power},
  comment     = {How a deep NN can model more complexity than a shallow one, given the same # of hidden nodes.  For a fixed # of hidden nodes, the number of piecewise linear regions increases faster with a deep NN than a shallow one.

See also: 
Montufar2014numLinRegnDeepNN
Bengio09lrnDeepArchAI},
  date        = {2013-12-20},
  eprint      = {1312.6098v5},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Pascanu2013numRespRgnsDeepNNpieceLin.pdf:PDF},
  keywords    = {cs.LG, cs.NE},
  url         = {https://arxiv.org/abs/1312.6098},
}

@Article{Souza10gpCorrBoost,
  author    = {Souza, LuziaVidal and Pozo, Aurora and da??Rosa, JoelMauricioCorrea and Neto, AnselmoChaves},
  title     = {Applying correlation to enhance boosting technique using genetic programming as base learner},
  journal   = {Applied Intelligence},
  year      = {2010},
  volume    = {33},
  number    = {3},
  pages     = {291--301},
  issn      = {0924-669X},
  abstract  = {This paper explores the Genetic Programming and Boosting technique to obtain an ensemble of regressors and proposes a new formula for the updating of weights, as well as for the final hypothesis. Differently from studies found in the literature, in this paper we investigate the use of the correlation metric as an additional factor for the error metric. This new approach, called Boosting using Correlation Coefficients (BCC) has been empirically obtained after trying to improve the results of the other methods. To validate this method, we conducted two groups of experiments. In the first group, we explore the BCC for time series forecasting, in academic series and in a widespread Monte Carlo simulation covering the entire ARMA spectrum. The Genetic Programming (GP) is used as a base learner and the mean squared error (MSE) has been used to compare the accuracy of the proposed method against the results obtained by GP, GP using traditional boosting and the traditional statistical methodology (ARMA). The second group of experiments aims at evaluating the proposed method on multivariate regression problems by choosing Cart (Classification and Regression Tree) as the base learner.},
  comment   = {Continuous boosting forecast algorithm, maybe like adaboost based Lillywhite13featCnstrct},
  doi       = {10.1007/s10489-009-0166-y},
  keywords  = {Boosting technique; Genetic programming; Regression methods; Time series},
  language  = {English},
  owner     = {sotterson},
  publisher = {Springer US},
  timestamp = {2014.01.21},
}

@Article{Geweke07smoothMixRegress,
  author    = {Geweke, John and Keane, Michael},
  title     = {Smoothly mixing regressions},
  journal   = {Journal of Econometrics},
  year      = {2007},
  volume    = {138},
  number    = {1},
  pages     = {252--290},
  month     = may,
  abstract  = {This paper extends the conventional Bayesian mixture of normals model by permitting state probabilities to depend on observed covariates. The dependence is captured by a simple multinomial probit model. A conventional and rapidly mixing MCMC algorithm provides access to the posterior distribution at modest computational cost. This model is competitive with existing econometric models, as documented in the paper's illustrations. The first illustration studies quantiles of the distribution of earnings of men conditional on age and education, and shows that smoothly mixing regressions are an attractive alternative to nonBayesian quantile regression. The second illustration models serial dependence in the S\&P 500 return, and shows that the model compares favorably with ARCH models using out of sample likelihood criteria.},
  comment   = {MCMC switching/learning of a mixture of regressors, each not necessarily linear. Predicts S\&P 500. Computationally efficient.

For quantile regression, or point foreasts, providing conditional distributions.},
  doi       = {10.1016/j.jeconom.2006.05.022},
  file      = {Geweke07smoothMixRegress.pdf:Geweke07smoothMixRegress.pdf:PDF;Geweke07smoothMixRegress.pdf:Geweke07smoothMixRegress.pdf:PDF},
  groups    = {Ensemble, PointDerived, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2008.12.12},
  url       = {http://www.sciencedirect.com.offcampus.lib.washington.edu/science?_ob=ArticleURL&_udi=B6VC0-4KGG1XW-2&_user=582538&_coverDate=05%2F31%2F2007&_rdoc=13&_fmt=high&_orig=browse&_srch=doc-info(%23toc%235940%232007%23998619998%23647270%23FLA%23display%23Volume)&_cdi=5940&_sort=d&_docanchor=&_ct=16&_acct=C000029718&_version=1&_urlVersion=0&_userid=582538&md5=0fded60aa147ca390aa70e0eac8915d5},
}

@InProceedings{Barbounis2005locRecurNeuralOptWind,
  author    = {Barbounis, T.G. and Theocharis, J.B.},
  title     = {Locally recurrent neural networks optimal filtering algorithms: application to wind speed prediction using spatial correlation},
  booktitle = {International Joint Conference on Neural Networks (IJCNN)},
  year      = {2005},
  volume    = {5},
  month     = jul,
  pages     = {2711--2716},
  doi       = {10.1109/IJCNN.2005.1556353},
  url       = {http://ieeexplore.ieee.org.offcampus.lib.washington.edu/search/srchabstract.jsp?arnumber=1556353&isnumber=33093&punumber=10421&k2dockey=1556353@ieeecnfs&query=((locally+recurrent+neural+networks+optimal+filtering+algorithms%3A+application+to+wind+speed+prediction+using+spatial+correlation)%3Cin%3Emetadata)&pos=0&access=no},
  abstract  = {This paper focuses on a locally recurrent multilayer network with internal feedback paths, the IIR-MLP. The computation of the partial derivatives of the network's output with respect to its trainable weights is achieved using backpropagation through adjoints and a second order global recursive prediction error (GRPE) training algorithm is developed. Also, a local version of the GRPE is presented in order to cope with the increased computational burden of the global version. The efficiency of the proposed learning schemes, as compared to conventional gradient based methods, is tested on the wind prediction problem from 15 min to 3 h ahead on a site, using spatial correlation and facilitating measurements from nearby sites up to 40 km away.},
  file      = {Barbounis2005locRecurNeuralOptWind.pdf:Barbounis2005locRecurNeuralOptWind.pdf:PDF;Barbounis2005locRecurNeuralOptWind.pdf:Barbounis2005locRecurNeuralOptWind.pdf:PDF},
  journal   = {International Joint Conference on Neural Networks (IJCNN)},
  keywords  = { backpropagation, geophysics computing, gradient methods, multilayer perceptrons, recurrent neural nets, wind 15 to 180 mins, 40 km, backpropagation, global recursive prediction error training algorithm, gradient method, internal feedback path, optimal filtering algorithm, partial derivative computation, recurrent multilayer network, recurrent neural network, spatial correlation, wind speed prediction},
  owner     = {sotterson},
  timestamp = {2008.07.03},
}

@TechReport{Mitra09scenarioHMM_GMM,
  author      = {Sovan Mitra},
  title       = {Optimisation of Stochastic Programming by Hidden {Markov} Modelling based Scenario Generation},
  institution = {Preprint Submitted to Elsevier},
  year        = {2009},
  note        = {arXiv:0904.1131v1},
  abstract    = {This paper formed part of a preliminary research report for a risk consultancy and academic research. Stochastic Programming models provide a powerful paradigm for decision making under uncertainty. In these models the uncertainties are represented by a discrete scenario tree and the quality of the solutions obtained is governed by the quality of the scenarios generated. We propose a new technique to generate scenarios based on Gaussian Mixture Hidden Markov Modelling. We show that our approach explicitly captures important time varying dynamics of stochastic processes (such as autoregression and jumps) as well as non-Gaussian distribution characteristics (such as skewness and kurtosis). Our scenario generation method enables richer robustness and scenario analysis through exploiting the tractable properties of Markov models and Gaussian mixture distributions. We demonstrate the benefits of our scenario generation method by conducting numerical experiments on FTSE-100 data.},
  comment     = {spinning reserves. fancy dependence?},
  file        = {Mitra09scenarioHMM_GMM.pdf:Mitra09scenarioHMM_GMM.pdf:PDF},
  owner       = {scot},
  timestamp   = {2010.11.24},
  url         = {http://arxiv.org/abs/0904.1131},
}

@Article{Daconti03incrPowTrans,
  author    = {Daconti, J.R. and Lawry, D.C.},
  title     = {Increasing power transfer capability of existing transmission lines},
  journal   = {Transmission and Distribution Conference, IEEE},
  year      = {2003},
  volume    = {3},
  pages     = {1004--1009},
  month     = sep,
  abstract  = {This paper initially describes the main constraints that limit the transmission system power transfer capability, as well as the existing methods to alleviate these constraints, such as transmission line uprating/upgrading techniques. It also discusses the main feasibility issues that need to be addressed aiming to evaluate when an uprating/upgrading solution should be recommended. After that, aspects of effectiveness, required technical analysis and usual methods for transmission line voltage and thermal uprating are presented. Finally, the dynamic thermal rate monitoring is emphasized as a good thermal uprating solution and its commercially available technologies are presented.},
  comment   = {Power line rating theory; underuse due to conservative stds; dummy powerline sensor; nice slides Two limits: thermal and over-voltage
Over voltage * circuit shorts, EMI, customer equipment damage * seems like handling overvoltage helps mre and costs more than thermal problems * I'm skipping most of this b/c I'm focused on thermal for now Thermal Restrictions (problems caused by overheating) * weakens conductor due to aluminum annealing * conductor to ground clearance * time frames covered by constraints: constant; long term (4 hrs); short term (15 mins) * must watch max temp, spatial clearance, mech. strength, mag. field * has equations for thermal balance (compare to Muhr08weathParamLineMon) Static Rate Monitoring * max current assuming low wind, max solar, max temperature -- nice histogram: this almsot never happens -- 15-30\% more power possible w/ dynamic rating based on actual conditions Five main dynamic line-rating techniques: * weather monitoring * tension monitoring * sag monitoring. * line temperature monitoring * and the new PTI ThermalRate technique (conductor replica, I think) How to do a thermal upgrade (increase power rating of line w/ thermal techniques * a bunch of mechanical and electrical stuff, like new towers, disconnect switches * do dynamic thermal rate monitoring (subject of this paper) Conductor replica methodology * their solution: put a short chunk of powerline outside, heat it, assume real line gets as hot * similar to Muhr08weathParamLineMon * IEEE Std 738 determines "effective wind speed", representing total cooling ==> dynamic line rating -- see: 06IEEEcurrentTempOvrhdCond * says accuracy below 5ft/s (1.5 m/s) wind speed is very critical -- Muhr08weathParamLineMon says EU standard lmits current at 35C and 0.6 m/s

* Nice presentation power point also downloaded, in files section of this bibtex entry},
  doi       = {10.1109/TDC.2003.1335079},
  file      = {Daconti03incrPowTrans.pdf:Daconti03incrPowTrans.pdf:PDF;Daconti03incrPowTrans.ppt:Daconti03incrPowTrans.ppt:PowerPoint;Daconti03incrPowTrans.pdf:Daconti03incrPowTrans.pdf:PDF},
  groups    = {Read},
  keywords  = { power transmission lines dynamic thermal rate monitoring, power transfer capability, thermal uprating, transmission line, transmission system, voltage uprating},
  owner     = {sotterson},
  timestamp = {2009.02.23},
}

@Article{Keelin11QuantParamDist,
  author    = {Keelin, Thomas W and Powley, Bradford W},
  title     = {Quantile-parameterized distributions},
  journal   = {Decision Analysis},
  year      = {2011},
  volume    = {8},
  number    = {3},
  pages     = {206--219},
  abstract  = {This paper introduces a new class of continuous probability distributions that are flexible enough to represent a wide range of uncertainties such as those that commonly arise in business, technology, and science. In many such cases, the nature of the uncertainty is more naturally characterized by quantiles than by parameters of familiar continuous probability distributions. In the practice of decision analysis, it is common to fit a hand-drawn curve to quantile outputs from probability elicitations on a continuous uncertain quantity and to then discretize the curve. The resulting discrete probability distribution is an approximation that cuts off the distribution's tails and eliminates intermediate values. Quantile-parameterized distributions address this problem by using quantiles themselves to parameterize a continuous probability distribution. We define quantile-parameterized distributions, illustrate their flexibility and range of applicability, and conclude with practical considerations when parameterizing distributions using inconsistent quantile assessments.
Keywords : continuous probability distribution ; probability encoding ; decision analysis ; quantile function ; inverse cumulative distribution function ; basis function},
  comment   = {A set of distributions that can be estimated from quantiles. Explains what to do if those quantiles are inconsistent (I think it means "are crossed over").

Unfortunately, I couldn't find the paper.

I wonder how these are related to Johnson System Distributions (George11estJohnsonDistBnd) or Stable distributions (Nolan15StableDistBootCh1), both of which can be estimated from quantiles.},
  doi       = {10.1287/deca.1110.0213},
  publisher = {INFORMS},
}

@Article{Baccala01parDirCoher,
  author               = {Baccala, Luiz A. and Sameshima, K.},
  title                = {Partial directed coherence: a new concept in neural structure determination},
  journal              = {Biological Cybernetics},
  year                 = {2001},
  volume               = {84},
  pages                = {463--474},
  abstract             = {This paper introduces a new frequency-domain approach to describe the relationships (direction of information flow) between multivariate time series based on the decomposition of multivariate partial coherences computed from multivariate autoregressive models. We discuss its application and compare its performance to other approaches to the problem of determining neural structure relations from the simultaneous measurement of neural electrophysiological signals. The new concept is shown to reflect a frequency-domain representation of the concept of Granger causality.},
  citeulike-article-id = {1228261},
  comment              = {Partial directed coherence: estimate arbitrary delay causality in spectra: a better partial coherence? Seems to be first paper * could be useful for feature selection, esp. offsite observattion? * big improvements in Baccald07genParDirCoher * PDC is relative effect of j on the future of i with other factors removed. * PDC excludes instantaneous regression; only has past effects of j on i * Seems perfect for forecasting variable selection except that it's linear and that you still need to pick select delay(s) for actual forecasts * a variant that emphasizes large signals in the thing ot be predicted? * use a sliding window PDC to show switching causality * draw causality graphs using peak freq. Wonder why they didn't use mean? Problems * requires reliable fittingof MAR (multivariate autoregression) models * must determine joint time series stationarity * significance is a big problem -- appears to depend upon analysis window length and num. of parallel signals. -- Papers have been published on this since this one was written.},
  file                 = {Baccala01parDirCoher.pdf:Baccala01parDirCoher.pdf:PDF;Baccala01parDirCoher.pdf:Baccala01parDirCoher.pdf:PDF},
  groups               = {Read},
  keywords             = {bibtex-import},
  owner                = {sotterson},
  posted-at            = {2007-04-15 20:59:05},
  timestamp            = {2009.03.16},
  url                  = {http://www.ncbi.nlm.nih.gov/pubmed/11417058?dopt=Abstract},
}

@Article{Karra17CopulaIdxDepMono,
  author    = {Karra, Kiran and Mili, Lamine},
  title     = {Copula Index for Detecting Dependence and Monotonicity between Stochastic Signals},
  journal   = {arXiv preprint arXiv:1703.06686},
  year      = {2017},
  abstract  = {This paper introduces a nonparametric copula-based approach for detecting the strength
and monotonicity of linear and nonlinear statistical dependence between bivariate contin-
uous, discrete or hybrid random variables and stochastic signals, termed CIM. We show
that CIM satisfies the data processing inequality and is consequently a self-equitable met-
ric. Simulation results using synthetic datasets reveal that the CIM compares favorably
to other state-of-the-art statistical dependency metrics, including the Maximal Informa-
tion Coefficient (MIC ), Randomized Dependency Coefficient (RDC ), distance Correlation
(dCor), Copula correlation (Ccor), and Copula Statistic (CoS) in both statistical power
and sample size requirements. Simulations using real world data highlight the importance
of understanding the monotonicity of the dependence structure.
Keywords: copula, statistical dependency, monotonic, equitability, discrete},
  file      = {Karra17CopulaIdxDepMono.pdf:Karra17CopulaIdxDepMono.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.06},
  url       = {https://arxiv.org/abs/1703.06686},
}

@Article{Karpatne17physNNlakeT,
  author   = {Karpatne, Anuj and Watkins, William and Read, Jordan and Kumar, Vipin},
  title    = {Physics-guided Neural Networks (PGNN): An Application in Lake Temperature Modeling},
  journal  = {arXiv preprint arXiv:1710.11431},
  year     = {2017},
  abstract = {This paper introduces a novel framework for combin-
ing scientific knowledge of physics-based models with
neural networks to advance scientific discovery. This
framework, termed as physics-guided neural network
(PGNN), leverages the output of physics-based model
simulations along with observational features to gen-
erate predictions using a neural network architecture.
Further, this paper presents a novel framework for us-
ing physics-based loss functions in the learning objective
of neural networks, to ensure that the model predic-
tions not only show lower errors on the training set but
are also scientifically consistent with the known physics
on the unlabeled set. We illustrate the effectiveness of
PGNN for the problem of lake temperature modeling,
where physical relationships between the temperature,
density, and depth of water are used to design a physics-
based loss function. By using scientific knowledge to
guide the construction and learning of neural networks,
we are able to show that the proposed framework en-
sures better generalizability as well as scientific consis-
tency of results.},
  comment  = {NN model of lake temp with physical model derived input features and a simple penalty term for physical inconsistency.  Works better than plain NN or plain physical model.

Could be useful for forecasting w/ mixed physical/NN models e.g. the PV model here: Marquez13solarFrcstSatGrndNN

Authors have other related papers too.

Was covered here: https://towardsdatascience.com/physics-guided-neural-networks-pgnns-8fe9dbad9414},
  file     = {Paper V2 (2018):Karpatne17physNNlakeT.pdf:PDF},
  url      = {https://arxiv.org/abs/1710.11431},
}

@InProceedings{Min08progCngstFrcstShrt,
  author    = {Liang Min and S. T. Lee and Pei Zhang and V. Rose and J. Cole},
  title     = {Short-term probabilistic transmission congestion forecasting},
  booktitle = {Proc. Third Int. Conf. Electric Utility Deregulation and Restructuring and Power Technologies},
  year      = {2008},
  pages     = {764--770},
  month     = apr,
  abstract  = {This paper introduces a probabilistic method for short-term transmission congestion forecasting, which is recently developed by EPRI. The proposed method applies the sequential Monte Carlo simulation (MCS) in a probabilistic load flow as the conceptual framework, adds all the significant uncertainties and their probability distributions to be modeled, develops the models, and most importantly specifies how to accurately model the key input assumptions in order to derive valid confidence levels of the forecasted congestion variables. The developed probabilistic method is successfully applied to the four-area WECC equivalent system. Focus is on the confidence levels of making such forecasts, so that a window of forecast-ability is defined, beyond which any forecast would be considered to contain little actionable information. Within the window of forecast-ability, the probabilistic forecasts of congestion would provide confidence limits and information for ranking the potential benefits of alleviating congestion at the various transmission bottlenecks.},
  doi       = {10.1109/DRPT.2008.4523508},
  file      = {:Min08progCngstFrcstShrt.pdf:PDF},
  keywords  = {Monte Carlo methods, load forecasting, power transmission planning, confidence levels, forecasted congestion variables, probabilistic load flow, probabilistic method, sequential Monte Carlo simulation, short-term transmission congestion forecasting, Artificial neural networks, Costs, Economic forecasting, Load flow, Load forecasting, Power system simulation, Predictive models, Probability distribution, Reliability, Uncertainty, Congestion forecasting, Monte Carlo Simulation (MCS), probabilistic load flow, probability distribution},
}

@Article{Muggeo07bivarDistLagSpline,
  author    = {Vito M. R. Muggeo},
  title     = {Bivariate distributed lag models for the analysis of temperature-by-pollutant interaction effect on mortality},
  journal   = {Environmetrics},
  year      = {2007},
  volume    = {18},
  number    = {3},
  pages     = {231--243},
  abstract  = {This paper introduces Bivariate Distributed Lags Models (BDLMs) to investigate synergic effect of temperature and airborne particles on mortality. These models seem particulary attractive since they allow to model interactions between such environmental variables accounting for possible delayed effects. A B-spline framework is used to approximate the coefficient surface of the temperature-by-pollutant interaction and possible alternatives are also discussed. A case study of mortality time-series data in Palermo, Italy, is presented to illustrate the model.},
  comment   = {2-D spline captures time delayed PRODUCT interactions w/ few params and linear regression Spline delayed lags Results: better residuals; intuitively matches biological knowledge Is Gasparrini10distLagNonLin better, non-product?},
  doi       = {10.1002/env.829},
  file      = {Muggeo07bivarDistLagSpline.pdf:Muggeo07bivarDistLagSpline.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.08.05},
}

@Article{Liu05featSelInteg,
  author    = {Huan Liu and Lei Yu},
  title     = {Toward integrating feature selection algorithms for classification and clustering},
  year      = {2005},
  volume    = {17},
  number    = {4},
  month     = apr,
  pages     = {491--502},
  issn      = {1041-4347},
  doi       = {10.1109/TKDE.2005.66},
  abstract  = {This paper introduces concepts and algorithms of feature selection, surveys existing feature selection algorithms for classification and clustering, groups and compares different algorithms with a categorizing framework based on search strategies, evaluation criteria, and data mining tasks, reveals unattempted combinations, and provides guidelines in selecting feature selection algorithms. With the categorizing framework, we continue our efforts toward-building an integrated system for intelligent feature selection. A unifying platform is proposed as an intermediate step. An illustrative example is presented to show how existing feature selection algorithms can be integrated into a meta algorithm that can take advantage of individual algorithms. An added advantage of doing so is to help a user employ a suitable algorithm without knowing details of each algorithm. Some real-world applications are included to demonstrate the use of feature selection in data mining. We conclude this work by identifying trends and challenges of feature selection research and development.},
  file      = {Liu05featSelInteg.pdf:Liu05featSelInteg.pdf:PDF;Liu05featSelInteg.pdf:Liu05featSelInteg.pdf:PDF},
  journal   = {Knowledge and Data Engineering, IEEE Transactions on},
  keywords  = { data mining, feature extraction, meta data, pattern classification, pattern clustering, very large databases categorizing framework, data classification, data clustering, data mining, feature selection, real-world applications, search strategies, unifying platform},
  owner     = {sotterson},
  timestamp = {2009.01.30},
}

@Article{Mohandes2003svmWind,
  author    = {M. A. Mohandes and T. O. Halawani and S. Rehman and Ahmed A. Hussain},
  title     = {Support vector machines for wind speed prediction},
  year      = {2003},
  volume    = {29},
  number    = {6},
  pages     = {939--947},
  doi       = {10.1016/j.physletb.2003.10.071},
  url       = {http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6V4S-4BGHGD3-1&_user=10&_coverDate=05%2F31%2F2004&_rdoc=1&_fmt=high&_orig=browse&_sort=d&view=c&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=de45be034f7fba31de4fc691880a7026},
  abstract  = {This paper introduces support vector machines (SVM), the latest neural network algorithm, to wind speed prediction and compares their performance with the multilayer perceptron (MLP) neural networks. Mean daily wind speed data from Madina city, Saudi Arabia, is used for building and testing both models. Results indicate that SVM compare favorably with the MLP model based on the root mean square errors between the actual and the predicted data. These results are confirmed for a system with order 1 to system with order 11.},
  file      = {Mohandes2003svmWind.pdf:Mohandes2003svmWind.pdf:PDF;Mohandes2003svmWind.pdf:Mohandes2003svmWind.pdf:PDF},
  journal   = {Renewable Energy},
  owner     = {sotterson},
  timestamp = {2008.07.03},
}

@Article{Strehl03clustEnsCombine,
  author    = {Strehl, Alexander and Ghosh, Joydeep},
  title     = {Cluster ensembles---a knowledge reuse framework for combining multiple partitions},
  journal   = {The Journal of Machine Learning Research},
  year      = {2003},
  volume    = {3},
  pages     = {583--617},
  abstract  = {This paper introduces the problem of combining multiple partitionings of a set of objects into a single consolidated clustering without accessing the features or algorithms that determined these partitionings. We first identify several application scenarios for the resultant 'knowledge reuse' framework that we call cluster ensembles. The cluster ensemble problem is then formalized as a combinatorial optimization problem in terms of shared mutual information. In addition to a direct maximization approach, we propose three effective and efficient techniques for obtaining high-quality combiners (consensus functions). The first combiner induces a similarity measure from the partitionings and then reclusters the objects. The second combiner is based on hypergraph partitioning. The third one collapses groups of clusters into meta-clusters which then compete for each object to determine the combined clustering. Due to the low computational costs of our techniques, it is quite feasible to use a supra-consensus function that evaluates all three approaches against the objective function and picks the best solution for a given situation. We evaluate the effectiveness of cluster ensembles in three qualitatively different application scenarios: (i) where the original clusters were formed based on non-identical sets of features, (ii) where the original clustering algorithms worked on non-identical sets of objects, and (iii) where a common data-set is used and the main purpose of combining multiple clusterings is to improve the quality and robustness of the solution. Promising results are obtained in all three situations for synthetic as well as real data-sets.

Keywords: cluster analysis, clustering, partitioning, unsupervised learning, multi-learner
systems, ensemble, mutual information, consensus functions, knowledge reuse},
  comment   = {Uses mutual information criteria to combine clustering results from separate clustering runs (different algorithms or initializations). I think it may even work when the clustering results don't have 100% the same objects (see abstract)? Has Matlab.

I used it on Eweline to combine multiple kmeans runs when clustering PV power ensembles (using spectral clustering: Ng01specClust). I think this gave me a repeatable, more robust result.

Paper says 2002 but journal web page says it was published in 2003.

Algorithm is included in the Matlab ClusterPack V2.0, available here: http://strehl.com/soft.html},
  file      = {Strehl03clustEnsCombine.pdf:Strehl03clustEnsCombine.pdf:PDF},
  owner     = {sotterson},
  publisher = {JMLR. org},
  timestamp = {2014.11.25},
  url       = {http://dl.acm.org/citation.cfm?id=944935},
}

@Article{Lamarche10MultiQRlambda,
  author    = {Lamarche, Carlos},
  title     = {Robust penalized quantile regression estimation for panel data},
  journal   = {Journal of Econometrics},
  year      = {2010},
  volume    = {157},
  number    = {2},
  pages     = {396--408},
  abstract  = {This paper investigates a class of penalized quantile regression estimators for panel data. The penalty
serves to shrink a vector of individual specific effects toward a common value. The degree of this shrinkage
is controlled by a tuning parameter ? . It is shown that the class of estimators is asymptotically unbiased
and Gaussian, when the individual effects are drawn from a class of zero-median distribution functions.
The tuning parameter, ? , can thus be selected to minimize estimated asymptotic variance. Monte Carlo
evidence reveals that the estimator can significantly reduce the variability of the fixed-effect version of
the estimator without introducing bias.},
  comment   = {Picking L-1 sparsity penalty lambda for multivariate QR. Works by finding an asymptotically unbiased estimator for the regression coefficients, and then minimizing that for the asymptotic variance to get lambda. Very interesting paper, but assumed model is not quite right for summed wind power plants or turbines.

Assumed model for quantiles:
* same linear regression coeffs for all individuals eg. same wind speed dependence
 - this could be expressed by a power curve which is a sum spline bases
 - but it would have to be the same expansion for all farms
* assumes that the difference between individuals is only a different bias coeff (one would want gain (normalized capacity) instead).
* ALSO, the only sparsity constraint is on that useless bias coeff.

I thought about transforming this into a log power prediction problem (exponential link function for generalized linear regression) but this has the sums in the wrong place for this.

Probably need a varying coeff model of some kind for this to work, but I don't know what that model is either...},
  file      = {Lamarche10MultiQRlambda.pdf:Lamarche10MultiQRlambda.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.11.02},
  url       = {http://www.sciencedirect.com/science/article/pii/S0304407610000850},
}

@Article{Faria03countMdlTechAdopt,
  author    = {Faria, Ana and Fenn, Paul and Bruce, Alistair},
  title     = {A count data model of technology adoption},
  journal   = {The Journal of Technology Transfer},
  year      = {2003},
  volume    = {28},
  number    = {1},
  pages     = {63--79},
  abstract  = {This paper investigates the main determinants 
of the adoption of flexible production technologies (FPTs), 
using a plant-level dataset of Portuguese manufacturing indus- 
try. Besides using a new dataset, this paper extends the frame- 
work of previous studies on technology adoption by taking 
into account the effect of demand uncertainty as an additional 
determinant of adoption. In order to examine this relation- 
ship, several econometric models for count data are estimated. 
These models deal with the discrete nature of the dependent 
variable and firm specific unobservable characteristics arising 
from the cross-section context. The main findings of the paper 
are: (i) rank effects and technological regimes are important 
determinants of technology adoption, as put forward in pre- 
vious models of technology diffusion; (ii) demand uncertainty 
has a significant positive impact on the likelihood of adopt- 
ing FPTs, which suggests that technological heterogeneity is 
important when modelling firms’ investment decisions; (iii) the 
estimates were improved after controlling for excess zeros and 
overdispersion that characterizes our data.},
  comment   = {Has good table of various methods.  Like idea for MN Solar Pathways in that it's count based.},
  file      = {:Faria03countMdlTechAdopt.pdf:PDF},
  publisher = {Springer},
  url       = {https://www.researchgate.net/profile/Paul_Fenn/publication/5152719_A_Count_Data_Model_of_Technology_Adoption/links/0deec51d49f511a573000000/A-Count-Data-Model-of-Technology-Adoption.pdf},
}

@InProceedings{Gronwald10carbonCopula,
  author       = {Marc Gronwald and Janina Ketterer and Stefan Tr\"{u}ck},
  title        = {The Dependence Structure Between Carbon Emission Allowances And Financial Markets - A Copula Analysis},
  booktitle    = {Global Finance Conference},
  year         = {2010},
  month        = jun,
  organization = {Global Finance Association},
  abstract     = {This paper investigates the relationship between CO2 emission allowance prices and those of various other financial variables and commodities. Different copulas are applied in order to model the complex dependence structure between the return series of carbon emission allowance prices and those of various commodities as well as other financial series. As suggested in the literature, the use of correlation as the only measure of dependence can lead to an underestimation of the risk of joint extreme price movements. What is more, copula models represent a more flexible method for deriving the nature of dependence and provide an appropriate fit also for the tails of multivariate distributions. The findings suggest that the relationship between EU emission allowance (EUA) future returns and those of the other commodities - in particular gas and oil markets - is relatively weak. However, we find some dependence between EUA futures and equity or energy index returns. These results at least somehow contradict earlier studies that report no statistically significant or even negative correlations between returns of emission allowances and other financial variables. Regarding the nature of dependence, we also find some evidence of weak symmetric tail dependence for most of the considered series. Key words: CO2 Emission Trading, Commodity Markets, Copula Models, Dependence Structure},
  comment      = {spinning reserves, a good overview of copulas that explains how to use them on a real (bivariate) problem},
  file         = {Gronwald10carbonCopula.pdf:Gronwald10carbonCopula.pdf:PDF},
  location     = {Poznan, Poland},
  owner        = {scot},
  timestamp    = {2010.12.06},
  url          = {http://www.glofin.org/2010GFC/gfc2010papers.htm},
}

@Article{Liu14FeatSelVaryCoeffUHdim,
  author    = {Liu, Jingyuan and Li, Runze and Wu, Rongling},
  title     = {Feature Selection for Varying Coefficient Models With Ultrahigh-Dimensional Covariates},
  journal   = {Journal of the American Statistical Association},
  year      = {2014},
  volume    = {109},
  number    = {505},
  pages     = {266--274},
  abstract  = {This paper is concerned with feature screening and variable selection for varying
coefficient models with ultrahigh dimensional covariates. We propose a new feature
screening procedure for these models based on conditional correlation coefficient. We
systematically study the theoretical properties of the proposed procedure, and establish
their sure screening property and the ranking consistency. To enhance the finite
sample performance of the proposed procedure, we further develop an iterative feature
screening procedure. Monte Carlo simulation studies were conducted to examine the
performance of the proposed procedures. In practice, we advocate a two-stage approach
for varying coefficient models. The two stage approach consists of (a) reducing
the ultrahigh dimensionality by using the proposed procedure and (b) applying regularization
methods for dimension-reduced varying coefficient models to make statistical
inferences on the coefficient functions. We illustrate the proposed two-stage approach
by a real data example.
Key Word: Feature selection, varying coefficient models, ranking consistency, sure screening
property.},
  comment   = {I suppose this is how to select what subsets of inputs should go into a varying coefficient model. Attached is the 2013 tech report, not the journal article.},
  doi       = {10.1080/01621459.2013.850086},
  file      = {Liu14FeatSelVaryCoeffUHdim_techRep.pdf:Liu14FeatSelVaryCoeffUHdim_techRep.pdf:PDF},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2014.11.10},
}

@INPROCEEDINGS{Wan00unscentedKalman,
  Author                   = {Wan, E.A. and Van Der Merwe, R.},
  Title                    = {The unscented {Kalman} filter for nonlinear estimation},
  Booktitle                = {Adaptive Systems for Signal Processing, Communications, and Control (AS-SPCC)},
  Year                     = {2000},
  Pages                    = {153--158},
  Abstract                 = {This paper points out the flaws in using the extended Kalman filter (EKE) and introduces an improvement, the unscented Kalman filter (UKF), proposed by Julier and Uhlman (1997). A central and vital operation performed in the Kalman filter is the propagation of a Gaussian random variable (GRV) through the system dynamics. In the EKF the state distribution is approximated by a GRV, which is then propagated analytically through the first-order linearization of the nonlinear system. This can introduce large errors in the true posterior mean and covariance of the transformed GRV, which may lead to sub-optimal performance and sometimes divergence of the filter. The UKF addresses this problem by using a deterministic sampling approach. The state distribution is again approximated by a GRV, but is now represented using a minimal set of carefully chosen sample points. These sample points completely capture the true mean and covariance of the GRV, and when propagated through the true nonlinear system, captures the posterior mean and covariance accurately to the 3\textsuperscript{rd} order (Taylor series expansion) for any nonlinearity. The EKF in contrast, only achieves first-order accuracy. Remarkably, the computational complexity of the UKF is the same order as that of the EKF. Julier and Uhlman demonstrated the substantial performance gains of the UKF in the context of state-estimation for nonlinear control. Machine learning problems were not considered. We extend the use of the UKF to a broader class of nonlinear estimation problems, including nonlinear system identification, training of neural networks, and dual estimation problems. In this paper, the algorithms are further developed and illustrated with a number of additional examples},
  DOI                      = {10.1109/ASSPCC.2000.882463},
  File                     = {Wan00unscentedKalman.pdf:Wan00unscentedKalman.pdf:PDF;Wan00unscentedKalman.pdf:Wan00unscentedKalman.pdf:PDF},
  Keywords                 = {adaptive Kalman filters, computational complexity, covariance analysis, filtering theory, learning (artificial intelligence), nonlinear estimation, nonlinear systems, parameter estimation, state estimationGaussian random variable, computational complexity, covariance, deterministic sampling approach, dual estimation problems, extended Kalman filter, machine learning, neural networks training, nonlinear control, nonlinear estimation, nonlinear system, nonlinear system identification, posterior mean, sample points, state distribution, state-estimation, system dynamics, unscented Kalman filter},
  Owner                    = {sotterson},
  Timestamp                = {2008.09.26},
  URL                      = {http://ieeexplore.ieee.org.offcampus.lib.washington.edu/search/srchabstract.jsp?arnumber=882463&isnumber=19085&punumber=7076&k2dockey=882463@ieeecnfs&query=((the+unscented+kalman+filter+for+nonlinear+)%3Cin%3Emetadata)&pos=3&access=no}
}

@InProceedings{Singh10correntropyFilt,
  author    = {Singh, A. and Princ\'{i}pe, J.C.},
  title     = {A closed form recursive solution for Maximum Correntropy training},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2010},
  pages     = {2070--2073},
  abstract  = {This paper presents a closed form recursive solution for training adaptive filters using the Maximum Correntropy Criterion (MCC). Correntropy has been recently proposed as a robust similarity measure between two random variables or signals, when the pdfs involved are heavy tailed and non-Gaussian. Maximizing the cross-correntropy between the output of an adaptive filter and the desired response leads to the Maximum Correntropy Criterion for adaptive systems training. We show that a closed form, recursive solution of the filter weights using this criterion yields a simple weighted least squares like formulation. Our simulations show that training the filter weights using this recursive solution is much faster than gradient based training, and more accurate than the RLS algorithm in cases where the error pdf is non-Gaussian and heavy tailed.},
  comment   = {correntropy dependence meas. improves speed/accuracy of recursive, adaptive linear filter w/ non-Gaussian data relative to spinnng reserves, as a scenario dependence measure?},
  doi       = {10.1109/ICASSP.2010.5495055},
  file      = {Singh10correntropyFilt.pdf:Singh10correntropyFilt.pdf:PDF},
  issn      = {1520-6149},
  keywords  = {adaptive filters, entropy, least mean squares methods, recursive estimation, RLS algorithm, adaptive filters, adaptive systems training, closed form recursive solution, cross-correntropy, filter weights, gradient based training, maximum correntropy criterion, maximum correntropy training, robust similarity measure, simple weighted least squares like formulation},
  owner     = {scot},
  timestamp = {2010.12.02},
}

@Article{Ramirez-Rosado09comp2windFrcst,
  author    = {Ignacio J. Ramirez-Rosado and L. Alfredo Fernandez-Jimenez and Cl{\'a}udio Monteiro and Jo{\~a}o Sousa and Ricardo Bessa},
  title     = {Comparison of two new short-term wind-power forecasting systems},
  journal   = {Renewable Energy},
  year      = {2009},
  volume    = {34},
  number    = {7},
  pages     = {1848--1854},
  issn      = {0960-1481},
  abstract  = {This paper presents a comparison of two new advanced statistical short-term wind-power forecasting systems developed by two independent research teams. The input variables used in both systems were the same: forecasted meteorological variable values obtained from a numerical weather prediction model; and electric power-generation registers from the SCADA system of the wind farm. Both systems are described in detail and the forecasting results compared, revealing great similarities, although the proposed structures of the two systems are different. The forecast horizon for both systems is ?, allowing the use of the forecasted values in electric market operations, as diary and intra-diary power generation bid offers, and in wind-farm maintenance planning.},
  comment   = {A reference for a power as an autoregressive input to an NN wind power forecasting system. Would have used one for Fraunhofer IWES WPMS system, but couldn't find a good one.},
  doi       = {10.1016/j.renene.2008.11.014},
  file      = {:Ramirez-Rosado09comp2windFrcst.pdf:PDF},
  keywords  = {Forecasting},
  owner     = {sotterson},
  timestamp = {2012.09.24},
  url       = {http://www.sciencedirect.com/science/article/pii/S0960148108004321},
}

@Article{Hagemann15priceDetDEintraday,
  author    = {Hagemann, Simon},
  title     = {Price determinants in the German intraday market for electricity: an empirical analysis},
  journal   = {The Journal of Energy Markets},
  year      = {2015},
  volume    = {8},
  number    = {2},
  pages     = {21-45},
  month     = {Jun},
  issn      = {1756-3607},
  abstract  = {This paper presents a first investigation of hourly price determinants in the German intraday market
for electricity. The influence of power plant outages, forecast errors of wind and solar power produc-
tion, load forecast errors and foreign demand and supply on intraday prices are explained from a
theoretical perspective. Furthermore the influences of the non-linear merit-order shape, ramping costs
and strategic market behavior are discussed. The empirical results from different regression analysis
with data from 2010 and 2011 show that most price determinants increase and decrease intraday
prices as expected. Nevertheless, only a minor share of power plant outages and solar power forecast
errors are traded on the electronic intraday trading platform, thus influencing prices not as strongly
as expected. Furthermore the price determinants influence intraday prices differently over the course
of the day which may be explained by an alternating liquidity provision.



Keywords: Intraday market for electricity, price modeling, price determinants.

This was the 2013 working paper abstract},
  comment   = {Use for price forecasting feature selection?

2013 working paper is from:},
  doi       = {10.21314/jem.2015.128},
  file      = {2013 Working Paper:Hagemann13priceDetDEintraday.pdf:PDF},
  owner     = {sotterson},
  publisher = {Incisive Media},
  timestamp = {2016.11.28},
  url       = {http://dx.doi.org/10.21314/jem.2015.128},
}

@Article{Batlle04scenarioMultiGarch,
  author    = {Batlle, Carlos and Barquin, Julian},
  title     = {Fuel prices scenario generation based on a multivariate GARCH model for risk analysis in a wholesale electricity market},
  journal   = {International Journal of Electrical Power \& Energy Systems},
  year      = {2004},
  volume    = {26},
  number    = {4},
  pages     = {273--280},
  month     = may,
  issn      = {0142-0615},
  abstract  = {This paper presents a fuel prices scenario generator in the frame of a simulation tool developed to support risk analysis in a competitive electricity environment. The tool feeds different exogenous risk factors to a wholesale electricity market model to perform a statistical analysis of the results. As the different fuel series that are studied, such as the oil or gas ones, present stochastic volatility and strong correlation among them, a multivariate Generalized Autoregressive Conditional Heteroskedastic model has been designed in order to allow the generation of future fuel prices paths. The model makes use of a decomposition method to simplify the consideration of the multidimensional conditional covariance. An example of its application with real data is also presented.},
  comment   = {spinning reserves},
  file      = {Batlle04scenarioMultiGarch.pdf:Batlle04scenarioMultiGarch.pdf:PDF},
  keywords  = {Fuels, Monte Carlo methods, Power system modeling, Risk analysis, Stochastic processes},
  owner     = {scot},
  timestamp = {2010.11.26},
  url       = {http://www.sciencedirect.com/science/article/B6V2T-4B4HBF9-3/2/9cfb6fef464883451623ee2e6684cfc8},
}

@InProceedings{Martin18germanMktLOB,
  author    = {Henry Martin and Scott Otterson},
  title     = {German Intraday Electricity Market Analysis and Modeling based on the Limit Order Book},
  booktitle = {International Conf. on the European Energy Market.},
  year      = {2018},
  address   = {Łódź, PL.},
  abstract  = {This paper presents a market model for the
SPOT German continuous intraday market for electric
trading based on the limit order book (LOB). We use
EPEX SPOT M7 order book data, which contains all
submitted to the German continuous intraday market, to
the historic course of the market. Thereby, we reconstruct
complete state of the LOB at every point in (trading
We validate our simulation by comparing the transactions
our simulation generated with the actual historical transactions
available from a different data set. The LOB based market
can be used to include price volatility risk and illiquidity
when simulating trading at the EPEX SPOT continuous
market. Furthermore, we present all preprocessing steps
decision rules necessary to correctly identify orders from
often ambiguous EPEX SPOT M7 order book data.},
  comment   = {Conf. paper on Henry's Master's thesis},
  file      = {:Martin18germanMktLOB.pdf:PDF},
}

@InProceedings{Sorjamaa05mutInfKNNpred,
  author    = {Antti Sorjamaa and Jin Hao and Amaury Lendasse},
  title     = {Mutual Information and k-Nearest Neighbors Approximator for Time Series Prediction},
  booktitle = {International Conference on Artificial Neural Networks (ICANN)},
  year      = {2005},
  pages     = {553--558},
  abstract  = {This paper presents a method that combines Mutual Information and k-Nearest Neighbors approximator for time series prediction. Mutual Information is used for input selection. K-Nearest Neighbors approximator is used to improve the input selection and to provide a simple but accurate prediction method. Due to its simplicity the method is repeated to build a large number of models that are used for long-term prediction of time series. The Santa Fe A time series is used as an example. Keywords: Time Series, Input Selection, Mutual Information, k-NN.},
  comment   = {Use of KNN mut info to select forecasting regression variables. Not a great paper, but maybe MI calc is useful?
* regressor is straight-forward KNN, trained w/ Direct method (target is at lookahead time)
* also use KNN to estimate mutual information Feature selection
* use only one-step-ahead problem to compute inputs for all step-aheads
 -- seems likely to fail! * brute force calc of all 2^N-1 input combinations
 -- times 1:10 num. of neighbors in the KNN mutual information estimator
 -- this IS the curse of dimensionality. It doesn't avoid it!
 -- does not explain "MI of what against what" but it could be every input combination against the desired output?
 -- anyway highest "MI" is used to select the inputs (at num. of neighbors that gives highest MI)
---- THIS SEEMS WRONG: Papana08EvalMutInfoDynSys says that MI increases w/ decreasing k Regression
* input selection is same for all lookaheads (seems wrong)
* do a massive leave-one-out test over all samples to select num. of neighbors for regressor KNN
 -- error criteria is MSE Results * works great when test on train
* not so great when have separate test
* MI calc is implemented in Matlab at: http://www.klab.caltech.edu/~kraskov/MILCA/
-- same MI method as in: Kraskov04EstMutInfKNN, apparently (but does it use Kraskov04EstMutInfKNN formula A3 or the other one used in the Kraskov04EstMutInfKNN tests?)
* Also a subset of MI funcs also parallelly implemented in R in the parmigene package:
 -- http://www.inside-r.org/packages/cran/parmigene/docs
* improvements, something like Peng minRedundMaxRel in Herrera06effectInputSelFuncApprox

Related: Kraskov04EstMutInfKNN,Herrera06effectInputSelFuncApprox,Papana08EvalMutInfoDynSys
},
  doi       = {10.1007/11550907},
  file      = {Sorjamaa05mutInfKNNpred.pdf:Sorjamaa05mutInfKNNpred.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2008.10.10},
  url       = {http://www.springerlink.com.offcampus.lib.washington.edu/content/9nc3mhv3pgan6mv6/?p=f2cdd181b9fb4e66b5093824e14e58a4&pi=0},
}

@Article{Taylor00quantRgrsnNNmultiPer,
  author    = {Taylor, James W},
  title     = {A quantile regression neural network approach to estimating the conditional density of multiperiod returns},
  journal   = {Journal of Forecasting},
  year      = {2000},
  volume    = {19},
  number    = {4},
  pages     = {299--311},
  abstract  = {This paper presents a new approach to estimating the conditional probability distribution of
multiperiod financial returns. Estimation of the tails of the distribution is particularly important
for risk management tools, such as value at risk models. A popular approach is to assume a
Gaussian distribution, and to use a theoretically derived variance expression which is a nonlinear
function of the holding period, k, and the 1-step-ahead volatility forecast, 1 ? + t sigma . The new method
avoids the need for a distributional assumption by applying quantile regression to the historical
returns from a range of different holding periods to produce quantile models which are functions
of k and 1 ? + t sigma . A neural network is used to estimate the potentially nonlinear quantile models.
Using daily exchange rates, the approach is compared to GARCH-based quantile estimates. The
results suggest that the new method offers a useful alternative for estimating the conditional
density.
Key words: quantile regression; neural networks; multiperiod returns; conditional density},
  comment   = {A neural network QR alg. that handles time dependence. Could be good for scenarios.

Training method is criticized in Cannon11quantRgrsnNNprecip},
  file      = {Taylor00quantRgrsnNNmultiPer.pdf:Taylor00quantRgrsnNNmultiPer.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.22},
}

@Article{Comin06techDiffuse5facts,
  author   = {Diego {Comin} and Bart {Hobijn} and Emilie {Rovito}},
  title    = {Five Facts You Need to Know About Technology Diffusion},
  journal  = {National Bureau of Economic Research},
  year     = {2006},
  abstract = {This paper presents a new data set on the diffusion of about 115 technologies in over 150 countries
over the last 200 years. We use this comprehensive data set to uncover general patterns of technology
diffusion. Our main 5 findings are as follows: (i) Once the intensive margin is measured,
technologies do not diffuse in a logistic way. (ii) Within a typical technology, the dispersion in the
adoption levels across countries is about 5 times larger than the cross-country dispersion in income
per capita. (iii) The rankings of countries by level of technology adoption are very highly correlated
across technologies. (iv) Within a typical technology, there has been convergence at an average rate
of 4 percent per year. (v) The speed of convergence for technologies developed since 1925 has been
three times higher than the speed of convergence for technologies developed before 1925.},
  comment  = {Intro to big tech adoption database, also has some conclusions about the general tech adopt shapes.  Interesting bits about how they are normalized, and isoloated from confounders like GDP.  Also that "intensive" technologies don't follow a Bass-like S-shaped curve.

But this paper is wordy yet confusing, and has the graphs annoyingly at the end.  I stopped reading it and moved on to Comin14techDiffusMeasCausConseq, which will have the latest stuff, is hopefully more fully developed, and since it's equally lengthy, I bet has much of the same info as is in this paper.},
  file     = {:Comin06techDiffuse5facts.pdf:PDF},
  groups   = {Scott:6},
  url      = {https://www.nber.org/papers/w11928},
}

@Article{Kaut14copulaMultiVarHeurist,
  author    = {Kaut, Michal},
  title     = {A copula-based heuristic for scenario generation},
  journal   = {Computational Management Science},
  year      = {2014},
  volume    = {11},
  number    = {4},
  pages     = {503--516},
  abstract  = {This paper presents a new heuristic for generating scenarios for two-stage
stochastic programs. The method uses copulas to describe the dependence be-
tween the marginal distributions, instead of the more common correlations. The
heuristic is then tested on a simple portfolio-selection model, and compared to
two other scenario-generation methods.

Keywords: stochastic programming; scenario generation; copulas},
  comment   = {Use for ReWP portfolio optimzation?  This one is tested for optimizing conditional value-at-risk (CVaR) as a risk measure (not prob of any value being under the promised regulation) but it's mulitvariate, etc.},
  file      = {Kaut14copulaMultiVarHeurist.pdf:Kaut14copulaMultiVarHeurist.pdf:PDF},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2016.12.09},
  url       = {http://link.springer.com/article/10.1007/s10287-013-0184-4},
}

@Article{Tsiligkaridis13covEstHiDimKrPrdExp,
  author    = {T. Tsiligkaridis and A. O. Hero},
  title     = {Covariance Estimation in High Dimensions Via Kronecker Product Expansions},
  journal   = IEEE_J_SP,
  year      = {2013},
  volume    = {61},
  number    = {21},
  pages     = {5347--5360},
  month     = nov,
  issn      = {1053-587X},
  abstract  = {This paper presents a new method for estimating high dimensional covariance matrices. The method, permuted rank-penalized least-squares (PRLS), is based on a Kronecker product series expansion of the true covariance matrix. Assuming an i.i.d. Gaussian random sample, we establish high dimensional rates of convergence to the true covariance as both the number of samples and the number of variables go to infinity. For covariance matrices of low separation rank, our results establish that PRLS has significantly faster convergence than the standard sample covariance matrix (SCM) estimator. The convergence rate captures a fundamental tradeoff between estimation error and approximation error, thus providing a scalable covariance estimation framework in terms of separation rank, similar to low rank approximation of covariance matrices . The MSE convergence rates generalize the high dimensional rates recently obtained for the ML Flip-flop algorithm , for Kronecker product covariance estimation. We show that a class of block Toeplitz covariance matrices is approximatable by low separation rank and give bounds on the minimal separation rank r that ensures a given level of bias. Simulations are presented to validate the theoretical bounds. As a real world application, we illustrate the utility of the proposed Kronecker covariance estimator for spatio-temporal linear least squares prediction of multivariate wind speed measurements.},
  comment   = {Wind speed temporal-spatio covariance estimation for day ahead forecasting, and other things.},
  doi       = {10.1109/TSP.2013.2279355},
  file      = {:papers\\Tsiligkaridis13covEstHiDimKrPrdExp.pdf:PDF},
  keywords  = {Gaussian processes, convergence, covariance matrices, estimation theory, mean square error methods, random processes, Gaussian random sample, Kronecker product covariance estimation, Kronecker product series expansion, ML flip-flop algorithm, MSE convergence rates, PRLS, approximation error, block Toeplitz covariance matrices, estimation error, low separation rank, multivariate wind speed measurement, permuted rank-penalized least-squares, spatio-temporal linear least squares prediction, Brain modeling, Convergence, Covariance matrices, Estimation, Least squares approximations, Standards, Symmetric matrices, Kronecker product decompositions, Structured covariance estimation, high dimensional convergence rates, mean-square error, multivariate prediction, penalized least squares},
  owner     = {sotterson},
  timestamp = {2017.07.03},
}

@Article{Carrion06effUnitCommMixInt,
  author    = {Carrion, M. and Arroyo, J.M.},
  title     = {A computationally efficient mixed-integer linear formulation for the thermal unit commitment problem},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2006},
  volume    = {21},
  number    = {3},
  pages     = {1371--1378},
  month     = aug,
  issn      = {0885-8950},
  abstract  = {This paper presents a new mixed-integer linear formulation for the unit commitment problem of thermal units. The formulation proposed requires fewer binary variables and constraints than previously reported models, yielding a significant computational saving. Furthermore, the modeling framework provided by the new formulation allows including a precise description of time-dependent startup costs and intertemporal constraints such as ramping limits and minimum up and down times. A commercially available mixed-integer linear programming algorithm has been applied to efficiently solve the unit commitment problem for practical large-scale cases. Simulation results back these conclusions},
  comment   = {The efficient mixed integer paper that Patrick H sent to me. 2011},
  doi       = {10.1109/TPWRS.2006.876672},
  file      = {Carrion06effUnitCommMixInt.pdf:Carrion06effUnitCommMixInt.pdf:PDF},
  keywords  = {binary variables;intertemporal constraints;large-scale cases;mixed-integer linear programming;ramping limits;thermal unit commitment problem;time-dependent startup costs;integer programming;linear programming;power generation dispatch;power generation scheduling;power system interconnection;thermal power stations;},
  owner     = {sotterson},
  timestamp = {2011.10.19},
}

@Article{Yang09featSelMLPrand,
  author    = {Jian-Bo Yang and Kai-Quan Shen and Chong-Jin Ong and Xiao-Ping Li},
  title     = {Feature Selection for {ML}P Neural Network: The Use of Random Permutation of Probabilistic Outputs},
  journal   = {Neural Networks, IEEE Transactions on},
  year      = {2009},
  volume    = {20},
  number    = {12},
  pages     = {1911--1922},
  month     = dec,
  issn      = {1045-9227},
  abstract  = {This paper presents a new wrapper-based feature selection method for multilayer perceptron (MLP) neural networks. It uses a feature ranking criterion to measure the importance of a feature by computing the aggregate difference, over the feature space, of the probabilistic outputs of the MLP with and without the feature. Thus, a score of importance with respect to every feature can be provided using this criterion. Based on the numerical experiments on several artificial and real-world data sets, the proposed method performs, in general, better than several selected feature selection methods for MLP, particularly when the data set is sparse or has many redundant features. In addition, as a wrapper-based approach, the computational cost for the proposed method is modest.},
  comment   = {MLP wrapper featsel via input perumations. Low comp. cost, especially good when little train dat, but good for high dim? Classification (here) could be extended to continous point, density, or arbitrary cost function forecasts (I think).

* train NN on full input set, then permute the inputs one a time to see if they mattered (don't need to train a new NN for each input test)
* result is a ranking of inputs by importance
-- rank comes from the summed abs diff between the NN posterior probabilities w/ and w/o the feature
-- posterior prob is from the softmax function (in classification, this is a class posterior prob.)
* can remove the least important and then try again.
* but this means you have to train a high dim NN if you start out with a large number of candidate inputs.
* says reason for better performance is that NN posterior probabilty is a better perf. meas than is input/output MI (Peng)
* NN is trained using entropy minimization

Performance
* mostly, it's better or no worse than alternative methods at any given number of inputs
* often beats Peng's mRMR featsel (peng05featSelMutInfo in speakerclust.bib)
-- especially on one test case with 100 candidate inputs
-- is about the same for one w/ 166 candidate inputs
-- I wonder how this method would compare w/ the one using the MI of candidates against the residual (e.g. Schaffernicht11wgtMutInfoFeatSel)
* alternatives are all filter methods e.g. Peng, (I think)
* seems to work best when there is little training data
* test cases are classification


IDEAS:

For continuous density forecast:
Use for probabilsitic forecastion: use same feature ranking criterion: difference between output distribution posterior probabilities w/ and w/o the feature. In this case, the posterior prob is of a single, continuous variable, the probability of a given amount of wind power or whatever. The stopping criterion could then be sharpness, CRPS, or whatever.

For continuous point forecast:
Maybe same idea as for density forecast, but use the distribution to predict the min RMSE or MAD or some other function? This would be a way of making a point forecast scaled to an arbitrary cost function.

See also: Bessa10infoWindPow},
  doi       = {10.1109/TNN.2009.2032543},
  file      = {Yang09featSelMLPrand.pdf:Yang09featSelMLPrand.pdf:PDF},
  groups    = {Ensemble},
  keywords  = {feature ranking criterion;multilayer perceptron neural network;probabilistic outputs random permutation;wrapper based feature selection method;feature extraction;multilayer perceptrons;probability;random processes;Algorithms;Computer Simulation;Database Management Systems;Decision Support Techniques;Humans;Neural Networks (Computer);Perception;Probability;},
  owner     = {scot},
  timestamp = {2011.04.13},
}

@Article{Lillywhite13featCnstrct,
  author    = {Kirt Lillywhite and Dah-Jye Lee and Beau Tippetts and James Archibald},
  title     = {A feature construction method for general object recognition},
  journal   = {Pattern Recognition},
  year      = {2013},
  volume    = {46},
  number    = {12},
  pages     = {3300--3314},
  issn      = {0031-3203},
  abstract  = {This paper presents a novel approach for object detection using a feature construction method called Evolution-COnstructed (ECO) features. Most other object recognition approaches rely on human experts to construct features. \{ECO\} features are automatically constructed by uniquely employing a standard genetic algorithm to discover series of transforms that are highly discriminative. Using \{ECO\} features provides several advantages over other object detection algorithms including: no need for a human expert to build feature sets or tune their parameters, ability to generate specialized feature sets for different objects, and no limitations to certain types of image sources. We show in our experiments that \{ECO\} features perform better or comparable with hand-crafted state-of-the-art object recognition algorithms. An analysis is given of \{ECO\} features which includes a visualization of \{ECO\} features and improvements made to the algorithm.},
  comment   = {Makes new features from old ones using weak regressor/adaboost and genetic algorithm. Maybe usable for regime detection, or aggregation? Seems that the transformation weighting could be CRPS for probabilistic forecast feature selection.

MAYBE ALL THESE IDEAS SHOULD BE MOVED TO ENERGYTOP.ORG, and only have stuff about the paper here?

Would need to do the continuous version of adaboost or adaboost compatible algs. e.g.
* AdaBoost.Rt: Solomatine04adaBoostContin
* AdaBoost.Rt: Shrestha06expAdaboostRT
* AdabBoost+: Kankanala14adaBoostPlus
* Committee Machine: Kenari13cmtteMchn
* EnsembleNN: Hassan13nnEnsbl
* Voting-Averaged: An10voteAvgEns
* AdaBoost.SVC.R: Gao13adaboost_SVC_R
* gp boosting: Souza09gpBoostFrcst
* gradient NN agg: Meng06gradientNNensAgg
* improved AdaBoost.Rt: Tian09newAdaboost_RT , Tian10elmAdaboost_RT
* correlation boosting: Souza10gpCorrBoost
* modular learning (not boosting, I think): Solomatine06modLrnFrcst
* adaboost vs. bag: Barutccuouglu03rgrsBagVsAdaBoost

If used an ensemble of probabilistic forecasts, would need some way fo combining the distributions. A couple hints:
* Lichtendahl13probOrQuantAvg
* Karvanen06quanMixLmomTrim

Note: there's a boosted quantile forecast in: Fenske11idRiskBoostAddQR

Note that there are a ton of adaboost ensemble pruning algorithms out there, if want to adapt or do streaming learning:
http://scholar.google.com/scholar?q=ensemble+pruning+adaboost+streaming&hl=en&as_sdt=0&as_vis=1&oi=scholart&sa=X&ei=wZreUon6HIeAtAbJgIGABA&ved=0CCgQgQMwAA},
  doi       = {10.1016/j.patcog.2013.06.002},
  file      = {Lillywhite13featCnstrct.pdf:Lillywhite13featCnstrct.pdf:PDF},
  keywords  = {Object detection},
  owner     = {sotterson},
  timestamp = {2014.01.21},
  url       = {http://www.sciencedirect.com/science/article/pii/S0031320313002549},
}

@Article{Bessa12adaptQuantCopulaFrcst,
  author    = {Ricardo J. Bessa and V. Miranda and A. Botterud and Z. Zhou and J. Wang},
  title     = {Time-adaptive quantile-copula for wind power probabilistic forecasting},
  journal   = {Renewable Energy},
  year      = {2012},
  volume    = {40},
  number    = {1},
  pages     = {29--39},
  issn      = {0960-1481},
  abstract  = {This paper presents a novel time-adaptive quantile-copula estimator for kernel density forecast and a discussion of how to select the adequate kernels for modeling the different variables of the problem. Results are presented for different case-studies and compared with splines quantile regression (QR). The datasets used are from NREL???s Eastern Wind Integration and Transmission Study, and from a real wind farm located in the Midwest region of the United States. The new probabilistic prediction model is elegant and simple and yet displays advantages over the traditional QR approach. Especially notable is the quality of the results achieved with the time-adaptive version, namely when evaluated in terms of prediction calibration, which is a characteristic that is advantageous for both system operators and wind power producers.},
  comment   = {Adaptive wind power density (not quantile) forecasts with power curve learning and copula derivatives.  Nearly the same results as splines (copula could be infinitesimally better).  Adaptation not yet convincing (to me).

* Problems with Nadaraya-Watson kernel smoother prob forecast (not the copula approach here)
   - unstable when kernel denominator term near zero
   - can only do kernel product for several explanatory variables
   - is a regression approach, not a statistically motivated density approach (like a copula, I guess)

BASIC COPULA IDEA
* directly forecast from NWP instead of from deterministic power forecasts
   - direct prob forecast is defined as preferred approach but why is not explained
* use a copula to model the joint distribution of NWP variables and power
   - separately transform each NWP input to uniform, as well as corresponding measured wind power
      - the marginal transforms are done with unconditional, fixed empirical cdfs i.e. just counts.
    - the copula joint rank relational model is done with a kernel product method.
* power forecast is the conditional distribution

NOT THE USUAL COPULA IDEA
* Differentiate the usual cdf copula to make a density estimator, with c(u,v)  and f(y) terms
* So this predicts a density instead of quantiles (I guess these are computed later by integration)
* The copula density needs uniform (x,y) --> (u,v);
* u and v  are estimated by a simple empirical cdf, or the appropriate KDE kernel
* KDE kernels used
  - Beta ("macro-beta"): is bounded so good for power or speed, lookahead, also has a bandwidth that must be tuned
  - von Mises: circular dependence on wind direction
* inputs:
   - NWP wind speed (or generic "composite" power curve wind power,depending upon experiment)
   - wind direction (for wind speed experiment)
   - forecast horizon (instead of separate model for different horizons)
* Distribution Estimators:
   - unconditional power density fy (y): eq. 1 corrected by eq. 15 (in con. dens. eq. 13)
   - copula dependence joint density: product kernel [ eq.2, example is eq. 11]
   - marginal converter c.d.f.s: unconditional empirical c.d.f.s (eq. 12., note that fy(y) is a density)

ADAPTATION
* use exponential forgetting
* Fast adaptation does not sound quite ready
  - must fiddle with foregetting lambda when need to retrain (otherwise get kernel instability)
  - could detect the "need to fiddle" time with novelty detection -- only suggested, actually done manually
  - seems like you could do this just as well with sliding window bootstrappping resampling and retrain
* Adaptation changes the calibration (bias)
* In future, they say the need
  - time adaptive forgetting factor
  - time adaptive kernel bandwidth
  - novelty detection?  Not in conclusions but mentioned in text

COMPARISION WITH QUANTILE SPLINES
* The two methods seem very similar
   - different performance depending on whether wind power or wind speed forecast is the input
   - generally, the method that has better calibration has worse sharpness
   - one may be good on tails, the other good in the middle
   - one may be good at one tail and the other at the other tail
* Scoring
  - reliability and sharpness
  - some kind of skill score
  - they say over and over again that should have an application-specific skill score



RELATED

Used in Bessa16gaussCplaDcsns, where a temporal copula is employed
Non-time-adaptive in: Bessa11quantCplaWindPow
Corresponding Tech note: Mendes11statWindFrcst},
  doi       = {10.1016/j.renene.2011.08.015},
  file      = {Bessa12adaptQuantCopulaFrcst.pdf:Bessa12adaptQuantCopulaFrcst.pdf:PDF},
  groups    = {PointDerived, doReadWPV_2},
  keywords  = {Wind power},
  owner     = {sotterson},
  timestamp = {2011.12.01},
  url       = {http://www.sciencedirect.com/science/article/pii/S0960148111004587},
}

@InProceedings{Gross02earlyDetSPRT,
  author    = {Gross, Kenny C and Lu, Wendy},
  title     = {Early Detection of Signal and Process Anomalies in Enterprise Computing Systems.},
  booktitle = {ICMLA},
  year      = {2002},
  pages     = {204--210},
  abstract  = {This paper presents a real-time machine learning
technique that has been adaptedfrom thefield ofstatistical
process control (SPC) to give early annunciation of
incipient anomalies in signals and processes involving
enterprise computing systems and associated networks. A
binary-hypothesis technique called the sequential
probability ratio test (SPRT) provides optimal detection of
change points for online surveillance of digitized signals
and demonstrates the dual advantages of high sensitivity
with good avoidance offalse alarms. SPRT-based systems
are being developed for a variety of applications to
enhance the reliability, availability, and serviceability of
enterprise computing systems.
Keywords: real-time fault monitoring; online defect
identification; process anomaly detection},
  comment   = {SPRT is a data-within-a-sliding-analysis-window likelihood ratio between a "normal operation" Gaussian (0 mean, sigma variance) and each of four corruptions of that Gaussian: "positive mean," "negative mean," "nominal variance," "inverse variance."   The four anomaly processes are tested within an analysis window, with preset mean or variance shifts, and and upper and lower boundary for the likelihood ratio.  If any of the anomaly processes are true (pass the LR test) then an alarm is triggered.

I think this is the test that Peter said STEAG used in our June 30, 2017 meeting in Essen.  Hist modifications for non-Gaussianinity and for H1 are in Deeskow17modNotesSPRT},
  file      = {:papers\\Gross02earlyDetSPRT.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.07.04},
  url       = {https://pdfs.semanticscholar.org/c719/16c04eafda6a4b07e73374ba3e7b421f530e.pdf},
}

@Article{Carvalho12probWindGridInt,
  author    = {de Magalhaes Carvalho, L. and da Rosa, M.A. and Martins Leite da Silva, A. and Miranda, V.},
  title     = {Probabilistic Analysis for Maximizing the Grid Integration of Wind Power Generation},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2012},
  volume    = {27},
  number    = {4},
  pages     = {2323--2331},
  month     = nov,
  issn      = {0885-8950},
  abstract  = {This paper presents a sequential Monte Carlo simulation algorithm that can simultaneously assess composite system adequacy and detect wind power curtailment events. A simple procedure at the end of the state evaluation stage is proposed to categorize wind power curtailment events according to their cause. Furthermore, the dual variables of the DC optimal power flow procedure are used to identify which transmission circuits are restricting the use of the total wind power available. In the first set of experiments, the composite system adequacy is assessed, incorporating different generation technologies. This is conducted to clarify the usual comparisons made between wind and thermal technologies which, in fact, depend on the performance measure selected. A second set of experiments considering several wind penetration scenarios is also performed to determine the operational rules or system components responsible for the largest amount of wind energy curtailed. The experiments are carried out on configurations of the IEEE-RTS 79 power system.},
  comment   = {Curtailment detection

Is based on historical data and power system sim. But maybe it could be turned into a forecast?},
  doi       = {10.1109/TPWRS.2012.2207411},
  file      = {Carvalho12probWindGridInt.pdf:Carvalho12probWindGridInt.pdf:PDF},
  groups    = {Use, doReadNonWPV_2},
  keywords  = {Interconnected systems;Load modeling;Monte Carlo methods;Power system stability;Propagation losses;Wind farms;Wind power generation;Monte Carlo methods;load flow;power grids;probability;wind power plants;DC optimal power flow procedure;IEEE-RTS 79 power system;composite system adequacy;grid integration;probabilistic analysis;sequential Monte Carlo simulation algorithm;thermal technology;transmission circuits;wind power curtailment event detection;wind power generation;wind technology;Chronological Monte Carlo simulation;composite system;intermittent energy sources;reliability assessment;},
  owner     = {sotterson},
  timestamp = {2013.02.21},
}

@Article{Morales10shortWindTrade,
  author    = {Morales, J.M. and Conejo, A.J. and P{\'e}rez-Ruiz, J.},
  title     = {Short-Term Trading for a Wind Power Producer},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2010},
  volume    = {25},
  number    = {1},
  pages     = {554--564},
  month     = feb,
  issn      = {0885-8950},
  abstract  = {This paper presents a technique to derive the best offering strategy for a wind power producer in an electricity market that includes various trading floors. Uncertainty pertaining to wind availability, market prices at the different trading stages, and balancing energy needs are properly taken into account. Risk on profit variability is suitably controlled at the cost of a small reduction in expected profit. The proposed technique translates into a linear programming problem of moderate size, which is readily solvable using commercially available software. A variety of numerical case studies demonstrate the interest and effectiveness of the proposed technique. Appropriate conclusions are duly drawn.},
  comment   = {Relevant to optimal spinning reserve project},
  doi       = {10.1109/TPWRS.2009.2036810},
  file      = {Morales10shortWindTrade.pdf:Morales10shortWindTrade.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  keywords  = {electricity market;market prices;short-term trading;trading stages;wind availability;wind power producer;commerce;power markets;power system economics;wind power;},
  owner     = {scot},
  timestamp = {2010.07.05},
}

@Article{Hagemann13intradayMktLiquid,
  author       = {Hagemann, Simon and Weber, Christoph},
  title        = {An empirical analysis of liquidity and its determinants in the German intraday market for electricity},
  journal      = {EWL Working Paper No. 17/2013},
  year         = {2013},
  month        = oct,
  abstract     = {This paper presents a theoretical and empirical analysis of liquidity in the German intraday market for electricity. Two models that aim at explaining intraday liquidity are developed. The first model considers the fundamental merit-order and intraday adjustment needs as the drivers of liquidity in a perfectly competitive market. The second model relaxes the assumption of perfect competition in the intraday market and assumes that the trading behavior of profit maximizing market participants influences the liquidity provision. The relevance of commonly used liquidity indicators like the bid ask-spread, resiliency, market depth, price variance, delay and search costs as well as trading volume and the number of trades are analyzed with respect to both models of liquidity. The empirical findings indicate that liquidity in the German intraday market can be explained by the trading model while the purely fundamental model is rejected.

Keywords: Intraday market, electricity, liquidity, fundamental model, trading model},
  comment      = {EWL Working Paper No. 17/2013
Review:
Maybe good price forecast features. German EPEX Intraday market doesn't behave in the fundamental/merit order economist sense because traders are not risk averse, and will delay trading to get a better deal, increasing liquidity (I think). Study discusses several metrics for market liquidity and then deduces how they would behave under classic, "fundamental" assumption and under a more complicated "trader model." It then checks the metrics to see which assumption is more likely to be true; it is overwhelmingly the trader model.

The dimensions of liquidity and their correlations might be good for a price forecast, since low liquidity prediction for the future would suggest that the price will go up if you buy a lot of electricity.

If TSO's can't consider price, they still must consider market liquidity in intraday market. This is a paper about that, and maybe that will lead to a "liquidity" forecast (trade when you forecast that enough electricity will be available, or maybe make this decision on every step). If do this, then will have to also figure out what the true TSO forecast was (not normalized) and must somehow remove that from the historical data.

This paper also talks about merit order stuff, maybe related to the work the other group did on UC price forecasting.

Says there are no German grid-wide load forecasts. Really?

Possible features from, Fig II, p. 19 and the neighborhood
1. Time of day
2. Day of week (binary?)
3. Bid Ask spread, a liquidity indicator(would need DA load curves. Available)
4. intraday (I think) price variance and high-low spread, a liquidity indicator
 (from past hour or something, for the delivery time in question?)
5. number of trades and volume, a liquidity indicator (for the delivery
 time in question?)},
  doi          = {10.2139/ssrn.2349565},
  file         = {Hagemann13intradayMktLiquid.pdf:Hagemann13intradayMktLiquid.pdf:PDF},
  groups       = {Read},
  howpublished = {EWL Working Paper No. 17/2013},
  owner        = {sotterson},
  timestamp    = {2015.07.18},
}

@TechReport{Hagemann15volIntradayEurope,
  author      = {Hagemann, Simon and Weber, Christoph and others},
  title       = {Trading Volumes in Intraday Markets-Theoretical Reference Model and Empirical Observations in Selected European Markets},
  institution = {University of Duisburg-Essen},
  year        = {2015},
  type        = {EWL Working Paper No. [03/15]},
  abstract    = {This paper presents an analytical benchmark model for national intraday adjustment needs under
consideration of fundamental drivers, market concentration and portfolio internal netting. The
benchmark model is used to calculate the intraday market outcomes if (i) large and small players as
well as transmissions operators trade and (ii) only large players and transmission system operators
trade. Transaction costs may prevent the competitive fringe from intraday market participation. The
theoretical national intraday trading volumes are calculated with market data from three European
countries with auction-based intraday markets (Italy, Portugal, Spain) and four countries with
continuous intraday markets (Denmark, France, Germany, United Kingdom). The model results allow
two main conclusions: The competitive fringe is not trading on exchanges in Denmark and France but
in Germany. The second conclusion is that the high observed volumes in auction-based intraday
markets cannot be explained by fundamentals or the auction-based design but are mainly caused by
market peculiarities. The same result applies to the UK.

Keywords: Renewables market integration, Liquidity modeling, continuous and auction-based
intraday markets.},
  comment     = {Only Germany has a "competitive fringe" and volumes are "too high" in auction based markets.},
  file        = {Hagemann15volIntradayEurope.pdf:Hagemann15volIntradayEurope.pdf:PDF},
  url         = {http://www.ewl.wiwi.uni-due.de/fileadmin/fileupload/BWL-ENERGIE/Arbeitspapiere/RePEc/pdf/wp1503_TradingVolumesInIntradayMarketsTheoreticalReferenceModelAndEmpiricalObservationsInSelectedEuropeanMarkets.pdf},
}

@Article{Matos09probSpinResRenew,
  author    = {Matos, M. and Lopes, J.P. and Rosa, M. and Ferreira, R. and Leite da Silva, A. and Sales, W. and Resende, L. and Manso, L. and Cabral, P. and Ferreira, M. and others},
  title     = {Probabilistic evaluation of reserve requirements of generating systems with renewable power sources: the Portuguese and Spanish cases},
  journal   = {International Journal of Electrical Power \& Energy Systems},
  year      = {2009},
  volume    = {31},
  number    = {9},
  pages     = {562--569},
  abstract  = {This paper presents an application of probabilistic methodologies to evaluate the reserve requirements of generating systems with a large penetration of renewable energy sources. The idea is to investigate the behavior of reliability indices, including those from the well-being analysis, when the major portion of the renewable sources comes from wind power and other intermittent sources. A new simulation process to address operating reserve adequacy is introduced, and the correspondent reliability indices are observed. Case studies on the Portuguese and Spanish generating systems are presented and discussed},
  comment   = {Juan Mi's recommendation},
  file      = {Matos09probSpinResRenew.pdf:Matos09probSpinResRenew.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2012.02.09},
}

@InProceedings{Dreher18pooledFreqRsrvWndCtrl,
  author    = {A. Dreher and D. Jost and S. Otterson and P. Hochloff},
  title     = {Pooled Frequency Restoration Reserve Provision by Wind Farms and Controllable Energy Units},
  booktitle = {International Conf. on the European Energy Market.},
  year      = {2018},
  address   = {Łódź, PL.},
  file      = {Dreher18pooledFreqRsrvWndCtrl.pdf:Dreher18pooledFreqRsrvWndCtrl.pdf:PDF},
  abstract  = {This paper presents an approach to quantify the
reliability or security level of a pool of flexibility providing
controllable energy units (CEUs). In this pool, a backup
unit secures the flexibility capacity of the largest flexibility
providing unit. The security level is then applied to determine
the flexibility capacity of a common pool of wind farms and
CEUs, making use of probabilistic wind forecasts. The paper
advances previous research conducted in a joint project with
industry partners on Frequency Restoration Reserve (FRR)
provision of pools of wind farms and controllable power plants.},
  comment   = {Combo of controllable plants and wind farms for req. contol reserve.  Used my dr1nn code (multi-task distribution regression)},
}

@Article{Genest07copulaEverything,
  author    = {Christian Genest and Anne-Catherine Favre},
  title     = {Everything You Always Wanted to Know about Copula Modeling but Were Afraid to Ask},
  journal   = {Journal of Hydrologic Engineering},
  year      = {2007},
  volume    = {12},
  number    = {4},
  pages     = {347--368},
  abstract  = {This paper presents an introduction to inference for copula models, based on rank methods. By working out in detail a small, fictitious numerical example, the writers exhibit the various steps involved in investigating the dependence between two random variables and in modeling it using copulas. Simple graphical tools and numerical techniques are presented for selecting an appropriate model, estimating its parameters, and checking its goodness-of-fit. A larger, realistic application of the methodology to hydrological data is then presented.},
  comment   = {Friendly copula intro. slides from: http://www-m4.ma.tum.de/courses/WS09-10/multiCopula/vortraege/7-Peintinger.pdf},
  doi       = {10.1061/(ASCE)1084-0699(2007)12:4(347)},
  file      = {Paper:Genest07copulaEverything.pdf:PDF;Slides:Genest07copulaEverythingSlides.pdf:PDF},
  keywords  = {Frequency analysis; Distribution functions; Risk management; Statistical models},
  owner     = {scot},
  publisher = {ASCE},
  timestamp = {2010.12.20},
  url       = {http://link.aip.org/link/?QHE/12/347/1},
}

@TechReport{Oh15mdlDepHiDimFactrCopulaTechRep,
  author      = {Dong Hwan Oh and Andrew J. Patton},
  title       = {Modelling Dependence in High Dimensions with Factor Copulas},
  institution = {Finance and Economics Discussion Series 2015-051. Washington: Board of Governors of the Federal Reserve System},
  year        = {2015},
  abstract    = {This paper presents flexible new models for the dependence structure, or copula, of economic variables
based on a latent factor structure. The proposed models are particularly attractive for relatively high di-
mensional applications, involving ?fty or more variables, and can be combined with semiparametric marginal
distributions to obtain ?exible multivariate distributions. Factor copulas generally lack a closed-form den-
sity, but we obtain analytical results for the implied tail dependence using extreme value theory, and we
verify that simulation-based estimation using rank statistics is reliable even in high dimensions. We consider
?scree?plots to aid the choice of the number of factors in the model. The model is applied to daily returns on
all 100 constituents of the S&P 100 index, and we ?nd signi?cant evidence of tail dependence, heterogeneous
dependence, and asymmetric dependence, with dependence being stronger in crashes than in booms. We
also show that factor copula models provide superior estimates of some measures of systemic risk.
Keywords: correlation, dependence, copulas, tail dependence, systemic risk.},
  comment     = {Factor (vine) copula for high dims.  Also handles tail correlation, I think.

This is a Fed Reserve report that was also publshed as an article in Oh15mdlDepHiDimFactrCopula},
  doi         = {http://dx.doi.org/10.17016/FEDS.2015.051},
  file        = {Oh15mdlDepHiDimFactrCopulaTechRep.pdf:Oh15mdlDepHiDimFactrCopulaTechRep.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2016.12.17},
}

@Article{Caird08enrgyEffAdoptUK,
  author    = {Caird, Sally and Roy, Robin and Herring, Horace},
  title     = {Improving the energy performance of UK households: Results from surveys of consumer adoption and use of low-and zero-carbon technologies},
  journal   = {Energy Efficiency},
  year      = {2008},
  volume    = {1},
  number    = {2},
  pages     = {149},
  abstract  = {This paper presents results from a UK Open University project which surveyed consumers’ reasons for adoption, and non-adoption, of energy efficiency measures and renewable energy systems—collectively called low- and zero-carbon technologies—and their experiences of using these technologies. Data were gathered during 2006 via an online questionnaire with nearly 400 responses, plus 111 in-depth telephone interviews. The respondents were mainly environmentally concerned, ‘green’ consumers and therefore these are purposive rather than representative surveys. The paper outlines results for four energy efficiency measures (loft insulation, condensing boilers, heating controls and energy-efficient lighting) and four household renewables (solar thermal water heating, solar photovoltaics, micro-wind turbines and wood-burning stoves). These green consumers typically adopted these technologies to save energy, money and/or the environment, which many considered they achieved despite rebound effects. The reasons for considering but rejecting these technologies include the familiar price barriers, but there were also other obstacles that varied according to the technology concerned. Nearly a third of the surveyed consumers had adopted household renewables, over half of which were wood stoves and 10\% solar thermal water heating systems. Most adopters of renewables had previously installed several energy efficiency measures, but only a fifth of those who seriously considered renewables actually installed a system. This suggests sell energy efficiency first, then renewables. There seems to be considerable interest in household renewables in the UK, especially among older, middle-class green consumers, but so far only relatively few pioneers have managed to overcome the barriers to adoption.
Keywords
Consumers Surveys Energy efficiency Renewable energy Low- and zero-carbon technologies Policies User-centred design improvements },
  publisher = {Springer},
  url       = {https://link.springer.com/article/10.1007%2Fs12053-008-9013-y#citeas},
}

@InProceedings{Lai05NewCmplxStepDiff,
  author    = {Lai, Kok-Lam and Crassidis, John L and Cheng, Yang and Kim, Jongrae},
  title     = {New complex-step derivative approximations with application to second-order kalman filtering},
  booktitle = {AIAA Guidance, Navigation and Control Conference, San Francisco, California},
  year      = {2005},
  abstract  = {This paper presents several extensions of the complex-step approximation to compute
numerical derivatives. For first derivatives the complex-step approach does not suffer sub-
straction cancellation errors as in standard numerical finite-difference approaches. There-
fore, since an arbitrarily small step-size can be chosen, the complex-step method can achieve
near analytical accuracy. However, for second derivatives straight implementation of the
complex-step approach does suffer from roundoff errors. Therefore, an arbitrarily small
step-size cannot be chosen. In this paper we expand upon the standard complex-step
approach to provide a wider range of accuracy for both the first and second derivative
approximations. Several formulations are derived using various complex numbers coupled
with Richardson extrapolations. The new extensions can allow the use of one step-size to
provide optimal accuracy for both derivative approximations. Simulation results are pro-
vided to show the performance of the new complex-step approximations on a second-order
Kalman filter.},
  comment   = {How to do CSD for 2\textsuperscript{nd} order partials and how to compute a Hessian from them. Shows improvement on Kalman filter application.

Journal paper: Lai05NewCmplxStepDiff

Maybe fancier stuff in: Abreu13genCmplxStepDiff},
  doi       = {10.2514/6.2005-5944},
  file      = {Lai05NewCmplxStepDiff.pdf:Lai05NewCmplxStepDiff.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.07.06},
}

@InProceedings{Li11tensoCovMatObjTrck,
  author    = {Peihua Li and Qi Sun},
  title     = {Tensor-based covariance matrices for object tracking},
  booktitle = {Proc. IEEE Int. Conf. Computer Vision Workshops (ICCV Workshops)},
  year      = {2011},
  pages     = {1681--1688},
  month     = nov,
  abstract  = {This paper presents tensor-based covariance matrices for object modeling and tracking. Unlike the traditional vector-based or matrix-based object representation, this method represents an object with a third-order tensor and has better capability to capture the intrinsic structure of the image data. We flatten the tensor to obtain all of its mode-n unfolding matrices, each one of which can be seen as a sample of observations of some high-dimensional random signals. For every mode-n unfolding matrix, we use the K-L transform to achieve the principal components of the column vectors. The covariance matrix of the reduced-dimensional signal via the K-L transform is used for modeling the object statistics. Based on this modeling, a distance measure is introduced for object tracking using the affine-invariant Riemannian metric. For adapting to the appearance changes of the object across time, we present an efficient, incremental model update mechanism. Experiments show that the proposed tracking method has promising performance.},
  comment   = {Good for high dim covariance estimation of spatio-temporal fields, or in general?

* FanaeeT16tensorAnomDetSrvy says that tensors are good for spatio-temporal.
* learning tensor approx with a NN: Che17neuralNetTensorRnk1
},
  doi       = {10.1109/ICCVW.2011.6130452},
  file      = {:Li11tensoCovMatObjTrck.pdf:PDF},
  keywords  = {affine transforms, covariance matrices, image representation, statistical analysis, target tracking, tensors, K-L transform, affine-invariant Riemannian metric, column vectors, distance measure, high-dimensional random signals, image data, incremental model update mechanism, intrinsic structure, mode-n unfolding matrices, object modeling, object representation, object statistics, object tracking, principal components, tensor-based covariance matrices, third-order tensor, vector-based representation, Adaptation models, Computational modeling, Covariance matrix, Lighting, Tensile stress, Transforms, Vectors},
  owner     = {sotterson},
  timestamp = {2017.06.26},
}

@Article{Perez90ModelDayLightAvail,
  author    = {Richard Perez and Pierre Ineichen and Robert Seals and Joseph Michalsky and Ronald Stewart},
  title     = {Modeling daylight availability and irradiance components from direct and global irradiance},
  journal   = {Solar Energy},
  year      = {1990},
  volume    = {44},
  number    = {5},
  pages     = {271 - 289},
  issn      = {0038-092X},
  abstract  = {This paper presents the latest versions of several models developed by the authors to predict short time-step solar energy and daylight availability quantities needed by energy system modelers or building designers. The modeled quantities are global, direct and diffuse daylight illuminance, diffuse irradiance and illuminance impinging on tilted surfaces of arbitrary orientation, sky zenith luminance and sky luminance angular distribution. All models are original except for the last one which is extrapolated from current standards. All models share a common operating structure and a common set of input data: Hourly (or higher frequency) direct (or diffuse) and global irradiance plus surface dew point temperature. Key experimental observations leading to model development are briefly reviewed. Comprehensive validation results are presented. Model accuracy, assessed in terms of root-mean-square and mean bias errors, is analyzed both as a function of insolation conditions and site climatic environment.},
  comment   = {How IWES models irradiation on inclined surface.  Referred to in: Saint-Drenan14commentsGenPVpow

Alternative seems to be Klucher79Evaluationmodelspredict},
  doi       = {http://dx.doi.org/10.1016/0038-092X(90)90055-H},
  file      = {Perez90ModelDayLightAvail.pdf:Perez90ModelDayLightAvail.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.30},
  url       = {http://www.sciencedirect.com/science/article/pii/0038092X9090055H},
}

@InProceedings{Ma09compIntSmartGrid,
  author    = {Ma, Y. and Long Zhou and Tse, N. and Osman, A. and Lai, L.L.},
  title     = {An initial study on computational intelligence for smart grid},
  booktitle = {Machine Learning and Cybernetics (ICMLC)},
  year      = {2009},
  volume    = {6},
  pages     = {3425--3429},
  month     = jul,
  abstract  = {This paper proposes a framework on how computational intelligence may be applied in developing smart grid to improve reliability and security of power system. Some examples will be used to demonstrate the feasibility.},
  comment   = {Ideas for new DTU projects?},
  doi       = {10.1109/ICMLC.2009.5212744},
  file      = {Ma09compIntSmartGrid.pdf:Ma09compIntSmartGrid.pdf:PDF},
  journal   = {Machine Learning and Cybernetics (ICMLC)},
  keywords  = {computational intelligence;power system reliability;power system security;smart grid;knowledge based systems;power distribution reliability;power engineering computing;power transmission reliability;},
  owner     = {scot},
  timestamp = {2010.09.06},
}

@InCollection{Guo05knnFeatSel,
  author    = {Guo, Gongde and Neagu, Daniel and Cronin, Mark TD},
  title     = {Using kNN model for automatic feature selection},
  booktitle = {Pattern Recognition and Data Mining},
  year      = {2005},
  publisher = {Springer},
  pages     = {410--419},
  doi       = {10.1007/11551188_44},
  abstract  = {This paper proposes a kNN model-based feature selection method
aimed at improving the efficiency and effectiveness of the ReliefF method by:
(1) using a kNN model as the starter selection, aimed at choosing a set of more
meaningful representatives to replace the original data for feature selection; (2)
integration of the Heterogeneous Value Difference Metric to handle
heterogeneous applications ? those with both ordinal and nominal features; and
(3) presenting a simple method of difference function calculation based on
inductive information in each representative obtained by kNN model. We have
evaluated the performance of the proposed kNN model-based feature selection
method on toxicity dataset Phenols with two different endpoints. Experimental
results indicate that the proposed feature selection method has a significant
improvement in the classification accuracy for the trial dataset.},
  file      = {Guo05knnFeatSel.pdf:Guo05knnFeatSel.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.05.02},
}

@Article{Chernozhukov10qrNoCross,
  author    = {Chernozhukov, Victor and Fern{\'a}ndez-Val, Iv{\'a}n and Galichon, Alfred},
  title     = {Quantile and Probability Curves Without Crossing},
  journal   = {Econometrica},
  year      = {2010},
  volume    = {78},
  number    = {3},
  pages     = {1093--1125},
  issn      = {1468-0262},
  abstract  = {This paper proposes a method to address the longstanding problem of lack of monotonicity in estimation of conditional and structural quantile functions, also known as the quantile crossing problem (Bassett and Koenker (1982)). The XX method consists in sorting or monotone rearranging the original estimated non-monotone curve into a monotone rearranged curve. We show that the rearranged curve is closer to the true quantile curve than the original curve in finite samples, establish a functional delta method for rearrangement-related operators, and derive functional limit theory for the entire rearranged curve and its functionals. We also establish validity of the bootstrap for estimating the limit law of the entire rearranged curve and its functionals. Our limit results are generic in that they apply to every estimator of a monotone function, provided that the estimator satisfies a functional central limit theorem and the function satisfies some smoothness conditions. Consequently, our results apply to estimation of other econometric functions with monotonicity restrictions, such as demand, production, distribution, and structural distribution functions. We illustrate the results with an application to estimation of structural distribution and quantile functions using data on Vietnam veteran status and earnings.},
  comment   = {An incredibly simple way to prevent quantile crossover: just sort them! Also good for other functions that should be monotonic, as long as they obey a "functional" central limit theorem. Lengthy proofs show that it works.

* Can be viewed as quantile bootstrapping, or as sampling an estimated cdf with a uniform distribution
* main trick: make a new cdf from the estimated quantiles (the new cdf MUST be monotonic)
* I didn't read much of the proofs...
* Rearrangement improves sharpness in 2 of 3 empirical test cases examples and reduces L* norm estimation error.
* suggested by Cannon11quantRgrsnNNprecip, who says it's implemented in the R quantreg function.
* recommends Dette06nonParamMonoRgrssn as a way of smoothed rearrangement.
* says Dette08quantRgrsNonCross is nice comparison of rearrangement and another technique

QUESTION: what does this do for time continuity? Should sorting be done over multiple frames?},
  doi       = {10.3982/ECTA7880},
  file      = {Chernozhukov10qrNoCross.pdf:Chernozhukov10qrNoCross.pdf:PDF},
  groups    = {Read, PointDerived, doReadNonWPV_1},
  keywords  = {Conditional quantiles, structural quantiles, monotonicity problem, rearrangement, isotonic regression, functional delta method},
  owner     = {sotterson},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2013.10.23},
}

@InProceedings{Lu10unitCommCnstrnt,
  author    = {Shuai Lu and Makarov, Y.V. and Yunhua Zhu and Ning Lu and Kumar, N.P. and Chakrabarti, B.B.},
  title     = {Unit commitment considering generation flexibility and environmental constraints},
  booktitle = {IEEE Power \& Energy Society (PES)},
  year      = {2010},
  pages     = {1--11},
  month     = jul,
  abstract  = {This paper proposes a new framework for the power system unit commitment process, incorporating generation flexibility requirements and environmental constraints into the existing unit commitment algorithm. The generation flexibility requirements are to address the uncertainty and variability associated with large amounts of intermittent resources as well as with load, which cause real-time balancing requirements to be variable and less predictable. The proposed flexibility requirements include capacity, ramp-rate and energy (or ramp duration) for both upward and downward balancing reserves. The environmental constraints include emission limits for fossil fuel-fired generators and ecological regulations for hydro power plants. The calculation of emission rates is formulated. Unit commitment under this new framework will be critical to the economic and reliable operation of the power grid and the minimization of its negative environmental impacts, especially when high penetration levels of intermittent resources are being approached, as required by the renewable portfolio standards in many states.},
  comment   = {Good argument for flexibility (ramps) \& and environment constraints in unit commitment, including costs, times
* slides describe swinging door algorithm to calculate capacity, ramp and ramp duration requirements
-- are they saying that this is old-school?
-- orig alg ref: Bristol09swingDoorTrend
* describes emissions costs of turning plants on and off
* include hydro plant environmental constraints
* slides have nice table of current practice for max deviation vs. lookahead and response time (bottom of slide 10)
* slides also show how expensive it is to turn off or on a coal gas power plant: $15K-$80K (slide 8)

Also: Makarov09OperWindCA
CAISO10caisoRnwblIntegSwngDr
Crampes18flexElecMktsRamp

},
  doi       = {10.1109/PES.2010.5589501},
  file      = {paper:Lu10unitCommCnstrnt.pdf:PDF;slides:Lu10unitCommCnstrnt_Slides.pdf:PDF},
  groups    = {Use, doReadNonWPV_1},
  issn      = {1944-9925},
  keywords  = {downward balancing;ecological regulations;environmental constraints;fossil fuel-fired generators;generation flexibility requirements;hydro power plants;intermittent resources;power grid;power system unit commitment process;ramp-rate;renewable portfolio standards;unit commitment;upward balancing;environmental factors;hydroelectric power stations;power generation dispatch;power generation scheduling;steam power stations;},
  owner     = {scot},
  timestamp = {2011.06.20},
}

@Article{Monteiro13solarFrcstHistSim,
  author    = {Monteiro, Claudio and Santos, Tiago and Fernandez-Jimenez, L Alfredo and Ramirez-Rosado, Ignacio J and Terreros-Olarte, M Sonia},
  title     = {Short-term power forecasting model for photovoltaic plants based on historical similarity},
  journal   = {Energies},
  year      = {2013},
  volume    = {6},
  number    = {5},
  pages     = {2624--2643},
  abstract  = {This paper proposes a new model for short-term forecasting of electric energy production in a photovoltaic (PV) plant. The model is called HIstorical SImilar MIning (HISIMI) model; its final structure is optimized by using a genetic algorithm, based on data mining techniques applied to historical cases composed by past forecasted values of weather variables, obtained from numerical tools for weather prediction, and by past production of electric power in a PV plant. The HISIMI model is able to supply spot values of power forecasts, and also the uncertainty, or probabilities, associated with those spot values, providing new useful information to users with respect to traditional forecasting models for PV plants. Such probabilities enable analysis and evaluation of risk associated with those spot forecasts, for example, in offers of energy sale for electricity markets. The results of spot forecasting of an illustrative example obtained with the HISIMI model for a real-life grid-connected PV plant, which shows high intra-hour variability of its actual power output, with forecasting horizons covering the following day, have improved those obtained with other two power spot forecasting models, which are a persistence model and an artificial neural network model.
Keywords: power forecasting; solar energy; data mining; genetic algorithm},
  comment   = {A kind of PV analog ensemble forecast for day ahead. Also has some kind of transition probability matrix and genetic algorithm.},
  file      = {Monteiro13solarFrcstHistSim.pdf:Monteiro13solarFrcstHistSim.pdf:PDF},
  owner     = {sotterson},
  publisher = {Multidisciplinary Digital Publishing Institute},
  timestamp = {2015.02.19},
  url       = {http://www.mdpi.com/1996-1073/6/5/2624},
}

@InProceedings{Mori05probShortForecastGaussProc,
  author    = {Mori, H. and Ohmi, M.},
  title     = {Probabilistic short-term load forecasting with {Gauss}ian processes},
  booktitle = {Intelligent Systems Application to Power Systems},
  year      = {2005},
  pages     = {6},
  month     = nov,
  abstract  = {This paper proposes a new probabilistic method for short-term load forecasting with the Gaussian processes (GP). In recent years, the degree of uncertainty increases as the power system becomes more deregulated and competitive. The power system players are concerned with maximizing the profit while minimizing the risk in the power market. As a result, it is important to consider the uncertainty of the predicted load in short-term load forecasting appropriately. The proposed method aims at extending load forecasting for the average point into that for the posterior distribution of the predicted load to handle the uncertainty of load forecasting. In this paper, the hyperparameters of the covariance function is evaluated in GP by the hierarchical Bayesian model after extending GP into the kernel-based method. The proposed method is tested for real data of one-step ahead daily maximum load forecasting in comparison with the conventional methods such as MLP, RBFN and SVR.},
  comment   = {Gaussian process load forecasting. Interesting in that it uses hierarchical base tuning of hyperparameters.},
  doi       = {10.1109/ISAP.2005.1599306},
  file      = {Mori05probShortForecastGaussProc.pdf:Mori05probShortForecastGaussProc.pdf:PDF;Mori05probShortForecastGaussProc.pdf:Mori05probShortForecastGaussProc.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_1},
  keywords  = { Bayes methods, Gaussian processes, covariance analysis, load distribution, load forecasting, power markets, power system economics Gaussian processes, MAP estimation, covariance function, hierarchical Bayesian model, kernel-based method, power market, power system, probabilistic short-term load forecasting},
  owner     = {scotto},
  timestamp = {2008.10.04},
  url       = {http://ieeexplore.ieee.org.offcampus.lib.washington.edu/search/srchabstract.jsp?arnumber=1599306&isnumber=33627&punumber=10658&k2dockey=1599306@ieeecnfs&query=((probabilistic+short-term+load+forecasting+with+gaussian+processes)%3Cin%3Emetadata)&pos=0&access=no},
}

@Article{Ummels07windGenCommitDisp,
  author    = {Ummels, B.C. and Gibescu, M. and Pelgrum, E. and Kling, W.L. and Brand, A.J.},
  title     = {Impacts of Wind Power on Thermal Generation Unit Commitment and Dispatch},
  journal   = {Energy Conversion, IEEE Transactions on},
  year      = {2007},
  volume    = {22},
  number    = {1},
  pages     = {44--51},
  month     = mar,
  issn      = {0885-8969},
  abstract  = {This paper proposes a new simulation method that can fully assess the impacts of large-scale wind power on system operations from cost, reliability, and environmental perspectives. The method uses a time series of observed and predicted 15-min average wind speeds at foreseen onshore- and offshore-wind farm locations. A Unit Commitment and Economic Dispatch (UC-ED) tool is adapted to allow for frequent revisions of conventional generation unit schedules, using information on current wind energy output and forecasts for the next 36 h. This is deemed the most faithful way of simulating actual operations and short-term planning activities for a system with large wind power penetration. The problem formulation includes ramp-rate constraints for generation schedules and for reserve activation, and minimum up-time and down-time of conventional units. Results are shown for a realistic future scenario of the Dutch power system. It is shown that problems such as insufficient regulating and reserve power-which are typically associated with the variability and limited predictability of wind power-can only be assessed in conjunction with the specifics of the conventional generation system that wind power is integrated into. For the thermal system with a large share of combined heat and power (CHP) investigated here, wind power forecasting does not provide significant benefits for optimal unit commitment and dispatch. Minimum load problems do occur, which result in wasted wind in amounts increasing with the wind power installed},
  comment   = {Wind integration w/ ramp constraints; wind is good but forecasts add little value and CHP's can aggravate wind spillage

Simulated integration of 8 GW of wind into Dutch system
* huge amt. of offshore: 6GW offshore; 2GW onshore
* lots of combined heating plants (CHP) appears to make wind harder to integrate!
-- must provide heat
-- then side-effect electricity is a must-take, just like wind and therefore sometimes causes wind curtailement
-- Necessarily? Could CHP's be designed to accept wind power for heating and not generate electricity?
* has thermal ramp constraints, not much said about it
* forecasts are partly simulated. I don't quite get the process, or how it relates to simulated production Value of wind
* emissions decrease proportionally up to about 4GW (curtailments are insignificant below that)
* at 8GW, save about 25\% of operating costs
* emissions decrease steadily w/ more added wind Value of forecasts is minimal for unit commitment and dispatch
* unit commitment and dispatch are about the same for different forecasts (not clear what the different forecasts are)
* forecasts provide a benefit worth only 0.02\% of system operating costs!
* I'm not sure about what kind of forecasts they used
* I'm also not sure about what kind of unit commitment algorithm they used (it's 2007, so maybe not probabilistic?)},
  doi       = {10.1109/TEC.2006.889616},
  file      = {Ummels07windGenCommitDisp.pdf:Ummels07windGenCommitDisp.pdf:PDF},
  groups    = {Read},
  keywords  = {CHP;Dutch power system;combined heat and power;large-scale wind power penetration;offshore wind farms;onshore wind farms;ramp-rate constraints;short-term planning activity;thermal generation;unit commitment and economic dispatch tool;unit scheduling;wind power forecasting;cogeneration;environmental factors;load forecasting;power generation dispatch;power generation reliability;power generation scheduling;power system interconnection;thermal power stations;wind power plants;},
  owner     = {scot},
  timestamp = {2011.05.26},
}

@Article{Vaccaro11adaptMultiFrcstOneDay,
  author    = {A. Vaccaro and P. Mercogliano and P. Schiano and D. Villacci},
  title     = {An adaptive framework based on multi-model data fusion for one-day-ahead wind power forecasting},
  journal   = {Electric Power Systems Research},
  year      = {2011},
  volume    = {81},
  number    = {3},
  pages     = {775--782},
  issn      = {0378-7796},
  abstract  = {This paper proposes a novel framework for one-day-ahead wind power forecasting based on information amalgamation from multiple sources. The final objective is to provide a better solution than could otherwise be achieved from the use of single-source data alone. The proposed framework combines multiple forecasting models and adaptive machine learning techniques for information processing. The input data sources are the wind forecast profiles computed by synoptic physical models and measured data coming from meteorological stations which are amalgamated via an adaptive supervised learning system. The latter is based on a local learning algorithm, called the Lazy Learning (LL) algorithm. This algorithm is sequentially updated, in order to adapt the whole architecture to ???new??? operating conditions. Experimental results obtained on a one-year time scenario show the effectiveness of the proposed data fusion paradigm in addressing the problem of one-day-ahead wind power forecasting.},
  comment   = {Interesting b/c it adaptively uses offsite observations and it works on day-ahead forecasts, where I didn't expect them to help.},
  doi       = {10.1016/j.epsr.2010.11.009},
  file      = {Vaccaro11adaptMultiFrcstOneDay.pdf:Vaccaro11adaptMultiFrcstOneDay.pdf:PDF},
  keywords  = {Wind forecasting},
  owner     = {sotterson},
  timestamp = {2013.03.15},
  url       = {http://www.sciencedirect.com/science/article/pii/S0378779610002816},
}

@Article{Li16loadProfileClustMultiRes,
  author   = {R. {Li} and F. {Li} and N. D. {Smith}},
  title    = {Multi-Resolution Load Profile Clustering for Smart Metering Data},
  journal  = {IEEE Transactions on Power Systems},
  year     = {2016},
  volume   = {31},
  number   = {6},
  pages    = {4473--4482},
  month    = nov,
  issn     = {0885-8950},
  abstract = {This paper proposes a novel multi-resolution clustering (MRC) method that for the first time classifies end customers directly from massive, volatile and uncertain smart metering data. It will firstly extract spectral features of load profiles by multi-resolution analysis (MRA), and then cluster and classify these features in the spectral-domain instead of time-domain. The key advantage is that the proposed method will allow a dynamic load profiling to be flexibly re-constructed from each spectral level. MRC addresses three key limitations in time-series based load profiling: i) large sample size: sample size is reduced by a novel two-stage clustering, which firstly clusters each customers' massive daily profiles into several Gaussian mixture models (GMMs) and then clusters all GMMs; ii) volatility: it avoids the interferences between different load features (e.g. magnitude, overall trend, spikes) by decomposing them onto different resolution levels, and then clustering separately; iii) uncertainties: as the GMM can give a probabilistic cluster membership instead of a deterministic one, an additive classification model based on the posterior probability is proposed to reflect the uncertainty between days. The proposed method is implemented on 6369 smart metered customers from Ireland, and compared with the load profiles used by the U.K. industry and traditional K-means clustering. The results show that the developed MRC outperformed the traditional methods in its ability in profiling load for big, volatile and uncertain smart metering data.},
  comment  = {Has probabilistic cluster membership output.  Useful for MNSP.  Seems to be spectral clust --> GMM --> Gmm clust.

If I don't find other easily probabilistic clustering algorithms, I could get this paper.},
  doi      = {10.1109/TPWRS.2016.2536781},
  keywords = {Gaussian processes, mixture models, probability, smart meters, spectral-domain analysis, time series, multi-resolution clustering method, MRC method, smart metering data, multi-resolution analysis, MRA, spectral-domain, dynamic load profiling, time-series based load profiling, sample size, Gaussian mixture models, GMM, load features, probabilistic cluster membership, additive classification model, posterior probability, Uncertainty, Feature extraction, Time-domain analysis, Market research, Smart meters, Multiresolution analysis, Shape, Big data, clustering, customer classification, GMM, load profiles, multi-resolution analysis, spectral analysis, X-means, smart grid, smart meter},
}

@Article{Ferrari04betaRgrssnRateProp,
  author    = {Ferrari, Silvia and Cribari-Neto, Francisco},
  title     = {Beta regression for modelling rates and proportions},
  journal   = {Journal of Applied Statistics},
  year      = {2004},
  volume    = {31},
  number    = {7},
  pages     = {799--815},
  abstract  = {This paper proposes a regression model where the response is beta distributed
using a parameterization of the beta law that is indexed by mean and dispersion parameters.
The proposed model is useful for situations where the variable of interest is
continuous and restricted to the interval (0; 1) and is related to other variables through
a regression structure. The regression parameters of the beta regression model are interpretable
in terms of the mean of the response and, when the logit link is used, of an odds
ratio, unlike the parameters of a linear regression that employs a transformed response.
Estimation is performed by maximum likelihood. We provide closed-form expressions for
the score function, for Fisher?s information matrix and its inverse. Hypothesis testing
is performed using approximations obtained from the asymptotic normality of the maximum
likelihood estimator. Some diagnostic measures are introduced. Finally, practical
applications that employ real data are presented and discussed.
Keywords and phrases: Beta distribution; maximum likelihood estimation; leverage; proportions;
residuals.},
  comment   = {Seems to be the first beta regression paper.},
  doi       = {10.1080/0266476042000214501},
  file      = {Ferrari04betaRgrssnRateProp.pdf:Ferrari04betaRgrssnRateProp.pdf:PDF},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2014.07.18},
}

@Article{VonSelasinsky14intradayMktDE,
  author   = {Von Selasinsky, Alexander},
  title    = {The Integration of renewables in continuous intraday markets for electricity},
  journal  = {Available at SSRN 2405454},
  year     = {2014},
  month    = feb,
  abstract = {This paper proposes an analytical model to explain how the uncertain
feed-in from renewable energy sources shapes continuous intraday mar-
kets for electricity. The basic idea of the model is to use the day-ahead
uniform-price auction as a reference framework to illustrate how forecast
errors are balanced in the intraday market. The model is used to give
insights into the price-setting decision of market participants, to explain
prices from the German electricity market and to show some determi-
nants of integration costs.},
  comment  = {How to model German intraday market?

*        "direction of the forecast error is a fundamental variable in
        determining intraday transactions in an elec. sys. w/ high RES"
        underestimate causes neg. prices


says Henriot14windIntradayCentral explains when it makes sense to make many bids for same power},
  file     = {VonSelasinsky14intradayMktDE.pdf:VonSelasinsky14intradayMktDE.pdf:PDF},
  url      = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2405454},
}

@InProceedings{Lai05partSwarmDispNonSmth,
  author    = {Lai, L.L. and Nieh, T.Y. and Vujatovic, D. and Ma, Y.N. and Lu, Y.P. and Wang, Y.W. and Braun, H.},
  title     = {Particle swarm optimization for economic dispatch of units with non-smooth input-output characteristic functions},
  booktitle = {Intelligent Systems Application to Power Systems, 2005. Proceedings of the 13\textsuperscript{th} International Conference on},
  year      = {2005},
  pages     = {5},
  month     = nov,
  abstract  = {This paper proposes an application of particle swarm optimization (PSO) to solve economic dispatch (ED) of units with non-smooth input-output characteristic functions. The IEEE 30-bus system with 6 generating units has been used as the simulation system to show the effectiveness of the algorithm. Results are compared to those detail by evolutionary programming (EP). It shows the PSO can produce slightly better results than those from EP},
  comment   = {Example of learning a monotonically increasing function, like the one we wanted for the NordPool spinning reserves project at DTU},
  doi       = {10.1109/ISAP.2005.1599314},
  file      = {Lai05partSwarmDispNonSmth.pdf:Lai05partSwarmDispNonSmth.pdf:PDF},
  keywords  = {IEEE 30-bus system;economic dispatch;evolutionary programming;input-output characteristic functions;optimal power flow;particle swarm optimization;evolutionary computation;particle swarm optimisation;power generation dispatch;power generation economics;power system simulation;},
  owner     = {sotterson},
  timestamp = {2012.07.16},
}

@InProceedings{Wu99spinRsrvRampCnstr,
  author    = {Hui Wu and Gooi, H.B.},
  title     = {Optimal scheduling of spinning reserve with ramp constraints},
  booktitle = {Power Engineering Society Winter Meeting, IEEE},
  year      = {1999},
  volume    = {2},
  pages     = {785--790},
  month     = jan,
  abstract  = {This paper proposes an effective method to schedule spinning reserve optimally. It initially employs Lagrangian relaxation technique to obtain the commitment schedule by relaxing the demand constraint, spinning reserve constraint and every unit ramp rate constraint. Using the forward dispatch, dispatch modification and backward dispatch, a generation schedule which satisfies the spinning reserve requirement and the ramp rate limits is obtained. This schedule is updated by the probabilistic reserve assessment to meet a given risk index. The optimal value of the risk index is selected based on the tradeoff between the total unit commitment schedule cost and the expected cost of energy not served. Finally, a unit decommitment technique is incorporated to solve the problem of reserve over-commitment in Lagrangian relaxation based unit commitment. The algorithm is shown to be efficient and the results are near optimal judging from the simulation runs performed on the extended IEEE reliability test system.},
  comment   = {Explains how ramp constraints are probababilistically used in spinning reserves allocation},
  doi       = {10.1109/PESW.1999.747264},
  file      = {Wu99spinRsrvRampCnstr.pdf:Wu99spinRsrvRampCnstr.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  keywords  = { Lagrangian relaxation technique; backward dispatch; demand constraint relaxation; dispatch modification; expected cost of energy not served; extended IEEE reliability test system; forward dispatch; generation schedule; optimal scheduling; probabilistic reserve assessment; ramp constraints; reserve over-commitment; spinning reserve; spinning reserve constraint relaxation; total unit commitment schedule cost; unit decommitment technique; unit ramp rate constraint relaxation; power generation dispatch; power generation economics; power generation scheduling; probability; relaxation;},
  owner     = {scot},
  timestamp = {2011.06.09},
}

@Article{Lu09UncorrMultiLinPCA,
  author    = {Haiping Lu and Plataniotis, K.N. and Venetsanopoulos, A.N.},
  title     = {Uncorrelated Multilinear Principal Component Analysis for Unsupervised Multilinear Subspace Learning},
  journal   = {Neural Networks, IEEE Transactions on},
  year      = {2009},
  volume    = {20},
  number    = {11},
  pages     = {1820--1836},
  abstract  = {This paper proposes an uncorrelated multilinear principal component analysis (UMPCA) algorithm for unsupervised subspace learning of tensorial data. It should be viewed as a multilinear extension of the classical principal component analysis (PCA) framework. Through successive variance maximization, UMPCA seeks a tensor-to-vector projection (TVP) that captures most of the variation in the original tensorial input while producing uncorrelated features. The solution consists of sequential iterative steps based on the alternating projection method. In addition to deriving the UMPCA framework, this work offers a way to systematically determine the maximum number of uncorrelated multilinear features that can be extracted by the method. UMPCA is compared against the baseline PCA solution and its five state-of-the-art multilinear extensions, namely two-dimensional PCA (2DPCA), concurrent subspaces analysis (CSA), tensor rank-one decomposition (TROD), generalized PCA (GPCA), and multilinear PCA (MPCA), on the tasks of unsupervised face and gait recognition. Experimental results included in this paper suggest that UMPCA is particularly effective in determining the low-dimensional projection space needed in such recognition tasks.},
  comment   = {PCA which works in original N-dim space (with adjacency, etc.) by successive projections onto tensors (each component is associated with N tensor vectors). The components are uncorrelated, as in the usual PCA. Works well on 3D gait recognition, probably by throwing away a lot of detail. It captures far less variance than the alternative PCA or multidim methods because it is constrained by the tensor compression. Has MATLAB.

Uses?
1. compressing wind vector fields, dims would be (lat,lon,speedU,speedV) ?
2. compressing spline basis projections:
 -- for an expansion to N splines the matrix for a point would be: e(i1,i2,...iN)
 -- and you would have T samples of these points?
 -- how do this for multidims

* author says future work includes how to order the tensors, etc.
* related to spatial PCA: Demsar13spatialPCA
* related to: Brand02IncrSVDmissDat, which like multilinear PCA, is related to tucker decomposition (I have not figured this out yet)
* for SVD and PLS, see: Zhao11MultiLinTensorSVDandPLS
* tensor selection relation to: Cichocki14TensorNetworksBigOpt ?
* see author's book here

* sped up here: Althoff12OnlnTnsrFactPCA (but maybe this is not uncorrelated)

Talk:
http://videolectures.net/icml08_lu_ump/?ar=1412704963
Matlab:
www.mathworks.com/matlabcentral/fileexchange/35432-uncorrelated-multilinear-principal-component-analysis--umpca-
Matlab for author's previous Multilinear PCA algorithm (they're not uncorrelated):
www.mathworks.com/matlabcentral/fileexchange/26168-multilinear-principal-component-analysis--mpca-},
  doi       = {10.1109/TNN.2009.2031144},
  file      = {Lu09UncorrMultiLinPCA.pdf:Lu09UncorrMultiLinPCA.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.07},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5272374},
}

@Article{Castronuovo04optimDailyWindHydro,
  author    = {Castronuovo, E.D. and Lopes, J.A.P.},
  title     = {On the optimization of the daily operation of a wind-hydro power plant},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2004},
  volume    = {19},
  number    = {3},
  pages     = {1599--1606},
  month     = aug,
  issn      = {0885-8950},
  abstract  = {This paper proposes the utilization of water storage ability to improve wind park operational economic gains and to attenuate the active power output variations due to the intermittence of the wind-energy resource. An hourly-discretized optimization algorithm is proposed to identify the optimum daily operational strategy to be followed by the wind turbines and the hydro generation pumping equipments, provided that a wind-power forecasting is available. The stochastic characteristics of the wind power are exploited in the approach developed in order to identify an envelope of recommended operational conditions. Three operational conditions were analyzed and the obtained results are presented and discussed.},
  comment   = {Wind + hydro with daily operation bounds. Simple, I think.},
  doi       = {10.1109/TPWRS.2004.831707},
  file      = {Castronuovo04optimDailyWindHydro.pdf:Castronuovo04optimDailyWindHydro.pdf:PDF;Castronuovo04optimDailyWindHydro.pdf:Castronuovo04optimDailyWindHydro.pdf:PDF},
  groups    = {DOE-PNL09, Use, doReadNonWPV_2, StochOpt},
  keywords  = { optimisation, power generation economics, pumped-storage power stations, stochastic processes, wind power, wind power plants, wind turbines discretized optimization algorithm, energy storage, hydrogeneration pumping equipment, stochastic process, wind energy resource, wind turbines, wind-hydro power plant},
  owner     = {sotterson},
  timestamp = {2009.03.03},
}

@Article{Bermudez07holtWintersAddPer,
  author    = {Berm{\'u}dez, Jos{\'e} D and Segura, Jose V and Vercher, Enriqueta},
  title     = {Holt--Winters forecasting: an alternative formulation applied to UK air passenger data},
  journal   = {Journal of Applied Statistics},
  year      = {2007},
  volume    = {34},
  number    = {9},
  pages     = {1075--1090},
  abstract  = {This paper provides a formulation for the additive Holt?Winters forecasting procedure that simplifies both obtaining maximum likelihood estimates of all unknowns, smoothing parameters and initial conditions, and the computation of point forecasts and reliable predictive intervals. The stochastic component of the model is introduced by means of additive, uncorrelated, homoscedastic and Normal errors, and then the joint distribution of the data vector, a multivariate Normal distribution, is obtained. In the case where a data transformation was used to improve the fit of the model, cumulative forecasts are obtained here using a Monte-Carlo approximation. This paper describes the method by applying it to the series of monthly total UK air passengers collected by the Civil Aviation Authority, a long time series from 1949 to the present day, and compares the resulting forecasts with those obtained in previous studies.},
  comment   = {Holt-Winters w/ additive periodicity (the airplane model). Confidence intervals and transforms covered. Nuts and bolts are described well.

The attached paper is a technote from 2005},
  doi       = {10.1080/02664760701592125},
  file      = {2005 Tech Note:Bermudez07holtWintersAddPer_TechNote.pdf:PDF;Bermudez07holtWintersAddPer.pdf:Bermudez07holtWintersAddPer.pdf:PDF},
  publisher = {Taylor \& Francis},
}

@InProceedings{Baccald07genParDirCoher,
  author    = {Baccald, L.A. and de Medicina, F.},
  title     = {Generalized Partial Directed Coherence},
  booktitle = {Digital Signal Processing, International Conference on},
  year      = {2007},
  pages     = {163--166},
  month     = jul,
  abstract  = {This paper re-examines the definition of partial directed coherence (PDC) which was recently introduced as a linear frequency-domain quantifier of the multivariate relationship between simultaneously observed time series for application in functional connectivity inference in neuroscience. The present reappraisal aims at improving PDC's performance under scenarios that involve severely unbalanced predictive modelling errors (innovations noise). The present modification turns out to be more robust in estimating imprecisions associated with finite time series samples.},
  comment   = {More noise robust, short sequence improvement over orig. partial directed coherence
* by the original PDC author: Baccala01parDirCoher
* argues against some kinda normalization that another author has proposed (do I have than paper already in my bibtex?)},
  doi       = {10.1109/ICDSP.2007.4288544},
  file      = {Baccald07genParDirCoher.pdf:Baccald07genParDirCoher.pdf:PDF;Baccald07genParDirCoher.pdf:Baccald07genParDirCoher.pdf:PDF},
  keywords  = {frequency-domain analysis, medical signal processing, neurophysiology, time seriesfunctional connectivity inference, generalized partial directed coherence, innovations noise, linear frequency-domain quantifier, neuroscience, time series, unbalanced predictive modelling errors},
  owner     = {sotterson},
  timestamp = {2009.03.16},
}

@InProceedings{Bessa10infoWindPow,
  author    = {Bessa, R.J. and Miranda, V. and Principe, J.C. and Botterud, A. and Wang, J.},
  title     = {Information theoretic learning applied to wind power modeling},
  booktitle = {International Joint Conference on Neural Networks (IJCNN)},
  year      = {2010},
  pages     = {1--8},
  abstract  = {This paper reports new results in adopting information theoretic learning concepts in the training of neural networks to perform wind power forecasts. The forecast "goodness"; is discussed under two paradigms: one is only concerned in measuring the deviation between the forecasted and realized values, the other is related with the value of the forecast in the electricity market for different agents. The results and conclusions are supported by a real case example.},
  comment   = {Neural net trained w/ info theory criteria optimizes for asymmetric cost function; pt frcst increases producer profit a little, may ease regulation.

Renyi's entropy
* A generalization of Shannon entropy (where alpha =1)
* is analytically useful for other problems

Parzen window PDF estimation
* needed for information theoretic stuff
* probability est. is sum of distances to other points.
-- I think it's a "kernel" density estimator approach?
-- Kind of makes sense: if lots of neighbors, then neighborhood is highly probable.
-- this is also kind of like Kraskov04EstMutInfKNN (which has a KNN distance measure)
-- Kraskov04EstMutInfKNN criticizes kernel density estimation: plug their approach into this paper?
* distance is "gaussian like"
* Gaussian convolution property simplifies Renyi's entropy expression, making things very tractable and kernel-like

Standard Neural net is trained with several info theory techniques

* interesting: include term for NWP oldness.
* Main trick was info. theory training
* other trick was to estimate up/down regulation costs due to errors and stick them into the training (related to Tryggvi's thing)

Some info techniques considered
* Maximum Correntropy Criterion (MCC)
* Minimum Error Entropy (MEE)
* Minimum Error Entropy with Fiducial Points (MEEF)
* Maximum Parametric Correntropy Criterion (MPCC)
-- good b/c can have asymmetric cost function w/ economic meaning
-- "error" feedback is function of true forecast target and a linear xform of the network output

Results
* Info techniques yield narrower anon dist., we more zero error counts. In terns of of NMA E, got better answers (but probably not for RMSE (doesn't say))
* MAE criteria loses the most $ but has smallest deviations; MPCC makes the most $ yet has slightly smaller deviation than MSE. But higher max deviation.

Also
* good review of machine learning techniques applied to wind power forecasting
* asymmetric error cost function a clue for spinning reserves scenario figure of merit?

See also: Yang09featSelMLPrand},
  file      = {Bessa10infoWindPow.pdf:Bessa10infoWindPow.pdf:PDF},
  issn      = {1098-7576},
  keywords  = {information theory, power markets, wind power, electricity market, information theoretic learning, wind power modeling},
  owner     = {scot},
  timestamp = {2010.11.24},
}

@Article{Bessa12adaptKernWndFrcst,
  author    = {Bessa, R.J. and Miranda, V. and Botterud, A. and Jianhui Wang and Constantinescu, E.M.},
  title     = {Time Adaptive Conditional Kernel Density Estimation for Wind Power Forecasting},
  journal   = {Sustainable Energy, IEEE Transactions on},
  year      = {2012},
  volume    = {3},
  number    = {4},
  pages     = {660--669},
  month     = oct,
  issn      = {1949-3029},
  abstract  = {This paper reports the application of a new kernel density estimation model based on the Nadaraya-Watson estimator, for the problem of wind power uncertainty forecasting. The new model is described, including the use of kernels specific to the wind power problem. A novel time-adaptive approach is presented. The quality of the new model is benchmarked against a splines quantile regression model currently in use in the industry. The case studies refer to two distinct wind farms in the United States and show that the new model produces better results, evaluated with suitable quality metrics such as calibration, sharpness, and skill score.},
  comment   = {How to adapt KDE.},
  doi       = {10.1109/TSTE.2012.2200302},
  file      = {Bessa12adaptKernWndFrcst.pdf:Bessa12adaptKernWndFrcst.pdf:PDF},
  keywords  = {estimation theory;load forecasting;power supply quality;regression analysis;splines (mathematics);wind power plants;Nadaraya-Watson estimator;United States;calibration;quality metric;spline quantile regression model;time adaptive conditional kernel density estimation model;wind farm;wind power uncertainty forecasting;Decision making;Forecasting;Kernel;Regression analysis;Uncertainty;Wind forecasting;Wind power generation;Decision-making;density estimation;kernel;time-adaptive;uncertainty;wind power forecasting},
  owner     = {sotterson},
  timestamp = {2014.02.26},
}

@Article{Lanzante84scrnRgrsnMonteC,
  author    = {Lanzante, John R},
  title     = {Strategies for assessing skill and significance of screening regression models with emphasis on {Monte Carlo} techniques},
  journal   = {Journal of climate and applied meteorology},
  year      = {1984},
  volume    = {23},
  number    = {10},
  pages     = {1454--1458},
  abstract  = {This paper reviews the considerations in evaluating the skill and significance of screening multiple linear regression (SMLR) models. Fonnulations and procedures are given along with relevant references to prior studies. Topics discussed include predictor selection, serial correlation, artificial skill, true skill, and Monte Carlo significance testing. New results with wide applicability in the assessment of SMLR model skiD and significance are presented in graphical fonn. However, the results are restricted to situations involving predictors which are independent of one another and are serially uncorrelated. The methodology presented is suggested for use in both model evaluation and experimental design.},
  comment   = {Friendly discussion of multilinear feature selection and transformation (looks like PCA). Might be useful for some high dimensional pre-feature selection filter -- better than marginal correlations etc., athough you'd have to orthogonalize everything first (I think).

Tries to use standard significance apriori signficance and skill tests, but based on empirical lookup from montecarlo runs.

Pseudo parameters generated so can estimate model skill and significance:
* effective sample size
* artificial skill
*I true skill},
  file      = {Lanzante84scrnRgrsnMonteC.pdf:Lanzante84scrnRgrsnMonteC.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {American Meteorological Society},
  timestamp = {2013.07.08},
}

@Article{Hornik89mlpUnivApprox,
  author    = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
  title     = {Multilayer feedforward networks are universal approximators},
  journal   = {Neural Networks},
  year      = {1989},
  volume    = {2},
  number    = {5},
  pages     = {359--366},
  issn      = {0893-6080},
  abstract  = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  comment   = {Neural networks can approximate almost any function.},
  doi       = {DOI: 10.1016/0893-6080(89)90020-8},
  file      = {Hornik89mlpUnivApprox.pdf:Hornik89mlpUnivApprox.pdf:PDF},
  keywords  = {Feedforward networks},
  owner     = {scot},
  timestamp = {2011.05.10},
  url       = {http://www.sciencedirect.com/science/article/B6T08-485RHTR-5R/2/5d13881c0b9d8128fde7950a6f55849d},
}

@Article{Anahua08markovPowCurvWind,
  author    = {E. Anahua and St. Barth and J. Peinke},
  title     = {{Markovian} power curves for wind turbines},
  journal   = {Wind Energy},
  year      = {2008},
  volume    = {11},
  number    = {3},
  pages     = {219--232},
  abstract  = {This paper shows a novel method to characterize wind turbine power performance directly from high-frequency fluctuating measurements. In particular, we show how to evaluate the dynamic response of the wind turbine system on fluctuating wind speed in the range of seconds. The method is based on the stochastic differential equations known as the Langevin equations of diffusive Markov processes. Thus, the fluctuating wind turbine power output is decomposed into two functions: (i) the relaxation, which describes the deterministic dynamic response of the wind turbine to its desired operation state, and (ii) the stochastic force (noise), which is an intrinsic feature of the system of wind power conversion. As a main result, we show that independently of the turbulence intensity of the wind, the characteristic of the wind turbine power performance is properly reconstructed. This characteristic is given by their fixed points (steady states) from the deterministic dynamic relaxation conditioned for given wind speed values. The method to estimate these coefficients directly from the data is presented and applied to numerical model data, as well as to real-world measured power output data. The method is universal and is not only more accurate than the current standard procedure of ensemble averaging (IEC-61400-12) but it also allows a faster and robust estimation of wind turbines' power curves.},
  comment   = {Estimates power curve depending upon time path (dynamical). Uses Markov stuff. Neat},
  file      = {Anahua08markovPowCurvWind.pdf:Anahua08markovPowCurvWind.pdf:PDF;Anahua08markovPowCurvWind.pdf:Anahua08markovPowCurvWind.pdf:PDF},
  owner     = {scotto},
  timestamp = {2008.07.04},
  url       = {http://www3.interscience.wiley.com.offcampus.lib.washington.edu/journal/116317944/abstract},
}

@Article{GalvaoJr11dynPanQR,
  author    = {Galvao Jr, Antonio F},
  title     = {Quantile regression for dynamic panel data with fixed effects},
  journal   = {Journal of Econometrics},
  year      = {2011},
  volume    = {164},
  number    = {1},
  pages     = {142--157},
  abstract  = {This paper studies a quantile regression dynamic panel model with fixed effects. Panel data fixed effects estimators are typically biased in the presence of lagged dependent variables as regressors. To reduce the dynamic bias, we suggest the use of the instrumental variables quantile regression method of Chernozhukov and Hansen (2006) along with lagged regressors as instruments. In addition, we describe how to employ the estimated models for prediction. Monte Carlo simulations show evidence that the instrumental variables approach sharply reduces the dynamic bias, and the empirical levels for prediction intervals are very close to nominal levels. Finally, we illustrate the procedures with an application to forecasting output growth rates for 18 OECD countries.
Keywords
 Quantile regression;
 Dynamic panel;
 Fixed effects;
 Instrumental variables},
  comment   = {Dynamic (autoregressive) quantile regression which is less sensitive to initial conditions. Seems useful for short term quantile forecasts of wind power, etc, where past power is a lagged input.

* "panel data": multi-variate time series, it seems},
  doi       = {10.1016/j.jeconom.2011.02.016},
  file      = {GalvaoJr11dynPanQR.pdf:GalvaoJr11dynPanQR.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_1},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.03.30},
  url       = {http://www.sciencedirect.com/science/article/pii/S0304407611000443},
}

@Article{Lee12varyCoeffFlexGenrlz,
  author    = {Lee, Young K and Mammen, Enno and Park, Byeong U and others},
  title     = {Flexible generalized varying coefficient regression models},
  journal   = {The Annals of Statistics},
  year      = {2012},
  volume    = {40},
  number    = {3},
  pages     = {1906--1933},
  abstract  = {This paper studies a very ?exible model that can be used widely
to analyze the relation between a response and multiple covariates.
The model is nonparametric, yet renders easy interpretation for the
e?ects of the covariates. The model accommodates both continuous
and discrete random variables for the response and covariates. It is
quite ?exible to cover the generalized varying coe?cient models and
the generalized additive models as special cases. Under a weak con-
dition we give a general theorem that the problem of estimating the
multivariate mean function is equivalent to that of estimating its uni-
variate component functions. We discuss implications of the theorem
for sieve and penalized least squares estimators, and then investigate
the outcomes in full details for a kernel-type estimator. The kernel
estimator is given as a solution of a system of nonlinear integral
equations. We provide an iterative algorithm to solve the system of
equations and discuss the theoretical properties of the estimator and
the algorithm. Finally, we give simulation results.},
  comment   = {A varying coeff model where all vars can be either the coeff input or the coeff multplication. Is somehow shown to avoid the curse of dimensionality yet it includes all interaction terms.

Maybe the large num. interactions could be toned down w/ one of the varying coeff screening ideas in, say:
 Duvenaud11AddGausProcIntrct
or the other ones in energytop.org
 <<Feature selection for varying coefficient models>>},
  file      = {Lee12varyCoeffFlexGenrlz.pdf:Lee12varyCoeffFlexGenrlz.pdf:PDF},
  owner     = {sotterson},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2014.11.09},
  url       = {http://projecteuclid.org/euclid.aos/1350394521},
}

@TechReport{Fraley08lassoBMA,
  author      = {Daniel Percival and Chris Fraley},
  title       = {Model-Averaged LASSO},
  institution = {Insightful Corporation},
  year        = {2008},
  month       = aug,
  abstract    = {This paper studies combining `1 regularization and Bayesian Model Averaging (BMA) techniques.},
  comment     = {L1 and Bayesian Model Averaging, BMA},
  file        = {Fraley08lassoBMA.pdf:Fraley08lassoBMA.pdf:PDF;Fraley08lassoBMA.pdf:Fraley08lassoBMA.pdf:PDF},
  groups      = {Ensemble, doReadWPV_2},
  owner       = {sotterson},
  timestamp   = {2009.08.13},
  url         = {http://www.insightful.com/lars/tech-lars-bma.pdf},
}

@Article{Lei16stochMktClrWind,
  author    = {Lei, Ming and Zhang, Jin and Dong, Xiaodai and Jane, J Ye},
  title     = {Modeling the bids of wind power producers in the day-ahead market with stochastic market clearing},
  journal   = {Sustainable Energy Technologies and Assessments},
  year      = {2016},
  volume    = {16},
  pages     = {151--161},
  abstract  = {This paper studies optimal bidding decision for a strategic wind power producer participating in a day-ahead market that employs stochastic market clearing and energy and reserver co-optimization. The proposed procedure to derive strategic offers relies on a stochastic bilevel model: the upper level problem represents the profit maximization of the strategic wind power producer, while the lower level one represents the market clearing and the corresponding price formulation aiming to co-optimize both energy and reserve. Using the Karush?Kuhn?Tucker optimality condition for the lower level problem, this stochastic bilevel model is reformulated as a stochastic mathematical program with equilibrium constraints and solved using a suitable relaxation scheme. The effectiveness of the proposed method is demonstrated by two illustrative case studies.

Keywords
Strategic wind power producer; Bilevel model; Stochastic market clearing; Stochastic mathematical program with equilibrium constraints; Relaxation scheme},
  comment   = {Maybe a way to deal with market liquidity modeling?},
  file      = {Lei16stochMktClrWind.pdf:Lei16stochMktClrWind.pdf:PDF},
  publisher = {Elsevier},
  url       = {http://www.sciencedirect.com/science/article/pii/S2213138816300303},
}

@Article{Tu12MdlAvgPartEff,
  author    = {Tu, Yundong},
  title     = {Model Averaging PartiaL Effect ({MAPLE}) Estimation with Large Dimensional Data},
  journal   = {Guanghua School of Management, Peking University.},
  year      = {2012},
  month     = aug,
  abstract  = {This paper studies the estimation of the marginal effect of one economic variable on
another in the presence of large amount of other economic variables|a problem frequently
faced by applied researchers. The estimation is motivated via model uncertainty so that
random components should be included to describe the economy according to the state of
the world. A condition named \Conditional Mean Independence" is shown to be sucient
to identify the partial eect parameter of interest. In the case that the parameter of interest
can be identied in more than one approximating model, we propose two estimators
for such a parameter: generalized-method-of-moment-based model averaging partial eect
(gMAPLE) estimator and entropy-based model averaging partial effect (eMAPLE) estimator.
Consistency and asymptotic normality of the MAPLE estimators are established under
a suitable set of conditions. Thorough simulation studies reveal that MAPLE estimators
outperform factor based, variable selection based and other model averaging estimators
available in the literature. An economic example is taken to illustrate the use of MAPLE
estimator to evaluate the eect of inherited control on rms' performance.
Key Words: Partial Eect; Treatment Eect; Model Averaging; Bayesian Model Averaging;
Jackknife Model Averaging; FOGLeSs; Variable Selection; Factor Models; Inherited
Control.},
  comment   = {This is either feature selection of dimension reduction or both, and is somehow an entropy weighting scheme. Anyway, it's said to be a way to extend Tu13supervsdFctrFrcst},
  file      = {Tu12MdlAvgPartEff.pdf:Tu12MdlAvgPartEff.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.06.17},
  url       = {https://editorialexpress.com/cgi-bin/conference/download.cgi?db_name=FEMES12&paper_id=138},
}

@Article{Atkeson99LocalLrn,
  author    = {Atkeson, Christopher G and Moore, Andrew W and Schaal, Stefan},
  title     = {Locally weighted learning},
  journal   = {Artificial Intelligence Review},
  year      = {1999},
  abstract  = {This paper surveys locally weighted learning, a form of lazy learning and memorybased
learning, and focuses on locally weighted linear regression.The survey discusses distance
functions, smoothing parameters, weighting functions, local model structures, regularization
of the estimates and bias, assessing predictions, handling noisy data and outliers, improving
the quality of predictions by tuning fit parameters, interference between old and new data,
implementing locally weighted learning efficiently, and applications of locally weighted
learning. A companion paper surveys how locally weighted learning can be used in robot
learning and control.
Keywords: locally weighted regression, LOESS, LWR, lazy learning, memory-based learning,
least commitment learning, distance functions, smoothing parameters, weighting functions,
global tuning, local tuning, interference.},
  comment   = {Very friendly, pictorial intro to local learning and regression. Good background for local linear quantile regression.

I need to eventually add my comments here.},
  file      = {Atkeson99LocalLrn.pdf:Atkeson99LocalLrn.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2014.04.04},
  url       = {http://www.autonlab.org/autonweb/14691.html},
}

@Article{Chevillon07directMultiStep,
  author    = {Guillaume Chevillon},
  title     = {Direct Multi-Step Estimation and Forecasting},
  journal   = {Journal of Economic Surveys},
  year      = {2007},
  volume    = {21},
  number    = {4},
  pages     = {746},
  abstract  = {This paper surveys the literature on multi-step forecasting when the model or the estimation method focuses directly on the link between the forecast origin and the horizon of interest. Among diverse contributions, we show how the current consensual concepts have emerged. We present an exhaustive overview of the existing results, including a conclusive review of the circumstances favourable to direct multi-step forecasting, namely different forms of non-stationarity and appropriate model design. We also provide a unifying framework which allows us to analyse the sources of forecast errors and hence of accuracy improvements from direct over iterated multi-step forecasting. Keywords. Multi-step forecasting; Adaptive estimation; Varying horizon; Structural breaks; Non-stationarity},
  comment   = {Literature review: Model Stationarity and accurcacy as criteria for when to do direct instead of multi-step forecasts},
  doi       = {10.1002/for.970},
  file      = {Chevillon07directMultiStep.pdf:Chevillon07directMultiStep.pdf:PDF;Chevillon07directMultiStep.pdf:Chevillon07directMultiStep.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.09.29},
  url       = {http://www3.interscience.wiley.com.offcampus.lib.washington.edu/journal/112218235/abstract},
}

@Article{Li01neuralWindPowGen,
  author    = {Shuhui Li and Wunsch, D.C. and O'Hair, E.A. and Giesselmann, M.G.},
  title     = {Using neural networks to estimate wind turbine power generation},
  journal   = {Energy Conversion, IEEE Transactions on},
  year      = {2001},
  volume    = {16},
  number    = {3},
  pages     = {276--282},
  month     = sep,
  issn      = {1558-0059},
  abstract  = {This paper uses data collected at Central and South West Services Fort Davis wind farm (USA) to develop a neural network based prediction of power produced by each turbine. The power generated by electric wind turbines changes rapidly because of the continuous fluctuation of wind speed and direction. It is important for the power industry to have the capability to perform this prediction for diagnostic purposes-lower-than-expected wind power may be an early indicator of a need for maintenance. In this paper, characteristics of wind power generation are first evaluated in order to establish the relative importance for the neural network. A four input neural network is developed and its performance is shown to be superior to the single parameter traditional model approach},
  comment   = {MLP Learns individual turbine power curve based on wind speed/dir from two towers
* direction more important for lower wind speeds
* there's a small change in power due to air density but they ignore it
* simple 4-8-1 feedforward structure
* do wierd speed/direction remapping, claimed to make MLP learn faster

* power is also scaled to fraction of rated power -- I don't know why Performance
* Baseline is simple lookup table
-- using only one direction and velocity
-- turbine customization is a scalar coeff * NN
-- 4 inputs: speed and dir from two met towers
-- trained individually for each of 12 turbines in farm
* NN is much better than lookup table

Uses
* single turbine NN easier to train than complex one required to predict whole farm
* says this could be used for single turbine fault detection
* suggests that this, cascaded to recurrent NN which forecasts wind, could improve power prediction
-- has this ever been done?
-- IDEA: could instead use output hinting (hint would be true wind; output would be power; maybe need extra layer for power
* BPA NowCaster sub-hourly forecasts?},
  doi       = {10.1109/60.937208},
  file      = {Li01neuralWindPowGen.pdf:Li01neuralWindPowGen.pdf:PDF},
  groups    = {Read},
  keywords  = {neural nets, power engineering computing, wind power, wind power plants, wind turbinesdiagnostic purposes, four input neural network, maintenance, power industry, prediction performance, wind direction fluctuation, wind farm, wind speed fluctuation, wind turbine power generation estimation},
  owner     = {sotterson},
  timestamp = {2009.01.12},
}

@InProceedings{Keogh01derivDTW,
  author               = {Keogh, Eamonn J. and Pazzani, Michael J.},
  title                = {Derivative Dynamic Time Warping},
  booktitle            = {SIAM International Conference on Data Mining (SDM)},
  year                 = {2001},
  abstract             = {this paper we address both these problems by introducing a modification of DTW. The crucial difference is in the features we consider when attempting to find the correct warping. Rather than use the raw data, we consider only the (estimated) local derivatives of the data},
  citeulike-article-id = {5917372},
  citeulike-linkout-0  = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.6686},
  comment              = {Dynamic time warping of derivatives yields less crazy results than straight DTW - alternative to straight DTW in eg. Giorgino09dtwRpkg ? - highly cited on google scholar Main idea: - DTW fails because it's too sensitve to magnitude distortions - DTW constrains may make avoid big errors but prevent correct alignment - DDTW focuses on shape by calc'ing derivatives, and then aligning them w/ DTW - but you might really care about RMS difference (DTW), not the shape (DDTW) Tests suggest that DDTW - hallucinates fewer bad alignments on data that has mag distortions but no time distortions (real data) - is much better at getting correct alignment on artifically time-distorted data with mag. distortions - note: it's not explained if either DTW or test DDTW had constraints Anecdotally, I found that DDTW did indeed create fewer crazy alignments when I was trying to compare wind farm power measurements with forecasts (for one hour horizon; not sure about 24 hour). However, straight DDTW still tries too hard to align low power values, thus hiding the large mag. phase errors it correctly detected; would have to weight the distance by measurement mag or something. Note: might also want to try an L1 distance instead of the L2 distance used in this paper. These quick experiments were on first bit of data from ~/proj/dongSpatTemp},
  file                 = {Keogh01derivDTW.pdf:Keogh01derivDTW.pdf:PDF},
  groups               = {Read},
  owner                = {scot},
  posted-at            = {2009-10-09 19:57:29},
  timestamp            = {2010.07.20},
  url                  = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.6686},
}

@Article{Grigg99ieeeRelTstSys,
  author    = {Grigg, C. and Wong, P. and Albrecht, P. and Allan, R. and Bhavaraju, M. and Billinton, R. and Chen, Q. and Fong, C. and Haddad, S. and Kuruganty, S. and Li, W. and Mukerji, R. and Patton, D. and Rau, N. and Reppen, D. and Schneider, A. and Shahidehpour, M. and Singh, C.},
  title     = {The {IEEE} Reliability Test System-1996. A report prepared by the Reliability Test System Task Force of the Application of Probability Methods Subcommittee},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {1999},
  volume    = {14},
  number    = {3},
  pages     = {1010--1020},
  month     = aug,
  issn      = {0885-8950},
  abstract  = {This report describes an enhanced test system (RTS-96) for use in bulk power system reliability evaluation studies. The value of the test system is that it will permit comparative and benchmark studies to be performed on new and existing reliability evaluation techniques. The test system was developed by modifying and updating the original IEEE RTS (referred to as RTS-79 hereafter) to reflect changes in evaluation methodologies and to overcome perceived deficiencies},
  comment   = {Standard IEEE system for testing power system operations. May have generator ramp rates.

Recommended by Juan Mi, maybe for ramp stuff generating units described in paragraph on bottom of p. 1014:
* table 6: unit startup times
* table 10: cycling restrictions and ramp rates},
  doi       = {10.1109/59.780914},
  file      = {Grigg99ieeeRelTstSys.pdf:Grigg99ieeeRelTstSys.pdf:PDF},
  keywords  = {IEEE Reliability Test System;benchmark studies;bulk power system reliability evaluation;comparative studies;computer simulation;evaluation methodologies;power system analysis computing;power system reliability;},
  owner     = {scot},
  timestamp = {2011.05.09},
}

@TechReport{Jonsson10spinRsrv,
  author      = {Tryggvi J{\'o}nsson and Juan Miguel Morales Gonz\'{a}lez and Marco Zugno and Henrik Madsen and Scott Otterson},
  title       = {Allocation and Deployment of Spinning Reserves},
  institution = {Technical University of Denmark (DTU)},
  year        = {2010},
  month       = oct,
  abstract    = {This report introduces an optimization model to determine the reserve needs in a power system with a high penetration of wind power. The reserve required to accommodate the net load forecast error is estimated using a probabilistic criterion based on the expected load not served. The net load is defined as the total system demand minus the wind generation. The proposed reserve determination model is specially tailored to the Danish pool market organization, which comprises several power and energy trading arenas sequentially arranged from one day ahead to real time. The uncertainty associated with the system load and wind production is dealt with by means of the multivariate treatment of the forecast errors. Specifically, the cross-correlogram of load and wind power forecast errors is analyzed with a view to identifying a structure easily exploitable.},
  comment     = {First spining reserves simulation result; Tryggvi's first report, delivered to energinet.dk, I think. This is a net demand forecast.},
  file        = {Jonsson10spinRsrv.pdf:Jonsson10spinRsrv.pdf:PDF},
  groups      = {Use, doReadNonWPV_1},
  location    = {Lyngby, Denmark},
  owner       = {scot},
  timestamp   = {2010.11.05},
}

@TechReport{Vindtekknikk12stblClassWindPow,
  author      = {Kjeller Vindtekknikk},
  title       = {{NOR}SEWIND WP4.3: Study of the effect of temporal averaging of wind data for power production calculation},
  institution = {Kjeller Vindtekknikk},
  year        = {2012},
  number      = {KVT/ALL/2011/R011},
  month       = mar,
  abstract    = {This report is an independent contribution to work package 4.3 of the NORSEWIND research project.
Power production is calculated for six different weather situations. Different power production
estimates are calculated based on the same wind time series with different time averaging intervals for
each of the six weather conditions.
The work is intended to enlighten how the variability in the wind speed affects the power production of
the wind turbines. It is shown that the estimated power production depends on the averaging period of
the wind speed time series for different classes of average and variability of the wind speed. By
applying this knowledge for the power production prognosis it should further be possible to develop
functions which will give more precise forecasts for the different weather situations, determined , by
among other factors, the turbulence intensity of the wind speed.},
  comment     = {Suggests that weather turbulence intensity classes could help improve wind power forecast accuracy. Norsewind report from Kjeller Vindtekknikk (Erik Berg & colleagues).},
  file        = {Vindtekknikk12stblClassWindPow.pdf:Vindtekknikk12stblClassWindPow.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2014.10.24},
}

@TechReport{Andrade17ercotRsrvLessMore,
  author      = {Juan Andrade and Yingzhang Dong and Ross Baldick},
  title       = {Impact of Renewable Generation on Operational Reserves Requirements: When More Could be Less},
  institution = {University of Texas at Austin},
  year        = {2017},
  type        = {White Paper UTEI/2016-11-2},
  abstract    = {This report presents a qualitative and quantitative
description of the impact of renewable generation
on the requirements for ancillary services.
The fundamental concepts related to ancillary
services are presented in the report.  First, the
need for ancillary services is described, and
then the quantification of those needs to satisfy
reliability requirements.  Then pricing of those
services is described.  The quantitative part of

the report presents an analysis of the impact
of nodal protocol revisions as well as installed
generation on the procured ancillary services
in ERCOT using a statistical approach.  This
approach allowed correlations to be identified
between procured reserves, installed power, and
demand.  In addition, the approach allowed a
ranking of nodal protocol revisions according
to their impact on reserves procurements.},
  comment     = {Although installed wind power has significantly increased over time, reserve requirements have actually decreased.  This is much like in Hirth13balPowRnwblGerman

This is according to the (Evernoted) IEEE Spectrum article "More Renewable Energy Means More Operating Reserves, Right? Wrong" By Juan Andrade, Yingzhang Dong and Ross Baldick
Posted 23 Mar 2017.  http://www.evernote.com/l/AA3-i4Wv4zFMTKMk7a-OtFhqwXvxI8XwHq4/

Ramps: Ramping reserve is least well defined in [6], is only sold as flexi-ramp in CAISO.  This is relevant to IRPWIND WP82.3},
  file        = {Andrade17ercotRsrvLessMore.pdf:Andrade17ercotRsrvLessMore.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2017.03.26},
  url         = {http://energy.utexas.edu/files/2017/01/UTAustin_FCe_Ancillary_2016.pdf},
}

@TechReport{Hodge15ValImprvShrtWndFrcst,
  author      = {Hodge, Bri-Mathias and Florita, A and Sharp, J and Margulis, M and Mcreavy, D},
  title       = {The Value of Improved Short-Term Wind Power Forecasting},
  institution = {NREL/TP-5D00-63175. National Renewable Energy Laboratory, Golden, CO},
  year        = {2015},
  abstract    = {This report summarizes an assessment of improved short-term wind power forecasting in the
California Independent System Operator (CAISO) market and provides a quantification of its
potential value. Performed for the Lockheed Martin Corporation, the study was accomplished in
a technology-agnostic fashion to estimate savings from regulation and flex reserves, as well as
production savings, to provide insight into their product within the context of current and future
CAISO markets. A simulation approach was required with a design of experiment to capture
feasible operating points, and state-of-the-art modeling and Western Interconnection (western
United States) data were used to estimate realistic value streams. Two major scenarios were
considered: (1) a low wind scenario (SC4) with 8% wind penetration and (2) a high wind
scenario (SC3) with 25% wind penetration. The design of experiments consisted of short-term
(sub-hourly) wind power forecasting improvements of 0%, 10%, 25%, and 50% above the
current state of the art. Results show that cost savings from flex reserves are estimated to range
from $1.27 to $17.1 million. Cost savings from regulation reserves are estimated to range from
$0.917 to $12.7 million. Production cost savings are estimated to range from $2.87 to $116
million. Total cost savings from improved short-term wind power forecasting are estimated to
range from $5.05 to $146 million. The study results led to three main points for consideration:
(1) Economics are changed by commitments, and reliability is changed by the dispatch, so short-
term forecasting inherently fails to address the lion?s share of improvement opportunities; (2)
The results from the study are a strong function of gas prices, which are currently low but have
an uncertain future; and (3) From the low penetration scenario, results show that cost savings are
likely ?in the noise.? What follows from the study?s outcomes is the recommendation that within
the current CAISO market an investment in short-term wind power forecasting technologies
would be risky; however, within the next decade or two, wind penetration levels will be
significant enough to warrant investment in short-term wind power forecasting technologies. In
the near term, these technologies might be better suited for reliability issues and later evolve into
more dispatch-centric devices as wind penetration levels increase.},
  comment     = {In California, short term wind power forecasts improvements will not provide large dispatch cost savings but I think it's saying that it would be good for "security"; at wind penetrations of the next decade or two, it will be worth spending money better short term forecasting for dispatch.

But I don't understand how you spend for "security" without improving the "dispatch" forecast.  Maybe I need to read the report.},
  file        = {Hodge15ValImprvShrtWndFrcst.pdf:Hodge15ValImprvShrtWndFrcst.pdf:PDF},
  url         = {http://www.nrel.gov/docs/fy15osti/63175.pdf},
}

@TechReport{Nielsen07intelWindPowPred,
  author      = {H. A. Nielsen and P. Pinson and T. S. Nielsen and L. E. Christiansen and H. Madsen and G. Giebel and J. Badger and X. G. Larsen and H. Ravn and J. T{\o}fting and L. Voulund},
  title       = {Intelligent wind power prediction systems - final report},
  institution = {Informatics and Mathematical Modelling, Technical University of Denmark, {DTU}},
  year        = {2007},
  abstract    = {This report summarizes the results of the Danish PSO-project Intelligent wind power pre- diction systems. During the project further improvement and automation of tools for short term wind power forecasting, typically with horizons up to 48 hours, have been studied. Also, the use of such forecasts in bidding on the spot marked have been considered. It has been demonstrated that it is indeed possible to achieve further automation and robustness by modification of the methods underlying tools such as the Wind Power Prediction Tool. Initialisation of a statistically based power curve model using a physical model results in significant performance improvements for high wind speeds during the first months of operation. Presumably, performance for events which are even more rare, as e.g. cut-outs, can be improved generally or at least until sufficient data from the particular type of event are available. For a complex site it has been shown that stability indices can be used to improve the accuracy of wind speed predictions based on a simple statistical model. The stability indices are derived from the same operational forecasting model that provides the usual inputs for wind speed predictions. This improvement indicates the advantages of using more of the data available within the operation forecast models. The physical basis for the improvement given by the stability indices is revealed by mesoscale modelling. Mesoscale modelling is then used to provide corrections for wind speed predictions based on wind speed, direction and stability. These corrections improved the prediction accuracy. The mesoscale model results are verified using two different models. The adaptive non-parametric methods underlying modern forecasting tools require a small number of tuning parameters to be selected. Methods for continuous adjustment of these parameters have been developed. The criteria used for adapting these are directly related to the prediction performance. Also a robust version of the adaptive non-parametric estimation method has been developed. For data containing outliers, this method results in markedly more accurate estimation of the underlying power curve. Operational robustness is easily achieved using meteorological forecasts from two or three different suppliers. When more than one forecast is available, combined forecasting can be used. The improvement in performance which can be achieved depends on the variance and correlation of the meteorological forecast errors. For realistic values of these, significant improvements are observed. Finally, the type of criterion used during estimation or when forming a bid on the spot marked may have economic consequences for wind power producers. Also, it has been shown to have consequences at the level of the electricity system as whole in that it affects the requirement for regulating power.},
  comment     = {Exploration of improvements to WPPT:
Power curve estimator init:
* power curve model is conditional linear regression (make makes curves somehow?) followed by ARX
* run climate sim over some kinda power estimation algorithm and use that to train the adaptive power curve model.
* Works best if only run sim for a couple months and for high wind speeds (> 10m/s)

Better MOS
* linear MOS correction using NWP "stability parameters" works in some sites, not in others.
* transform matrices could be a function of wind speed and dir
* but recommend recursive adjustment of the stability param corrections


Tuning param selection (covered in Christiansen07autoTuneWind)

Optimization criteria other than least squared
* two possible goals: robust estimation and price sensitive cost functions

* turns out to be related to quantile regression
* Robust non-parametric regression
-- standard robustification: symmetrically deemphasize big errors in cost function
-- alternative: define a "proportion" of errors to be de-emphasized, let algorithm figure out howmuch of the + or - errors get shrunk
-- not clear that it gained much over the standard method; not visibiel if evanuatle based on normalized RMSE
* Asymmetric, cost-based bidding
-- idea is to avoid errors that cost more e.g. trying to underpromise while bidding instead of overpromising
-- asymmetric bidding reduces balancing costs by 2-7 \%
-- but it's not clear that they are actually doing this in the improved WPPT. The method isn't shown.

Forecast combination (ensembles)
* if combine 2 forecasts, get >= performance when forecasts are only somehwat correlated and when error stats are known
* If error stats are correct, combination can never be worse.
* observed correlations range from 0.45 to 0.80 in Denmark (and spain?) ==> 5-15\% accuracy improvement
* analytical expression for combining 2 forecasts; allusion to more than 2
* Recommend 2 or 3 forecasts w/ correlation < 0.8},
  file        = {Nielsen07intelWindPowPred.pdf:Nielsen07intelWindPowPred.pdf:PDF;Nielsen07intelWindPowPred.pdf:Nielsen07intelWindPowPred.pdf:PDF},
  groups      = {Read},
  location    = {Richard Petersens Plads, Building 321, {DK-}2800 Kgs. Lyngby},
  owner       = {sotterson},
  series      = {IMM-Technical Report-2007-14},
  timestamp   = {2009.01.05},
  url         = {http://www2.imm.dtu.dk/pubdb/p.php?5328},
}

@Article{Graham09missDatRealWorld,
  author               = {John W. Graham},
  title                = {Missing data analysis: making it work in the real world.},
  journal              = {Annual review of psychology},
  year                 = {2009},
  volume               = {60},
  number               = {1},
  pages                = {549--576},
  issn                 = {0066-4308},
  abstract             = {This review presents a practical summary of the missing data literature, including a sketch of missing data theory and descriptions of normal-model multiple imputation (MI) and maximum likelihood methods. Practical missing data analysis issues are discussed, most notably the inclusion of auxiliary variables for improving power and reducing bias. Solutions are given for missing data challenges such as handling longitudinal, categorical, and clustered data with normal-model MI; including interactions in the missing data model; and handling large numbers of variables. The discussion of attrition and nonignorable missingness emphasizes the need for longitudinal diagnostics and for reducing the uncertainty about the missing data mechanism under attrition. Strategies suggested for reducing attrition bias include using auxiliary variables, collecting follow-up data on a sample of those initially missing, and collecting data on intent to drop out. Suggestions are given for moving forward with research on missing data and attrition.},
  citeulike-article-id = {3978230},
  citeulike-linkout-0  = {http://arjournals.annualreviews.org/doi/abs/10.1146/annurev.psych.58.110405.085530},
  citeulike-linkout-1  = {http://dx.doi.org/10.1146/annurev.psych.58.110405.085530},
  citeulike-linkout-2  = {http://view.ncbi.nlm.nih.gov/pubmed/18652544},
  citeulike-linkout-3  = {http://www.hubmed.org/display.cgi?uids=18652544},
  comment              = {Overview of missing data, defining categories of missingness but not very well written. I only skimmed it.},
  doi                  = {10.1146/annurev.psych.58.110405.085530},
  file                 = {Graham09missDatRealWorld.pdf:Graham09missDatRealWorld.pdf:PDF},
  keywords             = {missing-data},
  owner                = {scot},
  posted-at            = {2009-01-29 11:08:33},
  timestamp            = {2010.09.09},
}

@Article{Madadi17diffFrcstBrwnSpln,
  author    = {Najmeh Madadi and Azanizawati Ma'aram and Kuan Yew Wong},
  title     = {A simulation-based product diffusion forecasting method using geometric Brownian motion and spline interpolation},
  journal   = {Cogent Business {\&} Management},
  year      = {2017},
  volume    = {4},
  number    = {1},
  month     = {mar},
  abstract  = {This study addresses the problem of stochasticity in forecasting diffusion of a new product with scarce historical data. Demand uncertainties are calibrated using a geometric Brownian motion (GBM) process. The spline interpolation (SI) method and curve fitting process have been utilized to obtain parameters of the constructed GBM-based differential equation over the product’s life cycle (PLC). The constructed stochastic differential equation is coded as the forecast model and is simulated using MATLAB. The results are several sample demand paths generated from simulation of the forecast model. To evaluate the forecasting performance of the proposed method it is compared with Holt’s model, using actual data from the semiconductor industry. The comparison results confirm the applicability of the proposed method in the semiconductor industry. The method can be helpful for policy-makers who require the prediction of uncertain demand over a time horizon, such as decisions associated with aggregate production planning, capacity planning, and supply chain network design. Especially for the semiconductor industry with intensive capital investment the proposed approach can be useful for making decisions associated with capacity allocation and expansion.
Keywords: demand forecast, stochastic differential equation, simulation, uncertainties, GBM, interpolation},
  doi       = {10.1080/23311975.2017.1300992},
  editor    = {Joseph Amankwah-Amoah},
  file      = {Madadi17diffFrcstBrw.pdf:PDF},
  publisher = {Informa {UK} Limited},
}

@Article{Bentzien13qntScrDecomp,
  author    = {Bentzien, Sabrina and Friederichs, Petra},
  title     = {Decomposition and graphical portrayal of the quantile score},
  journal   = {Quarterly Journal of the Royal Meteorological Society},
  year      = {2013},
  issn      = {1477-870X},
  abstract  = {This study expands the pool of verification methods for probabilistic weather and climate predictions by a decomposition of the quantile score (QS). The QS is a proper score function and evaluates predictive quantiles on a set of forecast-observation pairs. We introduce a decomposition of the QS in reliability, resolution and uncertainty, and discuss the biases of the decomposition. Further, a reliability diagram for quantile forecasts is presented. Verification with the QS and its decomposition is illustrated on precipitation forecasts derived from the mesoscale weather prediction ensemble COSMO-DE-EPS of the German Meteorological Service. We argue that the QS is ready to become as popular as the Brier score in forecast verification.},
  comment   = {New probabilistic forecast score (quantile score) w/ good CRPS-like properties but which can detect more forecast defects. DWD likes it, has R.

See also Bentzien14quantScrDecomp},
  doi       = {10.1002/qj.2284},
  file      = {Bentzien13qntScrDecomp.pdf:Bentzien13qntScrDecomp.pdf:PDF},
  groups    = {Test, doReadNonWPV_1},
  keywords  = {probabilistic forecasting, forecast verification, quantile score, score decomposition, reliability, resolution, ensemble forecasting},
  owner     = {sotterson},
  publisher = {John Wiley \& Sons, Ltd},
  timestamp = {2013.11.13},
}

@Article{Stephenson00frcstOddsRatio,
  author    = {Stephenson, David B.},
  title     = {Use of the "Odds Ratio" for Diagnosing Forecast Skill},
  journal   = {Weather and Forecasting},
  year      = {2000},
  volume    = {15},
  number    = {2},
  pages     = {221--232},
  month     = apr,
  issn      = {0882-8156},
  abstract  = {This study investigates ways of quantifying the skill in forecasts of dichotomous weather events. The odds ratio, widely used in medical studies, can provide a powerful way of testing the association between categorical forecasts and observations. A skill score can be constructed from the odds ratio that is less sensitive to hedging than previously used scores. Furthermore, significance tests can easily be performed on the logarithm of the odds ratio to test whether the skill is purely due to chance sampling. Functions of the odds ratio and the Peirce skill score define a general class of skill scores that are symmetric with respect to taking the complement of the event. The study illustrates the ideas using Finley?s classic set of tornado forecasts.},
  booktitle = {Weather and Forecasting},
  comment   = {doi: 10.1175/1520-0434(2000)015<0221:UOTORF>2.0.CO;2
Review:
Odds ratio as a performance metric for binary events, like tornadoes. Probably relevant to ramp alarms, if somebody wants to do that.},
  doi       = {10.1175/1520-0434(2000)015<0221:UOTORF>2.0.CO;2},
  file      = {Stephenson00frcstOddsRatio.pdf:Stephenson00frcstOddsRatio.pdf:PDF},
  groups    = {Test, doReadNonWPV_2},
  owner     = {sotterson},
  publisher = {American Meteorological Society},
  timestamp = {2013.10.23},
  url       = {http://dx.doi.org/10.1175/1520-0434(2000)015<0221:UOTORF>2.0.CO;2},
}

@Article{Kwan12varsSpatPVus,
  author    = {Kwan, Calvin Lee},
  title     = {Influence of local environmental, social, economic and political variables on the spatial distribution of residential solar PV arrays across the United States},
  journal   = {Energy Policy},
  year      = {2012},
  volume    = {47},
  pages     = {332--344},
  abstract  = {This study used ZIP code level data from the 2000 US Census to investigate the influence of local environmental, social, economic and political variables on the distribution of residential solar PV arrays across the United States. Current locations of residential solar PVs were documented using data from the National Renewable Energy Laboratory's Open PV project. A zero-inflated negative binomial reression model was run to evaluate the influence of selected variables. Using the same model, predicted residential solar PV shares were generated and illustrated using GIS software. The results of the model indicate solar insolation, cost of electricity and amount of available financial incentives are important factors influencing adoption of residential solar PV arrays. Results also indicate the Southwestern region of the United States and the state of Florida are currently underperforming in terms of number of housing units with solar PV installations},
  comment   = {Also, many good references on RHS of journal web page.},
  publisher = {Elsevier},
  url       = {https://www.sciencedirect.com/science/article/pii/S0301421512003795},
}

@TechReport{Roth13multivarTdist,
  author      = {Roth, Michael},
  title       = {On the multivariate t distribution},
  institution = {Link{\"o}ping University Electronic Press},
  year        = {2013},
  number      = {LiTH-ISY-R-3059},
  month       = apr,
  abstract    = {This technical report summarizes a number of results for the multivariate
t distribution which can exhibit heavier tails than the Gaussian distribu?
tion. It is shown how t random variables can be generated, the probability
density function (pdf) is derived, and marginal and conditional densities of
partitioned t random vectors are presented. Moreover, a brief comparison
with the multivariate Gaussian distribution is provided. The derivations of
several results are given in an extensive appendix.

Keywords: Student?s t distribution, heavy tails, Gaussian distribution.},
  comment     = {* student's t dist has heavier tails the Gaussian dist
* conditional can be computed
   (also see: http://freakonometrics.hypotheses.org/15120)
   and Ding16condMultivarTdist
* uncorrelated does not mean independent
* must be estimated w/ an EM algorithm (slow for high dim??)},
  file        = {Roth12multivarTdist.pdf:Roth13multivarTdist:PDF},
  owner       = {sotterson},
  publisher   = {Link{\"o}ping University Electronic Press},
  timestamp   = {2017.03.19},
  url         = {http://users.isy.liu.se/en/rt/roth/student.pdf},
}

@PhdThesis{Brunet10imgPhDwithSplnTut,
  author      = {Florent Brunet},
  title       = {Contributions to Parametric Image Registration and {3D} Surface Reconstruction},
  year        = {2010},
  abstract    = {This thesis deals with the modelling and the estimation of parametric functions in Computer Vision. It focuses
on three main topics: range surface fitting, image registration, and 3D reconstruction of smooth surfaces. In
addition to these three main topics, we consider other transversal elements. In particular, we focus on the
general problem caused by the hyperparameters in parametric model estimation, which includes regularization
problems. All these topics are related by a single objective which is nowadays one of the most important goals
in Computer Vision: the reconstruction of an arbitrary surface from images taken in an arbitrarily deforming
environment. This thesis can be divided into four main parts.
The first part deals with the basics. It includes background on optimization and on parameter estimation.
The problems related to the hyperparameters are also explained and illustrated. The rest of the thesis is centred
on our original contributions.
The second part of this thesis deals with the problem of fitting a surface to range data. This problem consists
in finding a parametric smooth surface that approximates accurately a sparse set of 3D points. We consider two
main problems. First, we propose methods to automatically tune the hyperparameters such as a regularization
weight. Second, we show how heteroskedastic noise may be handled. Heteroskedastic noise is an important
problem since it is typical of range sensors, for instance Time-of-Flight cameras.
The third part of this thesis is dedicated to the problem of image registration. We propose three contributions
in this topic. First, we present a new warp (image deformation function) able to correctly model the effect of
perspective projection. Second, we show how to solve an important problem that typically happens in direct
image registration: the problem of the region of interest. Third, we propose a new framework to estimate in
a reliable way the hyperparameters needed in feature-based image registration (threshold of an M-estimator,
regularization weight, number of control points, etc).
The last part of this thesis deals with the problem of reconstructing an inextensible surface from a monocular
sequence of images. We also use the hypothesis that a reference shape is known. Using only the motion cue,
the problem is ill-posed but, nonetheless, satisfying and plausible results can be obtained. We propose two new
formulations to reconstruct the surface: the first one reconstruct a sparse set of points using a second order
cone program, and the second one reconstruct a smooth parametric surface using a least-squares minimization
problem.},
  comment     = {Thesis with nice intro section on splines. See bookmarks. Also has a good web site: http://www.brnt.eu/phd/node11.html\#SECTION00632500000000000000},
  file        = {Brunet10imgPhDwithSplnTut.pdf:Brunet10imgPhDwithSplnTut.pdf:PDF},
  institution = {Universit?e d?Auvergne},
  owner       = {sotterson},
  timestamp   = {2014.11.10},
  url         = {http://www.brnt.eu/phd/node11.html#SECTION00632500000000000000},
}

@Article{Rebours07survVoltAncSrvc_I,
  author   = {Y. G. Rebours and D. S. Kirschen and M. Trotignon and S. Rossignol},
  title    = {A Survey of Frequency and Voltage Control Ancillary Services mdash;Part I: Technical Features},
  journal  = {IEEE Transactions on Power Systems},
  year     = {2007},
  volume   = {22},
  number   = {1},
  pages    = {350--357},
  month    = feb,
  issn     = {0885-8950},
  abstract = {This two-part paper surveys the frequency and voltage control ancillary services in power systems from various parts of the world. In this first part, the nomenclature used to describe active power reserves across 11 systems is first reviewed in order to facilitate the comparison of frequency control ancillary services. The essential technical features of frequency and voltage control ancillary services are then described. Finally, the technical requirements adopted in eight jurisdictions (North America, continental Europe, Germany, France, Spain, the Netherlands, Belgium, and Great Britain) are compared. The companion paper surveys the economic features of these ancillary services},
  comment  = {See also Rebours07survCtlAncSrvII},
  doi      = {10.1109/TPWRS.2006.888963},
  file     = {:Rebours07survVoltAncSrvc_I.pdf:PDF},
  keywords = {frequency control, power system control, power system economics, voltage control, active power reserves, economic features, frequency-voltage control ancillary services, Automatic generation control, Europe, Frequency control, Legged locomotion, Power generation, Power generation economics, Power system control, Power system economics, Power systems, Voltage control, Ancillary services, frequency control, reactive power, spinning reserve, system services, voltage control},
}

@TechReport{Mitra08scenGenStoch,
  author      = {Sovan Mitra},
  title       = {Scenario Generation for Stochastic Programming},
  institution = {OptiRisk Systems},
  year        = {2008},
  number      = {Domain: Finance Reference Number: OPT 004},
  month       = apr,
  abstract    = {This white paper surveys stochastic programming scenario generation methods. We introduce the basic concepts relating to scenario generation, the main scenario genera- tion methods -by sampling, simulation and statistical approaches. We also review new scenario generation methods such as "hybrid" methods and a review of basic stochastic programming theory can be found in the appendices.},
  comment     = {Big review of scenario generation methods. Not super great but coverage is broad. Mostly finance.},
  file        = {Mitra08scenGenStoch.pdf:Mitra08scenGenStoch.pdf:PDF},
  groups      = {Read},
  owner       = {scot},
  timestamp   = {2010.11.23},
  url         = {http://www.optirisk-systems.com/papers/opt004.pdf},
}

@Article{Marquez13solarFrcstSatGrndNN,
  author    = {Marquez, Ricardo and Pedro, Hugo TC and Coimbra, Carlos FM},
  title     = {Hybrid solar forecasting method uses satellite imaging and ground telemetry as inputs to ANNs},
  journal   = {Solar Energy},
  year      = {2013},
  volume    = {92},
  pages     = {176--188},
  abstract  = {This work describes a new hybrid method that combines information from processed satellite images with Artificial Neural Networks
(ANNs) for predicting global horizontal irradiance (GHI) at temporal horizons of 30, 60, 90, and 120 min. The forecast model is applied
to GHI data gathered from two distinct locations (Davis and Merced) that represent well the geographical distribution of solar irradiance
in the San Joaquin Valley. The forecasting approach uses information gathered from satellite image analysis including velocimetry and
cloud indexing as inputs to the ANN models. To the knowledge of the authors, this is the first attempt to hybridize stochastic learning
and image processing approaches for solar irradiance forecasting. We compare the hybrid approaches using standard error metrics to
quantify the forecasting skill for the several time horizons considered.

Keywords: Solar forecasting; Hybrid methods; Stochastic learning; Remote sensing; Artificial neural networks},
  comment   = {Irradiance 30-120min forecasting NN features: satellite cloud tracking and cloud "indexing". Hybrid between "stochastic learning" and image processing.

Could maybe use physcial inspired features or const funcs, like in Karpatne17physNNlakeT},
  file      = {Marquez13solarFrcstSatGrndNN.pdf:Marquez13solarFrcstSatGrndNN.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2015.02.19},
  url       = {http://www.sciencedirect.com/science/article/pii/S0038092X13000881},
}

@Article{Vlachos06structPeriodicMeas,
  author               = {Vlachos, Michail and Yu, Philip and Castelli, Vittorio and Meek, Christopher},
  title                = {Structural Periodic Measures for Time-Series Data},
  journal              = {Data Mining and Knowledge Discovery},
  year                 = {2006},
  volume               = {12},
  number               = {1},
  pages                = {1--28},
  month                = jan,
  issn                 = {1384-5810},
  abstract             = {This work motivates the need for more flexible structural similarity measures between time-series sequences, which are based on the extraction of important periodic features. Specifically, we present non-parametric methods for accurate periodicity detection and we introduce new periodic distance measures for time-series sequences. We combine these new measures with an effective metric tree index structure for efficiently answering k-Nearest-Neighbor queries. The goal of these tools and techniques are to assist in detecting, monitoring and visualizing structural periodic changes. It is our belief that these methods can be directly applicable in the manufacturing industry for preventive maintenance and in the medical sciences for accurate classification and anomaly detection.},
  citeulike-article-id = {514366},
  comment              = {Does periodic waveform clustering, could maybe call it a regime clusterer. They plan to extend it to an online method (see conclusions)},
  doi                  = {10.1007/s10618-005-0016-4},
  file                 = {Vlachos06structPeriodicMeas.pdf:Vlachos06structPeriodicMeas.pdf:PDF;Vlachos06structPeriodicMeas.pdf:Vlachos06structPeriodicMeas.pdf:PDF},
  owner                = {sotterson},
  posted-at            = {2006-02-21 13:11:32},
  publisher            = {Springer},
  timestamp            = {2009.03.13},
}

@InProceedings{SantaEulalia11discrChoiceBassEV,
  author       = {Santa Eulalia, Luis Antonio and Neumann, Donald and Klasen, J{\"o}rg},
  title        = {A simulation-based innovation forecasting approach combining the bass diffusion model, the discrete choice model and system dynamics-an application in the german market for electric cars},
  booktitle    = {The third international conference on advances in system simulation},
  year         = {2011},
  organization = {Citeseer},
  abstract     = {This work presents a novel simulation-based 
forecasting approach combining concepts from the Bass 
Diffusion Model and the Discrete Choice Model from a System 
Dynamics perspective. The proposed approach allows for the 
forecasting of the adoption rate and its timing, by 
understanding the underlying preferences of individual 
customers and social forces influencing it. A real-scale 
preliminary application in the German market for electric 
cars, parameterized through a Conjoint Analysis, is provided. 
Simulation results indicate that battery charging technology 
and infrastructures are crucial for the success of electric cars 
in Germany. 
Keywords—Forecasting Innovation; System Dynamics; Bass 
Diffusion Model; Discrete Choice Model; Conjoint Analysis; 
Electric Vehicles (EV); German Electric Car Market.},
  comment      = {Sounds similar to Zarwi17discChoiceAdoptDiffTranspo},
  file         = {:SantaEulalia11discrChoiceBassEV.pdf:PDF},
  url          = {https://www.semanticscholar.org/paper/A-Simulation-Based-Innovation-Forecasting-Approach-Santa-Eulalia-Neumann/521a46d7fa4d6f092158986c9f2a4c8871d34b61},
}

@InCollection{Hennig99clustLinRgrsn,
  author    = {Hennig, C.},
  title     = {Models and Methods for Clusterwise Linear Regression},
  booktitle = {Classification in the Information Age},
  publisher = {Springer Berlin Heidelberg},
  year      = {1999},
  editor    = {Gaul, Wolfgang and Locarek-Junge, Hermann},
  series    = {Studies in Classification, Data Analysis, and Knowledge Organization},
  pages     = {179--187},
  isbn      = {978-3-540-65855-9},
  abstract  = {Three models for linear regression clustering are given, and corresponding methods for classification and parameter estimation are developed and discussed: The mixture model with fixed regressors (ML-estimation), the fixed partition model with fixed regressors (ML-estimation), and the mixture model with random regressors (Fixed Point Clustering). The number of clusters is treated as unknown. The approaches are compared via an application to Fisher?s Iris data. By the way, a broadly ignored feature of these data is discovered.},
  comment   = {Input partition via linear regression relationhips. Easy to implement? Can pick num. clusters.},
  doi       = {10.1007/978-3-642-60187-3_17},
  file      = {Hennig99clustLinRgrsn.pdf:Hennig99clustLinRgrsn.pdf:PDF},
  language  = {English},
  owner     = {sotterson},
  timestamp = {2014.03.07},
}

@Article{Denny07benGridWind,
  author    = {Denny, E. and O'Malley, M.},
  title     = {Quantifying the Total Net Benefits of Grid Integrated Wind},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2007},
  volume    = {22},
  number    = {2},
  pages     = {605--615},
  month     = may,
  issn      = {0885-8950},
  abstract  = {Throughout the world, significant development is being encouraged in wind energy for electricity generation. A complete cost benefit analysis has been conducted in this paper on the impacts of grid connected wind generation. A dispatch model is used to determine the dispatches upon which the costs and benefits can be ascertained for different wind capacities, plant mixes and loads. These costs and benefits are then used to generate net benefit curves for wind generation and the level where the costs exceed the benefits is established. These penetration levels can then be used by policy makers to determine the appropriate quantity of wind generation to promote. A large number of assumptions are tested and it was found that increased interconnection, high CO2 prices and a flexible plant mix are particularly beneficial for wind generation, and that there are positive net benefits for wind energy penetrations of 17\% and higher under the chosen set of assumptions for the test system},
  comment   = {Related to unreliable transmission and wind? I haven't read it. E. Denny refs it in her slides},
  doi       = {10.1109/TPWRS.2007.894864},
  file      = {Denny07benGridWind.pdf:Denny07benGridWind.pdf:PDF;Denny07benGridWind.pdf:Denny07benGridWind.pdf:PDF},
  keywords  = {cost-benefit analysis, power generation economics, power grids, power system interconnection, wind power plantscost benefit analysis, electricity generation, grid connected wind generation, grid integrated wind energy, policy makers, total net benefits},
  owner     = {sotterson},
  timestamp = {2009.05.27},
}

@Misc{Wikipedia15tikhreg,
  author   = {Wikipedia},
  title    = {Tikhonov regularization},
  month    = dec,
  year     = {2015},
  abstract = {Tikhonov regularization, named for Andrey Tikhonov,
is the most commonly used method of regularization of
ill-posed problems. In statistics, the method is known
as ridge regression, and with multiple independent dis-
coveries, it is also variously known as the Tikhonov?
Miller method, the Phillips?Twomey method, the con-
strained linear inversion method, and the method of
linear regularization. It is related to the Levenberg?
Marquardt algorithm for non-linear least-squares prob-
lems.},
  comment  = {Tikhonov regularization is a superset of ridge regression, and seems to be related to the regularization method used in psplines.  Read this (and other articles) to see if theres' a nice anlytical way to do multivariate spline smoothing.

Article takes a forward/inverse problem perspective
* realworld phenomena tend to be low pass filters
   ==> inverse process is high pass:  a noise amplifier
   ==> something about eigenvalues but I didn't understand this
           but see: Hansen08RegularizationToolsManualV4
* ordinary least squares solution creates a nullspace that can't be recovered on inverse (I think it says)

Ridge Regression
* penalty term,  as always in Tikhonov Regularization, is:  norm2(gamma*Coeffs)
* For ridge, gamma=alpha*I, where I is an identity matrix
* analytical solution for coefficients given at top of page 1 (but alpha must be chosen)

Tikhonov Regularization
* a superset of Ridge Regression
* penalty matrix is can be anything e.g. filter or Fourier coeffs to enforce smoothness
* but could also have terms that allow step changes e.g. at 0 or max wind power (point densities there, if regularizing densities)

Generalized Tikhonov Regularization
* Assume various terms have multivariate normal distribution
* analytical expressions, if the covariance matrices are known

SVD v.s. Tikhonov Reg (and condition number)
* ridge regression coeffs easily computed from SVD
* shows how Ridge regression affects matrix condition number
* Can also solve for "generalized case" coeffs
 - does this mean gamma doesn't have to be alpha*I ?
 - use generalized SVD to do this, not ordinary SVD
 - Hansen08RegularizationToolsManualV4 talks about a solution using Generalized SVD
   (but there, it seems possible to avoid GSVD, and therefore get degrees of freedom as normal?)

Tikhonov Reg v.s. Weiner coeffs
* a simple relationship shown

Leave-one-out cross-validation
* minimizes (squared summed error) / (squared degrees of freedom)
* why is that good?

Bayesian interpretation
* Tikhonov Reg. solution is most possible if have normal assumptions on data an coeffs

THREE QUESTIONS
1. is the "generalized case" gamma != alpha * I?
  Yes, this is what Elden77AlgRegIllCond calls it.
2. what are the degrees of freedom v.s. eigenvalues for "generalized case"?
   want degrees of freedom for CV (Cardinal15ridgeLambdaGrid)
3. what is it good that CV minimimzed err^2/degOfFree^2 ?

Some of these answers may come from the Grace Wahba paper cited},
  file     = {Wikipedia printout:Wikipedia15tikhReg.pdf:PDF},
  groups   = {Read},
  url      = {https://en.wikipedia.org/wiki/Tikhonov_regularization},
}

@InProceedings{Lemire07alterPieceLinSeg,
  author               = {Lemire, Daniel},
  title                = {A better alternative to piecewise Linear Time Series Segmentation},
  booktitle            = {SIAM International Conference on Data Mining (SDM)},
  year                 = {2007},
  month                = may,
  abstract             = {Time series are unstructured data; they are difficult to monitor, summarize and predict. Weather forecasts, stock market prices, medical data (ECG, EEG) are examples of non-stationary time series we wish to clean, classify and index. Segmentation organizes time series into few intervals having uniform characteristics (flatness, linearity, modality, monotonicity and so on). The popular piecewise linear model can determine where the data goes up or down and at what rate. Unfortunately, when the data does not follow a linear model, the computation of the local slope creates overfitting. We propose an adaptive time series model where the polynomial degree of each interval vary (flat, linear and so on). Given a number of regressors, the cost of each interval is its polynomial degree: flat intervals cost 1 regressor, linear intervals cost 2 regressors, and so on. Our goal is to minimize the Euclidean (l\_2) error. We present an optimal algorithm running in time O(n^2) as well as an online (O(n)) top-down heuristic. Over synthetic random walks, historical stock market prices, and electrocardiograms, the adaptive model provides a more accurate segmentation and is a better predictor of missing data points (leave-one-out cross-validation error). In other words, we simultaneously improve the goodness-of-fit and reduce local overfitting.},
  citeulike-article-id = {678820},
  comment              = {Segment a time series into constant and sloped line segments
* segments described by polynomials of the 0\textsuperscript{th} and 1\textsuperscript{st} order
* the mixed model overfits less than algs. that don't have the 0\textsuperscript{th} order option
* Python, C++ and Matlab available: http://www.daniel-lemire.com/fr/abstracts/SDM2007.html},
  eprint               = {cs.DB/0605103},
  file                 = {Lemire07alterPieceLinSeg.pdf:Lemire07alterPieceLinSeg.pdf:PDF;Lemire07alterPieceLinSeg.pdf:Lemire07alterPieceLinSeg.pdf:PDF},
  keywords             = {segmentation, time\_series},
  owner                = {sotterson},
  posted-at            = {2008-06-01 18:03:45},
  timestamp            = {2009.03.09},
  url                  = {http://arxiv.org/abs/cs.DB/0605103},
}

@Article{Calderon10psplineDerivSDE,
  author    = {Calderon, Christopher P and Martinez, Josue G and Carroll, Raymond J and Sorensen, Danny C},
  title     = {P-splines using derivative information},
  journal   = {Multiscale Modeling \& Simulation},
  year      = {2010},
  volume    = {8},
  number    = {4},
  pages     = {1562--1580},
  abstract  = {Time series associated with single-molecule experiments and/or simulations contain a wealth of
multiscale information about complex biomolecular systems. We demonstrate how a collection of
Penalized-splines (P-splines) can be useful in quantitatively summarizing such data. In this work,
functions estimated using P-splines are associated with stochastic differential equations (SDEs). It
is shown how quantities estimated in a single SDE summarize fast-scale phenomena, whereas
variation between curves associated with different SDEs partially reflects noise induced by motion
evolving on a slower time scale. P-splines assist in ?semiparametrically? estimating nonlinear
SDEs in situations where a time-dependent external force is applied to a single-molecule system.
The P-splines introduced simultaneously use function and derivative scatterplot information to
refine curve estimates. We refer to the approach as the PuDI (P-splines using Derivative
Information) method. It is shown how generalized least squares ideas fit seamlessly into the PuDI
method. Applications demonstrating how utilizing uncertainty information/approximations along
with generalized least squares techniques improve PuDI fits are presented. Although the primary
application here is in estimating nonlinear SDEs, the PuDI method is applicable to situations
where both unbiased function and derivative estimates are available.},
  comment   = {Penalized splines with Matlab. Also does something with stochastic differential equations (SDEs)

QR-based spline solver described in: Calderon09PpsplineStableQR (QR linear algebra, not probability QR)

Matlab and demo files: http://www.caam.rice.edu/tech_reports/2009/PuDI_demo_mfiles.zip
I've downloaded this to Eweline\frcst\common\contrib\PSQR},
  doi       = {10.1137/090768102},
  file      = {Calderon10psplineDerivSDE.pdf:Calderon10psplineDerivSDE.pdf:PDF},
  owner     = {sotterson},
  publisher = {SIAM},
  timestamp = {2014.10.30},
}

@InProceedings{Oliveira16copulaEnsANNfrcstTS,
  author    = {R. T. A. de Oliveira and T. F. O. de Assis and P. R. A. Firmino and T. A. E. Ferreira and A. L. I. Oliveira},
  title     = {Copulas-based ensemble of Artificial Neural Networks for forecasting real world time series},
  booktitle = {Proc. Int. Joint Conf. Neural Networks (IJCNN)},
  year      = {2016},
  pages     = {4089--4096},
  month     = jul,
  abstract  = {Time series combined forecasters have been superior to the respective single models in statistical terms. In this way, the linear combination functions, e.g. the simple average (SA) and the minimal variance (MV) approaches, have been the main alternatives for aggregation in the literature. In this work, it is proposed a copulas-based method for combining biased single models. Copulas are multivariate functions that operate on marginal probability distributions, allowing one to model the forecasters errors and then the dependence among them: a typical divide-and-conquer framework that can result in nonlinear accurate combined forecasters. The performance of the copulas-based combination method is assessed by means of a comparison with SA and MV models, based on two financial time series.},
  comment   = {For forecast aggregation and other copula forecasting.},
  doi       = {10.1109/IJCNN.2016.7727732},
  file      = {:Oliveira16copulaEnsANNfrcstTS.pdf:PDF},
  keywords  = {divide and conquer methods, neural nets, probability, time series, ANN, MV approaches, SA approaches, artificial neural networks, copulas-based ensemble, divide-and-conquer framework, financial time series, marginal probability distributions, minimal variance approaches, multivariate functions, real world time series forecasting, simple average approaches, statistical terms, Biological system modeling, Computational modeling, Forecasting, Mathematical model, Maximum likelihood estimation, Predictive models, Time series analysis},
}

@InCollection{Bernecker11QualSimRank,
  author    = {Bernecker, Thomas and Houle, Michael E and Kriegel, Hans-Peter and Kr{\"o}ger, Peer and Renz, Matthias and Schubert, Erich and Zimek, Arthur},
  title     = {Quality of similarity rankings in time series},
  booktitle = {Advances in Spatial and Temporal Databases},
  publisher = {Springer},
  year      = {2011},
  pages     = {422--440},
  abstract  = {Time series data objects can be interpreted as high- dimensional vectors, which allows the application of many traditional
distance measures as well as more specialized measures. However, many distance functions are known to suffer from poor contrast
in high-dimensional settings, putting their usefulness as similarity measures into question. On the other hand, shared-nearest-neighbor
distances based on the ranking of data objects induced by some primary distance measure have been known to lead to improved
performance in high-dimensional settings. In this paper, we study the performance of shared-neighbor similarity measures in
the context of similarity search for time series data objects. Our findings are that the use of shared-neighbor similarity
measures generally results in more stable performances than that of their associated primary distance measures.},
  comment   = {maybe useful for local linear QR},
  doi       = {10.1007/978-3-642-22922-0_25},
  file      = {Slides:Bernecker11QualSimRank_Slides.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.03.28},
}

@PhdThesis{Lemke10frcstTimeSeriesComboThesis,
  author      = {Lemke, Christiane},
  title       = {Combinations of Time Series Forecasts: When and Why Are They Beneficial?},
  year        = {2010},
  abstract    = {Time series forecasting has a long track record in many application areas. In forecas-
ting research, it has been illustrated that finding an individual algorithm that works
best for all possible scenarios is hopeless. Therefore, instead of striving to design
a single superior algorithm, current research efforts have shifted towards gaining a
deeper understanding of the reasons a forecasting method may perform well in some
conditions whilst it may fail in others. This thesis provides a number of contribu-
tions to this matter. Traditional empirical evaluations are discussed from a novel
point of view, questioning the benefit of using sophisticated forecasting methods
without domain knowledge. An own empirical study focusing on relevant off-the-
shelf forecasting and forecast combination methods underlines the competitiveness
of relatively simple methods in practical applications. Furthermore, meta-features of
time series are extracted to automatically find and exploit a link between application
specific data characteristics and forecasting performance using meta-learning. Fi-
nally, the approach of extending the set of input forecasts by diversifying functional
approaches, parameter sets and data aggregation level used for learning is discussed,
relating characteristics of the resulting forecasts to different error decompositions for
both individual methods and combinations. Advanced combination structures are
investigated in order to take advantage of the knowledge on the forecast generation
processes.
Forecasting is a crucial factor in airline revenue management; forecasting of the
anticipated booking, cancellation and no-show numbers has a direct impact on gene-
ral planning of routes and schedules, capacity control for fareclasses and overbooking
limits. In a collaboration with Lufthansa Systems in Berlin, experiments in the the-
sis are conducted on an airline data set with the objective of improving the current
net booking forecast by modifying one of its components, the cancellation forecast.
To also compare results achieved of the methods investigated here with the current
state-of-the-art in forecasting research, some experiments also use data sets of two
recent forecasting competitions, thus being able to provide a link between academic
research and industrial practice.},
  comment     = {Thesis on time series forecast combination, adaptive, too, I think. Paper for Shuo Chen?},
  file        = {Lemke10frcstTimeSeriesComboThesis.pdf:Lemke10frcstTimeSeriesComboThesis.pdf:PDF},
  institution = {Bournemouth University},
  url         = {http://dec.bournemouth.ac.uk/staff/bgabrys/publications/C_Lemke_PhD_thesis.pdf},
}

@Article{Fu11tseriesClustRev,
  author    = {Fu, Tak-chung},
  title     = {A review on time series data mining},
  journal   = {Engineering Applications of Artificial Intelligence},
  year      = {2011},
  volume    = {24},
  number    = {1},
  pages     = {164--181},
  abstract  = {Time series is an important class of temporal data objects and it can be easily obtained from scientific and financial applications. A time series is a collection of observations made chronologically. The nature of time series data includes: large in data size, high dimensionality and necessary to update continuously. Moreover time series data, which is characterized by its numerical and continuous nature, is always considered as a whole instead of individual numerical field. The increasing use of time series data has initiated a great deal of research and development attempts in the field of data mining. The abundant research on time series data mining in the last decade could hamper the entry of interested researchers, due to its complexity. In this paper, a comprehensive revision on the existing time series data mining research is given. They are generally categorized into representation and indexing, similarity measure, segmentation, visualization and mining. Moreover state-of-the-art research issues are also highlighted. The primary objective of this paper is to serve as a glossary for interested researchers to have an overall picture on the current time series data mining development and identify their potential research direction to further investigation.},
  comment   = {Somewhat highly cited (151 on GS)},
  file      = {Fu11tseriesClustRev.pdf:Fu11tseriesClustRev.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.04.24},
  url       = {http://www.sciencedirect.com/science/article/pii/S0952197610001727},
}

@Article{Hemmelmann09timeVarDirbrain,
  author   = {D. Hemmelmann and M. Ungureanu and W. Hesse and T. Wstenberg and J.R. Reichenbach and O.W. Witte and H. Witte and L. Leistritz},
  title    = {Modelling and analysis of time-variant directed interrelations between brain regions based on BOLD-signals},
  journal  = {Neuroimage},
  year     = {2009},
  volume   = {45},
  number   = {3},
  pages    = {722--737},
  issn     = {1053-8119},
  abstract = {Time-variant Granger Causality Index (tvGCI) was applied to simulated and measured BOLD signals to investigate the reliability of time-variant analysis approaches for the identification of directed interrelations between brain areas on the basis of fMRI data.

Single-shot fMRI data of a single image slice with short repetition times (200 ms, 16000 frames/subject, 64 ? 64 voxels) were acquired from 5 healthy subjects during an externally-driven, self-paced finger-tapping paradigm (57?59 single taps for each subject). BOLD signals were derived from the pre-supplementary motor area (preSMA), the supplementary motor area (SMA), and the primary motor cortex (M1).

The simulations were carried out by means of a Dynamic Causal Modelling (DCM) approach. The tvGCI as well as time-variant Partial Directed Coherence (tvPDC) were used to identify the modelled connectivity network (connectivity structure ? CS ? of the DCM). Different CSs were applied by using dynamic systems (Generalized Dynamic Neural Network ? GDNN) and trivariate autoregressive (AR) processes. The influence of the low-pass characteristics of the simulated hemodynamic response (Balloon model) and of the measuring noise was tested. Additionally, our modelling strategy considered ?spontaneous? BOLD fluctuations before, during, and after the appearance of the event-related BOLD component. Couplings which were extracted from the simulated signals were statistically evaluated (tvGCI for shuffled data, confidence tubes for tvGCI courses). We demonstrate that connections of our CS models can be correctly identified during the event-related BOLD component and with signal-to-noise-ratios corresponding to those of the measured data. The results based on simulations can be used to examine the reliability of connectivity identification based on BOLD signals by means of time-variant as well as time-invariant connectivity measures and enable a better interpretation of the analysis results using fMRI data.

A readiness-BOLD response was only detected in one subject. However, in two subjects a strong time-variant connection (tvGCI) from preSMA to SMA was observed 3 s before the tapping was executed. This connection was accompanied by a weaker rise of the tvGCI from preSMA to M1. These preceding interrelations were confirmed in the other subjects by the dynamics of tvGCI courses. Based on the results of tvGCI analysis, the time-evolution of an individual connectivity network is shown for each subject.},
  comment  = {For causality identification, the Time-variant Granger Causality Index (tvGCI) seems to be an alternative to partial directed coherence. But I haven't read this carefully enough to know if it was found to be a good one.},
  doi      = {DOI: 10.1016/j.neuroimage.2008.12.065},
  keywords = {Self-paced finger tapping},
  url      = {http://www.sciencedirect.com/science/article/B6WNP-4VCH6WN-9/2/283d37baa604c5eef9cf4afc114a0bce},
}

@InProceedings{Niu16ordinalMultiOutCNNage,
  author    = {Z. Niu and M. Zhou and L. Wang and X. Gao and G. Hua},
  title     = {Ordinal Regression with Multiple Output {CNN} for Age Estimation},
  booktitle = {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},
  year      = {2016},
  pages     = {4920--4928},
  month     = jun,
  abstract  = {To address the non-stationary property of aging patterns, age estimation can be cast as an ordinal regression problem. However, the processes of extracting features and learning a regression model are often separated and optimized independently in previous work. In this paper, we propose an End-to-End learning approach to address ordinal regression problems using deep Convolutional Neural Network, which could simultaneously conduct feature learning and regression modeling. In particular, an ordinal regression problem is transformed into a series of binary classification sub-problems. And we propose a multiple output CNN learning algorithm to collectively solve these classification sub-problems, so that the correlation between these tasks could be explored. In addition, we publish an Asian Face Age Dataset (AFAD) containing more than 160K facial images with precise age ground-truths, which is the largest public age dataset to date. To the best of our knowledge, this is the first work to address ordinal regression problems by using CNN, and achieves the state-of-the-art performance on both the MORPH and AFAD datasets.},
  doi       = {10.1109/CVPR.2016.532},
  file      = {Niu16ordinalMultiOutCNNage.pdf:Niu16ordinalMultiOutCNNage.pdf:PDF},
  keywords  = {face recognition, feature extraction, feedforward neural nets, image classification, learning (artificial intelligence), regression analysis, AFAD, AFAD datasets, Asian face age dataset, MORPH datasets, age estimation, age ground-truths, aging pattern nonstationary property, binary classification subproblems, deep convolutional neural network, end-to-end learning approach, feature extraction, feature learning, multiple output CNN learning algorithm, ordinal regression, regression model learning, Aging, Estimation, Face, Feature extraction, Measurement, Support vector machines, Training data},
  owner     = {sotterson},
  timestamp = {2017.03.08},
}

@InProceedings{Yu14extrmValGrpMdlThinMemb,
  author    = {Hang Yu and Jingjing Cheng and Dauwels, J.},
  title     = {Extreme-value graphical models with multiple covariates},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on},
  year      = {2014},
  pages     = {4553--4557},
  abstract  = {To assess the risk of extreme events such as hurricanes and floods, it is crucial to develop accurate extreme-value statistical models. Extreme events often display heterogeneity, varying continuously with a number of covariates. Previous studies have suggested that models considering covariate effects lead to reliable estimates of extreme value distributions. In this paper, we develop a novel model to incorporate the effects of multiple covariates. Specifically, we analyze as an example the extreme sea states in the Gulf of Mexico, where the distribution of extreme wave heights changes systematically with location and wind direction. The block maxima at each location and sector of wind direction are assumed to follow the Generalized Extreme Value (GEV) distribution. The GEV parameters are coupled across the spatio-directional domain through a graphical model, particularly, a multidimensional thin-membrane model. Efficient learning and inference algorithms are then developed based on the special characteristics of the thin-membrane model. Numerical results for both synthetic and real data indicate that the proposed model can accurately describe marginal behavior of extreme events.},
  comment   = {potentially high dim extreme value predictor based on graphical models, "thin membranes," copulas. Is supposed to handle high dims better than Jonathan14nonStatCondXtrmPenSpln

Seems like a good starting point, along w/ Jonathan14 b/c it continues all the themes I've been working on.

Gong14extrmQRvineCopula seems to do similar w/ vine copulas},
  doi       = {10.1109/ICASSP.2014.6854464},
  file      = {Yu14extrmValGrpMdlThinMemb.pdf:Yu14extrmValGrpMdlThinMemb.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.04.17},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6854464},
}

@Unpublished{Koenker10bayesBanoQR,
  author    = {Koenker, Roger},
  title     = {{Bayes} In The Ba{\~n}o: Some Snarky Remarks On {Bayes}ian Quantile Regression},
  year      = {2010},
  abstract  = {To bake a Bayesian (posterior) I was taught that you needed an L (likelihood) and
a p (prior), so it comes as something of a shock to discover that there are 15,200 web documents employing the phrase Bayesian quantile regression. Quantile regression would seem to be the very antithesis of a likelihood based procedure, committing the investigator to a parametric model for one paltry conditional quantile function, while professing ignorance, even indifference, about the rest of the Deus ex machina.},
  comment   = {Koenker's sarcastic critique of Bayesian methods that estimate over ranges of taus to avoid crossover in individually estimated quantiles. He says you can just use a Sandwich Estimator to do the crossover correction. He doesn't mention it, but his quantreg R package also includes the sorting based crossover fixing method from Chernozhukov10qrNoCross

The URL for this entry points to a web page with several other critiques of Bayesian QR.},
  file      = {Koenker10bayesBanoQR.pdf:Koenker10bayesBanoQR.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.04.21},
  url       = {http://www.econ.uiuc.edu/~roger/research/bayesqr/bayesqr.html},
}

@Unpublished{Knudsen12stochCtlOSRnord,
  author    = {Torben Knudsen},
  title     = {Notes on the Stochastic Control Problem in {OSR}Nordic},
  month     = mar,
  year      = {2012},
  abstract  = {To obtain some understanding of the stochastic control problem (SCP) in OSRNordic some simplified problems are discussed below. The idea is to investigate at least:
* Simplified versions of the SCP.
* Where does the stochastic enter?
* Are there known methods to solve this?
* Is it difficult to solve?},
  comment   = {Possibilities for simplifying the stochastic optimization problem for the OSRNordic I worked on at DTU.},
  file      = {Knudsen12stochCtlOSRnord.pdf:Knudsen12stochCtlOSRnord.pdf:PDF},
  groups    = {Use, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2013.10.11},
}

@InProceedings{Hirose12NNRMLknnLocLin,
  author    = {Hirose, H. and Soejima, Y. and Hirose, K.},
  title     = {NNRMLR: A Combined Method of Nearest Neighbor Regression and Multiple Linear Regression},
  booktitle = {Advanced Applied Informatics (IIAIAAI), 2012 IIAI International Conference on},
  year      = {2012},
  pages     = {351--356},
  abstract  = {To predict the continuous value of target variable using the values of explanation variables, we often use multiple linear regression methods, and many applications have been successfully reported. However, in some data cases, multiple linear regression methods may not work because of strong local dependency of target variable to explanation variables. In such cases, the use of the k nearest-neighbor method (k-NN) in regression can be an alternative. Although a simple k-NN method improves the prediction accuracy, a newly proposed method, a combined method of k-NN regression and the multiple linear regression methods (NNRMLR), is found to show prediction accuracy improvement. The NNRMLR is essentially a nearest-neighbor method assisted with the multiple linear regression for evaluating the distances. As a typical useful example, we have shown that the prediction accuracy of the prices for auctions of used cars is drastically improved.},
  comment   = {Global regression+dimReduce --> KNN distance function --> prediction from average of the y values of the k NN's, as estimated by that distance. Might somehow be a way of doing high dim local linear quantile regression.

* knn regression. kNN multivariate regression kNN w/ distance calculation being weighed by feature importance, as estimated by a global linear regression (usually regularized).
* It's not local linear regression; it's kNN "average value" regression with an unusual distance function for finding the k NN's. regulation)
* It's good because
 - KNN distance is aware of importance (correlation) between input dimensions and outputs
 - still local b/c of KNN
* Bad because it
 - doesn't know if correlation is positive or negative
 - doesn't estimate a function, so need a lot of points for KNN to be good
 - the correlation is global, not local

Can this be turned around?
* so that a function that's aware of the input distance can be produced,
* so that the importance is local
* seems like you need to do PLSR or something...
* Zhang12rflctLocLinKNN seems closer

ALGORITHM

Linear regulations tried
* none, just MLR
* stepwise (this is just feature slection, I think)
 -- best?
* ridge
* lasso
* elastic net
 -- best?
* adaptive lasso
 -- not shown but results said to be about the same as elastic net
 -- BUT, it used only about 1/3 of the features, so it would seem to be best

* KNN regression distance functions
 - used to pick the KNN's, whose y values are averaged to produce a prediction
 - distance is weighted Euclidean, with weight for each dime being the imporance in a global linear regression
 -- if the global linear regression coeff for a dim is big, then a difference in that dim is important
 -- so distance gets bigger if important coeffs are big; unimportant coeffs don't affect distance
 -- global regression means this is less local than you'd think, although KNN would be local
 - coeffs can be zeroed by lasso, ridge, etc. feature selection

* Interesting experiments in chosing the right ratio of test-to-train;
 -- they say it's about 70/30 train/test
 -- 90/10 gets better RMS in bootstrap tests, but I think they worried about stability on real data
* best (EL) method had three tunable parameters, 2 in elastic net, and then another for overall weight

* for high dims, KNN distance could also be the p-Gaussian, as in Francois05locKernHiDim

RESULTS (On 26 dim car price estimator problem)
 RMSE(MLL)
 ~ RMSE(regularized MLR)
 >> RMSE(KNN)
 > RMSE(regularized MLR KNN)

Howto use with QUANTILE REGRESSION?
* KNN weight coefficients are quantile regression coeffs (maybe L1-norm penalized, as in a couple papers I have in here)
* then do KNN somehow (but are predicting a local quantile. Huh.). Maybe look at Yu99quantregLocLinKNN},
  doi       = {10.1109/IIAI-AAI.2012.76},
  file      = {Hirose12NNRMLknnLocLin.pdf:Hirose12NNRMLknnLocLin.pdf:PDF},
  groups    = {Read, PointDerived, doReadNonWPV_2},
  owner     = {sotterson},
  timestamp = {2014.03.28},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337221},
}

@Article{Ahlstrom05futureWindForecastUtil,
  author    = {Ahlstrom and Jones, L. and Zavadil, R. and Grant, W.},
  title     = {The future of wind forecasting and utility operations},
  journal   = {IEEE Power and Energy Magazine},
  year      = {2005},
  volume    = {3},
  number    = {6},
  pages     = {57--64},
  month     = nov,
  issn      = {1540-7977},
  abstract  = {To run the power system most efficiently and effectively, we must focus our attention on the system as a whole rather than on individual components - and this is true regardless of whether or not the system includes wind generation. As larger amounts of wind energy are added to our power systems, we must certainly be aware of the impacts of its variability on the system, but we must also keep in mind that our power systems are already designed to deal with substantial amounts of uncertainty and variability. By making proper use of wind forecasting and integrating it into our control rooms and our systems, we can facilitate the cost-effective use of wind energy as a mainstream component of the energy system.},
  comment   = {* forecast accuracy gains possible by aggregating many plant powers

* maybe useful for optimization},
  doi       = {10.1109/MPAE.2005.1524621},
  file      = {Ahlstrom05futureWindForecastUtil.pdf:Ahlstrom05futureWindForecastUtil.pdf:PDF;Ahlstrom05futureWindForecastUtil.pdf:Ahlstrom05futureWindForecastUtil.pdf:PDF},
  keywords  = { load forecasting, power generation planning, wind power plants control rooms, power system, utility operations, wind energy cost-effective use, wind forecasting, wind generation},
  owner     = {sotterson},
  timestamp = {2009.01.14},
}

@Electronic{Asari05rgrsnSvdRidgeInfo,
  author       = {Hiroki Asari},
  month        = sep,
  year         = {2005},
  title        = {Technical Notes on Linear Regression and Information Theory},
  howpublished = {Web Page},
  organization = {Watson School of Biological Sciences},
  url          = {http://zadorlab.cshl.edu/asari/lineardecode.html},
  abstract     = {To understand how the brain processes sensory information, it is important to study the relationship between input stimuli and the output neural responses. Neuroscientists have typically looked at two complementary aspects of neural representations. The first, and best studied, is the encoding process by which a stimulus is converted by the nervous system into neural activity. Less studied is the decoding process, by which experimenters attempt to use neural activity to reconstruct the stimulus that evoked it. To characterize these processes, various methods have been developed to model the stimulus-response functions and to test their performance [2].

Here we briefly overview the basics and logics of these methods. The first part reviews linear regression methods with a certain regularization to find the best linear models. In particular, we will go through how ridge regression is related to the singular value decomposition (for details: [4]). The second part shows how to apply information theory to test the quality of linear filters (for details: [1], [5]).

We will discuss the connection of correlation functions to entropy and information, and a way to compute information by exploiting SVD.},
  comment      = {Good explanation of SVD and linear/ridge regression. Combined dimension reduction and linear regression also yield upper and lower bound on mutual information! Computation done efficiently in Fourier domain.

Could also be used as a pre-feature selection algorithm for very high dimensional signals (a Gassian distribution assumption (or copula transform) could possibly be better than the common marginal correlation or mutual information prefilter). A good MI screen since Gaussian MI is a lower MI bound (see below).

Overview
* Expresses linear regression in terms of SVD
* Ridge regression has a simple relationship with SVD
-- maybe more computationally efficient or parallizable?
-- could toss out insignficant SVD components before doing ridge?
* Upper and lower mutual information bounds for a multivariate Gaussian process
-- upper bound by estimating the determinstic part of one variable and then computing MI
---- this is done by somehow repeating one of the variables, I didn't figure this out
-- lower bound determined by data processing inequality
---- Gaussian distribution has the maximum entropy for given mean and covariance
---- some weighting by marginal covariance matrices
-- is expended to account for autocorrelations making a lagged correlation matrix
-- Fourier domain trick makes it much more efficient for high dimensional signals
* somehow, MI this is related to SVD (probably via covaraince matrix) but I skipped reading this for the moment.

Missing features fill-in advantage?
-- SVD has nice ways to handle missing features; maybe more robust than usual ridge regression
-- compute missing features in such a way as to get max MI?
---- different than corrcoeff-based giant normal assumption as in Amelia, Honaker10missValTseries?
-- Fourier trick makes it more efficient, could also make it better for fill-in (other papers talk about fourier domain missing feature fill-in)
-- Wavelets might work too
---- Fourier is more efficeint b/c ortogonal, just like wavelets
---- Ortogonal wavelets could be lower dimensional too, depending upon the type of wavelet},
  file         = {Asari05rgrsnSvdRidgeInfo.pdf:Asari05rgrsnSvdRidgeInfo.pdf:PDF},
  groups       = {Read},
  location     = {Long Island, NY},
  owner        = {sotterson},
  timestamp    = {2013.07.05},
}

@Article{Boney16introSemiSupLadderBlog,
  author    = {Rinu Boney},
  title     = {Introduction to Semi-Supervised Learning with Ladder Networks},
  journal   = {GitHub Blog},
  year      = {2016},
  month     = jan,
  abstract  = {Today, deep learning is mostly about pure supervised learning. A major drawback of supervised learning is that it requires a lot of labeled data and It is quite expensive to collect them. So, deep learning in the future is expected to unsupervised, more human-like.

?We expect unsupervised learning to become far more important in the longer term. Human and animal learning is largely unsupervised: we discover the structure of the world by observing it, not by being told the name of every object.?
? LeCun, Bengio, Hinton, Nature 2015},
  comment   = {A not especially clear tutorial on ladder networks but it has pointers to code.  Slide source is Raiko15supUnsupCombLadder, which is probably better.

I think there are also other implementations out there.},
  file      = {Boney16introSemiSupLadderBlog.pdf:Boney16introSemiSupLadderBlog.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2017.02.01},
  url       = {http://rinuboney.github.io/2016/01/19/ladder-network.html},
}

@Article{Bossavy11rampEns,
  author    = {Bossavy, Arthur and Girard, Robin and Kariniotakis, George},
  title     = {Forecasting Ramps of Wind Power Production with Meteorological Ensembles},
  year      = {2011},
  url       = {http://mc.manuscriptcentral.com/we?URL_MASK=ncn4QQ3mq5PHr8Knk8Xs},
  abstract  = {Today, there is a growing concern in developping short-term wind power forecasting tools able to provide reliable
information about particular, so-called 'extreme' situations. One of them is the large and sharp variation of the production
a wind farm can experience within a few hours called ramp event. Developping forecast information specially dedicated
to ramps is of primary interest both because of the difficulties usual models have to predict them, and the potential risk
they represent in the management of a power system. Firstable, we propose a methodology to characterize ramps of wind
power production with a derivative filtering approach derived from the edge detection literature. Then, we investigate the
skill of meteorological ensembles to forecast ramps, in a deterministic way or with probability forecasts associated to
prediction intervals. Through conditioning the probability forecasts, we show the ability of ensembles to provide reliable
and situation-dependent estimations of the ramp occurence. Our study relies on 18 months of wind power measures from
a 8 MW wind farm located in France and forecasts ensemble of 51 members from the EPS system of ECMWF.

KEYWORDS
Wind power forecasting; extreme situations; ramp event; large-scale variations; meteorological ensembles},
  file      = {My Review Comments, first round:Bossavy11rampEns_MyReview.pdf:PDF},
  groups    = {Ensemble, doReadNonWPV_1},
  journal   = {Wind Energy},
  owner     = {scot},
  timestamp = {2011.05.11},
}

@InProceedings{Kim05onlineVarSel,
  author       = {Kim, S.P. and Rao, Y.N. and Erdogmus, D. and Principe, J.C.},
  title        = {Tracking of multivariate time-variant systems based on on-line variable selection},
  booktitle    = {Machine Learning for Signal Processing, IEEE},
  year         = {2005},
  pages        = {123--132},
  organization = {IEEE},
  abstract     = {Tracking time-variant systems has been of great interest in many engineering fields. Specifically, when system statistics change both in space (multivariate) and time with a short stationary regime, conventional adaptive algorithms suffer from the tradeoff between convergence rate and accuracy. In this paper, we propose a tracking system consisting of a linear adaptive system accompanied by an on-line variable selection algorithm that is based on the least angle regression algorithm. This algorithm explicitly employs local (in time) correlation between the input and the output of an unknown system to select a subset of input variables at every time step. Therefore, it enables the multivariate adaptive Wter to track the temporal changes of correlated variables. Simulations involving tracking of multi-channel timevariant systems demonstrate superior performance of the proposed approach when compared with the Conventional methods.},
  comment      = {Use for regime switching? Improve this algorithm with Singh10correntropyFilt or the other online correntropy paper?},
  file         = {Kim05onlineVarSel.pdf:Kim05onlineVarSel.pdf:PDF},
  issn         = {1551-2541},
  owner        = {scot},
  timestamp    = {2010.12.02},
}

@Article{FanaeeT16tensorAnomDetSrvy,
  author    = {Hadi Fanaee-T and Jo{\~a}o Gama},
  title     = {Tensor-based anomaly detection: An interdisciplinary survey},
  journal   = {Knowledge-Based Systems},
  year      = {2016},
  volume    = {98},
  pages     = {130 - 147},
  issn      = {0950-7051},
  abstract  = {Traditional spectral-based methods such as PCA are popular for anomaly detection in a variety of problems and domains. However, if data includes tensor (multiway) structure (e.g. space-time-measurements), some meaningful anomalies may remain invisible with these methods. Although tensor-based anomaly detection (TAD) has been applied within a variety of disciplines over the last twenty years, it is not yet recognized as a formal category in anomaly detection. This survey aims to highlight the potential of tensor-based techniques as a novel approach for detection and identification of abnormalities and failures. We survey the interdisciplinary works in which TAD is reported and characterize the learning strategies, methods and applications; extract the important open issues in TAD and provide the corresponding existing solutions according to the state-of-the-art.},
  comment   = {Mentions space-time matrices as an example of a tensor based field.  Besides being good for high dim anomaly detection, maybe this would be good for high dim covariance estimation, and upscaling.

Tensors for covarinance estimation: Li11tensoCovMatObjTrck},
  doi       = {http://dx.doi.org/10.1016/j.knosys.2016.01.027},
  file      = {:FanaeeT16tensorAnomDetSrvy.pdf:PDF},
  keywords  = {Anomaly detection, Tensor analysis, Multiway data, Tensor decomposition, Tensorial learning},
  owner     = {sotterson},
  timestamp = {2017.06.26},
  url       = {http://www.sciencedirect.com/science/article/pii/S0950705116000472},
}

@InProceedings{Pan08xfrLrnDimRed,
  author    = {Pan, Sinno Jialin and Kwok, James T and Yang, Qiang},
  title     = {Transfer Learning via Dimensionality Reduction.},
  booktitle = {AAAI},
  year      = {2008},
  volume    = {8},
  pages     = {677--682},
  abstract  = {Transfer learning addresses the problem of how to utilize
plenty of labeled data in a source domain to solve
related but different problems in a target domain, even
when the training and testing problems have different
distributions or features. In this paper, we consider
transfer learning via dimensionality reduction. To solve
this problem, we learn a low-dimensional latent feature
space where the distributions between the source domain
data and the target domain data are the same or
close to each other. Onto this latent feature space, we
project the data in related domains where we can apply
standard learning algorithms to train classification
or regression models. Thus, the latent feature space
can be treated as a bridge of transferring knowledge
from the source domain to the target domain. The main
contribution of our work is that we propose a new dimensionality
reduction method to find a latent space,
which minimizes the distance between distributions of
the data in different domains in a latent space. The effectiveness
of our approach to transfer learning is verified
by experiments in two real world applications: indoorWiFi
localization and binary text classification.},
  comment   = {A way to adapt bhe NWP models change, or maybe just general adaptation if have some kinda running SVD. Another way: Pardoe10BoostRgrsnXfer},
  file      = {Pan08xfrLrnDimRed.pdf:Pan08xfrLrnDimRed.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.07.24},
  url       = {http://www.aaai.org/Papers/AAAI/2008/AAAI08-108.pdf},
}

@InProceedings{Chen16xgboostSclbl,
  author       = {Chen, Tianqi and Guestrin, Carlos},
  title        = {{XGB}oost: A scalable tree boosting system},
  booktitle    = {Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  year         = {2016},
  pages        = {785--794},
  organization = {ACM},
  abstract     = {Tree boosting is a highly effective and widely used machine
learning method. In this paper, we describe a scalable end-
to-end tree boosting system called XGBoost, which is used
widely by data scientists to achieve state-of-the-art results
on many machine learning challenges. We propose a novel
sparsity-aware algorithm for sparse data and weighted quan-
tile sketch for approximate tree learning. More importantly,
we provide insights on cache access patterns, data compres-
sion and sharding to build a scalable tree boosting system.
By combining these insights, XGBoost scales beyond billions
of examples using far fewer resources than existing systems.

Keywords
Large-scale Machine Learning},
  comment      = {Friendlier intros in: Chen15higgsBoostTr},
  file         = {:Chen16xgboostSclbl.pdf:PDF},
}

@InProceedings{Frank11svdRankInfo,
  author       = {Frank, Mario and Buhmann, Joachim M},
  title        = {Selecting the rank of truncated SVD by Maximum Approximation Capacity},
  booktitle    = {Information Theory Proceedings (ISIT), 2011 IEEE International Symposium on},
  year         = {2011},
  pages        = {1036--1040},
  organization = {IEEE},
  abstract     = {Truncated Singular Value Decomposition (SVD) cal-
culates the closest rank-k approximation of a given input matrix.
Selecting the appropriate rank k defines a critical model order
choice in most applications of SVD. To obtain a principled
cut-off criterion for the spectrum, we convert the underlying
optimization problem into a noisy channel coding problem.
The optimal approximation capacity of this channel controls
the appropriate strength of regularization to suppress noise.
In simulation experiments, this information theoretic method to
determine the optimal ra},
  comment      = {Good for pca dim picking but with SVD, and PCA has an SVD in it..  Also and different approach than Minka00AutoPCAdim, I guess.},
  file         = {:Frank11svdRankInfo.pdf:PDF},
}

@Article{Courtney13frcstBayAvg,
  author    = {Courtney, Jennifer F and Lynch, Peter and Sweeney, Conor},
  title     = {High resolution forecasting for wind energy applications using {Bayes}ian model averaging},
  journal   = {Tellus A},
  year      = {2013},
  volume    = {65},
  abstract  = {Two methods of post-processing the uncalibrated wind speed forecasts from the European Centre for Medium- Range Weather Forecasts (ECMWF) ensemble prediction system (EPS) are presented here. Both methods involve statistically post-processing the EPS or a downscaled version of it with Bayesian model averaging (BMA). The first method applies BMA directly to the EPS data. The second method involves clustering the EPS to eight representative members (RMs) and downscaling the data through two limited area models at two resolutions. Four weighted ensemble mean forecasts are produced and used as input to the BMA method. Both methods are tested against 13 meteorological stations around Ireland with 1 yr of forecast/observation data. Results show calibration and accuracy improvements using both methods, with the best results stemming from Method 2, which has comparatively low mean absolute error and continuous ranked probability scores. Keywords: ensemble forecasting, BMA, calibration, probability distribution, verification},
  comment   = {Basic description of Bayesian model averaging, with recent application Perhaps an advancement of: Sweeney11frcstCombo},
  doi       = {10.3402/tellusa.v65i0.19669},
  file      = {Courtney13frcstBayAvg.pdf:Courtney13frcstBayAvg.pdf:PDF},
  groups    = {Ensemble, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2013.02.20},
  url       = {http://www.tellusa.net/index.php/tellusa/article/view/19669},
}

@Article{Cornillon08forcstPrincComp,
  author    = {Cornillon, P. -A. and Imam, W. and Matzner-L{\o}ber,, E.},
  title     = {Forecasting Time Series Using Principal Component Analysis with Respect to Instrumental Variables},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {2008},
  volume    = {52},
  number    = {3},
  pages     = {1269--1280},
  issn      = {0167-9473},
  abstract  = {Two new forecasting methods of time series are introduced. They are both based on a factorial analysis method called spline principal component analysis with respect to instrumental variables (spline PCAIV). The first method is a straightforward application of spline PCAIV while the second one is an adaptation of spline PCAIV. In the modified version, the used criteria according to the unknown value that need to be predicted are differentiated. Those two forecasting methods are shown to be well adapted to time series.},
  comment   = {Spline and pca for forecasting. Seems to be good for a lot of variables (e.g. they used lagged variables as inputs). Is multivariate. Simplicity suggests good for missing features, model sel, etc.? Splines aren't across time
* I could extend with spline distributed lags (actually, that's what they're doing: distributed lags in time direction)
* a cheap way of doing multivariate nonlinear models and avoiding curse of dim:
 -- learn a sum of 1D nonlinear models
 -- here, they are splines, in the time direction
 -- splines are on the INPUTS, not the outputs
* solution is least squares

Tons of hooks for forecasting. See energytop.bib},
  doi       = {10.1016/j.csda.2007.06.017},
  file      = {Cornillon08forcstPrincComp.pdf:Cornillon08forcstPrincComp.pdf:PDF;Cornillon08forcstPrincComp.pdf:Cornillon08forcstPrincComp.pdf:PDF},
  groups    = {Read},
  location  = {Amsterdam, The Netherlands, The Netherlands},
  owner     = {sotterson},
  publisher = {Elsevier Science Publishers B. V.},
  timestamp = {2009.03.03},
}

@Article{Elden77AlgRegIllCond,
  author    = {Eld{\'e}n, Lars},
  title     = {Algorithms for the regularization of ill-conditioned least squares problems},
  journal   = {BIT Numerical Mathematics},
  year      = {1977},
  volume    = {17},
  number    = {2},
  pages     = {134--145},
  abstract  = {Two regularization methods for ill-conditioned least squares problems are studied from the point of view of numerical efficiency. The regularization methods are formulated as quadratically constrained least squares problems, and it is shown that if they are transformed into a certain standard form, very efficient algorithms can be used for their solution. New algorithms are given, both for the transformation and for the regularization methods in standard form. A comparison to previous algorithms is made and it is shown that the overall efficiency (in terms of the number of arithmetic operations) of the new algorithms is better.},
  comment   = {Said in Hansen08RegularizationToolsManualV4 to explain how to transform general Tikhonov reg. to standard form.},
  file      = {Elden77AlgRegIllCond.pdf:Elden77AlgRegIllCond.pdf:PDF},
  publisher = {Springer},
}

@TechReport{Srinivasa08mutInfoReview,
  author      = {Srinivasa, S.},
  title       = {A Review on Multivariate Mutual Information},
  institution = {University of Notre Dame},
  year        = {2008},
  abstract    = {Typically, mutual information is defined and studied between just two variables. Though the approach to evaluate bivariate mutual information is well established, several problems in multi-user information theory require the knowledge of interaction between more than two variables. Since there exists dependency between the variables, we cannot decipher their relationship without considering all of them at once. The seminal work on the informationtheoretic analysis of the interaction between more than two variables (or in other words, multivariate mutual information) was first studied in [1]. If several sources transmit information to a receiver, the bivariate model with certainly fail to discriminate effects due to uncontrolled sources from those due to random variability. We should not confuse the impairments due to system noise with the absence of knowledge of the association between the inputs. Besides, in a practical scenario, we don?t know in advance as to how many sources are transmitting information. By employing the multivariate model, we can effectively measure the effects due to the various transmitting sources. It provides a simple method for evaluating and testing dependencies in multidimensional frequency data or contingency tables. In section II, I will summarize the theoretical development and talk about numerous properties of multivariate mutual informations. Section III gives an introduction to total multivariate correlation analysis. Asymptotic hypothesis testing is discussed in section IV of this report. Some applications of multivariate mutual information are mentioned in section V. Section VI concludes the report. Section VII lists the key references used.},
  comment     = {General props of MI, like some kinds not always pos. However, this doc isn't especially clear. * explains same McGill interaction information that's in Brown09newInfoFeatSel * total correlation is positive, takes into account all higher order interactions, and be related to mutual information On the possibility of negative MI: is this just for differential entropy? Kraskov08MIChierClustMutInf says that MI from bins is non-neg (and invariant), unlike differential ENTROPY Google scholar lists it as 2008, and everybody who cites it also does.},
  file        = {Srinivasa08mutInfoReview.pdf:Srinivasa08mutInfoReview.pdf:PDF},
  groups      = {Read},
  journal     = {Univ. of Notre Dame, Notre Dame, Indiana},
  owner       = {scotto},
  timestamp   = {2011.05.14},
  url         = {http://www.nd.edu/~jnl/ee80653/Fall2005/tutorials/sunil.pdf},
}

@Article{Gneiting11ptFrcstMkEval,
  author    = {Gneiting, Tilmann},
  title     = {Making and evaluating point forecasts},
  journal   = {Journal of the American Statistical Association},
  year      = {2011},
  volume    = {106},
  number    = {494},
  pages     = {746--762},
  abstract  = {Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, with the absolute error and the squared error being key examples. The individual scores are averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive. A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance.},
  comment   = {seems obvious but says that should evaluate a point forecast based on its use. Some mathematical details. I haven't read this yet, so maybe there's more in it that isn't obvious.

A commentary on this and one other Gneiting paper is also attached to this bibtex entry. It came from:
http://tgmstat.wordpress.com/2013/07/03/how-to-properly-assess-point-forecast/
(The other Gneiting paper: Gneiting11quantOptFrcst)},
  file      = {Gneiting11ptFrcstMkEval.pdf:Gneiting11ptFrcstMkEval.pdf:PDF;Web commentary by Thiago G. Martins:Gneiting11ptFrcstMkEval_web.pdf:PDF},
  groups    = {Test, doReadNonWPV_2},
  owner     = {sotterson},
  publisher = {Taylor \& Francis},
  timestamp = {2013.10.23},
}

@Article{Pappenberger06blissNotUncert,
  author    = {Pappenberger, F. and Beven, K.J.},
  title     = {Ignorance is bliss: Or seven reasons not to use uncertainty analysis},
  journal   = {Water Resources Research},
  year      = {2006},
  volume    = {42},
  number    = {5},
  issn      = {0043-1397},
  abstract  = {Uncertainty analysis of models has received increasing attention over the last two decades in water resources research. However, a significant part of the community is still reluctant to embrace the estimation of uncertainty in hydrological and hydraulic modeling. In this paper, we summarize and explore seven common arguments: uncertainty analysis is not necessary given physically realistic models; uncertainty analysis cannot be used in hydrological and hydraulic hypothesis testing; uncertainty (probability) distributions cannot be understood by policy makers and the public; uncertainty analysis cannot be incorporated into the decision-making process; uncertainty analysis is too subjective; uncertainty analysis is too difficult to perform; uncertainty does not really matter in making the final decision. We will argue that none of the arguments against uncertainty analysis rehearsed are, in the end, tenable. Moreover, we suggest that one reason why the application of uncertainty analysis is not normal and expected part of modeling practice is that mature guidance on methods and applications does not exist. The paper concludes with suggesting that a Code of Practice is needed as a way of formalizing such guidance.},
  comment   = {Argument for admitting and publishing modeling and prediction uncertainty.

* Apparently, a lot of hydro modelers prefer deterministic model results, want to ignore uncertainty of their assumptions and data. The author argues that this is bad science, and advocates for a new Code of Conduct, requiring that undertainty be modeled, just as data quality is checked on simulation code is version controlled.
* Pierre suggests it's a good inspiration for how to make the "ramps forecasts don't help" argument * Says there are common reasons against undertainty modeling, none of which are tenable.
* But also, there's little guidance on how to model uncertainty
* References to uncertainty modeling software
* Reference to uncertainty modeling choice decition tree, on the author's web site},
  doi       = {10.1029/2005WR004820},
  file      = {Pappenberger06blissNotUncert.pdf:Pappenberger06blissNotUncert.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  publisher = {American Geophysical Union},
  timestamp = {2011.05.03},
}

@Article{Miller16windSpdRedLargeScl,
  author    = {Miller, Lee M and Kleidon, Axel},
  title     = {Wind speed reductions by large-scale wind turbine deployments lower turbine efficiencies and set low generation limits},
  journal   = {Proceedings of the National Academy of Sciences},
  year      = {2016},
  volume    = {113},
  number    = {48},
  pages     = {13570--13575},
  abstract  = {Understanding the limits of electricity generation from winds is a requirement for planning a renewable energy future. A difficulty in estimating such limits is that wind turbines remove kinetic energy from the atmosphere, so that many turbines should reduce wind speeds, ultimately setting a limit to how much kinetic energy can be taken out of the atmosphere. We show that this slowdown effect can be accounted for by detailed climate model simulations and a relatively simple method that does not directly simulate atmospheric dynamics. This slowdown effect is critical to consider, as it makes each turbine less productive and shows that few land areas can yield more than 1.0 We m?2 of electricity at large scales.},
  comment   = {When huge #'s of turbines are installed, planetary wind speed slows down.  This has the effect of reducing the amount of total wind power that can be reduced by a factor of 10X below current estimates.

With the new numbers, 18% of the windiest land areas (or 3% of the windiest ocean areas) would be
needed to meet the current primary energy demand of 18 TW.

This paper in evernote:
http://www.evernote.com/l/AA3Be9y9LndKg7cPoa2nT2SiN1rT3As-YLc/

Rebuttal in Evernote:
http://www.evernote.com/l/AA0V1xHzL29PgYhOrFprhksnanhADGk1XAI/},
  doi       = {10.1073/pnas.1602253113},
  file      = {Miller16windSpdRedLargeScl.pdf:Miller16windSpdRedLargeScl.pdf:PDF},
  owner     = {sotterson},
  publisher = {National Acad Sciences},
  timestamp = {2017.04.26},
  url       = {http://www.pnas.org/content/113/48/13570.abstract},
}

@Article{Zhou14graphEstMatNormGEMNI,
  author    = {Zhou, Shuheng},
  title     = {Gemini: Graph estimation with matrix variate normal instances},
  journal   = {Ann. Statist.},
  year      = {2014},
  volume    = {42},
  number    = {2},
  pages     = {532--562},
  month     = {04},
  abstract  = {Undirected graphs can be used to describe matrix variate distributions.
In this paper, we develop new methods for estimating the graphical structures
and underlying parameters, namely, the row and column covariance and in-
verse covariance matrices from the matrix variate data. Under sparsity condi-
tions, we show that one is able to recover the graphs and covariance matrices
with a single random matrix from the matrix variate normal distribution. Our
method extends, with suitable adaptation, to the general setting where repli-
cates are available. We establish consistency and obtain the rates of conver-
gence in the operator and the Frobenius norm. We show that having replicates
will allow one to estimate more complicated graphical structures and achieve
faster rates of convergence. We provide simulation evidence showing that we
can recover graphical structures as well as estimating the precision matrices,
as predicted by theory},
  comment   = {Estimates covariance matrices by both decomposing it with matrix normal element-wise product matrices and with the precision matrix.  Uses that to estimate a dependence graph.  I think there's some kind of sparsity constraint too.},
  doi       = {10.1214/13-AOS1187},
  file      = {:Zhou14graphEstMatNormGEMNI.pdf:PDF},
  fjournal  = {The Annals of Statistics},
  owner     = {sotterson},
  publisher = {The Institute of Mathematical Statistics},
  timestamp = {2017.07.08},
  url       = {http://dx.doi.org/10.1214/13-AOS1187},
}

@Article{Bertsimas13adptRobustOptUC,
  author    = {Bertsimas, D. and Litvinov, E. and Sun, X.A. and Jinye Zhao and Tongxin Zheng},
  title     = {Adaptive Robust Optimization for the Security Constrained Unit Commitment Problem},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2013},
  volume    = {28},
  number    = {1},
  pages     = {52--63},
  issn      = {0885-8950},
  abstract  = {Unit commitment, one of the most critical tasks in electric power system operations, faces new challenges as the supply and demand uncertainty increases dramatically due to the integration of variable generation resources such as wind power and price responsive demand. To meet these challenges, we propose a two-stage adaptive robust unit commitment model for the security constrained unit commitment problem in the presence of nodal net injection uncertainty. Compared to the conventional stochastic programming approach, the proposed model is more practical in that it only requires a deterministic uncertainty set, rather than a hard-to-obtain probability distribution on the uncertain data. The unit commitment solutions of the proposed model are robust against all possible realizations of the modeled uncertainty. We develop a practical solution methodology based on a combination of Benders decomposition type algorithm and the outer approximation technique. We present an extensive numerical study on the real-world large scale power system operated by the ISO New England. Computational results demonstrate the economic and operational advantages of our model over the traditional reserve adjustment approach.

Index Terms?Bilevel mixed-integer optimization, power system
control and reliability, robust and adaptive optimization, security
constrained unit commitment.},
  comment   = {An alternative to stochastic unit committment that requires LESS probabilistic forecast info. Seems to be a search for the worst case combination.

Results of a New England ISO experiment on this idea (I think, completion date is supposed to be April 2013).
http://hpc4energy.org/incubator/evaluation-of-robust-unit-commitment/

I think the tech report is here: Zhao14neISOrobustUC

* Questioned by: Li14DiscussAdaptRobustUC
* probably related slides attached

* Dokic15systIntegLrgData also does a "robust dispatch"},
  doi       = {10.1109/TPWRS.2012.2205021},
  file      = {paper (2013):Bertsimas13adptRobustOptUC.pdf:PDF;co-author slides (2010):Bertsimas13adptRobustOptUC_slides.pdf:PDF},
  groups    = {Use, doReadNonWPV_1},
  keywords  = {approximation theory;distributed power generation;power system security;statistical distributions;stochastic programming;Benders decomposition type algorithm;ISO New England;adaptive robust optimization;electric power system operations;hard-to-obtain probability distribution;nodal net injection uncertainty;outer approximation technique;price responsive demand;real-world large scale power system;security constrained unit commitment problem;stochastic programming approach;two-stage adaptive robust unit commitment model;variable generation resources;wind power;Adaptation models;Approximation algorithms;Computational modeling;Optimization;Robustness;Security;Uncertainty;Bilevel mixed-integer optimization;power system control and reliability;robust and adaptive optimization;security constrained unit commitment},
  owner     = {sotterson},
  timestamp = {2013.10.02},
}

@Article{Hong15ldFrcstCaseStdy,
  author   = {Hong, T and Shahidehpour, M},
  title    = {Load forecasting case study},
  journal  = {EISPC, US Department of Energy},
  year     = {2015},
  month    = jan,
  abstract = {University of North Carolina at Charlotte (UNCC) teamed with Illinois Institute of Technology (IIT),
ISO-New England, and North Carolina Electric Membership Corporation (NCEMC) to prepare a Load
Forecasting Case Study for the Eastern Interconnection States? Planning Council (EISPC) in   response to
the NARUC solicitation NARUC-2014-RFP042?DE0316. The work was supported by the Department of
Energy, National Energy Technology Laboratory, under Award Number DE-OE0000316.
The study includes two parts:
1) A comprehensive review of load forecasting topics for states, planning coordinators, and others.
This was covered in Chapters 1 through 6. In addition, a list of recommended actions is
summarized in Chapter 8.
2) Three case studies in three regions to assist Planning Coordinators and their relevant states with
applying state-of-the-art concepts, tools, and analysis to their forecasting regime. The case study
is presented in Chapter 7 and a glossary of terms can be found in Chapter 9.
This study is intended to be both a primer on load forecasting as well as provide an in-depth discussion of
load forecasting topics with a real-world demonstration that will be useful to state commissioners,
planning coordinators, utilities, legislators, researchers, and others. This study is also intended to simplify
and demystify the many complex concepts, terms, and statistics used in load forecasting.
A few key takeaways from this study include:
1) Load forecasting is the foundation for utility planning and it is a fundamental business problem in
the utility industry. Especially with the extraordinary risks confronting the electric utility industry
due to a potentially significant change in the resource mix resulting from environmental
regulation, aging infrastructure, the projected low cost of natural gas, and decreasing costs of
renewable technologies, it is crucial for utilities to have accurate load forecasts for resource
planning, rate cases, designing rate structures, financial planning, and so forth.
2) The states have varying degrees of authority to foster improvements in the databases, the
forecasting tools, and the forecasting processes. A comprehensive load forecasting process often
involves complicated data requirements, reliable software packages, advanced statistical methods,
and solid documentation to construct credible narratives to explain the potential future energy use
of customers. Load forecasting is not a static process. Rather, utilities and policymakers should be
continually looking for ways to improve the process, the databases, and advance the state-of-the-
art in forecasting tools. It is imperative that utilities devote substantial time and resources to the
effort to develop credible load forecasts.
3) Deployment of smart grid technologies has made high granular data available for load
forecasting. An emerging topic, hierarchical load forecasting, which produces load forecasts with
various hierarchies, such as geographic and temporal hierarchies, is of great importance in the
smart grid era. While customizing the models for each sub-region or utility would enhance the forecasting accuracy at the sub-regional level or utility level, the accuracy gained at a lower level
can be often translated to the enhanced forecasts at the aggregated levels.
4) Many factors influence the load forecasting accuracy, such as geographic diversity, data quality,
forecast horizon, forecast origin, and customer segmentation. The same model that works well in
one utility may not be the best model for another utility. Even within the same utility, a model
that forecasts well in one year may not generate a good forecast for another year. In order to
establish the credibility in load forecasting, utilities have to follow forecasting principles to
develop a consistent load forecasting methodology.
5) The recent recession has brought many utilities a paradigm change in how customers use
electricity and how much they use. The North Carolina Electric Membership Corporation case
study in Chapter 7 was designed to show how the same forecasting methodology would lead to
different results and varying degrees of forecasting accuracy in three supply areas of the same
state (North Carolina).
6) It is inappropriate to evaluate long-term load forecasts based on ex ante point forecasting
accuracy. Long term load forecasts should be probabilistic rather than point estimates. The
evaluation should also be based on probabilistic scoring rules.
7) All forecasts are wrong. While the ability to predict the future with as much accuracy as possible
would be ideal, a more realistic expectation, especially for long-term forecasts, is the insights on
the various risks that may confront a utility},
  comment  = {General review report w/ interesting seasonality graphs etc.  Advocates probabilsitic.  Shows architectures of forecasting systems at different time/space scales: planning, short term, spatial forecast.},
  file     = {Hong15ldFrcstCaseStdy.pdf:Hong15ldFrcstCaseStdy.pdf:PDF},
  url      = {http://pubs.naruc.org/pub/536E10A7-2354-D714-5191-A8AAFE45D626},
}

@InProceedings{Leahy16windTurbFaultMachLrn,
  author    = {K. Leahy and R. L. Hu and I. C. Konstantakopoulos and C. J. Spanos and A. M. Agogino},
  title     = {Diagnosing wind turbine faults using machine learning techniques applied to operational data},
  booktitle = {Proc. IEEE Int. Conf. Prognostics and Health Management (ICPHM)},
  year      = {2016},
  pages     = {1--8},
  month     = jun,
  abstract  = {Unscheduled or reactive maintenance on wind turbines due to component failures incurs significant downtime and, in turn, loss of revenue. To this end, it is important to be able to perform maintenance before it's needed. By continuously monitoring turbine health, it is possible to detect incipient faults and schedule maintenance as needed, negating the need for unnecessary periodic checks. To date, a strong effort has been applied to developing Condition monitoring systems (CMSs) which rely on retrofitting expensive vibration or oil analysis sensors to the turbine. Instead, by performing complex analysis of existing data from the turbine's Supervisory Control and Data Acquisition (SCADA) system, valuable insights into turbine performance can be obtained at a much lower cost. In this paper, data is obtained from the SCADA system of a turbine in the South-East of Ireland. Fault and alarm data is filtered and analysed in conjunction with the power curve to identify periods of nominal and fault operation. Classification techniques are then applied to recognise fault and fault-free operation by taking into account other SCADA data such as temperature, pitch and rotor data. This is then extended to allow prediction and diagnosis in advance of specific faults. Results are provided which show success in predicting some types of faults.},
  comment   = {Seems to be a good overview of diagnosing wind turbine faults using 10 minute SCADA data.

Copula power curve that "could be possibly used in the future": Gill12powCurvCopula

Used this to pre-filter out-of-nominal-operation points in order to refine their "no-fault" dataset.  Park14windPowCurveMon

},
  doi       = {10.1109/ICPHM.2016.7542860},
  file      = {Leahy16windTurbFaultMachLrn.pdf:Leahy16windTurbFaultMachLrn.pdf:PDF},
  keywords  = {SCADA systems, condition monitoring, fault diagnosis, learning (artificial intelligence), wind turbines, CMS, SCADA system, alarm data, condition monitoring systems, fault data, fault diagnosis, fault prediction, faulty operation period, incipient fault detection, machine learning techniques, maintenance scheduling, nominal operation period, operational data, pitch data, power curve, rotor data, south-east Ireland, temperature data, turbine health monitoring, turbine performance, turbine supervisory control and data acquisition system, wind turbine fault diagnosis, Blades, Generators, Maintenance engineering, Monitoring, Sensors, Wind speed, Wind turbines, FDD, Fault Detection, SCADA Data, SVM, Wind Turbine},
  owner     = {sotterson},
  timestamp = {2017.01.16},
}

@Unpublished{Ngiam11sparseFiltFtLrn,
  author    = {Ngiam, Jiquan and Chen, Zhenghao and Bhaskar, Sonia A and Koh, Pang W and Ng, Andrew},
  title     = {Sparse filtering},
  year      = {2011},
  abstract  = {Unsupervised feature learning has been shown to be effective at learning representations
that perform well on image, video and audio classification. However,
many existing feature learning algorithms are hard to use and require extensive
hyperparameter tuning. In this work, we present sparse filtering, a simple new
algorithm which is efficient and only has one hyperparameter, the number of features
to learn. In contrast to most other feature learning methods, sparse filtering
does not explicitly attempt to construct a model of the data distribution. Instead, it
optimizes a simple cost function ? the sparsity of `2-normalized features ? which
can easily be implemented in a few lines of MATLAB code. Sparse filtering scales
gracefully to handle high-dimensional inputs, and can also be used to learn meaningful
features in additional layers with greedy layer-wise stacking. We evaluate
sparse filtering on natural images, object classification (STL-10), and phone classification
(TIMIT), and show that our method works well on a range of different
modalities.},
  booktitle = {Advances in Neural Information Processing Systems},
  comment   = {Unsupervised feature learning that's easy in Matlab and fast -- said to obsolete deep autoencoding, can be merged with deep learning. Good for regime learning?

Blogger who said it obsoleted deep learning: http://danluu.com/linear-hammer/
2 other papers he said obsoleted deep autoencoding:
Coates11encodeTrnVQ, Coates11ntwkUnSupFtLrn


For regime learning, compare with adaboost methods, starting with Lillywhite13featCnstrct and the papers it references.},
  file      = {Ngiam11sparseFiltFtLrn.pdf:Ngiam11sparseFiltFtLrn.pdf:PDF},
  owner     = {sotterson},
  pages     = {1125--1133},
  timestamp = {2014.01.28},
  url       = {http://cs.stanford.edu/~jngiam/papers/NgiamKohChenBhaskarNg2011.pdf?},
}

@Article{Madiraju18deepUnsupTSclust,
  author   = {Naveen Sai Madiraju and S. M. Nazmus Sadat and Dimitry Fisher and Homa Karimabadi},
  title    = {Deep Temporal Clustering : Fully Unsupervised Learning of Time-Domain Features},
  journal  = {CoRR},
  year     = {2018},
  volume   = {abs/1802.01059},
  abstract = {Unsupervised learning of time series data, also known as temporal clustering, is
a challenging problem in machine learning. Here we propose a novel algorithm,
Deep Temporal Clustering (DTC), to naturally integrate dimensionality reduction
and temporal clustering into a single end-to-end learning framework, fully un-
supervised. The algorithm utilizes an autoencoder for temporal dimensionality
reduction and a novel temporal clustering layer for cluster assignment. Then it
jointly optimizes the clustering objective and the dimensionality reduction objec-
tive. Based on requirement and application, the temporal clustering layer can
be customized with any temporal similarity metric. Several similarity metrics
and state-of-the-art algorithms are considered and compared. To gain insight into
temporal features that the network has learned for its clustering, we apply a vi-
sualization method that generates a region of interest heatmap for the time series.
The viability of the algorithm is demonstrated using time series data from diverse
domains, ranging from earthquakes to spacecraft sensor data. In each case, we
show that the proposed algorithm outperforms traditional methods. The superior
performance is attributed to the fully integrated temporal dimensionality reduction
and clustering criterion},
  comment  = {Autoencoder clustering but somehow, the AE dim reduction is learned jointly while optimizing a cluster critereon -- or at least two of its cites do that ((Xie et al. (2016), Yang et al. (2016))) but they're said to be for "static: data, not TS.  Huh?  Weren't they time series clustering too?  Anyway, it's interesting as it has an LSTM layer mixed in with the autoencoder.  So it seems to be doing temporal compression, or at least shape learning.

Compare with Ryu18loadProfClustDpConvAutoEnc

Maybe related to: Li16waveletLSTMeegRecog},
  file     = {:Madiraju18deepUnsupTSclust.pdf:PDF},
  url      = {https://arxiv.org/abs/1802.01059},
}

@Article{Feng05wavltBtstrp,
  author    = {Feng, H. and Willemain, T.R. and Shang, N.},
  title     = {Wavelet-based bootstrap for time series analysis},
  journal   = {Communications in Statistics-Simulation and Computation},
  year      = {2005},
  volume    = {34},
  number    = {2},
  pages     = {393--413},
  issn      = {0361-0918},
  abstract  = {Using wavelet domain analysis and modeling of stochastic processes, we develop a unified bootstrap scheme that can be applied to both short-range dependent and long-range dependent stationary Gaussian time series. Our idea was motivated by the fact that the discrete wavelet transform is capable of converting long-range dependence in the time domain into short-range dependence in the wavelet domain. Hence we can use a simple Markov model to model the short-range dependence in the wavelet domain. The Markov model is used to generate a new version (the bootstrapped version) of the wavelet representation of the time series. The bootstrapped series is obtained by performing the inverse wavelet transform on the new wavelet representation of the original series. We compare our wavelet-based bootstrap with the moving block bootstrap for estimating the standard errors of the unit lag sample autocorrelation and the sample standard deviation. Our results show that the wavelet-based bootstrap can achieve performance better than the moving block bootstrap for both short-range dependent data and long-range dependent data.},
  comment   = {Instead of using sliding window bootstrap, uses markov bootstrap of wavelet coeffs, getting long-term dependence.

Could use to generate scenarios, somewhat like Pierre's method. Advantage could be that longer time dependencies could be modeled. Would still want the conditional bootstrap of Pierre, though.

Actually, this could be used for ensembles too (the individual pdf's in BMA).},
  file      = {Feng05wavltBtstrp.pdf:Feng05wavltBtstrp.pdf:PDF},
  groups    = {Ensemble, PointDerived, doReadNonWPV_2},
  owner     = {scot},
  publisher = {Taylor \& Francis},
  timestamp = {2010.12.01},
  url       = {http://web.ebscohost.com.globalproxy.cvt.dk/ehost/detail?vid=1&hid=106&sid=8f2edbfe-7fdf-42df-b0c1-06e2bb4cecd6%40sessionmgr113&bdata=JnNpdGU9ZWhvc3QtbGl2ZQ%3d%3d#db=buh&AN=17132504},
}

@Article{Eckersley18utilFuncImpossUncert,
  author   = {Eckersley, Peter},
  title    = {Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function)},
  journal  = {arXiv preprint arXiv:1901.00064},
  year     = {2018},
  abstract = {Utility functions or their equivalents (value functions, objective functions, loss
functions, reward functions, preference orderings) are a central tool in most current
machine learning systems. These mechanisms for defining goals and guiding
optimization run into practical and conceptual difficulty when there are independent,
multi-dimensional objectives that need to be pursued simultaneously and cannot be
reduced to each other. Ethicists have proved several impossibility theorems that
stem from this origin; those results appear to show that there is no way of formally
specifying what it means for an outcome to be good for a population without
violating strong human ethical intuitions (in such cases, the objective function
is a social welfare function). We argue that this is a practical problem for any
machine learning system (such as medical decision support systems or autonomous
weapons) or rigidly rule-based bureaucracy that will make high stakes decisions
about human lives: such systems should not use objective functions in the strict
mathematical sense.
We explore the alternative of using uncertain objectives, represented for instance
as partially ordered preferences, or as probability distributions over total orders.
We show that previously known impossibility theorems can be transformed into
uncertainty theorems in both of those settings, and prove lower bounds on how
much uncertainty is implied by the impossibility results. We close by proposing
two conjectures about the relationship between uncertainty in objectives and severe
unintended consequences from AI systems.},
  comment  = {Ways of modeling tradeoffs between irreconcilable outcomes  Maybe good for energy optimization: max comfort vs. average comfort over time vs. money... etc.  Several kind of probabilistic rankings or orderings are mentioned, which could help optimize an AI machine -- based on human preferences which are uncertain

Author's interest is in ethical delimmas but maybe this is good for something Clean Power Research will want me to do: cost vs. outage prob. vs. ... as chosen by operators twizzling some optimization knobs on a CPR user interface.

Discussed here:
https://www.technologyreview.com/s/612764/giving-algorithms-a-sense-of-uncertainty-could-make-them-more-ethical/},
  file     = {:Eckersley18utilFuncImpossUncert.pdf:PDF},
  url      = {https://arxiv.org/pdf/1901.00064.pdf?utm_campaign=the_algorithm.unpaid.engagement&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8SQPvr8mh-ICrJzt0vJhRCEPzgIalY3fRUX7Q4iMwG1rNFf-v4-GR9BNEI_UB8R-0z08Jm},
}

@Electronic{Pyle10rampObsNet,
  author       = {Richard Pyle},
  year         = {2010},
  title        = {Wind ramp prediction: improved predictability for wind energy production},
  howpublished = {Vaisala News 182},
  url          = {http://www.vaisala.com/en/press/vaisalanews/vaisalanews182/Pages/vn182_wind_ramp_prediction.aspx},
  abstract     = {Vaisala and Xcel Energy are developing a new wind observing and forecasting system to provide critical decision-making support for balancing wind power with traditional fossil fuel generation.},
  comment      = {A commercial wind ramp system, connected to an observation network},
  file         = {Pyle10rampObsNet.pdf:Pyle10rampObsNet.pdf:PDF},
  groups       = {Use, doReadWPV_2},
  location     = {Boulder, CO, USA},
  owner        = {scot},
  timestamp    = {2011.04.28},
}

@Article{Guyon03introFeatSel,
  author    = {Guyon, Isabelle and Elisseeff, Andr\'{e}},
  title     = {An introduction to variable and feature selection},
  journal   = {Journal of Machine Learning Research},
  year      = {2003},
  volume    = {3},
  pages     = {1157--1182},
  issn      = {1533-7928},
  abstract  = {Variable and feature selection have become the focus of much research in areas of appli- cation for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-e?ective pre- dictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better de?nition of the objective function, feature construction, feature rank- ing, multivariate feature selection, e?cient search methods, and feature validity assessment methods.},
  comment   = {Good overall review of feature selection wisdom circa 2003, video update in 2007

Excellent heuristics: general feature selection workflow (p. 1159)
* use domain knowledge to construct ad hoc feats
* normalize
* if interdependent feats: construct conjunctive feats e.g. feature products
 -- I think this makes it easier for featsel alg. to see interralatinoship w/ single feeat picking?
* disjunctive features: features from clustering or matrix factorization
* always compare w/ single variable ranking
 -- either as a baseline
 -- or to do a preliminary filtering
* clean out outlier examples by using top ranked features to detect (hmm... seems like could fail)
* start w/ linear predictor, forward selection, w/ probe or l0 stopping. Can use linear SVM.
 - - If these subsets beat foward method, then try nonlinear predictor on that subset (p. 1159) but I don't get this.

Why not also do nonlin featsel on forward method?
* if want stable, use bootstraps

 MOST IMPORTANT HEURISTIC: is probably to always start w/ linear predictor \& compare w/ ranking (also see p. 1178)

Variable Ranking
* Ranking: select variables individually, according to rank:
* Attributes
 -- Efficient, scaleable
 -- But misses cross-dependencies; can miss important variables.
 -- Also allows redundant variables (which I know causes neural wet problems)
 -- ?? Still, often works
* Ranking Criteria
-- correlation (ranking according to goodness of linear fit)
 -- singe variable classifers
 -- information theory (mutual information, could use Parzen windows, no mention of k-means MI)

How redundant variables can help:??
1.) multiple views can reduce noise??
2.) perfectly correlated variables may still not be co-linear. In fact, if the lines are parallel, the set of them can be an excellent feature set (figure 2)?

QUESTION: do correlated but non-collinear variables still screw up neural nets?

How use-less-by-itself variable is useful w/ others
* XOR example
* single varialbe ranking can miss these important combos

Variable Subset Selection ie. selecting by variable interdependenc, not just rank
* filters *
 wrappers, embedded methods
* nested subsets
* OBD: optimum brain damage
* qudratic approx of cost func
* Gram-Schmid orthog makes it easier to select for forward sel.
* Direct ojbective optimization e.g. shrinkage SVM, l1-norm svm

Feature Construction/Reduction
* Clustering
* informatino bottleneck
* Matrix factorization
* Mutual information maximizing linear transform

Validation
* recommends train/dev/test set, like Mari \& others
* cross-Val ok even for non-iid error conf, sometimes.
* leave-one-out crossval optimistic for non-indep data
* Another way to use unlabelled data, although there's still a combinatoric problem.
* permutation test to see if variable is relevant (either if add or for ranking?)

Variance (instability of subset selection)
* different subsets may have same predictive power, so get different answers dep. on init conds, random data variation
* "stabilize" by training on subsets, picking union of selections. Calls this a bootstrap.
?Note that Balasso uses tine intersection of subsets!?
Other, more sophisticated Bayesian tricks, too.

Unsupervised Feature selection
* a bunch of kinda ad hoc criteria

video: http://videolectures.net/bootcamp07_guyon_ifs/},
  file      = {Guyon03introFeatSel.pdf:Guyon03introFeatSel.pdf:PDF;Guyon03introFeatSel.pdf:Guyon03introFeatSel.pdf:PDF},
  groups    = {Read},
  keywords  = {Variable selection, feature selection, space dimensionality reduction, pat- tern discovery, ?lters, wrappers, clustering, information theory, support vector machines, model selection, statistical testing, bioinformatics, computational biology, gene expression, microarray, genomics, proteomics, QSAR, text classi?cation, information retrieval.},
  location  = {Cambridge, MA, USA},
  owner     = {sotterson},
  publisher = {MIT Press},
  timestamp = {2009.08.13},
  url       = {http://portal.acm.org/citation.cfm?id=944968&dl=#},
}

@InProceedings{Anagnostopoulos08adaptVarSel,
  author    = {Anagnostopoulos, Christoforos and Adams, Niall M. and Hand, David J.},
  title     = {Deciding what to observe next: adaptive variable selection for regression in multivariate data streams},
  booktitle = {ACM Symposium on Applied Computing (SAC)},
  year      = {2008},
  pages     = {961--965},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Variable selection can be valuable in the analysis of streaming data with costly measurements, as in intensive care monitoring or battery-powered sensor networks. In the presence of drift, selections must be constantly revised, calling for adaptive variable selection schemes. An important and novel problem arises from the fact that non-selected variables become missing variables, which induces bias upon subsequent decisions. Here, we consider adaptive variable selection in the context of linear regression, using only a fraction of the available regressors per timepoint. We suggest a scheme that fits a multivariate Gaussian over a sliding window using the EM algorithm and selects which variables to observe next using the Lasso algorithm. We experiment with simulated and real data to demonstrate that very high prediction accuracy may be retained using as little as 10 pct of the data.},
  comment   = {sliding window gaussian selects lasso observations},
  doi       = {10.1145/1363686.1363909},
  file      = {Anagnostopoulos08adaptVarSel.pdf:Anagnostopoulos08adaptVarSel.pdf:PDF},
  isbn      = {978-1-59593-753-7},
  location  = {Fortaleza, Ceara, Brazil},
  owner     = {scot},
  timestamp = {2010.09.27},
}

@Article{Rinnan13recursPLSR,
  author    = {Rinnan, {\AA}smund and Andersson, Martin and Ridder, Carsten and Engelsen, S{\o}ren Balling},
  title     = {Recursive weighted partial least squares (rPLS): an efficient variable selection method using PLS},
  journal   = {Journal of Chemometrics},
  year      = {2013},
  abstract  = {Variable selection is important in fine tuning partial least squares (PLS) regression models. This study introduces a novel variable weighting method for PLS regression where the univariate response variable y is used to guide the variable weighting in a recursive manner?the method is called recursive weighted PLS or just rPLS. The method iteratively reweights the variables using the regression coefficients calculated by PLS. The use of the regression vector to make up the weights is a reasonable idea from the fact that the weights in the regression vector ideally reflect the importance of the variables. In contrast to many other variable selection methods, the rPLS method has the advantage that only one parameter needs to be estimated: the number of latent factors used in the PLS model. The rPLS model has the fascinating output that it, under normal conditions, converges to a very limited number of variables (useful for interpretation), but it will exhibit optimal regression performance before convergence, normally including covarying neighbor variables. This study examines the properties of rPLS by application to a near-infrared spectroscopy dataset of feed samples predicting the protein content and to a metabolomics dataset modeling a reference metabolic parameter (creatinine) from nuclear magnetic resonance spectra of human urine.
Keywords:

 variable selection;
 PLS;
 NIR;
 NMR},
  comment   = {weighted PLS, but recursive means adaptive too, I think, although maybe it won't forget...},
  doi       = {10.1002/cem.2582/full},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.04.27},
}

@Article{Noh12varyCoeffFeatSelQR,
  author    = {Noh, Hohsuk and Chung, Kwanghun and Van Keilegom, Ingrid and others},
  title     = {Variable selection of varying coefficient models in quantile regression},
  journal   = {Electronic Journal of Statistics},
  year      = {2012},
  volume    = {6},
  pages     = {1220--1238},
  abstract  = {Varying coefficient (VC) models are commonly used to study
dynamic patterns in many scientific areas. In particular, VC models in
quantile regression are known to provide a more complete description of
the response distribution than in mean regression. In this paper, we develop
a variable selection method for VC models in quantile regression using a
shrinkage idea. The proposed method is based on the basis expansion of
each varying coefficient and the regularization penalty on the Euclidean
norm of the corresponding coefficient vector. We show that our estimator
is obtained as an optimal solution to the second order cone programming
(SOCP) problem and that the proposed procedure has consistency in vari-
able selection under suitable conditions. Further, we show that the esti-
mated relevant coefficients converge to the true functions at the univariate
optimal rate. Finally, the method is illustrated with numerical simulations
including the analysis of forced expiratory volume (FEV) data.
AMS 2000 subject classifications: Primary 62G35; secondary 62J07.
Keywords and phrases: Basis approximation, consistency in variable
selection, second order cone programming, shrinkage estimator.},
  comment   = {Feature selection for varying coefficients assuming penalized splines (penalty is across coeffs, not across the regression result). The varying coefficient function input variable is 1D.},
  file      = {Noh12varyCoeffFeatSelQR.pdf:Noh12varyCoeffFeatSelQR.pdf:PDF},
  owner     = {sotterson},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2014.11.10},
  url       = {http://projecteuclid.org/euclid.ejs/1341842803},
}

@Article{Cheng14fwdFeatSelUhiDimVarCoeff,
  author    = {Cheng, Ming-Yen and Honda, Toshio and Zhang, Jin-Ting},
  title     = {Forward variable selection for sparse ultra-high dimensional varying coefficient models},
  journal   = {arXiv preprint arXiv:1410.6556},
  year      = {2014},
  abstract  = {Varying coefficient models have numerous applications in a wide scope of scientific areas. While enjoying nice interpretability, they also allow flexibility in modeling dynamic impacts of the covariates. But, in the new era of big data, it is challenging to select the relevant variables when there are a large number of candidates. Recently several work are focused on this important problem based on sparsity assumptions; they are subject to some limitations, however. We introduce an appealing forward variable selection procedure. It selects important variables sequentially according to a sum of squares criterion, and it employs an EBIC- or BIC-based stopping rule. Clearly it is simple to implement and fast to compute, and it possesses many other desirable properties from both theoretical and numerical viewpoints. We establish rigorous selection consistency results when either EBIC or BIC is used as the stopping criterion, under some mild regularity conditions. Notably, unlike existing methods, an extra screening step is not required to ensure selection consistency. Even if the regularity conditions fail to hold, our procedure is still useful as an effective screening procedure in a less restrictive setup. We carried out simulation and empirical studies to show the efficacy and usefulness of our procedure.},
  comment   = {Seems to be a straightforward forward featsel w/ BIC-like stopping. Says it's ultra high dim; maybe there's a trick there.},
  file      = {Cheng14fwdFeatSelUhiDimVarCoeff.pdf:Cheng14fwdFeatSelUhiDimVarCoeff.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.12.07},
  url       = {http://arxiv.org/abs/1410.6556},
}

@Article{Fan03adaptTVlinModels,
  author    = {Fan, Jianqing and Yao, Qiwei and Cai, Zongwu},
  title     = {Adaptive varying-coefficient linear models},
  journal   = {Journal of the Royal Statistical Society, Series B},
  year      = {2003},
  volume    = {65},
  number    = {1},
  pages     = {57--80},
  abstract  = {Varying-coefficient linear models arise from multivariate nonparametric regression, non-linear time series modelling and forecasting, functional data analysis, longitudinal data analysis and others. It has been a common practice to assume that the varying coefficients are functions of a given variable, which is often called an "index". To enlarge the modelling capacity substantially, this paper explores a class of varying-coefficient linear models in which the index is unknown and is estimated as a linear combination of regressors and/or other variables. We search for the index such that the derived varying-coefficient model provides the least squares approximation to the underlying unknown multidimensional regression function. The search is implemented through a newly proposed hybrid backfitting algorithm. The core of the algorithm is the alternating iteration between estimating the index through a one-step scheme and estimating coefficient functions through one-dimensional local linear smoothing. The locally significant variables are selected in terms of a combined use of the "t"-statistic and the Akaike information criterion. We further extend the algorithm for models with two indices. Simulation shows that the methodology proposed has appreciable flexibility to model complex multivariate non-linear structure and is practically feasible with average modern computers. The methods are further illustrated through the Canadian mink-muskrat data in 1925-1994 and the pound-dollar exchange rates in 1974-1983. Copyright 2003 Royal Statistical Society.},
  comment   = {time varying linear regression feature selection via backfitting},
  file      = {Fan03adaptTVlinModels.pdf:Fan03adaptTVlinModels.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.09.27},
  url       = {http://econpapers.repec.org/RePEc:bla:jorssb:v:65:y:2003:i:1:p:57-80},
}

@Article{Brier50probFrcstVerif,
  author    = {Brier, Glenn W},
  title     = {Verification of forecasts expressed in terms of probability},
  year      = {1950},
  volume    = {78},
  number    = {1},
  pages     = {1--3},
  issn      = {0027-0644},
  abstract  = {Verification of weather forecasts has been a controversial
subject for more than a half century. There are a number
of reasons why this problem has been so perplexing to
meteorologists and others but one of the most important
difficulties seems to be in reaching an agreement on the
specification of a scale of goodness for weather forecasts.
Numerous systems have been proposed but one of the
greatest arguments raised against forecast verification is
that forecasts which may be the 11 best" according to the
accepted system of arbitrary scores may not be the most
useful forecasts. In attempting to resolve this difficulty
the forecaster may often find himself in the position of
choosing to ignore the verification system or to let it do
the forecasting for him by 11hedging" or 11playing the
system." This may lead the forecaster to forecast something
other than what he thinks will occur, for it is often
easier to analyze the effect of different possible forecasts
on the verification score than it is to analyze the weather
situation. It is generally agreed that this state of affairs
is unsatisfactory, as one essential criterion for satisfactory
verification is that the verification scheme should influence
the forecaster in no undesirable way. Unfortunately, the
criterion is difficult, if not impossible to satisfy, although
some schemes will be much worse than others in this
respect.
It is the purpose of this paper to discuss one situation
where it appears to be possible to devise a verification
scheme that cannot influence the forecaster in any undesirable
way. This is the case when forecasts are expressed
in terms of probability statements. The advantages of
expressing the degree of assumed reliability of a forecast
877086-oo--l
numerically have been discussed previously [I, 2, 3, 4] so
that the purpose here will not be to emphasize the enhanced
usefulness of such forecasts but rather to point out
how some aspects of the verification problem are simplified
or solved.},
  booktitle = {Monthly Weather Review},
  comment   = {The famous Brier Score},
  doi       = {10.1175/1520-0493%281950%29078%3C0001:VOFEIT%3E2.0.CO;2},
  file      = {Brier50probFrcstVerif.pdf:Brier50probFrcstVerif.pdf:PDF},
  groups    = {Test, CitaviImport1, doReadWPV_1},
  keywords  = {Brier score},
  ncite     = {1643},
  owner     = {sotterson},
  timestamp = {2013.09.26},
}

@INPROCEEDINGS{Guan09loadFrcstNNmultiWvlt,
  Author                   = {Che Guan and Luh, P.B. and Coolbeth, M.A. and Yige Zhao and Michel, L.D. and Ying Chen and Manville, C.J. and Friedland, P.B. and Rourke, S.J.},
  Title                    = {Very short-term load forecasting: Multilevel wavelet neural networks with data pre-filtering},
  Booktitle                = {Power Energy Society General Meeting, 2009. PES '09. IEEE},
  Year                     = {2009},
  Pages                    = {1--8},
  Month                    = jul,
  Abstract                 = {Very short term load forecasting predicts the load over one hour into the future in five minute steps, and is important in resource dispatch and area generation control. Effective forecasting, however, is difficult in view of noisy real-time data gathering and complicated features of load. This paper presents a method based on multilevel wavelet neural networks with novel pre-filtering. The key idea is to use a data pre-filtering method to detect and eliminate spikes within load, apply the wavelet technique to decompose the load into several frequency components, perform appropriate transformation on each component, and feed it together with other appropriate input to a separate neural network. Numerical testing demonstrates the significant value of data pre-filtering and multilevel wavelet neural networks, and shows that our method provides accurate forecasting.},
  DOI                      = {10.1109/PES.2009.5275296},
  File                     = {Guan09loadFrcstNNmultiWvlt.pdf:Guan09loadFrcstNNmultiWvlt.pdf:PDF},
  ISSN                     = {1944-9925},
  Keywords                 = {area generation control;data pre-filtering;multilevel wavelet neural networks;resource dispatch;very short-term load forecasting;load forecasting;neural nets;power generation control;power generation dispatch;power station load;}
}

@InProceedings{Guan10hybKalmanFrcstConfInt,
  author    = {Che Guan and Luh, P.B. and Michel, L.D. and Coolbeth, M.A. and Friedland, P.B.},
  title     = {Hybrid {Kalman} algorithms for very short-term load forecasting and confidence interval estimation},
  booktitle = {Power and Energy Society General Meeting, 2010 IEEE},
  year      = {2010},
  month     = jul,
  pages     = {1--8},
  doi       = {10.1109/PES.2010.5590077},
  abstract  = {Very short-term load forecasting predicts the load over
one hour into the future in five-minute steps and performs the
moving forecast every five minutes. To quantify forecasting
accuracy, the confidence interval is estimated in real-time. An
effective prediction with a small associated confidence interval is
important for area generation control and resource dispatch, and
can help the operator further make good decisions. We
previously presented a multi-level wavelet neural network
method, but it cannot produce a good confidence interval due to
the model itself. This paper presents a method of multiple
wavelet neural networks trained by hybrid Kalman algorithms.
The prediction, however, is difficult, since one effective model is
not able to capture complex load features at different
frequencies. Appropriate transformations on load components
also result in a complicated derivation in order to estimate an
accurate variance. The key idea is to use neural network trained
by extended Kalman filter for the low frequency component
which has a near linear input-output function relationship; and
use neural networks trained by unscented Kalman filter for high
frequency components which have nonlinear input-output
function relationships. Forecasts for load components from
individual networks are then transformed back and derived, and
combined to form the final load prediction with the good
confidence interval. Numerical testing demonstrates significant
value for load component predictions via hybrid Kalman filterbased
algorithms for training neural networks and the derivation
for confidence interval, and shows that our method provides the
accurate prediction.
Index Terms-- Confidence interval estimation, extended Kalman
filter, multilevel wavelet neural networks, unscented Kalman filter,
very short-term load forecasting.},
  file      = {Guan10hybKalmanFrcstConfInt.pdf:papers\\Guan10hybKalmanFrcstConfInt.pdf:PDF;Guan10hybKalmanFrcstConfInt.pdf:Guan10hybKalmanFrcstConfInt.pdf:PDF},
  issn      = {1944-9925},
}

@Article{Guan13loadFrcstWvltNNpreFilt,
  author   = {Guan, C. and Luh, P. B. and Michel, L. D. and Wang, Y. and Friedland, P. B.},
  title    = {Very Short-Term Load Forecasting: Wavelet Neural Networks With Data Pre-Filtering},
  journal  = {Power Systems, IEEE Transactions on},
  year     = {2012},
  volume   = {PP},
  number   = {99},
  pages    = {1},
  issn     = {0885-8950},
  abstract = {Very short-term load forecasting predicts the loads 1 h into the future in 5-min steps in a moving window manner based on real-time data collected. Effective forecasting is important in area generation control and resource dispatch. It is however difficult in view of the noisy data collection process and complicated load features. This paper presents a method of wavelet neural networks with data pre-filtering. The key idea is to use a spike filtering technique to detect spikes in load data and correct them. Wavelet decomposition is then used to decompose the filtered loads into multiple components at different frequencies, separate neural networks are applied to capture the features of individual components, and results of neural networks are then combined to form the final forecasts. To perform moving forecasts, 12 dedicated wavelet neural networks are used based on test results. Numerical testing demonstrates the effects of data pre-filtering and the accuracy of wavelet neural networks based on a data set from ISO New England.},
  comment  = {Has Matlab.},
  doi      = {10.1109/TPWRS.2012.2197639},
  file     = {Pre-published, 2012, same as final except for pagination:papers\\Guan12loadFrcstWvltNNpreFilt.pdf:PDF;Final version, 2013:Guan13loadFrcstWvltNNpreFilt.pdf:PDF},
}

@Article{Pinson12vShrtWndPowLogitNorm,
  author    = {Pinson, P.},
  title     = {Very-short-term probabilistic forecasting of wind power with generalized logit-normal distributions},
  journal   = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  year      = {2012},
  volume    = {61},
  number    = {4},
  pages     = {555--576},
  issn      = {1467-9876},
  abstract  = {Very-short-term probabilistic forecasts, which are essential for an optimal management of wind generation, ought to account for the non-linear and double-bounded nature of that stochastic process. They take here the form of discrete?continuous mixtures of generalized logit?normal distributions and probability masses at the bounds. Both auto-regressive and conditional parametric auto-regressive models are considered for the dynamics of their location and scale parameters. Estimation is performed in a recursive least squares framework with exponential forgetting. The superiority of this proposal over classical assumptions about the shape of predictive densities, e.g. normal and beta, is demonstrated on the basis of 10-min-ahead point and probabilistic forecasting at the Horns Rev wind farm in Denmark.},
  comment   = {10 minutes aheady, bounded output, dir. sensitiviity, autocorrelation... the works.. Current paper is the tech note, not the journal paper.

Related idea: Mauch13windPowFrcstLogitNorm},
  doi       = {10.1111/j.1467-9876.2011.01026.x},
  file      = {Pinson12vShrtWndPowLogitNorm.pdf:Pinson12vShrtWndPowLogitNorm.pdf:PDF},
  keywords  = {Bounded time series, Dynamic models, Probabilistic forecasting, Transformation, Wind power},
  owner     = {sotterson},
  publisher = {Blackwell Publishing Ltd},
  timestamp = {2014.07.20},
}

@Article{Kano13virtSensLocPLA,
  author    = {Kano, Manabu and Fujiwara, Koichi},
  title     = {Virtual sensing technology in process industries: trends and challenges revealed by recent industrial applications},
  journal   = {Journal of Chemical Engineering of Japan},
  year      = {2013},
  volume    = {46},
  number    = {1},
  pages     = {1--17},
  abstract  = {Virtual sensing technology is crucial for high product quality and productivity in any industry. This review aims to clarify the trend of research and application of virtual sensing technology in process industries. After a brief survey, practical issues are clarified by introducing recent questionnaire survey results: 1) changes in process characteristics and operating conditions, 2) individual difference of equipment, and 3) reliability of soft-sensors. Since input variable selection is crucial for high estimation performance, conventional methods and new group-wise variable selection methods are introduced, and the usefulness of the group-wise variable selection methods is demonstrated through industrial case studies. Just-in-time (JIT) modeling is dealt with as a promising virtual sensing technology that can cope with changes in process characteristics as well as nonlinearity. Recent developments leading to successful industrial applications are introduced: correlation-based JIT (CoJIT) modeling and locally weighted regression (LWR), especially LW-PLS, with modified similarity measures. Manufacturing processes in different industries are quite different in appearance, but they have very similar problems from the viewpoint of quality issue. There remain practical issues requiring further research efforts to realize high-performance, maintenance-free virtual sensing technology.},
  comment   = {Seems to use locally weighted PLS linear regression, which is what is probably better than locally weighted regression with PCA preprocessing. PLS would compress the inputs in a direction that is meaningful to the regression output.

Attached are two bad pdf scans (from Oldenburg Uni). I don't know which one is worse.},
  file      = {Kano13virtSensLocPLA.pdf:Kano13virtSensLocPLA.pdf:PDF;Kano13virtSensLocPLA_Scan2.pdf:Kano13virtSensLocPLA_Scan2.pdf:PDF},
  owner     = {sotterson},
  publisher = {Society of Chemical Engineers, Japan},
  timestamp = {2014.04.19},
  url       = {http://www.cheric.org/research/tech/periodicals/view.php?seq=1087997},
}

@Article{Wang14featSelChemMI,
  author    = {Wang, X. Rosalind AND Lizier, Joseph T. AND Nowotny, Thomas AND Berna, Amalia Z. AND Prokopenko, Mikhail AND Trowell, Stephen C.},
  title     = {Feature Selection for Chemical Sensor Arrays Using Mutual Information},
  journal   = {PLoS ONE},
  year      = {2014},
  volume    = {9},
  number    = {3},
  pages     = {1--17},
  month     = mar,
  abstract  = {We address the problem of feature selection for classifying a diverse set of chemicals using an array of metal oxide sensors. Our aim is to evaluate a filter approach to feature selection with reference to previous work, which used a wrapper approach on the same data set, and established best features and upper bounds on classification performance. We selected feature sets that exhibit the maximal mutual information with the identity of the chemicals. The selected features closely match those found to perform well in the previous study using a wrapper approach to conduct an exhaustive search of all permitted feature combinations. By comparing the classification performance of support vector machines (using features selected by mutual information) with the performance observed in the previous study, we found that while our approach does not always give the maximum possible classification performance, it always selects features that achieve classification performance approaching the optimum obtained by exhaustive search. We performed further classification using the selected feature set with some common classifiers and found that, for the selected features, Bayesian Networks gave the best performance. Finally, we compared the observed classification performances with the performance of classifiers using randomly selected features. We found that the selected features consistently outperformed randomly selected features for all tested classifiers. The mutual information filter approach is therefore a computationally efficient method for selecting near optimal features for chemical sensor arrays.},
  comment   = {Kraskov multivariate MI applied to classifier feature selection.  Brute force search for max MI(Y,feats) across every input feature combination.  For comparison, does exhaustive-feature-combo-and-train so know true optimal feature combo on a low dim problem (something around 18 but I'm not totally sure).

Pairwise MI (like Deng&Peng mRMR) misses feature synergy and don't remove redundancy specifically about the target variable.

Result is better than random feature selection and about the same as exhaustive search.  So, on this data set (known to work with various feature combos) the technique works and is less computationally intensive that brute force train-classifier-on-all-combinations search.  It's not compared to pariwise MI techniques, though, like Deng & Peng.

Seems like it would be better to compute partial MI, which does an approximately true stepwise search of true MI because MI can be factored into sums of partial MI (May11revVarSelNN)},
  doi       = {10.1371/journal.pone.0089840},
  file      = {Wang14featSelChemMI.pdf:Wang14featSelChemMI.pdf:PDF},
  groups    = {Read},
  publisher = {Public Library of Science},
}

@InProceedings{Oliva13dist2distRgrssn,
  author    = {Oliva, Junier and P{\'o}czos, Barnab{\'a}s and Schneider, Jeff},
  title     = {Distribution to distribution regression},
  booktitle = {Proceedings of The 30\textsuperscript{th} International Conference on Machine Learning},
  year      = {2013},
  pages     = {1049--1057},
  abstract  = {We analyze Distribution to Distribution re-
gression? where one is regressing a mapping
where both the covariate (inputs) and re-
sponse (outputs) are distributions. No pa-
rameters on the input or output distributions
are assumed, nor are any strong assumptions
made on the measure from which input dis-
tributions are drawn from. We develop an
estimator and derive an upper bound for the
L2 risk; also, we show that when the effective
dimension is small enough (as measured by
the doubling dimension), then the risk con-
verges to zero with a polynomial rate.},
  comment   = {Predict a distribution conditional on some other distribution. The input and output distributions don't have to have the same number of samples. Useful, for example, when you get many samples in one time frame and want to predict the distribution of the samples in the next time frame. Could be used for intraday market price forecasting, where use spot prices in 15 min period to predict distribution of prices in next 15 min period (or maybe in the next hour). Could be transactions or could be distribution of offers.

Potential uses: 1) ensemble calibration (inputs are ensemble wind
 speeds, outputs are ensemble wind powers).
 - would only make sense for single farm since speeds don't add
 - would be a kind of [[Power Curve Learning]]
 2) quantile persistence forecast, as in Pritchard10varQuantProbFrcst
 - similar to Markov time-dependence for prob forecast
 3.) could be used for analog ensembles (forecast from past distributions)
 (as in Nissen12analogEnsPowFrcst)

* a simpler dist-to-dist forecast: Pritchard10varQuantProbFrcst},
  file      = {Oliva13dist2distRgrssn.pdf:Oliva13dist2distRgrssn.pdf:PDF},
  url       = {http://jmlr.org/proceedings/papers/v28/oliva13.pdf},
}

@Article{Garcke17dimRedWindTurb,
  author    = {Garcke, Jochen and Iza-Teran, Rodrigo and Marks, Marvin and Pathare, Mandar and Schollbach, Dirk and Stettner, Martin},
  title     = {Dimensionality Reduction for the Analysis of Time Series Data from Wind Turbines},
  journal   = {To Appear (sometime, in some journal)},
  year      = {2017},
  abstract  = {We are addressing two related applications for the analysis of data from
wind turbines. First, we consider time series data arising from virtual sensors in
numerical simulations as employed during product development, and, second, we
investigate sensor data from condition monitoring systems of installed wind turbines.
For each application we propose a data analysis procedure based on dimensionality
reduction. In the case of virtual product development we develop tools to assist
the engineer in the process of analyzing the time series data from large bundles of
numerical simulations in regard to similarities or anomalies. For condition monitoring
we develop a procedure which detects damages early in the sensor data stream.},
  comment   = {Is used for Anomaly detection.  Uses PCA and dynamic time warping.  Certainly relevant to ModernWindABS.},
  file      = {:papers\\Garcke17dimRedWindTurb.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.07.10},
  url       = {http://garcke.ins.uni-bonn.de/research/pub.php?list=garcke},
}

@Article{Islam13pvAdoptTimeAttributes,
  author   = {Towhidul Islam and Nigel Meade},
  title    = {The impact of attribute preferences on adoption timing: The case of photo-voltaic (PV) solar cells for household electricity generation},
  journal  = {Energy Policy},
  year     = {2013},
  volume   = {55},
  pages    = {521 - 530},
  issn     = {0301-4215},
  note     = {Special section: Long Run Transitions to Sustainable Economic Structures in the European Union and Beyond},
  abstract = {We are concerned with micro-generation, individual households generating electricity using a renewable technology. We focus on modeling the adoption probability of photo-voltaic solar panels by households. Using data from Ontario, Canada where a generous feed-in-tariff is available to households generating electricity from solar panels, we measure household level preferences for panels and use these preferences along with household characteristics to predict adoption time intentions. We use discrete choice experiments to measure household level preferences and establish a causal link between the attributes of the technology and adoption time intentions using discrete time survival mixture analysis. Significant preferences included lower cost, greater energy savings and lower fossil fuel inflation. The conditional (hazard) probability of adoption at a particular time given no previous adoption showed that the attribute preferences had intuitively reasonable effects. The hazard probabilities allow us to compute the cumulative probability of adoption over a 10-year period per household. Technology awareness has a significant effect on the adoption probability, reinforcing the need for effective education. Our approach indicates the level of heterogeneity in preferences, particularly high for investment criteria and CO2 emissions. These findings suggest that education campaigns should explain more about investment criteria, feed-in tariffs and environmental effects.},
  doi      = {https://doi.org/10.1016/j.enpol.2012.12.041},
  keywords = {Micro-generation technology, Attribute preferences, Adoption timing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0301421512010944},
}

@InProceedings{Brochero13ensSelEvoMultipOpt,
  author    = {Brochero, Darwin and Gagn{\'e}, Christian and Anctil, Fran{\c{c}}ois},
  title     = {Evolutionary Multiobjective Optimization for Selecting Members of an Ensemble Streamflow Forecasting Model},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  year      = {2013},
  pages     = {1--8},
  abstract  = {We are proposing to use the Nondominated Sorting Genetic
Algorithm II (NSGA-II) for optimizing a hydrological forecasting
model of 800 simultaneous stream
ow predictors.
The optimization is based on the selection of the best 48 predictors
from the 800 that jointly define the \best" ensemble
in terms of two probabilistic criteria. Results showed that
the diculties in simplifying the ensembles mainly originate
from the preservation of the system reliability. We conclude
that Pareto fronts generated with NSGA-II allow the development
of a decision process based explicitly on the trade-o
between dierent probabilistic properties. In other words,
evolutionary multiobjective optimization oers more
exibility
to the operational hydrologists than a priori methods
that produce only one selection.

Categories and Subject Descriptors
I.2.8 [Artificial Intelligence]: Problem Solving, Control
Methods, and Search|Heuristic methods; I.6.6 [Simulation
and Modelling]: Simulation Output Analysis},
  comment   = {Biggest loss in hydro ensemble selection is preservation of system reliability. Combinatoric multi-objective opt trades off different probabilistic criteria.

An improvement (I think) of: Brochero11ensSelGreedy},
  file      = {Brochero13ensSelEvoMultipOpt.pdf:Brochero13ensSelEvoMultipOpt.pdf:PDF},
  groups    = {Ensemble, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2014.01.29},
  url       = {http://www.sigevo.org/gecco-2013/},
}

@Article{Cook13EnvlpPLS,
  author    = {Cook, RD and Helland, IS and Su, Z},
  title     = {Envelopes and partial least squares regression},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year      = {2013},
  volume    = {75},
  number    = {5},
  pages     = {851--877},
  abstract  = {We build connections between envelopes, a recently proposed context for efficient estimation in multivariate statistics, and multivariate partial least squares (PLS) regression. In particular, we establish an envelope as the nucleus of both univariate and multivariate PLS, which opens the door to pursuing the same goals as PLS but using different envelope estimators. It is argued that a likelihood-based envelope estimator is less sensitive to the number of PLS components that are selected and that it outperforms PLS in prediction and estimation.

Keywords:

 Dimension reduction;
 Envelope models;
 Envelopes;
 Maximum likelihood estimation;
 Partial least squares;
 SIMPLS algorithm},
  comment   = {Dimension reduction that is regression target (possibly multivariate) aware. Related to PLS but better. Fast start algorithm in 2014 paper.

Matlab envelope toolbox: Cook14envlpMatlab
Fast initializaton: Cook14envlpEstAlgs},
  doi       = {10.1111/rssb.12018/full},
  file      = {Cook13EnvlpPLS.pdf:Cook13EnvlpPLS.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.10.28},
}

@InProceedings{Saatcci10chngPtGausProc,
  author    = {Yunus Saat{\c{c}}i and Ryan Turner and Carl Edward Rasmussen},
  title     = {{Gauss}ian Process Change Point Models},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2010},
  month     = jun,
  abstract  = {We combine Bayesian online change point detection with Gaussian processes to create a nonparametric time series model which can handle change points. The model can be used to locate change points in an online manner; and, unlike other Bayesian online change point detection algorithms, is applicable when temporal correlations in a regime are expected. We show three variations on how to apply Gaussian processes in the change point context, each with their own advantages. We present methods to reduce the computational burden of these models and demonstrate it on several real world data sets.oo},
  comment   = {use for online regime detection, streamwise feature selection, adaptation, maybe extreme event warnings?},
  file      = {Saatcci10chngPtGausProc.pdf:Saatcci10chngPtGausProc.pdf:PDF},
  location  = {Haifa, Israel},
  owner     = {scot},
  timestamp = {2010.08.02},
  url       = {http://www.icml2010.org/abstracts.html},
}

@InProceedings{Rasmus15semiSupLadder,
  author    = {Rasmus, Antti and Berglund, Mathias and Honkala, Mikko and Valpola, Harri and Raiko, Tapani},
  title     = {Semi-supervised learning with ladder networks},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2015},
  pages     = {3546--3554},
  abstract  = {We combine supervised learning with unsupervised learning in deep neural net-
works. The proposed model is trained to simultaneously minimize the sum of su-
pervised and unsupervised cost functions by backpropagation, avoiding the need
for layer-wise pre-training. Our work builds on top of the Ladder network pro-
posed by Valpola [1] which we extend by combining the model with supervi-
sion. We show that the resulting model reaches state-of-the-art performance in
semi-supervised MNIST and CIFAR-10 classification in addition to permutation-
invariant MNIST classification with all labels.},
  comment   = {Deep neural net learning, extends ideas in Valpola15neurPCA2deepUnsupLrnBk

Extended and maybe simplified in Pezeshki16dcnstrLadderNet
Tutorial slides: Raiko15supUnsupCombLadder},
  file      = {Rasmus15semiSupLadder.pdf:Rasmus15semiSupLadder.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.02.01},
  url       = {https://papers.nips.cc/paper/5947-semi-supervised-learning-with-ladder-networks.pdf},
}

@Article{Li14DiscussAdaptRobustUC,
  author    = {Li, Y.Z.},
  title     = {Discussion of "Adaptive Robust Optimization for the Security Constrained Unit Commitment Problem"},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2014},
  volume    = {29},
  number    = {2},
  pages     = {996--996},
  month     = mar,
  issn      = {0885-8950},
  abstract  = {We commend the authors of [1] for application of an interesting optimization methodology, i.e., adaptive robust optimization to solve the security constrained unit commitment problem (SCUC). They present a two-stage adaptive robust unit commitment model for SCUC considering the uncertainty of nodal net injection in power system. The two-stage adaptive model is intended to obtain the first-stage unit commitment (UC) decision and the second-stage dispatch decision to become robust against all uncertain nodal net injection realizations. In [1], the first-stage UC decision is a vector of binary variables, which includes the on/off and start-up/shut-down status of each generation unit for each time interval of the commitment period. The second-stage dispatch decision is a vector of continuous variables, composed of the generation output, load consumption levels, resource reserve levels, and power flows in the transmission network for each time interval. In addition, the authors have pointed out that generation output, resource reserve levels, and power flow take positive sign, whereas load consumption levels take negative sign. Therefore, the discusser has certain doubt about [1, (8)]. If each element of the second-stage dispatch decision can take any real number, then equation (8) is of no question. However, actually, each element of takes either positive or negative sign as the authors pointed out. As a result, the discusser doubts about equation (8). This question is quite important because the dual of the dispatch problem, (8), is the mathematical basis of the proposed two-stage adaptive robust UC model.},
  comment   = {Casts doubt upon: Bertsimas13adptRobustOptUC},
  doi       = {10.1109/TPWRS.2014.2301351},
  file      = {Li14DiscussAdaptRobustUC.pdf:Li14DiscussAdaptRobustUC.pdf:PDF},
  keywords  = {optimisation;power generation dispatch;vectors;SCUC;adaptive robust optimization;binary variables vector;first-stage unit commitment decision;generation output;load consumption levels;optimization methodology;power flow;power system;resource reserve levels;second-stage dispatch decision;security constrained unit commitment problem;two-stage adaptive robust unit commitment model;uncertain nodal net injection realizations;Adaptation models;Adaptive systems;Mathematical model;Optimization;Power systems;Robustness;Security},
  owner     = {sotterson},
  timestamp = {2014.12.05},
}

@InProceedings{Ng02discrimVsGenNBvsLogRgrsn,
  author    = {Ng, Andrew Y and Jordan, Michael I},
  title     = {On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes},
  booktitle = {Advances in neural information processing systems},
  year      = {2002},
  pages     = {841--848},
  abstract  = {We compare discriminative and generative learning as typified by 
logistic regression and naive Bayes. We show, contrary to a widely-
held belief that discriminative classifiers are almost always to be 
preferred, that there can often be two distinct regimes of per-
formance as the training set size is increased, one in which each 
algorithm does better. This stems from the observation- which 
is borne out in repeated experiments- that while discriminative 
learning has lower asymptotic error, a generative classifier may also 
approach its (higher) asymptotic error much faster. },
  comment   = {Naive Bayes classifier can converge faster on small data sets than logistic regression, even though logistic regression is discriminative.  Maybe NB is a start for electric car probs.  Does this paper do ridge regression or LASSO on LR?  Seems like that might matter.

A possibly more readable chunk on this topic: Mitchell17nBayesLogRegBkCh},
  file      = {:Ng02discrimVsGenNBvsLogRgrsn.pdf:PDF},
  url       = {http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf},
}

@InProceedings{Wang06compGausProcNeuralForecast,
  author    = {Tzai-Der Wang and Shang-Jen Chuang and Colin Fyfe},
  title     = {Comparing {Gauss}ian Processes and Artificial Neural Networks for Forecasting},
  booktitle = {Joint Conference on Information Sciences (JCIS)},
  year      = {2006},
  abstract  = {We compare the use of artificial neural networks and
Gaussian processes for forecasting. We show that Ar-
tificial Neural Networks have the advantage of being
utilisable with greater volumes of data but Gaussian
processes can more easily be utilised to deal with non-
stationarity.},
  comment   = {Special GP kernel helps w/ nonstationarity (captures trend) better than MLP, but GP is fiddly
* is doing only one-step-ahead prediction (w/ 13 past samples in input)

* add a scalar product kernel to normal exponential kernel to capture trend
* extra bit in kernel -- causes covariance matrix instability, requiring use of less data
-- adds an extra gradient wal to find an extra hyper parameter
-- twiddling with learning weights
-- on periodic data w/ trends, this allows GP to outperform an MLP

* standard way of handling periodicities in GP doesn't work
* they say trend was harder to model than periodic (surprising to me)
* could trends be tracked just by including derivatives in input?

* also mentioned computational cost of GP's, as usual.
* claims that GP handles missing data better than MLP},
  doi       = {10.2991/jcis.2006.7},
  file      = {Wang06compGausProcNeuralForecast.pdf:Wang06compGausProcNeuralForecast.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2008.10.03},
  url       = {http://www.csu.edu.tw/csitshow/Hmanager/95data/423.pdf},
}

@InProceedings{Ghoshal07clustRegressDirMix,
  author    = {Subhashis Ghoshal and Changku Kang},
  title     = {Clusterwise Regression Using Dirichlet Mixtures},
  booktitle = {JSM},
  year      = {2007},
  pages     = {1624--1631},
  abstract  = {We consider a clustering based approach to non-parametric regression when data come from finitely many hidden sub-populations in each of which a simple parametric regression model holds. We try to recover the lost labels by a Bayesian clustering technique based on Dirichlet mixture of normals. Because sample is not split regionwise, we avoid the curse of dimensionality problem in higher dimension. The clusters are formed automatically within an MCMC scheme. Model parameters are estimated by least square method in each cluster. An ensemble of parametric regression estimates are formed, each based on a configuration formed in each MCMC step, and a simple averaging produces the final estimate. Our method also gives confidence bands and compares favorably with kernel, spline or GAM based method when clusters overlap less. The method is applied to analyze a donation data.},
  comment   = {MCMC clustering of linear regressors.

2008 paper includes num. of mixtures as an unknown and figures it out see also google cache b/c 2008 paper reference disappeared
http://209.85.173.132/search?q=cache:x0KmMhL1uksJ:www4.stat.ncsu.edu/~sghosal/papers.html+CLUSTERWISE+REGRESSION+USING+DIRICHLET+MIXTURES&hl=en&ct=clnk&cd=2&gl=us&client=firefox-a},
  file      = {:Ghoshal07clustRegressDirMix.pdf:PDF;Ghoshal07clustRegressDirMix.pdf:Ghoshal07clustRegressDirMix.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.12.12},
  url       = {http://www.amstat.org/meetings/jsm/2007/onlineprogram/index.cfm?fuseaction=abstract_details&abstractid=308580},
}

@Article{Staedler10mixRgrsnL1pen,
  author    = {St{\"a}dler, Nicolas and B{\"u}hlmann, Peter and van de Geer, Sara},
  title     = {l1-penalization for mixture regression models},
  journal   = {Test},
  year      = {2010},
  volume    = {19},
  number    = {2},
  pages     = {209--256},
  abstract  = {We consider a finite mixture of regressions (FMR) model for high-dimensional inhomogeneous data where the number of covariates may be much larger than sample size. We propose an l1-penalized maximum likelihood estimator in an appropriate parameterization. This kind of estimation belongs to a class of problems where optimization and theory for non-convex functions is needed. This distinguishes itself very clearly from high-dimensional estimation with convex loss- or objective functions as, for example, with the Lasso in linear or generalized linear models. Mixture models represent a prime and important example where non-convexity arises.

For FMR models, we develop an efficient EM algorithm for numerical optimization with provable convergence properties. Our penalized estimator is numerically better posed (e.g., boundedness of the criterion function) than unpenalized maximum likelihood estimation, and it allows for effective statistical regularization including variable selection. We also present some asymptotic theory and oracle inequalities: due to non-convexity of the negative log-likelihood function, different mathematical arguments are needed than for problems with convex losses. Finally, we apply the new method to both simulated and real data.

Keywords Adaptive Lasso, Finite mixture models, Generalized EM algorithm, Highdimensional
estimation, Lasso, Oracle inequality},
  comment   = {Feature selection for mixed regression models (different that clustered regression, I think). Anyway has L1-norm feature selection.},
  doi       = {10.1007/s11749-010-0197-z},
  file      = {Staedler10mixRgrsnL1pen.pdf:Staedler10mixRgrsnL1pen.pdf:PDF;Discussion paper (ArXiv):Staedler10mixRgrsnL1pen_discPpr.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_1},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2014.04.04},
}

@InProceedings{Seo00gausProcActiveSelRej,
  author    = {Sambu Seo and Wallat, M. and Graepel, T. and Obermayer, K.},
  title     = {{Gauss}ian process regression: active data selection and test point rejection},
  booktitle = {International Joint Conference on Neural Networks (IJCNN)},
  year      = {2000},
  volume    = {3},
  pages     = {241--246},
  abstract  = {We consider active data selection and test point rejection strategies for Gaussian process regression based on the variance of the posterior over target values. Gaussian process regression is viewed as transductive regression that provides target distributions for given points rather than selecting an explicit regression function. Since not only the posterior mean but also the posterior variance are easily calculated we use this additional information to two ends: active data selection is performed by either querying at points of high estimated posterior variance or at points that minimize the estimated posterior variance averaged over the input distribution of interest or (in a transductive manner) averaged over the test set. Test point rejection is performed using the estimated posterior variance as a confidence measure. We find that, for both a two-dimensional toy problem and a real-world benchmark problem, the variance is a reasonable criterion for both active data selection and test point rejection},
  comment   = {a way to keep the num. points stored in a gaussian process small? Also, outlier rejection during online training?

Could use for probabilistic forecast feature selection, where are looking at effect of a feature on the pdf. This might be a quick way to do it, somehow.},
  doi       = {10.1109/IJCNN.2000.861310},
  file      = {Seo00gausProcActiveSelRej.pdf:Seo00gausProcActiveSelRej.pdf:PDF;Seo00gausProcActiveSelRej.pdf:Seo00gausProcActiveSelRej.pdf:PDF},
  groups    = {Ensemble, PointDerived, doReadNonWPV_1},
  journal   = {International Joint Conference on Neural Networks (IJCNN)},
  keywords  = {Gaussian distribution, covariance matrices, estimation theory, learning (artificial intelligence), neural nets, statistical analysisGaussian process regression, active data selection, active learning, covariance matrix, function estimation, neural nets, posterior variance, test point rejection, transductive regression, two-dimensional toy problem},
  owner     = {sotterson},
  timestamp = {2009.03.02},
}

@Article{Wood14GnrlzdGAMlrgDat,
  author    = {Wood, Simon N and Goude, Yannig and Shaw, Simon},
  title     = {Generalized additive models for large data sets},
  journal   = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  year      = {2014},
  abstract  = {We consider an application in electricity grid load prediction, where generalized additive models are appropriate, but where the data set's size can make their use practically intractable with existing methods. We therefore develop practical generalized additive model fitting methods for large data sets in the case in which the smooth terms in the model are represented by using penalized regression splines. The methods use iterative update schemes to obtain factors of the model matrix while requiring only subblocks of the model matrix to be computed at any one time. We show that efficient smoothing parameter estimation can be carried out in a well-justified manner. The grid load prediction problem requires updates of the model fit, as new data become available, and some means for dealing with residual auto-correlation in grid load. Methods are provided for these problems and parallel implementation is covered. The methods allow estimation of generalized additive models for large data sets by using modest computer hardware, and the grid load prediction problem illustrates the utility of reduced rank spline smoothing methods for dealing with complex modelling problems.

Keywords:

 Correlated additive model;
 Electricity load prediction;
 Generalized additive model estimation},
  comment   = {High dim (I think) models w/ autocorrelations. Used for electricity demand forecast. I think this has the smoothing spline stuff too. Has R, maybe Matlab

Intro papers start here: Wood03ThinPltRgrsSpln

Might have matlab toolbox here: Wood08FastSmthGAM

gam{mgcv} seems to be the R function
https://stat.ethz.ch/pipermail/r-help/2011-September/290434.html

* expression for effective degrees of freedom on p.3. Is related to splines, maybe could be adapted to selecting range of lambdas for penalized splines, as is done in ridge regression.},
  doi       = {10.1111/rssc.12068/full},
  file      = {Wood14GnrlzdGAMlrgDat.pdf:Wood14GnrlzdGAMlrgDat.pdf:PDF},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2014.07.25},
}

@Article{Schmid07BoostAddPspline,
  author    = {Schmid, Matthias and Hothorn, Torsten},
  title     = {Boosting additive models using component-wise P-splines},
  year      = {2007},
  abstract  = {We consider an efficient approximation of Buhlmann & Yu's L2Boosting algorithm
with component-wise smoothing splines. Smoothing spline base-learners are replaced
by P-spline base-learners which yield similar prediction errors but are more advantageous
from a computational point of view. In particular, we give a detailed analysis
on the effect of various P-spline hyper-parameters on the boosting fit. In addition,
we derive a new theoretical result on the relationship between the boosting stopping
iteration and the step length factor used for shrinking the boosting estimates.
Key words: L2Boosting, P-splines, smoothing splines, additive models, variable selection,
component-wise base-learners},
  comment   = {Related to boosted quantile regression in Fenske11idRiskBoostAddQR
Tech note: there's a journal paper in 2008.

Mayr14evoBoost seems to prefer p-splines for boosting base learners. Maybe the ones in this paper are the kind he means?},
  file      = {Schmid07BoostAddPspline.pdf:Schmid07BoostAddPspline.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.07.25},
  url       = {http://epub.ub.uni-muenchen.de/2057/},
}

@Article{Buehlmann13corrRgrsnClustGlasso,
  author    = {B{\"u}hlmann, Peter and R{\"u}timann, Philipp and van de Geer, Sara and Zhang, Cun-Hui},
  title     = {Correlated variables in regression: clustering and sparse estimation},
  journal   = {Journal of Statistical Planning and Inference},
  year      = {2013},
  volume    = {143},
  number    = {11},
  pages     = {1835--1858},
  abstract  = {We consider estimation in a high-dimensional linear model with strongly correlated variables. We propose to cluster the variables first and do subsequent sparse estimation such as the Lasso for cluster-representatives or the group Lasso based on the structure from the clusters. Regarding the first step, we present a novel and bottom-up agglomerative clustering algorithm based on canonical correlations, and we show that it finds an optimal solution and is statistically consistent. We also present some theoretical arguments that canonical correlation based clustering leads to a better-posed compatibility constant for the design matrix which ensures identifiability and an oracle inequality for the group Lasso. Furthermore, we discuss circumstances where cluster-representatives and using the Lasso as subsequent estimator leads to improved results for prediction and detection of variables. We complement the theoretical analysis with various empirical results.
Keywords

 Canonical correlation;
 Group Lasso;
 Hierarchical clustering;
 High-dimensional inference;
 Lasso;
 Oracle inequality;
 Variable screening;
 Variable selection},
  comment   = {linear regression if have wind direction splines in the feature vector (group lasso)},
  file      = {Buehlmann13corrRgrsnClustGlasso.pdf:Buehlmann13corrRgrsnClustGlasso.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.04.04},
  url       = {http://www.sciencedirect.com/science/article/pii/S0378375813001225},
}

@Article{Chen13covPrecMatEstHiDimTseries,
  author    = {Chen, Xiaohui and Xu, Mengyu and Wu, Wei Biao},
  title     = {Covariance and precision matrix estimation for high-dimensional time series},
  journal   = {Ann. Statist.},
  year      = {2013},
  volume    = {41},
  number    = {6},
  pages     = {2994--3021},
  month     = {12},
  abstract  = {We consider estimation of covariance matrices and their inverses (a.k.a. precision matrices) for high-dimensional stationary and locally stationary time series. In the latter case the covariance matrices evolve smoothly in time, thus forming a covariance matrix function. Using the functional dependence measure of Wu [Proc. Natl. Acad. Sci. USA 102 (2005) 14150?14154 (electronic)], we obtain the rate of convergence for the thresholded estimate and illustrate how the dependence affects the rate of convergence. Asymptotic properties are also obtained for the precision matrix estimate which is based on the graphical Lasso principle. Our theory substantially generalizes earlier ones by allowing dependence, by allowing nonstationarity and by relaxing the associated moment conditions.},
  comment   = {Precision matrix from graphical LASSO, and it's locally dependent (adaptive?  Could be dependent on wind direction?)},
  doi       = {10.1214/13-AOS1182},
  file      = {Chen13covPrecMatEstHiDimTseries.pdf:Chen13covPrecMatEstHiDimTseries.pdf:PDF},
  fjournal  = {The Annals of Statistics},
  publisher = {The Institute of Mathematical Statistics},
  url       = {http://dx.doi.org/10.1214/13-AOS1182},
}

@Article{Belloni11sparseQRl1,
  author    = {Belloni, Alexandre and Chernozhukov, Victor and others},
  title     = {l1-penalized quantile regression in high-dimensional sparse models},
  journal   = {The Annals of Statistics},
  year      = {2011},
  volume    = {39},
  number    = {1},
  pages     = {82--130},
  abstract  = {We consider median regression and, more generally, a possibly infinite collection of quantile regressions in high-dimensional sparse models. In these models, the number of regressors p is very large, possibly larger than the sample size n, but only at most s regressors have a nonzero impact on each conditional quantile of the response variable, where s grows more slowly than n. Since ordinary quantile regression is not consistent in this case, we consider ?1-penalized quantile regression (l1-QR), which penalizes the l1-norm of regression coefficients, as well as the post-penalized QR estimator (post-l1-QR), which applies ordinary QR to the model selected by l1-QR. First, we show that under general conditions l1-QR is consistent at the near-oracle rate $\sqrt{s/n}\sqrt{\log(p\vee n)}$, uniformly in the compact set $\mathcal{U}\subset(0,1)$ of quantile indices. In deriving this result, we propose a partly pivotal, data-driven choice of the penalty level and show that it satisfies the requirements for achieving this rate. Second, we show that under similar conditions post-l1-QR is consistent at the near-oracle rate $\sqrt{s/n}\sqrt{\log(p\vee n)}$, uniformly over $\mathcal{U}$, even if the l1-QR-selected models miss some components of the true models, and the rate could be even closer to the oracle rate otherwise. Third, we characterize conditions under which l1-QR contains the true model as a submodel, and derive bounds on the dimension of the selected model, uniformly over $\mathcal{U}$; we also provide conditions under which hard-thresholding selects the minimal true model, uniformly over $\mathcal{U}$.},
  comment   = {After L-1 norm penalty linear QR, you may get better results by retraining normal linQR w/ only the selected features. Penalty coeff can be determined automatically, although the referenced paper is about additive models, it seems Has matlab and R. Compare w/ locLin p-Gaussian?

Matlab and R available on request
http://www.mit.edu/~vchern/

Paper comes from Arivx
http://arxiv.org/abs/0904.2931

Belloni13progEvalHiDim says that inference and featsel is hard, error prone.

Explanation of the penalty implementation is in:},
  doi       = {10.1214/10-AOS827},
  file      = {Belloni11sparseQRl1.pdf:Belloni11sparseQRl1.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_1},
  owner     = {sotterson},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2014.02.25},
  url       = {http://projecteuclid.org/euclid.aos/1291388370#info},
}

@Article{Shu14EstLrgCovMatTempDep,
  author      = {Hai Shu and Bin Nan},
  title       = {Estimation of Large Covariance and Precision Matrices from Temporally Dependent Observations},
  abstract    = {We consider the estimation of large covariance and precision matrices from high-dimensional sub-Gaussian observations with slowly decaying temporal dependence that is bounded by certain polynomial decay rate. The temporal dependence is allowed to be long-range so with longer memory than those considered in the current literature. The rates of convergence are obtained for the generalized thresholding estimation of covariance and correlation matrices, and for the constrained $\ell_1$ minimization and the $\ell_1$ penalized likelihood estimation of precision matrix. Properties of sparsistency and sign-consistency are also established. A gap-block cross-validation method is proposed for the tuning parameter selection, which performs well in simulations. As our motivating example, we study the brain functional connectivity using resting-state fMRI time series data with long-range temporal dependence.},
  comment     = {Models high dimensional, long memory processes with polynomial decay dominated (PDD) temporal dependence.  I believe that this also allows short-memory, all the way to i.i.d. (see eq. 1)},
  date        = {2014-12-16},
  eprint      = {1412.5059v4},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  file        = {Shu14EstLrgCovMatTempDep.pdf:Shu14EstLrgCovMatTempDep.pdf:PDF},
  keywords    = {math.ST, stat.ML, stat.TH},
  owner       = {sotterson},
  timestamp   = {2017.06.14},
  url         = {https://arxiv.org/abs/1412.5059},
}

@InProceedings{Bach08bolasso,
  author    = {Bach, Francis R.},
  title     = {Bolasso: model consistent Lasso estimation through the bootstrap},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2008},
  pages     = {33--40},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {We consider the least-square linear regression problem with regularization by the ?1-norm, a problem usually referred to as the Lasso. In this paper, we present a detailed asymptotic analysis of model consistency of the Lasso. For various decays of the regularization parameter, we compute asymptotic equivalents of the probability of correct model selection (i.e., variable selection). For a specific rate decay, we show that the Lasso selects all the variables that should enter the model with probability tending to one exponentially fast, while it selects all other variables with strictly positive probability. We show that this property implies that if we run the Lasso for several bootstrapped replications of a given sample, then intersecting the supports of the Lasso bootstrap estimates leads to consistent model selection. This novel variable selection algorithm, referred to as the Bolasso, is compared favorably to other linear regression methods on synthetic data and datasets from the UCI machine learning repository},
  comment   = {sparseness-enforcing linear regression, based on intersection of many bootstrapped runs ==> _easy to parallelize_ * video: http://videolectures.net/icml08_bach_bmcle/ * matlab: http://www.mathworks.com/matlabcentral/fileexchange/?term=tag%3A%22bolasso%22},
  doi       = {10.1145/1390156.1390161},
  file      = {Bach08bolasso.pdf:Bach08bolasso.pdf:PDF;Bach08bolasso.pdf:Bach08bolasso.pdf:PDF},
  isbn      = {978-1-60558-205-4},
  location  = {Helsinki, Finland},
  owner     = {sotterson},
  timestamp = {2009.08.12},
}

@InProceedings{SantosTeixeira10anomSubSpcTrk,
  author    = {dos Santos Teixeira, Pedro Henriques and Milidi{\'u}, Ruy Luiz},
  title     = {Data stream anomaly detection through principal subspace tracking},
  booktitle = {ACM Symposium on Applied Computing (SAC)},
  year      = {2010},
  pages     = {1609--1616},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {We consider the problem of anomaly detection in multiple co-evolving data streams. In this paper, we introduce FRAHST (Fast Rank-Adaptive row-Householder Subspace Tracking). It automatically learns the principal subspace from N numerical data streams and an anomaly is indicated by a change in the number of latent variables. Our technique provides state-of-the-art estimates for the subspace basis and has a true dominant complexity of only 5Nr operations while satisfying all desirable streaming constraints. FRAHST successfully detects subtle anomalous patterns and when compared against four other anomaly detection techniques, it is the only with a consistent F1 80\% in the Abilene datasets as well as in the ISP datasets introduced in this work.},
  comment   = {Detect anomalies by noticing when subspace rank changes.
- sort of a sliding window PCA with lagged inputs, as in dynamic PCA Use for?
- extreme events, like ramps - prediction confidence intervals
- regime detection/switching},
  doi       = {10.1145/1774088.1774434},
  file      = {SantosTeixeira10anomSubSpcTrk.pdf:SantosTeixeira10anomSubSpcTrk.pdf:PDF},
  isbn      = {978-1-60558-639-7},
  location  = {Sierre, Switzerland},
  owner     = {scot},
  timestamp = {2010.08.04},
}

@TechReport{Tateish09nonLinLassoReg,
  author      = {Shohei Tateish and Hidetoshi Matsui and Sadanori Konishi},
  title       = {Nonlinear regression modeling via the lasso-type regularization},
  institution = {Education and Research Hub for Mathematics-for-Industry},
  year        = {2009},
  type        = {MI Preprint Series},
  number      = {MI 2009-8},
  month       = feb,
  abstract    = {We consider the problem of constructing nonlinear regression models with Gaussian basis functions, using lasso regularization. Regularization with a lasso penalty is an advantageous in that it reduces some unknown parameters in linear regression models toward exactly zero. We propose imposing a weighted lasso penalty on a nonlinear regression model and thereby selecting the number of basis functions effectively. In order to select tuning parameters in the regularization method, we use model selection criteria derived from information-theoretic and Bayesian viewpoints. Simulation results demonstrate that our methodology performs well in various situations.},
  comment     = {num. of Gaussian regression basises selected w/ lasso. Information-theoretic tuning. Is this a Gaussian Processes?
* some of this looks not-too-hard to implement
* OK for lots of points?},
  file        = {Tateish09nonLinLassoReg.pdf:Tateish09nonLinLassoReg.pdf:PDF;Tateish09nonLinLassoReg.pdf:Tateish09nonLinLassoReg.pdf:PDF},
  location    = {Fukuoka, Japan},
  owner       = {sotterson},
  timestamp   = {2009.08.14},
  url         = {http://gcoe-mi.jp/english/publish_list/pub_inner/id:3/cid:11},
}

@Article{Lyu09indepRadGauss,
  author   = {Siwei Lyu and Simoncelli, Eero P.},
  title    = {Nonlinear Extraction of Independent Components of Natural Images Using Radial {Gauss}ianization.},
  journal  = {Neural Computation},
  year     = {2009},
  volume   = {21},
  number   = {6},
  pages    = {1485--1519},
  issn     = {0899-7667},
  abstract = {We consider the problem of efficiently encoding a signal by transforming it to a new representation whose components are statistically independent. A widely studied linear solution, known as independent component analysis (ICA), exists for the case when the signal is generated as a linear transformation of independent nongaussian sources. Here, we examine a complementary case, in which the source is nongaussian and elliptically symmetric. In this case, no invertible linear transform suffices to decompose the signal into independent components, but we show that a simple nonlinear transformation, which we call radial gaussianization (RG), is able to remove all dependencies. We then examine this methodology in the context of natural image statistics. We first show that distributions of spatially proximal band pass filter responses are better described as elliptical than as linearly transformed independent sources. Consistent with this, we demonstrate that the reduction in dependency achi},
  comment  = {Simple PCA and nonlinear approach to gaussianization that I believe may allow modeling heavier tail than a straight Gaussian distribution.
Does that mean it can model heavy tail =correlation=? The PCA steps would be easy to invert. Is there also a trick allowing the inversion of the radial transform? Spinning reserves scenario generation?},
  file     = {TechNote, nicer graphs:Lyu09indepRadGaussTechNote.pdf:PDF;Lyu09indepRadGauss.pdf:Lyu09indepRadGauss.pdf:PDF},
  keywords = {GAUSSIAN processes, DEPENDENCE (Statistics), NEURAL networks (Neurobiology), NEURAL transmission, NEURONS},
  url      = {http://search.ebscohost.com.globalproxy.cvt.dk/login.aspx?direct=true&db=afh&AN=40396048&site=ehost-live},
}

@InProceedings{Gomes10budgetStrmLrn,
  author    = {R. Gomes and A. Krause},
  title     = {Budgeted Nonparametric Learning from Data Streams},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2010},
  abstract  = {We consider the problem of extracting informative exemplars from a data stream. Examples of this problem include exemplar based clustering and nonparametric inference such as Gaussian process regression on massive data sets. We show that these problems require maximization of a submodular function that captures the informativeness of a set of exemplars, over a data stream. We develop an efficient algorithm, Stream-Greedy, which is guaranteed to obtain a constant fraction of the value achieved by the optimal solution to this NP-hard optimization problem. We extensively evaluate our algorithm on large real-world data sets.},
  comment   = {use for fast learning of wind farm characteristics during forecasting algorithm burn in?},
  file      = {Gomes10budgetStrmLrn.pdf:Gomes10budgetStrmLrn.pdf:PDF},
  location  = {Haifa, Israel},
  owner     = {scot},
  timestamp = {2010.08.02},
  url       = {http://www.icml2010.org/abstracts.html},
}

@Article{Messina08hmmScenGen,
  author    = {Messina, Enza and Toscani, Daniele},
  title     = {Hidden {Markov} models for scenario generation},
  journal   = {IMA Journal of Management Mathematics},
  year      = {2008},
  volume    = {19},
  number    = {4},
  pages     = {379--401},
  abstract  = {We consider the problem of modelling processes sequentially changing behaviour and unexpected changes that can hinder finding the best approximation function. These dynamics cannot be observed directly either because they are masked by observational noise or because the process generating them is too complex and involves too many variables. In this paper, the problem of modelling financial time series has been approached using hidden Markov models (HMMs), which have been shown to be suitable for sequential data analysis and in particular for financial time series modelling and forecasting. HMMs are essentially data-driven models that allow us to focus attention on the observation generation process, which is indeed final objective. The goal of our time series analysis model is the generation of scenarios to be included in decision models. Therefore, our focus will not be on determining the best forecast but in capturing the generation process behaviour in order to characterize its possible evolutions.},
  comment   = {state switching autoregressive scenario generation for spinning reserves relate to Pierre's HMM paper?},
  doi       = {10.1093/imaman/dpm026},
  eprint    = {http://imaman.oxfordjournals.org/content/19/4/379.full.pdf+html},
  file      = {Messina08hmmScenGen.pdf:Messina08hmmScenGen.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.11.23},
  url       = {http://imaman.oxfordjournals.org/content/19/4/379.abstract},
}

@Article{Girard03gaussProcUncertainMultiStep,
  author      = {Girard, A. and Rasmussen, C.E. and Quinonero-Candela, J. and Murray-Smith, R.},
  title       = {{Gauss}ian Process priors with uncertain inputs -- Application to multiple-step ahead time series forecasting},
  year        = {2003},
  url         = {http://eprints.gla.ac.uk/3117/},
  abstract    = {We consider the problem of multi-step ahead prediction in time series analysis using the non-parametric Gaussian process model. k-step ahead forecasting of a discrete-time non-linear dynamic system can be performed by doing repeated one-step ahead predictions. For a state-space model of the form y t = f(Yt-1 ,..., Yt-L ), the prediction of y at time t + k is based on the point estimates of the previous outputs. In this paper, we show how, using an analytical Gaussian approximation, we can formally incorporate the uncertainty about intermediate regressor values, thus updating the uncertainty on the current prediction.},
  file        = {Girard03gaussProcUncertainMultiStep.pdf:Girard03gaussProcUncertainMultiStep.pdf:PDF;Girard03gaussProcUncertainMultiStep.pdf:Girard03gaussProcUncertainMultiStep.pdf:PDF},
  institution = {Glasgow ePrints Service [http://eprints.gla.ac.uk/perl/oai2] (United Kingdom)},
  journal     = {Advances in Neural Information Processing Systems (NIPS)},
  keywords    = {TK Electrical engineering. Electronics Nuclear engineering},
  location    = {http://www.scientificcommons.org/16479770},
  publisher   = {MIT Press},
}

@INPROCEEDINGS{Girard03multiStepGaussProc,
  Author                   = {Girard, A. and C. E. Rasmussen and J. Quionero-Candela and R. Murray-Smith},
  Title                    = {Multiple-step ahead prediction for non linear dynamic systems -- A {Gauss}ian Process treatment with propagation of the uncertainty},
  Booktitle                = {Advances in Neural Information Processing Systems (NIPS)},
  Year                     = {2003},
  Pages                    = {529--536},
  Abstract                 = {We consider the problem of multi-step ahead prediction in time series analysis using the non-parametric Gaussian process model. k-step ahead forecasting of a discrete-time non-linear dynamic system can be performed by doing repeated one-step ahead predictions. For a state-space model of the form y_t = f(y_{t-1},...,y_{t-L}), the prediction of y at time t + k is based on the point estimates of the previous outputs. In this paper, we show how, using an analytical Gaussian approximation, we can formally incorporate the uncertainty about intermediate regressor values, thus updating the uncertainty on the current prediction},
  File                     = {:Girard03multiStepGaussProc.pdf:PDF;Girard03multiStepGaussProc.pdf:Girard03multiStepGaussProc.pdf:PDF},
  Owner                    = {sotterson},
  Timestamp                = {2008.10.03},
  URL                      = {http://www.kyb.mpg.de/publication.html?publ=2105}
}

@Article{Aid15optTradeIntraday,
  author    = {A{\"\i}d, Ren{\'e} and Gruet, Pierre and Pham, Huy{\^e}n},
  title     = {An optimal trading problem in intraday electricity markets},
  journal   = {arXiv preprint arXiv:1501.04575},
  year      = {2015},
  abstract  = {We consider the problem of optimal trading for a power producer in the context of intraday electricity markets. The aim is to minimize the imbalance cost induced by the random residual demand in electricity, i.e. the consumption from the clients minus the production from renewable energy. For a simple linear price impact model and a quadratic criterion, we explicitly obtain approximate optimal strategies in the intraday market and thermal power generation, and exhibit some remarkable properties of the trading rate. Furthermore, we study the case when there are jumps on the demand forecast and on the intraday price, typically due to error in the prediction of wind power generation. Finally, we solve the problem when taking into account delay constraints in thermal power production.

Key words: Optimal trading, power plant, intraday electricity markets, renewable energy,
market impact, linear-quadratic control problem, jumps, delay.},
  comment   = {Optimal trading on EEX intraday market.

Was eventually published in Jan 2016 here:
http://link.springer.com/article/10.1007%2Fs11579-015-0150-8},
  file      = {Aid15optTradeIntraday.pdf:Aid15optTradeIntraday.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.03.31},
  url       = {http://arxiv.org/abs/1501.04575},
}

@Article{Lian10shrinkHiDimVarCoeff,
  author    = {Lian, Heng},
  title     = {Flexible shrinkage estimation in high-dimensional varying coefficient models},
  journal   = {arXiv preprint arXiv:1008.2271},
  year      = {2010},
  abstract  = {We consider the problem of simultaneous variable selection and constant coefficient identification in high-dimensional varying coefficient models based on B-spline basis expansion. Both objectives can be considered as some type of model selection problems and we show that they can be achieved by a double shrinkage strategy. We apply the adaptive group Lasso penalty in models involving a diverging number of covariates, which can be much larger than the sample size, but we assume the number of relevant variables is smaller than the sample size via model sparsity. Such so-called ultra-high dimensional settings are especially challenging in semiparametric models as we consider here and has not been dealt with before. Under suitable conditions, we show that consistency in terms of both variable selection and constant coefficient identification can be achieved, as well as the oracle property of the constant coefficients. Even in the case that the zero and constant coefficients are known a priori, our results appear to be new in that it reduces to semivarying coefficient models (a.k.a. partially linear varying coefficient models) with a diverging number of covariates. We also theoretically demonstrate the consistency of a semiparametric BIC-type criterion in this high-dimensional context, extending several previous results. The finite sample behavior of the estimator is evaluated by some Monte Carlo studies.
keywords: Adaptive Lasso; Extended BIC; B-spline basis; Semivarying
coe?cient models; Varying coe?cient models;},
  comment   = {featsel for high dimension varying coeff models. Good for boosting?},
  file      = {Lian10shrinkHiDimVarCoeff.pdf:Lian10shrinkHiDimVarCoeff.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.12.07},
  url       = {http://arxiv.org/abs/1008.2271},
}

@Article{Lu05sepCovMatLikRatTest,
  author    = {Nelson Lu and Dale L. Zimmerman},
  title     = {The likelihood ratio test for a separable covariance matrix},
  journal   = {Statistics \& Probability Letters},
  year      = {2005},
  volume    = {73},
  number    = {4},
  pages     = {449 - 457},
  issn      = {0167-7152},
  abstract  = {We consider the problem of testing whether a covariance matrix has a separable (Kronecker product) structure. Such structure is of particular interest when the observed variables can be cross-classified by two factors, as occurs for example when comparable or identical characteristics are measured on several parts of each subject. We derive the likelihood ratio test for separability on the basis of a random sample from a multivariate normal population, and we establish an invariance property of the test statistic that allows us to table its null distribution. An example illustrates the methodology.},
  comment   = {Can you do a matrix normal distribution type Kronecker product covariance separation?},
  doi       = {http://dx.doi.org/10.1016/j.spl.2005.04.020},
  file      = {:papers\\Lu05sepCovMatLikRatTest.pdf:PDF},
  keywords  = {Kronecker product, Likelihood ratio test, Separability},
  owner     = {sotterson},
  timestamp = {2017.07.11},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167715205001495},
}

@InProceedings{Ide09ProxAnomDetSprs,
  author       = {Id{\'e}, Tsuyoshi and Lozano, Aurelie C and Abe, Naoki and Liu, Yan},
  title        = {Proximity-Based Anomaly Detection Using Sparse Structure Learning.},
  booktitle    = {SDM},
  year         = {2009},
  pages        = {97--108},
  organization = {SIAM},
  abstract     = {We consider the task of performing anomaly detection in
highly noisy multivariate data. In many applications involv-
ing real-valued time-series data, such as physical sensor data
and economic metrics, discovering changes and anomalies
in the way variables depend on one another is of particular
importance. Our goal is to robustly compute the ?correla-
tion anomaly? score of each variable by comparing the test
data with reference data, even when some of the variables
are highly correlated (and thus collinearity exists). To re-
move seeming dependencies introduced by noise, we focus
on the most significant dependencies for each variable. We
perform this ?neighborhood selection? in an adaptive manner
by fitting a sparse graphical Gaussian model. Instead of tra-
ditional covariance selection procedures, we solve this prob-
lem as maximum likelihood estimation of the precision ma-
trix (inverse covariance matrix) under the L1 penalty. Then
the anomaly score for each variable is computed by evaluat-
ing the distances between the fitted conditional distributions
within the Markov blanket for that variable, for the (two) data
sets to be compared. Using real-world data, we demonstrate
that our matrix-based sparse structure learning approach suc-
cessfully detects correlation anomalies under collinearities
and heavy noise.},
  comment      = {Says structure learned by straight lasso is unstable.  Same reason that Belloni13progEvalHiDim says that inference and featsel is hard, error prone.
},
  file         = {:Ide09ProxAnomDetSprs.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2016.12.07},
  url          = {http://epubs.siam.org/doi/abs/10.1137/1.9781611972795.9},
}

@Article{Lichtendahl13probOrQuantAvg,
  author    = {Lichtendahl, Kenneth C and Grushka-Cockayne, Yael and Winkler, Robert L},
  title     = {Is It Better to Average Probabilities or Quantiles?},
  journal   = {Management Science},
  year      = {2013},
  abstract  = {We consider two ways to aggregate expert opinions using simple averages: averaging probabilities and averaging quantiles. We examine analytical properties of these forecasts and compare their ability to harness the wisdom of the crowd. In terms of location, the two average forecasts have the same mean. The average quantile forecast is always sharper: it has lower variance than the average probability forecast. Even when the average probability forecast is overconfident, the shape of the average quantile forecast still offers the possibility of a better forecast. Using probability forecasts for GDP growth and inflation from the Survey of Professional Forecasters, we present evidence that both when the average probability forecast is overconfident and when it is underconfident, it is outperformed by the average quantile forecast. Our results show that averaging quantiles is a viable alternative and indicate some conditions under which it is likely to be more useful than averaging probabilities.
Keywords: probability forecasts; quantile forecasts; expert combination; linear opinion pooling},
  comment   = {Given a bunch of expert probabilistic forecasts, the average of their quantiles usually has a higher score than the average of their probabilities. Also, the mean of the average forecast is equal to the average of each expert's forecasts, a property that weighted probability averaging or logarithmic pooling.

Averaging probability distributions is also not bad, but not generally better.

The whole paper suggests that averaging or weighted averaging quantiles might work as a form of forecast aggregation.

Might be useful for
* upscaling, nodal , or combined PV/Wind forecasts (would be adding instead of averaging)
* ensembles of point forecasts using boosting, as I've speculated about in Lillywhite13featCnstrct
* computing ensemble quantiles from quantiles of individual members

* Three factors affect success of Q averaging
 1. num. forecasts
 2. expert disagreement
 3. degree of overconfidence
(but how to use this knowledge is not clear from this paper -- they just average)

* Analytical proofs of quantile average properties
 -- but they assume a narrow set of distributions
 -- all experts must have the same dist. family
 -- different between experts can be expressed by mean and variance parameters

* But, empirical results on arbitrary forecast distributions support claim
 -- expert forecasts are given in piecwise linear cdfs
 -- this means they won't be in that small list of distributions, or have the same family

* Four scoring rules used in comparision
 1. Linear (not strictly proper)
 2. logarithmic (local, meaning that it doesn't depend upon whole distribution)
 3. quadratic (he only strictly proper score that is local)
 4. crps
 -- actually, I didn't understand what 1. and 2. were)
 -- it wasn't clear to me which score was bettter


Related to quantile mixtures, as in Karvanen06quanMixLmomTrim and Jose14TrimOpinPoolCal

Superquantiles, which are "some kind of additive" might be a better way to express this problem: Rockafellar14superQuantRgrssn},
  file      = {Lichtendahl13probOrQuantAvg.pdf:Lichtendahl13probOrQuantAvg.pdf:PDF},
  groups    = {Read, Ensemble, PointDerived, CitaviImport1, doReadNonWPV_1},
  ncite     = {1},
  owner     = {sotterson},
  publisher = {INFORMS},
  timestamp = {2013.09.27},
  url       = {http://mansci.journal.informs.org/content/59/7/1594.abstract},
}

@Article{Kapl09tensSplnWvlt,
  author    = {Kapl, Mario and J\"{u}ttler, Bert},
  title     = {A multiresolution analysis for tensor-product splines using weightedspline wavelets},
  journal   = {Journal of Computational and Applied Mathematics},
  year      = {2009},
  volume    = {231},
  number    = {2},
  pages     = {828--839},
  issn      = {0377-0427},
  abstract  = {We construct biorthogonal spline wavelets for periodic splines which extend the notion of ``lazy'' wavelets for linear functions (where the wavelets are simply a subset of the scaling functions) to splines of higher degree. We then use the lifting scheme in order to improve the approximation properties with respect to a norm induced by a weighted inner product with a piecewise constant weight function. Using the lifted wavelets we define a multiresolution analysis of tensor-product spline functions and apply it to image compression of black-and-white images. By performing as a model problem image compression with black-and-white images, we demonstrate that the use of a weight function allows to adapt the norm to the specific problem.},
  comment   = {For speed vs. dir dependence model? Biorthogonal so good regrsn input?

* describes (I think) how to construct a 1-D periodic b-spline},
  doi       = {10.1016/j.cam.2009.05.006},
  file      = {Kapl09tensSplnWvlt.pdf:Kapl09tensSplnWvlt.pdf:PDF},
  location  = {Amsterdam, The Netherlands, The Netherlands},
  owner     = {scotto},
  publisher = {Elsevier Science Publishers B. V.},
  timestamp = {2010.08.12},
}

@InCollection{Deepa07reviewDiffuseNewProd,
  author    = {Deepa, Chandrasekaran and Tellis Gerard, J.},
  title     = {A Critical Review of Marketing Research on Diffusion of New Products},
  booktitle = {Review of Marketing Research},
  publisher = {Emerald Group Publishing Limited},
  year      = {2007},
  editor    = {Malhotra, Naresh K.},
  volume    = {3},
  series    = {Review of Marketing Research},
  pages     = {39--80},
  month     = aug,
  abstract  = {We critically examine alternate models of the diffusion of new products and the turning points of 
the diffusion curve. On each of these topics, we focus on the drivers, specifications, and estima-
tion methods researched in the literature. We discover important generalizations about the shape, 
parameters, and turning points of the diffusion curve and the characteristics of diffusion across 
early stages of the product life cycle. We point out directions for future research.},
  comment   = {Examines Bass diffusion, it's extensions and its weaknesses.},
  file      = {:Deepa07reviewDiffuseNewProd.pdf:PDF},
  issn      = {978-0-7656-1306-6, 978-0-85724-725-4/1548-6435},
  journal   = {Review of Marketing Research},
  url       = {https://doi.org/10.1108/S1548-6435(2007)0000003006},
}

@InCollection{Wilson10copulaProc,
  author    = {Andrew Wilson and Zoubin Ghahramani},
  title     = {Copula Processes},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2010},
  editor    = {J. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R.S. Zemel and A. Culotta},
  number    = {23},
  pages     = {2460--2468},
  abstract  = {We define a copula process which describes the dependencies between arbitrarily many random variables independently of their marginal distributions. As an example, we develop a stochastic volatility model, Gaussian Copula Process Volatility (GCPV), to predict the latent standard deviations of a sequence of random variables. To make predictions we use Bayesian inference, with the Laplace approximation, and with Markov chain Monte Carlo as an alternative. We find our model can outperform GARCH on simulated and financial data. And unlike GARCH, GCPV can easily handle missing data, incorporate covariates other than time, and model a rich class of covariance structures.},
  comment   = {volatility prediction related to spinning reserves maybe Erik Berge's NORSEWInD variance estimate

Can handle a huge # of variables: Elidan12copulaMachLrnTut
Can do conditional copulas: HernandezLobato13gaussProceCondTS
Is this used in Dubois15deepMineCopulaHyper, which has Python?},
  file      = {Wilson10copulaProc.pdf:Wilson10copulaProc.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.12.09},
  url       = {http://books.nips.cc/nips23.html},
}

@Article{Jewson03likSkillFrcst,
  author    = {Jewson, Stephen},
  title     = {Use of the likelihood for measuring the skill of probabilistic forecasts},
  journal   = {arXiv preprint physics/0308046},
  year      = {2003},
  abstract  = {We define the likelihood and give a number of justifications for its use as a skill measure for
probabilistic forecasts. We describe a number of different scores based on the likelihood, and briefly
investigate the relationships between the likelihood, the mean square error and the ignorance.},
  booktitle = {arXiv preprint physics/0308046},
  comment   = {Wants to evaluate continuous forecasts with a likelihood score, apparently, instead of CRPS.  Not highly cited, even though it's old. Possibly, this was an idea that didn't take off.

Jewson04problemBrierScore wants to replace Brier Score with a likelihood score too.  That paper did get cited at least once by an influential paper (Benedetti10scoreFrcstVerifRare)},
  file      = {Jewson03likSkillFrcst.pdf:Jewson03likSkillFrcst.pdf:PDF},
  groups    = {Test, CitaviImport1, doReadNonWPV_2},
  keywords  = {Likelihood},
  ncite     = {7},
  owner     = {sotterson},
  timestamp = {2013.09.26},
  url       = {http://arxiv.org/abs/physics/0308046v2},
}

@InProceedings{Gao15mutInfoStrongDep,
  author    = {Gao, Shuyang and Ver Steeg, Greg and Galstyan, Aram},
  title     = {Efficient Estimation of Mutual Information for Strongly Dependent Variables.},
  booktitle = {AISTATS},
  year      = {2015},
  abstract  = {We demonstrate that a popular class of non-
parametric mutual information (MI) estima-
tors based on k-nearest-neighbor graphs re-
quires number of samples that scales expo-
nentially with the true MI. Consequently, ac-
curate estimation of MI between two strongly
dependent variables is possible only for pro-
hibitively large sample size. This important
yet overlooked shortcoming of the existing es-
timators is due to their implicit reliance on
local uniformity of the underlying joint dis-
tribution. We introduce a new estimator that
is robust to local non-uniformity, works well
with limited data, and is able to capture rela-
tionship strengths over many orders of mag-
nitude. We demonstrate the superior perfor-
mance of the proposed estimator on both syn-
thetic and real-world data.},
  comment   = {Says Kraskov MI estimator (Kraskov04EstMutInfKNN) needs exponential num. points to estimate MI of strongly dependent variables.  Has a python implmentation of an MI estimatore built on Kraskov that is efficient for small sample sizes.  Also has a section covering which MI estimation technique is best in what situation.

See: Gao15mutInfoStrongDep

Python implementation
https://github.com/BiuBiuBiLL/NPEET_LNC},
  file      = {Gao15mutInfoStrongDep.pdf:Gao15mutInfoStrongDep.pdf:PDF},
  url       = {http://www.jmlr.org/proceedings/papers/v38/gao15.pdf},
}

@Article{Samworth12optWgtKNNclass,
  author    = {Samworth, Richard J and others},
  title     = {Optimal weighted nearest neighbour classifiers},
  journal   = {The Annals of Statistics},
  year      = {2012},
  volume    = {40},
  number    = {5},
  pages     = {2733--2763},
  abstract  = {We derive an asymptotic expansion for the excess risk (regret) of a
weighted nearest-neighbour classifier. This allows us to find the asymptotically
optimal vector of nonnegative weights, which has a rather simple form.
We show that the ratio of the regret of this classifier to that of an unweighted
k-nearest neighbour classifier depends asymptotically only on the dimension
d of the feature vectors, and not on the underlying populations. The
improvement is greatest when d = 4, but thereafter decreases as d ??.
The popular bagged nearest neighbour classifier can also be regarded as a
weighted nearest neighbour classifier, and we show that its corresponding
weights are somewhat suboptimal when d is small (in particular, worse than
those of the unweighted k-nearest neighbour classifier when d = 1), but are
close to optimal when d is large. Finally, we argue that improvements in the
rate of convergence are possible under stronger smoothness assumptions, provided
we allow negative weights. Our findings are supported by an empirical
performance comparison on both simulated and real data sets.},
  comment   = {Analytical k and neighbor distance weight picker improves performance up to 4 dims and not better than bagged KNN, for higher dim, as it bagged KNN approachs an optimal something. The KNN classifier algorithm implemented in the R kknn package.

Could this be a distance function for locLin QR? Would classify X after merging with QR relationship.},
  file      = {Samworth12optWgtKNNclass.pdf:Samworth12optWgtKNNclass.pdf:PDF},
  owner     = {sotterson},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2014.04.23},
  url       = {http://projecteuclid.org/euclid.aos/1359987536},
}

@Article{Alkhouli05gustFrntDetTmplt,
  author    = {Alkhouli, O. and DeBrunner, V.},
  title     = {Gust front detection in weather radar images by entropy matched functional template},
  journal   = {Image Processing (ICIP)},
  year      = {2005},
  volume    = {1},
  month     = sep,
  abstract  = {We describe a new gust front detection method using functional template correction (FTC) and entropy. The new method provides better boundaries than does a previously developed method by our group (V. DeBrunner and E. Matusiak, 2003). Our proposed method described in this paper requires only one template for detection, thereby reducing the computational complexity and so yielding an algorithm that is more suitable for real time detection than that previously developed one.},
  comment   = {Related to Mark's delta pressure front tracking idea but uses radar. Sort of rigid tracking of deformable body. Maybe good for ramps.},
  doi       = {10.1109/ICIP.2005.1529833},
  file      = {Alkhouli05gustFrntDetTmplt.pdf:Alkhouli05gustFrntDetTmplt.pdf:PDF;Alkhouli05gustFrntDetTmplt.pdf:Alkhouli05gustFrntDetTmplt.pdf:PDF},
  keywords  = {computational complexity, entropy, radar imaging, remote sensing by radar, weather forecasting computational complexity reduction, entropy matched functional template, functional template correction, gust front detection, weather radar images},
  owner     = {sotterson},
  timestamp = {2009.02.20},
}

@Article{Stowell09multiDentropyKDpart,
  author    = {Stowell, D. and Plumbley, M.D.},
  title     = {Fast Multidimensional Entropy Estimation by k-d Partitioning},
  journal   = {Signal Processing Letters, IEEE},
  year      = {2009},
  volume    = {16},
  number    = {6},
  pages     = {537--540},
  month     = jun,
  issn      = {1070-9908},
  abstract  = {We describe a nonparametric estimator for the differential entropy of a multidimensional distribution, given a limited set of data points, by a recursive rectilinear partitioning. The estimator uses an adaptive partitioning method and runs in Theta(N log N) time, with low memory requirements. In experiments using known distributions, the estimator is several orders of magnitude faster than other estimators, with only modest increase in bias and variance.},
  comment   = {Another, very fast way to calculate entropy. Has software. Maybe usable for fast image registration?

Software: http://www.elec.qmul.ac.uk/digitalmusic/downloads/\#kdpee (includes matlab wrapper) But will this fail for high dimensions? Kraskov08MIChierClustMutInf says any method based on partitioning fails for high dims. But Vejmelka07mutInfoKNNspeedup is a partitioning method that speeds up Kraskov MI est, esp. in high dims!},
  doi       = {10.1109/LSP.2009.2017346},
  file      = {Stowell09multiDentropyKDpart.pdf:Stowell09multiDentropyKDpart.pdf:PDF},
  keywords  = {adaptive partitioning method;fast multidimensional differential entropy estimation;k-d partitioning;multidimensional signal processing;nonparametric estimator;recursive rectilinear partitioning;adaptive estimation;entropy;multidimensional signal processing;nonparametric statistics;recursive estimation;},
  owner     = {scotto},
  timestamp = {2011.05.14},
}

@Article{BenTaieb14gradBoostLdFrcst,
  author    = {Ben Taieb, Souhaib and Hyndman, Rob J},
  title     = {A gradient boosting approach to the Kaggle load forecasting competition},
  journal   = {International Journal of Forecasting},
  year      = {2014},
  volume    = {30},
  number    = {2},
  pages     = {382--394},
  abstract  = {We describe and analyse the approach used by Team TinTin (Souhaib Ben Taieb and Rob J Hyndman) in the Load Forecasting track of the Kaggle Global Energy Forecasting Competition 2012. The competition involved a hierarchical load forecasting problem for a US utility with 20 geographical zones. The data available consisted of the hourly loads for the 20 zones and hourly temperatures from 11 weather stations, for four and a half years. For each zone, the hourly electricity loads for nine different weeks needed to be predicted without having the locations of either the zones or stations. We used separate models for each hourly period, with component-wise gradient boosting for estimating each model using univariate penalised regression splines as base learners. The models allow for the electricity demand changing with the time-of-year, day-of-week, time-of-day, and on public holidays, with the main predictors being current and past temperatures, and past demand. Team TinTin ranked fifth out of 105 participating teams.
Keywords

 Short-term load forecasting;
 Multi-step forecasting;
 Additive models;
 Gradient boosting;
 Machine learning;
 Kaggle competition},
  comment   = {Gradient boosting for load forecast: was 5\textsuperscript{th} place out of 105 in contest. The pdf is for the tech note. Get the journal paper.},
  file      = {BenTaieb14gradBoostLdFrcst.pdf:BenTaieb14gradBoostLdFrcst.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.07.23},
  url       = {http://www.sciencedirect.com/science/article/pii/S0169207013000812},
}

@InProceedings{Natarajan02segRegressMassive,
  author    = {Ramesh Natarajan and Edwin P. D. Pednault},
  title     = {Segmented Regression Estimators for Massive Data Sets},
  booktitle = {SIAM International Conference on Data Mining (SDM)},
  year      = {2002},
  abstract  = {We describe two methodologies for obtaining segmented regression estimators from massive training data sets. The first methodology, called Linear Regression Tree (LRT), is used for continuous response variables, and the second and complementary methodology, called Naive Bayes Tree (NBT), is used for categorical response variables. These are implemented in the IBM ProbETM (Probabilistic Estimation) data mining engine, which is an object-oriented framework for building classes of segmented predictive models from massive training data sets. Based on this methodology, an application called ATM-SETM for direct-mail targeted marketing has been developed jointly with Fingerhut Business Intelligence [1]).},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comment   = {Linear /possibly adaptive regression tree, partial correlation feature selection?, related to regime swtching},
  ee        = {http://www.siam.org/meetings/sdm02/proceedings/sdm02-33.pdf},
  file      = {Natarajan02segRegressMassive.pdf:Natarajan02segRegressMassive.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.02.18},
}

@Article{Comin10explrTechDiffus,
  author   = {Comin, Diego and Hobijn, Bart},
  title    = {An Exploration of Technology Diffusion},
  journal  = {American Economic Review},
  year     = {2010},
  volume   = {100},
  number   = {5},
  pages    = {2031-59},
  month    = {December},
  abstract = {We develop a model that, at the aggregate level, is similar to the one-sector 
neoclassical growth model; at the disaggregate level, it has implications for the 
path of observable measures of technology adoption. We estimate it using data 
on the diffusion of 15 technologies in 166 countries over the last two centuries. 
Our results reveal that, on average, countries have adopted technologies 45 
years after their invention. There is substantial variation across technologies 
and countries. Newer technologies have been adopted faster than old ones. The 
cross-country variation in the adoption of technologies accounts for at least 25 
percent of per capita income differences.},
  comment  = {An aggregate and granular tech diffusion model based on time lags, and which is multisector (PV, ecars, batteries,...?)

Has data on this page: https://www.aeaweb.org/articles?id=10.1257/aer.100.5.2031},
  doi      = {10.1257/aer.100.5.2031},
  file     = {:Comin10explrTechDiffus.pdf:PDF},
  url      = {http://www.aeaweb.org/articles?id=10.1257/aer.100.5.2031},
}

@Article{Fuentes07bayesEntSpatSamp,
  author    = {Montserrat Fuentes and Arin Chaudhuri and David M. Holland},
  title     = {{Bayes}ian entropy for spatial sampling design of environmental data},
  journal   = {Environmental and Ecological Statistics},
  year      = {2007},
  volume    = {14},
  number    = {3},
  pages     = {323--340},
  month     = sep,
  abstract  = {We develop a spatial statistical methodology to design national air pollution monitoring networks with good predictive capabilities while minimizing the cost of monitoring. The underlying complexity of atmospheric processes and the urgent need to give credible assessments of environmental risk create problems requiring new statistical methodologies to meet these challenges. In this work, we present a new method of ranking various subnetworks taking both the environmental cost and the statistical information into account. A Bayesian algorithm is introduced to obtain an optimal subnetwork using an entropy framework. The final network and accuracy of the spatial predictions is heavily dependent on the underlying model of spatial correlation. Usually the simplifying assumption of stationarity, in the sense that the spatial dependency structure does not change location, is made for spatial prediction. However, it is not uncommon to find spatial data that show strong signs of nonstationary behavior. We build upon an existing approach that creates a nonstationary covariance by a mixture of a family of stationary processes, and we propose a Bayesian method of estimating the associated parameters using the technique of Reversible Jump Markov Chain Monte Carlo. We apply these methods for spatial prediction and network design to ambient ozone data from a monitoring network in the eastern US.},
  comment   = {How to select pollution monitoring sensor locations; use for targeted observations proposal?},
  file      = {Fuentes07bayesEntSpatSamp.pdf:Fuentes07bayesEntSpatSamp.pdf:PDF;Fuentes07bayesEntSpatSamp.pdf:Fuentes07bayesEntSpatSamp.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.03.02},
}

@InProceedings{Liu17multDatFusCplaProc,
  author    = {J. Liu and I. Nevat and P. Zhang and G. W. Peters},
  title     = {Multimodal Data Fusion in Sensor Networks via Copula Processes},
  booktitle = {2017 IEEE Wireless Communications and Networking Conference (WCNC)},
  year      = {2017},
  pages     = {1-6},
  month     = {March},
  abstract  = {We develop an efficient data fusion algorithm for field reconstruction of multiple physical phenomena, which exhibit multiple modalities each with complex dependence behavior. In particular, we design a novel spatial model where multiple latent processes are modelled as Multi-Output Gaussian Process. We encode a linear dependency structure through a specified covariance function in both space and between different modalities of the spatial processes monitored. To account for different data modalities, we model the spatial dependence between each process via Copula dependence structures [1], thus allowing to choose any marginal distribution or process (possibly different) for each of the physical phenomena. We formulate the field reconstruction problem and develop a low complexity algorithm to approximate the intractable predictive posterior distribution. We show that our model significantly outperforms the model which treats the different physical phenomena independently in terms of prediction meansquared-errors (MSE). This provides the motivation to use our model for multimodal data fusion.},
  comment   = {Like correlation between pressure, wind speed, etc. in NWP grid cells across space.  Is also a copula process, which is flagged in Elidan12copulaMachLrnTut as being able to handle extremely high dimensions.},
  doi       = {10.1109/WCNC.2017.7925557},
  file      = {Liu17multDatFusCplaProc.pdf:Liu17multDatFusCplaProc.pdf:PDF},
  keywords  = {Correlation;Covariance matrices;Data integration;Estimation;Gaussian processes;Monitoring;Temperature measurement},
}

@Article{Liddell78circLinCorrCf,
  author              = {Liddell, I. G. and Ord, J. K.},
  title               = {Linear-Circular Correlation Coefficients: Some Further Results},
  journal             = {Biometrika},
  year                = {1978},
  volume              = {65},
  number              = {2},
  pages               = {448--450},
  issn                = {0006-3444},
  abstract            = {We develop Mardia's concept of a linear-circular correlation coefficient, obtaining distributions of statistics under various hypotheses. Mardia's data on simian natality are discussed in the light of our findings.},
  comment             = {correlation coeff between linear variable and a circular one something clever for a regression input? besides two inputs: [cos(theta), sin(theta)] ?},
  copyright           = {Copyright ? 1978 Biometrika Trust},
  file                = {Liddell78circLinCorrCf.pdf:Liddell78circLinCorrCf.pdf:PDF},
  jstor_articletype   = {primary_article},
  jstor_formatteddate = {Aug., 1978},
  owner               = {scot},
  publisher           = {Biometrika Trust},
  timestamp           = {2010.08.05},
  url                 = {http://www.jstor.org/stable/2335230},
}

@InProceedings{Veeramachaneni15cplaGrphMdlsWindRsrcEst,
  author    = {Veeramachaneni, Kalyan and Cuesta-Infante, Alfredo and O'Reilly, Una-May},
  title     = {Copula graphical models for wind resource estimation},
  booktitle = {Twenty-Fourth International Joint Conference on Artificial Intelligence},
  year      = {2015},
  abstract  = {We develop multivariate copulas for modeling
multiple joint distributions of wind speeds at a
wind farm site and neighboring wind source. A n-
dimensional Gaussian copula and multiple copula
graphical models enhance the quality of the predic-
tion site distribution. The models, in comparison
to multiple regression, achieve higher accuracy and
lower cost because they require less sensing data.},
  comment   = {Graphical copula model (C-Vine, I think) reduces the amount of data needed to predict wind speed at a farm, using airport weather stations.

Popular Press: Hardesty15windFarmSiteCopula

He did this with a Gaussian Copula in 2013: Veeramachaneni13CopulaBasedWind

May have python code: https://github.com/samandarr/DeepMining},
  file      = {Veeramachaneni15cplaGrphMdlsWindRsrcEst.pdf:Veeramachaneni15cplaGrphMdlsWindRsrcEst.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.27},
  url       = {https://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/viewPaper/11450},
}

@Article{Gneiting08multiVarEnsAssess,
  author    = {Gneiting, Tilmann and Stanberry, Larissa I and Grimit, Eric P and Held, Leonhard and Johnson, Nicholas A},
  title     = {Assessing probabilistic forecasts of multivariate quantities, with an application to ensemble predictions of surface winds},
  journal   = {Test},
  year      = {2008},
  volume    = {17},
  number    = {2},
  pages     = {211--235},
  abstract  = {We discuss methods for the evaluation of probabilistic predictions of vector-valued
quantities, that can take the form of a discrete forecast ensemble or a density forecast. In particular,
we propose a multivariate version of the univariate verification rank histogram or Talagrand diagram
that can be used to check the calibration of ensemble forecasts. In the case of density forecasts,
Box??? density ordinate transform provides an attractive alternative. The multivariate energy score
generalizes the continuous ranked probability score. It addresses both calibration and sharpness,
and can be used to compare deterministic forecasts, ensemble forecasts and density forecasts, using
a single loss function that is proper. An application to the University of Washington mesoscale
ensemble points at strengths and deficiencies of probabilistic short-range forecasts of surface wind
vectors over the North American Pacific Northwest.
Keywords Calibration, Density forecast, Ensemble postprocessing, Exchangeability, Forecast
verification, Probability integral transform, Proper scoring rule, Sharpness, Rank histogram},
  comment   = {A toolbox of diagnostics and scoring rules for multivariate density forecasts, illustrated on 2D wind speed forecasts (u,v). Scoring rules could be used for cost functions during training. Some of these techniques assume constant ensemble ordering (not available in DWD's LETKF ensembles). Also finds that climatology is actually the best predictive density for very complex terrain!

Rank histograms for visualizing over/under dispersion
* multivariate rank histogram: "less" means all values in vector are less.
 -- Are there cases where no vector is "less" than any other vector?
 -- ties are resolved randomly, but 2007 reference does it w/o randomness
 -- may operate on standardized (PCA) vectors
 -- if uniform rank (flat rank histogram) then ensemble members and observation are "exchangable" (?)
* Min. Spanning Tree (MST) rank histo
 -- calc a min euclidean distance path between all members
 -- somehow use that to rank the ensembles
 -- outlying observations lead to low values
 -- histograms are "center-outward ordered"
 ---- outlying observations have small values
* Box density ordinate transform (BOT) rank histo
 -- rank members based on their position on an estimated df
 -- histos look like MST histos
* Only BOT is illustrated on real data. Not clear which is better

Multivariate Sharpness
* they prefer |determinant sharpness" root of the ensemble covariance matrix

Scoring rules
* Proper scoring rule: the score is lowest for the true distribution
* CRPS: the usual 1D meausre
* Energy Score: a kind of multivariate CRPS
* Logarithmic Score:
 -- I think this is just -log of the observation probability data, given a modeled density
 -- the only 'local' proper scoriing rule
* Quadratic and Sperical Scores: similar to logarithmic score, but not logged

Calibration methods
* Error dressing (ED)
 -- mean ensemble: site-specific, daily bias correction (they not this isn't suitable for real forecasting)
 -- ensemble density: fit a distribution to a 2D (in this case) normal dist w/ station-dependent mean
* Climatological (CLI)
 -- mean ensemble: just the mean value at a station (??)
 ---- Climatology better than ensemble forecast when complex terrain not modeled by NWP because grid is too coarse. This is site specific so, when doing this on a grid, probably want a selectable climatology feature.
 -- ensemble density: fit a station-dependent normal},
  file      = {Tech Report:Gneiting08multiVarEnsAssess_TR.pdf:PDF},
  groups    = {Read, Test, doReadNonWPV_1},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2013.10.10},
}

@Article{Pritchard10varQuantProbFrcst,
  author    = {Pritchard, G.},
  title     = {Short-term variations in wind power: Some quantile-type models for probabilistic forecasting},
  journal   = {Wind Energy},
  year      = {2010},
  abstract  = {We discuss some ways of formulating quantile-type models for forecasting variations in wind power in the short term (within a few hours). Such models predict quantiles of the conditional distribution of the wind power available at some future time using information presently available. A natural reference for models of this kind is a 'probabilistic-persistence' quantile forecast whose only input is the present wind power. Using data from some New Zealand wind farms, we find that more complex quantile models can readily improve on probabilistic persistence in resolution but not in sharpness. The most valuable model inputs, apart from the present power, are found to be real-time air pressure measurements and a power total-variation indicator.},
  comment   = {Fancier "probabilistic persistence" quantile forecast using realtime measured power, pressure, and power variance is better than simple model. Fancy model has 2\textsuperscript{nd} order circular harmonic OK for spd/dir interaction, no comp. w/ spline, etc. Good for up to a few hours ahead. A very nice paper.

* interesting persistence plot: change in power v.s power now
* interesting wind direction vs. power now vs. power change plot
* cheated a bit by excluding zero power, thus excluding maintenance
* sigmoidal "change in pressure" feature (avoided spline b/c had too few low pressure measurements)
* total power variation, like Koenker's QR metric, but heuristically, nonlinearly scaled.

Gets non-linear and multiplicative efects by combinatorial interaction terms (but can still do linear QR)

* got a multiplicative wind direction and power persistence relationship
 -- instead of current power, could have used current (or forecasted) wind speed if available
 -- results in 30 terms (or 24?)
 -- this is a kind of tensor product, I guess
 -- inputs are current power projected onto spline basis times current wind direction Fourier terms (2\textsuperscript{nd} order)
 -- Kaufman05perSplnRgrsn angular kernel simplifies to cos(terms) only. Better?
 -- R create.baspline.basis {fda} says harmonics like used here are preferred over splines
 http://ugrad.stat.ubc.ca/R/library/fda/html/create.bspline.basis.html
 but how smooth/penalize?
 -- would wavelet circular be better (since could be local, like spline, and not global like Fourier Series)?

* also got third order multiplicative effect from interaction terms w/ total power variation
* interactions could be a huge dimensional input to linQR or PCA 1\textsuperscript{st}
* lots of interaction terms are found to be useful
 -- note: Bremnes04windLocQR tried interactions and didn't use them
* could use varying orders of interaction products and let boosting select the best

See also: Dokic15systIntegLrgData
Note: Persistence forecasts could also be a kind of dist-to-dist forecast: Oliva13dist2distRgrssn

One question about interactions is how to penalized
* typically, penalties are 1D (total variation or squared 1\textsuperscript{st} diff)
* these require that coeffs are in "order", so things "next to each other" don't change much
* in 2D or 3D need something else
* PCA or PLS is one option
* Multilinear uncorrelated PCA (Lu09UncorrMultiLinPCA) might be especially good, since it's "adjacency aware"},
  doi       = {10.1002/we.416},
  file      = {Pritchard10varQuantProbFrcst.pdf:Pritchard10varQuantProbFrcst.pdf:PDF},
  groups    = {Read, PointDerived, doReadWPV_2},
  owner     = {scot},
  timestamp = {2010.08.18},
}

@Article{Calsaverini09copulaInfo,
  author    = {Calsaverini, R.S. and Vicente, R.},
  title     = {An information-theoretic approach to statistical dependence: Copula information},
  journal   = {Europhysics Letters (EPL)},
  year      = {2009},
  volume    = {88},
  pages     = {68003},
  abstract  = {We discuss the connection between information and copula theories by showing that a copula can be employed to decompose the information content of a multivariate distribution into marginal and dependence components, with the latter quantified by the mutual information. We define the information excess as a measure of deviation from a maximum-entropy distribution. The idea of marginal invariant dependence measures is also discussed and used to show that empirical linear correlation underestimates the amplitude of the actual correlation in the case of non-Gaussian marginals. The mutual information is shown to provide an upper bound for the asymptotic empirical log-likelihood of a copula. An analytical expression for the information excess of T-copulas is provided, allowing for simple model identification within this family. We illustrate the framework in a financial data set.},
  comment   = {Copula decomposes info into info from each variable and dependence between variables; has a test for Normality
* Linear correlation always underestimates dependence if marginals aren't Gaussian
* "Information Excess" measure is another way to estimate Gaussianinity:
  - if there's excess, Gaussian is poor fit.
  - is also calculated for T-copula, so can select between them
* equations for linear and rank correlation correspondence (Gaussian, bivariate)},
  file      = {Calsaverini09copulaInfo.pdf:Calsaverini09copulaInfo.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  publisher = {IOP Publishing},
  timestamp = {2010.12.20},
}

@Article{Kitigawa09BiVarRedBtstrpIC,
  author      = {Kitagawa, Genshiro and Konishi, Sadanori},
  title       = {Bias and variance reduction techniques for bootstrap information criteria},
  journal     = {Annals of the Institute of Statistical Mathematics},
  year        = {2010},
  volume      = {62},
  pages       = {209--234},
  issn        = {0020-3157},
  abstract    = {We discuss the problem of constructing information criteria by applying the bootstrap methods. Various bias and variance reduction methods are presented for improving the bootstrap bias correction term in computing the bootstrap information criterion. The properties of these methods are investigated both in theoretical and numerical aspects, for which we use a statistical functional approach. It is shown that the bootstrap method automatically achieves the second-order bias correction if the bias of the first-order bias correction term is properly removed. We also show that the variance associated with bootstrapping can be considerably reduced for various model estimation procedures without any analytical argument. Monte Carlo experiments are conducted to investigate the performance of the bootstrap bias and variance reduction techniques.},
  affiliation = {The Institute of Statistical Mathematics 4-6-7 Minami-Azabu, Minato-ku Tokyo 106-8569 Japan},
  comment     = {intro paper: Kitigawa08btstrpInfoCrit},
  doi         = {10.1007/s10463-009-0237-1},
  file        = {Kitigawa09BiVarRedBtstrpIC.pdf:Kitigawa09BiVarRedBtstrpIC.pdf:PDF},
  issue       = {1},
  keyword     = {Mathematics and Statistics},
  owner       = {sotterson},
  publisher   = {Springer Netherlands},
  timestamp   = {2011.11.30},
}

@Article{Delucchi11allRenewWorld,
  author    = {Mark Delucchi},
  title     = {Wind, Water, and Solar Power for the World},
  journal   = {IEEE Spectrum},
  year      = {2011},
  month     = sep,
  abstract  = {We don?t need nuclear power, coal, or biofuels. We can get 100 percent of our energy from wind, water, and solar (WWS) power. And we can do it today?efficiently, reliably, safely, sustainably, and economically.},
  comment   = {Can power whole world with wind water and solar by 2030. Related to Mark Jacobson work. Related article in CNET (has links to pdf's of two long studies, with the details) https://www.evernote.com/view/f3892f09-b093-4747-a85c-2c90c2b29bb6?locale=en\#x=delucchi%2520&n=f3892f09-b093-4747-a85c-2c90c2b29bb6},
  file      = {Delucchi11allRenewWorld.pdf:Delucchi11allRenewWorld.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2012.02.18},
  url       = {http://spectrum.ieee.org/energy/renewables/wind-water-and-solar-power-for-the-world/0},
}

@Article{Lu07varyCoeffAdapt,
  author    = {Lu, Zudi and Tj{\o}stheim, Dag and Yao, Qiwei},
  title     = {Adaptive varying-coefficient linear models for stochastic processes: asymptotic theory},
  journal   = {Statistica Sinica},
  year      = {2007},
  volume    = {17},
  number    = {1},
  pages     = {177--198},
  abstract  = {We establish the asymptotic theory for the estimation of adaptive varying-
coe?cient linear models. More speci?cally, we show that the estimator of the
index parameter is root-n-consistent. It di?ers from the locally optimal estimator
that has been proposed in the literature with a prerequisite that the estimator is
within a n
?? -distance of the true value. To this end, we establish two fundamental
lemmas for the asymptotic properties of the estimators of parametric components
in a general semiparametric setting. Furthermore, the estimation for the coe?cient
functions is asymptotically adaptive to the unknown index parameter. Asymptotic
properties are derived using the empirical process theory for strictly stationary
?-mixing processes.
Key words and phrases: Adaptive varying-coe?cient model, asymptotic normality,
?-mixing, empirical process, index parameter, root-n consistency, uniform conver-
gence},
  comment   = {Time adaptive (possibly autoregressive) varing coeff model. Can be multidimensional.},
  file      = {Lu07varyCoeffAdapt.pdf:Lu07varyCoeffAdapt.pdf:PDF},
  owner     = {sotterson},
  publisher = {Institute of Statistical Science, Academia Sinica},
  timestamp = {2014.11.09},
}

@Article{Pedro12solarFrcstAlgsNoExog,
  author    = {Pedro, Hugo TC and Coimbra, Carlos FM},
  title     = {Assessment of forecasting techniques for solar power production with no exogenous inputs},
  journal   = {Solar Energy},
  year      = {2012},
  volume    = {86},
  number    = {7},
  pages     = {2017--2028},
  abstract  = {We evaluate and compare several forecasting techniques using no exogenous inputs for predicting the solar power output of a
1 MWp, single-axis tracking, photovoltaic power plant operating in Merced, California. The production data used in this work
corresponds to hourly averaged power collected from November 2009 to August 2011. Data prior to January 2011 is used to train
the several forecasting models for the 1 and 2 h-ahead hourly averaged power output. The methods studied in this work are: Persistent
model, Auto-Regressive Integrated Moving Average (ARIMA), k-Nearest-Neighbors (kNNs), Artificial Neural Networks (ANNs), and
ANNs optimized by Genetic Algorithms (GAs/ANN). The accuracy of the models is determined by computing error statistics such as
mean absolute error (MAE), mean bias error (MBE), and the coefficient of correlation (R2) for the differences between the forecasted
values and the measured values for the period from January to August of 2011. This work also addresses the accuracy of the different
methods as a function of the variability of the power output, which depends strongly on seasonal conditions. The findings show that the
ANN-based forecasting models perform better than the other forecasting techniques, that substantial improvements can be achieved with
a GA optimization of the ANN parameters, and that the accuracy of all models depends strongly on seasonal characteristics of solar
variability.

Keywords: Solar forecasting; Solar energy; Regression analysis; Stochastic learning},
  comment   = {Comparison of purely autoregressive (past power is only input) PV forecast models for 1,2 hrs ahead. NN is best.},
  file      = {Pedro12solarFrcstAlgsNoExog.pdf:Pedro12solarFrcstAlgsNoExog.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2015.02.19},
  url       = {http://www.sciencedirect.com/science/article/pii/S0038092X12001429},
}

@Article{Comin04crossCntryTheoryFact,
  author   = {D. Comin and B. Hobijn},
  title    = {Cross-country technology adoption: making the theories face the facts},
  journal  = {Journal of Monetary Economics},
  year     = {2004},
  volume   = {51},
  number   = {1},
  pages    = {39 - 83},
  issn     = {0304-3932},
  abstract = {We examine the diffusion of more than 20 technologies across 23 of the world's leading industrial economies. Our evidence covers major technology classes such as textile production, steel manufacture, communications, information technology, transportation, and electricity for the period 1788–2001. We document the common patterns observed in the diffusion of this broad range of technologies. Our results suggest a pattern of trickle-down diffusion that is remarkably robust across technologies. Most of the technologies that we consider originate in advanced economies and are adopted there first. Subsequently, they trickle down to countries that lag economically. Our panel data analysis indicates that the most important determinants of the speed at which a country adopts technologies are the country's human capital endowment, type of government, degree of openness to trade, and adoption of predecessor technologies. We also find that the overall rate of diffusion has increased markedly since World War II because of the convergence in these variables across countries.},
  comment  = {Imitator/Innovator model is explicit across time w/ lags and space (contries) and conditional (has other factors).  Interesting b/c doesn't assume S-curve (it leasts an a time curve) and solves the joint problem of cross-space and cross-time.},
  doi      = {https://doi.org/10.1016/j.jmoneco.2003.07.003},
  file     = {:Comin04crossCntryTheoryFact.pdf:PDF},
  keywords = {Economic growth, Historical data, Technology adoption},
  url      = {http://www.sciencedirect.com/science/article/pii/S0304393203001247},
}

@Article{He99multiVarQRsplines,
  author    = {He, Xuming and Ng, Pin},
  title     = {Quantile splines with several covariates},
  journal   = {Journal of Statistical Planning and Inference},
  year      = {1999},
  volume    = {75},
  number    = {2},
  pages     = {343--352},
  abstract  = {We extend univariate regression quantile splines to problems with several covariates. We
adopt an ANOVA-type decomposition approach with main effects captured by linear splines and
second-order `interactions' modeled by bi-linear tensor-product splines. Both univariate linear
splines and bi-linear tensor-product splines are optimal when fidelity to data are balanced by
a roughness penalty on the fitted function. The problem of sub-model selection and asymptotic
justification for using a smaller sub-space of the spline functions in the approximation are discussed.
Two examples are considered to illustrate the empirical performance of the proposed
methods. ? 1999 Elsevier Science B.V. All rights reserved.
AMS classiffication: 62G07
Keywords: Information criterion; Linear program; Model selection; Nonparametric regression;
Regression quantiles; Smoothing; Tensor-product spline},
  comment   = {Tensor spline approach to multivariate spline QR regression kind of like boosting. Has roughness penalty.

* tensor B-spline are optimal in bounded 2D space (see He98bivarQsmSplines)
* doesn't say how to build them
* roughness penalty chosen to minimize and information penalty, SIC(lambda)
* SIC described in Koenker05QuantRgrssnBook and Koenker94QuantSmthSplns
* only consider interaction terms

Procedure is an approximation to the ideal, kind of like boosting in Fenske11*
1.) individually smooth each interaction model via regression against the target and minimizing SIC(lambda)
2.) greedily add interaction models w/ lowest total SIC. But how are the interaction models weighted? are they prediction the residual of the previous stage models?

RESULTS
Just show what a 3D housing price quantile model looks like: no comparison w/ alternative models.},
  file      = {He99multiVarQRsplines.pdf:He99multiVarQRsplines.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.11.04},
  url       = {http://www.sciencedirect.com/science/article/pii/S0378375898001530},
}

@Article{Szabo16LrnDistRgrssn,
  author   = {Zolt{{\'a}}n Szab{{\'o}} and Bharath K. Sriperumbudur and Barnab{{\'a}}s P{{\'o}}czos and Arthur Gretton},
  title    = {Learning Theory for Distribution Regression},
  journal  = {Journal of Machine Learning Research},
  year     = {2016},
  volume   = {17},
  number   = {152},
  pages    = {1-40},
  abstract = {We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem makes the theoretical analysis quite challenging, since in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression-based alternative to distribution regression, where we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to prove that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels): we present an exact computational-statistical efficiency trade-off analysis showing that our estimator is able to match the one-stage sampled minimax optimal rate (Caponnetto and De Vito, 2007; Steinwart et al., 2009). This result answers a 1717-year-old open question, establishing the consistency of the classical set kernel (Haussler, 1999; G?rtner et al., 2002) in regression. We also cover consistency for more recent kernels on distributions, including those due to Christmann and Steinwart (2010).},
  comment  = {Goal is, from samples of distribution x_i , predict a possibly multivariate "label", y_i.  e.g. patient health measure from a set of blood pressure measurements.  Or, I guess, wind turbine health (good for ModernWindABS!).  Proposes to do it with ridge regression, which sounds pretty simple.  May have Matlab.

Distribution theory by author who wrote the Information Theoretical Estimation (ITE) Matlab toolbox: Szabo13infoTheoEstTlbx},
  file     = {Szabo16LrnDistRgrssn.pdf:Szabo16LrnDistRgrssn.pdf:PDF},
  url      = {http://jmlr.org/papers/v17/14-510.html},
}

@InCollection{Vejmelka07mutInfoKNNspeedup,
  author      = {Vejmelka, Martin and Hvavackova-Schindler, Katerina},
  title       = {Mutual Information Estimation in Higher Dimensions: A Speed-Up of a k-Nearest Neighbor Based Estimator},
  booktitle   = {Adaptive and Natural Computing Algorithms},
  publisher   = {Springer Berlin / Heidelberg},
  year        = {2007},
  editor      = {Beliczynski, Bartlomiej and Dzielinski, Andrzej and Iwanowski, Marcin and Ribeiro, Bernardete},
  volume      = {4431},
  series      = {Lecture Notes in Computer Science},
  pages       = {790--797},
  abstract    = {We focus on the recently introduced nearest neighbor based entropy estimator from Kraskov, Stogbauer and Grassberger (KSG) [10], the nearest neighbor search of which is performed by the so called box assisted algorithm [7]. We compare the performance of KSG with respect to three spatial indexing methods: box-assisted, k-D trie and projection method, on a problem of mutual information estimation of a variety of pdfs and dimensionalities. We conclude that the k-D trie method is significantly faster then box-assisted search in fixed-mass and fixed-radius neighborhood searches in higher dimensions. The projection method is much slower than both alternatives and not recommended for practical use.},
  affiliation = {Institute of Computer Science, Academy of Sciences of the Czech Republic, Pod Vod??renskou V?????? 2, 18207 Praha 8 Czech Republic},
  comment     = {K-nn mutual information estimator but sped up over Kraskov04EstMutInfKNN, mostly, for higher dims * not always faster but mostly * Kraskov08MIChierClustMutInf says any method based on partitioning will fail for higher dims, but this IS partitioning and works best for higher dims?!},
  doi         = {10.1007/978-3-540-71618-1_88},
  file        = {Vejmelka07mutInfoKNNspeedup.pdf:Vejmelka07mutInfoKNNspeedup.pdf:PDF},
  owner       = {scotto},
  timestamp   = {2011.05.20},
}

@Article{Kelly12threePssRgrssn,
  author    = {Kelly, Bryan and Pruitt, Seth},
  title     = {The three-pass regression filter: A new approach to forecasting using many predictors},
  journal   = {Fama-Miller Working Paper, Chicago Booth Research Paper No. 11-19},
  year      = {2012},
  month     = jun,
  abstract  = {We forecast a single time series using many predictor variables with a new estimator called the three-pass regression filter (3PRF). It is calculated in closed form and conveniently represented as a set of ordinary least squares regressions. 3PRF forecasts converge to the infeasible best forecast when both the time dimension and cross section dimension become large. This requires only specifying the number of relevant factors driving the forecast target, regardless of the total number of common (and potentially irrelevant) factors driving the cross section of predictors. We derive inferential theory in the form of limiting distributions for estimated relevant factors, predictive coefficients and forecasts, and provide consistent standard error estimators. We explore two empirical applications that exemplify the many predictor problem: Forecasting macroeconomic aggregates with a large panel of economic indices, and forecasting stock market aggregates with many individual assets' price-dividend ratios. These, combined with a range of Monte Carlo experiments, demonstrate the 3PRF's forecasting power.

Keywords: forecast, many predictors, factor model, Kalman filter, constrained least squares, principal components, partial least squares},
  comment   = {Some kinda high dimension reduction that's supposed to work at least as well as principal components (PCA), and better in cases where PCA doesn't work. It's also the inspiration for the partial quantile regression algorithm in Giglio13riskPartialQR

I am not sure I understand this paper...

Tu13supervsdFctrFrcst found that this is better than PCA but not as good as their technique on unseen test data. They also say it's a subset of PLS.},
  file      = {Kelly12threePssRgrssn.pdf:Kelly12threePssRgrssn.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.06.17},
  url       = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1868703},
}

@Article{Baldick95genUnitCommit,
  author    = {Baldick, Ross},
  title     = {The generalized unit commitment problem},
  year      = {1995},
  volume    = {10},
  number    = {1},
  month     = feb,
  pages     = {465--475},
  abstract  = {We formulate a generalized version of the unit commitment problem that can treat minimum up- and down-time constraints, power flow constraints, line flow limits, voltage limits, reserve constraints, ramp limits, and total fuel and energy limits on hydro and thermal units. We p r e pose an algorithm for this problem, based on Lagrangian decomposition, and demonstrate the algorithm with reference to a simple model system.},
  file      = {Baldick95genUnitCommit.pdf:Baldick95genUnitCommit.pdf:PDF;Baldick95genUnitCommit.pdf:Baldick95genUnitCommit.pdf:PDF},
  groups    = {DOE-PNL09},
  journal   = {Power Systems, IEEE Transactions on},
  owner     = {sotterson},
  timestamp = {2009.03.03},
}

@InProceedings{Snelson04warpGausProc,
  author    = {Edward Snelson and Carl Edward Rasmussen and Zoubin Ghahramani},
  title     = {Warped {Gauss}ian Processes},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2004},
  volume    = {16},
  pages     = {337--344},
  publisher = {MIT Press},
  abstract  = {We generalise the Gaussian process (GP) framework for regression by learning a nonlinear transformation of the GP outputs. This allows for non-Gaussian processes and non-Gaussian noise. The learning algorithm chooses a nonlinear transformation such that transformed data is well-modelled by a GP. This can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem, rather than as an ad-hoc step. We demonstrate on several real regression problems that learning the transformation can lead to significantly better performance than using a regular GP, or a GP with a fixed transformation.},
  comment   = {How to model non-Gaussian confidence intervals w/ GP's. Cited by several papers. Trick is to learn a transform such that the process can be modeled by a Gaussian Process (I suppose it could then be undone, so this could be like a Gaussian copula)

Could use for individual Ensemble forecast member pdfs in BMA (EM for mixtures of GP's are tractable, I think I remember from somewhere).

Could also use for point forecast error-derived prob forecast quantiles.},
  file      = {Snelson04warpGausProc.pdf:Snelson04warpGausProc.pdf:PDF},
  groups    = {Ensemble, PointDerived, doReadNonWPV_1},
  owner     = {scot},
  timestamp = {2010.06.28},
  url       = {http://www.kyb.mpg.de/publication.html?prj=153},
}

@Article{Abreu13genCmplxStepDiff,
  author    = {Abreu, Rafael and Stich, Daniel and Morales, Jos{\'e}},
  title     = {On the generalization of the complex step method},
  journal   = {Journal of Computational and Applied Mathematics},
  year      = {2013},
  volume    = {241},
  pages     = {84--102},
  abstract  = {We generalize the well known Complex Step Method for computing derivatives by
introducing a complex step in a strict sense. Exploring different combinations of terms, we
derive 52 approximations for computing the first order derivatives and 43 for the second
order derivatives. For an appropriate combination of terms and appropriate choice of the
step size in the real and imaginary directions, fourth order accuracy can be achieved in a
very simple and efficient scheme on a compact stencil. New different ways of computing
second order derivatives in one single step are shown. Many of the first order derivative
approximations avoid the problem of subtractive cancellation inherent to the classic finite
difference approximations for real valued steps, and the superior accuracy and stability
of the generalized complex step approximations are demonstrated for an analytic test
function.},
  comment   = {Summary of the zoo of complex step differentiatation methods for 1\textsuperscript{st} and 2\textsuperscript{nd} order derivatives, FFT's, etc. I think it should be easy to code.

I think it points out problems with Lai05NewCmplxStepDiff (Kalman application) and Lai08Extensionsfirstand (journal paper).},
  file      = {Abreu13genCmplxStepDiff.pdf:Abreu13genCmplxStepDiff.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.07.06},
  url       = {http://www.sciencedirect.com/science/article/pii/S0377042712004207},
}

@InCollection{Chapados08augFrcstGaussProc,
  author    = {Nicolas Chapados and Yoshua Bengio},
  title     = {Augmented Functional Time Series Representation and Forecasting with {Gauss}ian Processes},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  publisher = {MIT Press},
  year      = {2008},
  editor    = {J.C. Platt and D. Koller and Y. Singer and S. Roweis},
  volume    = {20},
  pages     = {265--272},
  abstract  = {We introduce a functional representation of time series which allows forecasts to be performed over an unspecified horizon with progressively-revealed information sets. By virtue of using Gaussian processes, a complete covariance matrix between forecasts at several time-steps is available. This information is put to use in an application to actively trade price spreads between commodity futures contracts. The approach delivers impressive out-of-sample risk-adjusted returns after transaction costs on a portfolio of 30 spreads.},
  comment   = {Gaussian Process forecasting at arbitrary time intervals from nonuniformly sampled data, supposed to be efficient long term, multi-step gauss proc Use for: * scenario generation -- don't need to Gaussianize, I think -- doesn't have to be purely autoregressive: can include exogenous inputs (and learns that relationship) -- has a covariance matrix between forecasts -- can interpolate between forecasts to odd times, since time is an input variable * NWP blending w/ statistical foreacast -- this can take in time chunks of predictions (I think), just like NWP, arriving in chunks -- so could model additional statistical dependence of that trajectory on current measurments not available when the NWP forecast was made -- this is inherently linear -- use to interpolate coarse time interval NWP (e.g. 1 hour) to finer increments (15 mins), using high sample rate exogenous inputs. * Nonlinear forecast error correction of ENFOR forecast time series -- like on Dong energy project -- is nonlinear -- would learn lagged relationships b/c have a forecast time series?},
  file      = {Chapados08augFrcstGaussProc.pdf:Chapados08augFrcstGaussProc.pdf:PDF;Chapados08augFrcstGaussProc.pdf:Chapados08augFrcstGaussProc.pdf:PDF},
  location  = {Cambridge, MA},
  owner     = {sotterson},
  timestamp = {2009.03.04},
  url       = {http://books.nips.cc/nips20.html},
}

@InProceedings{Duvenaud11AddGausProcIntrct,
  author    = {Duvenaud, David K and Nickisch, Hannes and Rasmussen, Carl E},
  title     = {Additive gaussian processes},
  booktitle = {Advances in neural information processing systems},
  year      = {2011},
  pages     = {226--234},
  abstract  = {We introduce a Gaussian process model of functions which are additive. An additive
function is one which decomposes into a sum of low-dimensional functions,
each depending on only a subset of the input variables. Additive GPs generalize
both Generalized Additive Models, and the standard GP models which use
squared-exponential kernels. Hyperparameter learning in this model can be seen
as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive but
tractable parameterization of the kernel function, which allows efficient evaluation
of all input interaction terms, whose number is exponential in the input dimension.
The additional structure discoverable by this model results in increased
interpretability, as well as state-of-the-art predictive power in regression tasks.},
  comment   = {Possibly, a way to select which interaction terms to use in an additive QR model, possibly boosted as in Fenske11*

Interaction terms are selected via expanding Elementary symmetric polynomial (wikipedia):
http://en.wikipedia.org/wiki/Elementary_symmetric_polynomial

Several other interesting forecasting applications are also here (people who have cited this paper):
http://scholar.google.com/scholar?cites=5037411748325088580&as_sdt=2005&sciodt=0,5&hl=en

Could use this for selecting interaction terms in: Taieb14boostMultiStepARfrcst
Or for QR boosting in Fenske11*},
  file      = {Duvenaud11AddGausProcIntrct.pdf:Duvenaud11AddGausProcIntrct.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.04},
  url       = {http://papers.nips.cc/paper/4221-additive-gaussian-processes},
}

@Article{Bertotti16innovDiffusCorrNtwk,
  author   = {M.L. Bertotti and J. Brunner and G. Modanese},
  title    = {Innovation diffusion equations on correlated scale-free networks},
  journal  = {Physics Letters A},
  year     = {2016},
  volume   = {380},
  number   = {33},
  pages    = {2475 - 2479},
  issn     = {0375-9601},
  abstract = {We introduce a heterogeneous network structure into the Bass diffusion model, in order to study the diffusion times of innovation or information in networks with a scale-free structure, typical of regions where diffusion is sensitive to geographic and logistic influences (like for instance Alpine regions). We consider both the diffusion peak times of the total population and of the link classes. In the familiar trickle-down processes the adoption curve of the hubs is found to anticipate the total adoption in a predictable way. In a major departure from the standard model, we model a trickle-up process by introducing heterogeneous publicity coefficients (which can also be negative for the hubs, thus turning them into stiflers) and a stochastic term which represents the erratic generation of innovation at the periphery of the network. The results confirm the robustness of the Bass model and expand considerably its range of applicability.},
  doi      = {https://doi.org/10.1016/j.physleta.2016.06.003},
  keywords = {Scale-free networks, Diffusion equations, Bass model, Stochastic equations},
  url      = {http://www.sciencedirect.com/science/article/pii/S0375960116303085},
}

@Article{Arbenz12fastCompFuncRVsGAEP,
  author    = {Philipp Arbenz and Paul Embrechts and Giovanni Puccetti},
  title     = {The GAEP algorithm for the fast computation of the distribution of a function of dependent random variables},
  journal   = {Stochastics},
  year      = {2012},
  volume    = {84},
  number    = {5-6},
  pages     = {569-597},
  abstract  = {We introduce a new algorithm for numerically computing the distribution of an increasing function of d dependent, non-negative random variables with a given joint distribution. We prove the convergence of the algorithm and give convergence rates under regularity conditions.},
  comment   = {Useful for upscaling or for ReWP porfolio analysis.  Is copula based.

For just the sum of RV's see: Arbenz11fastSumDepRVsAEP
See also Gijbels14distSumCopulaDep},
  doi       = {10.1080/17442508.2011.566337},
  eprint    = {http://dx.doi.org/10.1080/17442508.2011.566337},
  file      = {Arbenz12fastCompFuncRVsGAEP.pdf:Arbenz12fastCompFuncRVsGAEP.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.22},
  url       = {
        http://dx.doi.org/10.1080/17442508.2011.566337

},
}

@Article{Tomassi11suffDimRedPkg,
  author    = {Tomassi, D.R. and Forzani, L.M. and Cook, R.D.},
  title     = {LDR: A Package for Likelihood-Based Sufficient Dimension Reduction},
  journal   = {Journal of Statistical Software},
  year      = {2011},
  volume    = {39},
  number    = {i03},
  abstract  = {We introduce a new MATLAB software package that implements several recently proposed likelihood-based methods for sufficient dimension reduction. Current capabilities include estimation of reduced subspaces with a fixed dimension d, as well as estimation of d by use of likelihood-ratio testing, permutation testing and information criteria. The methods are suitable for preprocessing data for both regression and classifcation. Implementations of related estimators are also available. Although the software is more oriented to command-line operation, a graphical user interface is also provided for prototype computations. Keywords: dimension reduction, inverse regression, principal components.},
  comment   = {Matlab package for information-preserving dimension reduction.

Implements several DR methods:
- Isotonic Principal Fitted Components (Cook 2007)
- Extended Principal Fitted Components (Cook 2007)
- Principal Fitted Components (Cook and Forzani 2009a)
- Structured Principal Fitted Components (Cook and Forzani 2009a)
- Covariance Reduction models (Cook and Forzani 2008)
- Likelihood Acquired Directions (Cook and Forzani 2009b)
- Envelope Models for Multivariate Linear Regression (Cook, Li and Chiaromonte 2009)

Other dimension reduction methods are supplied for completness:
- Principal Components
- Partial Least Squares
- Sliced Inverse Regression
- Sliced Average Variance Estimator
- Directed Regression
- Can do linear or basis function, at least.
- Use for ramps, to force to smaller dim b/f mut info calc.
- similar to R package, "dr" p. 3
- longer writeup in Adragni09suffDimRed ?
- matlab package here: http://sites.google.com/site/lilianaforzani/ldr-package},
  file      = {Tomassi11suffDimRedPkg.pdf:Tomassi11suffDimRedPkg.pdf:PDF},
  owner     = {scotto},
  publisher = {American Statistical Association},
  timestamp = {2011.05.20},
  url       = {http://www.jstatsoft.org/},
}

@TechReport{Lizier12compNetsTransEnt,
  author      = {Lizier, Joseph and Rubinov, Mikail and others},
  title       = {Multivariate construction of effective computational networks from observational data},
  institution = {{Max Planck Institute for Mathematics in the Sciences}},
  year        = {2012},
  type        = {{Preprint}},
  abstract    = {We introduce a new method for inferring effective network structure given a multivariate time-
series of activation levels of the nodes in the network. For each destination node in the network,
the method identifies the set of source nodes which can be used to provide the most statistically
significant information regarding outcomes of the destination, and are thus inferred as those source
information nodes from which the destination is computed. This is done using incrementally condi-
tioned transfer entropy measurements, gradually building the set of source nodes for a destination
conditioned on the previously identified sources. Our method is model-free and non-linear, but
more importantly it handles multivariate interactions between sources in creating outcomes at des-
tinations, rejects spurious connections for correlated sources, and incorporates measures to avoid
combinatorial explosions in the number of source combinations evaluated. We apply the method to
probabilistic Boolean networks (serving as models of Gene Regulatory Networks), demonstrating
the utility of the method in revealing significant proportions of the underlying structural network
given only short time-series of the network dynamics, particularly in comparison to other methods.

Keywords: computational neuroscience, effective networks, information transfer, information theory},
  comment     = {Greedy feature selection using conditional transfer entropy (really, conditional mutual information with autoregressive lags).  Explains weaknesses, improvements of this approach.  Has a statistical test permutation stopping criterion.


* Wibral15BitsBrainsTE puts this paper on context
* Lizier14JIDTinfoToolkit has software  for doing the info calcs

Conditional Mutual Information (CMI)
* seems to be same as partial mutual information (PMI)
* Decomposed into sum of CMI's (eq 13, same as eq. 18 in May11revVarSelNN)
* Entropy decomposed into sum of
 - target variable memory storage
 - contirbution of causal variables
 - remaining entropy
* Maximizing

Problems w/ connecting links w/ TE only (p. 10)
* maximizing this will collect multivariate interactions like XOR
* eliminate redundant links
* Optimizing CMI (CTE) will reduce (elimimate?) these problems

Optimality criterion
* max transfer entropy
* using only inputs that contribute statistically significant info
* iterative greedy solution not necess. totally optimal but optimal in this sense

Stopping Criterion
* statistical significance test
* permute casual variables (not lags of state/target variable)
* do this many times to get quantile for significant variables
* means you'll need to evaluate MI many times after each variable added: a CPU pig
* Vlachos10nonUnifStSpcMI as a simpler MI ratio test but this paper's author criticized it here:
http://tinyurl.com/zok5rdg

Feed forward, single variable greedy optimization failures (p. 14), misses
*  interactions where inputs have zero  indiv. TE w/ targ but do have TE w/ targ together
* interactions where redundancy w/ already selected variable keeps to vars out, even though they have unique TE w/ targ when added together.

Greedy optimization embellishments (the usual thing for this type of alg.)
* if find no single feat w/ target TE, consider feat pair TE, triples, etc.
* go backwards and prune indiv. vars that don't add TE



Author says that Wibral15BitsBrainsTE is a good intro to transfer entropy.},
  file        = {Lizier12compNetsTransEnt.pdf:Lizier12compNetsTransEnt.pdf:PDF},
  mispreprint = {{MIS-Preprint 25/2012}},
  owner       = {sotterson},
  timestamp   = {2016.07.22},
  url         = {http://www.mis.mpg.de/preprints/2012/preprint2012_25.pdf},
}

@INCOLLECTION{Bocharov13tseriesClustGeom,
  Author                   = {Bocharov, Alexei and Thiesson, Bo},
  Title                    = {Segmentation of Nonstationary Time Series with Geometric Clustering},
  Booktitle                = {Pattern Recognition - Applications and Methods},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2013},
  Editor                   = {Latorre Carmona, Pedro and S{\'a}nchez, J. Salvador and Fred, Ana L.N.},
  Volume                   = {204},
  Series                   = {Advances in Intelligent Systems and Computing},
  Pages                    = {93--107},
  Abstract                 = {We introduce a non-parametric method for segmentation in regimeswitching
time-series models. The approach is based on spectral clustering of
target-regressor tuples and derives a switching regression tree, where regime
switches are modeled by oblique splits. Such models can be learned efficiently
from data, where clustering is used to propose one single split candidate at each
split level. We use the class of ART time series models to serve as illustration,
but because of the non-parametric nature of our segmentation approach, it readily
generalizes to a wide range of time-series models that go beyond the Gaussian
error assumption in ART models. Experimental results on S and P 1500 financial
trading data demonstrates dramatically improved predictive accuracy for the exemplifying
ART models.
Keywords: Regime-switching time series, Spectral clustering, Regression tree,
Oblique split, Financial markets.},
  DOI                      = {10.1007/978-3-642-36530-0_8},
  File                     = {Bocharov13tseriesClustGeom.pdf:Bocharov13tseriesClustGeom.pdf:PDF},
  ISBN                     = {978-3-642-36529-4},
  Keywords                 = {Regime-switching time series; Spectral clustering; Regression tree; Oblique split; Financial markets},
  Owner                    = {sotterson},
  Timestamp                = {2014.03.07}
}

@InProceedings{Sanguinetti05numSpectClust,
  author    = {Sanguinetti, G. and Laidler, J. and Lawrence, N.D.},
  title     = {Automatic Determination of the Number of Clusters Using Spectral Algorithms},
  booktitle = {Machine Learning for Signal Processing, 2005 IEEE Workshop on},
  year      = {2005},
  pages     = {55--60},
  month     = sep,
  abstract  = {We introduce a novel spectral clustering algorithm that allows us to automatically determine the number of clusters in a dataset. The algorithm is based on a theoretical analysis of the spectral properties of block diagonal affinity matrices; in contrast to established methods, we do not normalise the rows of the matrix of eigenvectors, and argue that the non-normalised data contains key information that allows the automatic determination of the number of clusters present. We present several examples of datasets successfully clustered by our algorithm, both artificial and real, obtaining good results even without employing refined feature extraction techniques},
  comment   = {For use w/ other spectral clustering algorithms},
  doi       = {10.1109/MLSP.2005.1532874},
  file      = {Sanguinetti05numSpectClust.pdf:Sanguinetti05numSpectClust.pdf:PDF},
  keywords  = {block diagonal affinity matrices;dataset clusters;eigenvectors;feature extraction;nonnormalised data;spectral algorithm;spectral clustering;eigenvalues and eigenfunctions;feature extraction;matrix algebra;pattern clustering;spectral analysis;},
  owner     = {sotterson},
  timestamp = {2011.11.16},
}

@Article{Tokdar11simulLinQR,
  author    = {Tokdar, Surya T and Kadane, Joseph B},
  title     = {Simultaneous linear quantile regression: A semiparametric bayesian approach},
  journal   = {Bayesian Analysis},
  year      = {2011},
  volume    = {6},
  number    = {4},
  pages     = {1--22},
  abstract  = {We introduce a semi-parametric Bayesian framework for a simultaneous
analysis of linear quantile regression models. A simultaneous analysis is essential
to attain the true potential of the quantile regression framework, but is computa-
tionally challenging due to the associated monotonicity constraint on the quantile
curves. For a univariate covariate, we present a simpler equivalent characterization
of the monotonicity constraint through an interpolation of two monotone curves.
The resulting formulation leads to a tractable likelihood function and is embedded
within a Bayesian framework where the two monotone curves are modeled via lo-
gistic transformations of a smooth Gaussian process. A multivariate extension is
suggested by combining the full support univariate model with a linear projection
of the predictors. The resulting single-index model remains easy to ?t and provides
substantial and measurable improvement over the ?rst order linear heteroscedastic
model. Two illustrative applications of the proposed method are provided.
Keywords: Bayesian Inference, Bayesian Nonparametric Models, Gaussian Pro-
cesses, Joint Quantile Model, Linear Quantile Regression, Monotone Curves.},
  comment   = {Bayesian QR across all taus at the same time. Avoids crossover and maybe it's more roubust b/c it uses the whole distribution. Koenker is skeptical of this kind of idea: Koenker10bayesBanoQR},
  file      = {Tokdar11simulLinQR.pdf:Tokdar11simulLinQR.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.04.21},
}

@TechReport{Calderon09PpsplineStableQR,
  author      = {Calderon, Christopher P and Martinez, Josue G and Carroll, Raymond J and Sorensen, Danny C},
  title       = {Psqr: A stable and efficient penalized spline algorithm},
  institution = {Rice University},
  year        = {2009},
  abstract    = {We introduce an algorithm for reliably computing quantities associated with several types
of semiparametric mixed models in situations where the condition number on the random
effects matrix is large. The algorithm is numerically stable and efficient. It was designed to
process penalized spline (P-spline) models without making unnecessary numerical approxi-
mations. The algorithm, PSQR (P-splines via QR), is formulated in terms of QR decompo-
sitions. PSQR can treat both exactly rank defficient and ill-conditioned matrices. The latter
situation often arises in large scale mixed models and/or when a P-spline is estimated using
a basis with poor numerical properties, e.g. a truncated power function (TPF) basis. We
provide concrete examples where unnecessary numerical approximations introduce both sub-
tle and dramatic errors that would likely go undetected, thus demonstrating the importance
of using this reliable numerical algorithm. Simulation results studying a univariate function
and a longitudinal data set are used to demonstrate the algorithm. Extensions and the utility
of the method in more general semiparametric regression applications are briefly discussed.
MATLAB scripts demonstrating implementation are provided in the Supplemental Materials.
Keywords: Demmler-Reinsch Basis, Mixed Models, P-Spline, Semiparametric Regression},
  comment     = {Penalized spline using natural splines instead of bplines. Has Matlab. Apparently used for stochastic differential equation solver in Calderon10psplineDerivSDE , which also has a link to what appears to be the matlab for this paper.},
  file        = {Calderon09PpsplineStableQR.pdf:Calderon09PpsplineStableQR.pdf:PDF},
  owner       = {sotterson},
  publisher   = {submitted},
  timestamp   = {2014.10.30},
  url         = {http://www.caam.rice.edu/tech_reports/2009/TR09-15.pdf},
}

@Article{McGovern11multiDimTSmotifWeath,
  author    = {McGovern, Amy and Rosendahl, Derek H. and Brown, Rodger A. and Droegemeier, Kelvin K.},
  title     = {Identifying predictive multi-dimensional time series motifs: an application to severe weather prediction},
  journal   = {Data Mining and Knowledge Discovery},
  year      = {2011},
  volume    = {22},
  pages     = {232--258},
  issn      = {1384-5810},
  abstract  = {We introduce an efficient approach to mining multi-dimensional temporal streams of real-world data for ordered temporal motifs that can be used for prediction. Since many of the dimensions of the data are known or suspected to be irrelevant, our approach first identifies the salient dimensions of the data, then the key temporal motifs within each dimension, and finally the temporal ordering of the motifs necessary for prediction. For the prediction element, the data are assumed to be labeled. We tested the approach on two real-world data sets. To verify the generality of the approach, we validated the application on several subjects from the CMU Motion Capture database. Our main application uses several hundred numerically simulated supercell thunderstorms where the goal is to identify the most important features and feature interrelationships which herald the development of strong rotation in the lowest altitudes of a storm. We identified sets of precursors, in the form of meteorological quantities reaching extreme values in a particular temporal sequence, unique to storms producing strong low-altitude rotation. The eventual goal is to use this knowledge for future severe weather detection and prediction algorithms.},
  comment   = {Time series data mining for weather forecasting. Could be useful for regime learning, too. Maybe a good feature for probabilsitic forecasts or ramp prediction.

Also analog ensemble forecast},
  doi       = {10.1007/s10618-010-0193-7},
  file      = {McGovern11multiDimTSmotifWeath.pdf:McGovern11multiDimTSmotifWeath.pdf:PDF},
  groups    = {Ensemble, doReadWPV_2},
  issue     = {1-2},
  keywords  = {Temporal data mining; Multi-dimensional; Severe weather},
  language  = {English},
  owner     = {sotterson},
  publisher = {Springer US},
  timestamp = {2013.03.13},
}

@Article{Elwell11incLrnDrift,
  author    = {Elwell, R. and Polikar, R.},
  title     = {Incremental Learning of Concept Drift in Nonstationary Environments},
  journal   = {Neural Networks, IEEE Transactions on},
  year      = {2011},
  volume    = {22},
  number    = {10},
  pages     = {1517--1531},
  issn      = {1045-9227},
  abstract  = {We introduce an ensemble of classifiers-based approach for incremental learning of concept drift, characterized by nonstationary environments (NSEs), where the underlying data distributions change over time. The proposed algorithm, named Learn++.NSE, learns from consecutive batches of data without making any assumptions on the nature or rate of drift; it can learn from such environments that experience constant or variable rate of drift, addition or deletion of concept classes, as well as cyclical drift. The algorithm learns incrementally, as other members of the Learn++ family of algorithms, that is, without requiring access to previously seen data. Learn++.NSE trains one new classifier for each batch of data it receives, and combines these classifiers using a dynamically weighted majority voting. The novelty of the approach is in determining the voting weights, based on each classifier's time-adjusted accuracy on current and past environments. This approach allows the algorithm to recognize, and act accordingly, to the changes in underlying data distributions, as well as to a possible reoccurrence of an earlier distribution. We evaluate the algorithm on several synthetic datasets designed to simulate a variety of nonstationary environments, as well as a real-world weather prediction dataset. Comparisons with several other approaches are also included. Results indicate that Learn++.NSE can track the changing environments very closely, regardless of the type of concept drift. To allow future use, comparison and benchmarking by interested researchers, we also release our data used in this paper.},
  comment   = {for analog ensemble forecasting, adaptation w/ memory of past events (I have written something on this in an org file (or a onenote page) somewhere when I was thinking about APG) and regime learning},
  doi       = {10.1109/TNN.2011.2160459},
  file      = {Elwell11incLrnDrift.pdf:Elwell11incLrnDrift.pdf:PDF},
  groups    = {Ensemble, doReadNonWPV_1},
  keywords  = {learning (artificial intelligence);pattern classification;Learn++.NSE algorithm;classifier-based approach;concept drift learning;dynamically weighted majority voting;incremental learning;nonstationary environment characteristics;Algorithm design and analysis;Heuristic algorithms;Humans;Knowledge based systems;Machine learning;Training;Tuning;Concept drift;incremental learning;learning in nonstationary environments;multiple classifier systems;Algorithms;Artificial Intelligence;Automatic Data Processing;Environment;Humans;Learning;Models, Neurological;Neural Networks (Computer);Nonlinear Dynamics},
  ncite     = {63},
  owner     = {sotterson},
  timestamp = {2013.10.02},
}

@Article{Clifton13extrmFuncThNovDet,
  author    = {D. A. Clifton and L. Clifton and S. Hugueny and D. Wong and L. Tarassenko},
  title     = {An Extreme Function Theory for Novelty Detection},
  journal   = IEEE_J_STSP,
  year      = {2013},
  volume    = {7},
  number    = {1},
  pages     = {28--37},
  month     = feb,
  issn      = {1932-4553},
  abstract  = {We introduce an extreme function theory as a novel method by which probabilistic novelty detection may be performed with functions, where the functions are represented by time-series of (potentially multivariate) discrete observations. We set the method within the framework of Gaussian processes (GP), which offers a convenient means of constructing a distribution over functions. Whereas conventional novelty detection methods aim to identify individually extreme data points, with respect to a model of normality constructed using examples of {\textquotedblleft}normal{\textquotedblright} data points, the proposed method aims to identify extreme functions, with respect to a model of normality constructed using examples of {\textquotedblleft}normal{\textquotedblright} functions, where those functions are represented by time-series of observations. The method is illustrated using synthetic data, physiological data acquired from a large clinical trial, and a benchmark time-series dataset.
Index Terms?Functional analysis, Gaussian processes, signal
processing algorithms.},
  comment   = {For ModernWindABS and ReWP},
  doi       = {10.1109/JSTSP.2012.2234081},
  file      = {Clifton13extrmFuncThNovDet.pdf:Clifton13extrmFuncThNovDet.pdf:PDF},
  keywords  = {Gaussian processes, signal detection, Gaussian processes, benchmark time-series dataset, discrete observations, extreme function theory, normal data points, normal functions, physiological data, probabilistic novelty detection, synthetic data, Data models, Hidden Markov models, Joints, Mathematical model, Predictive models, Probabilistic logic, Training, Functional analysis, Gaussian processes, signal processing algorithms},
  owner     = {sotterson},
  timestamp = {2016.12.17},
}

@Article{Clevert15dpLrnELU,
  author    = {Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  title     = {Fast and accurate deep network learning by exponential linear units (elus)},
  journal   = {arXiv preprint arXiv:1511.07289},
  year      = {2015},
  abstract  = {We introduce the ?exponential linear unit? (ELU) which speeds up learning in
deep neural networks and leads to higher classification accuracies. Like recti-
fied linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PRe-
LUs), ELUs alleviate the vanishing gradient problem via the identity for positive
values. However ELUs have improved learning characteristics compared to the
units with other activation functions. In contrast to ReLUs, ELUs have negative
values which allows them to push mean unit activations closer to zero like batch
normalization but with lower computational complexity. Mean shifts toward zero
speed up learning by bringing the normal gradient closer to the unit natural gra-
dient because of a reduced bias shift effect. While LReLUs and PReLUs have
negative values, too, they do not ensure a noise-robust deactivation state. ELUs
saturate to a negative value with smaller inputs and thereby decrease the forward
propagated variation and information. Therefore ELUs code the degree of pres-
ence of particular phenomena in the input, while they do not quantitatively model
the degree of their absence.
In experiments, ELUs lead not only to faster learning, but also to significantly bet-
ter generalization performance than ReLUs and LReLUs on networks with more
than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU
networks with batch normalization while batch normalization does not improve
ELU networks. ELU networks are among the top 10 reported CIFAR-10 results
and yield the best published result on CIFAR-100, without resorting to multi-view
evaluation or model averaging. On ImageNet, ELU networks considerably speed
up learning compared to a ReLU network with the same architecture, obtaining
less than 10% classification error for a single crop, single model network},
  comment   = {ELUs are faster than RELU b/c have negative.   Related poster by the authors also attached.},
  file      = {ArXiv Paper:Clevert15dpLrnELU.pdf:PDF;Poster:Clevert15dpLrnELU_poster.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.04},
}

@Article{Zuber10varImpSelDecorr,
  author         = {Verena Zuber and Korbinian Strimmer},
  title          = {Variable importance and model selection by decorrelation},
  journal        = {arXiv.org},
  year           = {2010},
  volume         = {arXiv:1007.5516v2},
  month          = aug,
  abstract       = {We introduce the CAR score, a simple criterion for ranking and selecting variables in linear regression that arises naturally in the best predictor formulation of the linear model. The CAR score measures the correlation between the response and the Mahalanobis-decorrelated predictors and reduces to marginal correlation if the predictors are uncorrelated. As a population quantity, the CAR score can be used irrespective of the choice of inference paradigm. We show here that the squared CAR score is a natural measure of variable importance and that it provides a canonical ordering of the explanatory variables. Classical model selection using AIC or other information criteria correspond to thresholding CAR scores at a fixed level. In computer simulations we demonstrate that CAR scores are highly effective for variable selection with a prediction error that compares favorable with the elastic net and other current regression procedures. We illustrate the CAR model selection approach by analyzing diabetes data as well as gene expression data from the human frontal cortex. An R package "care" implementing the approach is available from CRAN.},
  comment        = {Paper for R "care" package
* has a function that replaces R corpcor package mvr.shrink
-- mvr.shrink related to: Opgen-Rhein07rankShrink
* See also PCA component selection comments in Review of Nathans12mlrVarImp},
  comments       = {22 pages, 3 figures, 8 tables},
  eprint         = {1007.5516},
  file           = {Zuber10varImpSelDecorr.pdf:Zuber10varImpSelDecorr.pdf:PDF},
  oai2identifier = {1007.5516},
  owner          = {scot},
  part           = {version 2},
  timestamp      = {2010.09.07},
  url            = {http://arxiv.org/abs/1007.5516v2},
}

@Article{Veeramachaneni13CopulaBasedWind,
  author    = {Veeramachaneni, Kalyan and Feldman-Fitzthum, Teasha and O?Reilly, Una-May and Cuesta-Infante, Alfredo},
  title     = {Copula-Based Wind Resource Assessment},
  journal   = {NIPS},
  year      = {2013},
  abstract  = {We introduce the use of multivariate Gaussian Copula modeling to improve the
accuracy of wind resource assessment. The technique also serves to lower assess-
ment costs because it requires less sensing data than conventional methods.},
  comment   = {Models the wind resource assessment problem with a Gaussian Copula instead of MCP.

Later, he did this with a Copula Graphical model: Veeramachaneni15cplaGrphMdlsWindRsrcEst},
  file      = {Veeramachaneni13CopulaBasedWind.pdf:Veeramachaneni13CopulaBasedWind.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.27},
  url       = {http://www.kalyanv.org/publications.html},
}

@Article{Gijbels14distSumCopulaDep,
  author    = {Ir??ne Gijbels and Klaus Herrmann},
  title     = {On the distribution of sums of random variables with copula-induced dependence},
  journal   = {Insurance: Mathematics and Economics},
  year      = {2014},
  volume    = {59},
  pages     = {27 - 44},
  issn      = {0167-6687},
  abstract  = {We investigate distributional properties of the sum of d possibly unbounded random variables. The joint distribution of the random vector is formulated by means of an absolutely continuous copula, allowing for a variety of different dependence structures between the summands. The obtained expression for the distribution of the sum features a separation property into marginal and dependence structure contributions typical for copula approaches. Along the same lines we obtain the formulation of a conditional expectation closely related to the expected shortfall common in actuarial and financial literature. We further exploit the separation to introduce new numerical algorithms to compute the distribution and quantile function, as well as this conditional expectation. A comparison with the most common competitors shows that the discussed Path Integration algorithm is the most suitable method for computing these quantities. In our example, we apply the theory to compute Value-at-Risk forecasts for a trivariate portfolio of index returns.},
  comment   = {Relevant to probabilistic forecast aggregation and upscaling when dependence is modelled with copulas.

See also: Arbenz12fastCompFuncRVsGAEP},
  doi       = {http://dx.doi.org/10.1016/j.insmatheco.2014.08.002},
  file      = {:papers\\Gijbels14distSumCopulaDep.pdf:PDF},
  keywords  = {(G)AEP algorithm, Aggregation of risk, Copula, Dependence, Expected shortfall, Path integration, Sums of random variables, Value-at-risk},
  owner     = {sotterson},
  timestamp = {2017.07.10},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167668714000961},
}

@Article{Zhang01multiWvltFrcst,
  author    = {Zhang, B.-L. and Coggins, R. and Jabri, M.A. and Dersch, D. and Flower, B.},
  title     = {Multiresolution forecasting for futures trading using wavelet decompositions},
  journal   = {Neural Networks, IEEE Transactions on},
  year      = {2001},
  volume    = {12},
  number    = {4},
  pages     = {765--775},
  month     = jul,
  issn      = {1045-9227},
  abstract  = {We investigate the effectiveness of a financial time-series forecasting strategy which exploits the multiresolution property of the wavelet transform. A financial series is decomposed into an over complete, shift invariant scale-related representation. In transform space, each individual wavelet series is modeled by a separate multilayer perceptron (MLP). We apply the Bayesian method of automatic relevance determination to choose short past windows (short-term history) for the inputs to the MLPs at lower scales and long past windows (long-term history) at higher scales. To form the overall forecast, the individual forecasts are then recombined by the linear reconstruction property of the inverse transform with the chosen autocorrelation shell representation, or by another perceptron which learns the weight of each scale in the prediction of the original time series. The forecast results are then passed to a money management system to generate trades},
  comment   = {Basic intro to wavelet frcsting, modwt and a trous arrangment. A 2005 tech report seems to be largely copied from it: Ahmad05multiWvltFrcst},
  doi       = {10.1109/72.935090},
  file      = {Zhang01multiWvltFrcst.pdf:Zhang01multiWvltFrcst.pdf:PDF},
  keywords  = {Bayes methods;commodity trading;financial data processing;forecasting theory;multilayer perceptrons;time series;wavelet transforms;Bayese method;financial forecasting;futures trading;multilayer perceptron;multiresolution forecasting;time-series;wavelet decompositions;wavelet transform;Autocorrelation;Autoregressive processes;Bayesian methods;Financial management;History;Multilayer perceptrons;Neural networks;Predictive models;Wavelet coefficients;Wavelet transforms},
  owner     = {sotterson},
  timestamp = {2013.03.15},
}

@TechReport{Song15end2endDeepLnSpchRecog,
  author      = {Song, William and Cai, Jim},
  title       = {End-to-end deep neural network for automatic speech recognition},
  institution = {Technical Report CS224D, University of Stanford},
  year        = {2015},
  abstract    = {We investigate the efficacy of deep neural networks on speech recognition. Specif-
ically, we implement an end-to-end deep learning system that utilizes mel-filter
bank features to directly output to spoken phonemes without the need of a tra-
ditional Hidden Markov Model for decoding. The system will comprise of two
variants of neural networks for phoneme recognition. In particular, we utilize
convolutional for frame level classification and recurrent architecture with Con-
nectionist Temporal Classification loss for decoding the frames into a sequence of
phonemes. We carry out our experiments on the TIMIT dataset. We have managed
to obtain 22.1\% for the frame error rate with our CNN, which to our knowledge,
closely matches the state-of-the-art. Our decoded phone sequence achieves an
error of 29.4%.},
  comment     = {Table 2 shows that get best results when conv. NN's entirely replace MFCCs in speech recog.

Deng15deepLrnSpchLang has overview slides},
  file        = {Song15end2endDeepLnSpchRecog.pdf:Song15end2endDeepLnSpchRecog.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2017.02.01},
  url         = {https://pdfs.semanticscholar.org/528d/a8ef5cac3b69348b84a50ac2d596ddbee394.pdf},
}

@InProceedings{Mathieu13enrgArbtrgThrmCtlLds,
  author    = {J. L. Mathieu and M. Kamgarpour and J. Lygeros and D. S. Callaway},
  title     = {Energy arbitrage with thermostatically controlled loads},
  booktitle = {Proc. European Control Conf. (ECC)},
  year      = {2013},
  pages     = {2519--2526},
  month     = jul,
  abstract  = {We investigate the potential for aggregations of residential thermostatically controlled loads (TCLs), such as air conditioners, to arbitrage intraday wholesale electricity market prices via non-disruptive direct load control. Since wholesale electricity prices reflect power system conditions, arbitrage provides a service to the grid, helping to balance real-time supply and demand. While previous work on the energy arbitrage problem has used simple energy storage models, we use high fidelity TCL-specific models which allow us to understand and quantify the full capabilities and constraints of these time-varying systems. We explore two optimization/control frameworks for solving the arbitrage problem, both based on receding horizon linear programming. Since we find that the first approach requires significant computation, we develop a second approach involving decomposition of the optimal control problem into separate optimization and control problems. Simulation results show that TCLs could save on the order of 10\% of wholesale energy costs via arbitrage, with savings decreasing with price forecast error.},
  comment   = {A market model paper using a perfect price forecast saves 10% of costs, with savings decreasing with a worse price forecast.

Henry Martin said:
>>> Henry Martin 11/21/2016 1:22 pm >>>
Hi Scott,
the last phrase in the Abstract sums it up. Their approach for the disturbed price forecast is described in section C. They talk a lot about how they model the thermal loads, I think you can skip those parts.
In figure 6 they show the dependency of forecast error and energy cost savings. In the graph, the relationsship appears not to be very strong but in the text and in the conclusion they seem convinced, that there is an important relationship in between the forecast error and profitability. So maybe the graph doesn't tell the whole story.  Overall, this was one of the best paper I found in terms of market modeling/ real time market consideration.},
  file      = {Mathieu13enrgArbtrgThrmCtlLds.pdf:Mathieu13enrgArbtrgThrmCtlLds.pdf:PDF},
  keywords  = {air conditioning, forecasting theory, infinite horizon, linear programming, load regulation, optimal control, power grids, power markets, pricing, thermostats, time-varying systems, air conditioners, energy arbitrage problem, energy storage model, grid service, high fidelity TCL-specific model, intraday wholesale electricity market price arbitrage, nondisruptive direct load control, optimal control problem, optimization problem, power system conditions, price forecast error, real-time supply and demand, receding horizon linear programming, residential thermostatically controlled load, time-varying system, wholesale electricity price, wholesale energy cost, Atmospheric modeling, Computational modeling, Optimization, Sociology, Statistics, Switches},
  owner     = {sotterson},
  timestamp = {2016.11.21},
  url       = {http://ieeexplore.ieee.org/document/6669582/authors},
}

@Article{Vlachos10nonUnifStSpcMI,
  author    = {Vlachos, Ioannis and Kugiumtzis, Dimitris},
  title     = {Nonuniform state-space reconstruction and coupling detection},
  journal   = {Physical Review E},
  year      = {2010},
  volume    = {82},
  number    = {1},
  pages     = {016207},
  abstract  = {We investigate the state space reconstruction from multiple time series derived from continuous
and discrete systems and propose a method for building embedding vectors progressively using
information measure criteria regarding past, current and future states. The embedding scheme can
be adapted for different purposes, such as mixed modelling, cross-prediction and Granger causality.
In particular we apply this method in order to detect and evaluate information transfer in coupled
systems. As a practical application, we investigate in records of scalp epileptic EEG the information
flow across brain areas.},
  comment   = {Another Mutual information based state space lag detector.  Notes Kraskov dimension-dependent bias.

* has some Kraskov MI details metioned in Kugiumtzis13directCplngPMI
* has 2 derivations for CMI/PMI
 - the result is probably the same partial mutual information technique published first in Frenzel07partMutInfo
 - shows how CMI/PMI has less dimension-dependent bias (due to Kraskov alg.) than MI, making it more suitable for feature selection
* KNN k=10 for npts=4096-8192
suggests smaller K for Npts=512.  max dX,dZ=8, like in middle of a 16 dim featsel problem.
* A 40 dim prob. is "moderate," and I think K=10 was used there (but it is not stated)

* explains shortcomings of greedy search, has some (obvious) fixes:  Lizier12compNetsTransEnt

Stopping Criterion
* In theory, what to stop when MI stops increasing but can't do this straighforwardly
* iterations stop w/ MI ratio threshold: seems like a "strict" A=0.95 is recommended
 - Ordinary MI Kraskov increases, then decreases w/ increasing dimension, as features are added
 - trick is to compute prev. iteration MI by projecting MI after feat is added.
 - this makes the bias roughly equal b/f and after a feature is added
* May use MI instead of PMI/CMI b/c really want to max MI
* and.. I(X;Y|Z)>MI(X;Y) is possble (Frenzel07partMutInfo), which could cause a stop in the wrong place},
  doi       = {10.1103/PhysRevE.82.016207},
  file      = {Vlachos10nonUnifStSpcMI.pdf:Vlachos10nonUnifStSpcMI.pdf:PDF},
  publisher = {APS},
}

@InProceedings{Shah14studentTprocAltGauss,
  author    = {Amar Shah and Andrew Wilson and Zoubin Ghahramani},
  title     = {{Student-t Processes as Alternatives to Gaussian Processes}},
  booktitle = {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics},
  year      = {2014},
  editor    = {Samuel Kaski and Jukka Corander},
  volume    = {33},
  series    = {Proceedings of Machine Learning Research},
  pages     = {877--885},
  address   = {Reykjavik, Iceland},
  month     = {22--25 Apr},
  publisher = {PMLR},
  abstract  = {We investigate the Student-t process as an alternative to the Gaussian process as a nonparametric prior over functions. We derive closed form expressions for the marginal likelihood and predictive distribution of a Student-t process, by integrating away an inverse Wishart process prior over the covariance kernel of a Gaussian process model.  We show surprising equivalences between different hierarchical Gaussian process models leading to Student-t processes, and derive a new sampling scheme for the inverse Wishart process, which helps elucidate these equivalences. Overall, we show that a Student-t process can retain the attractive properties of a Gaussian process -- a nonparametric representation, analytic marginal and predictive distributions, and easy model selection through covariance kernels -- but has enhanced flexibility, and a predictive covariance that, unlike a Gaussian process, explicitly depends on the values of training observations.  We verify empirically that a Student-t process is especially useful in situations where there are changes in covariance structure, or in applications like Bayesian optimization, where accurate predictive covariances are critical for good performance. These advantages come at no additional computational cost over Gaussian processes.},
  comment   = {Covariance matrices that change depending upon the data.  It seems like this is better than doing a non-stationary marginal transform, with a fixed (non-matching) covariance matrix, as in Tastu15spcTimeTrajGaussCpla, etc.},
  file      = {:Shah14studentTprocAltGauss.pdf:PDF},
  url       = {http://proceedings.mlr.press/v33/shah14.html},
}

@InProceedings{Zou18multiTaskDisease,
  author    = {Zou, Bin and Lampos, Vasileios and Cox, Ingemar},
  title     = {Multi-Task Learning Improves Disease Models from Web Search},
  booktitle = {Proceedings of the 2018 World Wide Web Conference},
  year      = {2018},
  series    = {WWW '18},
  pages     = {87--96},
  address   = {Republic and Canton of Geneva, Switzerland},
  publisher = {International World Wide Web Conferences Steering Committee},
  abstract  = {We investigate the utility of multi-task learning to disease surveillance using Web search data. Our motivation is two-fold. Firstly, we assess whether concurrently training models for various geographies - inside a country or across different countries - can improve accuracy. We also test the ability of such models to assist health systems that are producing sporadic disease surveillance reports that reduce the quantity of available training data. We explore both linear and nonlinear models, specifically a multi-task expansion of elastic net and a multi-task Gaussian Process, and compare them to their respective single task formulations. We use influenza-like illness as a case study and conduct experiments on the United States (US) as well as England, where both health and Google search data were obtained. Our empirical results indicate that multi-task learning improves regional as well as national models for the US. The percentage of improvement on mean absolute error increases up to 14.8% as the historical training data is reduced from 5 to 1 year(s), illustrating that accurate models can be obtained, even by training on relatively short time intervals. Furthermore, in simulated scenarios, where only a few health reports (training data) are available, we show that multi-task learning helps to maintain a stable performance across all the affected locations. Finally, we present results from a cross-country experiment, where data from the US improves the estimates for England. As the historical training data for England is reduced, the benefits of multi-task learning increase, reducing mean absolute error by up to 40%.},
  acmid     = {3186050},
  comment   = {Multitask learning improves both linear regression elastic net and Gaussian process models.  They help by jointly learning a country with lots of data (US) and one with restricted data (England), and by jointly predicting US regions.

Multitask intro in Caruana97multiTaskLearninng},
  doi       = {10.1145/3178876.3186050},
  file      = {:Zou18multiTaskDisease.pdf:PDF},
  groups    = {Scott:1},
  isbn      = {978-1-4503-5639-8},
  keywords  = {disease surveillance, gaussian processes, multi-task learning, regularized regression, user-generated content, web search},
  location  = {Lyon, France},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3178876.3186050},
}

@InProceedings{Rabanser19empDatSetShiftDet,
  author    = {Stephan Rabanser and Stephan Günnemann and Zachary C. Lipton},
  title     = {Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift},
  booktitle = {ICLR Workshop on Debugging Machine Learning Models},
  year      = {2019},
  abstract  = {We might hope that when faced with unexpected
inputs, well-designed software systems would fire
off warnings. Machine learning (ML) systems,
however, which depend strongly on properties of
their inputs (e.g. the i.i.d. assumption), tend to
fail silently. This paper explores the problem of
building ML systems that fail loudly, investigating
methods for detecting dataset shift and identify-
ing exemplars that most typify the shift. We focus
on several datasets and various perturbations to
both covariates and label distributions with vary-
ing magnitudes and fractions of data affected. In-
terestingly, we show that while classifier-based
methods designed to explicitly discriminate be-
tween source and target domains perform well in
high-data settings, they perform poorly in low-
data settings. Moreover, across the dataset shifts
that we explore, a two-sample-testing-based ap-
proach using pre-trained classifiers for dimension-
ality reduction performs best.},
  comment   = {Ways of detecting when the data your ML algorithm was trained on no matches what you're testing on.

Extended Version linked to by author: https://www.kdd.in.tum.de/en/publications/
Has Python:  https://github.com/steverab/failing-loudly},
  file      = {Extended version of ICLR paper:Rabanser19empDatSetShiftDet_extended.pdf:PDF;ICLR paper:Rabanser19empDatSetShiftDet_ICLR.pdf:PDF},
  url       = {https://arxiv.org/abs/1810.11953},
}

@Article{Peduzzi96numEVvarLogRgrssn,
  author   = {Peter Peduzzi and John Concato and Elizabeth Kemper and Theodore R. Holford and Alvan R. Feinstein},
  title    = {A simulation study of the number of events per variable in logistic regression analysis},
  journal  = {Journal of Clinical Epidemiology},
  year     = {1996},
  volume   = {49},
  number   = {12},
  pages    = {1373 - 1379},
  issn     = {0895-4356},
  abstract = {We performed a Monte Carlo study to evaluate the effect of the number of events per variable (EPV) analyzed in logistic regression analysis. The simulations were based on data from a cardiac trial of 673 patients in which 252 deaths occurred and seven variables were cogent predictors of mortality; the number of events per predictive variable was (2527 = 36) for the full sample. For the simulations, at values of EPV = 2, 5, 10, 15, 20, and 25, we randomly generated 500 samples of the 673 patients, chosen with replacement, according to a logistic model derived from the full sample. Simulation results for the regression coefficients for each variable in each group of 500 samples were compared for bias, precision, and significance testing against the results of the model fitted to the original sample. For EPV values of 10 or greater, no major problems occurred. For EPV values less than 10, however, the regression coefficients were biased in both positive and negative directions; the large sample variance estimates from the logistic model both overestimated and underestimated the sample variance of the regression coeffi-cients; the 90\% confidence limits about the estimated values did not have proper coverage; the Wald statistic was conservative under the null hypothesis; and paradoxical associations (significance in the wrong direction) were increased. Although other factors (such as the total number of events, or sample size) may influence the validity of the logistic model, our findings indicate that low EPV can lead to major problems.},
  comment  = {Using random draws of real cardiovascular data, needed about 10 positive events per input variable for logistic regression -- can use to pick the maximum input feature dimension.  I believe that this was for "exact" logistic regression;  was the 1st hit for a google scholar search for "exact logistic regression"

I wonder if it assumed that all variables were independent?

Would the trick in King01logRgrssnRare be a help?
Does King02logistRgrssnMLvsExact eliminate the need for Exact LR?},
  doi      = {https://doi.org/10.1016/S0895-4356(96)00236-3},
  file     = {:Peduzzi96numEVvarLogRgrssn.pdf:PDF},
  keywords = {Monte Carlo, bias, precision, significance testing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0895435696002363},
}

@Article{deLuna05predictSpatioTemp,
  author    = {de Luna, Xavier and Genton, Marc G},
  title     = {Predictive spatio-temporal models for spatially sparse environmental data},
  journal   = {Statistica Sinica},
  year      = {2005},
  volume    = {15},
  pages     = {547--568},
  abstract  = {We present a family of spatio-temporal models which are geared to provide time-forward predictions in environmental applications where data is spatially sparse but temporally rich. That is measurements are made at few spatial locations (stations), but at many regular time intervals. When predictions in the time direction is the purpose of the analysis, then spatial-stationarity assumptions which are commonly used in spatial modeling, are not necessary. The family of models proposed does not make such assumptions and consists of a vector autoregressive (VAR) specification, where there are as many time series as stations. However, by taking into account the spatial dependence structure, a model building strategy is introduced which borrows its simplicity from the Box-Jenkins strategy for univariate autoregressive (AR) models for time series. As for AR models, model building may be performed either by displaying sample partial correlation functions, or by minimizing an information criterion. A simulation study illustrates the gain resulting from our modeling strategy. Two environmental data sets are studied. In particular, we find evidence that a parametric modeling of the spatio-temporal correlation function is not appropriate because it rests on too strong assumptions. Moreover, we propose to compare model selection strategies with an out-of-sample validation method based on recursive prediction errors. Key words and phrases: Accumulated prediction errors, spatio-temporal correlation, partial correlation, vector autoregression.},
  comment   = {Kristin says that this is pretty much what 3IER does for hour ahead forecasting

* geographic ordering for feature selection related slides in: deLuna05predictSpatioTemp trend removal explained in Hurvich09trendForecast},
  file      = {:deLuna05predictSpatioTemp.pdf:PDF;deLuna05predictSpatioTemp.pdf:deLuna05predictSpatioTemp.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.09.30},
  url       = {http://www.diva-portal.org/umu/opus/publication.xml?id=1126},
}

@InProceedings{Caruana04ensLibSelGen,
  author    = {Caruana, Rich and Niculescu-Mizil, Alexandru and Crew, Geoff and Ksikes, Alex},
  title     = {Ensemble Selection from Libraries of Models},
  booktitle = {Proceedings of the Twenty-first International Conference on Machine Learning},
  year      = {2004},
  series    = {ICML '04},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {We present a method for constructing ensem-
bles from libraries of thousands of models.
Model libraries are generated using different
learning algorithms and parameter settings.
Forward stepwise selection is used to add to
the ensemble the models that maximize its
performance. Ensemble selection allows en-
sembles to be optimized to performance met-
ric such as accuracy, cross entropy, mean
precision, or ROC Area. Experiments with
seven test problems and ten metrics demon-
strate the benefit of ensemble selection.},
  acmid     = {1015432},
  comment   = {Forward greedy selection picks ensemble w/ best average performance. Maybe most interesting thing is the methods used to geneerate the ensembles (in the appendix)

Possibly interesting for the adaboost stuff},
  doi       = {10.1145/1015330.1015432},
  file      = {Caruana04ensLibSelGen.pdf:Caruana04ensLibSelGen.pdf:PDF},
  groups    = {Ensemble, PointDerived, doReadNonWPV_1},
  isbn      = {1-58113-838-5},
  location  = {Banff, Alberta, Canada},
  owner     = {sotterson},
  timestamp = {2014.01.29},
}

@Article{Horenko06locPCAhmm,
  author      = {Horenko, Illia and Schmidt-Ehrenberg, Johannes and Sch\"{u}te, Christof},
  title       = {Set-Oriented Dimension Reduction: Localizing Principal Component Analysis Via Hidden {Markov} Models},
  journal     = {Computational Life Sciences II},
  year        = {2006},
  volume      = {4216},
  pages       = {74--85},
  abstract    = {We present a method for simultaneous dimension reduction and metastability analysis of high dimensional time series. The approach is based on the combination of hidden Markov models (HMMs) and principal component analysis. We derive optimal estimators for the log-likelihood functional and employ the Expectation Maximization algorithm for its numerical optimization. We demonstrate the performance of the method on a generic 102-dimensional example, apply the new HMM-PCA algorithm to a molecular dynamics simulation of 12?alanine in water and interpret the results.},
  affiliation = {Freie Universit?t Berlin, Department of Mathematics and Informatics, Arnimallee 6, D-14195 Berlin Germany Germany},
  booktitle   = {Computational Life Sciences II},
  comment     = {pca switching w/ HMM's
Use for?
- extreme events, like ramps
- prediction confidence intervals
- regime detection/switching
- maybe for phase error detection (combined w/ spline pca) on forecast errors at various horizons

Where to get categories?
- discriminative PCA: Jombart10discrimPCA
- correlation (subspace) clustering: see energytop.org
- relation to correlation clustering? (see energytop.org)},
  doi         = {10.1007/11875741_8},
  editor      = {Berthold, Michael and Glen, Robert and Fischer, Ingrid},
  file        = {Horenko06locPCAhmm.pdf:Horenko06locPCAhmm.pdf:PDF},
  publisher   = {Springer Berlin / Heidelberg},
  series      = {Lecture Notes in Computer Science},
  timestamp   = {2011.01.12},
}

@InProceedings{Braun16windFrcstRadGrad,
  author    = {Axel Braun},
  title     = {Multi-Regional Wind Power Forecasting Based on Spatio-temporal Input Information in an Artificial Intelligence Model},
  booktitle = {Wind Integration Workshop},
  year      = {2016},
  abstract  = {We present a model for jointly predicting wind
power production of the four German TSO control zones for
forecast horizons up to eight hours. Our method consists of an
artificial intelligence (AI) model combining different kinds of
input data, which provide spatio-temporal information on the
current and future states of local and control zone wind power
production. To reduce the large number of inputs from local
wind farm forecasts, a principal component analysis is applied
before these data enter the AI model.
Results show that combining times series methods and spatio-
temporal information in an AI model leads to an improved
forecast quality regarding mean and extreme errors. Moreo-
ver, characteristic forecast error patterns are significantly
reduced.
Keywords-component; regional wind power forecast, upscal-
ing, spatio-temporal information, artificial neural network,
principal component analysis},
  comment   = {FINISH THIS AFTER GETTING AXEL's EMAIL RESPONSE

German TSO regional forecasts are improved (RMSE, extreme errors) by autoregressive exponential weighting of past errors, high dim inputs w/ PCA, inclusion of temporal gradient of clear sky radiation, Time-of-day and year harmonics.  Extreme Learning NN works as well as slow-to-train Matlab NN's.
Uses radiation gradients to reduce German regional windpower forecast error.   Also used an extreme learning neural net.

Seems that PV has the kind of temporal errors (I think)

Inputs
1. TSO powers (3 of 4 regions, skip TransnetBW)
   - past
   - present
2  reference farms (69)
   - forecasts
   - trends (of forecasts, I think)
3. Time-related featues
   - Time
     --  time day & 1 Fourier harmonic
     -- time of year & 1 Fourier harmonic
   - Sun position via mean global clear sky radiation
     -- the average over "5 sites"
     -- also the 1st time diff (I think).  Axel calls it a "gradient"
     -- Beause of observed forecast error pattern
        - rapid drop and then rise of

Regression Target
* the 4 German TSO wind power production, and all-Germany
* don't report much about the Transnet BW "for clarity" but I suppose it's b/c results were bad


---------------------------------------------------

Axel's answers to my questions about this papper.  They're mixed in the text (were yellow in his email):

Hi Axel,
I just finished reading your WIW16 paper -- really interesting.  Could you answer some questions I have about it?



1.Figure 3: Does this figure mean that wind power production (not errors) rapidly decreases and then rapidly increases both at sunrise and sunset, and that the time order (decrease-->increase) is the same for both? Yes, although I haven't checked the coninsidence with sunrise and sunset in detail. It could happen some hours before or after. Do you know of a physical explanation for this? Unfortunately not. I think it must have something to do with the fact that land has more rapid temperature changes than sea. Following from that you have a cross in temperature after sunrise and sunset resp.. Maybe Thomas can help.

2.Page 2: A bullet point says that the global radiation input is the "mean over 5 sites."

1.Is this the 4 TSOs + Germany?  No, these are really five different lat/long-couples, i.e. the global radiation is calculated for each of those sites and averaged afterwards. I wanted to have a rough indicator of the error pattern.

2.Isn't Germany just the sum of the 4 TSO's?

3.Are the global radiation inputs the same for all ANNs, regardless of the region being predicted? Yes. It is worth a try to see if using the time series of the 5 sites separately would give better results, but I haven't done it.

3.Is it correct that there is a different Extreme Learning Machine for each predicted region i.e. that there is a separate ANN with one output for each? No, input and hidden layer of the ANN the Extreme Learning Machine consists of is the same and it has 4 Outputs. Only the Output Weights differ for each region

4.By "gradient" do you mean "first time difference" of a scalar average radiation? Yes  Usually, "gradient" means something multi-dimensional but that doesn't seem to be the case here.

5.Was the number of PCA components chosen by an optimization procedure? No. It was a rough estimation.

6.Page 4.  Were the extreme learning machine biases were also chosen randomly, in addition to the weights? Just looked it up: Biases of the input layer weights are set to zero.

7.Page 4.  The beta coefficients are actually vectors, aren't they?  With one vector, beta_i, for the ith prediction target?Yes you're right.

8.Page 4.  The equation has the term "IW dot I"  but this isn't a dot product, is it?  Isn't it a Matrix/vector multipication?Yes, you're right. Seems like I should have done this more carefully...

9.Page 4.  There are some sentences like "Grid Area Upscaling is signifcantly improved by each of the other forecast models".  But "improved" to you mean "out performed?" Yes.

10.Section "Impact of Offshore":  Do you think the difference in bias correction is related to the fact that the wind power is offshore, or is it just that the offshore data is bad?  (downregulation, out-of-date or wrong installed capacity information, ...) Second explanation: But it is not "bad data", it is just hard to predict with the information that we have.

11.It seems like extreme learning has no performance advantage (is that right?) so the main benefit is training ease and speed.  How much faster/easier was it? If I remember right, the time for Training a single forecast horizon reduced from about 60min to 2-5min. This means, one could train more than one Extreme Learning Machine, maybe get better results by combining the results and still be faster-> Something I haven't done yet.},
  date      = {November},
  file      = {Braun16windFrcstRadGrad.pdf:Braun16windFrcstRadGrad.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2016.11.12},
  url       = {http://windintegrationworkshop.org/vienna2016/},
}

@InProceedings{Braun14directUpscaling,
  author    = {Axel Braun and Jan Dobschinski},
  title     = {Error Reduction of Regional Wind Power Forecast by Integrating Spatio-temporal Information into an Artificial Intelligence Model},
  booktitle = {14\textsuperscript{th} Wind Integration Workshop},
  year      = {2014},
  abstract  = {We present a model for predicting the Germany-
wide wind power production for forecast horizons up to 3
hours. Our method consists of an artificial intelligence (AI)
model combining two different kinds of input data. On the one
hand these are online power productions of the four German
TSO regions, derived by upscaling reference farm
measurements, providing spatially distributed auto-regressive
information. On the other hand, wind power plant forecasts of
68 wind farms spatially distributed over Germany are used.
Additionally, previous values of all of these input data provide
temporal information. To reduce the large number of inputs
from the wind farm forecasts a principal component analysis
is applied before these data enter the AI model.
Results show that combining times series methods and spatio-
temporal information by integrating wind farm forecasts into
an AI model leads not only to an improvement of about 30\%
regarding persistence but also to significantly reduced extreme
errors.

Keywords-component; regional wind power forecast,
upscaling, spatio-temporal information, artificial neural network},
  comment   = {Axel's direct upscaling method.

File came from: K:\Organisation\Kongresse\2014\2014-11_Wind_Integration_Workshop\Axel_Braun},
  file      = {Braun14directUpscaling.pdf:Braun14directUpscaling.pdf:PDF},
  location  = {Berlin},
}

@Article{Ragwitz00nonLinStructWindGust,
  author    = {M. Ragwitz and H. Kantz},
  title     = {Detecting non-linear structure and predicting turbulent gusts in surface wind velocities},
  journal   = {Europhysics Letters (EPL)},
  year      = {2000},
  volume    = {51},
  number    = {6},
  pages     = {595--601},
  abstract  = {We present a new approach, based on a locally low-dimensional prediction scheme, which was originally developed for deterministic chaotic signals, to detect the deterministic structure of local surface wind velocities. By exploiting properties of the signal which mimic the deterministic structure created by non-linear coherent effects of a turbulent fluid and by resolving the strong inherent non-stationarity as a sequence of different dynamical regimes, short time predictions of future wind velocities can be performed. Although on average no reduction of the prediction error can be achieved using a non-linear model instead of a linear stochastic one, intermittent gusts can be predicted with significantly higher accuracy. An asymmetry in the predictability is observed, which suggests that an increase of the velocity is rather deterministic, whereas the subsequent decrease is dominated by a stochastic behaviour.},
  comment   = {delay embedding stuff; use for detecting gusts, regime changes?},
  file      = {Ragwitz00nonLinStructWindGust.pdf:Ragwitz00nonLinStructWindGust.pdf:PDF;Ragwitz00nonLinStructWindGust.pdf:Ragwitz00nonLinStructWindGust.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.02.12},
  url       = {http://stacks.iop.org/0295-5075/51/595},
}

@InProceedings{Klein15dynCNNweathPred,
  author    = {B. Klein and L. Wolf and Y. Afek},
  title     = {A Dynamic Convolutional Layer for short rangeweather prediction},
  booktitle = {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},
  year      = {2015},
  pages     = {4840--4848},
  month     = jun,
  abstract  = {We present a new deep network layer called {\textquotedblleft}Dynamic Convolutional Layer{\textquotedblright} which is a generalization of the convolutional layer. The conventional convolutional layer uses filters that are learned during training and are held constant during testing. In contrast, the dynamic convolutional layer uses filters that will vary from input to input during testing. This is achieved by learning a function that maps the input to the filters. We apply the dynamic convolutional layer to the application of short range weather prediction and show performance improvements compared to other baselines.},
  comment   = {One step ahead (I think) satellite weather image prediction, Convolutional NN filters are learned from data, I think.  Could use for forecasting or upscaling?  Maybe a way to incorporate distant NWP errors into uncertainty (big time steps) or an autoencoder feature learning approach (somehow?)},
  doi       = {10.1109/CVPR.2015.7299117},
  file      = {Klein15dynCNNweathPred.pdf:Klein15dynCNNweathPred.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {geophysics computing, learning (artificial intelligence), neural nets, weather forecasting, deep-network layer, dynamic convolutional layer, filter learning, performance improvement, short-range weather prediction, Computer architecture, Convolutional codes, Image generation, Radar imaging, Training, Weather forecasting},
  owner     = {sotterson},
  timestamp = {2017.03.08},
}

@InProceedings{Gutmann10noiseCntrstStatMdl,
  author    = {Gutmann, Michael and Hyv{\"a}rinen, Aapo},
  title     = {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  year      = {2010},
  pages     = {297--304},
  abstract  = {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance. In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.},
  comment   = {Possibly another way to make a conditional prob. forecast.  Here, the conditionality comes from logistic regression, and the distribution learning comes from trying to distinguish the observed distribution from random noise.

Tons of cites in Google Scholar of this paper, most from speech and language model but there was one for anomaly detection too.},
  file      = {:Gutmann10noiseCntrstStatMdl.pdf:PDF},
  url       = {http://proceedings.mlr.press/v9/gutmann10a.html},
}

@Article{Perez02newMdllSatelliteIrrad,
  author   = {Richard Perez and Pierre Ineichen and Kathy Moore and Marek Kmiecik and Cyril Chain and Ray George and Frank Vignola},
  title    = {A new operational model for satellite-derived irradiances: description and validation},
  journal  = {Solar Energy},
  year     = {2002},
  volume   = {73},
  number   = {5},
  pages    = {307 - 317},
  issn     = {0038-092X},
  abstract = {We present a new simple model capable of exploiting geostationary satellite visible images for the production of site/time-specific global and direct irradiances The new model features new clear sky global and direct irradiance functions, a new cloud-index-to-irradiance index function, and a new global-to-direct-irradiance conversion model. The model can also exploit operationally available snow cover resource data, while deriving local ground specular reflectance characteristics from the stream of incoming satellite data. Validation against 10 US locations representing a wide range of climatic environments indicates that model performance is systematically improved, compared to current visible-channel-based modeling practice.},
  comment  = {Decribes Solar Anywhere satellite calibration, I think, according to Kubiniec19solarSatelliteTuneGrndTR. 

Idea is, I think, that the GHI and DNI at the surface can be calculated using satellite irradiance measured of sunlight bouncing back off the earth.

A bunch of corrections are required.  The main ones are
* how much sunlight would hit the earth w/o clouds (seasonal, daily, spatial geometry, atmospheric turbidity)
* how reflective the earth is -- from earth bound snow measurements and a sliding window dynamic range estimation
* terrain (updated in Perez04satelliteIrradTerrain, I think).
* satelllite dynamic range decay over time and its calibration (I think)
* etc.

Some hacks are used and some geometric equations.

Output is GHI and DNI at each satellite image pixel.

},
  doi      = {https://doi.org/10.1016/S0038-092X(02)00122-6},
  file     = {:Perez02newMdllSatelliteIrrad.pdf:PDF},
  url      = {http://www.sciencedirect.com/science/article/pii/S0038092X02001226},
}

@Article{Maaten08VisualizingDataUsingTSNE,
  author   = {Maaten, Laurens van der and Hinton, Geoffrey},
  title    = {Visualizing data using t-SNE},
  journal  = {Journal of Machine Learning Research},
  year     = {2008},
  volume   = {9},
  number   = {Nov},
  pages    = {2579--2605},
  abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
  comment  = {The dimension reduction algorithm used by the ASML chip layout company that does anomaly detection.  Talked to head of their ML group and he referred me to this (ECML PKDD 2017).  Later in the conference somebody asked the author of Blau17NonRedundSpectDimRed why he'd omitted this technique as a benchmark.; he said that it was mainly used for 2D human interpretation, and that he thought (did not show) that his technqique would actually work better as a classification preprocessing step.},
  file     = {:Maaten08VisualizingDataUsingTSNE.pdf:PDF},
  url      = {http://www.jmlr.org/papers/v9/vandermaaten08a.html},
}

@InCollection{Wang07depSubspaceClust,
  author      = {Wang, Xufei and Li, Chunping},
  title       = {DBSC: A Dependency-Based Subspace Clustering Algorithm for High Dimensional Numerical Datasets},
  booktitle   = {Advances in Artificial Intelligence},
  publisher   = {Springer Berlin / Heidelberg},
  year        = {2007},
  editor      = {Orgun, Mehmet and Thornton, John},
  volume      = {4830},
  series      = {Lecture Notes in Computer Science},
  pages       = {832--837},
  abstract    = {We present a novel algorithm called DBSC, which finds subspace clusters in numerical datasets based on the concept of dependency . This algorithm uses a depth-first search strategy to find out the maximal subspaces: a new dimension is added to current k-subspace and its validity as a (k+1)-subspace is evaluated. The clusters within those maximal subspaces are mined in a similar fashion as maximal subspace mining does. With the experiments on synthetic and real datasets, our algorithm is shown to be both effective and efficient for high dimensional datasets.},
  affiliation = {School of Software,Tsinghua University, China MOE Key Laboratory for Information System Security},
  comment     = {Regime detection by dependence clustering?},
  doi         = {10.1007/978-3-540-76928-6_101},
  file        = {Wang07depSubspaceClust.pdf:Wang07depSubspaceClust.pdf:PDF},
  owner       = {scot},
  timestamp   = {2010.11.08},
}

@Article{Theis12MixCondiGaussScaleMixImg,
  author    = {Theis, Lucas and Hosseini, Reshad and Bethge, Matthias},
  title     = {Mixtures of conditional Gaussian scale mixtures applied to multiscale image representations},
  journal   = {PloS one},
  year      = {2012},
  volume    = {7},
  number    = {7},
  pages     = {e39857},
  abstract  = {We present a probabilistic model for natural images that is based on mixtures of Gaussian scale mixtures and a simple multiscale representation. We show that it is able to generate images with interesting higher-order correlations when trained on natural images or samples from an occlusion-based model. More importantly, our multiscale model allows for a principled evaluation. While it is easy to generate visually appealing images, we demonstrate that our model also yields the best performance reported to date when evaluated with respect to the cross-entropy rate, a measure tightly linked to the average log-likelihood. The ability to quantitatively evaluate our model differentiates it from other multiscale models, for which evaluation of these kinds of measures is usually intractable.},
  comment   = {A Gaussian mixture model with conditional mixture weights as well as covariances.  Seems like it might be good for regime detection or a copula internal probability model (conditional mixture could handle Gaussian extreme shortcomings, and conditional covariance would handle non-stationary covariance (like wind power where wind speed above or below the linear region changes the covariance matrix)).},
  doi       = {https://doi.org/10.1371/journal.pone.0039857},
  file      = {:Theis12MixCondiGaussScaleMixImg.pdf:PDF},
  publisher = {Public Library of Science},
  url       = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0039857},
}

@Article{Brown12condLikInfoFeatSel,
  author    = {Brown, Gavin and Pocock, Adam and Zhao, Ming-Jie and Luj{\'a}n, Mikel},
  title     = {Conditional likelihood maximisation: a unifying framework for information theoretic feature selection},
  journal   = {The Journal of Machine Learning Research},
  year      = {2012},
  volume    = {13},
  number    = {1},
  pages     = {27--66},
  abstract  = {We present a unifying framework for information theoretic feature selection, bringing almost two decades of research on heuristic filter criteria under a single theoretical interpretation. This is in response to the question: "what are the implicit statistical assumptions of feature selection criteria based on mutual information?". To answer this, we adopt a different strategy than is usual in the feature selection literature?instead of trying to define a criterion, we derive one, directly from a clearly specified objective function: the conditional likelihood of the training labels. While many hand-designed heuristic criteria try to optimize a definition of feature 'relevancy' and 'redundancy', our approach leads to a probabilistic framework which naturally incorporates these concepts. As a result we can unify the numerous criteria published over the last two decades, and show them to be low-order approximations to the exact (but intractable) optimisation problem. The primary contribution is to show that common heuristics for information based feature selection (including Markov Blanket algorithms as a special case) are approximate iterative maximisers of the conditional likelihood. A large empirical study provides strong evidence to favour certain classes of criteria, in particular those that balance the relative size of the relevancy/redundancy terms. Overall we conclude that the JMI criterion (Yang and Moody, 1999; Meyer et al., 2008) provides the best tradeoff in terms of accuracy, stability, and flexibility with small data samples.},
  comment   = {Unifying paper on information theoretic feature selection, including Peng MI. Has Matlab (toolbox has many MI estimators)
.  All Infotheoretic heuristic algorithms are doing an approximate search for conditional likelihood.  The best heuristic is joint mutual information (JMI), but this doesn't require high dim MI, so only uses one-to-one order relationships.

Joint Mutual Information (JMI)
* This is not conditional MI or partial MI, which algorithms like Frenzel07partMutInfo and later use
* it's JMI(X_k) = SUM_j(X_k, X_j ;  Y), where
 -  j indices all the previously selected features
 = X_k, X_j  are features j and k
* As noted by Vergara14revFeatSelMutInfo, it doesn't require high dim MI

According to Vergara14revFeatSelMutInfo, this paper:
"fell short of deriving (explaining) non-linear criteria using
min or max operators such as Conditional Mutual Information Maximization
(CMIM) [21], Informative Fragments [61], and ICAP [29]."

* generalized heurstics in peng05featSelMutInfo (speakerclust.bib)
Matlab: MIToobox for C and MATLAB (for =discrete= inputs, classification)
http://www.cs.man.ac.uk/~pococka4/MIToolbox.html},
  file      = {Brown12condLikInfoFeatSel.pdf:Brown12condLikInfoFeatSel.pdf:PDF},
  owner     = {sotterson},
  publisher = {JMLR. org},
  timestamp = {2015.03.27},
  url       = {http://www.jmlr.org/papers/v13/brown12a.html},
}

@InProceedings{Braun13windSolErrDep,
  author    = {Axel Braun and Rafael Fritz and Scott Otterson},
  title     = {Are Forecasting Errors of Wind and Solar Power Statistically Dependent?},
  booktitle = {International Workshop on Large Scale Integration of Wind Power into Power Systems as well as on Transmission Networks for Offshore Wind Power Plants},
  year      = {2013},
  month     = oct,
  abstract  = {We present an advanced procedure of testing
statistical independence of two time series. In particular, we
want to answer the question whether wind and solar power
forecast errors are statistically dependent. The procedure uses
mutual information as the measure of dependence and
calculates a distribution of subsample estimates of it, using the
non-parametric method of subsampling. Testing on
independence is done by a permutation test using Bayes rule
as the criterion of significant dependencies. The investigation
uses day ahead forecasts of Germany, german TSO regions
and the administrative region Harz in Germany with a
temporal resolution of 15 minutes. Various time lags between
both error time series are analyzed. We do not find any
statistical dependence which negotiates the question of the
need of a joint error forecast, at least for the data tested. We
discuss the dependence of our procedure on breaking or
preserving serial dependence in each of both error time series.
(Abstract)
Keywords-component; joint probabilistic forecast of wind
and solar errors; forecast errors; mutual dependence of time
series; mutual information; hypothesis test;},
  comment   = {Wind and PV forecast errors were not found to have significant correlation in Germany. Questionable: Tthis is German-wide, I think, and for day-ahead. It seems like the story might be different for other test case choices.

Also, Axel's subsampling bootstrap algorithm is suspect. He sorta did what I did, but I'm not confident e.g. he only contiguous-chunk subsampling instead of random. This would help remove autocorrelation, but I think we want to include that, in this case.},
  file      = {Paper:Braun13windSolErrDep.pdf:PDF;Workshop Slides:Braun13windSolErrDep_Slides.pptx:PowerPoint 2007+},
  groups    = {ErrDistProps, doReadWPV_2},
  location  = {London, UK},
  owner     = {sotterson},
  timestamp = {2013.10.15},
  url       = {http://www.windintegrationworkshop.org/},
}

@InProceedings{Pellet07parCorrCausal,
  author    = {Jean-Philippe Pellet and Andr{\'e} Elisseeff},
  title     = {A Partial Correlation-Based Algorithm for Causal Structure Discovery with Continuous Variables},
  booktitle = {Intelligent Data Analysis (IDA)},
  year      = {2007},
  pages     = {229--239},
  abstract  = {We present an algorithm for causal structure discovery suited in the presence of continuous variables. We test a version based on partial correlation that is able to recover the structure of a recursive linear equations model and compare it to the well-known PC algorithm on large networks. PC is generally outperformed in run time and number of structural errors.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comment   = {modified partial correlation works better than the one in BNT package for matlab - edges drawn when parcorr signficant, w/ network constraints - significance better than usual T-test b/c check it repeatedly, as the network grows, which adds more variables to the residual test, making it harder to be significant video lecture here: http://videolectures.net/ida07_pellet_apcba/},
  doi       = {10.1007/978-3-540-74825-0_21},
  file      = {Pellet07parCorrCausal.pdf:Pellet07parCorrCausal.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.03.06},
}

@InProceedings{Meyers18statClearSkyFit,
  author    = {Bennet Meyers and Michaelangelo Tabone and Emre Can Kara},
  title     = {Statistical Clear Sky Fitting Algorithm},
  booktitle = {World Conference on Photovoltaic Energy Conversion (WCPEC-7)},
  year      = {2018},
  address   = {WAIKOLOA, HAWAII},
  month     = jun,
  abstract  = {We present an algorithm that estimates a clear sky
performance signal from the measured power of a PV system.
The algorithm uses only observed power output, and assumes no
knowledge of weather, irradiance data, or system configuration
metadata. This is a novel approach to understanding the clear
sky behavior of an installed PV system, that does not rely on
traditional atmospheric and geometric modeling techniques.
Index Terms—photovoltaic systems, performance analysis,
clear sky, data analysis, data-driven modeling, generalized low
rank models, singular value decomposition, signal processing,
convex optimization},
  comment   = {Related to solaranywhere calibration.  From Patrick},
  file      = {:Meyers18statClearSkyFit.pdf:PDF},
  url       = {http://www.ieee-pvsc.org/WCPEC-7/},
}

@Article{Victor02binlessInfo,
  author    = {Victor, Jonathan D.},
  title     = {Binless strategies for estimation of information from neural data},
  journal   = {Physical Review E},
  year      = {2002},
  volume    = {66},
  number    = {5},
  pages     = {051903},
  month     = nov,
  abstract  = {We present an approach to estimate information carried by experimentally observed neural spike trains elicited by known stimuli. This approach makes use of an embedding of the observed spike trains into a set of vector spaces, and entropy estimates based on the nearest-neighbor Euclidean distances within these vector spaces at L. F. Kozachenko and N. N. Leonenko, Probl. Peredachi Inf. 23, 9 ~1987!\#. Using numerical examples, we show that this approach can be dramatically more efficient than standard bin-based approaches such as the ??direct??method at S. P. Strong, R. Koberle, R. R. de Ruyter van Steveninck, and W. Bialek, Phys. Rev. Lett. 80, 197 ~1998!\# for amounts of data typically available from laboratory experiments.},
  comment   = {Explains KNN entropy estimation but using only one nearest neighbor and in Euclidean instead of maximum-norm space.  Derivation may help in understanding  Kraskov04EstMutInfKNN or Kraskov04syncIndepMeasMIphd?},
  doi       = {10.1103/PhysRevE.66.051903},
  file      = {Victor02binlessInfo.pdf:Victor02binlessInfo.pdf:PDF},
  numpages  = {15},
  owner     = {scot},
  publisher = {American Physical Society},
  timestamp = {2011.06.01},
  url       = {http://pre.aps.org.globalproxy.cvt.dk/abstract/PRE/v66/i5/e051903},
}

@Article{Dhillon08featSelFeatClass,
  author    = {Dhillon, Paramveer S. and Foster, Dean and Ungar, Lyle H.},
  title     = {Efficient Feature Selection in the Presence of Multiple Feature Classes},
  journal   = {International Conference on Data Mining (ICDM)},
  year      = {2008},
  pages     = {779--784},
  month     = dec,
  issn      = {1550-4786},
  abstract  = {We present an information theoretic approach to feature selection when the data possesses feature classes. Feature classes are pervasive in real data. For example, in gene expression data, the genes which serve as features may be divided into classes based on their membership in gene families or pathways. When doing word sense disambiguation or named entity extraction, features fall into classes including adjacent words, their parts of speech, and the topic and venue of the document the word is in. When predictive features occur predominantly in a small number of feature classes, our information theoretic approach significantly improves feature selection. Experiments on real and synthetic data demonstrate substantial improvement in predictive accuracy over the standard $L_0$ penalty-based stepwise and stream wise feature selection methods as well as over Lasso and Elastic Nets, all of which are oblivious to the existence of feature classes.},
  comment   = {MDL coding for feature selection when features come in groups Use for offsite obs lag selection?
* "class" is the met tower
* "feature" in that classs is the data at a lag for that met tower

* This may encourage a feature selection algorithm to select fewer lags at one tower and explore others
-- but would a total information gain or redundancy filter work as well?},
  doi       = {10.1109/ICDM.2008.56},
  file      = {Dhillon08featSelFeatClass.pdf:Dhillon08featSelFeatClass.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.03.03},
}

@InCollection{Zliobaite09timSpcConcpDrft,
  author      = {{\v{Z}}liobait{\.e}, I.},
  title       = {Combining Time and Space Similarity for Small Size Learning under Concept Drift},
  booktitle   = {Foundations of Intelligent Systems},
  publisher   = {Springer Berlin / Heidelberg},
  year        = {2009},
  editor      = {Rauch, Jan and Ras, Zbigniew and Berka, Petr and Elomaa, Tapio},
  volume      = {5722},
  series      = {Lecture Notes in Computer Science},
  pages       = {412--421},
  abstract    = {We present concept drift responsive method for classifier training for sequential data. Relevant instance selection for training is based on similarity to the target observation. Similarity in space and in time is combined. The algorithm determines an optimal training set size. It can be used plugging in different base classifiers. The proposed algorithm shows the best accuracy in the peer group. The algorithm complexity is reasonable for the field applications.},
  affiliation = {Vilnius University Faculty of Mathematics and Informatics Naugarduko st. 24 LT-03225 Vilnius Lithuania},
  comment     = {Very simple way to pick training samples for online nonstationary training. Use for adaptation and/or regime learning.
* application is classification but seems convertible to continous regression
* training window length is chosen adaptivel
* two hyper parameters
* idea: don't use euclid dist but Mahalonibus or Gaussian or GMM lik dist; use BIC or something to test sample inclusion against null hypothesis that a sample doesn't belong to same regime.

Not sure how 1\textsuperscript{st} regime split would happen. Probably need to merge regimes too, w/ temporal constraints, just like speaker clustering. Seems like I just found a paper like this w/ gaussians. Look in nearby (in time) bibfile entries)

Could use this for picking ensemble forecast analogs, somehow?},
  doi         = {10.1007/978-3-642-04125-9_44},
  file        = {Zliobaite09timSpcConcpDrft.pdf:Zliobaite09timSpcConcpDrft.pdf:PDF},
  groups      = {Ensemble, doReadNonWPV_1},
  owner       = {scot},
  timestamp   = {2010.09.28},
}

@InCollection{Pal10entMIestGNNgraph,
  author    = {David Pal and Barnabas Poczos and Csaba Szepesvari},
  title     = {Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs},
  booktitle = {Advances in Neural Information Processing Systems 23},
  year      = {2010},
  editor    = {J. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R.S. Zemel and A. Culotta},
  pages     = {1849--1857},
  abstract  = {We present simple and computationally ef?cient nonparametric estimators of Renyi entropy and mutual information based on an i.i.d. sample drawn from an ? unknown, absolutely continuous distribution over Rd. The estimators are calculated as the sum of p-th powers of the Euclidean lengths of the edges of the ?generalized nearest-neighbor? graph of the sample and the empirical copula of the sample respectively. For the ?rst time, we prove the almost sure consistency of these estimators and upper bounds on their rates of convergence, the latter of which under the assumption that the density underlying the sample is Lipschitz continuous. Experiments demonstrate their usefulness in independent subspace analysis},
  comment   = {MI and Renyi est w/ copula. Nice b/c they have a theoretical upper bound. could be used for comparing different MI confidence intervals?},
  file      = {Pal10entMIestGNNgraph.pdf:Pal10entMIestGNNgraph.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2011.11.30},
}

@INPROCEEDINGS{Nielsen07improveAutoShortWind,
  Author                   = {Henrik Aalborg Nielsen and Pierre Pinson and Lasse Engbo Christiansen and Torben Skov Nielsen and Henrik Madsen and Jake Badger and Gregor Giebel and Hans F. Ravn},
  Title                    = {Improvement and automation of tools for short term wind power forecasting},
  Booktitle                = {European Wind Energy Conference and Exhibition (EWEC)},
  Year                     = {2007},
  Abstract                 = {We present the results from an on-going project financed by the Danish PSO-fund where a number of subjects relevant for further automation and improvement of short term wind power forecasts methods are studied. The technological basis of the project is adaptive forecast methods as the methods forming the basis of WPPT (Wind Power Prediction Tool) - a well proven system for wind power forecasting. In the projectwe investigate (i) initialization of the self-calibrating models, (ii) automatic selection of tuning parameters, (iii) robust estimation, and (iv) combination of several meteorological forecasts. In the paper we present why these aspects are considered important for further development of forecast systems. Following this we present the investigations performed and outline the solutions which follow from these investigations.},
  File                     = {:Nielsen07improveAutoShortWind.pdf:PDF;Nielsen07improveAutoShortWind.pdf:Nielsen07improveAutoShortWind.pdf:PDF},
  Owner                    = {scotto},
  Timestamp                = {2008.07.06}
}

@InProceedings{Baird05oneStepInvrtNN,
  author    = {Baird, L. and Smalenberger, D. and Ingkiriwang, S.},
  title     = {One-step neural network inversion with {PDF} learning and emulation},
  booktitle = {Neural Networks, 2005. IJCNN '05. Proceedings. 2005 IEEE International Joint Conference on},
  year      = {2005},
  volume    = {2},
  pages     = {966--971},
  abstract  = {We present two new types of neural networks (both of which can be trained with ordinary error backpropagation) and we present a new algorithm for learning a probability density function (pdf) from example vectors. It is normally difficult to invert a neural network, but for the new bijective neural network, it is efficient to find an input producing any desired output, and such an input is guaranteed to exist and to be unique. Furthermore, it can be used as one component in building a pdf neural network, which is a neural network with a nonnegative output, and for which it is guaranteed that the integral of the output is exactly 1.0 (as in a pdf function). Both of these can be used for supervised learning using standard error backpropagation. Finally, the new pdf learning algorithm is capable of using those networks to learn a pdf given i.i.d. samples drawn from that pdf, and to then generate new vectors from the learned pdf. This, in turn, allows inversion of a function with non-unique inverses, where each inverse is generated with just a single evaluation of the network.},
  comment   = {An invertible NN that can be used to learn a PDF. Could this be used in distribution regression instead of the logit function? It could model fancier regression relationships and maybe would make it easier to switch between cdf, pdf and quantiles?

* another invertible NN: Rippel13highDimProbDeepNN

Is a CPU hog: has a complexity for computing the Jacobian determinant that
scales as O(LD^3), where D is the dimension of the hidden layers and L is the number of hidden layers used
(from: http://arxiv.org/pdf/1505.05770v4.pdf)

* distribution regression e.g. Koenker13DistributionalvsQuantile},
  doi       = {10.1109/IJCNN.2005.1555983},
  file      = {Baird05oneStepInvrtNN.pdf:Baird05oneStepInvrtNN.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2015.07.11},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1555983},
}

@InProceedings{Dienst16AnomOffshrWnd,
  author    = {Dienst, Steffen and Beseler, Jonas},
  title     = {Automatic Anomaly Detection in Offshore Wind {SCADA} Data},
  booktitle = {WindEurope},
  year      = {2016},
  month     = {09},
  abstract  = {We propose a computationally simple anomaly de-
tection method that assists operators of offshore wind parks with
monitoring the operation of wind turbines. Previously published
methods focus on creating high quality predictive models for
specific physical operational aspects of turbines, like power
production or temperatures of the gearbox in relation to wind
speed and other exogenous factors. SCADA systems of wind
turbines typically provide many more sensor data time series
than are being used for monitoring purposes. We show how
this data can be used to automatically learn a large number of
simple models that in sum can alert the operator about a variety
of potentially defect related changes in different components.
A number of different insights applicable to similar problems
are provided in the conclusions. The system was developed and
applied in an offshore wind park and is used to support predictive
maintenance.},
  comment   = {It's one of the Anomaly Detection studies by Global Tech and Uni Leipzig (ModernWindABS project).   LASSO-based but Ide09ProxAnomDetSprs says structure learned by straight lasso is unstable.
  Belloni13progEvalHiDim says that inference and featsel is hard, error prone.

Seems like it's possible to improve upon this work.

Has paper, slides (globaltech), slides2 (uLeipzig) and poster.

I haven't read slides2, looks like there are more useful details there.

DATA
* on 4 months of data from 80 5MW turbines in N. Sea)
   - so short that they had test/train mismatch (winter/summer temps)
* system parameters were reset during train and test, causing relationships to change
* sample period is 10min (10ms if some kind of fault mode)
* 313 sensors/turinbe, w/ min/max/mean/std:  dim=1252
* they also make many lags of the values (don't say how many)
  - would a distributed lag model be better?
  - NOTE: lags all look into past, looking, say 30 minutes into future would also be useful!
* 63 of these sensors didn't change at all during the 4 months of training data
* Missing/changing data a big problem
 - 1-2% of sensors broken at any given time
 - counters are reset, no notice given, apparently
* redundant components switch which one is operating, no notice apparently

STATED MODEL GOALS
* don't want per-sensor threshold
 - they're really looking for time derivatives of things
 - joint behavior too, I think
* are not modeling vibration
   -  say that's already done by other sytems
   -  but maybe joint behavior is interesting?
* says speed for operator interaction is important, NN's or SVM's too slow
  - really?  Just get another CPU?
  - extreme learning machines would be nearly as fast
* should be mostly unsupervized
* should add onto currently detectable fault modes
* requires minimum knowledge of turbine
* don't want to do power curves (but shouldn't you?)

MODEL THEY BUILT
* LASSO linear regression predicts each sensor based on other sensors
* if prediction is bad, then determine root cause
* makes multiple (2-3) lasso models per measurement, apparently trying to predict it using the other measurements
   - 750 models/turbine
   -  new modes generated iteratively by training, removing best feature, training again until no improvement
   - NOTE: LASSO will remove correlated inputs, so this has the effect of allowing each correlated input to be involved in some model
   - NOTE: LASSO of some type is unstable but a structural LASSO is better (Ide09ProxAnomDetSprs)
* heuristic (or threshold) detects anomaly when the prediction doesn't fit
, based on operator-set level (each sensor?)
* extremely fast computation
   - learned 750 models (one turbine) over 4 months of data in only a minute
   - claim that this is important, allowing operator to doe something, I don't know what, interactively

ROOT CAUSE DETECTOR THEY BUILT
* Two heuristic rules predict most probable cause of anomaly
  1.) If value is stuck for some time period, then it's a bad sensor
   2.) if an input is correlated w/ output prediction error, then that input is bad
* Probable Cause is a thresholded count of the number fo times the models pick a sensor as a cause

USER INTERFACE
* built a graphical display showing turbine status in realtime
* data-link to the University, I think

RESULTS
* Does detect some faults, one 2 days sooner than existing SCADA thresholds.
* precision/recall couldn't be calculated b/c was severity of fault was operator subjective, unlabeled?
* majorit of anomalies were due to sensor defects
* test/train mismatch was a problem e.g. summer vs. winter temps == adaptation will be imp.

POSSIBLE IMPROVEMENTS
* replace all this with probabilistic models (probability of an output, or set of outputs, being X)
   - eg. based on quantile regression
   - could do copula so have a joint model of everything (pair copula)
      - copula covariance shrinkage could be kind of like lasso
      - vine copula for multivariate
      - there are many mixed integer clustering algorithms, producing a joint model whose proability could be assessed
*  Do nonlinear effects:
    - extreme learning machine would be fairly fast, yet linear
    - even faster would be to rank-transform variables first (like trend SVD analysis (Li11trendSVD))
* Root cause detection could be done by marginalizing or something with a graphical model
  - see Ide09ProxAnomDetSprs for one graphical model
* could use MI instead of correlation for nonlinear relationships (if have new, nonlinear model)
  - use PMI so can detect more than one input connected to a fault
* auto detection of counter reset
* auto detection of redundant, switching components
   - authors want to add them (e.g. add two redundant water pumps that are never both turned on)
   - but what if they are?
   - maybe better to take a missing feature approach and predict one with the other, the fill whichever is missing
     - would need training data w/ both in action
     - would need a "is turned off" missing feature detector/designator
* linear models should at least have interaction terms (or just use ELM)
* skip multiple model learning?  This is mainly dealing w/ LASSO removing correlated inputs
* Adaptation
  - summer/winter/weather
  - they say that operators change parameters all the time, and this changes the system state
    ==> will need some way to "cold start" the model
* Review the "lessons learned" part of the paper to see what else they found out
* Lag structure
   - future as well as past lags (they only do past)
   - use distributed lag model
   - a way that decomposes inputs into slope and level
   - maybe jointly, across inputs, w/ multilinear PCA or something
* add probabilistic power curve},
  file      = {Paper:Dienst16AnomOffshrWnd.pdf:PDF;Slides (GlobalTech):Dienst16AnomOffshrWnd_slides.pdf:PDF;Slides 2 (U. of Leipzig):Dienst16AnomOffshrWnd_slides2.pdf:PDF;Poster:Dienst16AnomOffshrWnd_poster.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2016.12.07},
  url       = {https://windeurope.org/summit2016/},
}

@Article{Xiao13FastbivarPsplinesSndwich,
  author    = {Xiao, Luo and Li, Yingxing and Ruppert, David},
  title     = {Fast bivariate P-splines: the sandwich smoother},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year      = {2013},
  volume    = {75},
  number    = {3},
  pages     = {577--599},
  issn      = {1467-9868},
  abstract  = {We propose a fast penalized spline method for bivariate smoothing. Univariate P-spline smoothers are applied simultaneously along both co-ordinates. The new smoother has a sandwich form which suggested the name sandwich smoother to a referee. The sandwich smoother has a tensor product structure that simplifies an asymptotic analysis and it can be fast computed. We derive a local central limit theorem for the sandwich smoother, with simple expressions for the asymptotic bias and variance, by showing that the sandwich smoother is asymptotically equivalent to a bivariate kernel regression estimator with a product kernel. As far as we are aware, this is the first central limit theorem for a bivariate spline estimator of any type. Our simulation study shows that the sandwich smoother is orders of magnitude faster to compute than other bivariate spline smoothers, even when the latter are computed by using a fast generalized linear array model algorithm, and comparable with them in terms of mean integrated squared errors. We extend the sandwich smoother to array data of higher dimensions, where a generalized linear array model algorithm improves the computational speed of the sandwich smoother. One important application of the sandwich smoother is to estimate covariance functions in functional data analysis. In this application, our numerical results show that the sandwich smoother is orders of magnitude faster than local linear regression. The speed of the sandwich formula is important because functional data sets are becoming quite large.},
  comment   = {A fast way of computing high dimensional p-splines, which according to this article, are supplanting thin plate spline because they have few knots and are faster to compute. Has R library.},
  doi       = {10.1111/rssb.12007},
  file      = {Xiao13FastbivarPsplinesSndwich.pdf:Xiao13FastbivarPsplinesSndwich.pdf:PDF},
  keywords  = {Asymptotics, Bivariate smoothing, Covariance function, Generalized linear array model, Non-parametric regression, Penalized splines, Sandwich smoother, Thin plate splines},
  owner     = {sotterson},
  timestamp = {2014.11.07},
}

@Article{Villani12GenSmthFinMix,
  author    = {Villani, Mattias and Kohn, Robert and Nott, David J},
  title     = {Generalized smooth finite mixtures},
  journal   = {Journal of Econometrics},
  year      = {2012},
  volume    = {171},
  number    = {2},
  pages     = {121--133},
  abstract  = {We propose a general class of models and a
 unified Bayesian inference method- ology for flexibly
 estimating the density of a response variable conditional
 on a possibly high-dimensional set of covariates. Our
 model is a finite mixture of component models with
 covariate-dependent mixing weights. The component
 densities can belong to any parametric family, with each
 model parameter being a deterministic function of
 covariates through a link function. Our MCMC methodology
 allows for Bayesian variable selection among the covari-
 ates in the mixture components and in the mixing
 weights. The model's parameterization and variable
 selection prior are chosen to prevent overfitting. We use
 simulated and real data sets to illustrate the
 methodology. Keywords: Bayesian inference, Conditional
 distribution, GLM, Markov Chain Monte Carlo, Mixture of
 Experts, Variable selection.},
  comment   = {Modeling a density conditioned upon a high dimensional input set. Includes smooth mixture of beta distributions, so can be bounded. Good for wind power estimation?. Is MCMC/Bayesian based, has feature selection, related to boosting approaches?},
  file      = {Villani12GenSmthFinMix.pdf:Villani12GenSmthFinMix.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.07.18},
  url       = {http://www.sciencedirect.com/science/article/pii/S0304407612001595},
}

@Unpublished{Haywood08multiStepForecastImprvTst,
  author    = {Dr John Haywood and G Tunnicliffe Wilson},
  title     = {Improved multi-step forecasting using a new statistical test},
  month     = mar,
  year      = {2008},
  abstract  = {We propose a general test of whether a time series model, with parameters es- timated by minimising the single-step forecast error sum of squares, is robust with respect to multi-step prediction, for some specified lead-time. The test may be applied to a, possibly seasonal, ARIMA model using the parameters and residuals following maximum likelihood estimation. It is based on a score statistic, evaluated at these estimated parameters, that measures the sensitivity of the multi-step forecast error variance with respect to the parameters. We show that the test has acceptable size properties for higher lead times, when applied to the exponentially weighted moving average predictor, and investigate how its power varies with the lead time, under the simple ARMA(1,1) alternative. We also demonstrate the high power of the test when it is applied to a process generated as the sum of a stochastic trend and cycle plus noise, which has been modelled by an autoregression. We use frequency domain meth- ods for the derivation and sampling properties of the test, and to give insight into its application. The test is illustrated on two real series, and a function for its general application is available from the authors.},
  comment   = {When many single steps ahead is better than training a single step predictor for correct lookahead time. AR, ARIMA or ARMA specific?
* model optimized by sum of squares.},
  file      = {Haywood08multiStepForecastImprvTst.pdf:Haywood08multiStepForecastImprvTst.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.09.29},
  url       = {http://www.mcs.vuw.ac.nz/Main/PublicationsDatabase?rm=details&id=980054958&authorString=John%20Haywood&author=OTHER&titleString=&subject=9999&rows=10&type=9999&year=9999&page=},
}

@InProceedings{Haywood05multiStepTest,
  author    = {Dr John Haywood},
  title     = {A test for improved multi-step forecasting},
  booktitle = {New Zealand econometrics study group meeting},
  year      = {2005},
  abstract  = {We propose a general test of whether a time series model, with parameters estimated by minimising the single-step forecast error sum of squares, is robust with respect to multi-step estimation, for some specied lead-time. The test statistic is based on a score function, defined as the derivative of the multi-step forecast error variance with respect to the model parameters, and evaluated at parameters estimated using the single-step criterion. We show that the test has acceptable size properties for higher lead times, when applied to the exponentially weighted moving average predictor, and investigate how its power varies with the lead time, under the simple ARMA(1,1) alternative. We also demonstrate the high power of the test when it is applied to a process generated as the sum of a stochastic trend and cycle plus noise, which has been modelled by a high order autoregression. We use frequency domain methods which give insight into the derivation and sampling properties of the test, but note that the test statistic may be expressed as a quadratic form in the residual sample autocorrelations. The test is illustrated on two real time series, which demonstrate its wide applicability.},
  comment   = {General test for when can do multi-step forecasting (w/ model trained on single steps)
* also applicable to Gaussian Processes? Mentions them},
  file      = {Haywood05multiStepTest.pdf:Haywood05multiStepTest.pdf:PDF;Haywood05multiStepTest.pdf:Haywood05multiStepTest.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.09.29},
  url       = {http://www.mcs.vuw.ac.nz/Main/PublicationsDatabase?rm=details&id=980042622&authorString=John%20Haywood&author=OTHER&titleString=&subject=9999&rows=10&type=9999&year=9999&page=2},
}

@Article{Tibshirani01numClustGap,
  author    = {Tibshirani, Robert and Walther, Guenther and Hastie, Trevor},
  title     = {Estimating the number of clusters in a data set via the gap statistic},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year      = {2001},
  volume    = {63},
  number    = {2},
  pages     = {411--423},
  issn      = {1467-9868},
  abstract  = {We propose a method (the "gap statistic") for estimating the number of clusters (groups) in a set of data. The technique uses the output of any clustering algorithm (e.g. K-means or hierarchical), comparing the change in within-cluster dispersion with that expected under an appropriate reference null distribution. Some theory is developed for the proposal and a simulation study shows that the gap statistic usually outperforms other methods that have been proposed in the literature.},
  comment   = {Highly cited num. clusters method that seems to have spawned other methods and which comes out well in comparison studies like this one: http://pami.uwaterloo.ca/groups/ras/PAMI_presentation.pdf},
  doi       = {10.1111/1467-9868.00293},
  file      = {Tibshirani01numClustGap.pdf:Tibshirani01numClustGap.pdf:PDF},
  keywords  = {Clustering, Groups, Hierarchy, Uniform distribution},
  owner     = {sotterson},
  publisher = {Blackwell Publishers Ltd.},
  timestamp = {2011.11.17},
}

@Article{Gneiting11compFrcstWgtQntSc,
  author    = {Gneiting, Tilmann and Ranjan, Roopesh},
  title     = {Comparing density forecasts using threshold-and quantile-weighted scoring rules},
  journal   = {Journal of Business \& Economic Statistics},
  year      = {2011},
  volume    = {29},
  number    = {3},
  abstract  = {We propose a method for comparing density forecasts that is based on weighted versions of the continuous
ranked probability score. The weighting emphasizes regions of interest, such as the tails or the center
of a variable?s range, while retaining propriety, as opposed to a recently developed weighted likelihood
ratio test, which can be hedged. Threshold- and quantile-based decompositions of the continuous ranked
probability score can be illustrated graphically and provide insight into the strengths and deficiencies of a
forecasting method.We illustrate the use of the test and graphical tools in case studies on the Bank of England?s
density forecasts of quarterly inflation rates in the United Kingdom, and probabilistic predictions
of wind resources in the Pacific Northwest.
KEY WORDS: Continuous ranked probability score; Predictive ability testing; Probabilistic forecast;
Proper scoring rule; Quantile; Weighted likelihood ratio test.},
  comment   = {How to compare density forecasts with emphasis on tails or whatever while still being proper. Also, CRPS is just the quantile scores at alphas, integrated across those alphas (alpha a.ka. tau a.k.a. "quantile probability"). Useful for reserve planning forecast evaluation, I guess.

* Eq 22  says CRPS is Contin. Integral of quantile scores over probs [0,1]
   - equally spaced quantiles: multiply QS sum by delta_alpha (a.k.a. delta  "tau")  or Newton-Cotes 
    - for unequally spaced quantiles, you'd 
       - individually multiply each QS by its delta_alpha, or 
       - better: trapezpoidal integration (matlab trapz)
       - or maybe better still: spline fit integrals, or whatever.
 * Eq (13) in the tech note says pretty much the same thing
  ALMOST the same as the usual QR loss function (Koenker05QuantRgrssnBook, eq. (1.9)). When I convert (1.9) I I don't get the multiplication by two.
* agrees more or less with Broecker12rawEnsQntScrCRPS
* CRPS is in same units as the observations
* Note that Argonne paper Mendes11statWindFrcst had to kind of punt on evaluating their density forecasts for different applications. Maybe this is the solution.
* Comparison of quantile score and expectile score: Ehm16quantileExpectileScrFrcst
},
  doi       = {10.1198/jbes.2010.08110},
  file      = {Paper 2011:Gneiting11compFrcstWgtQntSc.pdf:PDF;Tech Note 2008:Gneiting11compFrcstWgtQntSc_TechNote.pdf:PDF},
  groups    = {Read, Test, doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2014.05.26},
}

@Article{Moller12multiCopulaBMA,
  author    = {M{\"o}ller, Annette and Lenkoski, Alex and Thorarinsdottir, Thordis L},
  title     = {Multivariate probabilistic forecasting using ensemble {Bayes}ian model averaging and copulas},
  journal   = {Quarterly Journal of the Royal Meteorological Society},
  year      = {2012},
  abstract  = {We propose a method for post-processing an ensemble of multivariate forecasts in order to
obtain a joint predictive distribution of weather. Our method utilizes existing univariate postprocessing
techniques, in this case ensemble Bayesian model averaging (BMA), to obtain estimated
marginal distributions. However, implementing these methods individually offers no
information regarding the joint distribution. To correct this, we propose the use of a Gaussian
copula, which offers a simple procedure for recovering the dependence that is lost in the estimation
of the ensemble BMA marginals. Our method is applied to 48-h forecasts of a set of
five weather quantities using the 8-member University of Washington mesoscale ensemble. We
show that our method recovers many well-understood dependencies between weather quantities
and subsequently improves calibration and sharpness over both the raw ensemble and a
method which does not incorporate joint distributional information.},
  comment   = {Wind+PV+demand forecasts, upscaling, multiturbine, etc.},
  file      = {Moller12multiCopulaBMA.pdf:Moller12multiCopulaBMA.pdf:PDF},
  groups    = {Ensemble, CitaviImport1, doReadWPV_2},
  ncite     = {7},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2013.09.26},
}

@Article{Frenzel07partMutInfo,
  author               = {Stefan Frenzel and Bernd Pompe},
  title                = {Partial Mutual Information for Coupling Analysis of Multivariate Time Series},
  journal              = {Physical Review Letters},
  year                 = {2007},
  volume               = {99},
  number               = {20},
  abstract             = {We propose a method to discover couplings in multivariate time series, based on partial mutual information, an information-theoretic generalization of partial correlation. It represents the part of mutual information of two random quantities that is not contained in a third one. By suitable choice of the latter, we can differentiate between direct and indirect interactions and derive an appropriate graphical model. An efficient estimator for partial mutual information is presented as well.},
  citeulike-article-id = {2189735},
  comment              = {Feature selection for building an autoregressive, multivariate signal model.  Uses partial correlation generalized to mutual information (partial mutual information, or PMI) handles dependence at different lags. Is a modification of Kraskov04EstMutInfKNN
* This paper says PMI is not the same thing as Conditional Mutual Information although the terms are often conflated (and later authors keep conflating them)
* CMI/PMI classification featsel, maybe the first example: Fleuret04binFeatSelCMI

"kNN based PMI: (-) does not have a stopping criteria (+) lower complexity"
(says: http://stats.stackexchange.com/questions/35089/feature-selection-using-mutual-information-in-matlab)
"lower complexity" than May08varSelpartMutInfo
Note that could add boostrap stop criteria, as in May11revVarSelNN PMI algorithm but the extra MI evals would cost something.

I don't understand the algorithm, which is explained in the author's Diploma, which isn't online anywhere.

* Example shows that pairwise MI can't determine dependency structure that PMI can ID
* the technique used here may also be explained in Vlachos10nonUnifStSpcMI
, which has two derivations
* implemented in Java w/ Matlab/Python/... hooks Lizier14JIDTinfoToolkit
* C++ implentation (TIM library) which has a matlab interface
http://www.cs.tut.fi/~timhome/tim/tim/core/partial_mutual_information.htm
* applied to feature selection here: May08varSelpartMutInfo
* is said to be an improvement on a conditional mutual information approx in Kraskov04 (in Lizier14JIDTinfoToolkit_SupMat).  But this paper (Frenzel*) says "partial mutual information"?
* partial MI also has a bias/variance tradeoff w.r.t. k, just like Kraskov MI
* Ref (or note) [6] says PMI is not the same as Conditional MI
* Partial MI is used for featsel in many papers e.g. May08varSelpartMutInfo,
* FaultMap uses it? https://github.com/SimonStreicher/FaultMap (at least it uses JIDK)
* PMI bias/variance vs. k (fig 2)
 - 6d correlated Gaussian, 1000 points
 - bias ~ 0 for k<=4; noisy and pos until k=16, near zero from 15-58, then drops a little big negative and constant up to k=100
 - variance esp. large at k<=2, dcreases abuptly until k=22, decreases only gradually until k=100
 - same bias/variance trade off as KSG MI
 - you might say the best bias/variance is 20<k<40 ?

Alterntiave KSG information explanations
* GomezHerrero15cplDynTSens
* Kraskov04EstMutInfKNN
* Lizier14JIDTinfoToolkit_SupMat

* Gao15mutInfoStrongDep

* explains shortcomings of greedy search, has some (obvious) fixes:  Lizier12compNetsTransEnt},
  doi                  = {10.1103/PhysRevLett.99.204101},
  file                 = {Frenzel07partMutInfo.pdf:Frenzel07partMutInfo.pdf:PDF},
  keywords             = {information, information\_theory, information\_transfer, math},
  owner                = {sotterson},
  posted-at            = {2008-01-02 19:45:57},
  publisher            = {APS},
  timestamp            = {2009.03.16},
}

@Article{Zheng13sparseQR,
  author    = {Qi Zheng and Colin Gallagher and K.B. Kulasekera},
  title     = {Adaptive penalized quantile regression for high dimensional data},
  journal   = {Journal of Statistical Planning and Inference},
  year      = {2013},
  volume    = {143},
  number    = {6},
  pages     = {1029--1038},
  issn      = {0378-3758},
  abstract  = {We propose a new adaptive \{L1\} penalized quantile regression estimator for high-dimensional sparse regression models with heterogeneous error sequences. We show that under weaker conditions compared with alternative procedures, the adaptive \{L1\} quantile regression selects the true underlying model with probability converging to one, and the unique estimates of nonzero coefficients it provides have the same asymptotic normal distribution as the quantile estimator which uses only the covariates with non-zero impact on the response. Thus, the adaptive \{L1\} quantile regression enjoys oracle properties. We propose a completely data driven choice of the penalty level ?? n , which ensures good performance of the adaptive \{L1\} quantile regression. Extensive Monte Carlo simulation studies have been conducted to demonstrate the finite sample performance of the proposed method.},
  comment   = {LASSO-like QR with a closed form choice of sparsity penalty. Supposed to be great for high dim.},
  doi       = {10.1016/j.jspi.2012.12.009},
  file      = {Zheng13sparseQR.pdf:Zheng13sparseQR.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_2},
  keywords  = {Adaptive},
  owner     = {sotterson},
  timestamp = {2013.10.24},
  url       = {http://www.sciencedirect.com/science/article/pii/S0378375812003680},
}

@Article{Arbenz11fastSumDepRVsAEP,
  author    = {Philipp Arbenz and Paul Embrechts and Giovanni Puccetti},
  title     = {The {AEP} algorithm for the fast computation of the distribution of the sum of dependent random variables},
  journal   = {Bernoulli},
  year      = {2011},
  volume    = {17},
  number    = {2},
  pages     = {562--591},
  month     = {may},
  abstract  = {We propose a new algorithm to compute numerically the distribution function of the sum of d dependent, non-negative random variables with given joint distribution.
Keywords: convolution; distribution functions},
  comment   = {For probabilsitic upscaling, reserve allocation, or ReWP reserve markets.  Is Copula based.

For a generalized function of dependent RVs, see: Arbenz12fastCompFuncRVsGAEP},
  doi       = {10.3150/10-BEJ284},
  file      = {Arbenz11fastSumDepRVsAEP.pdf:Arbenz11fastSumDepRVsAEP.pdf:PDF},
  owner     = {sotterson},
  publisher = {Bernoulli Society for Mathematical Statistics and Probability},
  timestamp = {2017.05.22},
}

@InProceedings{Huang06inputSelStockForecast,
  author    = {Wei Huang and Shouyang Wang and Lean Yu and Yukun Bao and Lin Wang},
  title     = {A New Computational Method of Input Selection for Stock Market Forecasting with Neural Networks},
  booktitle = {International Conference on Computational Science},
  year      = {2006},
  pages     = {308--315},
  abstract  = {We propose a new computational method of input selection for stock market forecasting with neural networks. The method results from synthetically considering the special feature of input variables of neural networks and the special feature of stock market time series. We conduct the experiments to compare the prediction performance of the neural networks based on the different input variables by using the different input selection methods for forecasting S\&P 500 and NIKKEI 225. The experiment results show that our method performs best in selecting the appropriate input variables of neural networks.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comment   = {Simple NN lagged feature sel filter method w/ correlation \& relevance/redundancy tradeoff. Better than BIC/AIC/CAC Neural network, forecasting stock markets 
* one step ahead MLP or RBFN 
* sliding FIR structure, only past values; not multivariate 
* max lag is 8-12 (3 experiments) 
* for stocks, NN found to be better than ARIMA (so maybe skip feedback forecasting?) 

Problem w/ correlated inputs 
* NN's OK for noisy or meaningless inputs but not correlated
 -- correlated inputs confuse training, cause oscillations b/c of ambiguous cost func ==> overtraining
 -- IDEA: couldn't I just do PCA or ICA to decorrelatie? 
* information theoretic (AIC/BIC) criteria don't deselect correlated inputs
 -- another paper says that correlated inputs screw up linear regression (independent noise model is then wrong) 
* Chaos analysis criterion (CAC)
 -- the embedded delay stuff
 -- doesn't remove correlated inputs either 
* BIC/AIC/CAC explained in Qi01modelSelNNforecas 

This method: 

Filter technique (no knowledge of forecaster) 
* simple redundancy remover based on xcorr product 
* Relevance and Redundancy tradeoff handled by output and selected set correlation ratio. 
* Scan towards higher lags until hit max. 

Scour to max lag at each iteration allows lags to be skipped. 
* Weakness is that still doing single-variable relevance measurement (unlike joint MI approaches). So may miss important combinations. 
* Also, danger that noise (zero denominator) variables easy to select (like w/ Peng mRMR). But maybe OK if NN can reject irrelevant inputs? 

Results 
* Proposed correlation method is best. 
* CAC worse than persistence! 
* BIC closest to proposed method. 
* Not compared w/ selecting ALL lags, though, so I'm not sure if this was a hard problem...},
  doi       = {10.1007/11758549},
  file      = {Huang06inputSelStockForecast.pdf:Huang06inputSelStockForecast.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.01.29},
  url       = {http://www.springerlink.com/content/38969r3587811486/},
  volumen   = {4},
}

@InProceedings{Poczos10rankEstRenInfo,
  author    = {P{\'o}czos, B. and Kirshner, S. and Szepesv{\'a}ri, {Cs}.},
  title     = {{REGO}: Rank-based Estimation of {R\'e}nyi Information using {E}uclidean Graph Optimization},
  booktitle = {Artificial Intelligence and Statistics (AISTAT)},
  year      = {2010},
  volume    = {9},
  pages     = {852--859},
  month     = may,
  abstract  = {We propose a new method for a non-parametric estimation of R{\'e}nyi and Shannon information for a multivariate distribution using a corresponding copula, a multivariate distribution over normalized ranks of the data. As the information of the distribution is the same as the negative entropy of its copula, our method estimates this information by solving a Euclidean graph optimization problem on the empirical estimate of the distribution's copula. Owing to the properties of the copula, we show that the resulting estimator of R{\'e}nyi information is strongly consistent and robust. Further, we demonstrate its applicability in image registration in addition to simulated experiments.},
  comment   = {Another way to estimate mutual information. Copulas used. Requires lots of data for 10D estimation.

Video here: http://videolectures.net/aistats2010_poczos_rrbe/

* Estimates total entropy, not entropy between a pair of variables

* Inspired by sol'n to interesting sample-contamination removal problem (removes individual points, Huber 2009)

Why use cupola rank transform
1.) simplified est. prob. by removing terms about the individual marginals

2.) is robust to outliers
3.) estimators don't have to work on unbounded support (range is (0,1)). Seems like this could actually be bad??
4.) MI is preserved w/ an invertible transform, like the rank transform

5.) after rank transform, entropy == mutual information (b/c of uniform marginals)

Estimate MI w/ Rank -based Euclidean Graph Optimizaqtion (REGO)
* avoids density estimation
* actually estimates Renyii's entropy, which can be tuned to do Shannon's diffferential entropy w/ alpha=1
* Three methods tried
-- Minimal Spanning Tree (MST)
-- Traveling Salesman problem (TSP)
-- k-Nearest Neighbors (actually, Kraskov04* suggested a rank transform but actually does mean/var transform)

Advantages Proved ((theorems)
* it's stable
* it doesn't blow up w/ unbounded output values (is "maximally robust')

RESULTS
* cop-KNN converges fastest vs. num. samples
* k-MST is no good
* on 10D data, need 30K samples! How does this compare to ordinary Kraskov k-NN estimator?},
  date      = {2010-05},
  file      = {Poczos10rankEstRenInfo.pdf:Poczos10rankEstRenInfo.pdf:PDF},
  groups    = {Read},
  keywords  = {information theory, theory, mutual information},
  owner     = {scot},
  timestamp = {2010.12.20},
}

@InProceedings{Levina04maxLikIntrinsDim,
  author    = {Levina, Elizaveta and Bickel, Peter J},
  title     = {Maximum likelihood estimation of intrinsic dimension},
  booktitle = {Advances in neural information processing systems},
  year      = {2004},
  pages     = {777--784},
  abstract  = {We propose a new method for estimating intrinsic dimension of a
dataset derived by applying the principle of maximum likelihood to
the distances between close neighbors. We derive the estimator by
a Poisson process approximation, assess its bias and variance theo-
retically and by simulations, and apply it to a number of simulated
and real datasets. We also show it has the best overall performance
compared with two other intrinsic dimension estimators.},
  comment   = {The intrinsic dimension estimator used in Schnitzer12Localandglobal and other hubness papers. Said to be much lower than embedded dimension, often, in Schnitzer12Localandglobal},
  file      = {Levina04maxLikIntrinsDim.pdf:Levina04maxLikIntrinsDim.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.10.28},
  url       = {http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2005_94.pdf},
}

@Article{Gong14extrmQRvineCopula,
  author    = {Gong, Jinguo and Li, Yadong and Peng, Liang and Yao, Qiwei},
  title     = {Estimation of extreme quantiles for functions of dependent random variables},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year      = {2014},
  issn      = {1467-9868},
  abstract  = {We propose a new method for estimating the extreme quantiles for a function of several dependent random variables. In contrast with the conventional approach based on extreme value theory, we do not impose the condition that the tail of the underlying distribution admits an approximate parametric form, and, furthermore, our estimation makes use of the full observed data. The method proposed is semiparametric as no parametric forms are assumed on the marginal distributions. But we select appropriate bivariate copulas to model the joint dependence structure by taking advantage of the recent development in constructing large dimensional vine copulas. Consequently a sample quantile resulting from a large bootstrap sample drawn from the fitted joint distribution is taken as the estimator for the extreme quantile. This estimator is proved to be consistent under the regularity conditions on the closeness between a quantile set and its truncated set, and the empirical approximation for the truncated set. The simulation results lend further support to the reliable and robust performance of the method proposed. The method is further illustrated by a real world example in backtesting financial risk models.

Keywords:
 Bootstrap;Drawable vine copula;Empirical distribution function;Extreme quantile;Sample quantiles;Time series},
  comment   = {Extreme values, vine copulas, related to Yu14extrmValGrpMdlThinMemb},
  doi       = {10.1111/rssb.12103},
  file      = {arXiv paper:Gong14extrmQRvineCopula.pdf:PDF},
  keywords  = {Bootstrap, Drawable vine copula, Empirical distribution function, Extreme quantile, Sample quantiles, Time series},
  owner     = {sotterson},
  timestamp = {2015.04.17},
}

@Article{Tibshirani96lasso,
  author               = {Tibshirani, Robert},
  title                = {Regression shrinkage and selection via the lasso},
  journal              = {Journal of the Royal Statistical Society, Series B},
  year                 = {1996},
  volume               = {58},
  number               = {1},
  pages                = {267--288},
  abstract             = {We propose a new method for estimation in linear models. The "lasso" minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly zero and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models},
  citeulike-article-id = {416068},
  citeulike-linkout-0  = {http://www.ams.org/mathscinet-getitem?mr=1379242},
  comment              = {The original lasso paper *"leaps" cross-validation procedure (p. 279) -- 5-fold (4/5 train, 1/5 test, rotating) -- figure of merit is avg. prediction accuracy across folds -- isn't this standard?  Note that it's claimed that LASSO has the stability of ridge regression but lately, it's famous that LASSO is unstable.  See for example Nogueira18featSelStbl},
  file                 = {Tibshirani96lasso.pdf:Tibshirani96lasso.pdf:PDF;Tibshirani96lasso.pdf:Tibshirani96lasso.pdf:PDF},
  keywords             = {feature-selection, lasso, regression, ridge-regression, shrinkage},
  mrnumber             = {MR1379242},
  owner                = {sotterson},
  posted-at            = {2008-11-13 17:23:13},
  timestamp            = {2009.08.13},
  url                  = {http://www.ams.org/mathscinet-getitem?mr=1379242},
}

@InProceedings{Suzuki09mutInfoEstRatio,
  author    = {Suzuki, T. and Sugiyama, M. and Tanaka, T.},
  title     = {Mutual information approximation via maximum likelihood estimation of density ratio},
  booktitle = {Information Theory, 2009. ISIT 2009. IEEE International Symposium on},
  year      = {2009},
  pages     = {463--467},
  month     = {28 2009-july 3},
  abstract  = {We propose a new method of approximating mutual information based on maximum likelihood estimation of a density ratio function. The proposed method, Maximum Likelihood Mutual Information (MLMI), possesses useful properties, e.g., it does not involve density estimation, the global optimal solution can be efficiently computed, it has suitable convergence properties, and model selection criteria are available. Numerical experiments show that MLMI compares favorably with existing methods.},
  comment   = {Used in squared error loss dimension reduction paper
* uses Suzuki10suffDimRedSqMutInf, I think
* doesn't compare to Kraskov04
* has matlab: http://sugiyama-www.cs.titech.ac.jp/~sugi/software/MLMI/index.html},
  doi       = {10.1109/ISIT.2009.5205712},
  file      = {Suzuki09mutInfoEstRatio.pdf:Suzuki09mutInfoEstRatio.pdf:PDF},
  keywords  = {density ratio function;maximum likelihood estimation;maximum likelihood mutual information;model selection criteria;mutual information approximation;approximation theory;maximum likelihood estimation;},
  owner     = {sotterson},
  timestamp = {2011.11.30},
}

@InProceedings{Shotton11poseRecogDepth,
  author    = {Jamie Shotton and Andrew Fitzgibbon and Mat Cook and Toby Sharp and Mark Finocchio and Richard Moore and Alex Kipman and Andrew Blake},
  title     = {Real-Time Human Pose Recognition in Parts from Single Depth Images},
  booktitle = {Computer Vision and Pattern Recognition (CVPR)},
  year      = {2011},
  month     = jun,
  abstract  = {We propose a new method to quickly and accurately predict 3D positions of body joints from a single depth image, using no temporal information. We take an object recognition approach, designing an intermediate body parts representation that maps the difficult pose estimation problem into a simpler per-pixel classification problem. Our large and highly varied training dataset allows the classifier to estimate body parts invariant to pose, body shape, clothing, etc. Finally we generate confidence-scored 3D proposals of several body joints by reprojecting the classification result and finding local modes. The system runs at 200 frames per second on consumer hardware. Our evaluation shows high accuracy on both synthetic and real test sets, and investigates the effect of several training parameters. We achieve state of the art accuracy in our comparison with related work and demonstrate improved generalization over exact whole-skeleton nearest neighbor matching.},
  comment   = {How Microsoft Kinect finds joint positions. Random forests, mean shift and interesting synthesized training data},
  file      = {Shotton11poseRecogDepth.pdf:Shotton11poseRecogDepth.pdf:PDF},
  groups    = {Read},
  owner     = {scotto},
  timestamp = {2011.04.10},
}

@Article{Nafidi18inhomogDiffProcBassSDE,
  author       = {Nafidi, Ahmed and J{\'a}imez, Ram{\'o}n Guti{\'e}rrez and S{\'a}nchez, Ram{\'o}n Guti{\'e}rrez},
  title        = {A new inhomogeneous lognormal diffusion process with exogenous factors in diffusion coefficient},
  year         = {2018},
  volume       = {41},
  pages        = {131--137},
  abstract     = {We propose a new non-homogeneous one dimensional stochastic lognormal diffusion process,
by introducing time function (Exogenous factors) in the diffusion coefficient of the process
and which can be considered as an extension of the homogeneous lognormal process (see [1],
[2] and [3] ). From the corresponding Ito’s stochastic differential equation, the probabilistic
characteristics of the model are obtained such as the transition probability density function
and the moments of the process. Finally, we develop the statistical inference of this model
on the basis of maximum likelihood methodology with discrete sampling.

Keywords: Lognormal diffusion process, Exogenous factors, Statistical inference in diffu-
sion process, Trend function},
  booktitle    = {Fourteenth International Conference Zaragoza--Pau on Mathematics and its Applications: Jaca (Spain), September 12},
  comment      = {Another SDE Bass diffusion model},
  file         = {:Nafidi18inhomogDiffProcBassSDE.pdf:PDF},
  organization = {Prensas de la Universidad de Zaragoza},
  url          = {https://books.google.de/books?hl=en&lr=&id=Mj5VDwAAQBAJ&oi=fnd&pg=PA131&dq=bass+diffusion+model+stochastic+differential+equations&ots=bqYuZ7QSEb&sig=-j6ndO0pdUC1eOeehhBK0f4Ms9Y&redir_esc=y#v=onepage&q=bass%20diffusion%20model%20stochastic%20differential%20equations&f=false},
}

@Article{Tashiro16hierBassDiff,
  author   = {Tohru Tashiro},
  title    = {Hierarchical Bass model: a product diffusion model considering a diversity of sensitivity to fashion},
  journal  = {Physica A: Statistical Mechanics and its Applications},
  year     = {2016},
  volume   = {461},
  pages    = {824 - 832},
  issn     = {0378-4371},
  abstract = {We propose a new product diffusion model including the number of how many adopters or advertisements a non-adopter met until he/she adopts the product, where (non-)adopters mean people (not) possessing it. By this effect not considered in the Bass model, we can depict a diversity of sensitivity to fashion. As an application, we utilize the model to fit the iPod and the iPhone unit sales data, and so the better agreement is obtained than the Bass model for the iPod data. We also present a new method to estimate the number of advertisements in a society from fitting parameters of the Bass model and this new model.},
  comment  = {Bass model with an extra term intended to model slower adopters, doesn't seem better than ordinary Bass, but there is (I think) a new derivation for it that estimates population size.},
  doi      = {https://doi.org/10.1016/j.physa.2016.06.032},
  file     = {:Tashiro16hierBassDiff.pdf:PDF},
  keywords = {Product diffusion model, Bass model, Diversity of sensitivity to fashion},
  url      = {http://www.sciencedirect.com/science/article/pii/S0378437116303004},
}

@Article{Hido10outlierDetDensRat,
  author    = {Shohei Hido and Yuta Tsuboi and Hisashi Kashima and Masashi Sugiyama and Takafumi Kanamori},
  title     = {Statistical outlier detection using direct density ratio estimation},
  journal   = {Knowledge and Information Systems},
  year      = {2010},
  volume    = {26},
  number    = {2},
  pages     = {309--336},
  month     = {feb},
  abstract  = {We propose a new statistical approach to the problem of inlier-based outlier detection, i.e., finding outliers in the test set based on the training set consisting only of inliers. Our key idea is to use the ratio of training and test data densities as an outlier score. This approach is expected to have better performance even in high-dimensional problems since methods for directly estimating the density ratio without going through density estimation are available. Among various density ratio estimation methods, we employ the method called unconstrained least-squares importance fitting (uLSIF) since it is equipped with natural cross-validation procedures, allowing us to objectively optimize the value of tuning parameters such as the regularization parameter and the kernel width. Furthermore, uLSIF offers a closed-form solution as well as a closed-form formula for the leave-one-out error, so it is computationally very efficient and is scalable to massive datasets. Simulations with benchmark and real-world datasets illustrate the usefulness of the proposed approach.

Keywords

Outlier detection Density ratio Importance Unconstrained least-squares importance fitting (uLSIF) },
  comment   = {Somehow use this for SPRT, which has a density ratio in it?},
  doi       = {10.1007/s10115-010-0283-2},
  file      = {:Hido10outlierDetDensRat.pdf:PDF},
  publisher = {Springer Nature},
}

@Article{FerrariTrecate03clustPcwsAffine,
  author    = {Ferrari-Trecate, Giancarlo and Muselli, Marco and Liberati, Diego and Morari, Manfred},
  title     = {A clustering technique for the identification of piecewise affine systems},
  journal   = {Automatica},
  year      = {2003},
  volume    = {39},
  number    = {2},
  pages     = {205--217},
  abstract  = {We propose a new technique for the identi?cation of discrete-
time hybrid systems in the Piece-Wise A?ne (PWA) form. The identi-
?cation algorithm proposed in [9] is ?rst considered and then improved
under various aspects. Measures of con?dence on the samples are intro-
duced and exploited in order to improve the performance of both the
clustering algorithm used for classifying the data and the ?nal linear
regression procedure. Moreover, clustering is performed in a suitably de-
?ned space that allows also to reconstruct di?erent submodels that share
the same coe?cients but are de?ned on di?erent regions.},
  comment   = {For local linear regression neighborhood picking, maybe regime detection},
  file      = {FerrariTrecate03clustPcwsAffine.pdf:FerrariTrecate03clustPcwsAffine.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.03.31},
  url       = {http://www.sciencedirect.com/science/article/pii/S0005109802002248},
}

@InProceedings{Saon04featGaussianiz,
  author    = {Saon, G. and Dharanipragada, S. and Povey, D.},
  title     = {Feature space {Gauss}ianization},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2004},
  volume    = {1},
  month     = {17-21},
  abstract  = {We propose a non-linear feature space transformation for speaker/environment adaptation which forces the individual dimensions of the acoustic data for every speaker to be Gaussian distributed. The transformation is given by the preimage under the Gaussian cumulative distribution function (CDF) of the empirical CDF on a per dimension basis. We show that, for a given dimension, this transformation achieves minimum divergence between the density function of the transformed adaptation data and the normal density with zero mean and unit variance. Experimental results on both small and large vocabulary tasks show consistent improvements over the application of linear adaptation transforms only.},
  comment   = {spinning reserves. Actually, this is just marginal Gaussianization. Note that they also mention that this a rank transformation.},
  doi       = {10.1109/ICASSP.2004.1325989},
  file      = {Saon04featGaussianiz.pdf:Saon04featGaussianiz.pdf:PDF},
  issn      = {1520-6149},
  keywords  = { Gaussian cumulative distribution function; Gaussian distribution; acoustic data; density function; environment adaptation; feature space Gaussianization; linear adaptation transforms; nonlinear feature space transformation; speaker adaptation; speech recognition; unit variance; zero mean; Gaussian distribution; speech recognition; transforms;},
  owner     = {scot},
  timestamp = {2010.08.02},
}

@Article{Anderlucci15covPatMix,
  author    = {Anderlucci, Laura and Viroli, Cinzia and others},
  title     = {Covariance pattern mixture models for the analysis of multivariate heterogeneous longitudinal data},
  journal   = {The Annals of Applied Statistics},
  year      = {2015},
  volume    = {9},
  number    = {2},
  pages     = {777--800},
  abstract  = {We propose a novel approach for modeling multivariate longitudi-
nal data in the presence of unobserved heterogeneity for the analysis
of the Health and Retirement Study (HRS) data. Our proposal can be
cast within the framework of linear mixed models with discrete indi-
vidual random intercepts; however, differently from the standard for-
mulation, the proposed Covariance Pattern Mixture Model (CPMM)
does not require the usual local independence assumption. The model
is thus able to simultaneously model the heterogeneity, the associa-
tion among the responses and the temporal dependence structure.
We focus on the investigation of temporal patterns related to the
cognitive functioning in retired American respondents. In particular,
we aim to understand whether it can be affected by some individual
socio-economical characteristics and whether it is possible to identify
some homogenous groups of respondents that share a similar cogni-
tive profile. An accurate description of the detected groups allows
government policy interventions to be opportunely addressed.
Results identify three homogenous clusters of individuals with spe-
cific cognitive functioning, consistent with the class conditional dis-
tribution of the covariates. The flexibility of CPMM allows for a
different contribution of each regressor on the responses according to
group membership. In so doing, the identified groups receive a global
and accurate phenomenological characterization},
  comment   = {A sparse and fast mixture model for modeling a multi-variable cross-variable and temporal correlation (adapt to copula?). Temporal and non-temporal components combined can be expressed with a matrix-normal distribution -- core is a linear model (like a Gaussian) but mixture part allows it to be, for example, power-level and wind direction sensitive, and marginal transforms also make it nonlinear.  Seems to be able to do mixture with still assuring temporal corr of a particular individual which is a combination of mixtures.  Can handle missing data.  Solution is a fast (on this data) EM algorithm. I can figure out how to do a copula transform (matrix-normal marginals) then this could better than precision-matrix approaches like Tastu13scenSpatTimeCpla.

Adaptation to Copula
* matrix-normal dist is just another way to decompose a multivariate normal covariance matrix (Owen13monteCarloBook)
* would have to do max lik learning but that's no different than Tastu15spcTimeTrajGaussCpla
* compare with precision matrix sparsity in Tastu15spcTimeTrajGaussCpla
* compare with other stuff in energytop.org <<Covariance sparseness>>

TODO
* what is the marginal distribution?  Do other copulas specifically transfer to uniform and then to the marginal of the dependence matrix?
* look up conditional distribution
* matrix normal distribution info in: Owen13monteCarloBook

* Could also maybe use a trick like Bessa16gaussCplaDcsns?
* Responses could be reference farms or whatever.
"Response" means a regression output, the thing(s) you're predicting given other variables.  This paper avoids the common assumption in this kind of work that multivariate responses are independent of each other, given the latent mixture variable.  See Section 3.3},
  file      = {:papers\\Anderlucci15covPatMix.pdf:PDF},
  owner     = {sotterson},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2017.07.05},
  url       = {http://projecteuclid.org/euclid.aoas/1437397111},
}

@InProceedings{Ukkonen17ExplbDevSubsetsExplnNets,
  author    = {Antti Ukkonen and Vladimir Dzyuba and Matthijs van Leeuwe},
  title     = {Explaining Deviating Subsets through Explanation Networks},
  booktitle = {ECML PKDD. European Conference On Machine Learning \& Principles And Practice Of Knowledge Discovery In Databases},
  year      = {2017},
  abstract  = {We propose a novel approach to finding explanations of de-
viating subsets, often called subgroups. Existing approaches for subgroup
discovery rely on various quality measures that nonetheless often fail to
find subgroup sets that are diverse, of high quality, and most importantly,
provide good explanations of the deviations that occur in the data.
To tackle this issue we introduce explanation networks, which provide
a holistic view on all candidate subgroups and how they relate to each
other, offering elegant ways to select high-quality yet diverse subgroup
sets. Explanation networks are constructed by representing subgroups
by nodes and having weighted edges represent the extent to which one
subgroup explains another. Explanatory strength is defined by extending
ideas from database causality, in which interventions are used to quantify
the effect of one query on another.
Given an explanatory network, existing network analysis techniques can
be used for subgroup discovery. In particular, we study the use of Page-
Rank for pattern ranking and seed selection (from influence maximiza-
tion) for pattern set selection. Experiments on synthetic and real data
show that the proposed approach finds subgroup sets that are more likely
to capture the generative processes of the data than other methods.},
  comment   = {Detects unusual groups of varibles given a scalar (I think) target variable.  The groups explain the target value and, I'm guessing, are unusual.  It's used in fraud detect and fault detection.  So maybe good for diagnosing any faults detected in ModernWindABS.  Once nice thing about it, is that it uses PageRank, so some of the hairiness is handled by a standard algorithm.},
  file      = {:Ukkonen17ExplbDevSubsetsExplnNets.pdf:PDF},
  url       = {http://ecmlpkdd2017.ijs.si/program.html},
}

@Article{Kraemer08penPLSspline,
  author    = {Kr{\"a}mer, Nicole and Boulesteix, Anne-Laure and Tutz, Gerhard},
  title     = {Penalized Partial Least Squares with applications to B-spline transformations and functional data},
  journal   = {Chemometrics and Intelligent Laboratory Systems},
  year      = {2008},
  volume    = {94},
  number    = {1},
  pages     = {60--69},
  abstract  = {We propose a novel framework that combines penalization techniques with Partial Least Squares (PLS). We focus on two important applications. (1) We combine PLS with a roughness penalty to estimate high-dimensional regression problems with functional predictors and scalar response. (2) Starting with an additive model, we expand each variable in terms of a generous number of B-spline basis functions. To prevent overfitting, we estimate the model by applying a penalized version of PLS. We gain additional model flexibility by incorporating a sparsity penalty. Both applications can be formulated in terms of a unified algorithm called Penalized Partial Least Squares, which can be computed virtually as fast as PLS using the kernel trick. Furthermore, we prove a close connection of penalized PLS to preconditioned linear systems. In experiments, we show the benefits of our method to noisy functional data and to sparse nonlinear regression models.
Keywords

 NIR spectroscopy;
 Additive model;
 Dimensionality reduction;
 Nonlinear regression;
 Conjugate gradient;
 Krylov spaces},
  comment   = {PLS AND splines AND regularization, supposedly fast

Paper is 2006 arivx, w/ slightly different abstract than journal. Need journal paper too??},
  file      = {Kraemer08penPLSspline.pdf:Kraemer08penPLSspline.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.04.27},
  url       = {http://www.sciencedirect.com/science/article/pii/S0169743908001214},
}

@InProceedings{Xu08NonlinSimMulti,
  author    = {Xu, Jian-Wu and Bakardjian, Hovagim and Cichocki, Andrzej and Principe, Jose C.},
  title     = {A new nonlinear similarity measure for multichannel signals},
  booktitle = {International Joint Conference on Neural Networks (IJCNN)},
  year      = {2008},
  volume    = {21},
  number    = {2-3},
  pages     = {222--231},
  abstract  = {We propose a novel similarity measure, called the correntropy coefficient, sensitive to higher order moments of the signal statistics based on a similarity function called the cross-correntopy. Cross-correntropy nonlinearly maps the original time series into a high-dimensional reproducing kernel Hilbert space (RKHS). The correntropy coefficient computes the cosine of the angle between the transformed vectors. Preliminary experiments with simulated data and multichannel electroencephalogram (EEG) signals during behaviour studies elucidate the performance of the new measure versus the well-established correlation coefficient.},
  comment   = {possibly use for scenario similarity when multichannel},
  file      = {Xu08NonlinSimMulti.pdf:Xu08NonlinSimMulti.pdf:PDF},
  issn      = {0893-6080},
  journal   = {International Joint Conference on Neural Networks (IJCNN)},
  owner     = {scot},
  timestamp = {2010.12.02},
}

@Article{Toke16modelIntsLimOrdrBk,
  author    = {Ioane Muni Toke and Nakahiro Yoshida},
  title     = {Modelling intensities of order flows in a limit order book},
  journal   = {Quantitative Finance},
  year      = {2016},
  pages     = {1--19},
  month     = {nov},
  abstract  = {We propose a parametric model for the simulation of limit order books. We assume that limit orders, market orders and cancellations are submitted according to point processes with state-dependent intensities. We propose new functional forms for these intensities, as well as new models for the placement of limit orders and cancellations. For cancellations, we introduce the concept of ?priority index? to describe the selection of orders to be cancelled in the order book. Parameters of the model are estimated using likelihood maximization. We illustrate the performance of the model by providing extensive simulation results, with a comparison to empirical data and a standard Poisson reference.},
  comment   = {Uses a Hawkes process to model market order book.  Here's what Henry Martin said about it:

Concerning the Toke paper: I talked about it because it is cited in the review 2013Gould_limitOrderBooks (This is the awesome paper) but I did not read the Toke paper, so I can not say if it is worth the time or not. I just used it to see what they predict using the hawkes process and how they validate it but you should definitely have a look at the Gould paper.

I wonder: would autogregressive quantile regression be better?

Gould Paper: Gould13limitOrderBks
Preprint version that Henry started with:
Toke11mktMkingOBsprd

See Smirni16cs426dscrtEvntSimCrsNotes for Poisson arrival stuff},
  doi       = {http://dx.doi.org/10.1080/14697688.2016.1236210},
  file      = {Toke16modelIntsLimOrdrBk.pdf:Toke16modelIntsLimOrdrBk.pdf:PDF},
  keywords  = {Order book, Limit orders, Market orders, Cancellations, State-dependent point processes, Intensity-based models},
  owner     = {sotterson},
  publisher = {Informa {UK} Limited},
  timestamp = {2017.02.10},
}

@Article{Gao15copulaDensPenExpSeries,
  author    = {Gao, Yichen and Zhang, Yu Yvette and Wu, Ximing},
  title     = {Penalized exponential series estimation of copula densities with an application to intergenerational dependence of body mass index},
  journal   = {Empirical Economics},
  year      = {2015},
  volume    = {48},
  number    = {1},
  pages     = {61--81},
  issn      = {1435-8921},
  abstract  = {We propose a penalized maximum likelihood estimator of copula densities that is based on the multivariate exponential series density estimator. We employ an approximate likelihood cross validation method to select the smoothing parameter. We demonstrate the usefulness of the proposed method via Monte Carlo simulations. We apply this method to estimate copula densities between children's and parents' body mass indices (BMI). Our results suggest that the dependence relationship is generally asymmetric and stronger for females. We also find a higher intergenerational BMI dependence for low income families.},
  comment   = {Copula density estimated with a roughness-penalized exponential series.  Interesting.},
  doi       = {10.1007/s00181-014-0858-y},
  file      = {Gao15copulaDensPenExpSeries.pdf:Gao15copulaDensPenExpSeries.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.23},
  url       = {http://dx.doi.org/10.1007/s00181-014-0858-y},
}

@Article{AriasCastro13specClustLocPCA,
  author    = {Arias-Castro, Ery and Lerman, Gilad and Zhang, Teng},
  title     = {Spectral clustering based on local {PCA}},
  journal   = {arXiv preprint arXiv:1301.2007},
  year      = {2013},
  abstract  = {We propose a spectral clustering method based on local principal components analysis (PCA). After
performing local PCA in selected neighborhoods, the algorithm builds a nearest neighbor graph
weighted according to a discrepancy between the principal subspaces in the neighborhoods, and
then applies spectral clustering. As opposed to standard spectral methods based solely on pairwise
distances between points, our algorithm is able to resolve intersections. We establish theoretical
guarantees for simpler variants within a prototypical mathematical framework for multi-manifold
clustering, and evaluate our algorithm on various simulated data sets.
Keywords: multi-manifold clustering, spectral clustering, local principal component analysis, intersecting
clusters.},
  comment   = {Useful for line clustering or quantile clustering? Only problem is that dependent variable is only one of many variables. Would want to prioritize that somehow. Line clustering for 1dim X and 1dim Y would be fine, though.},
  file      = {AriasCastro13specClustLocPCA.pdf:AriasCastro13specClustLocPCA.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.03.31},
  url       = {http://arxiv.org/abs/1301.2007},
}

@Article{LI07nonParaAssessSpaceTimeCovar,
  author    = {Li, Bo and Genton, Marc G. and Sherman, Michael},
  title     = {A Nonparametric Assessment of Properties of SpaceTime Covariance Functions},
  journal   = {Journal of the American Statistical Association},
  year      = {2007},
  volume    = {102},
  pages     = {736--744},
  month     = jun,
  abstract  = {We propose a unified framework for testing various assumptions commonly made for covariance functions of stationary spatio-temporal random fields. The methodology is based on the asymptotic normality of space?time covariance estimators. We focus on tests for full symmetry and separability in this article, but our framework naturally covers testing for isotropy and Taylor?s hypothesis. Our test successfully detects the asymmetric and nonseparable features in two sets of wind speed data. We perform simulation experiments to evaluate our test and conclude that our method is reliable and powerful for assessing common assumptions on space?time covariance functions.},
  comment   = {Use for wind velocity basis function, upscaling, offsite observations?
See also: Porcu07covarStatSpaceTime, LI07nonParaAssessSpaceTimeCovar},
  file      = {LI07nonParaAssessSpaceTimeCovar.pdf:LI07nonParaAssessSpaceTimeCovar.pdf:PDF;LI07nonParaAssessSpaceTimeCovar.pdf:LI07nonParaAssessSpaceTimeCovar.pdf:PDF},
  owner     = {scotto},
  timestamp = {2008.12.27},
  url       = {http://ideas.repec.org/a/bes/jnlasa/v102y2007mjunep736-744.html},
}

@Misc{Tomioka09DALsparseRecons,
  author   = {Ryota Tomioka and Masashi Sugiyama},
  title    = {Dual Augmented Lagrangian Method for Efficient Sparse Reconstruction},
  year     = {2009},
  abstract = {We propose an efficient algorithm for sparse signal reconstruction problems. The proposed algorithm is an augmented Lagrangian method based on the dual sparse reconstruction problem. It is efficient when the number of unknown variables is much larger than the number of observations because of the dual formulation. Moreover, the primal variable is explicitly updated and the sparsity in the solution is exploited. Numerical comparison with the state-of-the-art algorithms shows that the proposed algorithm is favorable when the design matrix is poorly conditioned or dense and very large.},
  comment  = {A Lasso approach that can do "millions of variables in 20 minutes". Especially good for ill-conditioning and when nDims>>nSamps
* has matlab on home page: http://www.sat.t.u-tokyo.ac.jp/~ryotat/dal/index.html and downloaded here: ~/lib/matlab/dal_ver0.97
* paper submitted to IEEE Signal Processing
* tests are on simple binary features.

I don't know how well this would work for continuos vars

May be limited in num. of points it can handle:
* I tried it for selecting single lags (in middle of selectlag1feat.m) and found that it runs out of memory for only 68K samples (dim=144)

* while stepwisefit.m did not.
* dal worked OK for nSamps=1e3 *(see dal_test.m)},
  file     = {Tomioka09DALsparseRecons.pdf:Tomioka09DALsparseRecons.pdf:PDF;Tomioka09DALsparseRecons.pdf:Tomioka09DALsparseRecons.pdf:PDF},
  url      = {http://www.citebase.org/abstract?id=oai:arXiv.org:0904.0584},
}

@Article{Giglio13riskPartialQR,
  author    = {Giglio, Stefano and Kelly, Bryan and Pruitt, Seth and Qiao, Xiao},
  title     = {Systemic risk and the macroeconomy: An empirical evaluation},
  journal   = {Chicago Booth Research Paper},
  year      = {2013},
  number    = {12-49},
  abstract  = {We propose an empirical criterion for evaluating systemic risk measures
based on their ability to predict quantiles of future macroeconomic shocks. We
construct 17 measures of systemic risk in the US and Europe spanning several
decades. We propose dimension reduction estimators for constructing systemic
risk indexes from the cross section of measures and prove their consistency in
a factor model setting. Empirically, systemic risk indexes provide signi?cant
predictive information for the lower tail of future macroeconomic shocks, even
out-of-sample.
Keywords: systemic risk, systemic risk measures, quantile regression, dimen-
sion reduction, macroeconomic welfare},
  comment   = {A bit like partial least squares for QR. But does this really work?? I THINK, the idea is to do 1D QR for all features, calculate the correlation with them and the target, them make a single feature composed of all the inputs weighted by those correlations, and use that to do the final QR.

Really? Calcs a weighted avg. of features and then does QR? Does that even make sense?

Why QR output correlation w/ the target variable? Does that make sense?

Anyway, the nice thing is that the QR's are all 1D so could do spline QR for each

May have matlab (first page mentions Mark Flood sharing some code)

But, there's a consistency proof, and this is maybe what Dodge09partialQR said was missing.

The idea of this paper was inspired by this one: Kelly12threePssRgrssn
But instead, maybe the dim reduction approach in Tu13supervsdFctrFrcst would work better (since it was better than Kelly12*, and since it may be easy to extend to spline bases)

Kramer11dimRedKNN is another way to reduce a high dim space to 1D, and so would multidimensional scaling (MDS)},
  file      = {Giglio13riskPartialQR.pdf:Giglio13riskPartialQR.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2014.04.21},
  url       = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2158347},
}

@Article{Diebold95comparePredAcc,
  author    = {Francis X. Diebold and Roberto S. Mariano},
  title     = {Comparing Predictive Accuracy},
  journal   = {Journal of Business \& Economic Statistics},
  year      = {1995},
  volume    = {13},
  number    = {3},
  pages     = {253--263},
  abstract  = {We propose and evaluate explicit tests of the null hypothesis of no difference In the accuracy of
two competing forecasts. In contrast to previously developed tests, a wide variety of accuracy
measures can be used (in particular, the loss function need not be quadratic and need not even
be symmetric), and forecast errors can be non-Gaussian, nonzero mean, serially correlated,
and contemporaneously correlated. Asymptotic and exact finite-sample tests are proposed,
evaluated, and Illustrated.
KEY WORDS: Economic loss function; Exchange rates; Forecast evaluation; Forecasting;
Nonparametric tests; Sign test.},
  comment   = {Kristin says this is useful to show if two forecasts are equivalent or not.

Could be a stopping criteria for a wrapper forecasting feature selection algorithm.},
  file      = {Diebold95comparePredAcc.pdf:Diebold95comparePredAcc.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.09.30},
  url       = {http://www.ssc.wisc.edu/~bhansen/718/DieboldMariano1995.pdf},
}

@Article{SenGupta106asymmCircLinRgrsn,
  author    = {Ashis SenGupta and Fidelis I. Ugwuowo},
  title     = {Asymmetric circular-linear multivariate regression models with applications to environmental data},
  journal   = {Environmental and Ecological Statistics},
  year      = {2006},
  volume    = {13},
  number    = {3},
  pages     = {299--309},
  abstract  = {We propose asymmetric angular-linear multivariate regression models, which were motivated by the need to predict some environmental characteristics based on some circular and linear predictors. A measure of fit is provided through the residual analysis. Some applications using data from solar energy radiation experiment and wind energy are given.},
  comment   = {linear regression with addition of a separate nonlinear function of angle * not too relevant to wind speed/dir: no function like power ~ F(spd,dir) * instead, it's power ~ F1(spd) + F2(dir) * but maybe the wierd nonlinear F2()'s they try could be used somewhere? * eq(7) claims that wind is a linear function of speed, so obviously bogus},
  doi       = {10.1007/s10651-005-0013-1},
  file      = {SenGupta106asymmCircLinRgrsn.pdf:SenGupta106asymmCircLinRgrsn.pdf:PDF;SenGupta106asymmCircLinRgrsn.pdf:SenGupta106asymmCircLinRgrsn.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.05.08},
}

@Article{Athey19generalizedRandFrst,
  author    = {Athey, Susan and Tibshirani, Julie and Wager, Stefan and others},
  title     = {Generalized random forests},
  journal   = {The Annals of Statistics},
  year      = {2019},
  volume    = {47},
  number    = {2},
  pages     = {1148--1178},
  abstract  = {We propose generalized random forests, a method for nonparametric statistical estimation based on random forests (Breiman [Mach. Learn. 45 (2001) 5–32]) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: nonparametric quantile regression, conditional average partial effect estimation and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN.},
  comment   = {Random forest quantile regression and similar probabilistic problems using RFs.  Has R and C++},
  file      = {:Athey19generalizedRandFrst.pdf:PDF},
  publisher = {Institute of Mathematical Statistics},
  url       = {https://projecteuclid.org/euclid.aos/1547197251#abstract},
}

@Article{Fleuret04binFeatSelCMI,
  author   = {Fleuret, Fran{\c{c}}ois},
  title    = {Fast binary feature selection with conditional mutual information},
  journal  = {Journal of Machine Learning Research},
  year     = {2004},
  volume   = {5},
  number   = {Nov},
  pages    = {1531--1555},
  abstract = {We propose in this paper a very fast feature selection technique based on conditional mutual in-
formation. By picking features which maximize their mutual information with the class to predict
conditional to any feature already picked, it ensures the selection of features which are both individ-
ually informative and two-by-two weakly dependant. We show that this feature selection method
outperforms other classical algorithms, and that a naive Bayesian classifier built with features se-
lected that way achieves error rates similar to those of state-of-the-art methods such as boosting or
SVMs. The implementation we propose selects 50 features among 40, 000, based on a training set
of 500 examples in a tenth of a second on a standard 1Ghz PC.
Keywords: classification, mutual information, feature selection, naive Bayes, information theory,
fast learning},
  comment  = {Is this the 1st paper on conditional MI feature selection.  It's referred to (I think) in review paper: Vergara14revFeatSelMutInfo

* Is for binary features, here, detected "there or not" image edges or biomolecular activity.
* Works for AdaBoost (and several other) classification algorithms.
* fast, counting based algorithm, which I suppose is find for binary things.
* explains shortcomings of greedy search, has some (obvious) fixes:  Lizier12compNetsTransEnt},
  editora  = {Isabelle Guyon},
  file     = {Fleuret04binFeatSelCMI.pdf:Fleuret04binFeatSelCMI.pdf:PDF},
  url      = {http://www.jmlr.org/papers/v5/fleuret04a.html},
}

@Article{Zou05regSelElastNet,
  author    = {Hui Zou and Trevor Hastie},
  title     = {Regularization and variable selection via the elastic net},
  journal   = {Journal of the Royal Statistical Society, Series B},
  year      = {2005},
  volume    = {67},
  number    = {2},
  pages     = {301--320},
  abstract  = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the pn case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso},
  comment   = {Elastic Net LARS/LASSO/ridge regression

The Choice of Tuning parameters
- Section 3.5, p. 310
- Cross-validate on a grid of (effectively) lambda1, lambda2
-- a few lambda2's eg. (0, 0:01, 0:1, 1, 10, 100)
 -- foreach lambda2, run the full lasso path (all the k's)
 -- use 10-fold CV to pick the best (k,lambda2) pair
 -- also explains that the s and t that are used to cross-validate the lars procedure in the Sjostrand05matlabLassoLarsSpca code

Implementations
- matlab appears to be available here:
http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=3897
key: Sjostrand05matlabLassoLarsSpca ~lib/matlab/imm3897
- http://www-stat.stanford.edu/~tibs/glmnet-matlab/
 ~/lib/matlab/glmnet-matlab (fortran w/ a matlab wrapper)},
  file      = {Zou05regSelElastNet.pdf:Zou05regSelElastNet.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.09.10},
  url       = {http://ideas.repec.org/a/bla/jorssb/v67y2005i2p301-320.html},
}

@Article{Hofner12mboostR,
  author    = {Hofner, Benjamin and Mayr, Andreas and Robinzonov, Nikolay and Schmid, Matthias},
  title     = {Model-based boosting in R: a hands-on tutorial using the R package mboost},
  journal   = {Computational Statistics},
  year      = {2012},
  volume    = {29},
  number    = {1-2},
  pages     = {1--33},
  issn      = {0943-4062},
  abstract  = {We provide a detailed hands-on tutorial for the R add-on package mboost. The package implements boosting for optimizing general risk functions utilizing component-wise (penalized) least squares estimates as base-learners for fitting various kinds of generalized linear and generalized additive models to potentially high-dimensional data. We give a theoretical background and demonstrate how mboost can be used to fit interpretable models of different complexity. As an example we use mboost to predict the body fat based on anthropometric measurements throughout the tutorial.},
  comment   = {Does boosted quantile regression in R, among other things, including spline fits and other kinds of models. Might be nice because the models are supposed to be interpretable. Could be nice to see what things matter for extreme events, etc.

The pdf came from the R website: http://cran.r-project.org/web/packages/mboost/index.html},
  doi       = {10.1007/s00180-012-0382-5},
  file      = {Tutorial from R package:Hofner12mboostR.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_1},
  keywords  = {Boosting; Component-wise functional gradient descent; Generalized additive models; Tutorial},
  language  = {English},
  owner     = {sotterson},
  publisher = {Springer-Verlag},
  timestamp = {2013.11.19},
}

@Misc{Ortega09scenarioDyn,
  author    = {Ortega, Juan-Pablo and Pullirsch, Rainer and Teichmann, Josef and Wergieluk, Julian},
  title     = {A dynamic approach for scenario generation in Risk management},
  month     = aug,
  year      = {2009},
  note      = {arXiv:0904.0624v2 [q-fin.RM]},
  abstract  = {We provide a new dynamic approach to scenario generation for the purposes of risk management in the banking industry. We connect ideas from conventional techniques -- like historical and Monte Carlo simulation -- and we come up with a hybrid method that shares the advantages of standard procedures but eliminates several of their drawbacks. Instead of considering the static problem of constructing one or ten day ahead distributions for vectors of risk factors, we embed the problem into a dynamic framework, where any time horizon can be consistently simulated. Additionally, we use standard models from mathematical finance for each risk factor, whence bridging the worlds of trading and risk management. Our approach is based on stochastic differential equations (SDEs), like the HJM-equation or the Black-Scholes equation, governing the time evolution of risk factors, on an empirical calibration method to the market for the chosen SDEs, and on an Euler scheme (or high-order schemes) for the numerical evaluation of the respective SDEs. The empirical calibration procedure presented in this paper can be seen as the SDE-counterpart of the so called Filtered Historical Simulation method; the behavior of volatility stems in our case out of the assumptions on the underlying SDEs. Furthermore, we are able to easily incorporate "middle-size" and "large-size" events within our framework always making a precise distinction between the information obtained from the market and the one coming from the necessary a-priori intuition of the risk manager. Results of one concrete implementation are provided.},
  comment   = {spinning reservers},
  file      = {Ortega09scenarioDyn.pdf:Ortega09scenarioDyn.pdf:PDF},
  keywords  = {Quantitative Finance - Risk Management,Quantitative Finance - Computational Finance},
  owner     = {scot},
  refid     = {http://www.scientificcommons.org/43868052},
  timestamp = {2010.11.24},
  url       = {http://arxiv.org/abs/0904.0624v2},
}

@Article{Barron98minDescLen,
  author    = {Barron, A.; Rissanen, J.; Bin Yu},
  title     = {The minimum description length principle in coding and modeling},
  journal   = {Information Theory, IEEE Transactions on},
  year      = {1998},
  volume    = {44},
  number    = {6},
  pages     = {2743--2760},
  month     = oct,
  issn      = {0018-9448},
  abstract  = {We review the principles of minimum description length and stochastic complexity as used in data compression and statistical modeling. Stochastic complexity is formulated as the solution to optimum universal coding problems extending Shannon's basic source coding theorem. The normalized maximized likelihood, mixture, and predictive codings are each shown to achieve the stochastic complexity to within asymptotically vanishing terms. We assess the performance of the minimum description length criterion both from the vantage point of quality of data compression and accuracy of statistical inference. Context tree modeling, density estimation, and model selection in Gaussian linear regression serve as examples},
  comment   = {Base reference for minimum description length coding, used in diarization a lot. MDL is equivalent to BIC for parametric models but is different for non-parametric models (and can be used there, where BIC can't [[vijayasenan07diarizAgglomInfoBotneck]},
  doi       = {10.1109/18.720554},
  file      = {Barron98minDescLen.pdf:Barron98minDescLen.pdf:PDF;Barron98minDescLen.pdf:Barron98minDescLen.pdf:PDF},
  keywords  = {computational complexity, data compression, information theory, maximum likelihood estimation, modelling, prediction theory, reviews, source coding, statistical analysis, stochastic processesGaussian linear regression, Shannon coding, context tree modeling, data compression, density estimation, minimum description length principle, mixture coding, model selection, normalized maximized likelihood coding, optimum universal coding problem, predictive coding, review, source coding theorem, statistical inference, statistical modeling, stochastic complexity},
  owner     = {scotto},
  timestamp = {2008.02.01},
  url       = {http://ieeexplore.ieee.org.offcampus.lib.washington.edu/search/srchabstract.jsp?arnumber=720554&isnumber=15554&punumber=18&k2dockey=720554@ieeejrns&query=%28%28the+minimum+description%0D%0Alength+principle+in+coding+and+modeling%29%3Cin%3Emetadata%29&pos=0},
}

@Article{Langford12qrViaClassif,
  author    = {Langford, John and Oliveira, Roberto and Zadrozny, Bianca},
  title     = {Predicting conditional quantiles via reduction to classification},
  journal   = {arXiv preprint arXiv:1206.6860},
  year      = {2012},
  abstract  = {We show how to reduce the process of predicting conditional quantiles (and the median in particular) to solving classiffication. The accompanying theoretical statement shows that the regret of the classiffier bounds the regret of the quantile regression under a quantile loss. We also test this reduction empirically against existing quantile regression methods on large real-world datasets and discover that it provides state-of-the- art performance.},
  comment   = {Quantile regression via classification (interesting b/c there are a gazillion potential high dim classification algorithms). Better than linQR, maybe better than KDE but much, much faster. Somehow, maybe this could be a regime detector too, since it's got a classifier in there.},
  file      = {Langford12qrViaClassif.pdf:Langford12qrViaClassif.pdf:PDF},
  groups    = {doReadNonWPV_1},
  owner     = {sotterson},
  timestamp = {2014.03.30},
  url       = {http://arxiv.org/abs/1206.6860},
}

@InProceedings{Csato01onlineLearnWindField,
  author    = {Csat?, Lehel and Dan Cornford and Manfred Opper},
  title     = {Online Learning of Wind-Field Models},
  booktitle = {International Conference on Artificial Neural Networks (ICANN)},
  year      = {2001},
  pages     = {300--307},
  publisher = {Springer Verlag},
  abstract  = {We study online approximations to Gaussian process models for spatially distributed systems. We apply our method to the prediction of wind fields over the ocean surface from scatterometer data. Our approach combines a sequential update of a Gaussian approximation to the posterior with a sparse representation that allows to treat problems with a large number of observations.},
  comment   = {Gaussian Processes as prior for combining radar wind measurements into global field

Could use this for:
* multiturbine stuff?
* blending, like in NREL
* including offsite measurements

* solves local problem and ambiguity (Doppler alias??)
* Gaussian process prior on true wind speed (an xy velocity vector at each point)
* output of kernel function is 2x2 matrix: unusual, I think.
* GP has 2-D covariance matrix relating measurements across the field

* numerical weather prediction (NWP) can help w/ this prob, but they don't want to use it here
* says really should do a mixture of GP's since that's what exact method would predict (it's multi-modal in theory)
* approx base on single sweep through data (Cornford03sequentialSparseGaussProc says single sweep can be bad is a problem)

Oline implementation
* Bayesian update includes all past vectors
* this is too many vectors, so removes the ones that won't change mean est.
* seems to work},
  file      = {Csato01onlineLearnWindField.pdf:Csato01onlineLearnWindField.pdf:PDF;Csato01onlineLearnWindField.pdf:Csato01onlineLearnWindField.pdf:PDF},
  groups    = {Read},
  owner     = {scotto},
  timestamp = {2008.10.06},
  url       = {http://ki.cs.tu-berlin.de/publications/opper.html},
}

@Article{Cai12quantRgrsnSemiParamVC,
  author    = {Zongwu Cai and Zhijie Xiao},
  title     = {Semiparametric quantile regression estimation in dynamic models with partially varying coefficients},
  journal   = {Journal of Econometrics},
  year      = {2012},
  volume    = {167},
  number    = {2},
  pages     = {413--425},
  issn      = {0304-4076},
  note      = {Fourth Symposium on Econometric Theory and Applications (SETA)},
  abstract  = {We study quantile regression estimation for dynamic models with partially varying coefficients so that the values of some coefficients may be functions of informative covariates. Estimation of both parametric and nonparametric functional coefficients are proposed. In particular, we propose a three stage semiparametric procedure. Both consistency and asymptotic normality of the proposed estimators are derived. We demonstrate that the parametric estimators are root- n consistent and the estimation of the functional coefficients is oracle. In addition, efficiency of parameter estimation is discussed and a simple efficient estimator is proposed. A simple and easily implemented test for the hypothesis of a varying-coefficient is proposed. A Monte Carlo experiment is conducted to evaluate the performance of the proposed estimators.},
  comment   = {A mix of linear QR (possibly autoregressive) and nonlinear QR. Idea is dimension reduction. Several refs to other dim reduction methods on p. 2. Could be related to adaboost ideas.},
  doi       = {10.1016/j.jeconom.2011.09.025},
  file      = {Cai12quantRgrsnSemiParamVC.pdf:Cai12quantRgrsnSemiParamVC.pdf:PDF},
  groups    = {PointDerived, doReadNonWPV_2},
  keywords  = {Efficiency},
  owner     = {sotterson},
  timestamp = {2013.10.24},
  url       = {http://www.sciencedirect.com/science/article/pii/S0304407611002065},
}

@Article{King01logRgrssnRare,
  author    = {King, Gary and Zeng, Langche},
  title     = {Logistic regression in rare events data},
  journal   = {Political analysis},
  year      = {2001},
  volume    = {9},
  number    = {2},
  pages     = {137--163},
  abstract  = {We study rare events data, binary dependent variables with dozens to thousands of times
fewer ones (events, such as wars, vetoes, cases of political activism, or epidemiological
infections) than zeros (“nonevents”). In many literatures, these variables have proven dif-
ficult to explain and predict, a problem that seems to have at least two sources. First,
popular statistical procedures, such as logistic regression, can sharply underestimate the
probability of rare events. We recommend corrections that outperform existing methods
and change the estimates of absolute and relative risks by as much as some estimated
effects reported in the literature. Second, commonly used data collection strategies are
grossly inefficient for rare events data. The fear of collecting data with too few events has
led to data collections with huge numbers of observations but relatively few, and poorly
measured, explanatory variables, such as in international conflict data with more than a
quarter-million dyads, only a few of which are at war. As it turns out, more efficient sam-
pling designs exist for making valid inferences, such as sampling all available events (e.g.,
wars) and a tiny fraction of nonevents (peace). This enables scholars to save as much
as 99% of their (nonfixed) data collection costs or to collect much more meaningful ex-
planatory variables. We provide methods that link these two results, enabling both types of
corrections to work simultaneously, and software that implements the methods developed.},
  comment   = {It's OK to inflate the training data to include more rare events if you do a statistical correction afterwards.

Maybe relaxes limit in: Peduzzi96numEVvarLogRgrssn
Does eliminate the need for exact?  (King02logistRgrssnMLvsExact)},
  file      = {:King01logRgrssnRare.pdf:PDF},
  publisher = {Cambridge University Press},
  url       = {https://gking.harvard.edu/files/0s.pdf},
}

@Article{Comin13spatDiffTech,
  author   = {Diego A. {Comin} and Mikhail {Dmitriev} and Esteban {Rossi-Hansberg}},
  title    = {The Spatial Diffusion of Technology},
  journal  = {National Bureau of Economic Research},
  year     = {2013},
  abstract = {We study technology di¤usion across countries and over time empirically. We nd signicant
evidence that technology di¤uses slower to locations that are farther away from adoption leaders.
This e¤ect is stronger across rich countries and also when measuring distance along the south-
north dimension. A simple theory of human interactions can account for these empirical ndings.
The theory suggests that the e¤ect of distance should vanish over time, a hypothesis that we
conrm in the data, and that distinguishes technology from other ows like goods or investments.
We then structurally estimate the model. The parameter governing the frequency of interactions
is larger for newer and network-based technologies and for the median technology the frequency
of interactions decays by 73% every 1000 Kms. Overall, we document the signicant role that
geography plays in determining technology di¤usion across countries.},
  comment  = {Estimates the spatial coefficient of technology diffusion using data from many countries (I think this data is available)},
  file     = {:Comin13spatDiffTech.pdf:PDF},
  groups   = {[Scott:]},
  url      = {https://academic.microsoft.com/search?q=The%20Spatial%20Diffusion%20of%20Technology&f=&orderBy=0&skip=0&take=10},
}

@InCollection{Montufar2014numLinRegnDeepNN,
  author    = {Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  title     = {On the Number of Linear Regions of Deep Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 27},
  publisher = {Curran Associates, Inc.},
  year      = {2014},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  pages     = {2924--2932},
  abstract  = {We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.

Keywords: Deep learning, neural network, input space partition, rectifier, maxout},
  comment   = {How NN complexity increases w/ # hidden units for piecwise linear activations.

See also
Pascanu2013numRespRgnsDeepNNpieceLin
Bengio09lrnDeepArchAI},
  file      = {:Montufar2014numLinRegnDeepNN.pdf:PDF},
  url       = {http://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks.pdf},
}

@Article{Selen08layLinRgrOrder,
  author    = {Yngve Sel{\'e}n and Erik G. Larsson},
  title     = {Empirical {Bayes} linear regression with unknown model order},
  journal   = {Digital Signal Processing},
  year      = {2008},
  volume    = {18},
  number    = {2},
  pages     = {236--248},
  issn      = {1051-2004},
  abstract  = {We study the maximum a posteriori probability model order selection algorithm for linear regression models, assuming Gaussian distributed noise and coefficient vectors. For the same data model, we also derive the minimum mean-square error coefficient vector estimate. The approaches are denoted BOSS (Bayesian Order Selection Strategy) and BPM (Bayesian Parameter estimation Method), respectively. Both BOSS and BPM require a priori knowledge on the distribution of the coefficients. However, under the assumption that the coefficient variance profile is smooth, we derive ?empirical Bayesian? versions of our algorithms, which require little or no information from the user. We show in numerical examples that the estimators can outperform several classical methods, including the well-known AIC and BIC for order selection.},
  comment   = {Bayesian linear regression w/ order selection; good perf on data which meets its questionable assumptions; has Matlab
* Some possible problems
-- assumes that coeffs are smooth (why would they be?)
-- may assume that they decrease exponentially (why would they?)
-- still requires specifying hyper params
-- may require cross-validation
* anyway, it works on synthetic data that meets its assumptions e.g. smooth coeffs (which will break as soon as features are in a different order!)
* compare with Minka
* Matlab: http://www.it.uu.se/katalog/ys/software/empBOSS_BPM},
  doi       = {DOI: 10.1016/j.dsp.2007.03.005},
  file      = {Selen08layLinRgrOrder.pdf:Selen08layLinRgrOrder.pdf:PDF;Selen08layLinRgrOrder.pdf:Selen08layLinRgrOrder.pdf:PDF},
  groups    = {Read},
  keywords  = {Linear regression},
  owner     = {sotterson},
  timestamp = {2009.06.18},
  url       = {http://www.sciencedirect.com/science/article/B6WDJ-4NCR975-1/2/520e27156c9927434db7e8e937a49438},
}

@InProceedings{Das08sensSelWorstCase,
  author    = {Das, Abhimanyu and Kempe, David},
  title     = {Sensor Selection for Minimizing Worst-Case Prediction Error},
  booktitle = {International Conference on Information Processing in Sensor Networks (IPSN)},
  year      = {2008},
  pages     = {97--108},
  publisher = {IEEE Computer Society},
  abstract  = {We study the problem of choosing the "best" subset of ksensors to sample from among a sensor deployment of n > k sensors, in order to predict aggregate functions over all the sensor values. The sensor data being measured are assumed to be spatially correlated, in the sense that the values at two sensors can differ by at most a monotonically increasing, concave function of their distance. The goal is then to select a subset of sensors so as to minimize the prediction error, assuming that the actual values at unsampled sensors are worst-case subject to the constraints imposed by their distances from sampled sensors.Even selecting sensors for the optimal prediction of the mean, maximum or minimum is NP-hard; we present approximation algorithms to select near-optimal subsets of k sensors that minimize the worst-case prediction error. In general, we show that for any aggregate function satisfying certain concavity, symmetry and monotonicity conditions, the sensor selection problem can be modeled as a k-median clustering problem, and solved using efficient approximation algorithms designed for k-median clustering.Our theoretical results are complemented by experiments on tworeal-world sensor data sets; our experiments confirm that ouralgorithms lead to prediction errors that are usually less thanthe (normalized) standard deviation of the test data, using only around 10\% of the sensors.},
  comment   = {Could use for targeted observations or for generating diverse ensembles},
  doi       = {10.1109/IPSN.2008.40},
  file      = {Das08sensSelWorstCase.pdf:Das08sensSelWorstCase.pdf:PDF},
  groups    = {Ensemble, doReadWPV_2},
  isbn      = {978-0-7695-3157-1},
  location  = {Washington, DC, USA},
  owner     = {scot},
  timestamp = {2010.08.02},
}

@Unpublished{Anitescu11hiOrdCIstochProgBS,
  author    = {Mihai Anitescu and C. G. Petra},
  title     = {Higher-Order Confidence Intervals for Stochastic Programming Using Bootstrapping},
  note      = {Submitted to Mathematical Programming},
  month     = {10/2011},
  year      = {2011},
  abstract  = {We study the problem of constructing confiddence intervals for the optimal value of a stochastic programming problem by using bootstrapping. Bootstrapping is a resampling method used in the statistical inference of unknown parameters for which only a small number of samples can be obtained. One such parameter is the optimal value of a stochastic optimization problem involving complex spatio- temporal uncertainty, for example coming from weather prediction. However, bootstrapping works provably better than traditional inference technique based on the central limit theorem only for parameters that are finite-dimensional and smooth functions of the moments, whereas the optimal value of the stochastic optimization problem is not. In this paper we propose and analyze a new bootstrap-based estimator for the optimal value that gives higher-order confidence intervals.},
  comment   = {Bootstrapping inside of stochastic programming. Idea is to use fewer very expensive samples (NWP forecasts). Method has a lot of limitations

Might be a good way to find out about the limitations of bootstrapped CI in stoch. opt. Also, about the requirements for pdfs/scenarios in stoch. opt.

Was submitted in 2011, I think, but not published yet in Oct. 2013},
  file      = {Anitescu11hiOrdCIstochProgBS.pdf:Anitescu11hiOrdCIstochProgBS.pdf:PDF},
  groups    = {Use, doReadNonWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.01},
  url       = {http://www.mcs.anl.gov/publication/higher-order-confidence-intervals-stochastic-programming-using-bootstrapping},
}

@InProceedings{Nilsson07rgrsnManifKernDim,
  author    = {Nilsson, Jens and Sha, Fei and Jordan, Michael I.},
  title     = {Regression on manifolds using kernel dimension reduction},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2007},
  pages     = {697--704},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {We study the problem of discovering a manifold
that best preserves information relevant to a nonlinear
regression. Solving this problem involves
extending and uniting two threads of research.
On the one hand, the literature on sufficient dimension
reduction has focused on methods for
finding the best linear subspace for nonlinear regression;
we extend this to manifolds. On the
other hand, the literature on manifold learning
has focused on unsupervised dimensionality reduction;
we extend this to the supervised setting.
Our approach to solving the problem involves
combining the machinery of kernel dimension reduction
with Laplacian eigenmaps. Specifically,
we optimize cross-covariance operators in kernel
feature spaces that are induced by the normalized
graph Laplacian. The result is a highly flexible
method in which no strong assumptions are made
on the regression function or on the distribution
of the covariates. We illustrate our methodology
on the analysis of global temperature data and
image manifolds.},
  comment   = {use for finding/tuning lagged velocity basis function? Better than sufficient dim red.?},
  doi       = {10.1145/1273496.1273584},
  file      = {Nilsson07rgrsnManifKernDim.pdf:Nilsson07rgrsnManifKernDim.pdf:PDF},
  isbn      = {978-1-59593-793-3},
  location  = {Corvalis, Oregon},
  owner     = {scot},
  timestamp = {2010.08.31},
}

@Article{Das11subModSpectGreedSlct,
  author    = {Das, A. and Kempe, D.},
  title     = {Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection},
  journal   = {Arxiv preprint arXiv:1102.3975},
  year      = {2011},
  abstract  = {We study the problem of selecting a subset of k random variables from a large set, in order to obtain the best linear prediction of another variable of interest. This problem can be viewed in the context of both feature selection and sparse approximation. We analyze the performance of widely used greedy heuristics, using insights from the maximization of submodular functions and spectral analysis. We introduce the submodularity ratio as a key quantity to help understand why greedy algorithms perform well even when the variables are highly correlated. Using our techniques, we obtain the strongest known approximation guarantees for this problem, both in terms of the submodularity ratio and the smallest k-sparse eigenvalue of the covariance matrix. We further demonstrate the wide applicability of our techniques by analyzing greedy algorithms for the dictionary selection problem, and signi?cantly improve the previously known guarantees. Our theoretical analysis is complemented by experiments on real-world and synthetic data sets; the experiments show that the submodularity ratio is a stronger predictor of the performance of greedy algorithms than other spectral parameters.},
  comment   = {Use this for wrapper, nonlinear feat sel? Do the featsel lookahead using covariance matrix Rsquared ratio, which has submodularity guarantee.? Would this give a hint about a bound on what the multi-dim MI would have been if it had been computed? Or, could it bound the set of combinatoric multi-dim MI's that need to be computed?},
  file      = {Das11subModSpectGreedSlct.pdf:Das11subModSpectGreedSlct.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2012.08.13},
  url       = {http://arxiv.org/abs/1102.3975},
}

@Article{Renaud05wvltFiltPred,
  author    = {Renaud, O. and Starck, J-L and Murtagh, F.},
  title     = {Wavelet-based combined signal filtering and prediction},
  journal   = {Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on},
  year      = {2005},
  volume    = {35},
  number    = {6},
  pages     = {1241--1251},
  month     = dec,
  issn      = {1083-4419},
  abstract  = {We survey a number of applications of the wavelet transform in time series prediction. We show how multiresolution prediction can capture short-range and long-term dependencies with only a few parameters to be estimated. We then develop a new multiresolution methodology for combined noise filtering and prediction, based on an approach which is similar to the Kalman filter. Based on considerable experimental assessment, we demonstrate the powerfulness of this methodology.},
  comment   = {Good explanation of how to use wavelets in prediction. Uses Haar to avoid boundary problems. Kind of like lag selection.},
  doi       = {10.1109/TSMCB.2005.850182},
  file      = {Renaud05wvltFiltPred.pdf:Renaud05wvltFiltPred.pdf:PDF},
  keywords  = {Kalman filters;autoregressive processes;filtering theory;prediction theory;signal denoising;time series;wavelet transforms;Kalman filter;autoregression;multiresolution prediction;noise filtering;noise prediction;signal filtering;signal prediction;time series prediction;wavelet transform;Continuous wavelet transforms;Discrete wavelet transforms;Filtering;Image reconstruction;Noise reduction;Parameter estimation;Predictive models;Signal resolution;Wavelet coefficients;Wavelet transforms;Autoregression;Kalman filter;filtering;forecasting;model;resolution;scale;time series;wavelet transform;Algorithms;Computer Simulation;Models, Statistical;Regression Analysis;Signal Processing, Computer-Assisted;Stochastic Processes;Time Factors},
  owner     = {sotterson},
  timestamp = {2013.03.15},
}

@Article{Patton06modAsymDep,
  author    = {Patton, Andrew J.},
  title     = {MODELLING ASYMMETRIC EXCHANGE RATE DEPENDENCE},
  journal   = {International Economic Review},
  year      = {2006},
  volume    = {47},
  number    = {2},
  pages     = {527--556},
  issn      = {1468-2354},
  abstract  = {We test for asymmetry in a model of the dependence between the Deutsche mark and the yen, in the sense that a different degree of correlation is exhibited during joint appreciations against the U.S. dollar versus during joint depreciations. We consider an extension of the theory of copulas to allow for conditioning variables, and employ it to construct flexible models of the conditional dependence structure of these exchange rates. We find evidence that the mark/dollar and yen/dollar exchange rates are more correlated when they are depreciating against the dollar than when they are appreciating.},
  comment   = {Conditional dependence in copulas -- different response depending upon slopes. Possibly useful because ramps have such asymmetries (I think I remember reading). Could be an improvement to Pierre's conditional dependence copula thing (they used fuzzy sets to model the dependence, I think).},
  doi       = {10.1111/j.1468-2354.2006.00387.x},
  groups    = {PointDerived, doReadNonWPV_2},
  owner     = {scot},
  publisher = {Blackwell Publishing, Inc.},
  timestamp = {2010.12.06},
}

@InProceedings{Li99windPowNeuralKalmanSIMD,
  author    = {Shuhui Li and Wunsch, D.C. and O'Hair, E. and Giesselmann, M.G.},
  title     = {Wind turbine power estimation by neural networks with {Kalman} filter training on a {SIMD} parallel machine},
  booktitle = {International Joint Conference on Neural Networks (IJCNN)},
  year      = {1999},
  volume    = {5},
  pages     = {3430--3434},
  doi       = {10.1109/IJCNN.1999.836215},
  url       = {http://ieeexplore.ieee.org.offcampus.lib.washington.edu/search/srchabstract.jsp?arnumber=836215&isnumber=17983&punumber=6674&k2dockey=836215@ieeecnfs&query=((wind+turbine+power+estimation+by+neural+networks+with+kalman+filter+training+on+a+simd+parallel+machine)%3Cin%3Emetadata)&pos=0&access=no},
  abstract  = {We use a multi-layer perceptron (MLP) network to estimate wind turbine power generation. Wind power can be influenced by many factors such as wind speeds, wind directions, terrain, air density, vertical wind profile, time of day, and seasons of the year. It is usually important to train a neural network with multiple influence factors and big training data set. We have parallelized the extended Kalman filter (EKF) training algorithm, which can provide fast training even for large training data sets. The MLP network is then trained with the consideration of various possible factors, which can influence turbine power production. The performance of the trained network is studied from the point of view of information presented to the network through network inputs regarding different affecting factors and large training data set covering all the seasons of a year},
  file      = {Li99windPowNeuralKalmanSIMD.pdf:Li99windPowNeuralKalmanSIMD.pdf:PDF;Li99windPowNeuralKalmanSIMD.pdf:Li99windPowNeuralKalmanSIMD.pdf:PDF},
  journal   = {International Joint Conference on Neural Networks (IJCNN)},
  keywords  = {Kalman filters, learning (artificial intelligence), multilayer perceptrons, nonlinear filters, parallel machines, power engineering computing, wind turbinesKalman filter training, SIMD parallel machine, air density, extended Kalman filter, terrain, time of day, vertical wind profile, wind directions, wind speeds, wind turbine power estimation},
  owner     = {scotto},
  timestamp = {2008.07.06},
}

@Article{Litvinenko17approxLikHierCovMatSpatial,
  author   = {Litvinenko, Alexander and Sun, Ying and Genton, Marc G and Keyes, David},
  title    = {Likelihood Approximation With Hierarchical Matrices For Large Spatial Datasets},
  journal  = {arXiv preprint arXiv:1709.04419},
  year     = {2017},
  abstract = {We use available measurements to estimate the unknown parameters (variance, smoothness
parameter, and covariance length) of a covariance function by maximizing the joint Gaussian
log-likelihood function. To overcome cubic complexity in the linear algebra, we approximate the
discretized covariance function in the hierarchical (H-) matrix format. The H-matrix format has
a log-linear computational cost and storage O(kn log n), where the rank k is a small integer and
n is the number of locations. The H-matrix technique allows us to work with general covariance
matrices in an efficient way, since H-matrices can approximate inhomogeneous covariance func-
tions, with a fairly general mesh that is not necessarily axes-parallel, and neither the covariance
matrix itself nor its inverse have to be sparse. We demonstrate our method with Monte Carlo
simulations and an application to soil moisture data. The C, C++ codes and data are freely
available.

Keywords: Computational statistics; Hierarchical matrix; Large dataset; Mat´ern covariance;
Random Field; Spatial statistics.},
  comment  = {A decomposition of a giant cov. matrix, used for approximating Gaussian likelihood. Particularly interesting is that it can handle inhomogeneous covariance functions -- no need for forcing a form like in Tastu15spcTimeTrajGaussCpla and the others.  Has C/C++.  Could be used for giant spatio-temporal copulas in Prime/IRPWIND?

From: http://andrewgelman.com/2017/10/20/barry-gibb-came-fourth-barry-gibb-look-alike-contest/
The methods includes "hierarchical decompositions of the covariance matrix (there is also code). This is a really neat method for solving the problem and a really exciting new idea in the field."

},
  file     = {:Litvinenko17approxLikHierCovMatSpatial.pdf:PDF},
  url      = {https://arxiv.org/abs/1709.04419},
}

@InProceedings{Yan06weatherIDnwp,
  author    = {Wong Ka Yan and Yip Chi Lap and Li Ping Wah},
  title     = {Identifying Weather Systems from Numerical Weather Prediction Data},
  booktitle = {Pattern Recognition, International Conference on},
  year      = {2006},
  volume    = {4},
  pages     = {841--844},
  abstract  = {Weather systems such as tropical cyclones, fronts, troughs and ridges affect our daily lives. Yet, they are often manually located and drawn on weather charts based on forecasters' experience. To identify them, multiple atmospheric elements need to be considered, and the results may vary among forecasters. In this paper, we contribute to the fields of pattern recognition and meteorological computing by designing a generic model of weather systems, along with a genetic algorithm-based framework for finding them from multidimensional numerical weather prediction data. It was found that our method not only can locate weather systems with 80\% to 100\% precision, but also discover features that could indicate the genesis or dissipation of such systems that could be ignored by forecasters},
  comment   = {Paddy thought this might be useful for ramp event ID},
  doi       = {10.1109/ICPR.2006.677},
  file      = {Yan06weatherIDnwp.pdf:Yan06weatherIDnwp.pdf:PDF;Yan06weatherIDnwp.pdf:Yan06weatherIDnwp.pdf:PDF},
  issn      = {1051-4651},
  keywords  = {atmospheric techniques, feature extraction, geophysics computing, storms, weather forecastingfeature discovery, fronts, meteorological computing, multidimensional numerical weather prediction data, pattern recognition, ridges, system dissipation, system genesis, tropical cyclones, troughs, weather charts, weather system identification},
  owner     = {scotto},
  timestamp = {2008.07.07},
  url       = {http://ieeexplore.ieee.org.offcampus.lib.washington.edu/search/srchabstract.jsp?arnumber=1699971&isnumber=35820&punumber=11159&k2dockey=1699971@ieeecnfs&query=((+identifyingweather+systems+from+numericalweather+prediction+data+)%3Cin%3Emetadata)&pos=0&access=no},
}

@Book{Hyndman14frcstPrincPractice,
  title     = {Forecasting: principles and practice},
  publisher = {OTexts},
  year      = {2014},
  author    = {Hyndman, Rob J and Athanasopoulos, George},
  abstract  = {Welcome to our online textbook on forecasting. This textbook is intended to provide a comprehensive introduction to forecasting methods and to present enough information about each method for readers to be able to use them sensibly. We don?t attempt to give a thorough discussion of the theoretical details behind each method, although the references at the end of each chapter will fill in many of those details. The book is written for three audiences: (1) people finding themselves doing forecasting in business when they may not have had any formal training in the area; (2) undergraduate students studying business; (3) MBA students doing a forecasting elective. We use it ourselves for a second-year subject for students undertaking a Bachelor of Commerce degree at Monash University, Australia.

For most sections, we only assume that readers are familiar with algebra, and high school mathematics should be sufficient background. Readers who have completed an introductory course in statistics will probably want to skip some of Chapters 2 and 4. There are a couple of sections which require knowledge of matrices, but these are flagged.

At the end of each chapter we provide a list of ?further reading?. In general, these lists comprise suggested textbooks that provide a more advanced or detailed treatment of the subject. Where there is no suitable textbook, we suggest journal articles that provide more information.

We use R throughout the book and we intend students to learn how to forecast with R. R is free and available on almost every operating system. It is a wonderful tool for all statistical analysis, not just for forecasting. See Using R for instructions on installing and using R. The book is different from other forecasting textbooks in several ways.

 It is free and online, making it accessible to a wide audience.
 It uses R, which is free, open-source, and extremely powerful software.
 It is continuously updated. You don?t have to wait until the next edition for errors to be removed or new methods to be discussed. We will update the book frequently.
 There are dozens of real data examples taken from our own consulting practice. We have worked with hundreds of businesses and organizations helping them with forecasting issues, and this experience has contributed directly to many of the examples given here, as well as guiding our general philosophy of forecasting.
 We emphasise graphical methods more than most forecasters. We use graphs to explore the data, analyse the validity of the models fitted and present the forecasting results.

Use the table of contents on the right to browse the book. If you have any comments or suggestions on what is here so far, feel free to add them on the book page.

Happy forecasting!
Rob J Hyndman
George Athanasopoulos
May 2012.},
  comment   = {Nice Forecasting ebook. Most up-to-date version is html on a website but can buy a book or a pdf.

Good comparison of exponential weighting and ARIMA.  Lots of other stuff too.

},
  file      = {Book Notes:Hyndman14frcstPrincPractice_notes.pdf:PDF},
  url       = {https://www.otexts.org/fpp},
}

@Article{Buja05lossFuncBinClassProb,
  author        = {Buja, Andreas and Stuetzle, Werner and Shen, Yi},
  title         = {Loss functions for binary class probability estimation and classification: Structure and applications},
  journal       = {Working draft, November},
  year          = {2005},
  volume        = {3},
  __markedentry = {[Scott:1]},
  abstract      = {What are the natural loss functions or fitting criteria for binary class probability
estimation? This question has a simple answer: so-called “proper scoring rules”, that
is, functions that score probability estimates in view of data in a Fisher-consistent
manner. Proper scoring rules comprise most loss functions currently in use: log-loss,
squared error loss, boosting loss, and as limiting cases cost-weighted misclassification
losses. Proper scoring rules have a rich structure:
* Every proper scoring rules is a mixture (limit of sums) of cost-weighted misclassifi-
cation losses. The mixture is specified by a weight function (or measure) that describes
which misclassification cost weights are most emphasized by the proper scoring rule.
* Proper scoring rules permit Fisher scoring and Iteratively Reweighted LS algo-
rithms for model fitting. The weights are derived from a link function and the above
weight function.
* Proper scoring rules are in a 1-1 correspondence with information measures for
tree-based classification.
* Proper scoring rules are also in a 1-1 correspondence with Bregman distances that
can be used to derive general approximation bounds for cost-weighted misclassification
errors, as well as generalized bias-variance decompositions.
We illustrate the use of proper scoring rules with novel criteria for 1) Hand and
Vinciotti’s (2003) localized logistic regression and 2) for interpretable classification
trees. We will also discuss connections with exponential loss used in boosting.

Keywords: Boosting, stagewise regression, machine learning, proper scoring rules,
proper score functions, information measures, entropy, Gini index, Bregman distances, link
functions, binary response data, stumps, tree-based classification, CART, logistic regression,
Fisher scoring, iteratively reweighted least squares, stagewise fitting.},
  comment       = {What proper binary class prediction probability scoring rules have in common and other intersting points:

* Proper scoring rules can be taylored to tune algorithms with arbitrary costs e.g. for false or missed detect
* It is shown how boosting can generalize well for classification even though its estimation of probabilities is poor
* IRLS can be used for boosting
* tailored proper scores can be used to estimate tree-based models.},
  file          = {:Buja05lossFuncBinClassProb.pdf:PDF},
  url           = {http://www-stat.wharton.upenn.edu/~buja/PAPERS/paper-proper-scoring.pdf},
}

@Book{Mai14financeEngCopulasExpln,
  title     = {Financial engineering with copulas explained},
  publisher = {Springer},
  year      = {2014},
  author    = {Mai, J and Scherer, Matthias},
  isbn      = {978-1137346308},
  abstract  = {What can you expect from this book? We aim to provide you with an easy-to-read
introduction to current problems (and solutions, of course) in the field of dependence
modeling as it is required in today?s financial and insurance industry. If you enjoy
reading a chapter of the book after a long day at work or during a continental
flight, and understand the essence of the exposition, then we have succeeded. Clearly,
?easy-to-read? strongly depends on your mathematical training. We take as granted
familiarity with probability calculus and elementary statistics.1 Aimed at readers
from the financial industry, we try to illustrate the theory with real world examples.
We are always mathematically precise, but we do not aim at being complete with
respect to proofs and the latest generalizations of the presented results. Instead,
we visualize the results and explain how they can be applied. Finally, we direct
the interested reader to deeper literature, where proofs and further results are
given. The field of dependence modeling has grown impressively in recent years.2
Having said this, it is clear that this introduction has to prioritize certain aspects,
but we provide many references for those readers with an appetite for further
material.},
  file      = {Mai14financeEngCopulasExpln.pdf:Mai14financeEngCopulasExpln.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.02},
  url       = {http://design-boutique.co/by_jan_frederik_mai_financial_engineering_with_copulas_explained_financial_engineering_explained_paperback.pdf},
}

@Article{Skytte99powRegNordPool,
  author    = {Skytte, Klaus},
  title     = {The regulating power market on the {Nordic} power exchange {Nord Pool}: an econometric analysis},
  journal   = {Energy Economics},
  year      = {1999},
  volume    = {21},
  number    = {4},
  pages     = {295--308},
  issn      = {0140-9883},
  abstract  = {What differentiates the structure of Nord Pool from other power exchanges around the world is the way the balance from the spot market is maintained until the actual, physical delivery takes place, via the regulating power market in Norway. This paper reveals the pattern of the prices on the regulating power market, by analysing the cost of being unable to fulfil the commitments made on the spot market. Some power producers with unpredictable fluctuations (e.g. wind) will need to buy regulation services. The disclosed pattern implies that these producers must pay a limited premium of readiness in addition to the spot price; this premium is independent of the amount of regulation. The level of the premium of readiness for down-regulation is shown to be strongly influenced by the level of the spot price. On the other hand, it is demonstrated that the premium for up-regulation is less correlated to the spot price. Furthermore, it is found that the amount of regulation affects the price of regulating power for up-regulation more strongly than it does for down-regulation. The disclosed cost of using the regulating power market is a quadratic function of the amount of regulation. This asymmetric cost may encourage bidders with fluctuating production to be more strategic in their way of bidding on the spot market. By using such strategies the extra costs (for example wind power) needed to counter unpredictable fluctuations may be limited.},
  comment   = {Pierre says this suggests that poor market design can cause bad bidding that increases GHG's},
  file      = {Skytte99powRegNordPool.pdf:Skytte99powRegNordPool.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.12.10},
}

@Misc{Essoe14oddsProbRisk,
  author       = {Joey KaYee Essoe},
  title        = {Odds Are: On the difference between odds, probability, and risk ratio},
  howpublished = {Psychology in Action Blog},
  month        = feb,
  year         = {2014},
  abstract     = {What does it mean to say “smokers are X times more likely to get lung cancer than non-smokers?” What about when the weather channel says, “there is a 10% chance of rain?” The odds of 1 to 10 of winning?

These words are often used in casual conversations as somewhat interchangeable, and can be rather confusing. I remember being very excited to learn about them for the first time, so hopefully you will find this as interesting (or at least as clarifying!) as I did!},
  comment      = {Some definite equations for Risk, Odds and their ratios.   Somewhat more clear than other things.

Also: Ranganathan15oddsVsRisk},
  url          = {https://www.psychologyinaction.org/psychology-in-action-1/2014/02/03/odds-are-on-the-difference-between-odds-probability-and-risk-ratio},
}

@Article{Kuncheva14PCAFeatureExtraction,
  author    = {L. I. Kuncheva and W. J. Faithfull},
  title     = {{PCA} Feature Extraction for Change Detection in Multidimensional Unlabeled Data},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  year      = {2014},
  volume    = {25},
  number    = {1},
  pages     = {69--80},
  month     = jan,
  issn      = {2162-237X},
  abstract  = {When classifiers are deployed in real-world applications, it is assumed that the distribution of the incoming data matches the distribution of the data used to train the classifier. This assumption is often incorrect, which necessitates some form of change detection or adaptive classification. While there has been a lot of work on change detection based on the classification error monitored over the course of the operation of the classifier, finding changes in multidimensional unlabeled data is still a challenge. Here, we propose to apply principal component analysis (PCA) for feature extraction prior to the change detection. Supported by a theoretical example, we argue that the components with the lowest variance should be retained as the extracted features because they are more likely to be affected by a change. We chose a recently proposed semiparametric log-likelihood change detection criterion that is sensitive to changes in both mean and variance of the multidimensional distribution. An experiment with 35 datasets and an illustration with a simple video segmentation demonstrate the advantage of using extracted features compared to raw data. Further analysis shows that feature extraction through PCA is beneficial, specifically for data with multiple balanced classes.},
  comment   = {PCA for changepoint detection.  Good for ModernWindABS, espcially because it's unsupervised.

Belloni13progEvalHiDim says featsel (reduction too?) is hard when also doing inference
},
  doi       = {10.1109/TNNLS.2013.2248094},
  file      = {Kuncheva14PCAFeatureExtraction.pdf:Kuncheva14PCAFeatureExtraction.pdf:PDF},
  keywords  = {feature extraction, image classification, image segmentation, principal component analysis, video signal processing, PCA feature extraction, adaptive classification, classification error monitoring, multidimensional distribution, multidimensional unlabeled data, principal component analysis, semiparametric log-likelihood change detection criterion, video segmentation, Covariance matrix, Feature extraction, Gaussian distribution, Hidden Markov models, Monitoring, Principal component analysis, Standards, Change detection, feature extraction, log-likelihood detector, pattern recognition},
  owner     = {sotterson},
  timestamp = {2016.12.19},
}

@Article{R2rt16lstmWrittenMem,
  author    = {R2RT},
  title     = {Written Memories: Understanding, Deriving and Extending the LSTM},
  journal   = {R2RT blog (rtrt.com)},
  year      = {2016},
  month     = jul,
  abstract  = {When I was first introduced to Long Short-Term Memory networks (LSTMs), it was hard to look past their complexity. I didn?t understand why they were designed the way they were designed, just that they worked. It turns out that LSTMs can be understood, and that, despite their superficial complexity, LSTMs are actually based on a couple incredibly simple, even beautiful, insights into neural networks. This post is what I wish I had when first learning about recurrent neural networks (RNNs).

In this post, we do a few things:

We?ll define and describe RNNs generally, focusing on the limitations of vanilla RNNs that led to the development of the LSTM.
We?ll describe the intuitions behind the LSTM architecture, which will enable us to build up to and derive the LSTM. Along the way we will derive the GRU. We?ll also derive a pseudo LSTM, which we?ll see is better in principle and performance to the standard LSTM.
We?ll then extend these intuitions to show how they lead directly to a few recent and exciting architectures: highway and residual networks, and Neural Turing Machines.
This is a post about theory, not implementations. For how to implement RNNs using Tensorflow, check out my posts Recurrent Neural Networks in Tensorflow I and Recurrent Neural Networks in Tensorflow II.},
  comment   = {Nice pictures and derivation of recurrent nueral net LSTM (long short term memories) and extensions.  I used this in my May 31 2017 Tech Coffee Talk.},
  file      = {R2rt16lstmWrittenMem.pdf:R2rt16lstmWrittenMem.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.30},
  url       = {https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html},
}

@Article{Krause08sensPlaceGaussProc,
  author      = {Andreas Krause and Ajit Singh and Carlos Guestrin},
  title       = {Near-Optimal Sensor Placements in {Gauss}ian Processes: Theory, Efficient Algorithms and Empirical Studies},
  journal     = {Journal of Machine Learning Research},
  year        = {2008},
  volume      = {9},
  pages       = {235--284},
  month       = feb,
  abstract    = {When monitoring spatial phenomena, which can often be modeled as Gaussian processes (GPs), choosing sensor locations is a fundamental task. There are several common strategies to address this task, for example, geometry or disk models, placing sensors at the points of highest entropy (variance) in the GP model, and A-, D-, or E-optimal design. In this paper, we tackle the combinatorial optimization problem of maximizing the mutual information between the chosen locations and the locations which are not selected. We prove that the problem of finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1-1/e) of the optimum by exploiting the submodularity of mutual information. We also show how submodularity can be used to obtain online bounds, and design branch and bound search procedures. We then extend our algorithm to exploit lazy evaluations and local structure in the GP, yielding significant speedups. We also extend our approach to find placements which are robust against node failures and uncertainties in the model. These extensions are again associated with rigorous theoretical approximation guarantees, exploiting the submodularity of the objective function. We demonstrate the advantages of our approach towards optimizing mutual information in a very extensive empirical study on two real-world data sets.},
  comment     = {Combinatorial feature selection focused on Gaussian Process Regression; mutual information; missing features too see tutorial video and matlab: http://www.select.cs.cmu.edu/tutorials/icml08submodularity.html},
  file        = {Krause08sensPlaceGaussProc.pdf:Krause08sensPlaceGaussProc.pdf:PDF;Krause08sensPlaceGaussProc.pdf:Krause08sensPlaceGaussProc.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2009.02.20},
  url         = {http://www.select.cs.cmu.edu/publications/scripts/papers.cgi?Krause+Singh+Guestrin:jmlr08sensorplace},
  wwwfilebase = {jmlr2008-krause-singh-guestrin},
  wwwtopic    = {Observation Selection},
}

@Article{Bauer16tdysWeathFrcstImprv,
  author    = {Peter Bauer},
  title     = {Today's weather forecast: Good with a strong chance of improvement},
  journal   = {Earth},
  year      = {2016},
  abstract  = {When people look to their phones, computers or TVs for weather forecasts, they want the forecasts to be accurate and timely. What goes into creating a forecast is rarely a concern, so long as it tells them whether to carry an umbrella or jacket that day, when to evacuate because a hurricane is coming, or when to prepare for a heat wave. It?s the job of the weather scientists, armed with computer models and an array of weather-observing instruments, to make accurate predictions. Yet, even with advancing technology, improving forecasts is challenging.

Weather forecasts have improved significantly since the first numerical, physics-based computer models were implemented in the 1950s. This improvement is on display every day as we observe key weather variables with both ground-based instrument networks and satellites, and compare these variables with current forecasts. The overall improvement in weather forecasting since the mid-20th century has amounted to about a day per decade; in other words, today?s six-day forecast is about as accurate as a five-day forecast 10 years ago.

The trend of increasing accuracy, or skill, of current forecasts holds for both large-scale weather patterns, like frontal systems (think East Coast snow storms), and also for smaller-scale features like precipitation at the surface. In terms of severe weather warnings, a one-day margin translates to enormous savings of lives and property. A one-day gain of skill is therefore invaluable and well worth the investment needed to achieve it.

But what is needed in the future to allow us to continue improving our forecasts?},
  comment   = {Similar to the other Bauer article in Nature.  I grabbed a graph from this one for GIZ Colombia.  See also: Bauer15quietRevNWP},
  file      = {Bauer16tdysWeathFrcstImprv.pdf:Bauer16tdysWeathFrcstImprv.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.25},
  url       = {https://www.earthmagazine.org/article/todays-weather-forecast-good-strong-chance-improvement},
}

@TechReport{Schaeffer14mkCovMatPosDef,
  author      = {Schaeffer, Larry},
  title       = {Making covariance matrices positive definite},
  institution = {Centre for Genetic Improvement of Livestock, Department of Animal \& Poultry Science},
  year        = {2014},
  abstract    = {When working on simulation projects involving animals and genetics,
genetic parameters are obtained by searching the literature for estimates.
There are seldom any studies that include all of the traits of interest
in one comprehensive analysis. Instead, estimates are scavenged from
several different sources. After putting these estimates together into one
covariance matrix, the likelihood that the matrix is singular is greater
than 0. When the matrix is singular, of course, it cannot be inverted and
it cannot be used for simulating traits on animals. Thus, a method of
forcing the matrix to be positive definite would be useful, as long as most
of the correlations between traits are maintained close to the estimates
that were found. Estimated parameters have non-zero standard errors,
and thus, there is some flexibility in correlations that can be allowed.
Hayes and Hill (1981) presented the “bending” procedure to modify
eigenvalues of singular matrices, and Jorjani et al.(2003) gave a weighted
bending procedure. Finally, Meyer and Kirkpatrick (2010) presented
bending using a penalized maximum likelihood method. To me, however,
the bending procedure causes many correlations and actual variances to
differ more than expected from the original values. For example, if the
variance of one trait was 100, then after bending it could be 70. The
variances at least should not differ very much before and after modifica-
tion.
Thus, over the years I have played with different approaches, until I
think I have one that works, at least to my satisfaction. So the objective
of this little paper is to present the method and share it with whomever
wants to use it. There were some inconsistencies in the previous paper,
and so they have been corrected in this version, hopefully. Also, the
approach is a little different},
  comment     = {Simple way of forcing cov matrix positive semidefiniteness},
  file        = {Tech Note:Schaeffer14mkCovMatPosDef.pdf:PDF},
  url         = {http://www.aps.uoguelph.ca/~lrs/ELARES/PDforce.pdf},
}

@Article{Zivot13portTheoryMatrixEcon424,
  author    = {Eric Zivot},
  title     = {Portfolio Theory with Matrix Algebra. Course Notes, Econ 424/CFRM 462: Computational Finance and Financial Econometrics.},
  journal   = {University of Washington, Seattle},
  year      = {2013},
  abstract  = {When working with large portfolios, the algebra of representing portfolio
expected returns and variances becomes cumbersome. The use of matrix (lin-
ear) algebra can greatly simplify many of the computations. Matrix algebra
formulations are also very useful when it comes time to do actual computa-
tions on the computer. The matrix algebra formulas are easy to translate
into matrix programming languages like R. Popular spreadsheet programs
like Microsoft Excel, which are the workhorse programs of many financial
houses, can also handle basic matrix calculations. All of this makes it worth-
while to become familiar with matrix techniques for portfolio calculations.},
  comment   = {Optimal return given specified risk (standard deviation of return) or min. risk given specifed return.  Has R and a "... with R" textbook coming out soon. Very close but not quite right for ReWP because return return and risk have same units (in ReWP (reserve power), the return is money and the risk is power.  Also, I think this may be making an assumption of normality -- at least it does at the beginning, but I didn't follow the math.

Main point: It's simpler to search possible allocations by searchign for optimal choices.  Two ways:

1. max expected return w/ specified risk
2. min risk w/ specified returns

"risk" is characterized by standard deviation of return.  See p. 11

Exact pdf and slides came from:
https://faculty.washington.edu/ezivot/econ424/portfolioTheoryMatrix.pdf
https://faculty.washington.edu/ezivot/econ424/portfoliotheorymatrixslides.pdf

},
  file      = {:papers\\Zivot13portTheoryMatrixEcon424.pdf:PDF;:papers\\Zivot13portTheoryMatrixEcon424_slides.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.07.03},
  url       = {https://faculty.washington.edu/ezivot/econ424/econ424.htm},
}

@InProceedings{Glorot10undrstndDiffDpLrnFFnets,
  author    = {Glorot, Xavier and Bengio, Yoshua},
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  year      = {2010},
  pages     = {249--256},
  abstract  = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  comment   = {The reference people use when talking about Xavier Initialization for neural net training.

                   Simple blog post summarizing Xavier init: Jones15xavierInitBlog},
  file      = {:Glorot10undrstndDiffDpLrnFFnets.pdf:PDF},
  url       = {http://proceedings.mlr.press/v9/glorot10a.html},
}

@Article{Pinson06qualValProbFrcst,
  author    = {Pinson, P. and Juban, J. and Kariniotakis, G. N.},
  title     = {On the Quality and Value of Probabilistic Forecasts of Wind Generation},
  journal   = {Probabilistic Methods Applied to Power Systems (PMAPS)},
  year      = {2006},
  pages     = {1--7},
  month     = jun,
  abstract  = {While most of the current forecasting methods provide single estimates of future wind generation, some methods now allow one to have probabilistic predictions of wind power. They are often given in the form of prediction intervals or quantile forecasts. Such forecasts, since they include the uncertainty information, can be seen as optimal for the management or trading of wind generation. This paper explores the differences and relations between the quality (i.e. statistical performance) and the operational value of these forecasts. An application is presented on the use of probabilistic predictions for bidding in a European electricity market. The benefits of a probabilistic view of wind power forecasting are clearly demonstrated},
  comment   = {distribution-based trading more profitable than point forecast based; can be worse for C02, though.

Probabilistic forecasts
* build quantiles by stacking center intervals of increasing width

* produced by adative resampling, which I think is worse than quantile regression in Pinson07frcstEval
Forecast quality
* reliability: average match of predicted and actual distributions across time (at some horizon)
* sharpness: average central interval width over time (at some horizon, can't be calibrated)
* resolution: variance of central intervals (at some horizon, can't be calibrated)
* skill: a single number, not clear what it is
* 3 distribution forecasts (based on 3 different point forecasts) have roughly same quality. One a little better.

Profitability of bidding strategies based on point and dist. forecasts compared
* around half of energy on Dutch markets is traded in regulation! (in this simulation)
* bidding strategy -- point forecast: promise to deliver the expected value (point forecast)
-- distribution forecast: Probabilistic Choice (PC), explained in reference 20
* bidding from distribution forecasts better than and point-based bidding strategy
* most accurate distribution produces most profitable bidding
* profit comes from underpromsing electricity, INCREASING bid/production error
-- increases profit b/c on these markets, positive deviations more expensive than negative
-- My comment: this is not so good:
---- underpromising increases the amt. of regulated energy.
---- Usually, more regulated means more society cost
---- fast reacting plants produce more C02 (I think) so this is worse for global warming!},
  doi       = {10.1109/PMAPS.2006.360290},
  file      = {Pinson06qualValProbFrcst.pdf:Pinson06qualValProbFrcst.pdf:PDF},
  groups    = {Read, DOE-PNL09, Test, doReadWPV_2},
  keywords  = {probability, weather forecasting, wind power, wind power plantsEuropean electricity market, probabilistic forecast, wind generation, wind power prediction},
  ncite     = {18},
  owner     = {sotterson},
  timestamp = {2009.03.04},
}

@Article{BenBouallegue15QuantFrcstDiscrimVal,
  author    = {Ben Bouall{\`e}gue, Zied and Pinson, Pierre and Friederichs, Petra},
  title     = {Quantile forecast discrimination ability and value},
  journal   = {arXiv},
  year      = {2015},
  volume    = {141},
  number    = {693},
  pages     = {3415--3424},
  abstract  = {While probabilistic forecast verification for categorical forecasts is well established, some of
the existing concepts and methods have not found their equivalent for the case of continuous
variables. New tools dedicated to the assessment of forecast discrimination ability and forecast
value are introduced here, based on quantile forecasts being the base product for the continuous
case (hence in a nonparametric framework). The relative user characteristic (RUC) curve and
the quantile value plot allow analysing the performance of a forecast for a specific user in a
decision-making framework. The RUC curve is designed as a user-based discrimination tool
and the quantile value plot translates forecast discrimination ability in terms of economic value.
The relationship between the overall value of a quantile forecast and the respective quantile
skill score is also discussed. The application of these new verification approaches and tools is
illustrated based on synthetic datasets, as well as for the case of global radiation forecasts from
the high resolution ensemble COSMO-DE-EPS of the German Weather Service.},
  comment   = {Relation between CRPS and economic value, I think.  For PV and I would guess, wind.},
  file      = {BenBouallegue15QuantFrcstDiscrimVal.pdf:BenBouallegue15QuantFrcstDiscrimVal.pdf:PDF},
  publisher = {Wiley Online Library},
  url       = {http://arxiv.org/abs/1504.04211},
}

@Article{Opdyke03permTestPairMulti,
  author    = {J.D. Opdyke},
  title     = {Fast Permutation Tests that Maximize Power Under Conventional {Monte Carlo} Sampling for Pairwise and Multiple Comparisons},
  journal   = {Journal of Modern Applied Statistical Methods},
  year      = {2003},
  volume    = {2},
  number    = {1},
  pages     = {27--49},
  abstract  = {While the distribution-free nature of permutation tests makes them the most appropriate method for hypothesis testing under a wide range of conditions, their computational demands can be runtime prohibitive, especially if samples are not very small and/or many tests must be conducted (e.g. all pairwise comparisons). This paper presents statistical code that performs continuous-data permutation tests under such conditions very quickly ? often more than an order of magnitude faster than widely available commercial alternatives when many tests must be performed and some of the sample pairs contain a large sample. Also presented is an efficient method for obtaining a set of permutation samples containing no duplicates, thus maximizing the power of a pairwise permutation test under a conventional Monte Carlo approach with negligible runtime cost (well under 1\% when runtimes are greatest). For multiple comparisons, the code is structured to provide an additional speed premium, making permutation-style p-value adjustments practical to use with permutation test p-values (although for relatively few comparisons at a time). ?No-replacement? sampling also provides a power gain for such multiple comparisons, with similarly negligible runtime cost. Key words: Permutation test, Monte Carlo, multiple comparisons, variance reduction, multiple testing procedures, permutation-style p-value adjustments, oversampling, no-replacement sampling},
  comment   = {A permutation test that doesn't look at all perumations and avoids repeating samples (important for Kraskov MI sig test). * Is used in Francois06permTestMutInf to avoid blowing up the Kraskov MI estimator (Kraskov04EstMutInfKNN) * how does this compare to subsampling bootstrap methods? -- In: Geyer06subSampBootStrap (really Politis) -- In:Berg10subSampBSpval},
  file      = {Opdyke03permTestPairMulti.pdf:Opdyke03permTestPairMulti.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.03.09},
  url       = {http://www.datamineit.com/J.D.%20Opdyke%20-%20JMASM%20Vol2%20No1.pdf},
}

@InProceedings{Coates11encodeTrnVQ,
  author    = {Coates, Adam and Ng, Andrew},
  title     = {The importance of encoding versus training with sparse coding and vector quantization},
  booktitle = {Proceedings of the 28\textsuperscript{th} International Conference on Machine Learning (ICML-11)},
  year      = {2011},
  pages     = {921--928},
  abstract  = {While vector quantization (VQ) has been applied
widely to generate features for visual
recognition problems, much recent work has
focused on more powerful methods. In particular,
sparse coding has emerged as a strong
alternative to traditional VQ approaches and
has been shown to achieve consistently higher
performance on benchmark datasets. Both
approaches can be split into a training phase,
where the system learns a dictionary of basis
functions, and an encoding phase, where
the dictionary is used to extract features from
new inputs. In this work, we investigate
the reasons for the success of sparse coding
over VQ by decoupling these phases, allowing
us to separate out the contributions of
training and encoding in a controlled way.
Through extensive experiments on CIFAR,
NORB and Caltech 101 datasets, we compare
several training and encoding schemes, including
sparse coding and a form of VQ with
a soft threshold activation function. Our results
show not only that we can use fast VQ
algorithms for training, but that we can just
as well use randomly chosen exemplars from
the training set. Rather than spend resources
on training, we find it is more important to
choose a good encoder?which can often be
a simple feed forward non-linearity. Our results
include state-of-the-art performance on
both CIFAR and NORB.},
  comment   = {Basis learning not so important as encoding, as long as bases more or less tile the input space -- said to obsolete deep autoencoding, can be merged with deep learning. Good for regime learning?

Blogger who said it obsoleted deep learning: http://danluu.com/linear-hammer/
2 other papers he said obsoleted deep autoencoding:
Ngiam11sparseFiltFtLrn, Coates11ntwkUnSupFtLrn


For regime learning, compare with adaboost methods, starting with Lillywhite13featCnstrct and the papers it references.},
  file      = {Coates11encodeTrnVQ.pdf:Coates11encodeTrnVQ.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.01.28},
  url       = {http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Coates_485.pdf},
}

@InProceedings{Florita13idWindSolarRamps,
  author    = {Florita, A. and Hodge, B.-M. and Orwig, K.},
  title     = {Identifying Wind and Solar Ramping Events},
  booktitle = {Green Technologies Conference, 2013 IEEE},
  year      = {2013},
  pages     = {147--152},
  abstract  = {Wind and solar power are playing an increasing role in the electrical grid, but their inherent power variability can augment uncertainties in the operation of power systems. One solution to help mitigate the impacts and provide more flexibility is enhanced wind and solar power forecasting; however, its relative utility is also uncertain. Within the variability of solar and wind power, repercussions from large ramping events are of primary concern. At the same time, there is no clear definition of what constitutes a ramping event, with various criteria used in different operational areas. Here, the swinging door algorithm, originally used for data compression in trend logging, is applied to identify variable generation ramping events from historic operational data. The identification of ramps in a simple and automated fashion is a critical task that feeds into a larger work of 1) defining novel metrics for wind and solar power forecasting that attempt to capture the true impact of forecast errors on system operations and economics, and 2) informing various power system models in a data-driven manner for superior exploratory simulation research. Both allow inference on sensitivities and meaningful correlations, as well as quantify the value of probabilistic approaches for future use in practice.},
  comment   = {Uses swinging doors to calc ramp slopes and boundaries then makes some summary graphs. There seems to be one mistake in them (see comments). Otherwise, not too interesting.},
  doi       = {10.1109/GreenTech.2013.30},
  file      = {Florita13idWindSolarRamps.pdf:Florita13idWindSolarRamps.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2014.10.07},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6520043},
}

@InProceedings{Zhang13windPVcorr,
  author    = {Zhang, J and Hodge, BM and Florita, A},
  title     = {Investigating the Correlation Between Wind and Solar Power Forecast Errors in the Western Interconnection},
  booktitle = {ASME 7\textsuperscript{th} International Conference on Energy Sustainability and the 11\textsuperscript{th} Fuel Cell Science, Engineering, and Technology Conference},
  year      = {2013},
  abstract  = {Wind and solar power generation differ from conventional energy generation because of the variable and uncertain nature
of their power output. This variability and uncertainty can have significant impacts on grid operations. Thus, short-term forecasting of wind and solar power generation is uniquely helpful for balancing supply and demand in an electric power system. This paper investigates the correlation between wind
and solar power forecast errors. The forecast and the actual data were obtained from the Western Wind and Solar Integration Study. Both the day-ahead and 4-hour-ahead
forecast errors for the Western Interconnection of the United States were analyzed. A joint distribution of wind and solar power forecast errors was estimated using a kernel density
estimation method; the Pearson?s correlation coefficient between wind and solar forecast errors was also evaluated. The
results showed that wind and solar power forecast errors were
weakly correlated. The absolute Pearson?s correlation
coefficient between wind and solar power forecast errors
increased with the size of the analyzed region. The study is also
useful for assessing the ability of balancing areas to integrate wind and solar power generation.
Keywords: Grid integration, correlation, solar forecasting, western interconnection, wind forecasting, forecasting error
distribution},
  comment   = {These guys DO find correlation between wind and PV forecast errors!},
  file      = {Zhang13windPVcorr.pdf:Zhang13windPVcorr.pdf:PDF},
  groups    = {ErrDistProps, doReadNonWPV_1, PV, Wind},
  owner     = {sotterson},
  timestamp = {2013.09.27},
  url       = {htp://www.nrel.gov/docs/fy13osti/57816.pdf?},
}

@Article{Zhang14jointProbDistWndSolWest,
  author    = {Zhang, Jie and Hodge, Bri-Mathias and Florita, Anthony},
  title     = {Joint probability distribution and correlation analysis of wind and solar power forecast errors in the western interconnection},
  journal   = {Journal of Energy Engineering},
  year      = {2014},
  volume    = {141},
  number    = {1},
  pages     = {B4014008},
  abstract  = {Wind and solar power generation differ from conventional power generation because of the variable and uncertain nature of their power output. This can have significant impacts on grid operations. Short-term forecasting of wind and solar power generation is uniquely helpful for planning the balance of supply and demand in the electric power system because it allows for a reduction in the uncertainty associated with their output. As a step toward assessing the simultaneous integration of large amounts of wind and solar power, this article investigates the spatial and temporal correlation between wind and solar power forecast errors. The forecast and actual data analyzed are obtained from one of the world?s largest regional variable generation integration studies to date. Multiple spatial and temporal scales (day ahead, 4 h ahead, and 1 h ahead) of forecast errors for the Western Interconnection in the United States are analyzed. A joint probability distribution of wind and solar power forecast errors is estimated using kernel density estimation. The Pearson?s correlation coefficient and mutual information between wind and solar power forecast errors are also evaluated. The results show that wind and solar power forecast errors are inversely correlated, and the correlation between wind and solar power forecast errors becomes stronger as the geographic size of the analyzed region increases. The absolute value of the correlation coefficient is generally less than 0.1 in the case of small geographic regions, while it is generally between 0.15 and 0.6 in the case of large geographic regions. The forecast errors are less correlated on the day-ahead timescale, which influences economic operations more than reliability, and more correlated on the 4-h-ahead timescale, where reliability is more impacted by the forecasts. It is also found that the correlation between wind and solar power forecast errors in summer (July) is relatively stronger than in winter (January). The inverse correlation implies that in systems with high penetrations of both wind and solar power, reserves that are held to accommodate the variability of wind or solar power can be at least partially shared. In addition, interesting results are found through time and seasonal variation analyses of wind and solar power forecast errors, and these insights may be uniquely useful to operators who maintain the reliability of the electric power system.},
  file      = {Zhang14jointProbDistWndSolWest.pdf:Zhang14jointProbDistWndSolWest.pdf:PDF},
  owner     = {sotterson},
  publisher = {American Society of Civil Engineers},
  timestamp = {2017.04.13},
  url       = {http://ascelibrary.org/doi/abs/10.1061/(ASCE)EY.1943-7897.0000189},
}

@Article{Alexiadis99windForecastSpatCorr,
  author    = {Alexiadis, M.C. and Dokopoulos, P.S. and Sahsamanoglou, H.S.},
  title     = {Wind speed and power forecasting based on spatial correlation models},
  journal   = {Energy Conversion, IEEE Transactions on},
  year      = {1999},
  volume    = {14},
  number    = {3},
  pages     = {836--842},
  month     = sep,
  issn      = {1558-0059},
  abstract  = {Wind energy conversion systems (WECS) cannot be dispatched like conventional generators. This can pose problems for power system schedulers and dispatchers, especially if the schedule of wind power availability is not known in advance. However, if the wind speed can be reliably forecasted up to several hours ahead, the generating schedule can efficiently accommodate the wind generation. This paper illustrates a technique for forecasting wind speed and power output up to several hours ahead, based on cross correlation at neighboring sites. The authors develop an artificial neural network (ANN) that significantly improves forecasting accuracy comparing to the persistence forecasting model. The method is tested at different sites over a year},
  comment   = {Wind forecast w/ neural net with lag vector of local and distant observations; setup is somewhat artificial
* NN foecasts w/ one or two remote 10m towers from 1-40km apart much better than persistence
* inputs are vectors of lagged wind speed observations
-- no attempt to calculate correlation w/ observations at local site the forecast is being made for
* for long lookahead times distant observations are more correlated with forecast target than local observations!
* lookaheads range from "a few minutes" to 2 hours
* distant observations better for long lookahead times
-- trained NN's w/ and w/o distant obs lag vectors
-- measured error improvement over persistence
-- 5X better improvement over persistence w/ distants obs than w/ local only (2 hr lookahead)
-- 5 pct better at 15 min lookahead
* some kind of power forecast is faked, probably w/ piecewise linear curve; doesn't seem to have been measured
* very short lookahead (minutes?) is improved w/ very close observation towers (0.8m) Setup is somewhat artifical
* towers are all long prevailing winds
* winds that deviate far from prevailing are removed},
  doi       = {10.1109/60.790962},
  file      = {Alexiadis99windForecastSpatCorr.pdf:Alexiadis99windForecastSpatCorr.pdf:PDF},
  groups    = {Read},
  keywords  = {correlation methods, forecasting theory, neural nets, power generation planning, power generation scheduling, power system analysis computing, wind, wind power, wind power plantsartificial neural network, cross correlation, forecasting accuracy, power system dispatchers, power system schedulers, spatial correlation models, wind power availability schedule, wind power forecasting, wind power generation, wind speed forecasting},
  owner     = {sotterson},
  timestamp = {2009.01.12},
}

@Article{Singh07windPowPredNeural,
  author    = {Shikha Singh and T. S. Bhatti and and D. P. Kothari},
  title     = {Wind Power Estimation Using Artificial Neural Network},
  journal   = {Journal of Energy Engineering},
  year      = {2007},
  volume    = {133},
  number    = {1},
  pages     = {46--52},
  abstract  = {Wind energy conversion systems appear as an attractive alternative for electricity generation. To maximize the use of wind
generated electricity when connected to the electric grid, it is important to estimate and predict power produced by wind farms. The power
generated by electric wind turbines changes rapidly because of the continuous fluctuation of wind speed and wind direction. Wind power
can be affected by many other factors such as terrain, air density, vertical wind profile, time of a day, and seasons of a year and usually
fluctuates rapidly, imposing considerable difficulties on the management of combined electric power systems. It is important for the power
industry to have the capability to perform this prediction for diagnostic purposes?lower than expected wind power may be an early
indicator of a need for maintenance. A multilayer perceptron MLP network can be used to estimate wind turbine power generation. It is
usually important to train a neural network with multiple influence factors and big training data set. The extended Kalman filter training
algorithm has to be parallelized so that it can provide fast training even for large training data sets. The MLP network can then be trained
with the consideration of various possible factors, which can cause influence on turbine power production.

CE Database subject headings: Wind energy; Wind tunnels; Wind velocity; Wind direction.},
  comment   = {NN wind power forecasting w/ Kalman filter training},
  file      = {Singh07windPowPredNeural.pdf:Singh07windPowPredNeural.pdf:PDF;Singh07windPowPredNeural.pdf:Singh07windPowPredNeural.pdf:PDF},
  owner     = {scotto},
  timestamp = {2008.07.06},
  url       = {http://scitation.aip.org.offcampus.lib.washington.edu/dbt/dbt.jsp?KEY=JLEED9&Volume=133&Issue=1},
}

@PhdThesis{Cutler09rampUncertPhD,
  author      = {Cutler, Nicholas Jeffrey},
  title       = {Characterising the uncertainty in potential large rapid changes in wind power generation},
  year        = {2009},
  type        = {PhD Thesis},
  abstract    = {Wind energy forecasting can facilitate wind energy integration into a power system. In particular, the management of power system security would benefit from forecast information on plausible large, rapid change in wind power generation. Numerical Weather Prediction (NWP) systems are presently the best available tools for wind energy forecasting for projection times between 3 and 48 hours. In this thesis, the types of weather phenomena that cause large, rapid changes in wind power in southeast Australia are classified using observations from three wind farms. The results show that the majority of events are due to horizontal propagation of spatial weather features. A study of NWP systems reveals that they are generally good at forecasting the broad large-scale weather phenomena but may misplace their location relative to the physical world. Errors may result from developing single time-series forecasts from a single NWP grid point, or from a single interpolation of proximate grid points. This thesis presents a new approach that displays NWP wind forecast information from a field of multiple grid points around the wind farm location. Displaying the NWP wind speeds at the multiple grid points directly would potentially be misleading as they each reflect the estimated local surface roughness and terrain at a particular grid point. Thus, a methodology was developed to convert the NWP wind speeds at the multiple grid points to values that reflect surface conditions at the wind farm site. The conversion method is evaluated with encouraging results by visual inspection and by comparing with an NWP ensemble. The multiple grid point information can also be used to improve downscaling results by filtering out data where there is a large chance of a discrepancy between an NWP time-series forecast and observations. The converted wind speeds at multiple grid points can be downscaled to site-equivalent wind speeds and transformed to wind farm power assuming unconstrained wind farm operation at one or more wind farm sites. This provides a visual decision support tool that can help a forecast user assess the possibility of large, rapid changes in wind power from one or more wind farms.},
  comment     = {Ramp/Uncertainty prediction from NWP ensembles (on land)
* most ramps were from horizontal propagation; not downwelling, etc. (means can predict with horizontal measurements; can get by w/o atmospheric column data)
* has definition of ramps
* develops features for ramp prediction
* normalize wind speed to terrain; makes distant wind speeds comparable to speed at target site
* ensembles are also used to predict ramp time uncertainty
* some of this is in papers and there are also some nice slides on the internet
* I think this is the opposite of the offshore presentation I saw at Norcowe in Bergen, 2012},
  date        = {2009},
  file        = {Cutler09rampUncertPhD.pdf:Cutler09rampUncertPhD.pdf:PDF},
  groups      = {Ensemble, ErrDistProps, doReadNonWPV_1},
  institution = {University of New South Wales, Electrical Engineering \& Telecommunications Faculty of Engineering},
  owner       = {scot},
  timestamp   = {2010.11.23},
}

@InProceedings{Jursa07windPowVarSelParticle,
  author      = {Ren\'{e} Jursa},
  title       = {Variable selection for wind power prediction using particle swarm optimization},
  booktitle   = {Genetic and evolutionary computation (GECCO)},
  year        = {2007},
  pages       = {2059--2065},
  address     = {New York, NY, USA},
  publisher   = {ACM},
  abstract    = {Wind energy has an increasing influence on the energy supply in
many countries, but in contrast to conventional power plants it is
a fluctuating energy source. For its integration in the electricity
supply structure it is necessary to predict the wind power hours or
days ahead. There are models based on physical, statistical or
artificial intelligence approaches for the prediction of wind power.
In this paper a new short-term prediction method is described
based on variable selection using particle swarm optimization and
nearest neighbour search. As input variables for this prediction
method weather data of a numerical weather prediction model and
measured power data from wind farms of several locations in a
spread area are used. Additionally a prediction model based on
neural networks is described and the results of the new method
are compared to the results of the neural network approach.
As a result we get a reduction of the prediction error by using the
new prediction method. An additional error reduction is possible
by using the mean model output of the neural network model and
of the nearest neighbour search based prediction approach.
Categories and Subject Descriptors
J.2 [Physical Science and Engineering]: Mathematics and
statistics, engineering, earth and atmospheric science},
  citeseerurl = {http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=F1B386C3C35CA4F80C5B0307E43A12FB?doi=10.1.1.105.8706},
  comment     = {particle swarm wind power foreasting feature selection, from IWES (ISET) in 2007!},
  doi         = {10.1145/1276958.1277361},
  file        = {Jursa07windPowVarSelParticle.pdf:Jursa07windPowVarSelParticle.pdf:PDF;Jursa07windPowVarSelParticle.pdf:Jursa07windPowVarSelParticle.pdf:PDF},
  isbn        = {978-1-59593-697-4},
  location    = {London, England},
  owner       = {sotterson},
  timestamp   = {2008.07.03},
  url         = {http://portal.acm.org/citation.cfm?id=1277361#},
}

@INPROCEEDINGS{Campbell03estYieldWind,
  Author                   = {P.R.J. Campbell and K. Adamson},
  Title                    = {Estimation of Energy Yield from Wind Turbine Generators},
  Booktitle                = {PowerCON},
  Year                     = {2003},
  Abstract                 = {Wind energy has emerged as the leading renewable energy generation method, currently producing a power yield equivalent to 20 GW, with an estimated projection of 40-60 GW by 2010. In order to successfully integrate wind energy with traditional generation supplies it is necessary to have the ability to accurately forecast the available yield of a wind park over a given period. This paper presents a wind power and subsequently an energy yield forecast tool which is based on a multi-layered perceptron. The tool produces energy yield forecasts which can be used for two main purposes; firstly, delivery of wind (energy) yield estimations and secondly to assess the suitability of a given location for development into a wind park site. The tool makes use of a Multi-layered Perceptron which has been trained with historical data to produce a set of predicted wind speed data for a given period. This data is then processed in conjunction with independent variables, including Wind Turbine Generator (WTG) type and altitude to give an estimated power yield and expected uncertainty of the forecast (in terms of percentage capacity factor). Results indicate that by using a neural network approach the accuracy of the tool is sufficiently accurate to be considered as a feasible method for short to medium term (wind speed) power yield estimation for wind energy producers and utility operators},
  Owner                    = {sotterson},
  Timestamp                = {2008.07.03},
  URL                      = {http://www.actapress.com/Abstract.aspx?paperId=20268}
}

@Article{Archer07windBaseloadInterConn,
  author    = {Cristina L. Archer and Mark Z. Jacobson},
  title     = {Supplying Baseload Power and Reducing Transmission Requirements by Interconnecting Wind Farms},
  journal   = {Journal of Applied Meteorology and Climatology},
  year      = {2007},
  volume    = {46},
  pages     = {1701--1717},
  month     = nov,
  abstract  = {Wind is the world?s fastest growing electric energy source. Because it is intermittent, though, wind is not
used to supply baseload electric power today. Interconnecting wind farms through the transmission grid is
a simple and effective way of reducing deliverable wind power swings caused by wind intermittency. As
more farms are interconnected in an array, wind speed correlation among sites decreases and so does the
probability that all sites experience the same wind regime at the same time. The array consequently behaves
more and more similarly to a single farm with steady wind speed and thus steady deliverable wind power.
In this study, benefits of interconnecting wind farms were evaluated for 19 sites, located in the midwestern
United States, with annual average wind speeds at 80 m above ground, the hub height of modern wind
turbines, greater than 6.9 m s1 (class 3 or greater). It was found that an average of 33 pct and a maximum
of 47 pct of yearly averaged wind power from interconnected farms can be used as reliable, baseload electric
power. Equally significant, interconnecting multiple wind farms to a common point and then connecting
that point to a far-away city can allow the long-distance portion of transmission capacity to be reduced, for
example, by 20 pct with only a 1.6 pct loss of energy. Although most parameters, such as intermittency,
improved less than linearly as the number of interconnected sites increased, no saturation of the benefits
was found. Thus, the benefits of interconnection continue to increase with more and more interconnected
sites.},
  comment   = {Better grid connections reduce need for wind energy backups. Surprisingly, an average of 33 pct of total average power can be used for baseload (at some reliability I didn't check) with a large cluster of farms.},
  file      = {Archer07windBaseloadInterConn.pdf:Archer07windBaseloadInterConn.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.12.15},
  url       = {http://www.stanford.edu/group/efmh/winds/aj07_jamc.pdf},
}

@Article{Morales10scenarios,
  author    = {J.M. Morales and R. Minguez and A.J. Conejo},
  title     = {A methodology to generate statistically dependent wind speed scenarios},
  journal   = {Applied Energy},
  year      = {2010},
  volume    = {87},
  number    = {3},
  pages     = {843--855},
  issn      = {0306-2619},
  abstract  = {Wind power - a renewable energy source increasingly attractive from an economic viewpoint - constitutes an electricity production alternative of growing relevance in current electric energy systems. However, wind power is an intermittent source that cannot be dispatched at the will of the producer. Modeling wind power production requires characterizing wind speed at the sites where the wind farms are located. The wind speed at a particular location can be described through a stochastic process that is spatially correlated with the stochastic processes describing wind speeds at other locations. This paper provides a methodology to characterize the stochastic processes pertaining to wind speed at different geographical locations via scenarios. Each one of these scenarios embodies time dependencies and is spatially dependent of the scenarios describing other wind stochastic processes. The scenarios generated by the proposed methodology are intended to be used within stochastic programming decision models to make informed decisions pertaining to wind power production. The methodology proposed is accurate in reproducing wind speed historical series as well as computationally efficient. A comprehensive case study is used to illustrate the capabilities of the proposed methodology. Appropriate conclusions are finally drawn.},
  comment   = {Relevant to optimal spinning reserves project.
* Assumes that Gaussianizing RV's with a scalar transform makes their joint distribution a multi-variate Gaussian, which is not true in general. See Wu10gaussMargNotJoint.
* Probably should use true multivariate Gaussianization (see energytop.org)},
  doi       = {DOI: 10.1016/j.apenergy.2009.09.022},
  file      = {Morales10scenarios.pdf:Morales10scenarios.pdf:PDF},
  keywords  = {Wind speed correlation},
  owner     = {scot},
  timestamp = {2010.07.05},
  url       = {http://www.sciencedirect.com/science/article/B6V1T-4XG3DFR-2/2/95e4e4c672d7ff40a905b0bc8e730b64},
}

@InProceedings{Zhang09,
  author    = {Guoqiang Zhang and Boming Zhang and Hongbin Sun and Wenchuan Wu},
  title     = {Ultra-short term probabilistic transmission congestion forecasting considering wind power integration},
  booktitle = {Proc. 8th Int. Conf. Advances in Power System Control Operation and Management (APSCOM 2009)},
  year      = {2009},
  pages     = {1--6},
  month     = nov,
  abstract  = {Wind power continues its rapid growth in the world. High wind penetration brings significant uncertainty to transmission congestion management. For system security consideration, some monitoring indexes of transmission congestion management should be presented to system operators. In this paper, the transmission congestion probability forecasting for power systems with high wind power integration is discussed, and a probabilistic forecasting method for ultra-short term transmission congestion is introduced. Probability distribution is applied to express the uncertainty brought by wind power integration in this paper. The proposed forecasting method applies the Boundary Load Flow and Newton power flow algorithm as the section power flow calculation framework, and obtains the probability distribution of section power flow through Monte Carlo Simulation in which antithetic variable sampling is used to reduce sampling frequency, and then the congestion probability of transmission sections is predicted. Simulation results of IEEE 39-bus system validate the efficiency of the proposed probabilistic forecasting method. The ultra-short term probabilistic forecasts of transmission congestion would provide monitoring index and information to system operators, which is helpful to relieve congestion scenario on the transmission sections.},
  doi       = {10.1049/cp.2009.1776},
  file      = {:Zhang09probCngstFrcstWind.pdf:PDF},
}

@InProceedings{Potter09benProbRampFrcst,
  author    = {Potter, C.W. and Grimit, E. and Nijssen, B.},
  title     = {Potential benefits of a dedicated probabilistic rapid ramp event forecast tool},
  booktitle = {Power Systems Conference and Exposition (PSCE)},
  year      = {2009},
  pages     = {1--5},
  month     = mar,
  abstract  = {Wind power continues its rapid worldwide growth. In some places wind energy penetration is so high that the variation in wind energy production is the dominant driving force behind variation in the generation-load balance. To ensure a reliable supply of power, system operators must be able to schedule sufficient operating reserves. However, it is impractical to always supply 100\% back-up for all generation units. Typically the N-1 criterion is applied, establishing the effect of the loss of the single largest generation unit. Wind energy compels different operating requirements, because failure of an entire wind energy project is improbable, but rapid variation in energy output is common. In such an environment, wind energy forecasting has significant value, especially during times of rapid change, and the use of a probabilistic forecast tool can minimize reserve requirements.},
  comment   = {Ramp benefits imagined in an artificial market w/ no numbers.

Some ramp / prob forecast info, though.
* Ramps are rare: BPA sees a 20\% capacity ramp once every 2 weeks

* bogus argument about limiting costs by N-1? Who pays for N-1 protection?

* Reference [5] is another study showing reduced cost w/ prob. forecast (not a ramp forecast, though)
-- uses stochastic optimization, which it is claimed, is not done elsewhere
* Wants to fudge ramp timing errors by requiring temporally sloppy reserves. How much extra does that cost?
* results shown for unexplained prototype look better w/ temporal fudge; awful w/o it
* reserve cost calc seems wrong (1).
-- no cost for down regulation
-- don't know where PCAP comes from
-- don't understand the PCAP threshold argument
-- not explained how much reserve power you buy. It's actings as if power is binary.
* TOTAL marketing graph showing ostensible ramp forecast benefit
-- no numbers
-- looks hand drawn
-- not clear if there's any value for PCAP under which ramp forecasting yields a benefit.},
  doi       = {10.1109/PSCE.2009.4840109},
  file      = {Potter09benProbRampFrcst.pdf:Potter09benProbRampFrcst.pdf:PDF},
  keywords  = {dedicated probabilistic rapid ramp event forecast tool;generation-load balance;wind energy forecasting;wind energy penetration;wind energy production;wind power;load forecasting;probability;wind power;},
  owner     = {scot},
  timestamp = {2011.04.19},
}

@Article{Bel16gridFluctwndErr,
  author   = {G Bel and C P Connaughton and M Toots and M M Bandi},
  title    = {Grid-scale fluctuations and forecast error in wind power},
  journal  = {New Journal of Physics},
  year     = {2016},
  volume   = {18},
  number   = {2},
  pages    = {023015},
  month    = feb,
  abstract = {Wind power fluctuations at the turbine and farm scales are generally not expected to be correlated over large distances. When power from distributed farms feeds the electrical grid, fluctuations from various farms are expected to smooth out. Using data from the Irish grid as a representative example, we analyze wind power fluctuations entering an electrical grid. We find that not only are grid-scale fluctuations temporally correlated up to a day, but they possess a self-similar structure?a signature of long-range correlations in atmospheric turbulence affecting wind power. Using the statistical structure of temporal correlations in fluctuations for generated and forecast power time series, we quantify two types of forecast error: a timescale error ([http://ej.iop.org/images/1367-2630/18/2/023015/njpaa114dieqn1.gif] {${e}_{\tau }$} ) that quantifies deviations between the high frequency components of the forecast and generated time series, and a scaling error (  [http://ej.iop.org/images/1367-2630/18/2/023015/njpaa114dieqn2.gif] {${e}_{\zeta }$} ) that quantifies the degree to which the models fail to predict temporal correlations in the fluctuations for generated power. With no a priori knowledge of the forecast models, we suggest a simple memory kernel that reduces both the timescale error ( [http://ej.iop.org/images/1367-2630/18/2/023015/njpaa114dieqn3.gif] {${e}_{\tau }$} ) and the scaling error ([http://ej.iop.org/images/1367-2630/18/2/023015/njpaa114dieqn4.gif] {${e}_{\zeta }$} ).},
  comment  = {Some kinda decomposition of aggregated wind power forecast error, I think finding that errors don't always cancel  Useful for upscaling and maybe ReWP wind farm clustering.  I haven't read this.

News article is here:  http://phys.org/news/2016-03-re-thinking-renewable-energy.html},
  file     = {Bel16gridFluctwndErr.pdf:Bel16gridFluctwndErr.pdf:PDF},
  url      = {http://stacks.iop.org/1367-2630/18/i=2/a=023015},
}

@Article{Bandi16SpectrumWindPow,
  author    = {Bandi, Mahesh},
  title     = {The Spectrum of Wind Power Fluctuations},
  journal   = {Physical Review Letters},
  year      = {2016},
  month     = {Dec},
  abstract  = {Wind power fluctuations for an individual turbine and plant have been widely reported to follow
the Kolmogorov spectrum of atmospheric turbulence; both vary with fluctuation time scale ? as
?2/3. Yet, this scaling has not been explained through turbulence theory. Using turbines as probes
of turbulence, we show the ?2/3 scaling results from large scale influence of atmospheric turbulence.
Owing to this long-range influence spanning 100s of kilometers, when power from geographically
distributed wind plants is summed into aggregate power at the grid, fluctuations average (geographic
smoothing) and their scaling steepens from ?2/3 ! ?4/3, beyond which further smoothing is not
possible. Our analysis demonstrates grids have already reached this ?4/3 spectral limit to geographic
smoothing.},
  comment   = {How  much wind power variablity can be reduced by spatial averaging.  I haven't read this but it sounds quite relevant to reserve planning, optimal portfolios, and ReWP.

Compare with Martin15varWindLenTime},
  file      = {Bandi16SpectrumWindPow.pdf:Bandi16SpectrumWindPow.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.01.02},
  url       = {https://www.researchgate.net/publication/311455778_The_Spectrum_of_Wind_Power_Fluctuations},
}

@Article{Bludszuweit08frcstErrStats,
  author    = {Bludszuweit, H. and Dominguez-Navarro, J.A. and Llombart, A.},
  title     = {Statistical Analysis of Wind Power Forecast Error},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2008},
  volume    = {23},
  number    = {3},
  pages     = {983--991},
  month     = aug,
  issn      = {0885-8950},
  abstract  = {Wind power forecast error usually has been assumed
to have a near Gaussian distribution. With a simple statistical
analysis, it can be shown that this is not valid. To obtain a more
appropriate probability density function (pdf) of the wind power
forecast error, an indirect algorithm based on the Beta pdf is
proposed. Measured one-year time series from two different wind
farms are used to generate the forecast data. Three different forecast
scenarios are simulated based on the persistence approach.
This makes the results comparable to other forecast methods.
It is found that the forecast error pdf has a variable kurtosis
ranging from 3 (like the Gaussian) to over 10, and therefore it
can be categorized as fat-tailed. A new approximation function
for the parameters of the Beta pdf is proposed because results
from former publications could not be confirmed. Besides, a linear
approximation is developed to describe the relationship between
the persistence forecast and the related mean measured power.
An energy storage system (ESS), which reduces the forecast error
and smooths the wind power output, is considered. Results for this
case show the usefulness of the proposed forecast error pdf for
finding the optimum rated ESS power.
Index Terms: Error analysis, forecasting, wind, wind power
generation.},
  comment   = {Likes the beta dist for wind power forecast errors. Duque paper uses beta. NIPS 2010 paper has a beta copula method...

Could use beta for BMA ensimble dists, or maybe even for prob. forecasts from a point forecast.},
  doi       = {10.1109/TPWRS.2008.922526},
  file      = {Bludszuweit08frcstErrStats.pdf:Bludszuweit08frcstErrStats.pdf:PDF},
  groups    = {ErrDistProps, doReadNonWPV_1},
  keywords  = {Beta pdf;ESS;Gaussian distribution;approximation function;energy storage system;probability density function;statistical analysis;wind farms;wind power forecast error;Gaussian distribution;load forecasting;probability;wind power plants;},
  owner     = {scot},
  timestamp = {2010.12.13},
}

@Article{Mahoney12ensKalQntlFrcst,
  author    = {Mahoney, W.P. and Parks, K. and Wiener, G. and Yubao Liu and Myers, W.L. and Juanzhen Sun and Delle Monache, L. and Hopson, T. and Johnson, D. and Haupt, S.E.},
  title     = {A Wind Power Forecasting System to Optimize Grid Integration},
  journal   = {Sustainable Energy, IEEE Transactions on},
  year      = {2012},
  volume    = {3},
  number    = {4},
  pages     = {670--682},
  issn      = {1949-3029},
  abstract  = {Wind power forecasting can enhance the value of wind energy by improving the reliability of integrating this variable resource and improving the economic feasibility. The National Center for Atmospheric Research (NCAR) has collaborated with Xcel Energy to develop a multifaceted wind power prediction system. Both the day-ahead forecast that is used in trading and the short-term forecast are critical to economic decision making. This wind power forecasting system includes high resolution and ensemble modeling capabilities, data assimilation, now-casting, and statistical postprocessing technologies. The system utilizes publicly available model data and observations as well as wind forecasts produced from an NCAR-developed deterministic mesoscale wind forecast model with real-time four-dimensional data assimilation and a 30-member model ensemble system, which is calibrated using an Analogue Ensemble Kalman Filter and Quantile Regression. The model forecast data are combined using NCAR's Dynamic Integrated Forecast System (DICast). This system has substantially improved Xcel's overall ability to incorporate wind energy into their power mix.},
  comment   = {A fancy algorithm that converts wind ensembles to wind power. I haven't read yet if it had individual ensemble forecasts or what. The Kalman filter is an "analogue" KF -- don't know if that has anything to do with analog ensemble forecasting...},
  doi       = {10.1109/TSTE.2012.2201758},
  file      = {Mahoney12ensKalQntlFrcst.pdf:Mahoney12ensKalQntlFrcst.pdf:PDF},
  groups    = {Ensemble, doReadWPV_2},
  keywords  = {Kalman filters;decision making;power grids;regression analysis;weather forecasting;wind power;30-member model ensemble system;DICast;National Center for Atmospheric Research;Xcel Energy;analogue ensemble Kalman filter;day ahead forecast;dynamic integrated forecast system;economic decision making;economic feasibility;grid integration;mesoscale wind forecast model;quantile regression;real time 4D data assimilation;short term forecast;statistical postprocessing;wind energy;wind power forecasting system;Data assimilation;Data models;Forecasting;Predictive models;Wind energy;Wind forecasting;Wind speed;Data assimilation;forecasting;nowcasting;wind energy;wind power forecasting},
  owner     = {sotterson},
  timestamp = {2013.10.10},
}

@Article{Botterud10windPowFrcstUSmkts,
  author    = {Audun Botterud and Jianhui Wang and Vladimiro Miranda and Ricardo J. Bessa},
  title     = {Wind Power Forecasting in U.S. Electricity Markets},
  journal   = {The Electricity Journal},
  year      = {2010},
  volume    = {23},
  number    = {3},
  pages     = {71--82},
  issn      = {1040-6190},
  abstract  = {Wind power forecasting is becoming an important tool in electricity markets, but the use of these forecasts in market operations and among market participants is still at an early stage. The authors discuss the current use of wind power forecasting in U.S. ISO/RTO markets, and offer recommendations for how to make efficient use of the information in state-of-the-art forecasts.},
  comment   = {Advocates new use of probabilistic forecats

* Traditional deterministic criteria (e.g., loss of the largest unit in the system) should be replaced with probabilistic approaches that directly reflect
the uncertain nature of supply and demand
* dynamic reserve updating
* related slides also attached},
  doi       = {10.1016/j.tej.2010.03.006},
  file      = {Slides (related, on trading):Botterud10windPowFrcstUSmkts_slides.pdf:PDF;Paper:Botterud10windPowFrcstUSmkts.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.04},
}

@InProceedings{Hodge12frcstErrDistIntComp,
  author    = {Hodge, BM and Lew, D and Milligan, M and Holttinen, H and Sillanp{\"a}{\"a}, S and G{\'o}mez-L{\'a}zaro, E and Scharff, R and S{\"o}der, L and Lars{\'e}n, Guo and Giebel, G and others},
  title     = {Wind Power Forecasting Error Distributions: An International Comparison},
  booktitle = {The 11\textsuperscript{th} Annual International Workshop on Large-Scale Integration of Wind Power into Power Systems as well as on Transmission Networks for Offshore Wind Power Plants Conference},
  year      = {2012},
  location  = {Lisbon, Portugal},
  abstract  = {Wind power forecasting is essential for greater penetration of wind power into electricity systems. Because no wind forecasting system is perfect, a thorough understanding of the errors that may occur is a critical factor for system operation functions, such as the setting of operating reserve levels. This paper provides an international comparison of the distribution of wind power forecasting errors from operational systems, based on real forecast data. The paper concludes with an assessment of similarities and differences between the errors observed in different locations. Keywords?wind power forecasting, power system operation,},
  file      = {Hodge12frcstErrDistIntComp.pdf:Hodge12frcstErrDistIntComp.pdf:PDF},
  groups    = {Test, ErrDistProps, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2013.02.18},
}

@InProceedings{Hodge12entropyFrcstMetric,
  author    = {Hodge, Bri-Mathias and Orwig, Kirsten and Milligan, Michael R},
  title     = {Examining information entropy approaches as wind power forecasting performance metrics},
  booktitle = {12\textsuperscript{th} International Conference on Probabilistic Methods Applied to Power Systems},
  year      = {2012},
  publisher = {National Renewable Energy Laboratory},
  abstract  = {Wind power forecasting is expected to play an increasing role in power system operations as the amount of wind capacity on-line continues to increase. Traditional forecasting metrics, such as MAE and RMSE, neglect some of the information inherent in forecasting error distributions. Information entropy approaches, based on the R?nyi entropy, have been proposed as an alternative metric to assess different forecasting methods. In this work, we examine the parameters associated with the calculation of the R?nyi entropy in order to further the understanding of its application to assessing wind power forecasting errors.
Keywords- Power generation; stochastic systems; power engineeringand energy; wind power generation},
  comment   = {Naive use of Renji entropy (Kraskov w/b better, I think) as a forecast metric. Somebody used something similar to tune a forecast algorithm.

* the use of statistical measures beyond the variance has been shown to decrease the total system costs with stochastic unit commitment (ref 5)
-- standard forecasting metrics are only optimal if the error distribution is Gaussian (I think RMSE, he means)

IDEA: use info to ensembles that add the most multivariate information about some kind of point forecast error quantiles. For ensemlbes, the "point forecast" could be the error against the ensemble median (I think Pierre used this in his bootstrapping conditional prob forecast paper). This might have to be concitional so that ensembles are selected at certain times. Perhaps the conditioning variables are also selected w/ MI? Anyway, MI would be a fast way to pick features without running EM in BMA or whatever. For wind vs. power, the "point forecast" error could be against a learned power curve for the ensemble mean (or best ensemble?)},
  file      = {Hodge12entropyFrcstMetric.pdf:Hodge12entropyFrcstMetric.pdf:PDF},
  groups    = {Test, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.02},
}

@Article{Anastasiades13probFrcstWindVarIdxFeats,
  author    = {Anastasiades, Georgios and McSharry, Patrick},
  title     = {Quantile forecasting of wind power using variability indices},
  journal   = {Energies},
  year      = {2013},
  volume    = {6},
  number    = {2},
  pages     = {662--695},
  abstract  = {Wind power forecasting techniques have received substantial attention recently
due to the increasing penetration of wind energy in national power systems. While the initial
focus has been on point forecasts, the need to quantify forecast uncertainty and communicate
the risk of extreme ramp events has led to an interest in producing probabilistic forecasts.
Using four years of wind power data from three wind farms in Denmark, we develop quantile
regression models to generate short-term probabilistic forecasts from 15 min up to six hours
ahead. More specifically, we investigate the potential of using various variability indices as
explanatory variables in order to include the influence of changing weather regimes. These
indices are extracted from the same wind power series and optimized specifically for each
quantile. The forecasting performance of this approach is compared with that of appropriate
benchmark models. Our results demonstrate that variability indices can increase the overall
skill of the forecasts and that the level of improvement depends on the specific quantile.

Keywords: wind power forecasting; wind power variability; quantile forecasting; density
forecasting; quantile regression; continuous ranked probability score; quantile loss function;
check function},
  comment   = {Suggests that both forecast lags and empirical quanitiles (or similar) are good linear QR features. Features derived from point forecasts (sliding window stdev or empirical quantiles) improve linear quantile regression of extreme quantiles for horizons up to 6 hours (input is a wind power point forecast). But the point forecast is multi-step AR, which must suck for more than a couple hours, and I am not sure about the meaninfulness of the tests done on lookaheads more than 15 minutes.

Review of QR input features used in other work
* wind speed/dir
* temp
* pressure
* density
* friction velocity
(in this paper, it's only measured power and indices derived from it)

Point forecast source
* seems to be ARIMA but I'm not sure
* is a recursive, multi-step algorithm, while in other work FIR structures seem to work better
* input is past wind power measurements, which is actually harder to get in Germany than NWP, which would probably have produced better results for > 2 hours ahead
* they tried a sinusoidal diurnal model too but saw no improvement

Benchmark Q forecasts (in order of worst to best, on 1-step (15 min) forecast (Table 7))
* Climatology
* Cutoff-normal (from Pinson and others, I think, and even though it models diurnals, etc, it is bad)
 - has fancy sinusoidal term for diurnals (eq 24, p. 680)
* Past wind power persistence (dist of last n power measure meas., with n optimized for each quantile)
* 3-lagged series: FIR linear QR w/ 3 lagged point power forecasts (from ARIMA? Anyway, this is by far the best)
 (SHOULD I BE ADDING LAGS in my linQR experiments?)

Forecasts w/ variability features (one feature at a time is tacked on to the end of the 3-lagged series model)
* Q95: 95% empirical quantile of last n pow MEAS, smoothed by m point sliding mean (m, n, optimized per Q)
* Q05: 5% empirical quantile ...
* IQR: Q95-Q05
* SD: sliding window standard deviation, also w/ m,n optimized per Q

RESULTS
* types of results
 o individual quantile scores (1 step ahead only); CRPS
 --o CRPS tests done w/ same model, all quantiles: s/b
 o 1 step ahead, average over 24 steps ahead
 --o 24 step ahead results (iterative, not FIR) will be bad and longer horizons: probably not meaninful
 --o 24 step average has same QR model across all horizons: probably not useful
 --o only 1 step ahead tests individual Q scores

 ==> 1-step ahead Qscore is the most meaninful test

* Results for 1-step ahead, individual quantiles (table 7)
 o empirical Q95 or Q05 are usually the best inputs
 --o IQR never best
 --o SD best for 3 out of 11 quantiles
 o interesting Q swap
 --o empirical Q95 best for forecasting Q05
 --o empirical Q05 best for preedicting Q95
 --o reason given is better performance near high speed cutoff, but I don't quite buy or believe it

* Results for 24 step ahead quantile score
 o IQR or SD usually better than Q's in middle range (not always)
 o empirical Q05 and Q95 best for forecasting Q95 and Q05 (same swap as for 1-step ahead)


Variability index features for probabilistic wind power forecasts. Quantile dependent.},
  file      = {Anastasiades13probFrcstWindVarIdxFeats.pdf:Anastasiades13probFrcstWindVarIdxFeats.pdf:PDF},
  groups    = {Read, PointDerived, doReadWPV_1},
  owner     = {sotterson},
  publisher = {Multidisciplinary Digital Publishing Institute},
  timestamp = {2014.10.07},
  url       = {http://www.mdpi.com/1996-1073/6/2/662},
}

@InProceedings{Juban07probFrcstOptMgmtWnd,
  author    = {Juban, J. and Siebert, N. and Kariniotakis, G.N.},
  title     = {Probabilistic Short-term Wind Power Forecasting for the Optimal Management of Wind Generation},
  booktitle = {Power Tech, 2007 IEEE Lausanne},
  year      = {2007},
  pages     = {683--688},
  abstract  = {Wind power forecasting tools have been developed for some time. The majority of such tools usually provides single-valued (spot) predictions. Such predictions limits the use of tools for decision-making under uncertainty. In this paper we propose a method for producing the complete predictive probability density function (PDF). The method is based on kernel density estimation techniques. The preliminary results show that this method levels with state of the art one while being fast and producing the complete PDF. The results were obtained through real data from three French wind farms.},
  comment   = {Making prob forecasts from point forecasts using KDE. A good way to test against the IWES KDE implementation.},
  doi       = {10.1109/PCT.2007.4538398},
  file      = {Juban07probFrcstOptMgmtWnd.pdf:Juban07probFrcstOptMgmtWnd.pdf:PDF},
  groups    = {PointDerived, CitaviImport1, doReadWPV_2},
  keywords  = {decision making;load forecasting;probability;wind power plants;French wind farms;decision-making;kernel density estimation techniques;probabilistic short-term wind power forecasting;probability density function;single-valued predictions;wind generation optimal management;Decision making;Energy management;Kernel;Power generation;Probability density function;Uncertainty;Wind energy;Wind energy generation;Wind forecasting;Wind power generation},
  ncite     = {53},
  owner     = {sotterson},
  timestamp = {2013.09.30},
}

@InProceedings{Bielecki10charWPFerr,
  author       = {Bielecki, Mark F and Kemper, Jason J and Acker, Thomas L},
  title        = {A Methodology for Comprehensive Characterization of Errors in Wind Power Forecasting},
  booktitle    = {Proceedings of ASME 2010 4\textsuperscript{th} International Conference on Energy Sustainability},
  year         = {2010},
  organization = {ASME},
  abstract     = {Wind power forecasting will play a more important role in electrical system planning with the greater wind penetrations of the coming decades. Wind will comprise a larger percentage of the generation mix, and as a result forecasting errors may have more significant effects on balancing operations. The natural uncertainties associated with wind along with limitations in numerical weather prediction (NWP) models lead to these forecasting errors, which play a considerable role in the impacts and costs of utilityscale wind integration. The premise of this project was to examine errors between the actual and commercially forecasted power production data from a typical wind power plant in the Northwestern United States. An exhaustive statistical characterization of the forecast behavior and error trends was undertaken, which allowed the most important metrics for describing wind power forecast errors to be identified. This paper presents only the metrics chosen as most significant. While basic information about wind forecast accuracy such as the mean absolute error (MAE) is valuable, a more detailed description is useful for system operators or in wind integration studies. System planners have expressed major concern in the area of forecast performance during large wind ramping events. For such reasons, this methodology included the development of a comprehensive ramp identification algorithm to select significant ramp events from the data record, and particular attention was paid to the error analysis during these events. The algorithm allows user input to select ramps of any desired magnitude, and also performs correlation analysis between forecasted ramp events and actual ramp events that coincide within a desired timing window. From this procedure, an investigation of the magnitude and phase of forecast errors was conducted for various forecast horizons. The metrics found to be of most importance for error characterization were selected based on overall impacts, and were ranked in order of significance. These metrics included: mean absolute error, root mean square error, average step change value, standard deviation of step changes, mean bias levels, correlation coefficient of power values, mean temporal bias of ramp events, and others. While these metrics were selected and the methodology was developed for a single dataset, the entire process can be applied generally to any wind power and forecast time series. The implications for such a process include use for generating a synthetic wind power forecast for wind integration studies that will reproduce the same error trends as those found in a real forecast.},
  comment      = {Has ramp identification

Point forecast critera, only, it seems.},
  file         = {Bielecki10charWPFerr.pdf:Bielecki10charWPFerr.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2013.02.18},
  url          = {http://nau.edu/uploadedFiles/Academic/CEFNS/Centers-Institutes/Folder_Templates/_Media/A-Methodology-For-Comprehensive-Characterization-Of-Errors-In-Wind-Power-Forecasting.pdf},
}

@MastersThesis{Bielecki10frcstErrMSthesis,
  author      = {Mark F. Bielecki},
  title       = {STATISTICAL CHARACTERIZATION OF ERRORS IN WIND POWER FORECASTING},
  year        = {2010},
  abstract    = {Wind power forecasting will play a more important role in electrical system planning with the greater wind penetrations of the coming decades. Wind will most likely comprise a larger percentage of the generation mix, and as a result forecasting errors may have more significant effects on balancing operations. The natural uncertainties associated with wind along with limitations in numerical weather prediction (NWP) models lead to these forecasting errors, which play a considerable role in the impacts and costs of utility-scale wind integration. This thesis project was designed to examine errors between the actual and commercially forecasted power production data from a typical wind power plant in the Northwestern United States. An exhaustive statistical characterization of the forecast behavior and error trends was undertaken, which allowed the most important metrics for describing wind power forecast errors to be identified. While basic information about wind forecast accuracy such as the mean absolute error (MAE) is valuable, a more detailed description is useful for system operators or in wind integration studies. System planners have expressed major concern in the area of forecast performance during large wind ramping events. For such reasons, this methodology included the development of a comprehensive ramp identification algorithm to select significant ramp events from the data record, and particular attention was paid to the error analysis during these events. The algorithm allows user input to select ramps of any desired magnitude, and also performs correlation analysis between forecasted ramp events and actual ramp events that coincide within a desired timing window. From this procedure, an investigation of the magnitude and phase of forecast errors was conducted for various forecast horizons. The metrics found to be of most importance for error characterization were selected based on overall impacts, and were ranked in a rudimentary (and perhaps subjective) order of significance. These metrics included: mean absolute error, root mean square error, average magnitude of step changes, standard deviation of step changes, mean bias levels, correlation coefficient of power values, mean temporal bias of ramp events, and others. While these metrics were selected and the methodology was developed for a single dataset, the entire process can be applied generally to any wind power and forecast time series. The implications for such a process include use for generating a synthetic wind power forecast for wind integration studies that will reproduce the same error trends as those found in a real forecast.},
  comment     = {MS thesis on point forecast error metrics. Has ramp criteria. Nice poster also attached.

A quick grep doesn't find CRPS, so it must be point forecast only.},
  file        = {Master's Thesis:Bielecki10frcstErrMSthesis.pdf:PDF;Poster:Bielecki10frcstErrPoster.pdf:PDF},
  institution = {Northern Arizona University},
  owner       = {sotterson},
  timestamp   = {2013.02.18},
}

@Article{Bremnes04windLocQR,
  author    = {Bremnes, John Bj{\o}rnar},
  title     = {Probabilistic wind power forecasts using local quantile regression},
  journal   = {Wind Energy},
  year      = {2004},
  volume    = {7},
  number    = {1},
  pages     = {47--54},
  issn      = {1099-1824},
  abstract  = {Wind power forecasts are in various ways valuable for users in decision-making processes. However, most forecasts are deterministic, and hence possibly important information about uncertainty is not available. Complete information about future production can be obtained by using probabilistic forecasts, and this article demonstrates how such forecasts can be created by means of local quantile regression. The approach has several advantages, such as no distributional assumptions and flexible inclusion of predictive information. In addition, it can be shown that, for some purposes, forecasts in terms of quantiles provide the type of information required to make optimal economic decisions. The methodology is applied to data from a wind farm in Norway.},
  comment   = {Local quantile regression goes from wind speed/dir to wind power quantiles. Good ideas on quality metrics, constrained forecast outputs, forecast use, and ensemble forecasting based on rank. Has derivation of optimal bid. But the presentation is sometimes unclear -- especially about climatology -- and the experiments are test on train!

Algorithm: weighted sum of multiple linear QR's,
* weights are the distance from the test point to each "neighborhood" point
* I can't figure out what "distance" is -- maybe this is clear in refs 10-12?
* QR optimization is done jointly across all neighborhoods
- I think (it's not clear).
- one tunable parameter, h_lambda(x), kinda like kernel bandwidth, or radial basis function Gaussian variance
- somehow reduce crossover by constraining one of the parameters
- maybe explanations in refs 10-12
- I tuned the neighborhoods separatly in my DWD loc lin baseline (ref) forecast -- maybe this is better?

Forecast Inputs
* tried 10 brute force combos of spd,dir,month
- squared some terms
- multiplied some (interaction)
 - but doesn't seem to have used them, unlike Pritchard10varQuantProbFrcst who used lots of them
- NO LAGS! is
- dir and months seem to be linear, not cos/sin or harmonic spline coeffs
* selected just spd and dir, but procedure not totally described

Bounded forecast outputs
* preproc output targets: arcsin(sqrt(x/powMax))
- but used logistic xform in Bremnes06compQuantileWind
* then can never predict out-of-range forecasts
* does not censor densities like Cannon11quantRgrsnNNprecip
* preproc approach used here seems more simple: is there a downside? why don't others use it??

Quality Metrics
* binary reliability indicator:
- a function of predicted and measured bin counts
- evaluated by a binary Chi-square test
- see also Bremnes04precipQuantsVerif and Bremnes04quantFrcstVerif
- this would be useful for any prob forecast, would tell you if your have too many quantiles (too few counts)
* sharpness: average distance between quantiles
- they call this "length"
- sharpness like this inaccurate for distributions that are really multimodal!
* income if used for bidding
- derivation for optimal bid based on asymmetric forecast error costs
- optimal bid used for prob forecasts
 --- optimal bidding idea was also used in Pinson07tradeWindProbFrcst
 --- finds tau of optimal bid based on asymmetric regulation prices
 --- could be vulnerable to quantile crossover but it's not a total failure like it would be to compute expected value over a density derived by taking the derivative of a quantile forecast with crossed derivatives.
- for deterministic benchmark, bid was just the point forecast

Ensemble forecast idea (maybe a good one)
* rank ensemble members
* for 100 ensemble members the 25\textsuperscript{th} member is input to predicting the 25 percentile quantile QR
* Of course, this could be also include nearby members

Is generalized in Zugno13tradeWindGenMrktQs, where stochastic market prices are used.

IDEAS:
* adaboost idea again! random quantile forecasters w/ differnt ensemble member inputs predicting quantiles
* adaboost combiner calculates CRPS and selects (weights) best ones (somehow)
* simple improvent: dir/month splines
* simple improvement: better feature selection

Results
* seems to be the same year it was trained on: TEST ON TRAIN!
* comparison w/ some kinda climatology, with the only input being "month" but then it is said to have a forecast horizon. WHAT??

A comparison paper by this author is here: Bremnes06compQuantileWind},
  doi       = {10.1002/we.107},
  file      = {Bremnes04windLocQR.pdf:Bremnes04windLocQR.pdf:PDF},
  groups    = {Read, Ensemble, PointDerived, Test, Use, doReadWPV_1},
  keywords  = {wind power, probabilistic forecasts, quantile regression, economic value},
  owner     = {sotterson},
  publisher = {John Wiley \& Sons, Ltd.},
  timestamp = {2013.10.24},
}

@Article{Pinson10probForcast,
  author    = {Pierre Pinson},
  title     = {On probabilistic forecasting of wind power time-series},
  journal   = {Wind Energy},
  year      = {2010},
  abstract  = {Wind power generation is a nonlinear and bounded variable, partly owing to the power curve that converts wind to electric power, and partly owing to the very stochastic nature of wind itself. Predictive densities of wind power generation should account for that effect. Such densities are clearly not expected to be Gaussian, with higher order moments being directly related to their expectation and potentially to some external signal. It is proposed here to model such predictive densities with discretecontinuous mixtures of generalized logit-normal distributions and of probability masses at the bounds. In this framework, a transformation is employed so that transformed data can be modeled with censored Normal variables. Two types of models are proposed: a simple autoregressive model and a more advanced conditional parametric autoregressive model, for which wind direction is the variable conditioning the wind power dynamics. In both cases, the model parameters are adaptively and recursively estimated, time-adaptativity being the result of exponential forgetting of past observations. The probabilistic forecasting methodology is applied at the Horns Rev wind farm in Denmark, for 10-minute ahead probabilistic forecasting of wind power generation. Probabilistic forecasts generated from the proposed methodology clearly have higher skill than those obtained from a classical Gaussian assumption about wind power predictive densities. Corresponding point forecasts also exhibit significantly lower error criteria.},
  comment   = {Don't regime switch; learn continuous function dependence. Model cutoff and max power concentrations. (submitted; not yet approved as of 4/26/10). Good hints on benchmarks, etc.

General Properties
* Forecasting a distribution, not point forecasts
* Purely AR: no off-site observations
* lookahead time is 10 minutes, 1-step ahead
* predicting power makes this problem

HARDER THAN NECESSARY?
-- power distribution has difficulties explained below
-- could avoid by just predicting wind
-- _separately_ estimate power curve so problems are isolated, models simpler and more general?

Power data distribution difficulties
* Power cutoff and max pow: Power prob. mass is concentrated at 0 and max pow ==> trying to estimate w/ a single Gaussian or whatever doesn't work since asymmetric
* Variance depends on wind speed
* Variance also depends on wind direction (at Horn's Rev.)
* Distribution of forecast errors depends upon expected wind speed (from other studies)
* power curve is not symmetric about mid-power So, transform dist. to something more Gaussian first, before doing forecasting
* Generalized logit transform w/ discrete components does the trick
-- unlike common logit xform, allows asymmetric slopes around mid-power (like real curve)
-- argument that this also handles speed-dependent variance (I don't understand it, though)
-- also lumps some prob. mass at 0 and 1 (max pow) w/ discrete mixture comps
---- I believe they remove ("censor") points near {0,1}, and transform this part only
-- result of transform is roughly Normal ==> can do the usual DTU semi-linear least squares modeling (CP-AR)
-- then transform predictions back to power, I think, picking up the censored {0,1} points somehow?

Forecasting algorithm
* the usual CP-AR
* tuned to match true distribution (CPRS)
* point forecasts generated from disribution can be taylored to desired cost func e.g. MAE, RMSE, etc.

Results
* point forecasts barely beat persistence, but GL-Normal densities work better than all benchmarks
* dist. forecasts much better than benchmarks in terms of CPRS
* got speed-dependent variance, including saturation at {0,1}
* also captured direction-dependent variance},
  file      = {Pinson10probForcast.pdf:Pinson10probForcast.pdf:PDF},
  groups    = {Read, PointDerived, doReadWPV_2},
  owner     = {scot},
  timestamp = {2010.05.28},
  url       = {http://www2.imm.dtu.dk/~pp/publis.htm},
}

@Article{OrtegaVazquez10windOpCost,
  author    = {Ortega-Vazquez, M.A. and Kirschen, D.S.},
  title     = {Assessing the Impact of Wind Power Generation on Operating Costs},
  journal   = {Smart Grid, IEEE Transactions on},
  year      = {2010},
  volume    = {1},
  number    = {3},
  pages     = {295--301},
  month     = dec,
  issn      = {1949-3053},
  abstract  = {Wind power generation is taking an increasing share of the overall energy production in many power systems. While its low marginal operating cost reduces the overall cost of meeting the demand for electrical energy, the stochastic and intermittent nature of wind generation increases the uncertainty that the system operators face and obliges them to procure additional reserve capacity. This paper presents a methodology for quantifying fully the effect of wind power generation on the various components of the cost of operating the system.},
  comment   = {Stochastic optimization shows cost of wind power variability and forecast errors.

Wind reduces cost.
* complete power system simulation explained, could be a reference for our own experiments
* cost includes C02 emissions
-- not clear if would save money w/o emissions cost
* forecast error simulation is perhaps too simple

Figure of merit
* fraction of ideal operational cost benefit from wind which is available after forecast errors are included
* averaged over the test period, I think (eq. 3 and PHI on p. 299)

* includes cost of C02 emissions, w/ some standard value (eq. 24 and following paragraph)

Data and simulation
* Spanish wind power data (but I don't understand how the times were chosen
-- some kind of repeating
-- only use six days?
* forecasts --> some power curve
* turbine locations simulatedd individually
-- so spatial correlation accounted for
-- BUT spatial correlations aren't lagged! A big weakness, I think!
-- worthwhile if you've already got the plant sum?
-- why are they showing GERMAN data in figure 2?
* A standard IEEE power system model is optimized
* calculation on only load shed seems too simple
* load shed enters optimization, as in our spinning reserves paper

* spinning reserves happens after displatch, but I don't quite get how.
* Assume gaussian normal forecast errors
-- scenarios generated as in Pinson09probFrcstStatScenWind but w/o Gaussian copula
-- up to 93\% of wind power forecasting errors are Gaussian
-- really?
---- contradicts Pierre and Miquel)
---- isn't the other 7\% composed of extreme events that cause real-life increased reserve requirements?

Conclusions
* it's better to have a positive bias in forecasts
-- overpromise wind power (at least on the sites that overpromise naturally)
-- best results if have farms which both under and over promise
* a power system w/ tight ramp rates causes a lot of wind spillage (curtailment)
* wind power clearly saves cost, but saves a little less (incrementally) as wind penetration increases
* savings are larger for sites with smaller standard deviation of wind power
* save more if system has more up/down flexibility
* forecasting errors definitely cost money},
  doi       = {10.1109/TSG.2010.2081386},
  file      = {Ortega-Vazquez10windOpCost.pdf:Ortega-Vazquez10windOpCost.pdf:PDF},
  groups    = {Read, Test},
  keywords  = {electrical energy demand;energy production;intermittent nature;operating costs;power systems;stochastic nature;wind power generation;power generation economics;power systems;wind power plants;},
  owner     = {scot},
  timestamp = {2011.04.18},
}

@Article{Taylor09windPowDensFrcst,
  author    = {Taylor, J.W. and McSharry, P.E. and Buizza, R.},
  title     = {Wind Power Density Forecasting Using Ensemble Predictions and Time Series Models},
  journal   = {Energy Conversion, IEEE Transactions on},
  year      = {2009},
  volume    = {24},
  number    = {3},
  pages     = {775--782},
  month     = sep,
  issn      = {0885-8969},
  abstract  = {Wind power is an increasingly used form of renewable energy. The uncertainty in wind generation is very large due to the inherent variability in wind speed, and this needs to be understood by operators of power systems and wind farms. To assist with the management of this risk, this paper investigates methods for predicting the probability density function of generated wind power from one to ten days ahead at five U.K. wind farm locations. These density forecasts provide a description of the expected future value and the associated uncertainty. We construct density forecasts from weather ensemble predictions, which are a relatively new type of weather forecast generated from atmospheric models. We also consider density forecasting from statistical time series models. The best results for wind power density prediction and point forecasting were produced by an approach that involves calibration and smoothing of the ensemble-based wind power density.},
  comment   = {A wind ensemble to wind power density forecasting paper.},
  doi       = {10.1109/TEC.2009.2025431},
  file      = {Taylor09windPowDensFrcst.pdf:Taylor09windPowDensFrcst.pdf:PDF},
  groups    = {Ensemble, doReadWPV_2},
  keywords  = {ensemble predictions;point forecasting;probability density function;renewable energy;time series models;wind generation;wind power density forecasting;time series;wind power;},
  owner     = {scot},
  timestamp = {2010.11.23},
}

@Article{Wangdee06windLoadWpeedCorr,
  author    = {Wangdee, W. and Billinton, R.},
  title     = {Considering load-carrying capability and wind speed correlation of WECS in generation adequacy assessment},
  journal   = {Energy Conversion, IEEE Transactions on},
  year      = {2006},
  volume    = {21},
  number    = {3},
  pages     = {734--741},
  month     = sep,
  issn      = {1558-0059},
  abstract  = {Wind power is an intermittent energy source that behaves quite differently from conventional energy sources. The reliability impact of this highly variable energy source is an important aspect that needs to be assessed as wind power penetration becomes increasingly significant. Generation adequacy assessment including wind energy conversion systems (WECS) at multiple locations is described in this paper. Effective load-carrying capabilities (ELCC) obtained using the loss of load expectation (LOLE) and the loss of load frequency (LOLF) for a power system containing WECS are illustrated and compared. The results show that ELCC obtained using the LOLF and obtained using the LOLE for WECS can be considerably different, while they are similar for a conventional generating unit. The impact on the system reliability indices of wind speed correlation between two wind farms is also examined. The studies show that the degree of wind speed correlation between two wind farms has a considerable impact on the resulting reliability indices. The sequential Monte Carlo simulation approach is used as this methodology can facilitate a time series modeling of wind speeds, and also provides accurate frequency and duration assessments. An autoregressive moving average time series model is used in this study to simulate hourly wind speeds.},
  comment   = {Models aggregation of wind farm output and compared to demand Monte- carlo and ARMA modeling. Useful for optimization? Maybe useful forturbine interdependence?},
  doi       = {10.1109/TEC.2006.875475},
  file      = {Wangdee06windLoadWpeedCorr.pdf:Wangdee06windLoadWpeedCorr.pdf:PDF;Wangdee06windLoadWpeedCorr.pdf:Wangdee06windLoadWpeedCorr.pdf:PDF},
  keywords  = { Monte Carlo methods, power generation reliability, time series, wind power plants effective load-carrying capability, generation adequacy assessment, load-carrying capability, loss of load expectation, loss of load frequency, sequential Monte Carlo simulation approach, system reliability, time series modeling, variable energy source, wind energy conversion systems, wind power penetration, wind speed correlation},
  owner     = {sotterson},
  timestamp = {2008.12.05},
}

@InProceedings{Zhao12EconDsptchStochOptVar,
  author    = {Zhao, Z. and Tong, X.J.},
  title     = {Economic dispatch of wind integrated power systems with a conditional risk method},
  booktitle = {Advances in Power System Control, Operation and Management (APSCOM 2012), 9\textsuperscript{th} IET International Conference on},
  year      = {2012},
  pages     = {1--6},
  abstract  = {Wind power is considered as one of the most promising and fastest growing renewable energy due to its low generation cost and the minimum environmental impact. However, with the features of randomness and non-scheduling, which brings risks to the operation stability and dispatch security of power system incorporating wind power. Due to the randomness of wind power and operation risk of power system, conditional value-at-risk (CVaR) theory is introduced in this paper, in order to quantify the uncertain risk of power system incorporating wind power. To characterize the security of power system by insecure probability as a risk constraint, quantization of the risk function has been realized by Monte Carlo simulation, An optimal objective function is constructed by energy-saving object, Then construct the stochastic optimization dispatch model based on conditional risk value by penalized the previous objective function model. In this paper, the penalized stochastic optimization model is solved by level-function method. A subgradient variable is introduced in level-function method to solve the optimum solutions, and put them back to objective function to calculate the optimum value. An IEEE-30 bus system is tested in this paper to analyze the proposed methods of solving the stochastic optimization model under different confidence levels, different CVaR and different capacity of wind power, Result shows that the level function method have much less compute time than linear program method, it provides theoretical basis for an economic and security stochastic optimization dispatch of power system.},
  comment   = {Economic dispatch w/ VaR criteria and stochastic optimization},
  doi       = {10.1049/cp.2012.2130},
  file      = {Zhao12EconDsptchStochOptVar.pdf:Zhao12EconDsptchStochOptVar.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.12.04},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615040},
}

@InProceedings{Makarov08futureImpactWindBPA,
  author    = {Makarov, Y. V. and S. Lu and B. McManus and J. Pease},
  title     = {The Future Impact of Wind on {BPA} Power System Ancillary Services},
  booktitle = {Transmission and Distribution Conference, IEEE},
  year      = {2008},
  number    = {315},
  month     = jun,
  abstract  = {Wind power is growing in a very fast pace as an
alternative generating resource. As the ratio of wind power over
total system capacity increases, the impact of wind on various
system aspects becomes significant. This paper presents a
methodology to study the future impact of wind on BPA power
system ancillary services including load following and regulation.
Existing approaches for similar analysis include dispatch model
simulation and standard deviation evaluation. The methodology
proposed in this paper uses historical data and stochastic
processes to simulate the load balancing processes in BPA power
system. Then capacity, ramp rate and ramp duration
characteristics are extracted from the simulation results, and load
following and regulation requirements are calculated accordingly.
It mimics the actual power system operations therefore the results
can be more realistic yet the approach is convenient to perform.
Further, the ramp rate and ramp duration data obtained from
the analysis can be used to evaluate generator response or
maneuverability and energy requirement, respectively, additional
to the capacity requirement.
Index Terms?Wind Integration, Ancillary Service, Load
Following, Regulation, Hour-ahead Schedule, Real-time Schedule},
  comment   = {detailed descriptions of regulation and load following requirments (as used in simulation of BPA). could use this for optimization.

Uses swinging doors ramp definition.

CAISO10caisoRnwblIntegSwngDr},
  file      = {Makarov08futureImpactWindBPA.pdf:Makarov08futureImpactWindBPA.pdf:PDF},
  groups    = {DOE-PNL09},
  location  = {Houston, TX},
  owner     = {sotterson},
  timestamp = {2009.03.05},
}

@InBook{Weber08stochProgWindInteg,
  chapter   = {WILMAR: A Stochastic Programming Tool to Analyze the Large-Scale Integration of Wind},
  pages     = {437--458},
  title     = {Optimization in the Energy Industry},
  publisher = {Springer},
  year      = {2008},
  author    = {Christoph Weber and Peter Meibom and R?udiger Barth and Heike Brand},
  editor    = {Josef Kallrath, Panos M. Pardalos, Steffen Rebennack and Max Scheidt},
  number    = {19},
  series    = {Energy Systems},
  abstract  = {Wind power is highly variable and partly unpredictable and therefore energy systems of the future have to cope with increased variability and stochasticity. The paper describes the use of a novel stochastic programming model to assess the impact of increased wind power generation on electricity systems. This WILMAR model takes explicitly the stochastic behavior of wind generation and the forecast errors into account. Also a detailed modeling of power plant, grid and market characteristics is performed. WILMAR thus allows to assess the impact of increased wind generation on reserve needs and usage, power plant operation and system cost. Key words: Wind power, Electricity system, Stochastic programming, Electricity markets, Reserve power},
  comment   = {Wind power market modeling software, including reserves and forecast errors. It's free, I think.},
  file      = {Weber08stochProgWindInteg.pdf:Weber08stochProgWindInteg.pdf:PDF;Weber08stochProgWindInteg.pdf:Weber08stochProgWindInteg.pdf:PDF},
  groups    = {DOE-PNL09, Use, doReadWPV_2},
  location  = {Berlin, Heidelberg},
  owner     = {sotterson},
  timestamp = {2009.03.03},
}

@Article{Zugno13tradeWindGenMrktQs,
  author    = {Zugno, M. and J{\'o}nsson, T. and Pinson, P.},
  title     = {Trading wind energy on the basis of probabilistic forecasts both of wind generation and of market quantities},
  journal   = {Wind Energy},
  year      = {2013},
  volume    = {16},
  number    = {6},
  pages     = {909--926},
  issn      = {1099-1824},
  abstract  = {Wind power is not easily predictable and non-dispatchable. Nevertheless, wind power producers are increasingly urged to participate in electricity market auctions in the same manner as conventional power producers. The aim of this paper is to propose an operational strategy for trading wind energy in liberalized electricity markets and to assess its performance. At first, the so-called optimal quantile strategy is revisited. It is proved that without market power, i.e. under the price-taker assumption, this strategy maximizes expected market revenues. Forecasts of wind power production, of day-ahead and real-time market prices and of the system imbalance are inputs to this strategy. Subsequently, constraining of the bid that maximizes the expected revenues is proposed as a way to overcome the strategy's disregard of practical limitations and, at the same time, of risk. Two constraining techniques are introduced: constraining in the decision space and in the probability space. Finally, the trade of a wind power producer is simulated in a test case for the Eastern Danish (DK-2) price area of the Nordic Power Exchange (Nord Pool) during a 10 month period in 2008. The results of the test case show the financial benefits of the aforementioned strategy as well as the consequent interaction with the electricity market. This study will support a demonstration in the framework of the EU project ANEMOS.plus.},
  comment   = {Optimal quantile trading given price and wind generation forecasts for Danish Nordpool. The paper that Pierre-Julien will base his IRPWIND 82.4 code upon.

* day ahead forecast errors fixed mostly in regulation ("realtime") markets, not intraday (cite to Weber10AdqtMktDsngEur)
* quantiles are optimal: many cites, theory discussed in Gneiting11quantOptFrcst (but does this require asymmetric penalties?)
* stoch. mkt. prices generalize Pinson07tradeWindProbFrcst and Bremnes04windLocQR
* Tryggvi had a 2013 price forecasting paper (Jonsson13elecPriceQR). Did it use this idea?
* Marco had a 2010 paper modeling market feedback (Giabardo10feedbackElecMkt). Does this paper use that idea?},
  doi       = {10.1002/we.1531},
  file      = {Zugno13tradeWindGenMrktQs.pdf:Zugno13tradeWindGenMrktQs.pdf:PDF},
  keywords  = {electricity markets, probabilistic forecasting, stochastic optimization, decision theory},
  owner     = {sotterson},
  timestamp = {2015.03.04},
}

@InProceedings{Wang16evalStochMethScen,
  author    = {Yishen Wang and Zhi Zhou and Cong Liu and A. Botterud},
  title     = {Evaluating stochastic methods in power system operations with wind power},
  booktitle = {Proc. IEEE Int. Energy Conf. (ENERGYCON)},
  year      = {2016},
  pages     = {1--6},
  month     = apr,
  abstract  = {Wind power is playing an increasingly important role in electricity markets. However, it's inherent variability and uncertainty cause operational challenges and costs as more operating reserves are needed to maintain system reliability. Several operational strategies have been proposed to address these challenges, including advanced probabilistic wind forecasting techniques, dynamic operating reserves, and various unit commitment (UC) and economic dispatch (ED) strategies under uncertainty. This paper presents a consistent framework to evaluate different operational strategies in power system operations with renewable energy. We use conditional Kernel Density Estimation (KDE) for probabilistic wind power forecasting. Forecast scenarios are generated considering spatio-temporal correlations, and further reduced to lower the computational burden. Scenario-based stochastic programming with different decomposition techniques and interval optimization are tested to examine economic, reliability, and computational performance compared to deterministic UC/ED benchmarks. We present numerical results for a modified IEEE-118 bus system with realistic system load and wind data.},
  comment   = {Scenario-based stochastic programming reduces costs against deterministic unit commitment.  Simulation uses wind power scenarios generated from spati-temporal correlation model.  Test is  on IEEE 118 bus w/ "realistic load" and wind data.

* Use a Pinson paper for scenario generation (Papaefthymiou08spatioTempModel or Papaefthymiou09copulaPowUncert)
* Robust unit committment also mentioned but it's just said that it's "very conservative"
* scenario reduction mentioned but does not seem to have been done
* seem to use "decomposition" (2 or maybe 3 kinds) instead},
  doi       = {10.1109/ENERGYCON.2016.7514051},
  file      = {Wang16evalStochMethScen.pdf:Wang16evalStochMethScen.pdf:PDF},
  keywords  = {load forecasting, power generation dispatch, power generation economics, power generation reliability, power generation scheduling, power markets, stochastic programming, wind power plants, ED strategy, UC strategy, UC-ED benchmarks, advanced probabilistic wind forecasting technique, conditional KDE, conditional kernel density estimation, decomposition technique, dynamic operating reserves, economic dispatch strategy, electricity markets, interval optimization, modified IEEE-118 bus system, power system operations, renewable energy, scenario-based stochastic programming, spatio-temporal correlation, stochastic methods, system reliability, unit commitment strategy, wind power, Forecasting, Mathematical model, Probabilistic logic, Stochastic processes, Uncertainty, Wind forecasting, Wind power generation, Decomposition, Dynamic Operating Reserves, Electricity Markets, Interval Programming, Stochastic Programming, Wind Power},
  owner     = {sotterson},
  timestamp = {2017.03.11},
}

@Article{Zhang12optBidMktDAwind,
  author    = {Zhang, Haifeng and Gao, Feng and Wu, Jiang and Liu, Kun and Liu, Xiaolin},
  title     = {Optimal bidding strategies for wind power producers in the day-ahead electricity market},
  journal   = {Energies},
  year      = {2012},
  volume    = {5},
  number    = {11},
  pages     = {4804--4823},
  abstract  = {Wind Power Producers (WPPs) seek to maximize profit and minimize the
imbalance costs when bidding into the day-ahead market, but uncertainties in the hourly
available wind and forecasting errors make the bidding risky. This paper assumes that
hourly wind power output given by the forecast follows a normal distribution, and proposes
three different bidding strategies, i.e., the expected profit-maximization strategy (EPS), the
chance-constrained programming-based strategy (CPS) and the multi-objective bidding
strategy (ECPS). Analytical solutions under the three strategies are obtained. Comparisons
among the three strategies are conducted on a hypothetical wind farm which follows the
Spanish market rules. Results show that bid under the EPS is highly dependent on market
clearing price, imbalance prices, and also the mean value and standard deviation of wind
forecast, and that bid under the CPS is largely driven by risk parameters and the mean
value and standard deviation of the wind forecast. The ECPS combining both EPS and CPS
tends to choose a compromise bid. Furthermore, the ECPS can effectively control
the tradeoff between expected profit and target profit for WPPs operating in volatile
electricity markets.
Keywords: wind power; bidding; day-ahead electricity market; risk; chance-constrained
programming; multi-objective optimization},
  comment   = {Probabilistic day-ahead wind market bidding.},
  file      = {Zhang12optBidMktDAwind.pdf:Zhang12optBidMktDAwind.pdf:PDF},
  owner     = {sotterson},
  publisher = {Multidisciplinary Digital Publishing Institute},
  timestamp = {2015.04.17},
  url       = {https://www.mdpi.com/1996-1073/5/11/4804/pdf},
}

@Article{Pinson09adaptMarkovAR,
  author    = {P. Pinson and H. Madsen},
  title     = {Adaptive modeling and forecasting of wind power fluctuations with {Markov}-switching autoregressive models},
  journal   = {International Journal of Forecasting},
  year      = {2012},
  volume    = {31},
  pages     = {281--313},
  abstract  = {Wind power production data averaged at a few-minute rate exhibits successive periods with fluctuations of various dynamical nature, and of smaller and larger magnitude, which cannot be explained (so far) by the evolution of some explanatory variable. A relevant proposal is then to capture this regime-switching behaviour with an approach relying on Markov-Switching AutoRegressive (MSAR) models. An appropriate parameterization of the model coefficients is introduced, along with an adaptive estimation method allowing to accommodate long-term variations in the process characteristics. The objective criterion to be recursively optimized is based on penalized maximum-likelihood, with exponential forgetting of past observations. MSAR models are then employed for 1-step ahead point forecasting of 10-minute resolution time-series of wind power at two large offshore wind farms. They are favourably compared against persistence and AutoRegressive (AR) models. The interest of MSAR models for interval forecasting is finally evaluated and discussed.},
  comment   = {density forecasts submitted, not published},
  file      = {Journal paper submission, published in 2012:Pinson12adaptMarkovAR.pdf:PDF;Pinson09adaptMarkovAR.pdf:Pinson09adaptMarkovAR.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.03.04},
  url       = {http://www2.imm.dtu.dk/~pp/publis.htm},
}

@Article{Gallego11spdDirWindPowDyn,
  author    = {C. Gallego and P. Pinson and H. Madsen and A. Costa and A. Cuerva},
  title     = {Influence of local wind speed and direction on wind power dynamics ? Application to offshore very short-term forecasting},
  journal   = {Applied Energy},
  year      = {2011},
  volume    = {88},
  number    = {11},
  pages     = {4087--4096},
  issn      = {0306-2619},
  abstract  = {Wind power time series usually show complex dynamics mainly due to non-linearities related to the wind physics and the power transformation process in wind farms. This article provides an approach to the incorporation of observed local variables (wind speed and direction) to model some of these effects by means of statistical models. To this end, a benchmarking between two different families of varying-coefficient models (regime-switching and conditional parametric models) is carried out. The case of the offshore wind farm of Horns Rev in Denmark has been considered. The analysis is focused on one-step ahead forecasting and a time series resolution of 10{~}min. It has been found that the local wind direction contributes to model some features of the prevailing winds, such as the impact of the wind direction on the wind variability, whereas the non-linearities related to the power transformation process can be introduced by considering the local wind speed. In both cases, conditional parametric models showed a better performance than the one achieved by the regime-switching strategy. The results attained reinforce the idea that each explanatory variable allows the modelling of different underlying effects in the dynamics of wind power time series.},
  comment   = {Wind speed and direction have separate effects on wind power variability.},
  doi       = {10.1016/j.apenergy.2011.04.051},
  keywords  = {Energy systems modelling},
  owner     = {sotterson},
  timestamp = {2014.07.20},
}

@InProceedings{Lu14pairCopulaSpatWindPow,
  author    = {Q. Lu and W. Hu and Y. Min and F. Yuan and Z. Gao},
  title     = {Wind power uncertainty modeling considering spatial dependence based on Pair-copula theory},
  booktitle = {Proc. IEEE PES General Meeting | Conf. Exposition},
  year      = {2014},
  pages     = {1--5},
  month     = jul,
  abstract  = {Wind power uncertainty modeling forms the foundation of stochastic optimization problems in wind power integration. Due to the similarity of meteorological conditions, outputs of adjacent wind farms have a natural consistency, as well as the forecast errors. Therefore, spatial dependence is imperative for joint wind power uncertainty model, especially for power flow optimizations and transmission risk assessments. The Pair-copula theory is introduced in this paper to construct the spatial relevance for multiple wind farms. The detailed modeling procedure, including model selection, parameter fitting, goodness-of-fit testing and random scenario generation, are explicitly presented. Simulation results show that Pair-copula is flexible in high dimension correlation analyses and can effectively improve the model accuracy.},
  comment   = {Uses vine/pair copula to model wind power forecast uncertainty.  Compare with Becker16spatTempWndFrcstErrGrid},
  doi       = {10.1109/PESGM.2014.6938902},
  file      = {Lu14pairCopulaSpatWindPow.pdf:Lu14pairCopulaSpatWindPow.pdf:PDF},
  issn      = {1932-5517},
  keywords  = {Gaussian distribution, load flow, optimisation, risk management, stochastic programming, wind power plants, Gaussian distribution, detailed modeling procedure, forecast errors, goodness-of-fit testing, high dimension correlation analyses, meteorological condition similarity, model accuracy improvement, model selection, natural consistency, pair-copula theory, parameter fitting, power flow optimizations, random scenario generation, spatial dependence, spatial relevance, stochastic optimization problems, transmission risk assessments, wind farms, wind power integration, wind power uncertainty modeling, Joints, Optimization, Stochastic processes, Uncertainty, Wind farms, Wind forecasting, Wind power generation, D-Vine decomposition, Pair-copula, spatial dependence, wind power scenario generation, wind power uncertainty},
  owner     = {sotterson},
  timestamp = {2017.03.11},
}

@TechReport{Otterson11scenGenReview,
  author      = {Scott Otterson and Henrik Madsen and Pierre Pinson and Tryggvi J{\'o}nsson},
  title       = {Scenario generation: A review},
  institution = {Technical University of Denmark (DTU)},
  year        = {2011},
  number      = {IMM-Technical Report-2011-08},
  abstract    = {Wind power, while abundant and benign, is only available when the wind blows. If wind forecasts
were perfectly accurate, power producers could schedule in advance the exact amount of conventional
energy needed to make up for predicted low wind production. But forecasts are not perfectly accurate,
as is illustrated in Fig. 1.1, which shows a wind power time series, along with its forecasted value and
estimated error intervals. The blue contours cover the range of potential outcomes, which generally
increase with look-ahead time; the red line is the conditional expectation for each look-ahead time;
and the black line is the power that was actually measured. The error distribution also depends upon
the predicted wind power, and for this reason it is well known that forecasting via classical linear time
series analysis [2, 31] is inadequate.
Because of forecast uncertainty, power system operators must limit the risk of underproduction by
scheduling additional conventional reserve capacity, which may or may not be used. Conversely, they
may also need to waste wind power when inflexible conventional production has been scheduled and
wind power becomes available at an unpredicted time. Optimally scheduling conventional capacity is
crucial, as too many scheduling errors in a mixed conventional/wind system can cause more greenhouse
gas emissions than would be emitted by a system using conventional sources alone [41, 52].
The risk of over or under scheduling conventional production can be optimally balanced with
stochastic programming, in which future demand and production are simulated based on each of their
point forecasts and a randomly generated realization of their errors. A scheduling decision made at
one point in time affects subsequent scheduling options, so each decision must be made considering the
error probabilities at several decision points into the future; probabilistic forecasts like the one shown
in Fig. 1.1 are not adequate stochastic programming inputs, as they provide statistically independent
error distributions at each forecast look-ahead time. Therefore, the stochastic programming algorithm
needs a set of likely time series ? scenarios ? which capture temporal error dependence. A set of
scenarios corresponding to the probabilistic forecast in Fig. 1.1 is shown in 1.2.
Fig. 1.3. illustrates scenario generation in the reserve allocation process. A model of forecast
errors is estimated using historical demand and wind (and/or power) forecasts. Then, when it is
time to schedule reserve allocations, the errors of the latest forecasts are simulated by driving the
error models with random perturbations. The result is a set of likely scenarios, which a stochastic
programming algorithm uses to determine the final allocation. For each of a number of scenarios,
conventional sources are virtually scheduled to minimize the economic cost of simulated consumption
when given simulated wind production; the actual reserve scheduling is derived from the simulation
results. Depending upon the allocation method, the computational burden may be decreased by first
reducing the size of the set of scenarios. See [27] for an example with scenario reduction and [44] for
an example without it.
In this review, we discuss the problem of scenario generation. We begin with desirable scenario
performance attributes, and then summarize scenario generation and reduction algorithms which we
believe are relevant to wind power system integration.},
  comment     = {Good info on copulas and other scenario generation methods and performance metrics that I wrote down for the OSRnordic project, while working at DTU.

Use CRPSS but the reference forecast is not mentioned. Probably, it's an unconditional Gaussian based on price standard deviation, as in Jonsson13elecPriceQR. For wind, maybe it would be the unconditional quantile estimate, or climatology.},
  file        = {Otterson11scenGenReview.pdf:Otterson11scenGenReview.pdf:PDF;:Otterson11scenGenReview.docx:Word 2007+},
  groups      = {Ensemble, PointDerived, Test, doReadNonWPV_1},
  location    = {Kgs. Lyngby, Denmark},
  owner       = {sotterson},
  publisher   = {DTU Informatics, Building 305},
  series      = {IMM-Technical Report-2011-08},
  timestamp   = {2013.04.12},
  url         = {http://orbit.dtu.dk/en/publications/scenario-generation-a-review%28b7979f7a-cb76-46eb-bbf4-58719104a276%29.html},
}

@InProceedings{Hagemann09vExtrapTemp,
  author    = {S. Hagemann and B. Lange and Y. Saint-Drenan and J. Tambke},
  title     = {Uncertainty in the vertical extrapolation of the wind speed due to the measurement accuracy of the temperature difference},
  booktitle = {European Wind Energy Conference and Exhibition (EWEC)},
  year      = {2009},
  abstract  = {Wind resource assessment is a critical issue for the
development of offshore wind energy. Most offshore meteorological
masts are measuring the wind speed below 30 meters, which raises
the need of extrapolating the wind speed to hub height. For an
accurate vertical extrapolation, the consideration of the
atmospheric stability is required. The gradient method is often used
for determining the atmospheric stability. One disadvantage of this
method is that it is very sensitive to temperature measurement
errors. In this paper, the impact of the temperature measurement
accuracy on the interpolated wind speed is investigated by
conducting a sensitivity analysis. It was shown that the influence of
the measurement accuracy of the temperature difference on the
extrapolated wind speed can be very large, especially at low wind
speed and for stable conditions.
Keywords: wind energy, offshore, vertical},
  comment   = {Vertical extrapolation most sensitve for low wind speeds, highly stable atmosphere
* one of 2 papers on Saskia Hagemann's (NORSEWInD forecasting lead) web site
* sensitivity analysis of theoretical extrapolation equations
* Two areas where temp matters most
1 . low wind speed (do we care? Can it hallucinate high wind speed?)

2. stable atmosphere
* recommendation of required thermometer accuracy left to another paper

Anyway, since forecasted temperature must also be used for extrapolation, and since this is inaccurate, and since the inaccuracy matters most with low wind speed and stable atmosphere, then the triad of:

-- temp
-- wind speed
-- stability parameters (Z or something named like that)

may provide a hint for probabilistic forecasts i.e. Z should be added to the list of features going into probabilsitic forecast feature selection.},
  file      = {Hagemann09vExtrapTemp.pdf:Hagemann09vExtrapTemp.pdf:PDF},
  groups    = {Read, ErrDistProps, doReadNonWPV_1},
  owner     = {scot},
  timestamp = {2010.05.27},
  url       = {http://www.iset.uni-kassel.de/pls/w3isetdad/www_iset_new.main_page?p_lang=eng&p_owa_call=www_veroeff.show_veroeffdaten%3Fp_veroeff_nr=2209%26p_lang=eng},
}

@InProceedings{Liangyou08windFrcstRecurNeural,
  author    = {Hong Liangyou and Jiang Dongxiang and Huang Qian and Ding Yongshan},
  title     = {Wind Speed Forecasting Using Fully Recurrent Neural Network in Wind Power Plants},
  booktitle = {World Wind Energy Conference},
  year      = {2008},
  abstract  = {Wind speed forecasting is very important to the operation of wind power plants and power systems. Because of its nonlinearity and non-stationary, wind forecasting is a severe task. This paper deals with the problem of short-term wind speed forecasting based on historical time-serial meteorological data. A kind of recurrent neural network called PRNN (pipelined recurrent neural network) is adopted here to make prediction. It is a NARMAX ANN model (nonlinear auto regressive moving average artificial neural network with external inputs). First, the raw wind speed data is processed to be steady using logarithm difference method. Then, the phase space reconstruction method of chaotic theory is adopted to determine embedded dimension and time delay. Third, based on the selected embedded dimension and time delay, we develop a PRNN+TDL (tapped-delay-line filter) ANN model. Several on-line learning and optimal algorithms are used to train network. Finally, the ANN predicted result is amended based on the statistic characteristic of wind speed. The model is tested at a wind power plant over one year period. The raw data interval is 1 minutes. And predication term is about five minutes to one hour. The result shows that forecasting accuracy was effectively improved by the proposed method. The average predication error is within 10\%. In additional, model parameters are very important factors affecting predication precision. Different type wind data need different parameters.},
  comment   = {Delay embedding, neural nets for wind power forecastng - they warp wind speed with shifted sigmoid that fits between (0,1) -- but they are predicting speed, not power - then they calc a 1\textsuperscript{st} order difference of this, say this is necessary - select delay time (dynamical system lag) at 1\textsuperscript{st} zero of mutual information - a mix of linear tapped delay line and pipelined neural net},
  file      = {Liangyou08windFrcstRecurNeural.pdf:Liangyou08windFrcstRecurNeural.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.02.12},
  url       = {http://www.ontario-sea.org/Page.asp?PageID=1209&ContentID=1094},
}

@InProceedings{Khodayar15RobustDeepNNwindSpdPred,
  author    = {M. Khodayar and M. Teshnehlab},
  title     = {Robust deep neural network for wind speed prediction},
  booktitle = {Proc. 4th Iranian Joint Congress Fuzzy and Intelligent Systems (CFIS)},
  year      = {2015},
  pages     = {1--5},
  month     = sep,
  abstract  = {Wind speed prediction is a basic requirement of wind energy generation with large generation capacity for large-scale wind power penetration. The intermittency and stochastic quality of wind speed leads to a big challenge for high penetration of wind power in electricity systems due to error-prone wind speed prediction methods. There are many artificial neural network (ANN) approaches proposed in the recent literature in order to tackle this problem. However, hand engineering features and applying shallow architectures can lead to poor prediction performance in these methodologies. Deep neural networks are ANNs with great generalization capability that can automatically extract meaningful features from the data with as less prior knowledge as possible. In this paper we propose a stacked auto-encoder (SAE) neural network for ultra-short-term and short-term wind speed prediction. To the best of our knowledge this is the first paper that applies deep learning on wind speed prediction of a wind site. Moreover, a rough regression layer is applied at the top of this model in order to deal with uncertainty factors existing in the wind speed data. Experimental results show significant improvement compared to other ANN methods that applied shallow architectures in the literature and the traditional SAE.},
  doi       = {10.1109/CFIS.2015.7391664},
  file      = {:Khodayar15RobustDeepNNwindSpdPred.pdf:PDF},
  keywords  = {feature extraction, generalisation (artificial intelligence), learning (artificial intelligence), neural nets, power engineering computing, regression analysis, wind power, ANN approach, SAE neural network, artificial neural network, data feature extraction, deep learning, electricity, generalization capability, large-scale wind power penetration, robust deep neural network, rough regression layer, stacked autoencoder neural network, ultrashort-term wind speed prediction, uncertainty factors, wind energy generation, wind site, wind speed data, wind speed intermittency, wind speed stochastic quality, Artificial neural networks, Data models, Neurons, Training, Uncertainty, Wind speed, Artificial neural network, Deep neural network, Stacked auto-encoder, Uncertainty, Wind speed},
}

@Article{SalcedoSanz09frcstBankNN,
  author    = {Sancho Salcedo-Sanz and {\'A}ngel M. P{\'e}rez-Bellido and Emilio G. Ortiz-Garc{\'i}a and Antonio Portilla-Figueras and Luis Prieto and Francisco Correoso},
  title     = {Accurate short-term wind speed prediction by exploiting diversity in input data using banks of artificial neural networks},
  journal   = {Neurocomputing},
  year      = {2009},
  volume    = {72},
  number    = {4 - 6},
  pages     = {1336--1341},
  issn      = {0925-2312},
  note      = {<ce:title>Brain Inspired Cognitive Systems (BICS 2006) / Interplay Between Natural and Artificial Computation (IWINAC 2007)</ce:title>},
  abstract  = {Wind speed prediction is a very important part of wind parks management. Currently, hybrid physical-statistical wind speed forecasting models are used to this end, some of them using neural networks as the final step to obtain accurate wind speed predictions. In this paper we propose a method to improve the performance of one of these hybrid systems, by exploiting diversity in the input data of the neural network part of the system. The diversity in the data is produced by the physical models of the system, applied with different parameterizations. Two structures of neural network banks are used to exploit the input data diversity. We will show that our method is able to improve the performance of the system, obtaining accurate wind speed predictions better than the one obtained by the system using single neural networks. Keywords Short-Term wind speed forecasting; Global forecasting models; Diversity in input data; Neural networks banks},
  comment   = {A factorial combo of multiple global models and multiple NWP downscaling resolutions improves up-to-48-hour-ahead results, includes a kind of "deep learning"
* Also explains NWP forecast hour trick used in Sweeney11frcstCombo

* MM5 mesoccale downscaler initialized by global model inputs as well as atmospheric soundings, balloons, ...

Baseline ("initial") downscale to wind speed forecast NN:
* forecast target is wind speed from turbine nacelles: look at average across 5 turbines
* wind park bigger than NWP grid so include both in NN forecast algorithm

* same six input NN forecast algorithm as in Sweeney11frcstCombo, including cos projection of forecast time

Advanced downscale to wind speed forecast NN
* NWP
-- three global models: GFS, NOGAPS, CMC
-- three MM5 resolutions: 1, 2 and 3 km ===> have 9 NWP predictions

* NN forecast algorithm
1.) oracle "best out of 9"
---- NN for each NWP input (same type as baseline, I think)
---- pick output of NN with least error at each time instant
2.) one-neuron bank
---- train a single neuron network on the outputs of the individual NN's
3.) hidden-layer-bank
---- a 2\textsuperscript{nd} hidden layer is added
---- individual NWP NN's retain the same architecture as before
---- but they're merged into a single NN with an extra layer for combining their outputs
---- This works slightly better
---- is this a little bit like "deep learning?"

However
* Poor test of generalization:
-- 50 NN trainings, keeping best on test set.
-- So I'm not sure I believe the better results on the more complex model.
* Sweeney11frcstCombo: doesn't use the 2 hidden layer NN trick found best here, maybe because it wasn't really better?},
  doi       = {10.1016/j.neucom.2008.09.010},
  file      = {SalcedoSanz09frcstBankNN.pdf:SalcedoSanz09frcstBankNN.pdf:PDF},
  groups    = {Read, Ensemble, doReadNonWPV_1},
  keywords  = {Short-Term wind speed forecasting},
  owner     = {sotterson},
  timestamp = {2013.03.13},
  url       = {http://www.sciencedirect.com/science/article/pii/S0925231208004505},
}

@TechReport{Wan10equivWindPowCurve,
  author      = {Wan, Yih-Huei and Ela, Erik and Orwig, Kirsten},
  title       = {Development of an Equivalent Wind Plant Power Curve},
  institution = {NREL},
  year        = {2010},
  number      = {Tech. Rep. NREL/CP-550-48146},
  abstract    = {Wind turbine manufacturers publish and certify power curves for their
turbines. These turbine power curves are used for planning purposes and
estimating total wind power production. When a wind plant consisting of
many turbines connects to the utility grid and starts operation, the focus
shifts to the entire plant?s performance. An equivalent wind plant power
curve becomes highly desirable and useful in predicting plant output for a
given wind forecast. The National Renewable Energy Laboratory (NREL) has
worked with a utility to collect detailed data from a large wind power
plant to develop such an equivalent power curve. This paper will (1)
summarize available data, methodology, and the results of this work; (2)
discuss the validation process; and (3) explore the applicability of such
an approach on other wind power plants without detailed data.

Performance of a single wind turbine can be characterized by a power curve
? a graphical representation of the turbine electric power output as a
function of the hub - height wind speed. With such a curve, turbine power
output and energy production can be predicted without detailed knowledge of
turbine operations and its control schemes. An equivalent power curve for
the entire plant can serve the same purpose for plant and system operators
to predict the plant output for a given wind speed. However, unlike the
single - turbine power curve, a single curve cannot capture all the nuances
of a wind plant consisting of tens or even hundreds of turbines. A model
consisting of a set of power curves is required to fully characterize the
complex input/output relationship of a wind plant to account for the
effects of different wind directions, local terrain, and asymmetric turbine
layout in a wind plant. NREL has obtained data from a large wind plant for
the development of such a model. The data include plant output, wind speed,
and direction from meteorological towers, and nacelle anemometer readings
and power output from each turbine. From this extensive dataset, the
distributions of wind speed and power production of all turbines are
analyzed over a wide range of wind speeds. From these distributions, a
model is constructed to generate an equivalent power curve for the plant.},
  comment     = {Building a power curve for a wind farm. Several methods. 100% wind power production change vs. direction based on binned power curves. Nice plots. Power curves estimated by some binning standard, by polynomical fitting, by neural nets and by a physical model.

My immediate reason for saving this is that it has the equation for density that goes into the power equation.

However, they found that "The addition of air density did not increase the amount of useful information significantly in the neural network model." But maybe they didn't do a proper NN feature selection?},
  file        = {Wan10equivWindPowCurve.pdf:Wan10equivWindPowCurve.pdf:PDF},
  journal     = {NREL, Golden, CO, Tech. Rep. NREL/CP-550-48146},
  location    = {Golden, CO},
  owner       = {sotterson},
  timestamp   = {2013.05.14},
  url         = {http://www.nrel.gov/docs/fy10osti/48146.pdf},
}

@InProceedings{Aloraini14grphMdlTurbFaultDiag,
  author    = {A. Aloraini and M. Sayed-Mouchaweh},
  title     = {Graphical Model Based Approach for Fault Diagnosis of Wind Turbines},
  booktitle = {Proc. 13th Int. Conf. Machine Learning and Applications},
  year      = {2014},
  pages     = {614--619},
  month     = dec,
  abstract  = {Wind turbine operation and maintenance costs depend on the reliability of its components. Thus, a critical task is to detect and isolate faults, as fast as possible, and restore optimal operating conditions in the shortest time. In this paper, a machine learning of graphical models approach is proposed for fault diagnosis of wind turbines, in particular pitch system. The role of the latter is to adjust the blade pitch angle by rotating it according to the current wind speed in order to optimize the wind turbine power production. This is achieved by a controller based on blade pitch angles measured by two redundant sensors in each blade. Without the sensor accuracy reading, the controller can be misled and fail to achieve the optimal control strategy according to the current operation conditions. In addition, pitch angle sensors complete failure can lead to dangerous actions of the controller, while fixed or drifted bias of sensor measurements may decrease the controller's efficiency. To better control and overcome these challenges, we propose a methodology that is based on Gaussian acyclic graphical models and the lasso estimate. The methodology has shown the ability to model, and diagnose faults that occur in the pitch system in wind turbines during its normal run and could lead to a fast recovery to the optimal operating conditions.},
  comment   = {ModernWindABS},
  doi       = {10.1109/ICMLA.2014.97},
  file      = {Aloraini14grphMdlTurbFaultDiag.pdf:Aloraini14grphMdlTurbFaultDiag.pdf:PDF},
  keywords  = {Gaussian processes, blades, fault diagnosis, fault tolerant control, optimal control, pitch control (position), regression analysis, wind turbines, Gaussian acyclic graphical models, Lasso estimate, blade pitch angle sensors, component reliability, drifted bias, fault detection, fault diagnosis, fault isolation, fixed bias, graphical model based approach, machine learning, optimal control strategy, optimal operating conditions, pitch system, redundant sensors, sensor accuracy reading, wind speed, wind turbine maintenance cost, wind turbine operation cost, wind turbine power production optimization, Benchmark testing, Blades, Fault diagnosis, Graphical models, Sensors, Wind turbines, Wind turbine, pitch system, the lasso estimate},
  owner     = {sotterson},
  timestamp = {2016.12.19},
}

@Article{Shokrzadeh14windPowCrvAdvModels,
  author    = {Shokrzadeh, S. and Jozani, M.J. and Bibeau, E.},
  title     = {Wind Turbine Power Curve Modeling Using Advanced Parametric and Nonparametric Methods},
  journal   = {IEEE Transactions on Sustainable Energy},
  year      = {2014},
  volume    = {5},
  number    = {4},
  pages     = {1262--1269},
  abstract  = {Wind turbine power curve modeling is an important tool in turbine performance monitoring and power forecasting. There are several statistical techniques to fit the empirical power curve of a wind turbine, which can be classified into parametric and nonparametric methods. In this paper, we study four of these methods to estimate the wind turbine power curve. Polynomial regression is studied as the benchmark parametric model, and issues associated with this technique are discussed. We then introduce the locally weighted polynomial regression method, and show its advantages over the polynomial regression. Also, the spline regression method is examined to achieve more flexibility for fitting the power curve. Finally, we develop a penalized spline regression model to address the issues of choosing the number and location of knots in the spline regression. The performance of the presented methods is evaluated using two simulated data sets as well as an actual operational power data of a wind farm in North America.},
  comment   = {Power curve models. Simple one and machine learning: best is penalized spline. Penalty is the old-style ridge (l-2 norm) penalty, not roughness.(really?)

* has expression for penalized spline effective degrees of freedom},
  doi       = {10.1109/TSTE.2014.2345059},
  file      = {Shokrzadeh14windPowCrvAdvModels.pdf:Shokrzadeh14windPowCrvAdvModels.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.10},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6894235},
}

@Article{Clifton13machLrnPowCurvTurbShear,
  author    = {Clifton, A and Kilcher, L and Lundquist, JK and Fleming, P},
  title     = {Using machine learning to predict wind turbine power output},
  journal   = {Environmental Research Letters},
  year      = {2013},
  volume    = {8},
  number    = {2},
  pages     = {024009},
  abstract  = {Wind turbine power output is known to be a strong function of wind speed, but is also affected by turbulence and shear. In this work, new aerostructural simulations of a generic 1.5 MW turbine are used to rank atmospheric influences on power output. Most significant is the hub height wind speed, followed by hub height turbulence intensity and then wind speed shear across the rotor disk. These simulation data are used to train regression trees that predict the turbine response for any combination of wind speed, turbulence intensity, and wind shear that might be expected at a turbine site. For a randomly selected atmospheric condition, the accuracy of the regression tree power predictions is three times higher than that from the traditional power curve methodology. The regression tree method can also be applied to turbine test data and used to predict turbine performance at a new site. No new data are required in comparison to the data that are usually collected for a wind resource assessment. Implementing the method requires turbine manufacturers to create a turbine regression tree model from test site data. Such an approach could significantly reduce bias in power predictions that arise because of the different turbulence and shear at the new site, compared to the test site.
Keywords: machine learning, classification and regression trees, wind energy, wind turbine},
  comment   = {In simulation, the top three features for power prediction are wind speed, turbulence intensity and wind speed shear across the disk. Maybe direction is missing because it's only 1 turbine and a simulation, but anyway, it looks like turbulence and shear are important.

Power curve is estimated with a regression curve, is said to be 3X more accurate than a traditional curve.

Has nice graphs of power curve vs. turbulence.  I put it in the colombia course slides.},
  file      = {Clifton13machLrnPowCurvTurbShear.pdf:Clifton13machLrnPowCurvTurbShear.pdf:PDF},
  owner     = {sotterson},
  publisher = {IOP Publishing},
  timestamp = {2014.11.08},
  url       = {http://iopscience.iop.org/1748-9326/8/2/024009},
}

@Article{Marquez12CondMonWindTurbMeth,
  author    = {M{\'a}rquez, Fausto Pedro Garc{\'\i}a and Tobias, Andrew Mark and P{\'e}rez, Jes{\'u}s Mar{\'\i}a Pinar and Papaelias, Mayorkinos},
  title     = {Condition monitoring of wind turbines: Techniques and methods},
  journal   = {Renewable Energy},
  year      = {2012},
  volume    = {46},
  pages     = {169--178},
  issn      = {0960-1481},
  abstract  = {Wind Turbines (WT) are one of the fastest growing sources of power production in the world today and there is a constant need to reduce the costs of operating and maintaining them. Condition monitoring (CM) is a tool commonly employed for the early detection of faults/failures so as to minimise downtime and maximize productivity. This paper provides a review of the state-of-the-art in the \{CM\} of wind turbines, describing the different maintenance strategies, \{CM\} techniques and methods, and highlighting in a table the various combinations of these that have been reported in the literature. Future research opportunities in fault diagnostics are identified using a qualitative fault tree analysis.},
  doi       = {http://dx.doi.org/10.1016/j.renene.2012.03.003},
  file      = {Marquez12CondMonWindTurbMeth.pdf:Marquez12CondMonWindTurbMeth.pdf:PDF},
  keywords  = {Fault detection and diagnosis, Condition monitoring, Maintenance management, Wind turbines },
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2016.12.19},
  url       = {http://www.sciencedirect.com/science/article/pii/S0960148112001899},
}

@TechReport{Milligan09windIntCost,
  author      = {Milligan, Michael R and Kirby, Brendan},
  title       = {Calculating wind integration costs: separating wind energy value from integration cost impacts},
  institution = {National Renewable Energy Laboratory},
  year        = {2009},
  abstract    = {Wind variability and uncertainty cause an increase in power system operating costs as increasing amounts of wind generation are incorporated into the power generation mix. Accurately calculating these costs is important so that wind generation can be fairly compared with alternative generation technologies. Methods for calculating wind integration costs have matured over the last few years with the incorporation of mesoscale wind modeling, time-synchronized load data, and full power system simulation, including security constrained unit commitment and economic dispatch. All methods calculate wind integration costs by comparing total power system costs with and without wind generation. A simple comparison of the with- and without-wind costs is not sufficient, however, because the value of the wind energy itself is also included in this difference. In order to remove the energy value bias and calculate only the wind integration cost, current methods substitute an energy proxy into the base case. Unfortunately, it is difficult to craft an energy schedule that can be placed into the base case that does not have significant capacity and/or differential energy value itself. A flat block of energy, for example, is the equivalent of firm energy with 100\% capacity value, something no wind plant claims to be able to supply. This paper explores the issue by first articulating the problem and showing the cost impacts through examples. The authors then examine various alternative base energy schedules which mitigate the energy and capacity},
  comment     = {Method used to calc integration costs for the NREL Eastern Wind Transmission study.

Doesn't like power blocks, just like Malte Jansen (Jansen13pvCtlMkt)},
  file        = {Milligan09windIntCost.pdf:Milligan09windIntCost.pdf:PDF},
  owner       = {sotterson},
  publisher   = {National Renewable Energy Laboratory Golden},
  timestamp   = {2013.07.03},
}

@InProceedings{Lew11valWindFrcst,
  author    = {Lew, Debra and Milligan, Michael and Jordan, Gary and Piwko, Richard},
  title     = {The value of wind power forecasting},
  booktitle = {91\textsuperscript{st} AMS Annual Meeting, 2\textsuperscript{nd} Conference on Weather, Climate, and the New Energy Economy Proceedings, Washington, DC},
  year      = {2011},
  abstract  = {With increasing penetration of wind generation on interconnected power systems, system operators are faced with increased levels of variability and uncertainty. Given that the power output of wind plants is a function of wind speed, the level of wind generation on a power system varies from hour-to-hour and from day-to-day. And given that wind speed is a function of the weather, the amount of wind that a power system operator can expect for the next day is subject to the level of uncertainty in weather-related forecasts for the next day.
Power system operators presently use day-ahead load forecasts to predict how much energy must be delivered for each hour of the next day. This forecast enables day-ahead commitment of generation resources, some of which may need many hours advance notice to be ready to generate power during the next day. Power systems with high penetrations of wind generation use day-ahead wind forecasts to predict how much of the wind power will be available for each hour of the next day. Combining the wind forecast with the load forecast enables operators to commit the balance of the generation fleet to economically and securely serve load on the next day.
Forecasts are not perfect. Load forecasting is a very mature science since power system operators have been using day-ahead load forecasts in their security-constrained unit commitment (SCUC) processes for several decades. Day-ahead hourly load forecast errors are typically in the range of 1\% to 3\% (GE Energy, 2009). Today??? wind forecasts typically have errors in the range of 15\% to 20\% mean absolute error (MAE) for a single wind plant.
Wind integration studies are now routinely undertaken by utilities and ISO??? to determine the impact of wind variability and uncertainty on the power system. Cost impacts are typically determined by considering the cost of increased operating reserves needed to accommodate the variability and uncertainty. For many studies, the uncertainty, or wind forecast error, is a significant driver in these integration costs (DeMeo, 2007).
Even with imperfect forecasts, large-scale wind integration studies have demonstrated that using day-ahead wind power forecasts for unit commitment can dramatically improve system operation by reducing overall operating costs, reducing unserved energy, and reducing wind curtailment, while maintaining required levels of system reliability. This paper analyzes the potential benefits of improving the accuracy (reducing the error) of day-ahead wind forecasts on power system operations, assuming that wind forecasts were used for day-ahead security constrained unit commitment. In this paper, State-of-the-art (SOA) forecasts refer to current forecasting technology.
When wind forecasts are lower than actual wind plant output, more conventional generation is committed in the day ahead than is actually needed. This means that the committed conventional generation will be operated at lower power output than planned, which would be a less efficient operating point for the system (primarily due to lower efficiency at lower power levels for thermal units). If the wind forecast error is large enough, it may be necessary to curtail some of the excess wind (or other) generation.
When wind forecasts are higher than actual wind plant output, less conventional generation is committed in the day ahead than is actually needed during the day of operation. Turning on quick-start peaking units normally mitigates the shortage in committed generation, but this drives up system operating cost significantly because of their lower efficiency. If the wind forecast error is large enough, there is also a risk of operating reserve shortfalls or possibly load-shedding.
This paper builds on the extensive models developed for the Western Wind and Solar Integration Study (WWSIS) which examined the operating impact of large amounts of wind and solar energy in the Western Electricity Coordinating Council (WECC) (GE Energy, 2010). WWSIS examined up to 35\% wind and solar energy penetration in the WestConnect portion of WECC (See Figure 1) and up to 23\% wind and solar energy penetration in the rest of WECC. Averaged across WECC, this totaled up to 27\% wind and solar energy penetration.
This study uses these WECC models to evaluate the operating cost impacts of improved day-ahead wind forecasts. In order to estimate the potential impacts for the entire U.S. power grid, the WECC results are extrapolated according to the relative sizes of the WECC and U.S. power grids, as measured by annual load energy.},
  comment   = {NREL study on cost of forecasting errors, discusses ramp costs too.},
  file      = {Lew11valWindFrcst.pdf:Lew11valWindFrcst.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2013.07.02},
}

@Article{Brooks12fourierMissFill,
  author    = {Brooks, E.B. and Thomas, V.A. and Wynne, R.H. and Coulston, J.W.},
  title     = {Fitting the Multitemporal Curve: A {Fourier} Series Approach to the Missing Data Problem in Remote Sensing Analysis},
  journal   = {Geoscience and Remote Sensing, IEEE Transactions on},
  year      = {2012},
  volume    = {50},
  number    = {9},
  pages     = {3340--3353},
  month     = sep,
  issn      = {0196-2892},
  abstract  = {With the advent of free Landsat data stretching back decades, there has been a surge of interest in utilizing remotely sensed data in multitemporal analysis for estimation of biophysical parameters. Such analysis is confounded by cloud cover and other image-specific problems, which result in missing data at various aperiodic times of the year. While there is a wealth of information contained in remotely sensed time series, the analysis of such time series is severely limited due to the missing data. This paper illustrates a technique which can greatly expand the possibilities of such analyses, a Fourier regression algorithm, here on time series of normalized difference vegetation indices (NDVIs) for Landsat pixels with 30-m resolution. It compares the results with those using the spatial and temporal adaptive reflectance fusion model (STAR-FM), a popular approach that depends on having MODIS pixels with resolutions of 250 m or coarser. STAR-FM uses changes in the MODIS pixels as a template for predicting changes in the Landsat pixels. Fourier regression had an R2 of at least 90\% over three quarters of all pixels, and it had the highest RPredicted2 values (compared to STAR-FM) on two thirds of the pixels. The typical root-mean-square error for Fourier regression fitting was about 0.05 for NDVI, ranging from 0 to 1. This indicates that Fourier regression may be used to interpolate missing data for multitemporal analysis at the Landsat scale, especially for annual or longer studies.},
  comment   = {Fill gaps in a 1D time series by linearly interpolating gaps, then force-fitting them to a Fourier basis. Not an especially well written paper...

My guess at what the procedure is:
1.) fill gaps by linear interpolation
2.) est. Fourier coeffs by LSQ (not by doing FFT, as that would exactly reproduce the gap-fillled data
3.) Synthesize the gap fill data by applying to coeffs

What I don't understand:
Is the Fourier fit data used in the non-filled regions too? If the F.T. synthesized data is used only in the gaps, then are there big time discontinuties?

Question:
is this better or worse than the "DFT from times where there is data" trick in energytop.org <<Use direct DFT to estimate FT of partly missing data>> ? Original source for that idea was: http://www.mathworks.com/matlabcentral/newsreader/view_thread/105116},
  doi       = {10.1109/TGRS.2012.2183137},
  file      = {Brooks12fourierMissFill.pdf:Brooks12fourierMissFill.pdf:PDF},
  groups    = {Read},
  keywords  = {Fourier regression algorithm;Fourier regression fitting;Fourier series approach;Landsat data;MODIS pixels;NDVI time series;STAR-FM comparison;Spatial and Temporal Adaptive Reflectance Fusion Model comparison;biophysical parameter estimation;missing data problem;multitemporal analysis;multitemporal curve fitting;normalized difference vegetation indices;remote sensing analysis;remotely sensed data;remotely sensed time series;Fourier series;curve fitting;regression analysis;remote sensing;time series;vegetation;},
  owner     = {sotterson},
  timestamp = {2012.12.05},
}

@Article{Grasman19stochBassMdl,
  author    = {Grasman, Johan and Kornelis, Marcel},
  title     = {Forecasting product sales with a stochastic Bass model},
  journal   = {Journal of Mathematics in Industry},
  year      = {2019},
  volume    = {9},
  number    = {1},
  pages     = {2},
  abstract  = {With the Bass model and data of previous sales a point estimate off uture sales can be
madeforthepurposeofstockmanagement.Inordertoobtaininformationaboutthe
accuracyofthatestimateaconfidenceintervalcanbeofuse.Inthisstudysuchan
intervalisconstructedfromaBassmodelextendedwithanoiseterm.Thesizeofthe
noiseisassumedtobeproportionalwiththeyearlysales.Itisalsoassumedthatthe
deviationfromthedeterministicsolutionissufficientlysmalltomakeasmallnoise
approximation.Thisperturbationtakestheformofatimedependent
Ornstein–Uhlenbeckprocess.Forthevarianceoftheperturbationanexactexpression
canbegivenwhichisneededinordertoobtainconfidenceintervals.
Keywords: Bassmodel;Ornstein–Uhlenbeckprocess;Sensitivityofparameterto
data;Confidencedomain

},
  comment   = {Bass diffusion with confidence intervals},
  file      = {:Grasman19stochBassMdl.pdf:PDF},
  publisher = {Springer},
  url       = {https://link.springer.com/article/10.1186/s13362-019-0059-6},
}

@Article{Tascikaraoglu14windFrstCombo,
  author    = {Tascikaraoglu, A and Uzunoglu, M},
  title     = {A review of combined approaches for prediction of short-term wind speed and power},
  journal   = {Renewable and Sustainable Energy Reviews},
  year      = {2014},
  volume    = {34},
  pages     = {243--254},
  abstract  = {With the continuous increase of wind power penetration in power systems, the problems caused by the volatile nature of wind speed and its occurrence in the system operations such as scheduling and dispatching have drawn attention of system operators, utilities and researchers towards the state-of-the-art wind speed and power forecasting methods. These methods have the required capability of reducing the influence of the intermittent wind power on system operations as well as of harvesting the wind energy effectively. In this context, combining different methodologies in order to circumvent the challenging model selection and take advantage of the unique strength of plausible models have recently emerged as a promising research area. Therefore, a comprehensive research about the combined models is called on for how these models are constructed and affect the forecasting performance. Aiming to fill the mentioned research gap, this paper outlines the combined forecasting approaches and presents an up-to date annotated bibliography of the wind forecasting literature. Furthermore, the paper also points out the possible further research directions of combined techniques so as to help the researchers in the field develop more effective wind speed and power forecasting methods.

Keywords
Forecasting methods; Combined prediction; Hybrid prediction; Wind speed; Wind power},
  comment   = {I think this is about using both NWP and Pow meas for ST wind pow frcsts.  Nothing spectactular, but the table is a start for something for the GIZ course.Focus on wind power forecasting combination. Good paper for Shuo Chen.},
  file      = {Tascikaraoglu14windFrstCombo.pdf:Tascikaraoglu14windFrstCombo.pdf:PDF},
  publisher = {Elsevier},
  url       = {http://www.sciencedirect.com/science/article/pii/S1364032114001944},
}

@Article{Lai13geoHierLdFrcst,
  author    = {Lai, Sen-Hao and Hong, Tao},
  title     = {When one size no longer fits all: Electric load forecasting with a geographic hierarchy},
  journal   = {White Paper, SAS},
  year      = {2013},
  abstract  = {With the deployment of smart grid technologies, many utilities can now take advantage
of hourly or sub-hourly data from millions of smart meters .

There are many upsides to this ? such as the fact that utilities can potentially charge
customers different rates based on the time of day they use electricity . However, there
are downsides as well:
*	Many	forecasting	methodologies	are	outdated.
*	The	days	of	one-size-fits-all	models	are	gone	for	the	utility	forecaster.

This paper tackles these considerations through an electric load-forecasting case study .
In particular, the paper:
*	Investigates	how	a	number	of	approaches	using	geographic	hierarchy	and
weather station data can improve the predictive analytics used to determine
future electric usage .
*	Demonstrates	why	using	geographic	hierarchies	is	now	imperative	for	utilities.

It also illustrates why utility forecasters must be equipped with solutions that allow
them to retrain models multiple times each year .},
  file      = {Lai13geoHierLdFrcst.pdf:Lai13geoHierLdFrcst.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.03.13},
  url       = {http://www.sas.com/en_us/whitepapers/when-one-size-no-longer-fits-all-106226.html},
}

@TechReport{Gneiting04calibProbWindForecast,
  author      = {Tilmann Gneiting and Kristin Larson and Kenneth Westrick and Marc G. Genton and Eric Aldric},
  title       = {Calibrated Probabilistic Forecasting at the Stateline Wind Energy Center: The Regime-Switching Space-Time ({RST}) Method},
  institution = {Department of Statistics, University of Washington},
  year        = {2004},
  number      = {454},
  month       = sep,
  abstract    = {With the global proliferation of wind power, accurate short-term forecasts of wind resources at wind energy sites are becoming paramount. Regime-switching space-time (RST) models merge meteorological and statistical expertise to obtain accurate and calibrated, fully probabilistic forecasts of wind speed and wind power. The model formulation is parsimonious, yet takes account of all the salient features of wind speed: alternating atmospheric regimes, temporal and spatial correlation, diurnal and seasonal non-stationarity, conditional heteroscedasticity, and non-Gaussianity. The RST method identifies forecast regimes at the wind energy site and its a conditional predictive model for each regime. Geographically dispersed meteorological observations in the vicinity of the wind farm are used as on-site predictors. The RST technique was applied to 2-hour ahead forecasts of hourly average wind speed at the Stateline wind farm in the US Pacific Northwest. In July 2003, for instance, the RST forecasts had root-mean-square error (RMSE) 28.6\% less than the persistence forecasts. For each month in the test period, the RST forecasts had lower RMSE than forecasts using state-of-the-art vector time series techniques. The RST method provides probabilistic forecasts in the form of predictive cumulative distribution functions, and those were well calibrated and sharp. The RST prediction intervals were substantially shorter on average than prediction intervals derived from univariate time series techniques. These results suggest that quality meteorological data from sites upwind of wind farms can be eciently used to improve short-term forecasts of wind resources. It is anticipated that the RST technique can be successfully applied at wind energy sites all over the world.},
  comment     = {Tech note that led to Gneiting06calProbFrcst},
  file        = {Gneiting04calibProbWindForecast.pdf:Gneiting04calibProbWindForecast.pdf:PDF},
  groups      = {PointDerived, Test, doReadWPV_2},
  owner       = {sotterson},
  timestamp   = {2008.09.30},
}

@Article{Gneiting06calProbFrcst,
  author    = {Gneiting, Tilmann and Larson, Kristin and Westrick, Kenneth and Genton, Marc G. and Aldrich, Eric},
  title     = {Calibrated Probabilistic Forecasting at the Stateline Wind Energy Center: The Regime-Switching SpaceTime Method},
  journal   = {Journal of the American Statistical Association},
  year      = {2006},
  volume    = {101},
  pages     = {968--979},
  month     = sep,
  abstract  = {With the global proliferation of wind power, the need for accurate short-term forecasts of wind resources at wind energy sites is becoming paramount. Regime-switching space-time (RST) models merge meteorological and statistical expertise to obtain accurate and calibrated, fully probabilistic forecasts of wind speed and wind power. The model formulation is parsimonious, yet takes into account all of the salient features of wind speed: alternating atmospheric regimes, temporal and spatial correlation, diurnal and seasonal nonstationarity, conditional heteroscedasticity, and non-Gaussianity. The RST method identifies forecast regimes at a wind energy site and fits a conditional predictive model for each regime. Geographically dispersed meteorological observations in the vicinity of the wind farm are used as off-site predictors. The RST technique was applied to 2-hour-ahead forecasts of hourly average wind speed near the Stateline wind energy center in the U.S. Pacific Northwest. The RST point forecasts and distributional forecasts were accurate, calibrated, and sharp, and they compared favorably with predictions based on state-of-the-art time series techniques. This suggests that quality meteorological data from sites upwind of wind farms can be efficiently used to improve short-term forecasts of wind resources.},
  comment   = {One of the 1\textsuperscript{st} offsite obs papers, and it also measures the effect on probabilistic forecasts.

Earlier tech note: Gneiting04calibProbWindForecast},
  file      = {Gneiting06calProbFrcst.pdf:Gneiting06calProbFrcst.pdf:PDF;Gneiting06calProbFrcst.pdf:Gneiting06calProbFrcst.pdf:PDF},
  groups    = {DOE-PNL09, Ensemble, PointDerived, doReadWPV_1},
  owner     = {sotterson},
  timestamp = {2009.03.04},
  url       = {http://ideas.repec.org/a/bes/jnlasa/v101y2006p968-979.html},
}

@Article{Zhang18probLoadFrcstRandFrstQD,
  author   = {Wenjie Zhang and Hao Quan and Dipti Srinivasan},
  title    = {Parallel and reliable probabilistic load forecasting via quantile regression forest and quantile determination},
  journal  = {Energy},
  year     = {2018},
  volume   = {160},
  pages    = {810 - 819},
  issn     = {0360-5442},
  abstract = {With the rapidly increasing complexity of operational challenges in smart grid environment, the traditional load point forecasting methods are no longer adequate. Probabilistic load forecasting has been proven to be more suitable in these environments due to their superior ability to provide more advanced uncertainty quantification. Most of the probabilistic forecasting methods, however are either insufficiently accurate or take very long training time. While probabilistic forecasting using quantile forecasts has been popular in research, the industry has been adopting another form of probabilistic forecasts, namely prediction intervals (PIs). The direct PI construction (DPIC) method typically employed for deciding the corresponding upper and lower quantile pair in PIs, however cannot guarantee the reliability of constructed PIs. This paper not only proposes a parallel and improved load quantile forecasting method but also solves the reliability issue of DPIC by proposing an alternative quantile determination (QD) method. Case studies show that the proposed load quantile forecasting method is both more accurate and more computationally efficient than the state-of-the-art methods and the reliability issue of DPIC is considerably alleviated by QD.},
  comment  = {A random forest load forecast, used to predict some kind of interval that's used in actual industry problems.  So maybe a "use of probabilistic forecast" paper too.

paper wasn't available for free...},
  doi      = {https://doi.org/10.1016/j.energy.2018.07.019},
  file     = {:Latinne01limNtreesRandForest.pdf:PDF},
  keywords = {Probabilistic forecasting, Quantile regression forest, Gradient boosting machine, Prediction interval, Uncertainty, Reliability, Electric load forecasting},
  url      = {http://www.sciencedirect.com/science/article/pii/S0360544218313148},
}

@Article{Smaragdis11timeFreqImpute,
  author    = {Smaragdis, P. and Raj, B. and Shashanka, M.},
  title     = {Missing data imputation for time-frequency representations of audio signals},
  journal   = {Journal of signal processing systems},
  year      = {2011},
  volume    = {65},
  number    = {3},
  pages     = {361--370},
  abstract  = {With the recent attention towards audio processing in the time-frequency domain we increasingly encounter the problem of missing data within that representation. In this paper we present an approach that allows us to recover missing values in the time-frequency domain of audio signals. The presented approach is able to deal with real-world polyphonic signals by operating seamlessly even in the presence of complex acoustic mixtures. We demonstrate that this approach outperforms generic missing data approaches, and we present a variety of situations that highlight its utility.},
  comment   = {How to reconstruct missing data in freq domain, w/ proper audio phase calculation. Useful for bootstrapping e.g. in:},
  file      = {Smaragdis11timeFreqImpute.pdf:Smaragdis11timeFreqImpute.pdf:PDF},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2012.12.05},
  url       = {http://www.springerlink.com/index/A4018XW32U686927.pdf},
}

@Article{Doherty05resrvDemandWind,
  author    = {Doherty, R. and O'Malley, M.},
  title     = {A new approach to quantify reserve demand in systems with significant installed wind capacity},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2005},
  volume    = {20},
  number    = {2},
  pages     = {587--595},
  month     = may,
  issn      = {0885-8950},
  abstract  = {With wind power capacities increasing in many electricity systems across the world, operators are faced with new problems related to the uncertain nature of wind power. Foremost of these is the quantification and provision of system reserve. In this paper a new methodology is presented which quantifies the reserve needed on a system taking into account the uncertain nature of the wind power. Generator outage rates and load and wind power forecasts are taken into consideration when quantifying the amount of reserve needed. The reliability of the system is used as an objective measure to determine the effect of increasing wind power penetration. The methodology is applied to a model of the all Ireland electricity system, and results show that as wind power capacity increases, the system must increase the amount of reserve carried or face a measurable decrease in reliability.},
  comment   = {Probabilistic reserve sizing in Ireland, considering wind and load forecast uncertainty, and outage rates},
  doi       = {10.1109/TPWRS.2005.846206},
  file      = {Doherty05resrvDemandWind.pdf:Doherty05resrvDemandWind.pdf:PDF;Doherty05resrvDemandWind.pdf:Doherty05resrvDemandWind.pdf:PDF},
  groups    = {DOE-PNL09, Use, doReadNonWPV_1},
  keywords  = { load forecasting, power generation faults, power generation reliability, power system security, wind power plants Ireland electricity system, generator outage rates, power forecast, power generation faults, power generation reliability, power generation security, power penetration, wind power capacity},
  owner     = {sotterson},
  timestamp = {2009.03.03},
}

@TechReport{Mendes11statWindFrcst,
  author      = {Joana Mendes and Ricardo J. Bessa and Hrvoje Keko and Jean Sumali and Vladimiro Miranda and Carlos Ferreira and J{\~a}o Gama and Adun Botterud and Zhi Zhou and Jianhui Wang},
  title       = {Development and Testing of Improved Statistical Methods for Wind Power Forecasting},
  institution = {Argonne National Laboratory},
  year        = {2011},
  number      = {ANL/DIS-11-17},
  month       = sep,
  abstract    = {Within wind power forecasting, we have investigated improvements in the statistical algorithms used for wind power point forecasting, wind power uncertainty forecasting, and wind power ramp forecasting. For wind power point forecasting, we have focused on the training criteria used in the computational learning algorithms which convert weather forecasts and observational data to a point forecast for wind power. In particular, we focused on the use of training criteria from information theoretic learning (ITL). In contrast to the standard minimum square error (MSE) criterion, ITL criteria are not built on the assumption of a Gaussian distribution of the forecasting errors. We tested the new approaches on real-world data from wind farms in the U.S. Midwest. Our experiments show distinct advantages of using the new ITL training criteria compared to MSE, in terms of reduced wind power forecasting errors. For wind power uncertainty forecasting, we have developed two new methods for characterizing the wind power forecast uncertainty. The two methods are based on kernel density estimation (KDE) combined with either the Quantile Copula or Nadaraya Watson estimators. We developed time-adaptive versions for both KDE algorithms, which is an import contribution to the current state-of-the-art. The new algorithms were tested on dataset from the Eastern Wind Integration and Transmission Study (EWITS), as well as on two large-scale wind farms located in the U.S. Midwest. Results show that the new KDE algorithms tend to give a better match to the observed wind power distribution (i.e. better calibration) than traditional quantile regression. For wind power ramp forecasting, we have developed a new method for probabilistic ramp event detection, and we performed an extensive experimental evaluation using recently proposed ramp definitions.},
  comment     = {Gigantic report with a lot of detail on how to evaluate and test forecasts, both point and probabilistic. Also a lot on new forecast algorithm training cost functions and training under concept drift.

New adaptive KDE (with copulas, I think), supposed to be better than quantile regression. Ramp forecasts too.

picked beta kernel in Chen99betaKern
Journal paper: Bessa12adaptQuantCopulaFrcst

picked at least the kernel bandwidth using this R package:
hdrcde: Highest density regions and conditional density estimation
http://cran.r-project.org/web/packages/hdrcde/index.html
(paper is Hyndman96estCondDens)

Kind of punted on picking the best forecast algorithm vs. application. Maybe Gneiting11compQuantWgt would help?

KDE better than spline in some ways (spline QR compared to is: Nielsen06quantRegr)},
  file        = {Mendes11statWindFrcst.pdf:Mendes11statWindFrcst.pdf:PDF},
  groups      = {PointDerived, doReadWPV_1},
  location    = {Oak Ridge, TN, USA},
  owner       = {sotterson},
  timestamp   = {2013.10.01},
  url         = {http://www.dis.anl.gov/projects/windpowerforecasting.html},
}

@Standard{06IEEEcurrentTempOvrhdCond,
  title        = {Draft Standard for Calculating the Current-Temperature of Bare Overhead Conductors},
  organization = {IEEE},
  author       = {IEEE},
  number       = {IEEE Std P738/D2},
  month        = aug,
  year         = {2006},
  url          = {http://ieeexplore.ieee.org/servlet/opac?punumber=4152544},
  comment      = {IEEE standard which can use "effective wind speed" to calc total power line cooling, and thus dynamic current rating},
  owner        = {sotterson},
  timestamp    = {2009.02.23},
}

@Article{16UnderstandingLocalDiurnalWind,
  author    = {studylib.net},
  title     = {Understanding Local Diurnal Winds},
  journal   = {studylib.net},
  year      = {2016},
  owner     = {sotterson},
  timestamp = {2017.05.01},
  url       = {https://studylib.net/doc/16006792/understanding-local-diurnal-winds},
}

@TechReport{3tier07windRampSGII,
  author      = {3TIER},
  title       = {A Prototype Day-Ahead Rapid Wind Ramp Event Forecast System Stage Gate II ? Interim Report},
  institution = {3TIER},
  year        = {2007},
  comment     = {3TIER's ramp prediction report. IDEA for improvement * use regression w/ arbitrary cost function (penalizing ramp misses in proportion to ancilary costs entailed).},
  file        = {3tier07windRampSGII.pdf:3tier07windRampSGII.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2008.12.10},
}

@Book{9781319042387,
  title     = {Universe},
  publisher = {Macmillan Education},
  year      = {2015},
  author    = {Freedman, Roger and Geller, Robert and Kaufmann, William J.},
  isbn      = {978-1319042387},
  comment   = {Shows that venus has only two global convection cells vs. six for the earth, because it spins faster},
  date      = {2015-09-23},
  ean       = {9781319042387},
  owner     = {sotterson},
  timestamp = {2017.04.25},
  url       = {http://www.ebook.de/de/product/27059771/roger_freedman_robert_geller_william_j_kaufmann_universe.html},
}

@TechReport{ABB05DakotaTransIncr,
  author      = {ABB},
  title       = {Dakotas Wind Transmission Study Task 2: Transmission Technologies to Increase Power Transfers (Final Report)},
  institution = {ABB Inc. Electric Systems Consulting},
  year        = {2005},
  number      = {2005-10977-3 R2},
  month       = aug,
  comment     = {Dynamic line rating very advantageous for wind generation; good theory and tech overview for Dakota study 

* 15-20 pct increase in thermal rating if dyanamic 

Bigger rating increase if source is wind power (170 pct in one Dakota scenario on p. 11) 
* Wind power when blowing wind (wind power starts at > 3.5m/s) 
* convection cooling up to 85 pct of total cooling (2.12m/s at perfect angle) 
* this increases by 180 pct over 0.61 m/s in conductor rating tables 
* ==> rating increased by 170 pct over table ratings 
* equations for effect of wind speed and direction (p. 9)
 -- ampacity is solved for numerically: the current carrying capacity of the line (def'n from Wikipedia)
 -- wind direction correction factor ranges from about 0.1 to 1 (I checked in Matlab)
 -- this means that wind direction will be important 

When does dynamic thermal rating help? 
* when it's windy/cold 
* short lines: "security" or "stability" ratings are stricter than thermal for long lines, limit current first 
* nice graph of this on p. 6 
* IEEE and CIGRE standards listed 

Common conservative steady state limits: 
* Ambient temperature: 40 deg C 
*Wind speed: 0.61 m/s (2 ft/sec) 
* Solar Radiation: 1000 W/m2 
* Maximum conductor temperature: 80 deg C 

Other
* new power lines have fiber optics builtin; can pass sag and temp information 
* latest advances do probabilistic conductor temp modeling 
* has table of conductor options effect of wind speed 
* * presentation slides (tasks 1, 3 and 4, as well as 2, are also linked into this bibtex entry},
  file        = {ABB05DakotaTransIncr.pdf:ABB05DakotaTransIncr.pdf:PDF;ABB05DakotaTransIncr.ppt:ABB05DakotaTransIncr.ppt:PowerPoint;ABB05DakotaTransIncr.pdf:ABB05DakotaTransIncr.pdf:PDF},
  groups      = {Read},
  location    = {940 Main Campus Drive, Suite 300 Raleigh, NC 27606},
  owner       = {sotterson},
  timestamp   = {2009.02.23},
  url         = {http://www.wapa.gov/Ugp/PowerMarketing/WindHydro/Dakota%20Wind%20Study-Task%202%2010-19-05.pdf},
}

@Conference{AbdulRahman13hybrdUncertCAISO,
  author    = {Khaled Abdul-Rahman and Hani Alarian and Clyde Loutan},
  title     = {Hybrid Approach for Incorporating Uncertainty in CAISO Market Operations},
  booktitle = {FERC Technical Conference: Increasing RT and DA Market Efficiency through Improved Software},
  year      = {2013},
  comment   = {Slides explaining how CAISO has mixed use of probabilistic and deterministic forecasts for its power system markets

ramp constraint explained here: AbdulRahman12flexRampCAISO (and in other places in energy.bib)},
  file      = {AbdulRahman13hybrdUncertCAISO.pdf:AbdulRahman13hybrdUncertCAISO.pdf:PDF},
  groups    = {Use, doReadWPV_1},
  owner     = {sotterson},
  timestamp = {2013.10.11},
}

@Article{Abe-Ouchi13ippcPaeloAssCycRep,
  author    = {Ayako Abe-Ouchi and J{\"u}rg Beer and Andrey Ganopolski et. al},
  title     = {Chapter 5: Information from Paleoclimate Archives},
  journal   = {IPCC 5th Assessment Cycle Report},
  year      = {2013},
  comment   = {Change in sun power accounted for no more than 0.1 degrees of global temp rise since 1880 (in 2013)

Cited by LarryM15sunTempOppSkepSci},
  file      = {Abe-Ouchi13ippcPaeloAssCycRep.pdf:Abe-Ouchi13ippcPaeloAssCycRep.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.06},
  url       = {http://www.ipcc.ch/report/ar5/wg1/},
}

@InBook{Abernethy06weibullAnlysCh1,
  chapter   = {Chapter 1: An Overview of Weibul Analysis},
  title     = {The New Weibull Handbook},
  year      = {2006},
  author    = {Abernethy, Robert B},
  comment   = {Seems like a good intro to failure analysis.  Looks a bit amateur but well reviewed by Royal Stats Soc.

See also: Lien13unlckWeibullAnlys},
  file      = {Abernethy06weibullAnlysCh1.pdf:Abernethy06weibullAnlysCh1.pdf:PDF},
  journal   = {The New Weibull Handbook,},
  owner     = {sotterson},
  timestamp = {2017.01.02},
  url       = {http://www.barringer1.com/tnwhb.htm},
}

@PhdThesis{Achterberg07cnstrntIntPrg,
  author      = {Tobias Achterberg},
  title       = {Constraint Integer Programming},
  year        = {2007},
  comment     = {Andre recommendation: Explains SCIP and gives deeping insight into the heuristic algorithms for solving Mixed Integer Problems},
  contributor = {Berichter: Prof. Dr. Drs. h. c. Gr{\"o}tschel, Prof. Dr. Robert E. Bixby},
  file        = {Achterberg07cnstrntIntPrg.pdf:Achterberg07cnstrntIntPrg.pdf:PDF},
  institution = {Technische Universit{\"a}t Berlin},
  owner       = {sotterson},
  timestamp   = {2011.10.18},
  url         = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.3053&rep=rep1&type=pdf},
}

@Book{Afgan04renewEngSustBook,
  title     = {New And Renewable Energy Technologies For Sustainable Development},
  publisher = {CRC Press},
  year      = {2004},
  author    = {Naim Afgan and Maria da Graca Carvalho},
  comment   = {P. 124 has Kalman filter approach. On google books.},
  owner     = {sotterson},
  timestamp = {2008.07.03},
  url       = {http://books.google.com/books?id=cRq9safls9kC&pg=PT137&lpg=PT137&dq=wind+prediction&source=web&ots=_6ysbA1Ocr&sig=AZHLTN8miH3MEFwt96eEj1DWw94&hl=en&sa=X&oi=book_result&resnum=1&ct=result#PPT138,M1},
}

@InProceedings{Agarwal10IncrMDSoos,
  author    = {Agarwal, Arvind and Phillips, Jeff M and Daum{\'e} III, Hal and Venkatasubramanian, Suresh},
  title     = {Incremental Multi-Dimensional Scaling},
  booktitle = {The Learning Workshop at Snowbird},
  year      = {2010},
  comment   = {An incremental multidimensional scaling algorithm. Main use is for out of sample points (very small error against batch, when data really lives in low dim output space) but incremental not as good b/c of poor intialization. But saves memory: only need to store 2*k points, where k is the output dimension (low dimension).

Alg. not really described here: must see: Agarwal10UnivMDS (I think)},
  file      = {Agarwal10IncrMDSoos.pdf:Agarwal10IncrMDSoos.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2014.06.27},
  url       = {http://www.cs.utah.edu/~jeffp/},
}

@Book{Aggarwal17OutlierAnalysisBook,
  title     = {Outlier Analysis},
  publisher = {Springer},
  year      = {2017},
  author    = {Charu C. Aggarwal},
  edition   = {Second Edition},
  doi       = {10.1007/978-3-319-47578-3},
  file      = {Aggarwal17OutlierAnalysisBook.pdf:Aggarwal17OutlierAnalysisBook.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2016.12.19},
  url       = {http://www.springer.com/us/book/9781461463955},
}

@Book{Agresti07introCategDatAnl,
  title     = {An Introduction to Categorical Data Analysis},
  publisher = {Wiley-Interscience},
  year      = {2007},
  author    = {Alan Agresti},
  edition   = {Second Edition},
  comment   = {Looks like it would be a good book to scan for logistic regression, GLM stuff, mixed models, random effects, etc.},
  file      = {:Agresti07introCategDatAnl.pdf:PDF},
  url       = {https://mregresion.files.wordpress.com/2012/08/agresti-introduction-to-categorical-data.pdf},
}

@InCollection{Akaike92infoThryMaxLik,
  author    = {Akaike, Hirotogu},
  title     = {Information theory and an extension of the maximum likelihood principle},
  booktitle = {Breakthroughs in statistics},
  publisher = {Springer},
  year      = {1992},
  pages     = {610--624},
  comment   = {What seems to be the fundamental reference for AIC stopping criteria.},
  url       = {https://link.springer.com/chapter/10.1007/978-1-4612-0919-5_38},
}

@Misc{Allen12bpaHyProM,
  author       = {Chris Allen},
  title        = {TIP 259: Short-Term Hydropower Production and Marketing Optimization (HyProM)},
  howpublished = {Web Page},
  month        = oct,
  year         = {2012},
  comment      = {A BPA project on stochastic optimization of its power system -- Fraunhofer IOSB-AST is a partner!

started in 2012; complete in 2014

Possibly related slides in Kelman12stochOptColRiv},
  file         = {Allen12bpaHyProM.pdf:Allen12bpaHyProM.pdf:PDF},
  groups       = {Use, doReadWPV_2},
  owner        = {sotterson},
  timestamp    = {2013.10.02},
  url          = {http://www.bpa.gov/Doing%20Business/TechnologyInnovation/Pages/Technology-Innovation-Projects.aspx},
}

@Misc{Allerton08simWindCrossDist,
  author       = {Allerton, Tom},
  title        = {Simulating the distribution and cross-correlation of wind farm output},
  howpublished = {The 64\textsuperscript{th} European Study Group with Industry},
  month        = apr,
  year         = {2008},
  note         = {E.ON Engineering},
  comment      = {Presentation slides spinning reservers overview},
  file         = {Allerton08simWindCrossDist.pdf:Allerton08simWindCrossDist.pdf:PDF},
  owner        = {scot},
  timestamp    = {2010.11.24},
  url          = {http://www.ma.hw.ac.uk/esgi08/E.html},
}

@Article{Anctil04nnGenTrnSampStrm,
  author  = {Anctil, Fran{\c{c}}ois and Lauzon, Nicolas},
  title   = {Generalisation for neural networks through data sampling and training procedures, with applications to streamflow predictions},
  journal = {Hydrology and Earth System Sciences Discussions},
  year    = {2004},
  volume  = {8},
  number  = {5},
  pages   = {940--958},
  comment = {Several ways of training NNs when generalization is a problem.  They prefer model aggregation (a.k.a. ensembles) but I didn't read it carefully enough to understand what the model ensembles were.  This technique is something like that used in May08partMutInfoWaterFrcst.},
  file    = {Anctil04nnGenTrnSampStrm.pdf:Anctil04nnGenTrnSampStrm.pdf:PDF},
}

@Book{Anderson76boxJenkinsBook,
  title         = {Time series analysis and forecasting : the Box-Jenkins approach},
  publisher     = {Butterworth, London ; Boston :},
  year          = {1976},
  author        = {Anderson, O. D.},
  isbn          = {0408706759},
  catalogue-url = { http://nla.gov.au/nla.cat-vn1119641 },
  comment       = {Probably a good reference for Box-Jenkins forecasting},
  language      = {English},
  life-dates    = { 1976 - },
  owner         = {scot},
  subjects      = { Box-Jenkins forecasting. },
  timestamp     = {2010.12.14},
  type          = {Book},
}

@Misc{Andres16probGrphMdlsCourse,
  author       = {Bjoern Andres and Bernt Schiele},
  title        = {Probabilistic Graphical Models and Their Applications. Max Planck Institute for Informatics},
  howpublished = {Course Notes:},
  month        = oct,
  year         = {2016},
  comment      = {Course notes for graphical models.  For now, I'm just borrowing some slides -- can download the rest later.},
  owner        = {sotterson},
  timestamp    = {2017.01.22},
  url          = {https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/teaching/courses/probabilistic-graphical-models-and-their-applications/},
}

@Article{Antolik03ModelOutputStatistics,
  author    = {Mark S. Antolik},
  title     = {Model Output Statistics (MOS) -- Objective Interpretation of NWP Model Output},
  journal   = {NOAA/National Weather Service, USA},
  year      = {2003},
  comment   = {I borrowed the 1st few slides for my Colombia talk.  Simple, very old fashioned (but it IS old).},
  owner     = {sotterson},
  timestamp = {2017.04.30},
  url       = {http://www.nws.noaa.gov/mdl/synop/millersv/frame.html},
}

@TechReport{Antoniou03sodarMeas,
  author      = {Ioannis Antoniou and Hans E. J{\o}rgensen and Frank Ormel and Stuart Bradley and Sabine von H?nerbein and Stefan Emeis and G?nter Warmbier},
  title       = {On the theory of SODAR measurement techniques},
  institution = {Ris{\o} National Laboratory},
  year        = {2003},
  number      = {Ris{\o}-R-1410(EN)},
  month       = apr,
  comment     = {How SODAR compares to anemometers and other wind measurement devices.},
  file        = {Antoniou03sodarMeas.pdf:Antoniou03sodarMeas.pdf:PDF;Antoniou03sodarMeas.pdf:Antoniou03sodarMeas.pdf:PDF},
  location    = {Roskilde, Denmark},
  owner       = {sotterson},
  timestamp   = {2009.02.19},
  url         = {http://www.risoe.dtu.dk/Knowledge_base/publications/Reports/ris-r-1410.aspx?sc_lang=en},
}

@Article{Arlot16commentsRndFrstGuidTour,
  author    = {Arlot, Sylvain and Genuer, Robin},
  title     = {Comments on: A random forest guided tour},
  journal   = {Test},
  year      = {2016},
  volume    = {25},
  number    = {2},
  pages     = {228--238},
  comment   = {Expansion of ideas in Biau16randFrstGuideTour},
  file      = {:Arlot16commentsRndFrstGuidTour.pdf:PDF},
  publisher = {Springer},
  url       = {https://hal.archives-ouvertes.fr/hal-01297557/document},
}

@InProceedings{Armstrong07sagLidarNonContactTemp,
  author    = {Oisin Michael Armstrong},
  title     = {Transmission line conductor sag measurement with lidar survey and non-contact temperature determination},
  booktitle = {Conference of the Remote Sensing \& Photogrammetry Society (RSPSoc)},
  year      = {2007},
  comment   = {characterize line by measuring sag w/ LIDAR and estimate cond. temp


Also see: Lu08TransLineModLidarAcc},
  file      = {Armstrong07sagLidarNonContactTemp.pdf:Armstrong07sagLidarNonContactTemp.pdf:PDF;Armstrong07sagLidarNonContactTemp.pdf:Armstrong07sagLidarNonContactTemp.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.02.23},
  url       = {http://www.ceg.ncl.ac.uk/rspsoc2007/papers.htm},
}

@InProceedings{Ausfuehrung08EinServicedes,
  author = {Verordnung zur Ausf?hrung and der Verordnung and zum EEG-Ausgleichsmechanismus and (Ausgleichsmechanismus-Ausf?hrungsverordnung and - AusglMechAV) and AusglMechAV},
  title  = {Ein Service des Bundesministeriums der Justiz und f?r Verbraucherschutz in Zusammenarbeit mit der juris GmbH - www.juris.de},
  year   = {2008},
  file   = {:BGBI10tsoIntradayRules.pdf:PDF},
}

@Article{AWStruewind17colombiaWindRsrcMap,
  author    = {AWS Truewind},
  title     = {Wind Resource Of Colombia: Mean annual Wind Speed at 200m Resolution},
  journal   = {Web Download},
  year      = {2017},
  file      = {AWStruewind17colombiaWindRsrcMap.pdf:AWStruewind17colombiaWindRsrcMap.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.06},
  url       = {https://www.awstruepower.com/assets/Wind-Resource-Map-COLUMBIA-11x17.pdf},
}

@Unpublished{Baier11mqrnn,
  author    = {Andr{\'e} Baier},
  title     = {{MQRNN}- Umsetzung in {Java}: Definition, Training, Verbesserung},
  month     = jan,
  year      = {2011},
  comment   = {Andre's NN quantile regression alogrithm. It avoids crossover by estimating all the Tau's at once. A problem with this could be that the model and features are the same for each quantile.

Potential problems w/ single NN for all taus approach
* Features are different for each quantile (seems likely at high or low points on the wind power curve).
* Also, it seems likely that the optimal num. of nodes is different.
* Koenker10bayesBanoQR has these complaints about analogous Bayesian procedures

Perhaps Cannon11quantRgrsnNNprecip, which does one quantile at a time, is better, with Chernozhukov10qrNoCross postprocessing.

report: ~/work/IWES/proj/EWeLiNE/doc/IWESpapers/MQRNN.doc
code: http://fbisvn-01.iwes.fraunhofer.de/svn/matlab/Projekte/qrnn/},
  groups    = {PointDerived, doReadWPV_1},
  owner     = {sotterson},
  timestamp = {2013.10.15},
}

@TechReport{Barbose18trackSunTrendInstPVprice,
  author      = {Galen Barbose and Na{\:i}m Darghouth and Kristina LaCommare and Dev Millstein and Joe Rand},
  title       = {Tracking the Sun: Installed Price Trends for Distributed Photovoltaic Systems in the United States - 2018 Edition},
  institution = {Lawrence Berkeley National Laboratory},
  year        = {2018},
  comment     = {Tracks installed PV costs and trends in US and subregions.  Prices vary widely.  Perhaps useful public data for MNSP because int knows about incentives, "soft costs" etc.  So could a separate learning rate model like in Trancik15imprvRinfSolWind

Is maybe based on OpenPV data, which has been criticized in Yu18deepSolarDB},
  file        = {:Barbose18trackSunTrendInstPVprice.pdf:PDF},
  url         = {https://emp.lbl.gov/tracking-the-sun},
}

@InProceedings{Barton08windForecastNeedBPA,
  author    = {ForecastsSteven B. Barton},
  title     = {Wind in the {BPA} Balancing Authority Area and the Need for Forecasts},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  year      = {2008},
  comment   = {BPA need for forecasts, some explanation of sub-60 minute need
* 4X increase in generation in 3 years
-- 1.5GW generation in 2008; 3GW in 2009; 6GW in 2011
-- currently 14 pct penetraion (peak fraction of peak demand?)
* Hydro backup
* "wind generator-supplied" hour ahead forecast errors:
-- 32 pct off by more than 25 pct
-- -3 pct bias * Biggest ramp
-- 60 minute: 1.2GW increase (77 pct capacity)
* Graph of wind vs. imbalance
-- looks like a backup something is fixing the imbalance, sometime after it's detected
* BA tries to balance according to predicted wind energy output
* > hour forecasts: used for generation and load scheduling
* sub 60 minute forecast used for predictive control, reserve positioning, hydro management
-- somehow, this is different that > hour forecast uses
-- maybe it's just the predictive control part},
  file      = {Barton08windForecastNeedBPA.pdf:Barton08windForecastNeedBPA.pdf:PDF;Barton08windForecastNeedBPA.pdf:Barton08windForecastNeedBPA.pdf:PDF},
  groups    = {IWFTM08, Read},
  owner     = {sotterson},
  timestamp = {2008.09.19},
  url       = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
}

@Misc{Bentzien13ensPostProcTlag,
  author       = {Sabrina Bentzien and Petra Friederichs},
  title        = {Ensemble postprocessing for probabilistic quantitative precipitation forecasts},
  howpublished = {COSMO User Seminar},
  month        = mar,
  year         = {2013},
  comment      = {Precip. ensembles post processed and weighted using Bayesian LASSO and Quantile regression. Also showed benefit of ensemble time lagging. Used COSMO-DE and DWD was involved somehow, so I'd better read it.},
  file         = {Bentzien13ensPostProcTlag.pdf:Bentzien13ensPostProcTlag.pdf:PDF},
  groups       = {Ensemble, doReadWPV_1},
  owner        = {sotterson},
  timestamp    = {2014.01.29},
  url          = {ftp://ftp.dwd.de/pub/DWD/Forschung_und_Entwicklung/CUS2013_presentations_PDF/Predictability_and_Ensemble_Systems/bentzien_cosmo2013.pdf},
}

@Misc{Bhattarai10polyDistLag,
  author       = {Keshab Bhattarai},
  title        = {Polynomial Distributed (Dynamic) Models},
  howpublished = {Course Notes, Lecture 21, Economietrics 1, The University of Hull},
  year         = {2010},
  comment      = {Polynomial distributed lags enforce structure on regression lag estimation, can be estimated from data

Lagged varlables in regression
- one way is to copy lagged versions of variables into regression input
- dynamic PCA does this and so did I on other stuff
- problem: colinearity makes regression coefficients hard to estimate (I think this makes it hard for NN's too; somebody I bibtex'ed said the training oscillated "senselessly," I think I remember)
- note: if whitened, then colinearity would be less of a problem but does that help?

Polynomial lags
- enforce structure on lags with some kind of polynomial relation across time
- relationship at many lags determined by only a few parameters

Three types mentioned here:
1. Koyck Procedure
-- adaptive, autogressive relation
-- always tapers off geometrically (so zero lag always has greatest influence)
-- have to pick a memory coefficient (AIC type ideas for picking it)
-- autoregressive equation includes input variable
---- so errors get correlated and other probs
---- eg. multicollinearity
-- better than simple exponential forgetting? (I ask)
2. Almon's polynomial lag model
-- kth order polyomial determines coefficient on lagged variable (eq. 7)
-- doesn't have to decline geometrically
-- fewer coeffs than Koyck
-- still collinear, probably (McDowell04polyDistLag says they're less collinear)
-- estimate w/ ordinary least squares
-- effect of inputs have finite maximum lag
-- this is a kind of FIR whose coeffs have a polynomial relationship
---- so, fewer coeffs than max lag
---- so, probably good when don't have too many training points
---- a much better explanation is in McDowell04polyDistLag
3. Autogressive Distributed Lag (ARDL)
-- autoregressive: lagged output is in output equation
-- infinitely long lag for effect of inputs
-- the equation is ARMA, right??},
  file         = {Bhattarai10polyDistLag.pdf:Bhattarai10polyDistLag.pdf:PDF},
  groups       = {Read, Ensemble, doReadNonWPV_2},
  owner        = {scot},
  url          = {http://www.hull.ac.uk/php/ecskrb/METRICS/dynamic-19.ppt},
}

@Article{Biermann05eegNowUpscale,
  author  = {Biermann, K and Cali, {\"U} and F{\"u}ller, G and Haubold, R and Heinze, D and Herrmann, O and Jursa, R and Lange, B and Moradi, J and Rohrig, K and others},
  title   = {Entwicklung eines Rechenmodells zur Windleistungsprognose f{\"u}r das Gebiet des deutschen Verbundnetzes},
  journal = {Kassel: Institut f{\"u}r Solare Energieversorgungstechnik (ISET)},
  year    = {2005},
  comment = {Description of the now upscaling used by the German TSO's. This was the reference Stephan used in: Vogt15HybridPhysMLrgnFrcst},
  url     = {http://publica.fraunhofer.de/documents/N-154116.html},
}

@Book{Bishop06PatternRecogMachLrnBook,
  title     = {Pattern recognition and machine learning},
  publisher = {springer},
  year      = {2006},
  author    = {Bishop, Christopher M},
  comment   = {Bishop's classic machine learning book.  Stephan V. really likes it, says it has a lot on multivariate normal distributions.},
  file      = {:Bishop06PatternRecogMachLrnBook.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.27},
  url       = {http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf},
}

@Misc{Blah238_14_specularVsReflective,
  author       = {blah238},
  title        = {Difference between Albedo and Surface Reflectance},
  howpublished = {gis.stackexchange.com},
  month        = oct,
  year         = {2014},
  comment      = {Specular reflectance: is like what you get off a mirror; 

Albedo:  is diffuse, going in many directions.},
  file         = {:Blah238_14_specularVsReflective.pdf:PDF},
  url          = {https://gis.stackexchange.com/questions/36726/difference-between-albedo-and-surface-reflectance},
}

@InProceedings{Blatchford08caisoRenewInteg,
  author    = {Jim Blatchford},
  title     = {CAISO Supporting Integration of Renewables},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  year      = {2008},
  comment   = {* CA will use mostly wind to reach its 20 pct renewable RPS goal
* nice map of where wind is good in CA
* explanation of hour ahead bidding rules
* quite a bit of required instrumentation, including
-- "realtime MW production"
-- "1 Met tower" * example of differnent measured power curves (from Truewind)

Lessons learned:
* beter data quality \& outage reporting
* 2 met stations per site
* use individual turbine anemometers
* use competitive forecasting
* 4 error metrics explained ("RFB error metrics")
* wind power exceeds all-time system peak for CAISO (50GW)
* emphasize ramp forecasting},
  file      = {Blatchford08caisoRenewInteg.pdf:Blatchford08caisoRenewInteg.pdf:PDF;Blatchford08caisoRenewInteg.pdf:Blatchford08caisoRenewInteg.pdf:PDF},
  groups    = {IWFTM08, Read},
  owner     = {sotterson},
  timestamp = {2008.09.19},
  url       = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
}

@Article{Bolton02Statisticalfrauddetection,
  author    = {Bolton, Richard J and Hand, David J},
  title     = {Statistical fraud detection: A review},
  journal   = {Statistical science},
  year      = {2002},
  pages     = {235--249},
  owner     = {sotterson},
  publisher = {JSTOR},
  timestamp = {2016.12.19},
}

@PhdThesis{borg05elecLoadModelCompInt,
  author      = {Rutger Willem Ter Borg},
  title       = {Electricity Load Modelling using Computational Intelligence},
  year        = {2005},
  comment     = {Kernel machines better than neural networks (FANN library) for wind power prediction. Also easer to train (see 6.3, p. 74) * however, the time period is not that short: 24 hrs or something. * also data has artificial noise added},
  file        = {borg05elecLoadModelCompInt.pdf:borg05elecLoadModelCompInt.pdf:PDF;borg05elecLoadModelCompInt.pdf:borg05elecLoadModelCompInt.pdf:PDF},
  institution = {Technische Universiteit Delft},
  owner       = {scotto},
  timestamp   = {2008.07.08},
  url         = {http://repository.tudelft.nl/consumption/idcplg?IdcService=GET_FILE&RevisionSelectionMethod=latestReleased&dDocName=025320},
}

@Article{Bouallegue13,
  author    = {Bouallegue, Zied Ben and Theis, Susanne E. and Gebhardt, Christoph},
  title     = {Enhancing COSMO-DE ensemble forecasts by inexpensive techniques},
  year      = {2013},
  volume    = {22},
  number    = {1},
  pages     = {49--59},
  issn      = {0941-2948},
  booktitle = {Meteorologische Zeitschrift},
  groups    = {Ensemble, CitaviImport1, doReadWPV_2},
  keywords  = {time-lagged ensembles, ensemble},
  ncite     = {2},
  owner     = {sotterson},
  publisher = {E. Schweizerbart'sche Verlagsbuchhandlung},
  timestamp = {2013.09.26},
}

@Book{Bowman97smthKDE,
  title     = {Applied Smoothing Techniques for Data Analysis: The Kernel Approach with S-Plus Illustrations: The Kernel Approach with S-Plus Illustrations},
  publisher = {Oxford University Press},
  year      = {1997},
  author    = {Bowman, Adrian W and Azzalini, Adelchi},
  comment   = {The KDE approach commonly used at IWES, e.g. in Jansen13pvCtlMkt

The Matlab is here: http://fbisvn-01.iwes.fraunhofer.de/svn/MatlabRepo/REBal/trunk/Functions/Probablistic_Forecast/},
  groups    = {PointDerived, doReadWPV_1},
  owner     = {sotterson},
  timestamp = {2013.10.15},
  url       = {http://books.google.de/books?hl=en&lr=&id=7WBMrZ9umRYC&oi=fnd&pg=PA1&dq=Applied+smoothing+techniques+for+data+analysis+-+The+kernel+approach+with+S-Plus+illustrations,&ots=L4W80GCc5Q&sig=ObyV9W9-zQ8jC_9p4GrJXLqYDGk&redir_esc=y#v=onepage&q=Applied%20smoothing%20techniques%20for%20data%20analysis%20-%20The%20kernel%20approach%20with%20S-Plus%20illustrations%2C&f=false},
}

@Book{Boyd04CnvxOptBook,
  title     = {Convex optimization},
  publisher = {Cambridge University Press},
  year      = {2004},
  author    = {Boyd, Stephen P and Vandenberghe, Lieven},
  comment   = {Textbook for Standford EE364a,b and CVX101.

Convex opt, linear programming (a little). Stochastic programming (some?)

Course pages:
EE364a: http://www.stanford.edu/class/ee364a/
EE364b: http://www.stanford.edu/class/ee364b/
CVX101: https://class.stanford.edu/courses/Engineering/CVX101/Winter2014/info},
  file      = {Boyd04CnvxOptBook.pdf:Boyd04CnvxOptBook.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.05.12},
  url       = {http://www.stanford.edu/~boyd/cvxbook/},
}

@Misc{BPA09gapsPrelimNeedAssmnt,
  author       = {BPA},
  title        = {Checking for gaps in {BP}A's future power supply: A Preliminary Needs Assessment},
  howpublished = {factsheet},
  month        = mar,
  year         = {2009},
  comment      = {More evidence about BPA getting tapped out on wind integration capacity
-- this document just came out this month. Text on p. 5-6 confirms what we've been hearing
* BPA has highest wind penetration of any balancing facility in US (20 pct I think: check)
* assumed load growth
* fish impacts reduce flexibility, eats into baseload since must reserve some for wind integration
* Canada takes a big chunk BPA power},
  file         = {BPA09gapsPrelimNeedAssmnt.pdf:BPA09gapsPrelimNeedAssmnt.pdf:PDF;BPA09gapsPrelimNeedAssmnt.pdf:BPA09gapsPrelimNeedAssmnt.pdf:PDF},
  groups       = {Read},
  owner        = {sotterson},
  timestamp    = {2009.03.31},
}

@InCollection{Brand02IncrSVDmissDat,
  author    = {Brand, Matthew},
  title     = {Incremental singular value decomposition of uncertain data with missing values},
  booktitle = {Computer Vision?ECCV 2002},
  publisher = {Springer},
  year      = {2002},
  pages     = {707--720},
  comment   = {SVD that learns one point at t time, and can handle missing features, hints for Matlab. Could use for missing data handling if reconstruct from low dim (missing data) to high dim original space. Maybe could also use for some kind of adaptive PCA or MDS, if figure out how to delete (or weight) old points -- hints may be in Bengio04OutOfSmplExt -- possibly with memory so old lessons don't go away.

Note that there are many, many updates of this idea since 2002 -- should look at them.

This idea is also somehow related to Tucker decompositions for tensor splines -- another possibly useful idea.
 -- did I get the tucker thing by looking up citations? I don't see it in the paper
 -- Tucker decomp, though, is related to multilinear PCA, which I'm planning to do: Lu09UncorrMultiLinPCA

My old thesis notes (README.org) pointed to a Matlab implementation of this but that site is now gone (in 2014). There may be other implemenations.},
  doi       = {10.1007/3-540-47969-4_47},
  file      = {Brand02IncrSVDmissDat.pdf:Brand02IncrSVDmissDat.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.06.27},
}

@TechReport{Breiman04consistRandForest,
  author      = {Leo Breiman},
  title       = {Consistency for a simple model of random forests},
  institution = {Statisticis Department, University of California, Berkeley},
  year        = {2004},
  number      = {670},
  month       = sep,
  comment     = {Biau16randFrstGuideTour says that this explains the GINI splitting criterion used in random forests.  I didn't find that here, but here are some empirical...

... Tidbits about random forest _classification_
1. bootrapping isn't that important
2. splitting based on median of a variable (not using class labels) significantly increases error rate

Understandable Gini impurity explanation is here: Vincent17giniPurity},
  file        = {:Breiman04consistRandForest.pdf:PDF},
}

@InProceedings{Bright04bestEnsChllng,
  author    = {Bright, DR and Nutter, PA},
  title     = {On the challenges of identifying the "best" ensemble member in operational forecasting},
  booktitle = {Preprints, 16\textsuperscript{th} Conf. on Numerical Weather Prediction, Seattle, WA, Amer. Meteor. Soc. J},
  year      = {2004},
  volume    = {11},
  comment   = {Eliminating entire met model ensemble for the future based on errors at a given time doesn't work, but can ingore an ensemble during instants in time when it is implausble (as judged by expert forecasters)

But is seems like people have been doing this since 2004, so maybe the state of the art has improved.},
  file      = {Bright04bestEnsChllng.pdf:Bright04bestEnsChllng.pdf:PDF},
  groups    = {Ensemble, doReadWPV_1},
  owner     = {sotterson},
  timestamp = {2014.01.29},
  url       = {https://ams.confex.com/ams/pdfpapers/69092.pdf},
}

@Article{Burger16googleDeepMindAphaGoReinfLrn,
  author    = {Christopher Burger},
  title     = {Google DeepMind's AlphaGo: How it works},
  journal   = {tastehit},
  year      = {2016},
  month     = mar,
  comment   = {Google's DeepMind, which is the world champ at Go, uses deep reinforcement learning.  This article explains a little about how it works.

Google is thinking about using DeepMind to do something like a net demand forecast: Murgia17googleDeepMindUKgrid

Also see: ERCIMnews16spclIssueMachLrn},
  file      = {Burger16googleDeepMindAphaGoReinfLrn.pdf:Burger16googleDeepMindAphaGoReinfLrn.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.27},
}

@InCollection{Busoniu10multiAgntReinfLrnOvrview,
  author    = {Bu{\c{s}}oniu, Lucian and Babu{\v{s}}ka, Robert and De Schutter, Bart},
  title     = {Multi-agent reinforcement learning: An overview},
  booktitle = {Innovations in Multi-Agent Systems and Applications-1},
  publisher = {Springer},
  year      = {2010},
  pages     = {183--221},
  file      = {Busoniu10multiAgntReinfLrnOvrview.pdf:Busoniu10multiAgntReinfLrnOvrview.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2016.12.23},
}

@TechReport{Cabrera16adaptProbPriceFrcst,
  author    = {Cabrera, Brenda L{\'o}pez and Schulz, Franziska and others},
  title     = {Time-Adaptive Probabilistic Forecasts of Electricity Spot Prices with Application to Risk Management.},
  year      = {2016},
  address   = {Sonderforschungsbereich 649, Humboldt University, Berlin, Germany},
  comment   = {Adaptive day ahead price forecast for Germany.  They find that renewables significantly affect price (no suprise).

They also have a demand forecast: Cabrera14genQuantFrcstDemandFuncD
Maybe that's joint?  Since they mention the effect of renewables, they must have somehow done a net demand forecast, it seems},
  file      = {Cabrera16adaptProbPriceFrcst.pdf:Cabrera16adaptProbPriceFrcst.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2016.12.21},
  url       = {http://econpapers.repec.org/paper/humwpaper/sfb649dp2016-035.htm},
}

@TechReport{CAISO10caisoRnwblIntegSwngDr,
  author      = {CAISO},
  title       = {Integration of Renewable Resources: Technical Appendices for California ISO Renewable Integration. Version 1},
  institution = {California ISO},
  year        = {2010},
  comment     = {Description of CAISO's use of ramp metrics (swinging door) in grid integration of renewable energy.

See also: 
Lu10unitCommCnstrnt
CAISO10caisoRnwblIntegSwngDr
Makarov09OperWindCA
Crampes18flexElecMktsRamp


},
  file        = {:CAISO10caisoRnwblIntegSwngDr.pdf:PDF},
  url         = {https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiZi9Gr9PrXAhUrAZoKHYFUCoQQFggsMAA&url=http%3A%2F%2Fwww.caiso.com%2F282d%2F282d85c9391b0.pdf&usg=AOvVaw1U3W6a4by3fR6_zOleCK2J},
}

@Article{Cama16TrumpWindpowKillBirds,
  author    = {Timothy Cama},
  title     = {Trump: Wind power "kills all your birds"},
  journal   = {The Hill},
  year      = {2016},
  month     = aug,
  comment   = {Quote I used in ModernWindABS kickoff slides},
  file      = {Cama16TrumpWindpowKillBirds.pdf:Cama16TrumpWindpowKillBirds.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.01.27},
  url       = {http://thehill.com/policy/energy-environment/290093-trump-wind-power-kills-all-your-birds},
}

@Article{Cao08Stochasticlearningoptimization,
  author    = {Cao, Xi-Ren},
  title     = {Stochastic learning and optimization-a sensitivity-based approach},
  journal   = {IFAC Proceedings Volumes},
  year      = {2008},
  volume    = {41},
  number    = {2},
  pages     = {3480--3492},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2016.12.24},
}

@Misc{Cardinal15ridgeLambdaGrid,
  author       = {Cardinal},
  title        = {Implementing ridge regression: Selecting an intelligent grid for lambda},
  howpublished = {stats.stackexchange.com blog post},
  month        = jul,
  year         = {2015},
  comment      = {Ridge regression lambda candidates generated by evenly sampling the degrees of freedom available in the design matrix.

* relationhsip between design matrix eigenvalues and ridge regression degrees of freedom
  (see Hastie09elemStatLrnBook, p. 68)
* must solve for lambda numerically
* Idea is worked out best by commentor "cardinal".},
  file         = {Cardinal15ridgeLambdaGrid.pdf:Cardinal15ridgeLambdaGrid.pdf:PDF},
  groups       = {Read},
  url          = {http://stats.stackexchange.com/questions/32246/implementing-ridge-regression-selecting-an-intelligent-grid-for-lambda},
}

@Article{Chang03Reviewpaperhealth,
  author    = {Chang, Peter C and Flatau, Alison and Liu, SC},
  title     = {Review paper: health monitoring of civil infrastructure},
  journal   = {Structural health monitoring},
  year      = {2003},
  volume    = {2},
  number    = {3},
  pages     = {257--267},
  owner     = {sotterson},
  publisher = {Sage Publications},
  timestamp = {2016.12.19},
}

@InProceedings{chen00gaussianization,
  author    = {Scott Shaobing Chen and Ramesh A. Gopinath},
  title     = {{Gauss}ianization},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2000},
  pages     = {423--429},
  comment   = {295: ReadNo
Review:
NOTE: copied from speakerclust.bib, just so don't need to include that in documents. See other Gaussianization entries in that bib file.},
  file      = {NIPS paper:chen00gaussianization.pdf:PDF;slides:chen00gaussianizationSlides.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.12.07},
  url       = {http://citeseer.nj.nec.com/456021.html},
}

@Unpublished{Chen11spinRsrvUClitRev,
  author    = {Peiyuan Chen and Tuan Le},
  title     = {Literature Review on Spinning Reserve and Unit Commitment in Power Systems ({OSR}Nordic Project)},
  year      = {2011},
  comment   = {Discusses probabilistic criteria for spinning reserves, including one in the "TSOs' operation handbook, UCTE"

eq. 2.10 is the one mentioned above.},
  file      = {Chen11spinRsrvUClitRev.pdf:Chen11spinRsrvUClitRev.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.11},
}

@Conference{Chen13robustOptMISO,
  author    = {Yonghong Chen and Xing Wang and Yongpei Guan},
  title     = {Applying Robust Optimization to {MISO} Look-ahead Unit Commitment},
  booktitle = {FERC Technical Conference on Increasing Real-Time and Day- Ahead Market Efficiency through Improved Software},
  year      = {2013},
  month     = jun,
  comment   = {Slides suggest that they're using a few scenarios in their planning -- not sure where they come from, or exactly what "scenario" means. Anyway, they are also looking at Robust Optimization.

Related to Wang13lkAhdUCrobustOpt},
  file      = {Chen13robustOptMISO.pdf:Chen13robustOptMISO.pdf:PDF},
  groups    = {Use, doReadWPV_1},
  location  = {Washington DC, USA},
  owner     = {sotterson},
  timestamp = {2014.01.23},
  url       = {http://www.ferc.gov/CalendarFiles/20130710154949-M1_Chen.pdf},
}

@InProceedings{Chentanez04Intrinsicallymotivatedreinforcement,
  author    = {Chentanez, Nuttapong and Barto, Andrew G and Singh, Satinder P},
  title     = {Intrinsically motivated reinforcement learning},
  booktitle = {Advances in neural information processing systems},
  year      = {2004},
  pages     = {1281--1288},
  owner     = {sotterson},
  timestamp = {2016.12.24},
}

@Article{Cho07regimeSwTest,
  author    = {Jin Seo Cho and Halbert White},
  title     = {Testing for Regime Switching},
  year      = {2007},
  volume    = {75},
  number    = {6},
  month     = nov,
  pages     = {1671--1720},
  url       = {http://ideas.repec.org/a/ecm/emetrp/v75y2007i6p1671-1720.html},
  file      = {Cho07regimeSwTest.pdf:Cho07regimeSwTest.pdf:PDF;Cho07regimeSwTest.pdf:Cho07regimeSwTest.pdf:PDF},
  journal   = {Econometrica},
  owner     = {sotterson},
  timestamp = {2008.12.12},
}

@InProceedings{Chow99dynPCAintTransf,
  author    = {KC Chow and KJ Tan and H Tabe and J Zhang and NF Thornhill},
  title     = {Dynamic principal component analysis using integral transforms},
  booktitle = {AIChE Annual Meeting},
  year      = {1999},
  number    = {232(c)},
  month     = nov,
  comment   = {PCA captures lagged dependency: time, frequency and wavelets compared.

* Goal: Compress multi-channel signals into smaller space, capturing time dependent lags.
-- up top 53 channels in input case, I think
-- additional synthetic and real test cases w/ noise for a few channels
-- comparison is kinda handwavy

Three approaches compared (nice flowchart explaining when to use what (Fig 1))
1. Time domain
* add shifted version of signals to PCA input (DPCA, as in Ku95dynPCA)
* works on 53 variable case
* totally fails on 3 channel microphone case, lot's of noise, probably w/ a lot of reverb
* method doesn't produce a "correlation signature":
2. Fourier domain
* replace time series w/ power spectrum and then PCA
* if successful, can capture most of variation in a single spectrum (1\textsuperscript{st} PCA comp)
* works for noisy 3 channel microphone case
* good for showing "related persistent oscillations" (same-frequency bins are correlated)
* shows correlations between between trends showing simimlar behavior at different times
3. Wavelet domain
* input is time-shifted versions of wavelet coeffs at different resolutions
* pick delay/resolution that captures most variance in a single PCA comp
* can find band AND duration limited correlations (but they kind-of say that Fourier PCA can also)
* can efficiently find time delays (but can't Fourier also do that if do IFFT?)
* procedure is manual and not thoroughly tested},
  file      = {Chow99dynPCAintTransf.pdf:Chow99dynPCAintTransf.pdf:PDF},
  location  = {Miami Beach, FL},
  owner     = {scot},
  timestamp = {2010.07.29},
  url       = {http://personal-pages.ps.ic.ac.uk/~nina/Research/ConfPapers/ChowEtAl_AIChE_1998.pdf},
}

@Misc{Cibulka08transTechCapImpact,
  author       = {Lloyd Cibulka},
  title        = {Transmission Technologies for Increased Capacity and Reduced Impacts},
  howpublished = {Presentation Slides},
  month        = nov,
  year         = {2008},
  comment      = {Tech for increasing transmission cap. or reducing impact. Related to dynamic line rating.},
  file         = {Cibulka08transTechCapImpact.ppt:Cibulka08transTechCapImpact.ppt:PowerPoint},
  owner        = {sotterson},
  timestamp    = {2009.05.27},
}

@Misc{Cibulka09transTechAttr,
  author       = {Lloyd Cibulka},
  title        = {Electric Transmission Technologies: Attributes and Considerations},
  howpublished = {Presentation Slides},
  month        = jan,
  year         = {2009},
  comment      = {Lloyd's slides comparing tech. for increasing transmsn line capcity. Dynamic line rating is one of them.},
  file         = {Cibulka09transTechAttr.ppt:Cibulka09transTechAttr.ppt:PowerPoint},
  owner        = {sotterson},
  timestamp    = {2009.05.27},
}

@Misc{CLS09windSarFactSht,
  author       = {CLS},
  title        = {Soprano CLS Wind Product Fact Sheet},
  howpublished = {web page},
  month        = feb,
  year         = {2009},
  comment      = {CLS NORSEWInD SAR overview: good explanation of errors * SAR wind estimate comes from wind-dependent speckle * Soprano wide swath resolution is 975mx975m * RMS error around 1.87 m/s for 2-20 m/s -- done w/ Bayesian approach, which improves by 20\% -- error much less for 5-9m/s Backscatter error causes * rain -- IDEA: use SafeWind rain data as confidence estimator * oil spill * tide currents * algae * melt water * ice * atmospheric stratification (how does this effec radar?) -- IDEA: add NWP inputs to estimate SAR confidence * being near the coast -- unfortunate: where wind power people want to use SAR! -- IDEA: spatially dependent SAR confidence -- IDEA: spatially dependent SAR feature selection * wind speed -- ==> nonlinear -- bad when < 2m/s or > 20m/s (just categorize as <2m/s or >20m/s) -- these are the extremes SafeWind is looking for, I think, so SAR is tricky for that\ Direction estimation * comes from NWP * doesn't include streak features in SAR images, as some algo's do * 30 degree RMS error},
  file         = {CLS09windSarFactSht.pdf:CLS09windSarFactSht.pdf:PDF},
  groups       = {Read},
  owner        = {scot},
  timestamp    = {2010.05.27},
  url          = {http://soprano.cls.fr/fact_sheets/wind_product.html},
}

@Article{Clyde10bayesAdaptSampVarSelAvg,
  author    = {Clyde, M. and Ghosh, J. and Littman, M.},
  title     = {{Bayes}ian adaptive sampling for variable selection and model averaging},
  journal   = {Journal of Computational and Graphical Statistics},
  year      = {2010},
  comment   = {targeted sampling, online feature selection, ensemble systems
* "to appear" so I guess it's submitted but not yet published?
* has R code so would be easy to try},
  file      = {Clyde10bayesAdaptSampVarSelAvg.pdf:Clyde10bayesAdaptSampVarSelAvg.pdf:PDF},
  owner     = {scot},
  publisher = {Citeseer},
  timestamp = {2010.12.06},
}

@Article{codeLab17solFrcstTypesWeb,
  author    = {codeLab},
  title     = {Solar Forecast Types Web},
  journal   = {Control, Dynamics and Estimation Lab. Buffalo University},
  year      = {2017},
  comment   = {Nice table of solar forecasting types.  Use for GIZ},
  file      = {codeLab17solFrcstTypesWeb.pdf:codeLab17solFrcstTypesWeb.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.13},
  url       = {http://code.eng.buffalo.edu/cloud/index.html},
}

@TechReport{Coley17disChoicePVadopt,
  author      = {S. Coley and N. Enbar and M. Bingham and J. Kinnell},
  title       = {Applying Discrete Choice Experiment Modeling to Photovoltaic Adoption Forecasting},
  institution = {EPRI},
  year        = {2017},
  number      = {3002011011},
  address     = {Palo Alto, CA},
  comment     = {Recommended by Ben Norris, CPR Consulting

Gavin Novotny wrote:

NREL “generic adoption forecast”
•	https://www.nrel.gov/analysis/dgen/
•	“geospatially rich, bottom-up, market-penetration model”
•	Used to assist Maine state legislature in forecasting DPV by 2021
•	Successor to “SolarDS” model
},
  file        = {:Coley17disChoicePVadopt.pdf:PDF},
}

@InProceedings{Collier08windForecastPowerModel,
  author    = {Craig Collier and Jonathan Collins and Jeremy Parkes},
  title     = {Forecasting for Utility --Scale Wind Farms:The Power Model Challenge},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  year      = {2008},
  comment   = {* map of Garrard Hassan's global wind projects (10 contries, 4 continents, 6.5GW) Forecasting method (has good system diagrams) * inputs are -- several NWP forecasts -- historic data -- live dta -- site geography (different than NWP?) * model methods are: -- adaptive statistics -- time series -- climatology * optimized combo of models (suppliers, so they use many external forecasts?) -- graph showing 8 models combined to one modelthat is better than any of them -- this is over more than just NWP models, seems to include statistical models too -- no explanation of the models, however * models are adapted based on wind forecast error Power Forecast * upto 100\% deviation from manufacturer's powercurve (so need to estimate) * inputs: wind forecast, live data, geography * power forecast is adaptive * two methods -- scale up to single site power curve -- forecast at sub-site locations and build a wind-to-power convertion model -- advanced model can handle ---- topological, and waking effects ---- changes in density (shows a 200W difference in curve between warm and cold days) ---- single turbine dropouts (or subregion dropouts) -- advanced model improves accuracy by 7\% MAE capacity, given perfect wind forecast -- advanced model is about 1\% MAE accurate than simple model (w/ real wind predicrtions) * calculates power error bars using power curve and forecast error bars -- calculates expected value or something? -- especially useful for high wind speed shutdown -- improves short term forecasts},
  file      = {Collier08windForecastPowerModel.pdf:Collier08windForecastPowerModel.pdf:PDF;Collier08windForecastPowerModel.pdf:Collier08windForecastPowerModel.pdf:PDF},
  groups    = {IWFTM08, Read},
  owner     = {sotterson},
  timestamp = {2008.09.19},
  url       = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
}

@Article{Colombani12sparsePLScmpr,
  author    = {Colombani, Carine and Croiseau, Pascal and Fritz, S and Guillaume, F and Legarra, Andres and Ducrocq, Vincent and Robert-Grani{\'e}, Christ{\`e}le},
  title     = {A comparison of partial least squares (PLS) and sparse PLS regressions in genomic selection in French dairy cattle},
  journal   = {Journal of dairy science},
  year      = {2012},
  volume    = {95},
  number    = {4},
  pages     = {2120--2131},
  comment   = {partial PLS for dim reduction: is sparse better?},
  file      = {Colombani12sparsePLScmpr.pdf:Colombani12sparsePLScmpr.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2014.10.17},
  url       = {http://www.sciencedirect.com/science/article/pii/S0022030212001749},
}

@TechReport{consentec10germanRiskTR,
  author      = {Consentec},
  title       = {Gutachten zur Dimensionierung des Regelleistungs-bedarfs unter dem NRV},
  institution = {CONSENTEC Consulting f{\"u}r Energiewirtschaft und -technik GmbH},
  year        = {2010},
  number      = {17.12.2010},
  comment     = {Describes risk market for German primary, secondary, and tertiary reserves; from Markus Speckman (IWES)},
  file        = {Google Translated English version:consentec10germanRiskTR-english.pdf:PDF;original:consentec10germanRiskTR.pdf:PDF},
  location    = {Tulpenfeld 4, 53113 Bonn},
  owner       = {sotterson},
  timestamp   = {2012.02.09},
  url         = {http://www.consentec.de/wp-content/uploads/2012/01/Gutachten_zur_Hoehe_des_Regelenergiebedarfes_2010.pdf},
}

@Conference{Cooke07vinesOverview,
  author    = {Cooke, RM and Morales, O. and Kurowicka, D.},
  title     = {Vines in overview},
  booktitle = {Brazilian Conference on Statistical Modelling in Insurance and Finance},
  year      = {2007},
  pages     = {2--20},
  file      = {Cooke07vinesOverview.pdf:Cooke07vinesOverview.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.11.24},
}

@InProceedings{Cornford03sequentialSparseGaussProc,
  author    = {Dan Cornford and Lehel Csato and Manfred Opper},
  title     = {Sequential, Sparse Learning in {Gauss}ian Processes},
  booktitle = {GeoComputation},
  year      = {2003},
  comment   = {Online basis vector picking for Gaussian Processes, uses "basis recycling" via multiple passes Recycling * single sweep, like in Csato01onlineLearnWindField can be bad * recycling based on Minka's Expectation Propagation * allows you to pick up bases that were previously lost, I think Other * Matlab implementation at: http://www.ncrg.aston.ac.uk/Projects/SSGP/ * is applied to wind field modelling but maybe has other uses * use for -- multi-turbine -- offsite measurements -- NREL-style model blending note: title in conference index is different, is: "Sparse, Sequential Bayesian Geostatistics"},
  file      = {Cornford03sequentialSparseGaussProc.pdf:Cornford03sequentialSparseGaussProc.pdf:PDF;Cornford03sequentialSparseGaussProc.pdf:Cornford03sequentialSparseGaussProc.pdf:PDF},
  owner     = {scotto},
  timestamp = {2008.10.06},
  url       = {http://www.geocomputation.org/2003/},
}

@Article{Costa08revYngHistWindPred,
  author    = {Alexandre Costa and Antonio Crespo and Jorge Navarro and Gil Lizcano and Henrik Madsen and Everaldo Feitosa},
  title     = {A review on the young history of the wind power short-term prediction},
  year      = {2008},
  volume    = {12},
  number    = {6},
  pages     = {1725--1744},
  file      = {Costa08revYngHistWindPred.pdf:Costa08revYngHistWindPred.pdf:PDF;Costa08revYngHistWindPred.pdf:Costa08revYngHistWindPred.pdf:PDF},
  groups    = {DOE-PNL09},
  journal   = {Renewable and Sustainable Energy Reviews},
  owner     = {sotterson},
  timestamp = {2009.03.03},
}

@Book{Cover06ElemInfoTheoryBook,
  title     = {Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)},
  publisher = {Wiley-Interscience},
  year      = {2006},
  author    = {Thomas M. Cover and Joy A. Thomas},
  isbn      = {0471241954},
  comment   = {An update of the information theory book I used at UW},
  owner     = {sotterson},
  timestamp = {2008.12.03},
}

@TechReport{Crawley10longDat_2_12,
  author      = {S. P. Crawley},
  title       = {{NOR}SEWIND DELIVERABLE 2.12: REPORT ON LONG-TERM DATA SOURCES},
  institution = {Garrad Hasan},
  year        = {2010},
  comment     = {Preliminary long term observation selection for NORSEWInD; used to adjust short term measurements (was named: 101378BR01c_Deliverable2.12.pdf) * Finding existing long term observation records that can be used to correct new measurements added to NORSEWInd. * looks like they'll cluster these, correct them, and make single indices for each region. * Could cause problems -- how handle lags isn't specified -- long term correlation approach may mask rare but significant correlations * so, I'd like to get the raw data, if possible Sources * Coastal Met: not necess. reliable * Long-term offshore: likely to be OK * May use wind farm data but downtime maybe a problem. * National or regional Combined indices for validation * High resolution hindcasts will be evaluated * Inland data sources are explicitly removed so must focus on offshore. * plan to group these into 8 zones Not a huge amount of data for DK forecasting (as NORSEWInD requires) * Zone maps suggest only about 10 in official DK Zone. * Maybe others outside are relevant? (maybe another 20). Data set will be fairly long 5 years of data\@project start; 7\@end. * s/provide 2.3\% standard deviation in mean est.},
  file        = {Crawley10longDat_2_12.pdf:Crawley10longDat_2_12.pdf:PDF},
  groups      = {Read},
  owner       = {scot},
  timestamp   = {2010.06.10},
}

@Book{Cutululis13interconOptEERA-DTOC,
  title  = {Report on Design Tool on Variability and predictability: Interconnection optimisation and power plant system services. Task 2.1: Power output variability and predictability. EERA-DTOC, D2.8: WP 2.},
  year   = {2013},
  author = {Cutululis, {Nicolaos Antonio} and Faiella, {Luis Mariano} and Scott Otterson and {Barahona Garzón}, Braulio and Jan Dobschinski},
  note   = {FP7-ENERGY-2011-1/ no282797},
  file   = {:Cutululis13interconOptEERA-DTOC.pdf:PDF},
  url    = {http://orbit.dtu.dk/en/publications/report-on-design-tool-on-variability-and-predictability-d28(52715b17-cdc6-47e7-ba59-5080bef14333).html},
}

@Article{Davidson15InsideDOEsWindFrcst,
  author  = {Francesca Davidson},
  title   = {Inside The DOE?s Wind Forecasting Effort},
  journal = {North American Windpower},
  year    = {2015},
  volume  = {12},
  number  = {2},
  month   = mar,
  comment = {US Dept. of Energy research on wind power forecasting, is mostly NWP. Is written by Francesca (from the old 3TIER, now Vaisala).

She says:

A comprehensive list of relevant topography-related phenomena that are poorly represented in models includes warm fronts in stable atomospheric conditions, gap flows, mountain waves, topographic wakes, convective outflows, marine pushes, land-sea breezes, slope and drainage flows, and low-level jets.

Are there weather categories that make these more likely?},
  file    = {Davidson15InsideDOEsWindFrcst.pdf:Davidson15InsideDOEsWindFrcst.pdf:PDF},
  groups  = {Read},
  url     = {http://www.nawindpower.com/issues/NAW1503/index.html},
}

@Article{Daye11ManagingPVIntermittencyStandards,
  author    = {Tony Daye},
  title     = {Managing Intermittency: Standards and Recommended Practices in Solar Power Forecasting},
  journal   = {ERCOT Emerging Technologies Working Group},
  year      = {2011},
  comment   = {NOt sure if it's too old or not, but I put a few slides into Colombia talk.},
  file      = {Daye11ManagingPVIntermittencyStandards.ppt:Daye11ManagingPVIntermittencyStandards.ppt:PowerPoint},
  owner     = {sotterson},
  timestamp = {2017.05.02},
  url       = {http://greeningthegrid.org/trainings-1/webinars/implementing-wind-and-solar-power-forecasting},
}

@Misc{DeepLrnSchool2016_slides,
  author    = {Deep Learning School},
  title     = {September 24-25, 2016 Stanford, CA},
  month     = sep,
  year      = {2016},
  comment   = {Corresponding videos in my OneDrive Acct:
https://onedrive.live.com/?id=4BBD96B3698748F8%21206864&cid=4BBD96B3698748F8
C:\Users\scott\OneDrive\shareHW\school\lectures_misc\DeepLearningSchool2016},
  file      = {DeepLrnSchool2016_slides.pdf:DeepLrnSchool2016_slides.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.02.17},
  url       = {https://www.bayareadlschool.org/},
}

@Misc{Deesko15digFingerPrintFaultSlides,
  author       = {Peter Deesko and F. Bärmann},
  title        = {The Digital Fingerprint of Equipment Faults -- an Idea for Expanding the SR::SPC Module},
  howpublished = {Presentation Slides},
  month        = mar,
  year         = {2015},
  comment      = {Peter Deeskow's presentation on his proposed improvements to the STEAG Fault Monitoring system.},
  file         = {:Deesko15digFingerPrintFaultSlides.pdf:PDF},
}

@Article{Deeskow17modNotesSPRT,
  author    = {Peter Deeskow},
  title     = {SPRT modification notes},
  journal   = {STEAG Energy Services GmbH},
  year      = {2017},
  comment   = {These emails are related to the June 30 meeting with Peter Deeskow,  at STEAG.  We are also referring to this paper:  Gross02earlyDetSPRT

Peter's modifications, which handle non-Gaussianinity specify H1, are on the next page.},
  file      = {:papers\\Deeskow17modNotesSPRT.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.07.14},
}

@Article{DelleMonache13analogEns,
  author    = {Delle Monache, Luca and Eckel, F Anthony and Rife, Daran L and Nagarajan, Badrinath and Searight, Keith},
  title     = {Probabilistic weather prediction with an analog ensemble},
  journal   = {Monthly Weather Review},
  year      = {2013},
  volume    = {141},
  number    = {10},
  pages     = {3498--3516},
  comment   = {An analog ensemble paper

Slides attached.},
  doi       = {10.1175/MWR-D-12-00281.1},
  file      = {DelleMonache13analogEns.pdf:DelleMonache13analogEns.pdf:PDF;Slides (2012):DelleMonache13analogEns_slides2012.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.07.09},
}

@Misc{Deng15deepLrnSpchLang,
  author       = {Li Deng},
  title        = {Deep Learning for Speech/Language Processing},
  howpublished = {Tutorial given at Interspeech, Sept 6, 2015},
  month        = sep,
  year         = {2015},
  comment      = {Slides showing that everything is gone!  No MFCC's, maybe not even an FFT!

Song15end2endDeepLnSpchRecog has more details.},
  file         = {Deng15deepLrnSpchLang.pdf:Deng15deepLrnSpchLang.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2017.01.23},
  url          = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/07/interspeech-tutorial-2015-lideng-sept6a.pdf},
}

@InProceedings{DePasquale07randSubMissFeat,
  author    = {Joseph DePasquale and Robi Polikar},
  title     = {Random Feature Subset Selection for Analysis of Data with Missing Features},
  booktitle = {International Joint Conference on Neural Networks (IJCNN)},
  year      = {2007},
  pages     = {2379--2384},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comment   = {An ensemble approach to missing features but for classification; convert to clustering or regression?},
  doi       = {10.1109/IJCNN.2007.4371330},
  file      = {DePasquale07randSubMissFeat.pdf:DePasquale07randSubMissFeat.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.01.30},
}

@Article{DiMarzio09circRgrsnLocPol,
  author    = {Di Marzio, M. and Panzera, A. and Taylor, C.C.},
  title     = {Circular-circular nonparametric regression using local polynomial fitting},
  journal   = {submitted but I don't know where},
  year      = {2009},
  comment   = {local polynomial circular-to-circular regression. Has a circular kernel. Use for lagged velocity basis somehow?},
  file      = {DiMarzio09circRgrsnLocPol.pdf:DiMarzio09circRgrsnLocPol.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.12.22},
  url       = {http://www.dmqte.unich.it/personal/dimarzio.htm},
}

@InProceedings{Ding04KknnInKmeansClust,
  author       = {Ding, Chris and He, Xiaofeng},
  title        = {K-nearest-neighbor consistency in data clustering: incorporating local information into global optimization},
  booktitle    = {Proceedings of the 2004 ACM symposium on Applied computing},
  year         = {2004},
  pages        = {584--589},
  organization = {ACM},
  comment      = {Using KNN to improve kmeans consistence. Old paper but maybe still a good idea?

Was this enhanced in Karegowda12kmeansKNNcasc

Could it be enhanced with the new KNN distance ideas?
e.g. Tarlow13knnStochDistLrn (and other papers in energy.bib)

or the high dimensional KNN hub ideas? Some other KNN distance paper I can't find right now.},
  file         = {Ding04KknnInKmeansClust.pdf:Ding04KknnInKmeansClust.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2014.04.04},
  url          = {http://dl.acm.org/citation.cfm?id=968021},
}

@Article{Dobschinski16develEwelineGOR_Slides,
  author    = {Jan Dobschinski and Malte Siefert},
  title     = {Development of Innovative Weather and Power Forecast Models for the Grid Integration of Weather-Dependent Energy Sources. Project Eweline},
  journal   = {GOR Workshop},
  year      = {2016},
  comment   = {Jan's slides about Eweline from some workshop.  Use as a source for the GIZ Colombia course.},
  file      = {Dobschinski16develEwelineGOR_Slides.pptx:Dobschinski16develEwelineGOR_Slides.pptx:PowerPoint 2007+},
  owner     = {sotterson},
  timestamp = {2017.04.12},
}

@InProceedings{Dobschinski19ensOffshore,
  author    = {J. Dobschinski and S. Hagemann and B. Lange and A. Wessel and L. von Bremen},
  title     = {Calibration and combination of a regional multi model ensemble for predicting offshore wind speeds},
  booktitle = {European Wind Energy Conference and Exhibition (EWEC)},
  year      = {2010},
  month     = apr,
  publisher = {Institut f?r Solare Energieversorgungstechnk e. V. K?nigstor 59, D-34119 Kassel Tel. +49-561-7294-0;Fax +49-561-7294-100},
  comment   = {The first Gaussain Ensemble Dressing paper from Jan?

Reliability
* does predicted error dist. match measured?
* show w/ reliability plot:
-- observed coverage rate vs. predicted (example on p. 11)
-- I'm not clear on what "coverage" means

Skill
* advantage over simple reference e.g. climatology
* Ranked Probability Skill Score
* I don't know what this is either

Simple poor man's ensemble
* 23 forecast algorithms
* assume normality: mean/var of forecast at each time yields gaussian dist params/probs
* poor reliability

Gaussian Ensemble Dressing (what they propose)
* assume each ensemble is a Gaussian
-- mean related to point forecast value
-- but shared covariance across ensemble
* probablity forecast is a weighted linear combination of Gaussians

* but equation must be oversimplified since means are fixed over time

* param tuning cost func:
-- "Ranked Probability score"
-- seems to be different than prev. ensemble dressing approaches

* anyway, works much better than simple approach

Question
* how does this compare to DTU's quantile regression?},
  file      = {Dobschinski19ensOffshore.pdf:Dobschinski19ensOffshore.pdf:PDF},
  groups    = {Read, Ensemble, doReadWPV_2},
  location  = {Warsaw},
  owner     = {scot},
  timestamp = {2010.05.27},
  url       = {http://www.iset.uni-kassel.de/pls/w3isetdad/www_iset_new.main_page?p_lang=eng&p_owa_call=www_veroeff.show_veroeffdaten%3Fp_veroeff_nr=2341%26p_lang=eng},
}

@Book{Downey13thinkBayesBook,
  title     = {Think {Bayes}},
  publisher = {" O'Reilly Media, Inc."},
  year      = {2013},
  author    = {Downey, Allen B},
  comment   = {Bayesian inference book, discrete, in python. Might be a good shortcut sometimes.

MS OneNote tester is reading it, with the idea of automating some kinda test procedure:
http://blogs.msdn.com/b/johnguin/archive/2014/03/14/books-on-my-reading-list-right-now-think-bayes-and-10-print.aspx},
  file      = {Downey13thinkBayesBook.pdf:Downey13thinkBayesBook.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.04.26},
  url       = {http://www.greenteapress.com/thinkbayes/},
}

@Misc{Drechsler11multiImputeSlides,
  author       = {J{\"o}rg Drechsler},
  title        = {Missing Data and Imputation},
  howpublished = {Seminar Slides, Sommersemester, Ludwig-Maximilians-Universit{\"a}t},
  year         = {2011},
  comment      = {Explaination of how Amelia satisfies its logical bounds. Also, seminar on general multiple imputation. * explains Rubin's rules and other stuff Seminar page here: http://www.statistik.lmu.de/~fkreuter/imputation_sose2011/},
  file         = {Slides:Drechsler11multiImputeSlides.pdf:PDF},
  groups       = {Read},
  owner        = {sotterson},
  timestamp    = {2012.08.06},
  url          = {http://www.statistik.lmu.de/~fkreuter/imputation_sose2011/downloads/Imputationsvorlesung_5_slides.pdf},
}

@Misc{DTU07RadarAtSeaAppB,
  author       = {DTU},
  title        = {Appendix B: {RADAR\@SEA} - Detailed Project Description},
  howpublished = {Proposal},
  year         = {2007},
  comment      = {The technical description of the DTU Radar\@Sea project File came from Sven Creutz Thomsen, originally named: RadarAtSea-AppB-ProjDescription.pdf I'm not sure about the date.},
  file         = {DTU07RadarAtSeaAppB.pdf:DTU07RadarAtSeaAppB.pdf:PDF},
  groups       = {Read},
  owner        = {scot},
  timestamp    = {2010.05.26},
}

@Misc{DTU07safeWind,
  author       = {DTU},
  title        = {SafeWind: Multi-scale data assimilation, advanced wind modelling and forecasting with emphasis to extreme weather situations for a safe large-scale wind power integration. Annex I - ?Description of Work?},
  howpublished = {Proposal},
  month        = nov,
  year         = {2007},
  comment      = {SafeWind project description. Originally from Sven Creutz Thomsen. Filename was: SAFEWIND_DoW.pdf},
  file         = {DTU07safeWind.pdf:DTU07safeWind.pdf:PDF},
  groups       = {Read},
  owner        = {scot},
  timestamp    = {2010.05.26},
}

@Article{DWD06OperationellesNWVSystem,
  author    = {DWD},
  title     = {Operationelles NWV-System: Operationelle Einf{\"u}hrung des ICON-EU Nestes},
  journal   = {Deutscher Wetterdienst},
  year      = {2106},
  comment   = {ICON-EU model I had a picture of in Colombia slides},
  file      = {DWD06OperationellesNWVSystem.pdf:DWD06OperationellesNWVSystem.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.30},
  url       = {https://www.dwd.de/DE/fachnutzer/forschung_lehre/numerische_wettervorhersage/nwv_aenderungen/_functions/DownloadBox_modellaenderungen/icon/pdf_2015/pdf_icon_02_07_2015.pdf;jsessionid=60F4C68E613519E3B98406DF21EF9FFE.live11053?__blob=publicationFile&v=2},
}

@Article{DWD17RegionalmodellCOSMODE,
  author    = {DWD},
  title     = {Regionalmodell COSMO--DE},
  journal   = {Deutscher Wetterdienst},
  year      = {2017},
  comment   = {Used for Colombia class},
  file      = {DWD17RegionalmodellCOSMODE.pdf:DWD17RegionalmodellCOSMODE.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.30},
  url       = {http://www.dwd.de/DE/forschung/wettervorhersage/num_modellierung/01_num_vorhersagemodelle/regionalmodell_cosmo_de.html?nn=512942},
}

@Misc{Eckel08efficientEnsAnalog,
  author       = {F. Anthony Eckel},
  title        = {Efficient Production of High Quality, Probabilistic Weather Forecasts},
  howpublished = {Slides},
  year         = {2008},
  comment      = {Analog ensemble forecasts explained, compared with other types. Also, a new performance measure, I think.

Weakness of ensemble is that rare event may not have occured in training data (I WONDER, does there need to be a hybrid approach?)},
  file         = {Slides:Eckel08efficientEnsAnalog.pptx:PowerPoint 2007+},
  groups       = {Ensemble, Test, doReadNonWPV_1},
  owner        = {sotterson},
  timestamp    = {2013.10.08},
  url          = {http://www.atmos.washington.edu/~cliff/Tony.pptx?},
}

@Electronic{ECMWF11ensembPamph,
  author    = {ECMWF},
  year      = {2011},
  title     = {The ECMWF Ensemble Prediction System},
  url       = {http://www.ecmwf.int/about/information_leaflets/EPS.pdf},
  comment   = {Friendly ECMWF pamphlet describing meteorology of how ensembles are generated},
  file      = {ECMWF11ensembPamph.pdf:ECMWF11ensembPamph.pdf:PDF},
  groups    = {Ensemble, doReadWPV_1},
  owner     = {scot},
  timestamp = {2011.04.11},
}

@InProceedings{Efron82jackBootResamp,
  author    = {Efron, B.},
  title     = {The jackknife, the bootstrap and other resampling plans},
  booktitle = {SIAM CBMS-NSF Regional Conference Series in Applied Mathematics},
  year      = {1982},
  volume    = {1},
  comment   = {Describes corrected percentile method used in matlab's bootci.m},
  file      = {1980 Tech Note, covers same stuff?:Efron82jackBootResamp_TechNote.pdf:PDF},
  owner     = {scot},
  timestamp = {2011.06.06},
}

@Misc{Eisenbrand09optMethFin_CrsNotes,
  author       = {Friedrich Eisenbrand},
  title        = {Optimization Methods in Finance Course Notes: Part 4 Mean-Variance Portfolio Theory},
  howpublished = {Course Notes},
  month        = oct,
  year         = {2009},
  comment      = {Financial optimzation course slides. Includes linear stochastic programming, portfolio optimization (or the start of it). Relevant for ReWP portfolio selection. Also general optimization and maybe optimal TSO trading in Eweline.

The "variance of a sum" slides, "PART 4 MEAN-VARIANCE PORTFOLIO THEORY" give a simple way to compute the variance of any sum, given the pairwise covariances. Useful for ReWP pool selection: having only computed a small number of pairwise covariances, can compute the variance of the sum of any pool. Since forecasting accuracy improves with decreasing variance this is an approximate way to select a pool with the best forecastability. Could verify this by looking at the empirical results of EERA DTOC, and seeing if this approximation is true on that data.

The idea would be:

1.) compute pairwise FORECAST ERROR covariances of all farms
2.) for a given power target compute all power sum combinations that produce enough power (at some quantile?)
3.) find the cost of power from all power sum combinations
4.) find the error variance for all combinations (computationally cheap)


For all combos satisfying 2.) somehow start from lowest cost/lowest error variance combination. Possibly compute the real quantile forecast. If it fails to meet the power target, compute the sum forecast for the next best combination. Repeat until satisfied.

Or something... is related to Jan's PhD thesis forecastability stuff, and to EERA DTOC.

Or, alternatively, use the covariance matrix to cluster farms into "groups with the minimum summed covarariance" and do something else...},
  file         = {Eisenbrand09optMethFin_CrsNotes.pdf:Eisenbrand09optMethFin_CrsNotes.pdf:PDF},
  groups       = {Read},
  url          = {http://disopt.epfl.ch/page-10555-en.html},
}

@Misc{Electropaedia19solarPower,
  author       = {Electropaedia},
  title        = {Solar Power (Technology and Economics)},
  howpublished = {Web Page},
  year         = {2018},
  comment      = {Overview of Solar energy with lots of definitions, numbers, standards and facts.  A good source of graphs.

Facts I've used from it
* Temperature has a significant impact on PV efficiency: reduces about 0.5% for every C degree temp increased
* PV power output vs. irradiance curve is almost linear above a low irradiance threshold.  
   - The slope increases with decreasing module temperature.
   - so high power saturation doesn't happen like it does with wind power
   - nonlinearity starts at around 400 W/m^2 intensity (Grunow04weakLightPV)
* PV module efficiency test standards
   - STC:  irrad=1000 W/m^2, T=25 C, Air Mass = 1.5
   - NOCT: irrad=800 W/m^2, T=20C, wind=1 m/s, rear side of panel is open (NOCT is considered more realistic)},
  file         = {:Electropaedia19solarPower.pdf:PDF},
  url          = {https://www.mpoweruk.com/solar_power.htm},
}

@Misc{Elidan12copulaMachLrnTut,
  author       = {Gal Elidan},
  title        = {Copulas and Machine Learning: UAI 2012 Tutorial},
  howpublished = {Presentation Slides},
  month        = aug,
  year         = {2012},
  comment      = {Tutorial Slides running from copulas to vine copulas to graphical models to Gaussian Processes.

  Copula Processes can handle huge dimensions

Author's corresponding Paper: Elidan13copulasMachLrn
Extension to multimodalit, sensor networkd: Liu17multDatFusCplaProc

Copula Processes
  - Python: Dubois15deepMineCopulaHyper
  - reference: Wilson10copulaProc (same thing as Elidan means?)
  - Can
    - do conditional:  HernandezLobato13gaussProceCondTS
    - do huge # dimensions: Elidan12copulaMachLrnTut
    - handle missing data: Wilson10copulaProc
},
  file         = {Elidan12copulaMachLrnTut.pdf:Elidan12copulaMachLrnTut.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2017.01.22},
  url          = {http://www.auai.org/uai2012/tutorials.shtml},
}

@InProceedings{El-Sharkawi08neuralAncWind,
  author    = {Mohamed A. El-Sharkawi},
  title     = {Neural Network Ancillaries For Wind Energy Forecasting},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  year      = {2008},
  comment   = {Good basic Neural Net stuff to keep in mind
* similar to talk we heard at UW w/ BPA folks
* says adaptation is really important (is this for demand forecasting or for short term wind forecasting?)},
  file      = {El-Sharkawi08neuralAncWind.pdf:El-Sharkawi08neuralAncWind.pdf:PDF;El-Sharkawi08neuralAncWind.pdf:El-Sharkawi08neuralAncWind.pdf:PDF},
  groups    = {IWFTM08, Read},
  owner     = {sotterson},
  timestamp = {2008.09.19},
  url       = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
}

@TechReport{Emory19poissonBinomCnnct,
  author      = {Emory},
  title       = {The Connection Between the Poisson and Binomial Distributions},
  institution = {The Oxford Math Center, Emory University},
  year        = {2019},
  type        = {techreport},
  comment     = {Derivation of the Poisson distribution as an approximation to the binomial distribution as the number of trials gets large and the probability of success is low.  For the beginning years of MNSP, adoption probabilities, p are certainly small, but n is not large.  However, it's also true that, when you are measuring the sum of Bernouli trials with varying probabilties, almost all small, then the best fit is a Poisson (Hodges60poissonAprxPoissonBin)},
  file        = {:Emory19poissonBinomCnnct.pdf:PDF},
  url         = {http://www.oxfordmathcenter.com/drupal7/node/297},
}

@InProceedings{Enyeart08largeWindImpact,
  author       = {Steve Enyeart},
  title        = {Large Wind Integration Impacts on Operations / System Reliability},
  booktitle    = {Workshop on Grid Connected Renewable Energy and Cogeneration/Independent Power},
  year         = {2008},
  organization = {Bonneville Power Administration (BPA)},
  comment      = {ramps are really large changes in the ERROR. Different from other ramp def'ns slide 12: Wind Ramps are large unscheduled changes in the output of a wind farm or the aggregate of all wind farms in a Control Area. (unscheduled is bolded)},
  file         = {Slides:Enyeart08largeWindImpact.pdf:PDF},
  owner        = {scot},
  timestamp    = {2011.05.02},
  url          = {http://www.google.dk/url?sa=t&source=web&cd=3&ved=0CCcQFjAC&url=http%3A%2F%2Fwww.usea.org%2FPrograms%2FAPP%2FKolkata_Workshop_4-08%2FWind_Integration_Impacts_on_BPA_Grid.pdf&ei=-be-TcXIOI2cOruOuPkF&usg=AFQjCNGY9fVZAOgO2RciimW9V4i9uZM9jw},
}

@Article{ERCIMnews16spclIssueMachLrn,
  author    = {ERCIM News},
  title     = {Special theme: Machine Learning},
  journal   = {ERCIM News},
  year      = {2016},
  number    = {107},
  month     = {Oct},
  comment   = {Reinhard emailed this around, pointing to several interesting articles.  I've highlighted a few more in the TOC that are also interesting.  Read this on the plane or back in MN during Xmas?

Good facts about how much data deep learning needs.

Random forest and deep NN were best in anomaly detection

Micro-data Learning: the Other End of the Spectrum

        "...the ImageNet data-base used in image recognition contains about 1.2 million labelled examples; DeepMinds's AlphaGo used more than 38 million positions to train their algorithm to play Go; and the same company used more than 38 days of play to train a neural network to play Atari 2600 games, such as Space Invaders or Breakout."},
  file      = {ERCIMnews16spclIssueMachLrn.pdf:ERCIMnews16spclIssueMachLrn.pdf:PDF},
  owner     = {sotterson},
  series    = {Special theme: Machine Learning},
  timestamp = {2016.12.19},
  url       = {www.ercim.eu},
}

@Article{etip-pv17asessPVfrcstObsNeed,
  author    = {etip-pv},
  title     = {Assessing the need for better forecasting and observability of PV},
  journal   = {A white paper by the European technology and innovation platform PV, Working group on grid integration},
  year      = {2017},
  comment   = {has a simple power model:

P 'proportional to' G ( 1+k logG )  where G is the global irradiance and k is an installation parameter.

* between 0 min and 2 h where there is no satisfactory physical forecasting technique for irradiance, stochastic learning techniques are used.

So, time series methods are used


Univariate:
* STL: seasonal decomposition of time series by Loess
* Holt-Winters seasonal method
* TSLM: linear model fit with time series components
* ARIMA: autoregressive integrated moving average
* BATS: exponential smoothing stateespace model with BoxeCoxtransformation, ARMA errors, Trend and  Seasonal components
* Nnetar: Feedeforward neural networks with a single hidden layer and lagged inputs for forecasting univariate time series.

Multivariate
* MLR: MultieLinear Regression Model
* SVM: Support Vector Machine
* ANN: Artificial Neural Network
* Regression Tree},
  file      = {:etip-pv17asessPVfrcstObsNeed.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.03},
}

@Article{Evans15nwpIntroClassNotes,
  author    = {Clark Evans},
  title     = {Numerical Weather Prediction: Introduction.},
  journal   = {Atm Sci 950 Class Notes, University of Wisconsin-Milwaukee},
  year      = {2015},
  comment   = {1st day writeup for an NWP class.  Has a good overview.

General NWP
* solves dynamical equations (TODO which ones?)
* UK met office
 - global
 - 17 km gird
 - 70 levels
 - to 80 knm
 - 1.24e8 gird points

Discretizing
* can be modeled as a spectrum (Fourier series) or as a grid point
* effective resolution: 6delX to 7delX (see reference)
* or want at least 6 points covering a wave cycle
* nice graph (fig 2) showing energy in atmopshere (schematically showing what is cut off)
* Fig 3: variable resolution model plot (US)
* Fig 4: terrain following coordinates
* Horizontal discretization
 - low heights: tighter, terrain following (Fig 4)
 - high heights constant height, pressure, potential temp.
* Vertical discretization
 - 30-100 levels
 - finer vertical levels when there is finer horizontal levels
 - top is highest level of interest, up to the stratopsphere

Numerical Methods
* modern NWP uses 4th to 5th order Taylor expansion for derivatives
* truncation in deriv approx causes errors
* chaotic system ==> extremely sensitive to initial conditions


Courant Number (eq. 11, p. 11)
==> coarse grid ==> coarse time sample period
* feature speed vs. spatial res. vs. time res.
* C<=1
is a limit on NWP numerical stability
C<=1 (generally, depends upon finite differencing scheme used
C = U*delT/delX; U is max translation speed of any features
 U is usually max wind speed @ tropospheric jet streak
good example on. p. 12

Initialization
1.  prev. run forecast
2.  climatology
3.  lower res. higher area model (downscaling)
4.  a mix: DART

Boundaries (lateral)
 -- global: periodic boundaries
 -- local need transition and damping buffer zone (see figure 8)

Boundaries (vertical)
* buffer layer at top
* bottom layer: try to model surace of earth
-  minimum:  topography and land use information (e.g., land versus water, grassland...
 - Heat and moisture transport between the surface and model
- friction
- parameterizations to down to centimeters
- ocean state, altough often treated as fixed throughout the meteorological simulation duration.
  (MBL modeling)

Initial Conditions
* Weahter is chaotic, so extrememe sensitive to this
* IC's generally are coarser than model's resolution (cold start) but some higher res too, I guess (warm start, hot start)
* IC's must be plausible, satisfying balance conditions
 - hydrostatic
 - geostrophic
 - in a way mitigate intertia-gravity waves
 - close to observations
 - DWD ENTKF (mention ensemble init)
* After IC, coarse models spin up in 1st 6-12 hrs of model forecast
* Observations added by an assimilation method
* Can also init from previous model run (cycling): See figure 9

Initial conditions for Limited area models
* Use some other model's init
* can be coupled to assimilatin data w/ DART

Model parameterization
* parameterize a proces when
 - scales can't be resolved on model gird (most important)
 - too CPU intensive to model (rare)
 - don't know how to model (relevant)
* things being parameterized.  Example: WRF-ARW version 3.7.1, released in August 2015
 - fourteen planetary boundary layer,
 - twenty-one cloud microphysics
 -  thirteen cumulus convection
 - eight shortwave and longwave radiation parameterizations!
* Also
 - land surface
 - ocean boundary layer
 - radiation
* Types of Parameterization
 - stochastic
 - physical},
  file      = {Evans15nwpIntroClassNotes.pdf:Evans15nwpIntroClassNotes.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.03.31},
  url       = {http://derecho.math.uwm.edu/classes/AtmSci950.html},
}

@Article{Ferrari13betaRgrsnUpdateSlides,
  author    = {Ferrari, Silvia LP},
  title     = {Beta regression modeling: recent advances in theory and applications},
  journal   = {13\textsuperscript{th} School of Regression Models, February},
  year      = {2013},
  comment   = {Slides on recent updates from the main author of beta regression papers (as far as I can tell)},
  file      = {Ferrari13betaRgrsnUpdateSlides.pdf:Ferrari13betaRgrsnUpdateSlides.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.07.18},
}

@Article{Foley12methAdvWindPowFrcst,
  author    = {Foley, Aoife M and Leahy, Paul G and Marvuglia, Antonino and McKeogh, Eamon J},
  title     = {Current methods and advances in forecasting of wind power generation},
  journal   = {Renewable Energy},
  year      = {2012},
  volume    = {37},
  number    = {1},
  pages     = {1--8},
  comment   = {Source for July 2015 ecofys grid integration Task 1 report.

Advances, challenges
* extreme (rare) situations at different temporal and spatial scales
* multimodel ensemble techniques
* use of autocorrelated (and thus not independent) successive hourly mean wind speeds, though invalidating all of the usual statistical tests, has no appreciable effect on the shape of the pdf estimated from the data
* offshore is harder than onshore b/c roughness not constant w/ time (waves) and smaller, so wind speed changes are stronger
* Poor availability of meteorological data to validate NWP outputs for these offshore locations


Statistical models of wind speed vs. wind power
* wind speed prediction models produce lower errors than models, which attempt to predict wind power outputs.},
  doi       = {10.1016/j.renene.2011.05.033},
  file      = {Foley12methAdvWindPowFrcst.pdf:Foley12methAdvWindPowFrcst.pdf:PDF},
  groups    = {Read},
  publisher = {Elsevier},
}

@Article{Fonseca15modelDepCopulaCrsNts,
  author    = {Pedro Fonseca},
  title     = {Modeling Dependencies, Lecture 8},
  journal   = {Quantitative Finance, Spring, University of Zurich},
  year      = {2015},
  comment   = {Why and how to model dependies, mainly Pearson/Spearman/Kendall and copulas.  Nice cookbooks and recipies.  Good summary of tail correlation, calibration.

Note that Student-T distribution has 2 sided tail correlation and is non-zero even when linear corr is zero.

Says correlation matrix is hard to calibrate.},
  file      = {:papers\\Fonseca15modelDepCopulaCrsNts.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.07.05},
  url       = {https://people.math.ethz.ch/~farkas/teaching/QF_FS15.html},
}

@Misc{Fox05nonParamRgrs,
  author       = {John Fox},
  title        = {Introduction to Nonparametric Regression. Lecture Notes (corrected)},
  howpublished = {web},
  year         = {2005},
  note         = {McMaster University, Canada},
  comment      = {"non-parametric" regression just means "not linear!" He also has a book: http://www.amazon.com/Introduction-Nonparametric-Regression-Probability-Statistics/dp/0471745839},
  file         = {Fox05nonParamRgrs.pdf:Fox05nonParamRgrs.pdf:PDF},
  owner        = {scot},
  timestamp    = {2010.06.03},
  url          = {http://socserv.mcmaster.ca/jfox/Courses/Oxford-2005/slides-handout.pdf},
}

@InBook{Fox08btstrpRgrsnBookChap,
  chapter   = {Chapter 21: Bootstrapping Regression Models},
  pages     = {587--606},
  title     = {Applied Regression Analysis and Generalized Linear Models, Second Edition},
  publisher = {SAGE Publications, Inc},
  year      = {2008},
  author    = {John Fox},
  comment   = {A short chapter on bootstrapped regression, which I keep hearing about but don't understand. Maybe this is related to random forests (continuous versions, if any), or continous adaboost?

In any case, it expands on the stuff covered in Geyer06subSampBootStrap

See also: Fox08glmBookChap},
  file      = {Fox08btstrpRgrsnBookChap.pdf:Fox08btstrpRgrsnBookChap.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.02.08},
  url       = {http://www.sagepub.com/books/Book226359/samples},
}

@InBook{Fox08glmBookChap,
  chapter   = {Chapter 15: Generalized Linear Models},
  pages     = {379--424},
  title     = {Applied Regression Analysis and Generalized Linear Models, Second Edition},
  publisher = {SAGE Publications, Inc},
  year      = {2008},
  author    = {John Fox},
  comment   = {Short chapter on generalized linear models.  I've marked it up but don't remember what I concluded.  See also Fox08btstrpRgrsnBookChap},
  file      = {:Fox08glmBookChap.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.02.08},
  url       = {http://www.sagepub.com/books/Book226359/samples},
}

@InCollection{Fox09BayesLearSwitchLinSys,
  author    = {Emily Fox and Erik Sudderth and Michael Jordan and Alan Willsky},
  title     = {Nonparametric {Bayes}ian Learning of Switching Linear Dynamical Systems},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2009},
  editor    = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
  volume    = {21},
  comment   = {Good for regime switching, ramp detection?},
  file      = {Fox09BayesLearSwitchLinSys.pdf:Fox09BayesLearSwitchLinSys.pdf:PDF;Fox09BayesLearSwitchLinSys.pdf:Fox09BayesLearSwitchLinSys.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.12.15},
}

@Article{Friederichs07censQRprecip,
  author    = {Friederichs, P. and Hense, A.},
  title     = {Statistical Downscaling of Extreme Precipitation Events Using Censored Quantile Regression},
  journal   = {Monthly Weather Review},
  year      = {2007},
  volume    = {135},
  number    = {6},
  pages     = {2365--2378},
  month     = jun,
  issn      = {0027-0644},
  booktitle = {Monthly Weather Review},
  comment   = {doi: 10.1175/MWR3403.1
Review:
A test metric for censored quantile regression -- a small hack to Gneiting07strictPropScore

May need to use a score something like this for left AND right censored wind power quantiles, although I'm not sure if this is better than just CRPS.},
  doi       = {10.1175/MWR3403.1},
  file      = {Friederichs07censQRprecip.pdf:Friederichs07censQRprecip.pdf:PDF},
  groups    = {Test, doReadWPV_1},
  owner     = {sotterson},
  publisher = {American Meteorological Society},
  timestamp = {2013.10.23},
}

@Unpublished{Friendly12CollinRgrssn,
  author    = {Michael Friendly},
  title     = {Collinearity in regression (the dreaded disease, and how to live with it)},
  note      = {Lecture notes, Psych 6140. York University, Toronto.},
  month     = oct,
  year      = {2012},
  comment   = {How to handle collinearity in regression, and how to visualize it. Course page covers other interesting topics from a friendlier, Psych student level: MDS, structural equation modes, Factor analysis, matrix algebra reference.

* Collinearity is a data problem
 -- Some predictors nearly linearly dependent
 --Consequences: large std. errors --> large CIs (NS)
 -- Not a problem if we are only interested in pure prediction
* Measuring & understanding collinearity:
 -- VIF: 1/(1-R2 xi|others) - -> involvement of each variable
 -- Variance proportions: how variables are involved
* Visualizing collinearity:
 -- Tableplots: what information to pay attention to
 -- Biplots: sources of collinearity among the small dimensions
* Remedies:
 -- Re-express or re-define variables often helps
 -- So too does thoughtful model selection
 -- Statistical remedies (PCA regression, ridge-regression) cure the problem, but often at a cost of more difficult interpretation.
 -- de-mean before computing interaction terms},
  file      = {Friendly12CollinRgrssn.pdf:Friendly12CollinRgrssn.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2014.07.06},
  url       = {http://www.psych.yorku.ca/lab/psy6140/lectures/Collinearity2x2.pdf},
}

@Article{Fritz17windPVfrcstUMrbrgTalk,
  author    = {Rafael Fritz},
  title     = {Windenergie und Photovoltaik: Speicher und Prognosen zur Netzintegration},
  journal   = {Lehrerfortbildung des FB Physik, Uni Marburg},
  year      = {2017},
  comment   = {I'll use a couple slides from this for the GIZ Colombia course},
  file      = {Fritz17windPVfrcstUMrbrgTalk.pptx:Fritz17windPVfrcstUMrbrgTalk.pptx:PowerPoint 2007+},
  owner     = {sotterson},
  timestamp = {2017.04.12},
}

@Misc{Fruchter02frcstNewProdBass,
  author       = {Gila E. Fruchter},
  title        = {Forcasting the Sales of New Products and the Bass Model},
  howpublished = {Course Notes: Mareting Models in Practice, Bar-Ilan University, Israel},
  year         = {2002},
  comment      = {Slides on Bass Diffusion extensions, more detailed derivations, and some sample coeffs from various products.},
  file         = {:Fruchter02frcstNewProdBass.pdf:PDF},
  url          = {https://faculty.biu.ac.il/~fruchtg/829/lec/6.pdf},
}

@Article{Gagnon16distPVadoptMktFctr,
  author       = {Gagnon, Pieter and Sigrin, Ben},
  title        = {Distributed PV Adoption - Sensitivity to Market Factors},
  year         = {2016},
  month        = {2},
  abstractnote = {NREL staff used the dSolar (distributed solar) model to forecast the adoption of distributed, behind-the-meter PV through the year 2050 for 9 different scenarios. The scenarios varied in their assumptions about a carbon tax, the cost of PV systems in the future, and what credit would be given for excess generation once current net metering policies expire.},
  file         = {:Gagnon16distPVadoptMktFctr.pdf:PDF},
  place        = {United States},
  url          = {https://www.nrel.gov/docs/fy16osti/65984.pdf},
}

@Article{Gagnon17distPVprojKentucky2040,
  author       = {Gagnon, Pieter and Das, Paritosh},
  title        = {Projections of Distributed Photovoltaic Adoption in Kentucky through 2040},
  year         = {2017},
  month        = {6},
  abstractnote = {NREL has used the dGen (Distributed Generation Market Demand Model) to project the adoption of distributed Photovoltaics in Kentucky through 2040. This analysis was conducted by the STAT Network at the request of the Kentucky Energy Office.},
  doi          = {10.2172/1365709},
  file         = {:Gagnon17distPVprojKentucky2040.pdf:PDF},
  place        = {United States},
  url          = {https://www.osti.gov/biblio/1365709},
}

@Article{Galanis06windKalmanNonLin,
  author    = {G. Galanis and P. Louka and P. Katsafados and Pytharoulis and G. Kallos},
  title     = {Applications of {Kalman} filters based on non-linear functions to numerical weather predictions},
  year      = {2006},
  volume    = {24},
  pages     = {24512460},
  file      = {Galanis06windKalmanNonLin.pdf:Galanis06windKalmanNonLin.pdf:PDF;Galanis06windKalmanNonLin.pdf:Galanis06windKalmanNonLin.pdf:PDF},
  journal   = {Annals of Geophysics},
  owner     = {scotto},
  timestamp = {2008.07.06},
}

@Misc{Gale08cholesky,
  author       = {Thomson Gale},
  title        = {Cholesky Decomposition},
  howpublished = {International Encyclopedia of the Social Sciences},
  year         = {2008},
  comment      = {Cholesky decomp. used in economics and social sciences.
Uses
* Can use to solve linear equations
* variance matrix decomposition good for vector autoregression
* can simulate the  response to one variable
* CAN ATTRIBUTE FORECAST ERRORS to disturbance in variables!

Properties and Problems
* Cholesky is numerically stable and accurate
* fewer floating point ops and less memory than alternatives
* However square root inside is problematic for collinear, ill-conditioned problems
* So can Cholesky be replaced with the LDL a.k.a LDLT in Wikipedia15cholesky, which avoids the square root?},
  file         = {Gale08cholesky.pdf:Gale08cholesky.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2015.12.22},
  url          = {http://www.encyclopedia.com/topic/Cholesky_Decomposition.aspx},
}

@Article{Gallegoa11locDirVarCoeff,
  author    = {C. Gallegoa and P. Pinson and H. Madsen and A. Costaa and A. Cuerva},
  title     = {Modelling the influence of local wind direction and local wind speed by means of varying-coefficient models for offshore wind power, very short-term forecasting},
  journal   = {Energy (submitted preprint to)},
  year      = {2011},
  comment   = {Circular (angular) harmonics integrated into DTU's CPVAR Pierre recommended, said it was spherical but it's not; only circular},
  file      = {Gallegoa11locDirVarCoeff.pdf:Gallegoa11locDirVarCoeff.pdf:PDF},
  owner     = {scot},
  timestamp = {2011.01.19},
}

@Article{Gardner06Oneclassnovelty,
  author    = {Gardner, Andrew B and Krieger, Abba M and Vachtsevanos, George and Litt, Brian},
  title     = {One-class novelty detection for seizure analysis from intracranial EEG},
  journal   = {Journal of Machine Learning Research},
  year      = {2006},
  volume    = {7},
  number    = {Jun},
  pages     = {1025--1044},
  owner     = {sotterson},
  timestamp = {2016.12.19},
}

@Manual{Gasparrini10dlnmManual,
  title     = {dlnm: Distributed Lag Non-linear Models.},
  author    = {Gasparrini, A and Armstrong, B},
  year      = {2010},
  note      = {R package version 1.2.4},
  comment   = {R library Paper on method: Gasparrini10distLagNonLin Simpler 2D approach is Muggeo07bivarDistLagSpline, which only captures products},
  owner     = {scot},
  timestamp = {2010.08.05},
}

@Article{Gav08concetrateImpSolTherm,
  author    = {Big Gav},
  title     = {Concentrating On The Important Things - Solar Thermal Power},
  journal   = {The Oil Drum: Australia and New Zealand},
  year      = {2008},
  comment   = {Nice graph of solar insolation vs. time of year and latitude.  Always pretty big at the equator.  This came from google image search, but I couldn't actually find this image on the page Google says it came from.

Using for Colombia GIZ course.},
  file      = {Gav08concetrateImpSolTherm_insVsSeasonLat.gif:Gav08concetrateImpSolTherm_insVsSeasonLat.gif:GIF image},
  owner     = {sotterson},
  timestamp = {2017.04.29},
  url       = {https://www.google.com/search?q=solar+irradiance+vs.+sandstorms+dust+solar+power&source=lnms&tbm=isch&sa=X&ved=0ahUKEwi0u9b_isrTAhWBPywKHcjUD30Q_AUIBigB&biw=1920&bih=988#imgrc=K0yUWyFSuSkmbM:},
}

@TechReport{Gayo11reliaWindFinalRprt,
  author      = {Juan Bueno Gayo},
  title       = {Final Report - RELIAWIND (Reliability focused research on optimizing Wind Energy systems design, operation and maintenance: Tools, proof of concepts, guidelines \& methodologies for a new generation)},
  institution = {European Union 7th Framework Programme},
  year        = {2011},
  number      = {212966},
  month       = mar,
  file        = {:Gayo11reliaWindFinalRprt.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2017.06.19},
}

@Book{Geron17HandsOnMachineLearningWithScikitLearnAndTensorFlowConceptsToolsAndTechniquesToBuildIntelligentSystems,
  title     = {Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems},
  publisher = {" O'Reilly Media, Inc."},
  year      = {2017},
  author    = {G{\'e}ron, Aur{\'e}lien},
  comment   = {Stephan Vogt recommended this book. I bought the Kindle version.},
  groups    = {Scott Otterson:6},
  url       = {https://www.amazon.de/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291},
}

@Book{Getoor07Introductionstatisticalrelational,
  title     = {Introduction to statistical relational learning},
  publisher = {MIT press},
  year      = {2007},
  author    = {Getoor, Lise},
  owner     = {sotterson},
  timestamp = {2016.12.24},
}

@Article{Gianotti13climateMdlToughBlog,
  author    = {Rebecca Gianotti},
  title     = {Wading into the Fire, Episode 6: Climate modelling is a tough gig},
  journal   = {Blog: science and the world: Storytelling that makes Science Personal},
  year      = {2013},
  comment   = {Nice graph of weather/climate time/space scales.  I used for GIZ course},
  file      = {Gianotti13climateMdlToughBlog.pdf:Gianotti13climateMdlToughBlog.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.25},
  url       = {https://scienceandtheworld.wordpress.com/2013/04/29/wading-into-the-fire-episode-6-climate-modelling-is-a-tough-gig/},
}

@TechReport{Giebel03shortWindStateOfArtOverview,
  author      = {Gregor Giebel},
  title       = {The State-Of-The-Art in Short-Term Prediction of Wind Power. A Literature Overview},
  institution = {Ris{\o} National Laboratory},
  year        = {2003},
  type        = {Deliverable report D1.1},
  comment     = {Has old stuff like autoregressive},
  file        = {Giebel03shortWindStateOfArtOverview.pdf:Giebel03shortWindStateOfArtOverview.pdf:PDF;Giebel03shortWindStateOfArtOverview.pdf:Giebel03shortWindStateOfArtOverview.pdf:PDF},
  groups      = {DOE-PNL09},
  owner       = {scotto},
  timestamp   = {2008.07.04},
}

@TechReport{Giebels03predictorFactSheetWind,
  author      = {Giebel, G.},
  title       = {Short-Term Forecasting Fact Sheet for Prediktor},
  institution = {Zephyr collaboration},
  year        = {2003},
  comment     = {- Zephyr Predictor is better than persistence after 4 hours - The benefits of a Kalman filter as MOS for Prediktor has been researched by Giebel. Since generally the error in the NWP varies randomly and not so much in a correlated fashion, the use for an adaptive system such as the Kalman filter is limited. While it is possible to tune the filter to yield better results than a static MOS system, the improvements are pronounced in only few cases. Part of this is the need for the filter to be relatively ?stiff? (non-varying between two data points) to discourage runaway behaviour. - was this a linear Kalman? - better iterative algorithms available?},
  file        = {:Giebels03predictorFactSheetWind.pdf:PDF;Giebels03predictorFactSheetWind.pdf:Giebels03predictorFactSheetWind.pdf:PDF},
  owner       = {scotto},
  timestamp   = {2008.07.06},
  url         = {http://130.226.56.153/zephyr/publ/Zephyr_Prediktor_FactSheet.pdf},
}

@Misc{Gilleland13verifRpkg,
  author       = {Eric Gilleland},
  title        = {R Package 'verification', NCAR Research Application},
  howpublished = {Software library},
  month        = aug,
  year         = {2013},
  comment      = {R package that I could use. Read later, when I'm ready to do this.

Not sure if this can work on densities. It's focused on scoring for binary events or on ensembles for continuous forecasts. A quick inspection did not reveal a way to plug in quantiles, etc. There is something that bins a continuous forecast but then the measurement seems to be categorical, if not binary.

vignettes here:
http://rss.acs.unt.edu/Rdoc/library/verification/html/00Index.html

Is mentioned here:
http://www.cawcr.gov.au/projects/verification/\#Methods_for_foreasts_of_continuous_variables},
  file         = {Weather Examples (2012):Gilleland13verifRpkg_WeathEx.pdf:PDF},
  groups       = {PointDerived, Test, CitaviImport1, doReadWPV_2},
  keywords     = {verification},
  owner        = {sotterson},
  timestamp    = {2013.09.26},
  url          = {http://cran.r-project.org/web/packages/verification/index.html},
}

@Article{Giorgino09dtwRpkg,
  author    = {Toni Giorgino},
  title     = {Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package},
  journal   = {Journal of Statistical Software},
  year      = {2009},
  volume    = {31},
  number    = {7},
  pages     = {1--24},
  month     = aug,
  issn      = {1548-7660},
  accepted  = {2009-06-26},
  bibdate   = {2009-06-26},
  coden     = {JSSOBK},
  comment   = {Citation for dtw R package - is Keogh01derivDTW really what's wanted for forecast phase errors?},
  day       = {14},
  file      = {Giorgino09dtwRpkg.pdf:Giorgino09dtwRpkg.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  submitted = {2008-09-30},
  timestamp = {2010.07.20},
  url       = {http://www.jstatsoft.org/v31/i07},
}

@Article{Gneiting03windEnsemble,
  author    = {Gneiting, Tilmann and Raftery, A E},
  title     = {Weather forecasting and ensemble methods},
  journal   = {Science},
  year      = {2003},
  volume    = {310},
  pages     = {248--49},
  comment   = {Literature survey: using multiple forecasts provide better mean forecasts and useful probabilisitic outputs
* "Ensemble forecast": multiple ( 5-100) runs of NWP w/ diff. initial conditions and/or the numerical atmosphere model * statistical post processing is required
* also useful for assimilating observations into the model
* Mean ensemble forecast outperforms individual forecasts in ensemble (like classifier combination e.g. ROVER)

Three ways of generating initial conditions (10 day ahead)
1.) seek directions of rapid error growth via selective sampling (bred-vector perturbation (used by NCEP)
2.) selective sampling by a singular vector technique (used by ECMWF)
3.) perturbed observations w/ Mone Carllo (used by MSC)

* ECMWF seems best overall; others strong in lead or tail of 10 day forecast (some big test)
* ECMWF also randomizes parameters

Questions. It's not clear if:
* randomizing parameters is really better than statistical post processing
* selective is better than Monte Carlo
* Suggests that maximizing sharpness of forecast output distribution via calibration may help (Gneiting has a paper on this)

This might be too much into the guts of NWP to be useful for Eweline. But maybe OK for 1K ensemble?},
  file      = {Gneiting03windEnsemble.pdf:Gneiting03windEnsemble.pdf:PDF;Gneiting03windEnsemble.pdf:Gneiting03windEnsemble.pdf:PDF},
  groups    = {doReadNonWPV_2},
  owner     = {scotto},
  timestamp = {2008.12.27},
}

@Article{Good16CloudTrackDefPVfrcst,
  author    = {Garret Good},
  title     = {Cloud-Tracking with Deformation},
  journal   = {Fraunhofer IWES. Project Eweline Presentation},
  year      = {2016},
  comment   = {Cloud tracking for short term PV forecasting.  I believe Garret presented this at an Eweline Talk in 2016.  But I got the slides from Jan?s Group intro talk: JanGrpISE_Gruppenvorstellung_at_ISE.pptx},
  file      = {Good16CloudTrackDefPVfrcst.pptx:Good16CloudTrackDefPVfrcst.pptx:PowerPoint 2007+},
  owner     = {sotterson},
  timestamp = {2017.04.12},
}

@Book{Goodfellow16DeepLrnBook,
  title     = {Deep learning},
  publisher = {MIT press},
  year      = {2016},
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  comment   = {A highly recommended deep learning book.},
  file      = {:Goodfellow16DeepLrnBook.pdf:PDF;:Goodfellow16DeepLrnBook_Slides.pdf:PDF},
}

@InProceedings{Graler12timeSpaceVnCopula,
  author    = {Gr{\"a}ler, Benedikt and Pebesma, Edzer},
  title     = {Modelling Dependence in Space and Time with Vine Copulas},
  booktitle = {Ninth International Geostatistics Congress},
  year      = {2012},
  month     = jun,
  publisher = {Geostats},
  comment   = {Time/space dependencies implemented with vine-coupulae in R spcopula package. Nice presentation slides too. Good for probabilistic forecast aggregation (upscaling), SDE's (which want certain distributions), etc.

R package: spcopula
http://r-forge.r-project.org/projects/spcopula/},
  file      = {paper:Graler12timeSpaceVnCopula.pdf:PDF;Slides:Graler12timeSpaceVnCopula_Slides.pdf:PDF},
  location  = {Oslo, Norway},
  owner     = {sotterson},
  timestamp = {2014.02.07},
  url       = {http://ifgi.uni-muenster.de/~b_grae02/publications/Geostats_2012_Oslo_Modelling_Dependence_in_Space_and_Time.pdf},
}

@TechReport{Grell95descMesoMM5,
  author      = {Georg A. Grell and Jimy Dudhia and David R. Stauffer},
  title       = {A Description of the Fifth-Generation Penn State/NCAR Mesoscale Model (MM5)},
  institution = {Mesoscale And Microscale Meteorology Division, National Center For Atmospheric Research (NCAR)},
  year        = {1995},
  number      = {NCAR/TN-398 + STR},
  month       = jun,
  groups      = {DOE-PNL09},
  owner       = {sotterson},
  timestamp   = {2009.03.03},
}

@Article{Guan17intradayMktDE,
  author    = {Wenlin Guan},
  title     = {Analyse historischer Daten des Intradaymarktes f?r Strom der EPEX Spot},
  journal   = {Master's Thesis Talk},
  year      = {2017},
  comment   = {Seems to be something like an analog forecasast of intraday market prices on the German EPEX spot market.  This is the Master's Thesis tald (I think) of Henry Martin's officemate's boyfriend.  },
  file      = {:papers\\Guan17intradayMktDE.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.08.22},
}

@Article{Guo13GaussGraphStructLrnSlides,
  author    = {Jiang Guo},
  title     = {Gaussian Graphical Models: Structure Estimation},
  journal   = {Presentation given at Princeton University},
  year      = {2013},
  month     = jan,
  comment   = {Nice overview of Gaussian covariance/precision matrix learning.

Interesting bits
* Any covariance matrix can be converted to a Gaussian Markov Random Field
* this graph can be learned with fewer training points than dimensions
* A greedy learning algorith (TIGER) seems best in 2013, in terms of correctly learning the graph
   (seems important for ModernWindABS b/c graph may be used for diagnosis)},
  file      = {Guo13GaussGraphStructLrnSlides.pdf:Guo13GaussGraphStructLrnSlides.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.15},
  url       = {http://ir.hit.edu.cn/~jguo/},
}

@InBook{Guptay07symbDynFiltPatternRecog,
  chapter   = {2},
  pages     = {17--71},
  title     = {Pattern Recognition: Theory and Application},
  publisher = {Nova Science Publishers, Inc.},
  year      = {2007},
  author    = {Shalabh Guptay and Asok Ray},
  editor    = {Erwin A. Zoeller},
  comment   = {Unsupervised time-series pattern partitioning using SDF. Information theoretic, Markov, Wavelet based. * SDF: symbolic dynamic filtering * use for detecting ramps across a farm? * information-theoretic},
  file      = {:Guptay07symbDynFiltPatternRecog.pdf:PDF;Guptay07symbDynFiltPatternRecog.pdf:Guptay07symbDynFiltPatternRecog.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.08.21},
  url       = {http://www.mne.psu.edu/Ray/journalAsokRay/bookChapters/012GuptaRayBookChapter.pdf},
}

@Unpublished{GutierrezOsuna11kdnnLecNotes,
  author    = {Ricardo Gutierrez-Osuna},
  title     = {Kernel density estimation and Nearest Neighbors: Lecture Notes 7 and 8},
  note      = {CSCE 666 Pattern Analysis, Perception and Instrumentation Lab, Texas A \& M University},
  year      = {2011},
  comment   = {Great course notes w/ quick fundamentals. Seems like KDE better for densities than KNN (KNN noisy, affected by curse of dim) and KNN good for classification. Perhaps KNN not a good choice for local linear neighbhorhoods?

KERNEL DENSITY ESTIMATION (L7)

Histograms are bad
* arbitrary bin edges and n-dim orietation affect density estimate
* dicontinuities at bin edges
* vulnerable to curse of dimensionality

General non-parametric density estimation
* estimate density at x by num. points that fall within volume, centered at x
* volume must be big enough to capture points
* vol must be small enough so that constant density assumption is fairly true (otherwise smear density)

Parzen Window
* vol (V) is a hypercube (so train point kernel weight is 1 (inside) or 0 (outside)
* it's a bit like a histogram but bin edge locations come from data

 Bias-variance tradeoff determined by Parzen hypercube edge, h, or some width param in other kernels
* see that the estimated density is a CONVOLUTION of true density with the hypercube (or other kernel)
* bandwidth choice methods
 -- assume a standard true distribution e.g. Gaussian ==> closed form rule
 -- interquartile spread is more robust
 -- max likelihood cross validation (assuming Gausian kernel? not clear)
 -- also see Hansen09nonParamE718notes

Multivariate density estimation
* different variance in each dim means bandwidths must be different, OR...
 -- use a full-covariance matrix
 -- unit variance normalize each dim individually
 -- pre-whiten (orthogonalize and make identity cov matrix before KDE)
* Product kernels
 -- a product of 1D kernels
 -- allows univariate bandwidth selection
 -- NOT THE SAME AS ASSUMING INDEPENDENT FEATURES! (summation order is different)

Naive Bayes classifier
* like a product kernel, but multiply across Bayes Rule flipped likilihoods for each dimension
* handles the curse of dim
* can work as well and NN's or decition trees in high dims, sometimes

NEAREST NEIGHBORS (L8)

KDE vs. KNN density estimate
* fix vol (V) and determine num. points (k) from the data
* fix k and calculate V
* both converge as npts --> infinity,with the right k/V choices

KNN volume
* is hypersphere with radius at distance to kth farthest point
* a single point means that it's very noise sensitive

KNN is not a good density estimator
* local noise sensitive (so bad for local linear regression neighborhoods?)
* heavy tails
* discontinuities
* not a true probability, since it diverges
* Tran03knnDensClust and Tran06knnDensClustHiDim agree, but say KNN is still good for clustering

KNN classifier characteristics
* main use of KNN is as a classifier (simple, nearly optimal approx of optimal Bayes)
* highly succeptible to curse of dim (again, so not good for local linear regression?)
* sensitivity to nosy features is KNN achilles' heel!

Feature weighting is one solution ot noise sensitivity
* not to be confused with distance-weighting (NOT RECOMMENDED)
 -- p-Gaussian and many other papers say distance weighting is good? e.g. Verleysen05cursDimMinTser
* feature weighting methods
 -- performance bias: iteratively weight by performance
 -- preset bias: set by MI or correlation w/ target
 -- wrapper feature subset selection

Lazy/Eager learing
* Lazy: data processing all done at query time
* Eager: compress training at train time, less computation at test time

Fast KNN search
* Bucketing
* k-d trees},
  file      = {GutierrezOsuna11kdnnLecNotes.pdf:GutierrezOsuna11kdnnLecNotes.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2014.04.22},
  url       = {http://psi.cse.tamu.edu/teaching/lecture_notes/},
}

@Electronic{Hagedorn0911envVer1,
  author       = {Hagedorn, Renate},
  year         = {2009},
  title        = {Ensemble Verification I},
  howpublished = {Course notes, 2009, 2011},
  url          = {http://old.ecmwf.int/newsevents/training/meteorological_presentations/MET_PR.html},
  booktitle    = {ECMWF Training Course},
  comment      = {Friendly Overview also w/ newer powerpoint by Pena based on this (I think).},
  file         = {2009 Powerpoint:Hagedorn09envVer1.pptx:PowerPoint 2007+;2011 pdf (new slides):Pena11ensVer.pdf:PDF},
  groups       = {Test, CitaviImport1, doReadWPV_1},
  owner        = {sotterson},
  timestamp    = {2013.09.26},
}

@Misc{Hagemann12norsewindDataDescProc,
  author    = {Saskia Hagemann and Melih Kurt},
  title     = {Description of Data and Processing: {NOR}SEWInD Deliverable 1.18},
  year      = {2012},
  comment   = {Had some description of the processing of the Norsewind mast data. Maybe other stuff too.},
  owner     = {sotterson},
  timestamp = {2012.09.24},
}

@Article{Haibach17predMaintWndSemiAnomDet,
  author    = {Marco Haibach and Nils Treiber},
  title     = {Predictive maintenance for offshore wind parks. Error detection by means of Semi-Supervised Anomaly Detection},
  journal   = {?},
  year      = {2017},
  file      = {:Haibach17predMaintWndSemiAnomDet_slides.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.11},
}

@Article{Haldrup04regimeSwMemElecPrice,
  author      = {Niels Haldrup and Morten O. Nielsen},
  title       = {A Regime Switching Long Memory Model for Electricity Prices},
  journal     = {Economics Working Papers},
  year        = {2004},
  number      = {2004-2},
  month       = apr,
  comment     = {Note: this was publushed in journal of econometrics, 2006},
  file        = {Haldrup04regimeSwMemElecPrice.pdf:Haldrup04regimeSwMemElecPrice.pdf:PDF;Haldrup04regimeSwMemElecPrice.pdf:Haldrup04regimeSwMemElecPrice.pdf:PDF},
  institution = {School of Economics and Management, University of Aarhus},
  owner       = {sotterson},
  timestamp   = {2008.12.12},
  url         = {http://ideas.repec.org/p/aah/aarhec/2004-2.html},
}

@InCollection{Hamill01verVisEnsFrcst,
  author    = {Tom Hamill},
  title     = {Verification and visualization of ensemble forecasts},
  booktitle = {Mediterranean School of Mesoscale Meteorology},
  publisher = {NOAA},
  year      = {2001},
  month     = mar,
  note      = {Lecture Notes},
  comment   = {Nice slides (pdf and ppt) of ways to verify and visualize prob. forecasts -- including multi-dimensional!

Implementing this would be an easy way to satisfy Eweline plot rqts.

Also see: Hamill01rankHistoVerif},
  file      = {Slides PDF:Hamill01verVisEnsFrcst.pdf:PDF;Slides PPT:Hamill01verVisEnsFrcst.ppt:PowerPoint},
  groups    = {Test, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.01},
  url       = {http://www.esrl.noaa.gov/psd/people/tom.hamill/MSMM_hamill.html},
}

@InProceedings{Hamill05calCombo,
  author    = {Hamill, Tom},
  title     = {Forecast Calibration and Combination},
  booktitle = {ECMWF Workshop},
  year      = {2005},
  comment   = {Friendly intro slides, possibly useful for upscaling. Also has hints about calibration training data size, how to use forecast analogs, etc.

Read when have ensemble data ready to train.

Also see: Hamill01rankHistoVerif},
  file      = {Hamill05calCombo.pdf:Hamill05calCombo.pdf:PDF},
  groups    = {Ensemble, Test, CitaviImport1, doReadNonWPV_1},
  ncite     = {0},
  owner     = {sotterson},
  timestamp = {2013.09.26},
  url       = {http://www.ecmwf.int/newsevents/meetings/workshops/2005/TIGGE/Hamill.pdf},
}

@InCollection{Hamill08ensCalReFrcst,
  author    = {Tom Hamill},
  title     = {Ensemble forecast calibration \& using reforecasts},
  booktitle = {Bologna ARPA-SIM},
  publisher = {NOAA},
  year      = {2008},
  note      = {Presentation Slides},
  comment   = {Nice calibration overview, with lots of regression times, discussion of how much data is needed, and how to get it with reforcasting.

Nice pdf and ppt (get after US gov't shutdown is over!)

also see: Hamill01rankHistoVerif},
  file      = {Hamill08ensCalReFrcst.pdf:Hamill08ensCalReFrcst.pdf:PDF},
  groups    = {Ensemble, Test, doReadWPV_2},
  location  = {Bolognia, Italy},
  owner     = {sotterson},
  timestamp = {2013.10.01},
  url       = {http://www.esrl.noaa.gov/psd/people/tom.hamill/MSMM_hamill.html},
}

@Unpublished{Hamill13ensVerUse,
  author    = {Tom Hamill},
  title     = {Ensemble weather prediction: verification, numerical, issues, and use},
  note      = {Presentation Slides, NOAA Earth System Research Lab},
  comment   = {Friendly intro to ensemble prediction, assimilation, scoring and use
* has CRPS and CRPSS
* talks about economic scores
* how assimilation works
I don't really know the year this was published},
  file      = {Hamill13ensVerUse.ppt:Hamill13ensVerUse.ppt:PowerPoint},
  groups    = {Ensemble, Test, Use, doReadWPV_1},
  owner     = {sotterson},
  timestamp = {2013.11.07},
  url       = {http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=0CC0QFjAA&url=http%3A%2F%2Fwww.esrl.noaa.gov%2Fgsd%2FProbFcst%2FMeeting_notes%2Fpresentations%2FHamill%25201%2520-%2520ensemble%2520overview.ppt&ei=YXF7UsqSApCVswa3noDYCA&usg=AFQjCNFWvJlDHHI2fApUYbW65LLN3s7fiQ&cad=rja},
}

@InProceedings{Han-LimChoi08infoObsTarg,
  author    = {Han-Lim Choi, MIT},
  title     = {Algorithm and Sensitivity Analysis of Information-Theoretic Ensemble-Based Observation Targeting},
  booktitle = {Conference on Probability and Statistics},
  year      = {2008},
  comment   = {Use for targeted sampling? Uses ensembles too? Uses some kinda mutual information, ensemble Kalman filter too? His PhD thesis is here: http://web.mit.edu/hanlimc/www/hl.docs/Thesis_hanlimc.pdf},
  file      = {Han-LimChoi08infoObsTarg.pdf:Han-LimChoi08infoObsTarg.pdf:PDF},
  owner     = {scotto},
  timestamp = {2011.05.15},
}

@Misc{Hansen08RegularizationToolsManualV4,
  author  = {Per Christian Hansen},
  title   = {Regularization Tools Manual (Version 4.1)},
  month   = mar,
  year    = {2008},
  comment = {Matlab toolbox of many regularization techniques, including Tikhonov so ridge regression and penalized regression w/ gamma ~= alpha*IdentyMatrix

* for generalized tikh reg., x0 can be zero (so I don't need to know the problem that solves!)
* can transform general penalty matrix and solve w/ normal SVD methods
   ==> would know degrees of freedom, as in: Cardinal15ridgeLambdaGrid
   - but they don't do that: prefer GSVD approach b/c of Matlab's "coarse granularity" whatever that means.

* Argument for eigenvalues v.s. low pass / high pass filring of forward and inverse problems (claimed in: Wikipedia15tikhreg).  P. 14
* how to transform general Tikhonov to standard for originall from [22] (Elden77AlgRegIllCond)},
  file    = {Hansen08RegularizationToolsManualV4.pdf:Hansen08RegularizationToolsManualV4.pdf:PDF},
  url     = {http://www.imm.dtu.dk/~pcha/Regutools/},
}

@Misc{Hansen09nonParamE718notes,
  author       = {Bruce Hansen},
  title        = {Lecture Notes on Nonparametrics},
  howpublished = {Econ 718, University of Wisconsin},
  year         = {2009},
  comment      = {Handy kernel reference: Many kernel function definitions, bandwidth rules of thumb, derivatives, convergence rates, density estimation, density derivative estimation (for transforms!). Curse of dimensionality..

Also see: GutierrezOsuna11kdnnLecNotes
Econ 718 web page: http://www.ssc.wisc.edu/~bhansen/718/},
  file         = {Hansen09nonParamE718notes.pdf:Hansen09nonParamE718notes.pdf:PDF},
  groups       = {Read},
  owner        = {sotterson},
  timestamp    = {2014.04.20},
  url          = {http://www.ssc.wisc.edu/~bhansen/718/NonParametrics1.pdf},
}

@Article{Hastie93LocalrgrsnKernCarp,
  author    = {Hastie, Trevor and Loader, Clive},
  title     = {Local regression: Automatic kernel carpentry},
  journal   = {Statistical Science},
  year      = {1993},
  pages     = {120--129},
  comment   = {A classic local regressions paper, with kernel advice. Maybe worth reading for the fundamentals?

Related to local linear quantile regression.},
  file      = {Hastie93LocalrgrsnKernCarp.pdf:Hastie93LocalrgrsnKernCarp.pdf:PDF},
  owner     = {sotterson},
  publisher = {JSTOR},
  timestamp = {2014.04.04},
  url       = {http://www.jstor.org/stable/2246148},
}

@InCollection{Helseth13stochOptHydro,
  author    = {Arild Helseth},
  title     = {Designing a Stochastic Optimization Model for Long-Term Hydro-Thermal Scheduling},
  booktitle = {International Conference on Stochastic Programming},
  year      = {2013},
  comment   = {Stochastic optimization and scenario fans have been applied to the full Nordic system, but with aggregate area representation

Also 1000's of individual reservoirs, although I don't understand what "aggregate area" that means for whether or not they were summed up.},
  file      = {Helseth13stochOptHydro.pdf:Helseth13stochOptHydro.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.02},
  url       = {http://dinamico2.unibg.it/icsp2013/},
}

@InProceedings{Hengst02Discoveringhierarchyreinforcement,
  author    = {Hengst, Bernhard},
  title     = {Discovering hierarchy in reinforcement learning with HEXQ},
  booktitle = {ICML},
  year      = {2002},
  volume    = {2},
  pages     = {243--250},
  owner     = {sotterson},
  timestamp = {2016.12.24},
}

@Article{Heppelmann16ewelineIFPpres,
  author    = {T. Heppelmann},
  title     = {Presentation at EWeLiNE IFP},
  journal   = {Project Eweline},
  year      = {2016},
  comment   = {Colombia GIZ ref.  Solar},
  owner     = {sotterson},
  timestamp = {2017.04.27},
}

@InProceedings{Hering07modelShortTermWindPred,
  author    = {Amanda S. Hering},
  title     = {Models for Short-Term Wind Speed Prediction},
  booktitle = {Joint Statistical Meetings},
  year      = {2007},
  month     = jul,
  comment   = {Kristin says this is relevant to de Luna and Genton variable picking method},
  file      = {Hering07modelShortTermWindPred.pdf:Hering07modelShortTermWindPred.pdf:PDF;Hering07modelShortTermWindPred.pdf:Hering07modelShortTermWindPred.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.09.30},
}

@Inproceedings{Hibell04segNNadaboost,
  Author                   = {Hibell, Lewis Eric and Wang, Xunxian and Brown, David J},
  Title                    = {Segmented Neural Network Modelling Using AdaBoost},
  Booktitle                = {IASTED International Conference on Neural Networks and Computational Intelligence},
  Year                     = {2004},
  Owner                    = {sotterson},
  Timestamp                = {2014.01.21},
  URL                      = {http://www.actapress.com/PaperInfo.aspx?PaperID=16022&reason=500}
}

@Article{Hodge16IntroWndSolrFrcstVal,
  author    = {Bri-Mathias Hodge},
  title     = {An Introduction to Wind and Solar Power Forecasting},
  journal   = {Clean Energy Solutions Center: Greening the Grid.},
  year      = {2016},
  comment   = {Developing country solar forecasting training slides from NREL.  Wind, Solar, value of forecasts.},
  file      = {Hodge16IntroWndSolrFrcstVal.pdf:Hodge16IntroWndSolrFrcstVal.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.13},
  url       = {https://cleanenergysolutions.org/sites/default/files/documents/greening-the-grid-webinar-intro-to-re-forecasting-21april2016.pdf},
}

@Article{Hodges60poissonAprxPoissonBin,
  author    = {Hodges, Joseph L and Le Cam, Lucien},
  title     = {The Poisson approximation to the Poisson binomial distribution},
  journal   = {The Annals of Mathematical Statistics},
  year      = {1960},
  volume    = {31},
  number    = {3},
  pages     = {737--740},
  comment   = {The sum of Bernoulli (1,0) variables with different probablitiies has, if they are independent, an approximate Poisson distribution with mean=lambda=SUM(p_i), where p_i is each variable's probability of success, and lambda is the mean of the  Poisson distribution. (even without this paper, you can also calculate the expected value simply from the sum of independent variables.  See my OneNote page (Expected number of adoptions across population per year). 

A coarse bound for this to be true: the probabilities are all mall, and n (# samps) is large.  Bothe are true for WattPlanGrid adoption probs.

But there are much tighter bounds.  The approximate bound they finally arrive at is that the Poisson is valid, even if some of the p_i's are large, as long as their sum is small compared to SUM(p_i)

Note that the Poisson is also an approximation to the Binomial distribution for large n and small p (Emory19poissonBinomCnnct)},
  file      = {:Hodges60poissonAprxPoissonBin.pdf:PDF},
  publisher = {JSTOR},
  url       = {https://www.jstor.org/stable/pdf/2237582.pdf?seq=1#page_scan_tab_contents},
}

@Article{Hoffmann07KernelPCAnovelty,
  author    = {Hoffmann, Heiko},
  title     = {Kernel PCA for novelty detection},
  journal   = {Pattern Recognition},
  year      = {2007},
  volume    = {40},
  number    = {3},
  pages     = {863--874},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2016.12.19},
}

@Article{Hollenbach14missDatImpCopula,
  author    = {Hollenbach, Florian M and Metternich, Nils W and Minhas, Shahryar and Ward, Michael D},
  title     = {Fast \& Easy Imputation of Missing Social Science Data},
  journal   = {arXiv preprint arXiv:1411.0647},
  year      = {2014},
  comment   = {Says copula missing data imputation is faster and usually better than Amelia II (except sometimes, when Amelia II is better for binary inputs).  Both are better than the other gold standard imputation algorithm.

Includes a way to consider spatio-temporal dependence between variables.

Mixed binary inputs: I'm not sure what Amelia does for discrete inputs.  Does it do a conditional Gaussian, as mentioned in Horton07adoMissDatComp?

Uses: Hardin13genRealisticCorrMat

See also: Ding16gaussCopulaMissDatEM},
  file      = {Hollenbach14missDatImpCopula.pdf:Hollenbach14missDatImpCopula.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.03.16},
  url       = {https://arxiv.org/abs/1411.0647},
}

@Conference{Holttinen14frcstErrImpctsVTT,
  author    = {Hannele Holttinen and Jari Miettinen and Juha Kiviluoma},
  title     = {Forecast error impact on imbalance cost and balancing market in {Finland}},
  booktitle = {EERA JP Wind Forecasting Workshop},
  year      = {2014},
  month     = jan,
  comment   = {VTT in Finland is demo'ing a kind-of probabilstic forecast (confidence bars) in a control room. Also, they show fewer forecast errors when aggregating spatially separate wind farms.},
  file      = {Holttinen14frcstErrImpctsVTT.pptx:Holttinen14frcstErrImpctsVTT.pptx:PowerPoint 2007+},
  groups    = {PointDerived, doReadWPV_1},
  location  = {Roskilde, Denmark},
  owner     = {sotterson},
  timestamp = {2014.01.29},
}

@Article{Hu16solPnlClimImpct,
  author    = {Hu, Aixue and Levis, Samuel and Meehl, Gerald A. and Han, Weiqing and Washington, Warren M. and Oleson, Keith W. and van Ruijven, Bas J. and He, Mingqiong and Strand, Warren G.},
  title     = {Impact of solar panels on global climate},
  journal   = {Nature Clim. Change},
  year      = {2016},
  volume    = {6},
  number    = {3},
  pages     = {290--294},
  month     = mar,
  issn      = {1758-678X},
  comment   = {PV panels cause a bit of global warming.

For wind  power see (at least):

Evernote: http://www.evernote.com/l/AA0_6pgHVlBAbqBHsAPW3QlihjoZRiJrbpE/},
  file      = {Hu16solPnlClimImpct.pdf:Hu16solPnlClimImpct.pdf:PDF},
  owner     = {sotterson},
  publisher = {Nature Publishing Group},
  timestamp = {2017.04.26},
  url       = {http://dx.doi.org/10.1038/nclimate2843},
}

@InProceedings{Hu98Multiagentreinforcementlearning,
  author       = {Hu, Junling and Wellman, Michael P and others},
  title        = {Multiagent reinforcement learning: theoretical framework and an algorithm.},
  booktitle    = {ICML},
  year         = {1998},
  volume       = {98},
  pages        = {242--250},
  organization = {Citeseer},
  owner        = {sotterson},
  timestamp    = {2016.12.24},
}

@Misc{Hurvich09trendForecast,
  author       = {Clifford M. Hurvich},
  title        = {Course Handouts, B90.2302, C22.0018: FORECASTING TIME SERIES DATA, Chapter 2: Trend-Line Fitting and Forecasting},
  howpublished = {web site},
  year         = {2009},
  comment      = {Why/how to remove linear and polynomial trends before doing ARIMA modeling

Remove trends before fitting and ARIMA model b/c ARIMA assumes stationarity (non-trending).

According to the syllabus, these lecture notes were derived from the following books:
F.X. Diebold, "Elements of Forecasting", 4?th Ed. (Thomson)., Chapt 5
C.W.J. Granger, "Forecasting in Business and Economics", 2\textsuperscript{nd} Ed. (Harcourt Brace)., p 23-46.},
  file         = {Hurvich09trendForecast.pdf:Hurvich09trendForecast.pdf:PDF;Hurvich09trendForecast.pdf:Hurvich09trendForecast.pdf:PDF},
  groups       = {Read},
  owner        = {sotterson},
  timestamp    = {2009.03.11},
  url          = {http://pages.stern.nyu.edu/~churvich/},
}

@TechReport{Jaeger08neuralRecurTut,
  author      = {Herbert Jaeger},
  title       = {A tutorial on training recurrent neural networks, covering {BPPT}, {RTRL}, {EKF} and the "echo state network" approach (revision 3)},
  institution = {Fraunhofer Institute for Autonomous Intelligent Systems (AIS)},
  year        = {2008},
  url         = {http://www.faculty.iu-bremen.de/hjaeger/pubs/ESNTutorialRev.pdf},
  file        = {Jaeger08neuralRecurTut.pdf:Jaeger08neuralRecurTut.pdf:PDF},
  groups      = {recurNN},
  owner       = {scotto},
  timestamp   = {2008.07.07},
}

@InProceedings{Jansen13pvCtlMkt,
  author    = {Malte Jansen and Markus Speckmann},
  title     = {Participation Of Photovoltaic Systems In Control Reserve Markets},
  booktitle = {CIRED 22\textsuperscript{nd} International Conference on Electricity Distribution},
  year      = {2013},
  number    = {Paper 0245},
  month     = jun,
  comment   = {Malte Jansen's paper, maybe good to know for Eweline

Milligan doesn't like power blocks either (Milligan09windIntCost)

The KDE approach for probabilistic forecasts was taken from: Bowman97smthKDE},
  file      = {Jansen13pvCtlMkt.doc:Jansen13pvCtlMkt.doc:Word},
  groups    = {Use, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2013.03.11},
}

@TechReport{JCGM08guideUncertMeas,
  author      = {JCGM},
  title       = {Guide to the Expression of Uncertainty in Measurement},
  institution = {Joint Committee for Guides in Metrology Working Group 1},
  year        = {2008},
  number      = {JCGM 100:2008},
  comment     = {The GUM method that I think CPR uses for satelllite irradiation meas. using ground instruments (probably: Habte14calMeasUncertRadiometric). 

Referenced in: Habte15radiometUncertGUM

In 2019, references indicated that this guide has or will be updated e.g.
https://iopscience.iop.org/article/10.1088/0143-0807/37/2/025803/pdf , which mentions a Bayesian method.},
  file        = {:JCGM08guideUncertMeas.pdf:PDF},
  url         = {http://www.milliemicronanopico.com/uncertainnumbers/files/evaluating_un_metrologia_2008.pdf},
}

@Book{Jemberie04infoTheoryHydro,
  title     = {Information Theory and Artificial Intelligence to Manage Uncertainty in Hydrodynamic and Hydrological Models},
  publisher = {Taylor \& Francis Ltd},
  year      = {2004},
  author    = {Abebe ANDUALEM Jemberie},
  comment   = {Combined physical and data driven models. Neural Nets, etc. capture model error statistics. Looks applicable to wind too.},
  owner     = {sotterson},
  timestamp = {2008.07.03},
  url       = {http://books.google.com/books?id=eoSwda604XYC&printsec=frontcover&source=gbs_summary_r&cad=0},
}

@Book{Johnson10elmMatlabStyleBk,
  title     = {The elements of MATLAB style},
  publisher = {Cambridge University Press},
  year      = {2010},
  author    = {Johnson, Richard K},
  comment   = {The Fraunhofer IWES matlab style book.

Updated here: Johnson14matlabStyle20Bk},
  url       = {https://www.amazon.com/Elements-MATLAB-Style-Richard-Johnson/dp/0521732581},
}

@Book{Johnson14matlabStyle20Bk,
  title     = {MATLAB Style Guidelines 2.0},
  publisher = {Datatool},
  year      = {2014},
  author    = {Richard Johnson},
  comment   = {Updated version of Johnson10elmMatlabStyleBk},
  file      = {:Johnson14matlabStyle20Bk.pdf:PDF},
  url       = {http://datatool.com/index.html},
}

@Article{Jones15xavierInitBlog,
  author  = {Andy Jones},
  title   = {An Explanation of Xavier Initialization},
  journal = {andy's blog},
  year    = {2015},
  comment = {Simple explanation of Xaviar init, and how the classic algorithm differs from the one in Cafe.  Commenters not that the biases are not initialized in this way because they are constant (which I don't quite get, since weights are too, during initilization).

The original paper on Xavier init is: Glorot10undrstndDiffDpLrnFFnets},
  file    = {:Jones15xavierInitBlog.pdf:PDF},
  url     = {http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization},
}

@Standard{juris10VerordnungzurAusfuhrung,
  title        = {Verordnung zur Ausf{\"u}hrung der Verordnung zum {EEG}-Ausgleichsmechanismus (Ausgleichsmechanismus-Ausf{\"u}hrungsverordnung- AusglMechAV)},
  organization = {BGBl},
  institution  = {Deutsch Bundesministeriums der Justiz und f{\"u}r Verbraucherschutz},
  author       = {Juris},
  year         = {2010},
  url          = {https://www.gesetze-im-internet.de/ausglmechav/BJNR013400010.html},
  comment      = {How German TSOs must trade DA corrections on the intraday market. We don't understand this text yet.},
  file         = {juris10VerordnungzurAusfuhrung.pdf:juris10VerordnungzurAusfuhrung.pdf:PDF},
}

@Misc{Jursa06windEnsembleModel,
  author    = {Ren{\'e} Jursa},
  title     = {Wind Power Prediction by using Ensemble Models},
  year      = {2006},
  comment   = {Compares NN, mixture of experts, SVM's. ME wins. I can't figure out when/where this was published.},
  file      = {Jursa06windEnsembleModel.pdf:Jursa06windEnsembleModel.pdf:PDF},
  owner     = {scotto},
  timestamp = {2008.07.04},
  url       = {http://renknownet.iset.uni-kassel.de/renknowNET/obj.download;jsessionid=8d339f2630d8f5a1ce6485974c368a4be2416618b368?objName=641&lang=de},
}

@Article{Kaban12detNoMeaningDists,
  author    = {Kab{\'a}n, Ata},
  title     = {Non-parametric detection of meaningless distances in high dimensional data},
  journal   = {Statistics and Computing},
  year      = {2012},
  volume    = {22},
  number    = {2},
  pages     = {375--385},
  comment   = {Simple way to detect if dim is so high that distances in it are meaningless. Probability bound is coarse, but this one makes no distribution assumptions. Seems to work on some real data.

Good for detecting that you need to dimension reduce for sure. Since it's coarse, I'm not sure if it could tell you when to stop dimension reducing.

I also wonder if this predicts meaninglessness even after hubness tricks or p-Gaussians. If so, it could ID when there is no hope in trying them.},
  doi       = {10.1007/s11222-011-9229-0},
  file      = {Kaban12detNoMeaningDists.pdf:Kaban12detNoMeaningDists.pdf:PDF},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2014.04.04},
}

@Conference{Kaboudan04wvltFrcst,
  author    = {Kaboudan, Mak},
  title     = {Wavelets in Forecasting},
  booktitle = {International Summer Workshop on the Economic, Financial and Managerial Applications of Computational Intelligence},
  year      = {2004},
  comment   = {Intro to wavelet forecating. Haar. Inverse wavelet transform.},
  file      = {Kaboudan04wvltFrcst.pdf:papers\\Kaboudan04wvltFrcst.pdf:PDF;Kaboudan04wvltFrcst.pdf:Kaboudan04wvltFrcst.pdf:PDF},
  location  = {Taiwan},
  owner     = {sotterson},
  timestamp = {2013.03.16},
  url       = {http://www.aiecon.org/conference/efmaci2005/pdf/Wavelets_in_Forecasting_paper.pdf},
}

@InProceedings{Kadar07windPowCorr,
  author    = {K{\'a}d{\'a}r, P.},
  title     = {Evaluation of Correlation the Wind Speed Measurements and Wind Turbine Characteristics},
  booktitle = {Computational Intelligence and Informatics, International Symposium of Hungarian Researchers (CINTI)},
  year      = {2007},
  month     = nov,
  comment   = {Buidling wind-to-power curves from distant, off-site wind measurements},
  file      = {Kadar07windPowCorr.pdf:Kadar07windPowCorr.pdf:PDF;Kadar07windPowCorr.pdf:Kadar07windPowCorr.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.12.05},
}

@Article{Kaelbling96Reinforcementlearningsurvey,
  author    = {Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  title     = {Reinforcement learning: A survey},
  journal   = {Journal of artificial intelligence research},
  year      = {1996},
  volume    = {4},
  pages     = {237--285},
  owner     = {sotterson},
  timestamp = {2016.12.24},
}

@InProceedings{Kalmikov11windPowFmdmntls,
  author       = {Alex Kalmikov and Katherine Dykes and Kathy Araujo},
  title        = {Wind Power Fundamentals},
  booktitle    = {MIT Wind Week 2011},
  year         = {2011},
  month        = {2011},
  note         = {Slide presentation: MIT Wind Energy Group \& Renewable Energy Projects in Action Renewable Energy Projects in Action http://web.mit.edu/windenergy/windweek/WindWeek.html},
  comment      = {nice power curve explanation},
  file         = {Kalmikov11windPowFmdmntls.pdf:Kalmikov11windPowFmdmntls.pdf:PDF},
  howpublished = {Slide presentation: MIT Wind Energy Group \& Renewable Energy Projects in Action Renewable Energy Projects in Action},
  owner        = {sotterson},
  timestamp    = {2013.05.14},
  url          = {http://web.mit.edu/windenergy/windweek/Presentations/Wind%20Energy%20101.pdf},
}

@Article{Kamburugamuve18antmySprkFlnkMPI,
  author  = {Supun {Kamburugamuve} and Pulasthi {Wickramasinghe} and Saliya {Ekanayake} and Geoffrey C. {Fox}},
  title   = {Anatomy of machine learning algorithm implementations in MPI, Spark, and Flink},
  journal = {International Journal of High Performance Computing Applications},
  year    = {2018},
  volume  = {32},
  number  = {1},
  pages   = {61--73},
  comment = {MPI is more efficient than Spark or Flink, largely due to its general control structures, in-place computational and broadcast-to-all-nodes ability, and efficient communicaton prototocols. They also have limited or no ability to nest loops, so you have to do that outside of them, and then create new processes on each iteration.

Spark and Flink, which use simple data flow graphs, can be much easier to use, however, and can get nearly the same efficiency on simple algorithms like Terrasort and K-means.  But they did badly on MDS, especially Flink (which is also hard to program for that algorithm).

Lots of current research is trying to improve ML job control like Spark and Flink, so that they can have performance comparable to MPI.

Note that Spark is moving away from RDD interfaces to data frames:  Aug 2018 version of: 
https://spark.apache.org/docs/latest/ml-guide.html},
}

@InProceedings{Kariniotakis08anemosWindForecast,
  author    = {George Kariniotakis},
  title     = {ANEMOS Leading {Europe}an Union Research on Wind Power Forecasting},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  year      = {2008},
  comment   = {ANEMOS history and technology overview: current and future directions, pretty good
* explanation of who is in ANEMOS and history
* for both onshore and offshore New models they've developed
* power curve
* upscaling of regional forecasts
* prediction combination
* NWP filtering
* online uncertainty estimation
-- adapted resampling
-- quantile regression
-- prediction risk indices
* handling incomplete information

Modeling Techniques
* Neural and fuzzy nets
* regression
* Kalman, etc.

Change from ACCURACY driven forecasts to VALUE driven
* accuracy: RMS, etc. *
value: money, risk? They don't say

Move to Optimal Power System Management
* want to develop tools based on stochastic paradigm
* uses
-- reserve est.
-- congestion mgmt
-- scheduling
-- storage
-- trading

Safewind:
* focus on extreme event prediction},
  file      = {Kariniotakis08anemosWindForecast.pdf:Kariniotakis08anemosWindForecast.pdf:PDF;Kariniotakis08anemosWindForecast.pdf:Kariniotakis08anemosWindForecast.pdf:PDF},
  groups    = {IWFTM08, Read},
  owner     = {sotterson},
  timestamp = {2008.09.19},
  url       = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
}

@InProceedings{Kelley00timeFreqWaveletTurbWind,
  author    = {N.D. Kelley and R.M. Osgood and J.T. Bialasiewicz and A. Jakubowski},
  title     = {Using time-frequency and wavelet analysis to assess turbulence/rotor interactions},
  booktitle = {ASME Wind Energy Symposium},
  year      = {2000},
  comment   = {Analysis of turbulent events w/ wavelents, funky dynamical system plots * turb events only last a few seconds ==> need wavelets * also means that minute-scale forecasts can ignore them (although "general turbulence" must reduce efficiency, I'd guess.},
  file      = {:Kelley00timeFreqWaveletTurbWind.pdf:PDF;Kelley00timeFreqWaveletTurbWind.pdf:Kelley00timeFreqWaveletTurbWind.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.08.21},
  url       = {http://www.nrel.gov/docs/fy00osti/27151.pdf},
}

@InProceedings{Kelly08newWindModel,
  author    = {Robert Kelly},
  title     = {New Modeling Techniques: From Wind Assessment and Forecasting to Wind Resource Management},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  year      = {2008},
  comment   = {The value of wind forecasting (from Precision Wind). Nice powerpoint of wind physics (appendix)
* says forecasts struggle to beat persistence at sub-hourly level

* lists typicall errors and methods v.s time ahead
* says current level of prediction accuracy reduces value of wind

* says their approach is 90\% physics, 10\% adjustments based on statistics and on-site meas.
* they use plant-scale power curves
* say they use Kalman filters ("e.g.") to minimize residual errors},
  file      = {Kelly08newWindModel.pdf:Kelly08newWindModel.pdf:PDF;Kelly08newWindModel.pdf:Kelly08newWindModel.pdf:PDF},
  groups    = {IWFTM08, Read},
  owner     = {sotterson},
  timestamp = {2008.09.19},
  url       = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
}

@InCollection{Kelman12stochOptColRiv,
  author    = {Rafael Kelman and Raphael Chabar},
  title     = {JOINT USE OF LARGE-{SC}ALE STOCHASTIC OPTIMIZATION TECHNIQUES {AND} HYDROLOGIC MODELING APPLIED TO THE COLUMBIA RIVER SYSTEM},
  booktitle = {Reservoir System Modeling Technologies Conference},
  publisher = {PSR, Inc.},
  year      = {2012},
  type      = {Presentation Slides},
  month     = feb,
  comment   = {stoch programming used by ISOs and utilities in Europe, Asia and Latin America (and soon, in the US PNW, I think)

* used for System dispatch (ISO)

* big deal in Brazil, where it's 85\% hydro (they also model their transmission network)

* stochastic programming used for Probabilistic hydrothermal dispatch (model, only, this time, I think).

* Is scenario based.

* Other stochastic programming applications also discussed e.g. natural gas (something).

* use Amazon cloud computing and Hadoop!

* In the PNW (US pacific northwest?) I think a prototype has been built
-- Measures sustained production capacity for a given number of hours and reliability level (peak capacity)
-- includes all the major hydro plus: wind farms, small hydro and solar (and all their enviro constraints).
-- also equipment availability
-- possibly, the guy was talking about the BPA's TIP259 project:

---- http://www.bpa.gov/Doing%20Business/TechnologyInnovation/Pages/Technology-Innovation-Projects.aspx
---- Also probably in Allen12bpaHyProM (note the Fraunhofer partner!)},
  file      = {Kelman12stochOptColRiv.pdf:Kelman12stochOptColRiv.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  location  = {Portland, USA},
  owner     = {sotterson},
  timestamp = {2013.10.02},
}

@Article{Kiehl01Neuralsourcesinvolved,
  author    = {Kiehl, Kent A and Laurens, Kristin R and Duty, Timothy L and Forster, Bruce B and Liddle, Peter F},
  title     = {Neural sources involved in auditory target detection and novelty processing: an event-related fMRI study},
  journal   = {Psychophysiology},
  year      = {2001},
  volume    = {38},
  number    = {1},
  pages     = {133--142},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2016.12.19},
}

@Misc{Kirschen12OptPowFlw,
  author       = {Daniel Kirschen},
  title        = {Optimal Power Flow},
  howpublished = {Course Notes: Power System Economics},
  year         = {2012},
  comment      = {Readable lecture slides on basic optimal power flow.

See also: Zhang13RobustoptPowFlwVaR
The whole course is here: http://www.ee.washington.edu/research/real/power_system_economics.html},
  file         = {Kirschen12OptPowFlw.pptx:Kirschen12OptPowFlw.pptx:PowerPoint 2007+},
  owner        = {sotterson},
  timestamp    = {2014.11.26},
  url          = {http://www.ee.washington.edu/research/real/power_system_economics.html},
}

@Article{Kneip08regFitFuncMod,
  author    = {Kneip, A. and Ramsay, J.O.},
  title     = {Combining registration and fitting for functional models},
  journal   = {Journal of the American Statistical Association},
  year      = {2008},
  volume    = {103},
  number    = {483},
  pages     = {1155--1165},
  issn      = {0162-1459},
  comment   = {Combines time and scale alignment. Use for detecting NWP phase error?},
  doi       = {10.1198/016214508000000517},
  groups    = {Read},
  owner     = {scot},
  publisher = {ASA},
  timestamp = {2010.11.09},
}

@Article{Knight98reviewEEGblood,
  author    = {Knight, Robert T and Nakada, Tsutomu},
  title     = {A review of EEG and blood flow data},
  journal   = {Reviews in the Neurosciences},
  year      = {1998},
  volume    = {9},
  number    = {1},
  pages     = {57--70},
  owner     = {sotterson},
  timestamp = {2016.12.19},
}

@InCollection{Koenker94confIntRankInvQR,
  author    = {Koenker, Roger},
  title     = {Confidence intervals for regression quantiles},
  booktitle = {Asymptotic statistics},
  publisher = {Springer},
  year      = {1994},
  pages     = {349--359},
  comment   = {A chapter of this book describes the default quantile confidence interval test for R's quantreg (the rank inversion test, starting on p. 353, and bookmarked).

Book chapter is exactly the same as the separate paper, but the scanning is better and OCR works.

It's also explained in Koenker\s "quantile regression" book (2005), Section 3.4.5. (from p. 4 of the quantreg vignette).},
  doi       = {10.1007/978-3-642-57984-4_29},
  file      = {Koenker94confIntRankInvQR.pdf:Koenker94confIntRankInvQR.pdf:PDF},
  groups    = {Test, doReadNonWPV_2},
  owner     = {sotterson},
  timestamp = {2014.04.02},
}

@Unpublished{Kozmik12simDiffStochDynOpt,
  author    = {V{\'a}clav Kozm{\'i}k},
  title     = {Similarities and differences between stochastic programming, dynamic programming and optimal control},
  note      = {Faculty of Mathematics and Physics. Charles University in Prague},
  year      = {2012},
  comment   = {Orienteering overview slides describing how these different kinds of optimization relate to each other.},
  file      = {Kozmik12simDiffStochDynOpt.pdf:Kozmik12simDiffStochDynOpt.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.02.06},
  url       = {http://www.karlin.mff.cuni.cz/~kozmik/?lang=cz&semester=zs2012},
}

@InCollection{Kramer13SortHiDimNN,
  author    = {Kramer, Oliver},
  title     = {Sorting High-Dimensional Patterns with Unsupervised Nearest Neighbors},
  booktitle = {Agents and Artificial Intelligence},
  publisher = {Springer},
  year      = {2013},
  pages     = {250--267},
  comment   = {The KNN --> 1D line idea in Kramer11dimRedKNN with missing features and some kind of noise margin.

Uses KNN to convert multidimensional space to 1D manifold},
  doi       = {10.1007/978-3-642-36907-0_17},
  file      = {Kramer13SortHiDimNN.pdf:Kramer13SortHiDimNN.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2014.05.09},
}

@Article{Kraskov03hierClustMutInf,
  author    = {Kraskov, A. and Stogbauer, H. and Andrzejak, R. G. and Grassberger, P.},
  title     = {Hierarchical Clustering Based on Mutual Information},
  journal   = {eprint arXiv:q-bio/0311039},
  year      = {2003},
  month     = nov,
  adsnote   = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl    = {http://adsabs.harvard.edu/abs/2003q.bio....11039K},
  comment   = {Clusters using MI; Improved in [Aghagolzadeh07hierClustMutInf]. Best discussion in Kraskov papers about MI dimension scaling
* does mutual information scaling done here have anything to do with Ho10condEntropErr ?
* IDEA: use Schnitzer12Localandglobal hub processing to reduce dim-dependent bias?

Related: Aghagolzadeh07hierClustMutInf,Kraskov08MIChierClustMutInf
},
  eprint    = {arXiv:q-bio/0311039},
  file      = {Kraskov03hierClustMutInf.pdf:Kraskov03hierClustMutInf.pdf:PDF},
  keywords  = {Quantitative Biology - Quantitative Methods, Computer Science - Computational Complexity, Physics - Biological Physics},
  owner     = {sotterson},
  timestamp = {2008.11.14},
}

@Article{Kraskov08MIChierClustMutInf,
  author     = {Kraskov, A. and Grassberger, P.},
  title      = {{MIC}: Mutual Information based hierarchical clustering},
  journal    = {ArXiv e-prints},
  year       = {2008},
  month      = sep,
  adsnote    = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl     = {http://adsabs.harvard.edu/abs/2008arXiv0809.1605K},
  comment    = {Partial MI est explanation, including mention of dimension bias. Clusters by hierarchically decomposing MI and also uses this for ICA.
* some good MI info
* uses MI grouping property to decompose MI
* distance metric normalizes by total information (I don't quite get why this matters)
-- Ahrens08infoSkillProbFrcst says MI is alread a triangle-inequality-solving metric
-- was he only talking about pairs of scalars?
* I think: MI is diff. between "surprise" if Xk are indepent (entropies add) and their joint "surprise" (joint entropy)
* Lebesque measure: a measure of length, area or volume in Euclidean space (Wikipedia)
* differential entropy can be neg and variant to transforms
* Relationship between differential (continuous) entropy and entropy when binned (Shannon?)
* true mutual information is the limit of a sum of binned entropies.

* But binned (Shannon?) MI is invariant and non-neg. Multivariate or pairwise only??
-- Srinivasa05mutInfoReview and Brown09newInfoFeatSel say interaction info can be neg . So I guess Kraskov08 (this paper) talks about Shannon.
-- is the pos. info here actually Shannon info?
-- Brown09newInfoFeatSel eq. 3 is the relation between interaction and Shannon information
* common info-theoretic distance function is biased by cluster dimensions; their choice is not, and is also a proper metric
* stuff about algorithmic MI, related to symbol processing, but I didn't read carefully enough to get it. Shannon MI estimation:
* this paper's method supposed to be better than others for high dimensions

* Somebody proved that can estimate entropy by sorting a scalar variable, calculating the 1\textsuperscript{st} difference and doing some log sums.
* This can be extended use the difference between the kth neighbors.

* . . . and the difference idea extended here to higher dims by sorting across the MAX dim difference.
* I don't know why this is OK.
* MI estimation (roughly) finds distance to k neighbors in joint space (X,Y) and then compares the number of neighbors found at that distance in the individual spaces, X, Y
* Then could estimate mewl the individually calculated entropies. Bit this causes many sources of error, so they don't do that here b/c the same k in higher spaces can produce a much higher max (which causes non-canceling errors, assuming zero mean error or sonneteers, I suppose)
* But why use MAX in the first place? wouldn't this be less of a problem injured Euclidean distance or an average or something?
* Need to read Kraskov04EstMutInfKNN, which has a fuller explanation of MI estimation
* num neighbors choice (k):
-- lower k: more noise, less bias
-- higher k: less noise more bias
-- choice of k for this experiment  (p. 17-18)
---- used k=100 to find least dependent neighbors, since MI est==0, regardless of k, but less noise
---- used k=3 for clustering since looking for max MI at high dimension (want to avoid dim. dep bias).
* p. 18: MI estimate is SYSTEMATICALLY UNDERESTIMATED FOR HIGHER DIMENSIONS, exactly as I found in the ramp paper work.
* But bias seems to disappear if MI is really zero, so OK for independence detection.
* Also, bias disappears as num. samples --> infinity (p. 8)
* Kraskov04
* also discusses neg. bias w/ low sample sizes MI clustering
* agglomerative
* MI is Shannon for waveforms and Unix disk compression (!) for symbol-like signals, like DNA sequences
* two ways of drawing dendrograms; they prefer the one where height corresponds to MI-based distance
* the use a distance function that divides by dimension (is this another way of removing the dimension bias?)
* interesting waveform clustering: use embedded delays [see ref 35, mentioned on p. 17]
-- could be used for lagged variable determination during feature selection

MI-based independent components analysis (p. 16-)
1. run ICA on lagged copies
2. cluster ICA components
3. eyeball the best clustering from the dendrogram
4. "project onto those clusters" and inverse transform (I don't understand this step)},
  eprint     = {0809.1605},
  eprinttype = {arXiv},
  file       = {Kraskov08MIChierClustMutInf.pdf:Kraskov08MIChierClustMutInf.pdf:PDF;Kraskov08MIChierClustMutInf.pdf:Kraskov08MIChierClustMutInf.pdf:PDF},
  groups     = {Read},
  keywords   = {Quantitative Biology - Quantitative Methods},
  owner      = {sotterson},
  timestamp  = {2008.11.14},
  url        = {http://arxiv.org/abs/0809.1605},
}

@InProceedings{Krause07nonmyopicActiveLrn,
  author               = {Krause, Andreas and Guestrin, Carlos},
  title                = {Nonmyopic active learning of {Gauss}ian processes: an exploration-exploitation approach},
  booktitle            = {International Conference on Machine Learning (ICML)},
  year                 = {2007},
  pages                = {449--456},
  publisher            = {ACM Press},
  citeulike-article-id = {1741522},
  comment              = {Does mutual information-based feature selection at each step; handles non-stationarity},
  doi                  = {10.1145/1273496.1273553},
  file                 = {Krause07nonmyopicActiveLrn.pdf:Krause07nonmyopicActiveLrn.pdf:PDF;Krause07nonmyopicActiveLrn.pdf:Krause07nonmyopicActiveLrn.pdf:PDF},
  isbn                 = {9781595937933},
  keywords             = {gaussian-process, gp-optimization},
  location             = {New York, NY, USA},
  owner                = {sotterson},
  posted-at            = {2007-10-08 14:49:08},
  timestamp            = {2009.02.25},
}

@Conference{Krause09,
  author    = {Andreas Krause and Carlos Guestrin},
  title     = {Intelligent Information Gathering and Submodular Function Optimization},
  booktitle = {IJCAI Tutorial},
  year      = {2009},
  month     = jun,
  comment   = {Good tutorial on submodular functions},
  file      = {Tutorial Slides:Krause09intelInfoSubmodOpt.ppt:PowerPoint},
  groups    = {Read},
  location  = {Pasadena},
  owner     = {sotterson},
  timestamp = {2012.09.12},
}

@Article{Ku95dynPCA,
  author    = {Wenfu Ku and Robert H. Storer and Christos Georgakis},
  title     = {Disturbance detection and isolation by dynamic principal component analysis},
  journal   = {Chemometrics and Intelligent Laboratory Systems},
  year      = {1995},
  volume    = {30},
  number    = {1},
  pages     = {179--196},
  issn      = {0169-7439},
  note      = {InCINC '94 Selected papers from the First International Chemometrics Internet Conference},
  comment   = {Find ID or ARX (and sorta ARMA) models by PCA'ing lagged copies of regression inputs Dynamic PCA (DPCA) procedure
* build a matrix with lagged copies of inputs running down the columnns
* do PCA
* if there is a zero singular value, then the corresponding singular vector yields the model coeffs
 -- b/c the linear relationship between coeffs is captured by the homogenous equaton
 ---- solution is in the noise space
 -- pick max lag so that get exaclty one zero-valued singular value
 -- the "zero-valued" means some algorithm decides that you have one non-significant principal value
 -- lot's of heuristics for picking num. of comps.
 ---- use Minka00AutoPCAdim (speakerclust.bib) to pick num. comps?
 -- is like Benesty00adaptEigSrcLoc (speakerclust.bib) where they're looking for the min eigenvalue?
* say that maxlag is usually 1 or 2, but need more for truly nonlinear relationships
* Chow99dynPCAintTransf finds that Fourier or Wavelet techniques may work better in noise
* refers to a similar "canonical analysis" technique for generating an ARIMA model
 -- see p. 184, ref to Macgregor and Wong [32]
* preprocess w/ multivariate Gaussianization to ensure that PCA is meaningful?
 -- since there are PCA Gaussianization techniques, is a joint process possible?},
  doi       = {DOI: 10.1016/0169-7439(95)00076-3},
  file      = {Ku95dynPCA.pdf:Ku95dynPCA.pdf:PDF},
  groups    = {Read},
  keywords  = {Dynamic multivariate statistical process control},
  owner     = {scot},
  timestamp = {2010.07.29},
  url       = {http://www.sciencedirect.com/science/article/B6TFP-3YMFRT3-V/2/8271069a38d8cfbc7c8dc6c282c5aa9b},
}

@Misc{Kuan11quantRgrsnIntro,
  author       = {Chung-Ming Kuan},
  title        = {Introduction to Quantile Regression},
  howpublished = {Course Notes, Department of Finance \& CRETA. National Taiwan University},
  month        = apr,
  year         = {2011},
  comment      = {Course notes for quantile regression, include a feature selection hint for quantile regression on slide 19.

Also as a measure of goodness of fit on slide 19

Also, several other possible feature sel. criteria e.g. the Wald test, etc. Related ideas are in the SAS manual:
http://support.sas.com/documentation/cdl/en/statug/63962/HTML/default/statug_qreg_sect016.htm},
  file         = {Kuan11quantRgrsnIntro.pdf:Kuan11quantRgrsnIntro.pdf:PDF},
  groups       = {PointDerived, Test, doReadNonWPV_1},
  owner        = {sotterson},
  timestamp    = {2013.10.22},
  url          = {http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0CDgQFjAB&url=http%3A%2F%2Fhomepage.ntu.edu.tw%2F~ckuan%2Fpdf%2FLec-QReg-slide_spring%25202010.pdf&ei=ozdmUt3zKsKc0AXSlIGABQ&usg=AFQjCNHftccyV5bMzT-i_9XGYfVENZyF4w&bvm=bv.55123115,d.d2k},
}

@Conference{Kuester13EstAvailActPowRsrv,
  author       = {K{\"u}ster, Kristie Kaminski and Schneider, Dominik and Siefert, Malte and Speckmann, Markus},
  title        = {Estimation of Available Active Power in Wind Farms for Operating Reserve Provision},
  booktitle    = {Brasil Windpower 2013},
  year         = {2013},
  organization = {Fraunhofer IWES},
  comment      = {Physical model works a lot better than NN for predicting available under curtailment.

Friendly slides by an IWES student(?)},
  file         = {Paper:Kuester13EstAvailActPowRsrv.pdf:PDF;Slides:Kuester13EstAvailActPowRsrv_slides.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2014.11.14},
  url          = {http://www.brazilwindpower.org/archives/anuario/Sala%203%20-%2015h45%20-%20Estimation%20of%20Available%20Active%20Power%20in%20Wind%20Farms%20for%20Operating%20Reserve%20Provision.pdf},
}

@INPROCEEDINGS{Kukreja06lassoNonLinSysID,
  Author                   = {Sunil L. Kukreja and Johan Lofberg and Martin J. Brenner},
  Title                    = {A least absolute shrinkage and selection operator (LASSO) for nonlinear system identification System Identification},
  Booktitle                = {Symposium on System Identification (IFAC)},
  Year                     = {2006},
  Volume                   = {14},
  Number                   = {1},
  File                     = {Kukreja06lassoNonLinSysID.pdf:Kukreja06lassoNonLinSysID.pdf:PDF;Kukreja06lassoNonLinSysID.pdf:Kukreja06lassoNonLinSysID.pdf:PDF},
  Owner                    = {sotterson},
  Timestamp                = {2009.08.17}
}

@Article{Kumaran07Whichcomputationalmechanisms,
  author    = {Kumaran, Dharshan and Maguire, Eleanor A},
  title     = {Which computational mechanisms operate in the hippocampus during novelty detection?},
  journal   = {Hippocampus},
  year      = {2007},
  volume    = {17},
  number    = {9},
  pages     = {735--748},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2016.12.19},
}

@TECHREPORT{Kurowicka10vinesLrn,
  Author                   = {Kurowicka, D. and Cooke, R.M},
  Title                    = {Vines and continuous non-parametric {Bayes}ian belief nets, with emphasis on model learning},
  Institution              = {Delft Institute of Applied Mathematics, Risk and Environmental Modelling},
  Year                     = {2010},
  Month                    = aug,
  File                     = {Kurowicka10vinesLrn.pdf:Kurowicka10vinesLrn.pdf:PDF},
  Owner                    = {scot},
  Timestamp                = {2010.11.24},
  URL                      = {http://dutiosc.twi.tudelft.nl/~risk/index.php?option=com_docman&task=doc_details&gid=207&&Itemid=13}
}

@InProceedings{Kusiak08dataMineWind,
  author    = {Andrew Kusiak},
  title     = {Prediction of Wind Farm Power and Ramp Rates: A Data-Mining Approach},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  year      = {2008},
  comment   = {Machine learning approach to short term predicitons (10 minutes to 4 hours) and ramps * seems relevant to NowCaster * focuses on power prediction baedon wind farm data, not weather models * decision trees! Not clear what they're used for, though * shows 2.2 \% MAE on 10 minute predictions (plant or foarm or what?) * predicts ramp RATES, not absolute power! -- doesn't look that accurate, though},
  file      = {Kusiak08dataMineWind.pdf:Kusiak08dataMineWind.pdf:PDF;Kusiak08dataMineWind.pdf:Kusiak08dataMineWind.pdf:PDF},
  groups    = {IWFTM08, Read},
  owner     = {sotterson},
  timestamp = {2008.09.19},
  url       = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
}

@Article{Kusiak16shareDataWind,
  author    = {Kusiak, Andrew},
  title     = {Share data on wind energy},
  journal   = {Nature},
  year      = {2016},
  volume    = {529},
  number    = {7584},
  pages     = {19},
  comment   = {This article:
 http://www.evernote.com/l/AA0YFtn7eIlMYritm5ZW4tXZL_ybj5mnpa8/
says that this Kusiak artiicle says that:
  More precise estimates of how much renewable energy will be generated bring many benefits. First, wind farms can more efficiently control the number and capacity of turbines in operation to meet electricity demand. This could increase power generation by at least 10\% and potentially up to 16\%, according to a recent commentary in Nature},
  file      = {Kusiak16shareDataWind.pdf:Kusiak16shareDataWind.pdf:PDF},
  owner     = {sotterson},
  publisher = {Nature Publishing Group},
  timestamp = {2017.05.25},
  url       = {http://www.nature.com/news/renewables-share-data-on-wind-energy-1.19104},
}

@Article{Laing11introTropMeteo,
  author    = {Laing, A and Evans, JL},
  title     = {Introduction to tropical meteorology, 2nd Ed.},
  journal   = {University Corporation for Atmospheric Research, Boulder, CO},
  year      = {2011},
  comment   = {Online book on tropical meteorology (also in print),

Low level jets
http://www.goes-r.gov/users/comet/tropical/textbook_2nd_edition/print_3.htm#page_8.2.0

Low-level jets (LLJs) are regional wind maxima in the lower troposphere that are defined by Stensrud (1996)122 as follows:

Narrow area of maximum in wind speed below 700 hPa
Vertical wind profile with speed increasing to the jet core and decreasing above
Horizontal wind shear exist on side of the jet, e.g., weaker winds at the edge of the jet},
  owner     = {sotterson},
  timestamp = {2017.04.25},
  url       = {http://www.goes-r.gov/users/comet/tropical/textbook_2nd_edition/index.htm},
}

@TECHREPORT{Landberg02Pof,
  Author                   = {Lars Landberg},
  Title                    = {Predicting the output from wind farms -- a tutorial},
  Institution              = {Ris{\o} National Laboratory},
  Year                     = {2000},
  Type                     = {Presentation Slides},
  File                     = {:Landberg0Pof.pdf:PDF;Landberg0Pof.pdf:Landberg0Pof.pdf:PDF},
  HowPublished             = {Presentation Slides},
  Owner                    = {scotto},
  Timestamp                = {2008.07.06},
  URL                      = {http://www.ee.qub.ac.uk/blowing/activity/Dublin/main.htm}
}

@InProceedings{Lange08windForecastExperience,
  author    = {Matthias Lange},
  title     = {Operational Wind Power Forecasting Experiences from {Europe}, {North America} and {Australia}},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  year      = {2008},
  comment   = {Previento talk. Good ramp and thermal stratification info * thermal stratification affects true wind at hub height * they model this w/ different model types and then select and weight based on weather conditions * forecast errors are non-Guassian * says met towers nice, but not strictly needed},
  file      = {Lange08windForecastExperience.pdf:Lange08windForecastExperience.pdf:PDF;Lange08windForecastExperience.pdf:Lange08windForecastExperience.pdf:PDF},
  groups    = {IWFTM08, Read},
  owner     = {sotterson},
  timestamp = {2008.09.19},
  url       = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
}

@Conference{Lange08wpmsTSOs,
  author    = {B. Lange},
  title     = {Experiences from the Development of Wind Power Forecasts for Six {European TSOs}},
  booktitle = {Second Workshop on Best Practice in the Use of Short-term Forecasting of Wind Power},
  year      = {2008},
  month     = may,
  comment   = {Reference describing Fraunhofer IWES WPMS use},
  location  = {Madrid, Spain},
  owner     = {sotterson},
  timestamp = {2012.09.24},
}

@Article{Lange16bestPrctcWndPowFrcst,
  author    = {Matthias Lange and Ulrich Focken and Anne Lenz},
  title     = {Best practice in wind power forecasting},
  journal   = {Southwest Power Pool Stakeholder Meeting},
  year      = {2016},
  comment   = {Great forecasting slides by Matthias Lange & Co, of energy&meteo.   Guessing about the date.

Use for Colombia class.

Good info / slide

Requirements for good forecast
-  Multiple weather model input
-  Advanced and robust forecasting system
-  Meteorological know-how to tune the models
-  Extra module for very short-term forecasting (0 to 6 hours)
-  Specific module for ramps
- Use of measurement data: historical as well as online

* good graph showing trubine height v.s gepstropic wind, other hight effects
* nice 3D power curve
* nice graph showing what curtailments do to power curve (put in with downregulation section)

Reasons for wind power forecasting errors
* Avoidable forecasting errors due to lacking information from wind farms
- Missing turbine availability information (due to maintenance e.g.)
- Unannounced curtailments
- Wrong pooling of wind farms
* Machine features that are difficult to predict, but where additional data
from wind turbines could help:
- turbine shut downs due to
-- high wind speeds
-- low temperatures
* Forecasting errors in NWP Data

* good graph showing diurnal pattern

 VALUE OF DATA from Wind farms
* Wind power measurements (very high influence on accuracy)
* Very short term forecast (0 ? 10 h)
* Essential for ramp forecasting
* Wind turbine availability (very high influence on accuracy)
* Adjust prediction for all prediction horizons
* Include availability into forecasting process
* Meteorological measurements (high influence on accuracy)
* Very short term forecast
* Detect deviations and correct forecast

Standardization and accuracy of data is important
* Availability of wind farm:
* Number of turbines up and running or
* Maximum available power output?
* Scheduled Maintenance
* Update of time periods through maintenance crew

* How does forecasting error move?
 - agrees with Schluter that error does not move in same dir as wind (not always advection)

Forecast Combination
* block diagram
* graph shows that large errors are avoided.


* Curtailment
* Information flow between grid operator and wind farm operator},
  file      = {Lange16bestPrctcWndPowFrcst.pdf:Lange16bestPrctcWndPowFrcst.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.13},
  url       = {https://www.spp.org/documents/28867/energymeteo_spp_qa.pdf},
}

@Article{LarryM15sunTempOppSkepSci,
  author    = {LarryM},
  title     = {Sun \& climate: moving in opposite directions},
  journal   = {Skeptical Science},
  year      = {2015},
  comment   = {Increase in solar radiation (TSI) accounts for < 0.1 deg. C since 1880 (in 2013).  Source for this (and Figure 2) is:Abe-Ouchi13ippcPaeloAssCycRep},
  file      = {LarryM15sunTempOppSkepSci.pdf:LarryM15sunTempOppSkepSci.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.06},
  url       = {https://www.skepticalscience.com/solar-activity-sunspots-global-warming.htm},
}

@InProceedings{Lauer00algorithmdistributedreinforcement,
  author       = {Lauer, Martin and Riedmiller, Martin},
  title        = {An algorithm for distributed reinforcement learning in cooperative multi-agent systems},
  booktitle    = {In Proceedings of the Seventeenth International Conference on Machine Learning},
  year         = {2000},
  organization = {Citeseer},
  owner        = {sotterson},
  timestamp    = {2016.12.24},
}

@InProceedings{LeCun12deepLrnInvarFeatHier,
  author       = {LeCun, Yann},
  title        = {Learning invariant feature hierarchies},
  booktitle    = {European Conference on Computer Vision},
  year         = {2012},
  pages        = {496--505},
  organization = {Springer},
  comment      = {A deep learning tutorial with a focus on how to do feature learning.  Good to know.  Video with slides. I couldn't get the paper.

Video:  https://www.youtube.com/watch?v=u6q2pLE6Dc4
Slides: https://pdfs.semanticscholar.org/e882/a6014ac4d66b1035729305ff8aa76ba5d09d.pdf},
  file         = {Slides:LeCun12deepLrnInvarFeatHier.pdf:PDF},
  url          = {http://link.springer.com/chapter/10.1007/978-3-642-33863-2_51},
}

@Article{Lee16dimRedPCA,
  author    = {Dongheui Lee},
  title     = {Machine Learning in Robotics Lecture 8: Dimensionality Reduction},
  journal   = {Institute of Automatic Control Engineering Technische Universita{\"a}t Mu{\"u}nchen},
  year      = {2016},
  comment   = {Henry Martin's professor's slides on dimension reduction.  Used for Colombia talk},
  file      = {Lee16dimRedPCA.pdf:Lee16dimRedPCA.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.03},
}

@Article{Lefebvre04kalmanNonLinPerfComp,
  author    = {Lefebvre, Tine and Bruyninckx, Herman and De Schutter, Joris},
  title     = {{Kalman} filters for non-linear systems: a comparison of performance},
  journal   = {International Journal of Control},
  year      = {2004},
  volume    = {77},
  number    = {7},
  pages     = {639653},
  comment   = {Compares several kinds of nonlinear Kalman filters, including unscented. Shows how they fit into bigger theory.},
  file      = {Lefebvre04kalmanNonLinPerfComp.pdf:Lefebvre04kalmanNonLinPerfComp.pdf:PDF;Lefebvre04kalmanNonLinPerfComp.pdf:Lefebvre04kalmanNonLinPerfComp.pdf:PDF},
  owner     = {scotto},
  timestamp = {2008.07.20},
  url       = {http://www.informaworld.com.offcampus.lib.washington.edu/smpp/content~content=a714029271~db=all~order=page},
}

@InProceedings{LETTER2019,
  author = {LETTER and OPEN ACCESS},
  title  = {Environmental Research Letters},
  year   = {2019},
  file   = {:C\:/Users/scott/Downloads/Edelenbosch_2018_Environ._Res._Lett._13_124004.pdf:PDF},
}

@InCollection{Lew12tusconFrcstWrkshp,
  author    = {Debbie Lew},
  title     = {Overview of Tucson Forecasting Workshop},
  booktitle = {UWIG},
  year      = {2012},
  comment   = {Claim that ramp forecasts are needed, forecast use (TSO assumes worst case if get two forecasts),
* claim that need specific ramp forecasts (Drake Bartlett, Xcel Energy)
* doubt about offsite met towers
* regime based MOS done by AWS
* RMSE not good, need to express error in "cost space," dependent on specific markets and power systems (Corinna Moehrlen, WEPROG)
* analog ensembles touted as the future
 see also: Nissen12analogEnsPowFrcst
* forecast use (TSO assumes worst case if get two forecasts) David Bell, EIRGRID (Ireland)
* strong need for solar forecast (Jim Blatchford, CAISO)
* Xcell guy says that need 1MW reserve for every "1-5" MW of wind (capacity, expected production, ?). There's a graph and something about energy zones but I don't understand it.
* Xcell guy says that only 15\% of coal plant cycling (somewhere) was due to wind; a lot of the rest was maintenance (I think).},
  file      = {Lew12tusconFrcstWrkshp.pdf:Lew12tusconFrcstWrkshp.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.04.25},
}

@Article{Lisman01Storagerecallnovelty,
  author    = {Lisman, John E and Otmakhova, Nonna A},
  title     = {Storage, recall, and novelty detection of sequences by the hippocampus: elaborating on the SOCRATIC model to account for normal and aberrant effects of dopamine},
  journal   = {Hippocampus},
  year      = {2001},
  volume    = {11},
  number    = {5},
  pages     = {551--568},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2016.12.19},
}

@Article{Lizier14JIDTinfoToolkit_SupMat,
  author  = {Lizier, Joseph T},
  title   = {Supplementary Material: JIDT: An information-theoretic toolkit for studying the dynamics of complex systems},
  year    = {2014},
  comment = {Supplementary detail about the types of Mutual Information, referenced in the nice Table 1 in Lizier14JIDTinfoToolkit

Conditional Mutual Information: explanation of how Kraskov04 was improved for condittional MI
(but I think Frenzel07partMutInfo says conditional MI is often conflated with with partial MI)},
  file    = {Lizier14JIDTinfoToolkit_SupMat.pdf:Lizier14JIDTinfoToolkit_SupMat.pdf:PDF},
  url     = {http://journal.frontiersin.org/file/downloadfile/870/octet-stream/presentation%201.pdf/796/2/113567},
}

@Article{Lorenz14WhereSolPowFrcst,
  author    = {Elke Lorenz},
  title     = {Where we are today with solar power forecasting},
  journal   = {WIRE Workshop: Renewable Energies Forecasting, Paris},
  year      = {2014},
  comment   = {Nice presentation on PV forecasting.  Used in colombia course},
  file      = {Lorenz14WhereSolPowFrcst.pdf:Lorenz14WhereSolPowFrcst.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.02},
  url       = {http://www.wire1002.ch/fileadmin/user_upload/Major_events/Final_Workshop_Paris_2014/Presentations/Lorenz.pdf},
}

@Article{Lynch06nwpIntroOverviewClass_Slides,
  author    = {Peter Lynch},
  title     = {Numerical Weather Prediction Class Notes: Intro \& Overview},
  journal   = {Meteorology \& Climate Cehtre, School of Mathematical Sciences, University College Dublin},
  year      = {2006},
  comment   = {Class notes used by Eugenia Kalnay (http://www.atmos.umd.edu)  apparently borrowed from Peter Lynch.  This was the intro lecture with the filename:

CH1-1_IntroPL-EK (1).pdf

(looks like she may have made a change?},
  file      = {Lynch06nwpIntroOverviewClass_Slides.pdf:Lynch06nwpIntroOverviewClass_Slides.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.03.31},
  url       = {http://www.atmos.umd.edu/~ekalnay/#AOSC614},
}

@Article{MacKay92bayesInterpNNtrn,
  author    = {MacKay, David JC},
  title     = {Bayesian interpolation},
  journal   = {Neural computation},
  year      = {1992},
  volume    = {4},
  number    = {3},
  pages     = {415--447},
  comment   = {The Bayesian Regularizaion backprop algorithm used by Matlab's 'trainbr'.

See: http://www.mathworks.com/help/nnet/ref/trainbr.html},
  file      = {paper:MacKay92bayesInterpNNtrn.pdf:PDF},
  publisher = {MIT Press},
  url       = {http://www.mitpressjournals.org/toc/neco/4/3},
}

@Article{Madsen05frcstPerfEval,
  author    = {Madsen, H. and Pinson, P. and Kariniotakis, G. and Nielsen, H.A. and Nielsen, T.S.},
  title     = {Standardizing the performance evaluation of shortterm wind power prediction models},
  journal   = {Wind Engineering},
  year      = {2005},
  volume    = {29},
  number    = {6},
  pages     = {475--489},
  comment   = {Things that should be measured when evaluationg a wind power forecast algorithm. Recommends a correlation-weighted combination of climatology and persistence. I think Kristin Larson at 3TIER gave me a similar reference. Note: the pdf is from an ANEMOS report, but google scholar serves it up as the Wind Energy journal publication. I'm guessing that the text is very similar.},
  file      = {Madsen05frcstPerfEval.pdf:Madsen05frcstPerfEval.pdf:PDF},
  owner     = {sotterson},
  publisher = {Multi-Science},
  timestamp = {2012.03.28},
}

@Book{Madsen08timeSeriesBook,
  title     = {Time series analysis},
  publisher = {Chapman \& Hall/CRC London},
  year      = {2008},
  author    = {Madsen, H.},
  volume    = {140},
  comment   = {Henrik's time series book w/ classical methods},
  owner     = {scot},
  timestamp = {2010.12.14},
}

@Unpublished{Madsen10optSpinResProj,
  author    = {Madsen, H},
  title     = {DTU Optimal Spinning Reserve Project: Annex 1: Project Description},
  year      = {2010},
  comment   = {A possible project for Juan Miguel Morales Gonzales. Tryggvi J?nsson and Marco Zugno are alread committed to it.},
  file      = {Madsen10optSpinResProj.pdf:Madsen10optSpinResProj.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.07.05},
}

@InCollection{Maggio11frcstExpERCOT,
  author    = {David Maggio},
  title     = {{ISO} Experiences with Wind Forecasting -- {ERCOT}},
  booktitle = {ERCOT, Real-Time market Integration},
  publisher = {UWIG Workshop},
  year      = {2011},
  month     = feb,
  comment   = {How ERCOT PLANS to use probabilistic forecast forecasts in its operations: ERAT and ELRAS are tools in progress

work in unit committment and ramp warnings, of some kind.},
  file      = {Maggio11frcstExpERCOT.pdf:Maggio11frcstExpERCOT.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  location  = {Albany, NY, USA},
  owner     = {sotterson},
  timestamp = {2013.10.01},
}

@InCollection{Maggio11probUseRealLife,
  author    = {David Maggio},
  title     = {Using Probabilistic Information in Real-Life},
  booktitle = {ERCOT, Real-Time market Integration},
  publisher = {UWIG Workshop},
  year      = {2011},
  month     = feb,
  comment   = {How ERCOT uses probabilistic forecast forecasts in its operations.},
  file      = {Maggio11probUseRealLife.pdf:Maggio11probUseRealLife.pdf:PDF},
  groups    = {Use, doReadWPV_1},
  owner     = {sotterson},
  timestamp = {2013.10.01},
}

@InProceedings{Makarov08Iow,
  author    = {Makarov, Y.V and C. Loutan and J. Ma and P. de Mello and S. Lu},
  title     = {Impacts of wind generation on regulation and load following requirements in the California system},
  booktitle = {IEEE Power and Engineering Society General Meeting},
  year      = {2008},
  location  = {Pittsburgh, PA},
  month     = jul,
  groups    = {DOE-PNL09},
  owner     = {sotterson},
  timestamp = {2009.03.05},
}

@TechReport{Makarov08valRsrcTimeResp,
  author      = {Makarov, Y.V. and Jian Ma and Shuai Lu and Tony Nguen},
  title       = {Assessing the Value of Regulation Resources Based On Their Time Response Characteristics},
  institution = {Pacific Northwest National Lab},
  year        = {2008},
  number      = {PNNL Project Report PNNL-17632, Prepared for CERTS and California Energy Commission},
  month       = jun,
  groups      = {DOE-PNL09},
  owner       = {sotterson},
  timestamp   = {2009.03.05},
}

@InProceedings{Makarov08windMitigateBPA,
  author    = {Yuri V. Makarov},
  title     = {Mitigating the Operational Impacts of Wind Energy Resources in the {BPA} Control Area},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  year      = {2008},
  comment   = {Handling wind variability w/ hydro and flywheel storage; also some regulatory alogrithm following stuff, good for optimization
* 6 papers referenced that might be good for optimization work
* initially selected hydro, flywheels and batteries
-- eliminated Na-S and Ni-Cd batteries (other chemistries, like NiMH and Lithium Ion rejected earlier
* QUESTION: why was pumped storage never considered?
* seems like hydro+flywheel is adequate
* simulations w/ modeled hydro and flywheels:
-- flywheel provides regulation up service (easy to turn on?)
-- hydro for down (easy to turn off?)
-- flywheel reduces stress on hydro (do these wear out when you turn them up?)
* need to take into account interdependencies between ramp speed, capacity and ramp duration (but not well explained)
* Ramps calculated w/ swinging door algorithm ("calculated?"
* Swinging doors used here too: Crampes18flexElecMktsRamp)

See also: CAISO10caisoRnwblIntegSwngDr
},
  file      = {Makarov08windMitigateBPA.pdf:Makarov08windMitigateBPA.pdf:PDF},
  groups    = {IWFTM08, Read},
  owner     = {sotterson},
  timestamp = {2008.09.19},
  url       = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
}

@TechReport{Mandel07tutEnsembKalman,
  author      = {Jan Mandel},
  title       = {A Brief Tutorial on the Ensemble {Kalman} Filter},
  institution = {University of Colorado at Denver and Health Sciences Center, CENTER FOR COMPUTATIONAL MATHEMATICS},
  year        = {2007},
  number      = {UCDHSC/CCM Report No. 242},
  month       = feb,
  comment     = {* model combination?
* offsite observations?
* MarkS says there might be a couple typos, maybe in Bayes Theorem},
  file        = {:Mandel07tutEnsembKalman.pdf:PDF;Mandel07tutEnsembKalman.pdf:Mandel07tutEnsembKalman.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2009.01.22},
  url         = {http://www-math.cudenver.edu/ccm/reports/rep242.pdf},
}

@Book{Manly16multivarPrimrBk,
  title     = {Multivariate statistical methods: a primer},
  publisher = {CRC Press},
  year      = {2016},
  author    = {Manly, Bryan FJ and Alberto, Jorge A Navarro},
  comment   = {Easy to read reference for PCA and many other things.},
}

@Article{Markou03Noveltydetectionreviewpart,
  author    = {Markou, Markos and Singh, Sameer},
  title     = {Novelty detection: a review?part 1: statistical approaches},
  journal   = {Signal processing},
  year      = {2003},
  volume    = {83},
  number    = {12},
  pages     = {2481--2497},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2016.12.19},
}

@Article{Markou03Noveltydetectionreviewparta,
  author    = {Markou, Markos and Singh, Sameer},
  title     = {Novelty detection: a review?part 2:: neural network based approaches},
  journal   = {Signal processing},
  year      = {2003},
  volume    = {83},
  number    = {12},
  pages     = {2499--2521},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2016.12.19},
}

@Article{Mathiesen15solFrcstNWPslides,
  author    = {Patrick Mathiesen and Jan Kleissl},
  title     = {Solar Energy Forecasting Using Numerical Weather Prediction (NWP) Models},
  journal   = {Sanyo Corporation},
  year      = {2015},
  comment   = {Talks about NWP errors in solar.  Used for Colombia talk.},
  file      = {Mathiesen15solFrcstNWPslides.pptx:Mathiesen15solFrcstNWPslides.pptx:PowerPoint 2007+},
  owner     = {sotterson},
  timestamp = {2017.05.04},
  url       = {http://documents.tips/documents/solar-energy-forecasting-using-numerical-weather-prediction-nwp-models-patrick-mathiesen-sanyo-fellow-ucsd-jan-kleissl-ucsd.html},
}

@InProceedings{Mechali06wakeHorn,
  author    = {Martin M{\'e}chali and Rebecca Barthelmie and Sten Frandsen and Leo Jensen and Pierre-Elouan R??thor??},
  title     = {Wake effects at Horns Rev and their influence on energy production},
  booktitle = {European Wind Energy Conference and Exhibition (EWEC)},
  year      = {2006},
  comment   = {offshore wake measurements (no terrain confounding) powerpoints here: www.ewec2006proceedings.info/allfiles/322_Ewec2006presentation.ppt},
  file      = {Mechali06wakeHorn.pdf:Mechali06wakeHorn.pdf:PDF;Mechali06wakeHorn.pdf:Mechali06wakeHorn.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.08.28},
  url       = {http://www.dongenergy.com/NR/rdonlyres/575EFE4E-CB73-4D4D-B894-987AC9008027/0/40.pdf},
}

@InCollection{Mendez13hiDimVoronoiAdj,
  author    = {Mendez, Juan and Lorenzo, Javier},
  title     = {Computing Voronoi Adjacencies in High Dimensional Spaces by Using Linear Programming},
  booktitle = {Mathematical Methodologies in Pattern Recognition and Machine Learning},
  publisher = {Springer},
  year      = {2013},
  pages     = {33--49},
  comment   = {Possibly a way to use tesselation for high dimension KNN-based local linear regression, as is proposed for low dim (3) in Gupta08adaptLocLinKNN

I couldn't find the paper on subitio but asked Lorenzo for it on ResearchGate},
  doi       = {10.1007/978-1-4614-5076-4_3},
  owner     = {sotterson},
  timestamp = {2014.04.22},
}

@InCollection{Meng06gradientNNensAgg,
  author    = {Meng, Jiang and An, Kun},
  title     = {Aggregating Regressive Estimators: Gradient-Based Neural Network Ensemble},
  booktitle = {MICAI 2006: Advances in Artificial Intelligence},
  publisher = {Springer Berlin Heidelberg},
  year      = {2006},
  editor    = {Gelbukh, Alexander and Reyes-Garcia, CarlosAlberto},
  volume    = {4293},
  series    = {Lecture Notes in Computer Science},
  pages     = {316--326},
  isbn      = {978-3-540-49026-5},
  comment   = {continuous forecasting w/ NN ensembles, possibly like upscaling, possibly compatible with adaboost techniques},
  doi       = {10.1007/11925231_30},
  owner     = {sotterson},
  timestamp = {2014.01.21},
}

@TechReport{Merwe00UnscentedParticle,
  author      = {Rudolph van der Merwe and Arnaud Doucet and Nando de Freitas and Eric Wan},
  title       = {The Unscented Particle Filter},
  institution = {Cambridge University Engineering Department},
  year        = {2000},
  type        = {Tech report},
  number      = {CUED/F-INFENG/TR-380},
  month       = aug,
  comment     = {Extended Kalman Filter --> Unscented Kalman -> Unscented Particle Filter Implemnted in the ReBEL matlab tookit: http://choosh.csee.ogi.edu/rebel// Eric Wan is the OGI guy who Zak says invnted UKF},
  file        = {:Merwe00UnscentedParticle.pdf:PDF;Merwe00UnscentedParticle.pdf:Merwe00UnscentedParticle.pdf:PDF},
  location    = {Cambridge, England},
  owner       = {scotto},
  timestamp   = {2008.07.20},
  url         = {http://cslu.cse.ogi.edu/nsel/research/ukf.html},
}

@Article{Meyer00degFreedomShpCnstr,
  author    = {Meyer, M. and M. Woodroofe},
  title     = {On the Degrees of Freedom in Shape-Restricted Regression},
  journal   = {Annals of Statistical},
  year      = {2000},
  volume    = {28},
  number    = {28},
  pages     = {10831104},
  month     = {aug},
  comment   = {Useful for penalized spline and montonicity?  Referenced by Koenker05QuantRgrssnBook},
  doi       = {10.1214/aos/1015956708},
  file      = {Meyer00degFreedomShpCnstr.pdf:Meyer00degFreedomShpCnstr.pdf:PDF},
  publisher = {Institute of Mathematical Statistics},
  url       = {http://dx.doi.org/10.1214/aos/1015956708},
}

@Article{Meyer02naiveFrcstMethR,
  author    = {David Meyer},
  title     = {Naive Time Series Forecasting Methods},
  journal   = {R News},
  year      = {2002},
  volume    = {2},
  number    = {2},
  pages     = {7--10},
  month     = jun,
  comment   = {Nice review of simple forecasting methods in R:

* 1\textsuperscript{st} order AR
* exponential weighting
* trend
* seasonal
* increasing seasonal

There's just one coefficient to estimate for each additional trick and they do it by least squares. Confidence bounds included. These are mostly Holt Winters approaches. Really simple, and I guess, robust.

See also: Winters60exponMvAvgSalesFrcst (primary reference?)
See also: Jonsson13spotFrcstWind (use of Holt Winters)
See also: exponential weighting vs. Holt-Winters: Taylor07frcstSprMktPriceQR},
  file      = {Meyer02naiveFrcstMethR.pdf:Meyer02naiveFrcstMethR.pdf:PDF;Rnews_2002-2.pdf:Meyer02naiveFrcstMethR.pdf:PDF},
  groups    = {Read},
  owner     = {scotto},
  timestamp = {2010.08.03},
  url       = {http://CRAN.R-project.org/doc/Rnews/},
}

@Misc{Meyers19toolsPVdatSci,
  author       = {Bennet Meyers},
  title        = {Tools for PV Data Science Applied math, statistics, and signal processing for gaining insight from PV data},
  howpublished = {Technical Presentation Slides},
  month        = feb,
  year         = {2019},
  comment      = {Related to SolarAnywhere degradation.  From Patrick.},
  file         = {:Meyers19toolsPVdatSci.pdf:PDF},
}

@Conference{Milligan03statWindPowModels,
  author       = {M. Milligan and M. Schwartz and Y. Wan},
  title        = {Statistical Wind Power Forecasting Models: Results for U.S. Wind Farms},
  booktitle    = {Windpower},
  year         = {2003},
  number       = {NREL/CP-500-33956},
  month        = may,
  organization = {NREL},
  comment      = {ARMA beats persistence sometimes for 10, 60 minute wind forecasts but need ensembles or switching models},
  file         = {Milligan03statWindPowModels.pdf:Milligan03statWindPowModels.pdf:PDF;Milligan03statWindPowModels.pdf:Milligan03statWindPowModels.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2009.01.05},
}

@InProceedings{Minka00AutoPCAdim,
  author    = {Thomas P. Minka},
  title     = {Automatic Choice of Dimensionality for {PCA}},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2000},
  pages     = {598-604},
  comment   = {Bayesian choice of # of PCA dimensions model order selection criteria used (more or less) by Hansen ICA detect
* has matlab
* longer tech note: Minka00AutoPCAdimTechNote (go there for matlab too)
* Technote, maybe w/ more comments: Minka00AutoPCAdimTechNote
* See also: Pena03descSctrLinDep
* See also: Schnitzer12Localandglobal (also has matlab)
* See also: Frank11svdRankInfo
* I COPIED THIS FROM SPEAKERCLUST.BIB},
  file      = {Minka00AutoPCAdim.pdf:Minka00AutoPCAdim.pdf:PDF},
  groups    = {Read},
  owner     = {scotto},
  timestamp = {2007.06.03},
  url       = {http://vismod.media.mit.edu/tech-reports/TR-514-ABSTRACT.html},
}

@TechReport{Minka00AutoPCAdimTechNote,
  author      = {Tomas P. Minka},
  title       = {Automatic chice of dimensionality for {PCA}},
  institution = {MIT Media Laboratory, Vision and Modeling Group},
  year        = {2000},
  number      = {514},
  comment     = {Bayesian choice of # of PCA dimensions model order selection criteria used (more or less) by Hansen ICA detect

NOTE: This tech note was corrected in 2008 (see the tech note URL) but I haven't downloaded that

* published in NIPS 2000 [Minka00AutoPCAdim]
* matlab available at technote URL
* Paper: Minka00AutoPCAdim
* see also: Pena03descSctrLinDep
* See also: Schnitzer12Localandglobal (also has matlab)
* I COPIED THIS FROM SPEAKERCLUST.BIB},
  file        = {Minka00AutoPCAdimTechNote.pdf:Minka00AutoPCAdimTechNote.pdf:PDF},
  owner       = {scotto},
  timestamp   = {2007.06.05},
  url         = {http://research.microsoft.com/~minka/papers/pca/},
}

@TechReport{Minka00matrixAlg4stat,
  author      = {Thomas P. Minka},
  title       = {Old and New Matrix Algebra Useful for Statistics},
  institution = {Carnegie Mellon University},
  year        = {2000},
  comment     = {398: ReadNo
Review:
Many matrix algebra and calculus tricks; has matlab code too!

* see also: Roweis99matrixIdentities
* see also: Roweis99gaussIdentities

NOTE: was copied from speakerclust.bib},
  file        = {Minka00matrixAlg4stat.pdf:Minka00matrixAlg4stat.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2015.02.04},
  url         = {http://www.stat.cmu.edu/~minka/papers/matrix.html},
}

@TechReport{Minka01BayesLinRegress,
  author    = {Thomas P. Minka},
  title     = {{Bayes}ian Linear Regression},
  year      = {2001},
  comment   = {Priors for linear regression params and data error variance estimate; changepoint and basis regression * invariant priors (get same posteriors, regardless of data scaling (and bias?)) - Ridge regression is not invariant! - Jeffry's linear regression is "improper," whatever that means, they solve by empirical Bayes * linearity check with known variance * Use evidence for linearity to select the best nonlinear transform for a linear regression input. This is nice because the evidence is ideas of the scale of the nonlinear transform output. Same as looking for zero valued regression coefficients, except that this approach doesn't handle alpha selection. * similar trick to test for Gausianinity * priors for unknown variance: use a Jeffrey's prior, then fixed-point iterate Piecewise Regression * changepoint detection - uniform prior on where transition is - determination is mainly model fit - says switching regression is more flexible, can reoccupy model states * basis method is more general - bases are polynomial, piecewise linear, gaussian, tanh - MLP is basis function regression w/ flexible basis funcs: Why not just use them?},
  file      = {Minka01BayesLinRegress.pdf:Minka01BayesLinRegress.pdf:PDF;Minka01BayesLinRegress.pdf:Minka01BayesLinRegress.pdf:PDF},
  owner     = {scotto},
  timestamp = {2008.08.29},
}

@InBook{Mitchell17nBayesLogRegBkCh,
  chapter   = {Generative and Discriminative Classifiers: Naive Bayes and Logistic Regression},
  title     = {Machine Learning},
  publisher = {McGraw Hill},
  year      = {2017},
  author    = {Tom Mitchell},
  edition   = {2nd},
  comment   = {For a possible 2nd edition of the book "Machine Learning"  Apparently there was no 2nd edition yet in 2017, when I downloaded this.

1997 edition: Mitchell97nBayesLogRegBk

Original paper on this idea: Ng02discrimVsGenNBvsLogRgrsn},
  file      = {:Mitchell17nBayesLogRegBkCh.pdf:PDF},
  url       = {http://www.cs.cmu.edu/~tom/NewChapters.html},
}

@InBook{Mitchell97nBayesLogRegBk,
  title     = {Machine Learning},
  publisher = {McGraw Hill},
  year      = {1997},
  author    = {Tom Mitchell},
  edition   = {1st},
  comment   = {A machine learning book on some older-school models.  An updated chapter compares Naive Bayes with Logistic regression (Mitchell17nBayesLogRegBkCh)},
  file      = {:MitchellnBayesLogRegBk.pdf:PDF},
  url       = {http://www.cs.cmu.edu/~tom/NewChapters.html},
}

@TechReport{Mohrlen19selectRESfrcstRecmnd,
  author      = {Corinna M{\:o}hrlen and John Zack and Jakob W. Messner and Jethro Browell and Craig Collier and Aidan Tuohy and Justin Sharp and Gregor Giebel},
  title       = {Recommended practices for selecting renewable power forecasting solutions},
  institution = {International Energy Agency},
  year        = {2019},
  note        = {Prepared as part of the IEA Wind Task 36, WP 2.1},
  comment     = {How to pick an RES forecast.  Gregor emailed it to me.},
  file        = {:Mohrlen19selectRESfrcstRecmnd.pdf:PDF},
}

@Article{Molina11WindFarmTechnical,
  author    = {Molina, MG and Alvarez, JG},
  title     = {Wind Farm-Technical Regulations, Potential Estimation and Siting Assessment},
  journal   = {InTech: Rijeka, Croatia},
  year      = {2011},
  comment   = {Source of wind turbine image for Colombia course.  I didn't read this.},
  owner     = {sotterson},
  timestamp = {2017.05.02},
  url       = {https://www.intechopen.com/books/wind-farm-technical-regulations-potential-estimation-and-siting-assessment/technical-and-regulatory-exigencies-for-grid-connection-of-wind-generation},
}

@InProceedings{Monaldo03sarWindANSWRS,
  author    = {Andy Oldroyd},
  title     = {The Sar Measurement of Ocean Surface Winds: an Overview},
  booktitle = {Second Workshop on Coastal and Marine Applications of SAR},
  year      = {2003},
  comment   = {The white paper explaining the SAR algorithm used for NORSEWInD. (according to ANSWRS: APL/NOAA SAR Wind Retrieval System Software Documentation Version 5.0) - Year 1 NORSEWInD report (Oldroyd08norseWindRep1) says ANSWRS is the primary algorithm},
  file      = {Monaldo03sarWindANSWRS.pdf:Monaldo03sarWindANSWRS.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  timestamp = {2010.05.26},
  url       = {http://earth.esa.int/workshops/cmasar_2003/papers/E02mona.pdf},
}

@PhdThesis{Morales10phdThesis,
  author      = {Juan Miguel Morales Gonz\'{a}lez},
  title       = {Impact on System Economics and Security Economics of a High Penetration of Wind Power},
  year        = {2010},
  comment     = {Juan Mi's dissertation spinning reserves

Author name is wrong: Where does the Gonzales go in Juan MI's name?},
  file        = {Morales10phdThesis.pdf:Morales10phdThesis.pdf:PDF},
  groups      = {Test, Use, doReadNonWPV_1},
  institution = {Universidad de Castilla - La Mancha},
  owner       = {scotto},
  timestamp   = {2010.12.12},
}

@PhdThesis{Morillo13PenEstFDAphd,
  author      = {Morillo, Mar{\'\i}a del Carmen Aguilera},
  title       = {Penalized estimation methods in functional data analysis},
  year        = {2013},
  comment     = {Does PCA and PLS on spline data and compares. Author also has a paper on the comparison that I haven't bibtexted yet.

PCA paper here: Aguilera13psplinePCA},
  file        = {Morillo13PenEstFDAphd.pdf:Morillo13PenEstFDAphd.pdf:PDF},
  institution = {University of Granada},
  owner       = {sotterson},
  timestamp   = {2014.11.23},
  url         = {http://digibug.ugr.es/bitstream/10481/29831/1/2197066x.pdf},
}

@Misc{Myers12memorylessExpPropCS547,
  author       = {Daniel Myers},
  title        = {CS 547 Lecture 9: Conditional Probabilities and the Memoryless Property},
  howpublished = {Course Notes, University of Wisconsin, Madison},
  year         = {2012},
  comment      = {Explains why exponential processes are memoryless and the consequences.  This matters for queing, etc. but I was looking for this info because Poisson regression assumes the exponential process because it's memoryless ()},
  file         = {:Myers12memorylessExpPropCS547.pdf:PDF},
  url          = {http://pages.cs.wisc.edu/~dsmyers/cs547/},
}

@Book{Nabney02NetlabBook,
  title     = {NETLAB: algorithms for pattern recognition},
  publisher = {Springer-Verlag New York, Inc.},
  year      = {2002},
  author    = {Ian T. Nabney},
  isbn      = {1-85233-440-1},
  comment   = {Reference for Matlab NETLAB library Things I'm citing it for: * automatic relevance determination (ARD)},
  location  = {New York, NY, USA},
  owner     = {sotterson},
  timestamp = {2008.12.03},
}

@Article{Naeaetaenen05Memorybasedafferent,
  author    = {N{\"a}{\"a}t{\"a}nen, Risto and Jacobsen, Thomas and Winkler, Istv{\'a}n},
  title     = {Memory-based or afferent processes in mismatch negativity (MMN): A review of the evidence},
  journal   = {Psychophysiology},
  year      = {2005},
  volume    = {42},
  number    = {1},
  pages     = {25--32},
  owner     = {sotterson},
  publisher = {Wiley Online Library},
  timestamp = {2016.12.19},
}

@Article{NASA15dustMiddleEast,
  author    = {NASA},
  title     = {Dust Storm Sweeps Across Middle East},
  journal   = {NASA Earth Observatory},
  year      = {2015},
  comment   = {Nice cust storm picture.  Used for Colombia course},
  file      = {NASA15dustMiddleEast.pdf:NASA15dustMiddleEast.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.29},
  url       = {https://earthobservatory.nasa.gov/NaturalHazards/view.php?id=86571},
}

@Article{NASA16SOlarRadiationClimate,
  author    = {NASA},
  title     = {SOlar Radiation and Climate Experiment (SORCE)},
  journal   = {Web Homepage},
  year      = {2016},
  comment   = {Slide source for GIZ colombia talk},
  file      = {NASA16SOlarRadiationClimate.pdf:NASA16SOlarRadiationClimate.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.06},
  url       = {http://lasp.colorado.edu/home/sorce/},
}

@Article{Nason02WaveletPacketNonStatWind,
  author    = {Guy P. Nason and Theofanis Sapatinas},
  title     = {Wavelet packet transfer function modelling of nonstationary time series},
  journal   = {Statistics and Computing},
  year      = {2002},
  volume    = {12},
  number    = {1},
  pages     = {45--56},
  issn      = {0960-3174},
  comment   = {Wavelet Packet of offsite measurements linearly regressed to predict current site. * supposed to handle non-stationarity * include binned direction (helped a lot) * picked to p 5\% of 1022 variables by correlation, then used some kind of CART and GLM picking algorithm to come up with 5 variables, one of which was wind * wavelets capture long term oscillation (23 hours, 4.7 days...) * Seemed to work about the same as linear regression.},
  doi       = {10.1023/A:1013168221710},
  file      = {Nason02WaveletPacketNonStatWind.pdf:Nason02WaveletPacketNonStatWind.pdf:PDF;Nason02WaveletPacketNonStatWind.pdf:Nason02WaveletPacketNonStatWind.pdf:PDF},
  location  = {Hingham, MA, USA},
  owner     = {sotterson},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2008.08.21},
}

@TechReport{Nason99waveletpacketWind,
  author      = {Guy P. Nason and Theofanis Sapatinas and Andrew Sawczenko},
  title       = {Wavelet packet modelling of nonstationary wind energy time series},
  institution = {University of Bristol, Dept. of Mathematics},
  year        = {1999},
  comment     = {Tech report for Nason02WaveletPacketNonStatWind. not sure what's different},
  file        = {Nason99waveletpacketWind.pdf:Nason99waveletpacketWind.pdf:PDF;Nason99waveletpacketWind.pdf:Nason99waveletpacketWind.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2008.08.21},
  url         = {http://www.springerlink.com.offcampus.lib.washington.edu/content/m321584543u425nh/?p=bd73094f72d54ce39c89c27a5604a6ca&pi=6},
}

@Misc{Nau15smoothMAexpHolt,
  author       = {Robert F. Nau},
  title        = {Moving average and exponential smoothing models},
  howpublished = {web page course notes},
  year         = {2015},
  note         = {Fuqua School of Business. Duke University},
  comment      = {Very simple explanation of Holt exponential weighting: Holt linear exponential weighting is just exponential smoothing with separate smoothing of bias and trend. Also, has links to other simple forecasting models like ARIMA.

Main course page is:
Statistical forecasting: notes on regression and time series analysis
http://people.duke.edu/~rnau/411home.htm},
  file         = {Nau15smoothMAexpHolt.pdf:Nau15smoothMAexpHolt.pdf:PDF},
  url          = {http://people.duke.edu/~rnau/411avg.htm},
}

@TechReport{Nelson19mnEfcnyPot2020apdxA,
  author      = {Carl Nelson and others},
  title       = {Minnesota Energy Efficiency Potential Study: 2020–2029 Appendix A},
  institution = {Minnesota Department of Commerce, Division of Energy Resources},
  year        = {2019},
  type        = {techreport},
  number      = {Contract # 121430},
  month       = mar,
  comment     = {Appendix A (March 2019) describes the method Josh Quinnel will use to synthesize load profiles for the MN Solar Pathways project.},
  file        = {:Nelson19mnEfcnyPot2020apdxA.pdf:PDF},
  url         = {https://www.mncee.org/mnpotentialstudy/home/},
}

@TechReport{NERC08accomHiVarGen,
  author      = {NERC},
  title       = {Accommodating High Levels of Variable generation},
  institution = {North American Electric Reliability Corporation (NERC)},
  year        = {2008},
  location    = {Princeton, NJ USA},
  month       = nov,
  file        = {NERC08accomHiVarGen.pdf:NERC08accomHiVarGen.pdf:PDF;NERC08accomHiVarGen.pdf:NERC08accomHiVarGen.pdf:PDF},
  groups      = {DOE-PNL09},
  owner       = {sotterson},
  timestamp   = {2009.03.03},
}

@Article{Newport17introSolarRad,
  author    = {Newport},
  title     = {Introduction to Solar Radiation},
  journal   = {Web Publication},
  year      = {2017},
  comment   = {Sun background:

Fraunhofer Lines!  Sun looks like a 5800K blackbody, has find struction due to absorbtion of cooler outer Gas:  Fraunofer lines.

*  the mean earth/sun distance of 149,597,890 km
* irradiance of sun ? 1 AU -- is called the solar constant.
 --  is the total integrated irradiance over the entire spectrum
 -- Currently accepted values are about 1360-1367 W m'2.
 -- changes 6.6% of due to variation of earth-to-sun distance

* straight down (zero zenith) loss due to atmosphere:
   (1367-1050)/1367=23% direct beam
   (1367-1120)/1367=18% horizontal

* Air mass #'s:
With the sun overhead, direct radiation that reaches the ground passes straight through the entire atmosphere, all
of the air mass, overhead. We call this radiation "Air Mass 1 Direct" (AM 1D (/p/81389)) radiation, and for
standardization purposes we use a sea level reference site. The global radiation with the sun overhead is similarly
called "Air Mass 1 Global" (AM 16) radiation. Because it passes through no air mass, the extraterrestrial spectrum
is called the "Air Mass 0" spectrum.},
  file      = {Newport17introSolarRad.pdf:Newport17introSolarRad.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.24},
  url       = {https://www.newport.com/t/introduction-to-solar-radiation},
}

@InProceedings{Ng02specClust,
  author    = {Andrew Y. Ng and Michael I. Jordan and Yair Weiss and others},
  title     = {On Spectral Clustering: Analysis and an algorithm},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2002},
  volume    = {14},
  pages     = {849--856},
  publisher = {MIT; 1998},
  comment   = {The impressive "few lines of matlab" clustering algorithm seen at the UW CS colloquium during my PhD

* picking num. clusters: maybe Xiaoyan08spectClustSelfAdapt
* combining multiple kmens runs to get one spectral clustering answer: Strehl03clustEnsCombine

* There is a redundant entry in speakerclust.bib for it
* google scholar and others say this was published in 2002. Others say 2001, but I'll stick w/ 2002},
  file      = {Ng02specClust.pdf:Ng02specClust.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.11.25},
  url       = {http://citeseer.nj.nec.com/ng01spectral.html},
}

@InProceedings{Ng16nutsBoltsDeepLrn,
  author    = {Andrew Ng},
  title     = {Nuts and Bolts of Applying Deep Learning},
  booktitle = {Deep Learning School},
  year      = {2016},
  address   = {Stanford, CA, USA},
  month     = sep,
  comment   = {Excellent YouTube tutorial on trend and practical tricks of Deep Learning.

Video:
https://www.youtube.com/watch?v=F1ka6a13S9I

My Notes:
https://onedrive.live.com/edit.aspx/work/IWES?cid=4bbd96b3698748f8&id=documents&wd=target%28Tutorials.one%7C93389E1B-FCE1-4F63-9068-1B49828D94D3%2FDeep%20Learning%7C77BDD301-A413-4EF5-AF2C-D6C489C96B7D%2F%29},
  journal   = {Deep Learning School},
  owner     = {sotterson},
  timestamp = {2017.01.27},
  url       = {https://www.bayareadlschool.org/},
}

@InProceedings{Nielsen06shortWindAdvStatMeth,
  author    = {T.S. Nielsen and H. Madsen and H. Aa. Nielsen and P. Pinson and G. Kariniotakis and N. Siebert and I. Marti and M. Lange and U. Focken and L. von Bremen and P. Louka and G. Kallos and G. Galanis},
  title     = {Short-term Wind Power Forecasting Using Advanced Statistical Methods},
  booktitle = {European Wind Energy Conference and Exhibition (EWEC)},
  year      = {2006},
  comment   = {Kalman improves long distance (>24 hr) forecasts. Somehow incorporates nonlinear dynamics},
  file      = {:Nielsen06shortWindAdvStatMeth.pdf:PDF;Nielsen06shortWindAdvStatMeth.pdf:Nielsen06shortWindAdvStatMeth.pdf:PDF},
  owner     = {scotto},
  timestamp = {2008.07.06},
  url       = {http://anemos.cma.fr/download/publications/pub_2006_paper_EWEC06_WP3statistical.pdf},
}

@TechReport{Nielsen07aprioriPowCurvInit,
  author      = {T. S. Nielsen and H. A. Nielsen and G. Giebel},
  title       = {Initialisation of power curve models using \`{a} priori information},
  institution = {Informatics and Mathematical Modelling, Technical University of Denmark, {DTU}},
  year        = {2007},
  comment     = {Power curver learning? Need to ask author for pdf.},
  location    = {Richard Petersens Plads, Building 321, {DK-}2800 Kgs. Lyngby},
  owner       = {sotterson},
  series      = {IMM-Technical Report-2007-13},
  timestamp   = {2009.01.05},
  url         = {http://www2.imm.dtu.dk/pubdb/p.php?5329},
}

@Article{NREL17ntnlRenEnrgyLab,
  author    = {NREL},
  title     = {National Renewable Energy Laboratory},
  journal   = {US Dept. of Energy},
  year      = {2017},
  comment   = {For the GIZ Colombia class, I used at least the PV efficiency graph, attached.},
  file      = {NREL17ntnlRenEnrgyLab_PVmoduleEff2017.jpg:NREL17ntnlRenEnrgyLab_PVmoduleEff2017.jpg:JPG image},
  owner     = {sotterson},
  timestamp = {2017.04.27},
  url       = {https://www.nrel.gov/pv/national-center-for-photovoltaics.html},
}

@Misc{Oldroyd07norsewindProp,
  author       = {Andrew Oldroyd},
  title        = {Northern Seas Wind Index Database, Part B},
  howpublished = {Proposal},
  year         = {2007},
  comment      = {Project description for NORSEWInD The dates on this one are obsolete (too early). Original file came from Henrik, named: NORSEWInD_Parb_B_final.pdf (maybe I added the NORSEWInD part of the name)},
  file         = {Oldroyd07norsewindProp.pdf:Oldroyd07norsewindProp.pdf:PDF},
  groups       = {Read},
  owner        = {scot},
  timestamp    = {2010.05.26},
}

@TechReport{Oldroyd08norseWindRep1,
  author      = {Andy Oldroyd},
  title       = {{NOR}SEWInD Year 1 Periodic Report},
  institution = {Oldbaum Services Limited},
  year        = {2008},
  month       = aug,
  comment     = {First summary report for NORSEWInD 8/2008-7/2009 * NWP, instruments (LIDAR,satellite,SODAR) seems to be a project focus * LIDAR acceptance tests done in 2009; s/ have data for them * SODAR acceptance tests s/b done in 2010 (so not much SODAR available in 2010) * oil rig flow simulations are done; will influcence corrections for oil rig instrument installations * Offshore data started in October 2009 * Wind atlas has 2 kind of resolution: more detail where likely wind farms (near shore, shallow water) * database designed to make post processing traceable; raw data always available -- store Natural Power ZephIR LiDAR, Leosphere, WindCUBE LiDAR, AQ Systems AQ500 SoDAR, Met mast and satellite data. -- however, it's not clear that NWP will be stored in this database (p. 26); just the direct measurments Forecasting * projected uses Danish data from energinet.dk -- 15 zones covering DK -- (later Pierre tells me there are 400 spatial measurements in here but need separate permissions to get it?) -- has (p. 33) met forecasts, wind power forecasts, measurements -- CP-VAR gets spatio-temporal relations in addition to those found by NWP -- model dynamics conditioned by prevailing wind direction -- plan to go beyond VAR to include external measurements Satellite data * prime source isn Envisat ASAR, wide swath, vertical xmit/rcv (VV mode) * Riso and CLS both collect and process -- why 2 is not described -- their different algorithms are described * QuikSCAT data available -- potential validator, direction indicator -- lower resolution 25kx25x or 12.5kx12.5k vs. 1kx1k for SAR Met Mast -- manual QC!! -- stability params calculated -- but I think this data stream died in 1999},
  file        = {Oldroyd08norseWindRep1.pdf:Oldroyd08norseWindRep1.pdf:PDF},
  groups      = {Read},
  owner       = {scot},
  timestamp   = {2010.05.26},
  url         = {https://storage.planzone.com/storage/get/63396620},
}

@InProceedings{OMalley08allIslandGridStudy,
  author    = {Mark O'Malley},
  title     = {All-Island Grid Study \& Some Research Results},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  year      = {2008},
  comment   = {Cost feasibility of more UK renewables, stochastic unit committment w.r.t wind forecasts, maybe relevant to optimization
* relevant paper is cited
* up to 42\% renewables is feasible by 2020
-- 25\% C02 reduction
-- only 7\% extra cost over other scenarios (5 evaluated)
*wind is cheapest renewable at current volume and will be for high volumes too (graph, slide 11)
* Study takes into account a bunch of factors
-- reserves needed for RE variability
-- pumped storage
-- transmission lines
-- forecasting errors
-- new thermal old style generation
* many refinements suggested

Stochastic Unit Committment using Wind Forecasts
* "obvious step for large wind penetrations"
* need range of forecasts (or REALLY, good error bounds?)
* improves reliability, lowers costs
* methodology not explained in slides: need to go to paper

Role of forecasting and energy storage (in conclusions)
* says improved forecasting dosen't provide significant benefit?!?!! (contradicts many other studies)
* also says storage doesn't provide sig. benefit
* could this be an Irelend specific effect, or due to the fact that they added a ton of transmission?},
  file      = {OMalley08allIslandGridStudy.pdf:OMalley08allIslandGridStudy.pdf:PDF},
  groups    = {IWFTM08, Read, Use, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2008.09.19},
  url       = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
}

@Article{Opgen-Rhein07corr2caus,
  author    = {Opgen-Rhein, R. and Strimmer, K.},
  title     = {From correlation to causation networks: a simple approximate learning algorithm and its application to high-dimensional plant gene expression data},
  journal   = {BMC Systems Biology},
  year      = {2007},
  volume    = {1},
  number    = {1},
  pages     = {37},
  comment   = {Explains correlation shrinkage linear regression used by fitlm() in R package "care" - the care docs reference this paper - also expains wjy linear regresson coeff is ratio of partial var and cor (eq. 1)},
  doi       = {10.1186/1752-0509-1-37},
  file      = {Opgen-Rhein07corr2caus.pdf:Opgen-Rhein07corr2caus.pdf:PDF},
  owner     = {scot},
  publisher = {BioMed Central Ltd},
  timestamp = {2010.10.21},
  url       = {http://www.biomedcentral.com/1752-0509/1/37},
}

@Conference{Otterson10rampOffSite,
  author    = {Scott D. Otterson AND E. P. Grimit AND Andy W. Wood},
  title     = {Short-term wind power ramp forecasting using statistical and machine-learning techniques and off-site observations},
  booktitle = {Probability and Statistics in the Atmospheric Sciences},
  year      = {2010},
  comment   = {Some of my old forecating work at 3tier plus older ramp stuff author order came from web site but andy flipped it in the slides},
  file      = {slides:Otterson10rampOffSite.pdf:PDF},
  owner     = {scot},
  timestamp = {2010.09.04},
  url       = {http://ams.confex.com/ams/90annual/techprogram/programexpanded_573.htm},
}

@Article{Otterson12wndPowFrcstOffsiteObs,
  author    = {Scott Otterson},
  title     = {Wind power forecasting with a large number of unreliable offsite observations: Feature selection and missing data treatment},
  journal   = {Norcowe Offshore Wind Conference},
  year      = {2012},
  comment   = {My slides for the Norsewind work.},
  owner     = {sotterson},
  timestamp = {2017.05.02},
}

@Misc{Padoan083dHarmLect2,
  author       = {Paolo Padoan},
  title        = {{Laplace}'s equation in {3D} spherical and {3D} cylindrical geometries},
  howpublished = {Lecture 2 Physics 105B: Mathematical and Computational Physics},
  year         = {2008},
  comment      = {has equs for 3d cylindrical harmonics use for lagged wind speed basis? but is cylindrical symmetry here what I want? also see Sernelius10laplacePoisson (I think w/o cylindrical symmetry)},
  file         = {Padoan083dHarmLect2.pdf:Padoan083dHarmLect2.pdf:PDF},
  owner        = {scotto},
  timestamp    = {2010.08.19},
  url          = {http://cass246.ucsd.edu/~ppadoan/new_website/PHYSICS_105B_2008/phys105B.php},
}

@Article{Papana08EvalMutInfoDynSys,
  author    = {A. Papana and D. Kugiumtzis},
  title     = {Evaluation of mutual information estimators on nonlinear dynamic systems},
  journal   = {Nonlinear Phenomena In Complex Systems},
  year      = {2008},
  volume    = {11},
  pages     = {225},
  comment   = {Mut info calc for lag determination: k-nearest neighbors is best
.  Subsequently uses it to compute partial MI.

Compares, among others:
* Moddemeijer (what I used for my thesis)
* Fraser86indepCoordMutInfo
* says best is k-nearest neighbors (Kraskov04EstMutInfKNN)
-- varies least w/ choice of k compared to change w/ other estimator free parameters
-- apparently, must add some noise for this test case, though

Interesting facts about mutual information:
* almost always positively biased
* increases w/ finer partition (of distribution estimate, I think)

* increases w/ sample size
* decreases w/ noise level (no surprise)
* increases when k decreases (for KNN-MI)
* large k yields poor MI est for small data (KNN-MI
-- OPPOSITE of recommendation in Stogbauer04leastDepMutInfo, which says small k for lots of data!
-- however, in this paper k goes from 2 to 64; while in Stogbauer04leastDepMutInfo k=
* est. is more stable w/ increasing noise level (huh?)
* inconsistent for all estimators tested on these nonlinear relationships


Comparisons:
* adaptive parititioning of 2D plan had been considered the most efficient and accurate
* maybe k-means is, after this study
-- estimate varies least w/ free parameer (K in KNN)
-- is more computationally effective for fine partitioing b/c of data structures (probably an efficient knn implementation)
* fixed bin histogram seem bad

Partial Mutual Information
* the authors modified the algorithm to make PMI: Kugiumtzis13directCplngPMI

Related: Kraskov04EstMutInfKNN,Stogbauer04leastDepMutInfo
},
  file      = {Papana08EvalMutInfoDynSys.pdf:Papana08EvalMutInfoDynSys.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.10.10},
  url       = {http://www.citebase.org/abstract?id=oai:arXiv.org:0809.2149},
}

@TechReport{Parks11windFrcstXcelCAR,
  author      = {Keith Parks and Yih-Huei Wan and Gerry Wiener and Yubao Liu},
  title       = {Wind Energy Forecasting: A Collaboration of the National Center for Atmospheric Research (NCAR) and Xcel Energy},
  institution = {Xcel Energy, National Renewable Energy Laboratory (NCAR), and University Corporation for Atmospheric Research (UCAR)},
  year        = {2011},
  number      = {NREL/SR-5500-52233},
  month       = oct,
  comment     = {35\% day ahead forecast improvement w/ turbine data, keeping forecast in wind domain and converting to power later. Also, from here: http://www.energycentral.com/functional/news/news_detail.cfm?did=22325836 "The system gets its accuracy in part by tracking winds closer to the height of the turbine hubs, instead of at ground level where most forecasting models get their information. The new system also uses advanced computer models that crunch information from satellites, aircraft, weather radar and other sources." This article suggests that it works on 15 minute invervals too: http://www.energycentral.com/functional/news/news_detail.cfm?did=22329319 and also that up to 30 minute phase errors originating in global models are still a big problem.},
  file        = {Parks11windFrcstXcelCAR.pdf:Parks11windFrcstXcelCAR.pdf:PDF},
  location    = {Boulder, Colorado},
  owner       = {sotterson},
  timestamp   = {2011.11.17},
  url         = {http://www.nrel.gov/wind/systemsintegration/pdfs/2011/wan_wind_energy_forecasting.pdf},
}

@Article{Parra96Statisticalindependencenovelty,
  author    = {Parra, Lucas and Deco, Gustavo and Miesbach, Stefan},
  title     = {Statistical independence and novelty detection with information preserving nonlinear maps},
  journal   = {Neural Computation},
  year      = {1996},
  volume    = {8},
  number    = {2},
  pages     = {260--269},
  owner     = {sotterson},
  publisher = {MIT Press},
  timestamp = {2016.12.19},
}

@InProceedings{Pease09rampTrackBPA,
  author    = {John Pease},
  title     = {"Critical Short-term Forecasting Needs for Large and Unscheduled Wind Energy on the {BP}A System" {BP}A Wind Ramp Event Tracking System},
  booktitle = {Workshop on the Best Practice in the Use of Short-term Forecasting of Wind Power},
  year      = {2009},
  publisher = {Bonneville Power Administration},
  comment   = {Has BPA definition for ramps, also why ramps are important.},
  file      = {Pease09rampTrackBPA.pdf:Pease09rampTrackBPA.pdf:PDF},
  groups    = {Use, doReadNonWPV_1},
  owner     = {scot},
  timestamp = {2010.06.09},
  url       = {http://powwow.risoe.dk/publ/JPease_(BPA)-BPAWindRampEventTrackingSystem_BestPracticeSTP-3_2009.pdf},
}

@Article{Peinke2007shortWindLocMeas,
  author    = {Joachim Peinke and Peter Schaumann and Stephan Barth},
  title     = {Short Time Prediction of Wind Speeds from Local Measurements},
  journal   = {Wind Energy},
  year      = {2007},
  pages     = {93--98},
  comment   = {The PDF seems broken},
  doi       = {10.1007/978-3-540-33866-6_16},
  file      = {Peinke2007shortWindLocMeas.pdf:Peinke2007shortWindLocMeas.pdf:PDF;Peinke2007shortWindLocMeas.pdf:Peinke2007shortWindLocMeas.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.07.03},
  url       = {http://www.springerlink.com/content/r64u6xg11856g57n/},
}

@Article{peng05featSelMutInfo,
  author    = {Hanchuan Peng and Fuhui Long and Ding, C.},
  title     = {Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy},
  journal   = {Transactions on Pattern Analysis and Machine Intelligence},
  year      = {Aug. 2005},
  volume    = {27},
  number    = {8},
  pages     = {1226-1238},
  issn      = {0162-8828},
  comment   = {NOTE: I COPIED THIS FROM SPEAKERCLUST.BIB AND ./PAPERS.SPEAKERCLUST
Use to select good channel pairs?,

 Heuristics used in this paper are generalized inBrown12condLikInfoFeatSel (energy.bib)

 Yang09featSelMLPrand in energy.bib might be better?

has matlab here:
http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=14608&objectType=File, other version, including C/C++ here: http://research.janelia.org/peng/proj/mRMR/index.htm

Implemented in R in the parmigene package, mrnet function:
http://www.inside-r.org/packages/cran/parmigene/docs/mrnet

kind of like Alfonso10monMutInfoOpt but not exactly (?)


Schaffernicht09residMutInfoFeatSel: a way to avoid the redundancy subtraction, which is a bit of a hack},
  doi       = {10.1109/TPAMI.2005.159},
  file      = {:peng05featSelMutInfo.pdf:PDF},
  keywords  = { feature extraction, pattern classification, statistical analysis arrhythmia, cancer cell lines, first-order incremental feature selection, handwritten digits, linear discriminate analysis, lymphoma tissues, maximal statistical dependency criterion, minimal-redundancy-maximal-relevance criterion, mutual information criteria, naive Bayes, pattern classification systems, support vector machine},
  owner     = {sotterson},
  timestamp = {2017.04.05},
  url       = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1453511},
}

@InCollection{Philbrick11probFrcstPlan,
  author    = {Russ Philbrick},
  title     = {Planning Applications of Probabilistic Forecasts},
  booktitle = {Polaris Systems Optimization},
  publisher = {UWIG Workshop},
  year      = {2011},
  month     = feb,
  comment   = {Claims that dynamic reserves may be as effective as stochastic methods (but I don't understand what he meant by that).

Nice consulting slide showing scenario spread vs. power system response times. Other nice visulualizations too.

Degrees of forecast uncertainty use:
* Operator situational awareness
* Dynamic procurement of ancillary services
* Direct use in dispatch engine / planning tools

Might be worth it to look up this guy's company and find out what his customers use.},
  file      = {Philbrick11probFrcstPlan.pdf:Philbrick11probFrcstPlan.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.01},
}

@Article{Phung09lrnImbalSets,
  author    = {Phung, Son Lam and Bouzerdoum, Abdesselam and Nguyen, Giang Hoang},
  title     = {Learning pattern classification tasks with imbalanced data sets},
  year      = {2009},
  comment   = {Friendly overview of techniques for classification learning w/ imbalanced sets. Maybe useful for learning extreme events?},
  file      = {Phung09lrnImbalSets.pdf:Phung09lrnImbalSets.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.04.04},
  url       = {http://ro.uow.edu.au/infopapers/792/},
}

@InBook{Pidwirny14FndmtlsPhyslGeogCh7p,
  chapter   = {CHAPTER 7: Introduction to the Atmosphere (p). Global Scale Circulation of the Atmosphere},
  title     = {Fundamentals of Physical Geography (2nd Ed.)},
  publisher = {University of British Columbia Okanagan},
  year      = {2014},
  author    = {Michael Pidwirny and Scott Jones},
  comment   = {Good info on coriolis, monsoon, etc.  Use for Colombia course.

PUT PDF on SP4 desktop in here.},
  owner     = {sotterson},
  timestamp = {2017.04.30},
  url       = {http://www.physicalgeography.net/fundamentals/7p.html},
}

@Misc{Pillow16mathNeurTlsSlides,
  author       = {Jonathan Pillow},
  title        = {Mathematical Tools for Neuroscience NEU 314, Spring 2016},
  howpublished = {Course Notes},
  month        = feb,
  year         = {2016},
  comment      = {Relation between General Linear Model (GLM, old) and Generalized Linear Model (GLM, newish, what people usually mean) and Logistic and Poisson Regression (subclasses of Generalized Linear Models).  And many other things....},
  file         = {:Pillow16mathNeurTlsSlides.pdf:PDF},
  url          = {http://pillowlab.princeton.edu/teaching/mathtools16/},
}

@Article{Pinson13cwcNotProper,
  author    = {Pierre Pinson and Julija Tastu},
  title     = {Proper Evaluation of Neural Network and Learning Systems based Prediction Intervals},
  journal   = {IEEE Transactions on neurla networks and learning systems},
  year      = {2013},
  comment   = {Pierre says that the Coverage Width-based Criterion (CWC) is not proper, and therefore a poor probabilsitic forecast performance metric. Submitted but not yet accepted.},
  file      = {Pinson13cwcNotProper.pdf:Pinson13cwcNotProper.pdf:PDF},
  groups    = {Test, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.29},
  url       = {http://pierrepinson.com/docs/pinsontastu_properscore.pdf},
}

@Article{Potter2008_WE,
  author               = {Potter, Cameron W. and Lew, Debra and Mccaa, Jim and Cheng, Sam and Eichelberger, Scott and Grimit, Eric},
  title                = {Creating the Dataset for the Western Wind and Solar Integration Study (U.S.A.)},
  year                 = {2008},
  volume               = {32},
  number               = {4},
  month                = jun,
  pages                = {325--338},
  issn                 = {0309-524X},
  url                  = {http://www.ingentaconnect.com/content/mscp/wind/2008/00000032/00000004/art00001},
  citeulike-article-id = {3472776},
  journal              = {Wind Engineering},
  owner                = {egrimit},
  posted-at            = {2008-11-02 02:21:22},
  publisher            = {Multi-Science Publishing Co Ltd},
  timestamp            = {2009.03.04},
}

@Misc{Premraj17coOccrAlarmsWindTurb,
  author       = {Rahul Premraj and Michael Wigbers},
  title        = {Discovering co-occurring alarms in turbines using data analytics},
  howpublished = {Presentation Slides},
  month        = mar,
  year         = {2017},
  comment      = {The wind turbine fault alarm slides that Alex Lutz told me about.  Idea is to notice which alarms precede failures using :market basket analysis, whatever that is.},
  file         = {:Premraj17coOccrAlarmsWindTurb.pdf:PDF},
}

@Book{Rachev91probMetStochMod,
  title     = {Probability Metrics and the Stability of Stochastic Models.},
  publisher = {Wiley},
  year      = {1991},
  author    = {Rachev, S.T.},
  editor    = {Chichester},
  comment   = {Kantorovich distance reference from Growe-Kuska03scenarioRedTree. Could be a Euclidean norm.},
  owner     = {scotto},
  timestamp = {2010.12.12},
}

@Unpublished{Radovanovic14hubNNgraphTalk,
  author    = {Radovanovi{\'c}, Milo{\v{s}}},
  title     = {Hubs in Nearest-Neighbor Graphs: Origins, Applications and Challenges},
  note      = {Talk given at the Institute of Mathematical Sciences, Tokyo},
  month     = mar,
  year      = {2014},
  comment   = {Overview of current hub research. Interesting that they're now planning to use hubs in clustering, not just get rid of them.},
  file      = {Radovanovic14hubNNgraphTalk.ppt:Radovanovic14hubNNgraphTalk.ppt:PowerPoint},
  owner     = {sotterson},
  publisher = {The},
  timestamp = {2014.04.26},
  url       = {http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=6&ved=0CFQQFjAF&url=http%3A%2F%2Fwww.ism.ac.jp%2F~fukumizu%2FCoopMathWS2014%2FRadovanovic-HubsInNNGraphs.ppt&ei=zOlbU4nwA4KyPPWegJAO&usg=AFQjCNF8jtulzzZY_sWasVIqwr6oo3AEMA&cad=rja},
}

@Article{Raiko15supUnsupCombLadder,
  author    = {Tapani Raiko},
  title     = {Combining Supervised and Unsupervised Learning (and the Ladder Network},
  journal   = {Deep Learning summer school, Aalto University, Finland},
  year      = {2015},
  comment   = {Tutorial slides on ladder networks.


Code:
Boney16introSemiSupLadderBlog lists a couple sources for code},
  file      = {Raiko15supUnsupCombLadder.pdf:Raiko15supUnsupCombLadder.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.02.02},
  url       = {https://users.ics.aalto.fi/praiko/summerschool/supervised-unsupervised-ladder.pdf},
}

@Misc{Ramanathan14PPfraudDetDeepLrn,
  author       = {Venkatesh Ramanathan},
  title        = {PayPal's Fraud Detection with Deep Learning},
  howpublished = {H2O World},
  month        = nov,
  year         = {2014},
  comment      = {Slides about Pay Pals suspicious transaction detector.  Very practical: how big it was, etc.},
  file         = {Ramanathan14PPfraudDetDeepLrn.pdf:Ramanathan14PPfraudDetDeepLrn.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2017.01.26},
  url          = {http://www.slideshare.net/0xdata/paypal-fraud-detection-with-deep-learning-in-h2o-presentationh2oworld2014},
}

@Unpublished{Ramsay08funcAnalNotes,
  author    = {J. O. Ramsay},
  title     = {Notes on Functional data analysis},
  month     = sep,
  year      = {2008},
  comment   = {Functional analysis workshop notes - explains how to register curves by minimizing amplitude and phase error },
  file      = {Ramsay08funcAnalNotes.pdf:Ramsay08funcAnalNotes.pdf:PDF},
  groups    = {Read},
  owner     = {scot},
  timestamp = {2010.11.05},
  url       = {http://gbi.agrsci.dk/~shd/public/FDA2008/},
}

@Article{Ranganathan15oddsVsRisk,
  author    = {Ranganathan, Priya and Aggarwal, Rakesh and Pramesh, CS},
  title     = {Common pitfalls in statistical analysis: Odds versus risk},
  journal   = {Perspectives in clinical research},
  year      = {2015},
  volume    = {6},
  number    = {4},
  pages     = {222},
  comment   = {Risk: prob(event): numEvents/numPopulation                       
Odds: prob(event1)/prob(not event1)                              
Risk Ratio (Relative Risk): Risk(event,group1)/Risk(event,group2)
Odds Ratio (Relative Risk): Odds(event,group1)/Odds(event,group2)

Must use odds ratio when using Restrospective (Case-Control) studies 

Also Essoe14oddsProbRisk},
  file      = {:Ranganathan15oddsVsRisk.pdf:PDF},
  publisher = {Wolters Kluwer--Medknow Publications},
  url       = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4640017/},
}

@Article{Rebours07survCtlAncSrvII,
  author    = {Rebours, Y.G. and Kirschen, D.S. and Trotignon, M. and Rossignol, S.},
  title     = {A survey of frequency and voltage control ancillary services -- Part II: Economic features},
  journal   = {Power Systems, IEEE Transactions on},
  year      = {2007},
  volume    = {22},
  number    = {1},
  pages     = {358--366},
  comment   = {Also see: Rebours07survVoltAncSrvc_I},
  file      = {Rebours07survCtlAncSrv_II.pdf:Rebours07survCtlAncSrv_II.pdf:PDF},
  groups    = {Use, doReadNonWPV_2},
  owner     = {sotterson},
  publisher = {IEEE},
  timestamp = {2012.06.04},
}

@InBook{Refaeilzadeh09crossVal,
  chapter   = {Cross Validation},
  title     = {Encyclopedia of database systems},
  publisher = {Springer},
  year      = {2009},
  author    = {Payam Refaeilzadeh and Lei Tang and Huan Liu},
  editor    = {Liu, L. and {\"O}zsu, M.T.},
  comment   = {Heuristic argument for why people use 10-fold cross validation and not 20-fold, etc.},
  file      = {Refaeilzadeh09crossVal.pdf:Refaeilzadeh09crossVal.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2012.04.24},
  url       = {http://www.public.asu.edu/~ltang9/papers/ency-cross-validation.pdf},
}

@Article{Rizzo17RegentsEarthScience,
  author    = {William Rizzo},
  title     = {Atmospheric Processes},
  journal   = {New York State Earth Science Regents Examination},
  year      = {2017},
  comment   = {A study guide for NY State Earth Science Exam.  Might be 8th grade level (https://www.whiteplainspublicschools.org/Domain/1535)?

The Climate and Weather Sections were relevant.

NIce picture of global circulation, Hadley cells, etc.  I used this in GIZ Colombia course.
https://www.google.de/search?q=global+wind+system&tbm=isch&imgil=gV3vxgRqA-poGM%253A%253BHJ77QFT3S4D5qM%253Bhttps%25253A%25252F%25252Fwww.slideshare.net%25252Fgbbantayearth%25252Fair-masses-global-winds&source=iu&pf=m&fir=gV3vxgRqA-poGM%253A%252CHJ77QFT3S4D5qM%252C_&usg=__nckgb0uBqEDW4ZcLdjS4u3rzmoY%3D&biw=1745&bih=898&ved=0ahUKEwj228vgt73TAhVib5oKHeIfCicQyjcIfA&ei=ZR3-WLaVMuLe6QTiv6i4Ag&gws_rd=cr#gws_rd=cr&imgdii=VemkWPo-FzEdvM:&imgrc=8fUH42meoGg-4M:},
  owner     = {sotterson},
  timestamp = {2017.04.25},
  url       = {http://www.regents-earthscience.com/atmospheric-processes.html},
}

@Misc{Roark10rampFrcstPressRls,
  author       = {Dottie Roark},
  title        = {ERCOT Using New Forecasting Tool to Prepare for Wind Variability},
  howpublished = {Press Release},
  month        = mar,
  year         = {2010},
  comment      = {AWS ramp alarm tool description, not clear how it's used
* 6 hour ahead
* predicts large change in power
* descibes likely weather cause
* start time, duration, max ramp rate
* operator says: "For example, we can look at the program and see a graph that may indicate a 45 percent chance of a systemwide wind generation decrease of 375 MW over 15 minutes, starting between 6:45 and 7 pm,:
* not clear how this is used in the balance problem},
  file         = {Roark10rampFrcstPressRls.pdf:Roark10rampFrcstPressRls.pdf:PDF},
  groups       = {Use, doReadWPV_1},
  owner        = {scot},
  timestamp    = {2011.05.03},
  url          = {http://www.ercot.com/news/press_releases/2010/nr-03-25-10a},
}

@Article{Roberts94probabilisticresourceallocating,
  author    = {Roberts, Stephen and Tarassenko, Lionel},
  title     = {A probabilistic resource allocating network for novelty detection},
  journal   = {Neural Computation},
  year      = {1994},
  volume    = {6},
  number    = {2},
  pages     = {270--284},
  owner     = {sotterson},
  publisher = {MIT Press},
  timestamp = {2016.12.19},
}

@Article{Rodriguez07copulaContag,
  author    = {Rodriguez, J.C.},
  title     = {Measuring financial contagion: A copula approach},
  year      = {2007},
  volume    = {14},
  number    = {3},
  pages     = {401--423},
  issn      = {0927-5398},
  journal   = {Journal of Empirical Finance},
  owner     = {scot},
  publisher = {Elsevier},
  timestamp = {2010.12.06},
}

@Article{Rosenblatt52multivTransfPIT,
  author              = {Rosenblatt, Murray},
  title               = {Remarks on a Multivariate Transformation},
  journal             = {The Annals of Mathematical Statistics},
  year                = {1952},
  volume              = {23},
  number              = {3},
  pages               = {470--472},
  issn                = {0003-4851},
  comment             = {Shows that distribution of PIT is uniform if density estimation is correct},
  copyright           = {1952, Institute of Mathematical Statistics},
  file                = {Rosenblatt52multivTransfPIT.pdf:Rosenblatt52multivTransfPIT.pdf:PDF},
  jstor_articletype   = {primary_article},
  jstor_formatteddate = {Sep., 1952},
  owner               = {sotterson},
  publisher           = {Institute of Mathematical Statistics},
  timestamp           = {2008.10.01},
  url                 = {http://www.jstor.org/stable/2236692},
}

@TechReport{Roweis99gaussIdentities,
  author      = {Sam Roweis},
  title       = {{Gauss}ian Identities},
  institution = {University of Toronto},
  year        = {1999},
  comment     = {397: ReadYes
Review:
Cheat sheet w/ many useful matrix Gaussian distribution tricks

* see also: Roweis99matrixIdentities

NOTE: was copied from speakerclust.bib},
  file        = {Roweis99gaussIdentities.pdf:Roweis99gaussIdentities.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2015.02.04},
  url         = {http://www.cs.toronto.edu/~roweis/notes/gaussid.pdf},
}

@TechReport{Roweis99matrixIdentities,
  author      = {Sam Roweis},
  title       = {Matrix Identities},
  institution = {University of Toronto},
  year        = {1999},
  comment     = {396: ReadYes
Review:
Cheat sheet w/ many useful matrix algebra and calculus tricks

* see also Minka00matrixAlg4stat
* see also Roweis99gaussIdentities

NOTE: was copied from speakerclust.bib},
  file        = {Roweis99matrixIdentities.pdf:Roweis99matrixIdentities.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2015.02.04},
  url         = {http://www.cs.toronto.edu/~roweis/notes/matrixid.pdf},
}

@Conference{Ruiz12frcstUncertTrade,
  author    = {Pablo A. Ruiz},
  title     = {Forecast Uncertainty and Trading Decision-making},
  booktitle = {Charles River Associates},
  year      = {2012},
  month     = feb,
  publisher = {UWIG Workshop on Variable Generation Forecasting Applications},
  comment   = {Friendly consultant slides on use of prob. forecasts and scenarios in power systems. Would be worth tracking down this company to find published examples of its customers using such forecasts.},
  file      = {Ruiz12frcstUncertTrade.pdf:Ruiz12frcstUncertTrade.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  location  = {Tucson, AZ, USA},
  owner     = {sotterson},
  timestamp = {2013.10.01},
}

@Book{Russell03Artificialintelligencemodern,
  title     = {Artificial intelligence: a modern approach},
  publisher = {Prentice hall Upper Saddle River},
  year      = {2003},
  author    = {Russell, Stuart Jonathan and Norvig, Peter and Canny, John F and Malik, Jitendra M and Edwards, Douglas D},
  volume    = {2},
  owner     = {sotterson},
  timestamp = {2016.12.24},
}

@Misc{Saabas15featSelStabilRFE,
  author       = {Ando Saabas},
  title        = {Selecting good features – Part IV: stability selection, RFE and everything side by side},
  howpublished = {Blog Post: Diving into data},
  month        = jan,
  year         = {2015},
  comment      = {Classifier feature selection (stability and recursive feature elimination (RFE)) in Python.  Is compared to several other techniques.

Stability (see e.g. Nogueira18featSelStbl, not sure if this is exactly the same)
* sklearn does it in randomized lasso and randomized logistics regression
* must give it a regularization parameter so need to wrap in a crossvalidation pipeline that also selects this
* maybe randomized logistic regresion contains that wrapper?
* is good for both pure featsel and generalization
* is built in to logistic regression
* for pure featsel is among the best, in author's experience

Recursive feature elimination (RFE)
* sklearn does it: 
   (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)
* a greedy wrapper, where you eliminate either the best or worst feature (not sure what best/worst means)
* feat ranking comes from this ordering
* need a stopping criteria, so I guess it, too, needs to be in a cross-validating pipeline
* RFECV function is that pipeline},
  file         = {:Saabas15featSelStabilRFE.pdf:PDF},
  url          = {https://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/},
}

@Article{Saint-Drenan09pvSysTechClass,
  author    = {Yves-Marie Saint-Drenan and Stefan Bofinger},
  title     = {PV System technology: Introduction, Class Notes},
  journal   = {University of Kassel, Institute for Solar Energy Technology},
  year      = {2009},
  comment   = {I borrowed these slides for my GIZ Colombia course.

They were originally in:   K:\Gruppen\OE_220_Energieinformatik\OE_225_Prognosen_fuer_Energiesysteme\Vorlagen_Pr?sentationen\Solar\RE2},
  owner     = {sotterson},
  timestamp = {2017.04.27},
}

@Article{Saint-Drenan14commentsGenPVpow,
  author    = {Yves-Marie Saint-Drenan and Rafael Fritz},
  title     = {Comments on: genPVpow The path from Ggh to Pout},
  journal   = {Fraunhofer IWES Design Outline},
  year      = {2014},
  comment   = {How IWES goes from radiation to estimate of output power.  From Rafael.  Include in Colombia?
Was originally called 20141030-genPVpower-DokuNotes
Papers it references are:
Skartveit98hourlydiffusefraction
Perez90ModelDayLightAvail
Klucher79Evaluationmodelspredict},
  file      = {Saint-Drenan14commentsGenPVpow.pdf:Saint-Drenan14commentsGenPVpow.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.30},
}

@InProceedings{Sandtorkai45GlobalTechI,
  author = {Am Sandtorkai and Dock},
  title  = {Global Tech I Offshore Wind GmbH},
  year   = {2045},
}

@InProceedings{Sanusi16demandWindFrcstPttrn,
  author    = {U. S. Sanusi and D. Corne},
  title     = {Improving forecast accuracy for grid demand and renewables supply with pattern-match features},
  booktitle = {2016 IEEE Symposium Series on Computational Intelligence (SSCI)},
  year      = {2016},
  pages     = {1-8},
  month     = {Dec},
  doi       = {10.1109/SSCI.2016.7849849},
  file      = {Sanusi16demandWindFrcstPttrn.pdf:Sanusi16demandWindFrcstPttrn.pdf:PDF},
  keywords  = {learning (artificial intelligence);pattern matching;power engineering computing;renewable energy sources;renewable materials;smart power grids;GHG emissions;electricity demand;energy demand data;forecast accuracy;grid demand;grid-relevant applications;machine learning;pattern matching;pattern-match features;planning;pricing;renewables availability;renewables engagement;renewables supply;short term forecasts;smart grid applications;smart grid control;staffing;time series prediction;wind-speed data;Context;Forecasting;Pattern matching;Prediction algorithms;Time series analysis;Weather forecasting;feature engineering;feature selection;forecasting;pattern matching;prediction;regression;renewables;time series},
}

@Article{Schafer05shrinkCov,
  author               = {Sch{\"a}fer, Juliane and Strimmer, Korbinian},
  title                = {A Shrinkage Approach to Large-Scale Covariance Matrix Estimation and Implications for Functional Genomics},
  journal              = {Statistical Applications in Genetics and Molecular Biology},
  year                 = {2005},
  volume               = {4},
  number               = {1},
  citeulike-article-id = {2863723},
  citeulike-linkout-0  = {http://www.bepress.com/sagmb/vol4/iss1/art32/},
  comment              = {The partial correlation method implemented in R corpcor pkh; better than some lasso methods
* covariance shrinkage
* on average, it's better on genome stuff in Kramer09gaussGraphModelsLasso,
* combine w/ Markov constraints in Deng09largeGausCovMarkov ?},
  file                 = {Schafer05shrinkCov.pdf:Schafer05shrinkCov.pdf:PDF},
  keywords             = {file-import-08-06-05},
  owner                = {scot},
  posted-at            = {2008-06-05 09:27:49},
  timestamp            = {2010.08.02},
  url                  = {http://www.bepress.com/sagmb/vol4/iss1/art32/},
}

@InProceedings{Schelter04tVarCausMultiVar,
  author    = {B Schelter and M Winterhalder and J Timmer},
  title     = {Time varying causal influences in multivariate time series},
  booktitle = {Workshop: Recent advances in time series analysis},
  year      = {2004},
  comment   = {Nonstationary, noise robust dependence w/ extended Kalman filter (EKF) estimating MAR (VAR?) in partial directed coherence coeffs
* could be useful for feature selection, esp. offsite observattion? Maybe regime detection/clusteriing? * Partial directed coherence: -- Weiner-like cross-spectral normalized Fourier transform of the time domain filter coefficients (which are derived somehow. Here, they propose a Kalman filter, which is kind of neat). -- Unlike with partial correlation, the correlation of other variables is not subtracted but is divided out. * EKF allows time varying param and noise estimation. -- My Note: can do better than EKF e.g. unscented Kalman filter? * nice graphs on toy example showing time varying coherence estimates * Significance testing is missing -- Significance testing is in Ludwig06*? -- At least, they are in reference 38 of Winterhalder05*},
  file      = {Schelter04tVarCausMultiVar.pdf:Schelter04tVarCausMultiVar.pdf:PDF;Schelter04tVarCausMultiVar.pdf:Schelter04tVarCausMultiVar.pdf:PDF},
  groups    = {Read},
  owner     = {sotterson},
  timestamp = {2009.03.16},
  url       = {http://www.fdm.uni-freiburg.de/people/schelter/Publications/AbstractZypern2004BS.pdf},
}

@Misc{Seppa08realtimeRenewRating,
  author       = {Tapani O. Seppa},
  title        = {Multi-Area Real-Time Rating Study: Implications re. contingency management, reliability and renewable integration},
  howpublished = {Presentation},
  month        = dec,
  year         = {2008},
  note         = {CIEE - PIER TRP PAC Meeting Presentations ? December 4, 2008},
  comment      = {How much dynamic thermal rating can increase wind power transmission.},
  file         = {Seppa08realtimeRenewRating.pdf:Seppa08realtimeRenewRating.pdf:PDF;Seppa08realtimeRenewRating.pdf:Seppa08realtimeRenewRating.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2009.02.24},
  url          = {http://ciee.ucop.edu/piertrans/documents/trpac120408.pdf},
}

@Misc{Sernelius10laplacePoisson,
  author       = {Bo E. Sernelius},
  title        = {{Laplace}'s and Poisson's equations},
  howpublished = {Course Notes Lecture 4 TFYY67, Electrodynamics Link?ping University, Department of Physics, Chemistry and Biology},
  year         = {2010},
  comment      = {Spherical and Cylindrical harmonics (hard to find on line)

Maybe cylindrical harmonics could be a basis for lagged wind velocity (angle/speed,lag) also see: Padoan083dHarmLect2},
  file         = {Sernelius10laplacePoisson.pdf:Sernelius10laplacePoisson.pdf:PDF},
  owner        = {scotto},
  timestamp    = {2010.08.19},
  url          = {http://www.ifm.liu.se/~boser/elma/},
}

@Book{Shalizi15datAnalElemViewBk,
  title     = {Advanced data analysis from an elementary point of view},
  publisher = {Cambridge University Press},
  year      = {2015},
  author    = {Shalizi, Cosma Rohilla},
  month     = may,
  comment   = {Machine learning and stats course notes --> textbook. Explains logistic regression, among many other things.

May 2015 draft of CMU lecture notes:
http://www.stat.cmu.edu/~cshalizi/uADA/15/
Author will change quite a few things (see "abstract") before Cambridge finally publishes it


Logistic Regression
* model
 - predicts categorical output using what is basically linear regression (I looked at binary categories)
 - output is the probability of a class being "true"
 - linear regression does not do bounded (0,1) probability
 - so instead, linearly predict the (unbounded) logit of probability
 - predicted distribution is sharper (more confident) for larger regression coeffs (vs. same coeffs all linearly scaled down)
* training
 - Newton's method is a maximum likelihood method
 - IRLS (interative recursive least squares)
 -- goal is least squares fit to transformed output
 -- iterative weighted least squares regression gives higher weight to certain (low estimated variance) samples
 -- Taylorize the output to avoid divide by zero at 0,1 boundaries
 -- is actually equivalent to Newton's!

Degrees of Freedom
* Section 8.3.2 talks about degrees of freedom
* Exercises in 8.8 say smooth.spline lets you set the effective degrees of freedom

Multidimensional Splines
* Section 8.4

.},
  file      = {2015 preprint draft:Shalizi15datAnalElemViewBk_draft.pdf:PDF},
  groups    = {Read},
  journal   = {Preprint of book found at http://www. stat. cmu. edu/~ cshalizi/ADAfaEPoV},
  url       = {http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/},
}

@Misc{Shelton10semipartCorrRgrsn,
  author       = {Amy Shelton},
  title        = {Partial \& Semi-Partial Correlation and Multiple Regression},
  howpublished = {Course Notes: Advanced Research Design \& Analysis (200.315), Spring 2010 Dept. of Psychology and Brain Science Johns Hopkins University},
  year         = {2010},
  comment      = {Semipartial corr measures regression var. imp. better than partial; supressor variables explained, etc.
* good explanation of correlation vs. partial correlation vs. semipartial correlation
* semi partial is best measure of a predictor variable's contribution to explaining variance.

But there are regession-specific ways to calculate it...
* Regression weights are same as semi partial regression except missing denominator square root
* Interesting connection: correlation -> beta -> regression coeffs equations in matrix form W/ correlation matrix.
* predictor collinearity screws up significance testing but not regression accuracy.
* surpressor variables are confusing but not incorrect:
-- Special case of multicollinearity
-- are variables that increase the values of R2 by virtue of their correlations with other predictors and NOT the dependent variable
-- e.g. positive correlation w/ dependent variable but negative regression coeff.
-- easy to ID: significant regression weights b/? (reg) \& r (simple corr) have opposite signs

Course web page: http://www.psy.jhu.edu/~ashelton/courses/stats315},
  file         = {Week 2 Course Notes:Shelton10semipartCorrRgrsn.pdf:PDF},
  groups       = {Read},
  owner        = {scot},
  timestamp    = {2010.08.03},
  url          = {http://www.psy.jhu.edu/~ashelton/courses/stats315/week2.pdf},
}

@PhdThesis{Siebert08regPowForecast,
  author      = {Nils Siebert},
  title       = {Development of methods for regional wind power forecasting},
  year        = {2008},
  comment     = {Regional forecasting defined. Models and variable selection. (I superficially skimmed subsections of this thesis)

Regional forecasting defined:
* predict summed power output of possibly hundreds of wind farms spread across a country

Aggregation (what IWES calls upscaling)
* analytical expressions for how geographic smoothing decreases regional forecast
-- decreases with num of farms
-- decreases w/ area covered by farms

Regional forecasting models
* lots of them
* I think they will settle on a fuzzy net

Feature selection
* what NWP grid points or observed measurements to to predict region

* also how many
* mutual information plots
* may do a fancy feature selection
* I thought it might be related to picking offsite met towers
-- however, this is for predicting a SUM, and not using distant obs. to get a lookahead for a SINGLE site
-- so I stopped reading...},
  file        = {Siebert08regPowForecast.pdf:Siebert08regPowForecast.pdf:PDF;Siebert08regPowForecast.pdf:Siebert08regPowForecast.pdf:PDF},
  institution = {Ecole des Mines de Paris},
  owner       = {sotterson},
  timestamp   = {2009.01.22},
}

@InProceedings{Sierra08genFuzzClimEvo,
  author    = {Enrique Sierra and Daniel Lorenzetti and Alejandro Hossian and Hugo Moyano and Horacio Le{\'o}n and Daniel Pandolfi and Rodolfo G{\'o}mez and Ram{\'o}n Garc\'{\i}a-Mart\'{\i}nez},
  title     = {Using Genetic Fuzzy Algorithms to Model the Evolution of Climate Variables at San Jorge Gulf Area},
  booktitle = {MICAI},
  year      = {2008},
  pages     = {382--393},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comment   = {cites Schlueter, uses delta pressure to estimate lags},
  doi       = {10.1007/978-3-540-88636-5_37},
  file      = {Sierra08genFuzzClimEvo.pdf:Sierra08genFuzzClimEvo.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2009.02.20},
}

@Misc{Simonoff09transfRegrsn,
  author       = {Jeffrey Simonoff},
  title        = {Transformations in regression},
  howpublished = {Class Notes, Regression and Multivariate Data Analysis (B90.2301 / C22.0017), New York University},
  year         = {2009},
  comment      = {What true relationship is implied by logging regression inputs and outputs?
- how to handle multiplicative, exponential and other non-linear relationships

- could use as input pre-rpocessor for lasso or stepwise feature selection i.e. assume all relationships and let featsel pick the ones that help

Parabolic or quadratic
* just add x^2 terms and do 2-input variable regresson Inherently nonlinear relationships
* example is two-compartment model from pharmokinetics
* need to use a nonlinear algorithm ==> lose meaningful R^2 stats ==> t and F stats are a pain,

Linearizable Relationships
* can be made linear by logging stuff
* means you can keep all the nice linear regression props

1.) log-log <==> multiplicative/multiplicative * log inputs and outputs
* useful when proportional changes in x (multipiicative) result in prop. chngs in y (multiplicative)
* works for y=a*x^b: --> log y = log a _ b log x (linear)
* appropriate for WIND SPEED TO POWER CURVE since p ~ s^3
* also for money, like interest equations (constant elasticity model)
* Reikard08tempStateRampWind does log-log for speed prediction w/ lagged spd and temp: WHY?

2.) semilog <==> additive/multiplicative
* log y only
* works for y=a*b^x * useful when adding to x multiplies y
* good for suvival model?
* good for semielastic relationship
* may want to solve w/ proporital hazards regression instead of least squared

3.) semilog <==> multiplicative/additive
* log x only
* works for 10^y = a*x^b
* good when y is a "pure number" like a score or return AND predictor is long right-tailed
-- I'm not sure what this means
-- but use for predicting a confidence measure or something?

Mixing and Matching
* It's OK to log some inputs and not others
* but logging output makes an assumption about the relationship of all inputs

See also:
Granger76frcstXfrmSeries
Ahmed10empCompForecast
Fink09xformFAQs (a bit more on implied regression relationships, and other stuff)
Bremnes06compQuantileWind
Gill01interpIntrctHierGLM},
  file         = {Simonoff09transfRegrsn.pdf:Simonoff09transfRegrsn.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2009.09.01},
  url          = {http://pages.stern.nyu.edu/~jsimonof/classes/2301/},
}

@Conference{SisonLebrilla13highPV,
  author    = {Elaine Sison-Lebrilla},
  title     = {High Penetration {PV} Initiative},
  booktitle = {Final Project Presentation Slides},
  year      = {2013},
  comment   = {Final presentation slides for Hawaii (?) High PV initiative project

Highlights
* use probablilistic forecasts
* solar and wind forecasts seem to be integrated.

Should track down the referenced work to see what they really did.},
  file      = {Sison_Lebrilla13highPV.pdf:Sison_Lebrilla13highPV.pdf:PDF},
  groups    = {Use, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2013.10.01},
  url       = {http://calsolarresearch.ca.gov/component/option,com_sobipro/Itemid,0/pid,55/sid,66/},
}

@Article{Sivillo97,
  author    = {Sivillo, Joel K. and Ahlquist, Jon E. and Toth, Zoltan},
  title     = {An ensemble forecasting primer},
  year      = {1997},
  volume    = {12},
  number    = {4},
  pages     = {809--818},
  issn      = {1520-0434},
  booktitle = {Weather and Forecasting},
  comment   = {Somewhat old, maybe read later?},
  groups    = {Ensemble, CitaviImport1, doReadWPV_2},
  ncite     = {85},
  owner     = {sotterson},
  timestamp = {2013.09.26},
}

@TechReport{Skamarock08advRsrchWRFv3,
  author      = {William C. Skamarock and Joseph B. Klemp and Jimy Dudhia and David O. Gill and Dale M. Barker and Michael G. Duda and Xiang-Yu Huang and Wei Wang and Jordan G. Powers},
  title       = {A Description of the Advanced Research WRF Version 3},
  institution = {Mesoscale and Microscale Meteorology Division National Center for Atmospheric Research (NCAR)},
  year        = {2008},
  number      = {NCAR/TN?475+STR},
  location    = {Boulder, CO, USA},
  month       = jun,
  file        = {Skamarock08advRsrchWRFv3.pdf:Skamarock08advRsrchWRFv3.pdf:PDF;Skamarock08advRsrchWRFv3.pdf:Skamarock08advRsrchWRFv3.pdf:PDF},
  groups      = {DOE-PNL09},
  owner       = {sotterson},
  timestamp   = {2009.03.03},
}

@InProceedings{Sklar59copulaSklarThrm,
  author    = {Sklar, A.},
  title     = {Fonctions de r\'{e}partition \`{a} n dimensions et leurs marges},
  booktitle = {Publ. Inst. Statist. Univ},
  year      = {1959},
  volume    = {8},
  pages     = {229--231},
  comment   = {The original reference for Sklar's theorem in copulas. I got this reference from Wikpedia but other papers also cite it.},
  journal   = {Publ. Inst. Statist. Univ.},
  location  = {Paris},
  owner     = {scot},
  timestamp = {2010.12.06},
}

@Article{Smirni16cs426dscrtEvntSimCrsNotes,
  author    = {Evgenia Smirni},
  title     = {Course Notes. CS 426 , Simulation, Fall 2016},
  journal   = {Willian and Mary College},
  year      = {2016},
  comment   = {How to simulate Poisson arriaval times.  Used for Henry Martin's MS thesis, and electricity market order book simulation.

Topics (see pdf bookmarks)
* Simulation Stationary Possion arrival times (2 methods, use 2nd uniform method): Section 7.3
* piecewise constant nonstationary Poisson simulation: Section 7.5
* piecewise linear nonstationary Poisson estimation (piecewise linear, Section 7.5)

I didn't find a place where they described how to estimate nonstationary Poisson parameters.},
  file      = {:Smirni16cs426dscrtEvntSimCrsNotes.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.01.28},
  url       = {http://www.cs.wm.edu/~esmirni/Teaching/cs526},
}

@Article{Smyth07emAlgGaussMixMod,
  author    = {Padhraic Smyth},
  title     = {The EM Algorithm for Gaussian Mixtures. Course Notes, CS 274A: Probabilistic Learning: Theory and Algorithms, Winter.},
  journal   = {University of California, Irvine},
  year      = {2007},
  comment   = {A clear set of lecture notes on the EM algorith for Gaussian Mixture Models.  I wrote some notes in it on how to modify a plain GMM for the case where you have discrete mixture components as well as Gaussian ones.

The course home page is:
http://www.ics.uci.edu/~smyth/courses/cs274/},
  file      = {Smyth07emAlgGaussMixMod.pdf:Smyth07emAlgGaussMixMod.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.02.09},
  url       = {http://www.ics.uci.edu/~smyth/courses/cs274/notes/EMnotes.pdf},
}

@Article{Sohn03reviewstructuralhealth,
  author    = {Sohn, Hoon and Farrar, Charles R and Hemez, Francois M and Shunk, Devin D and Stinemates, Daniel W and Nadler, Brett R and Czarnecki, Jerry J},
  title     = {A review of structural health monitoring literature: 1996--2001},
  journal   = {Los Alamos National Laboratory, USA},
  year      = {2003},
  owner     = {sotterson},
  timestamp = {2016.12.19},
}

@Misc{solarGis17colombiaSolarMap,
  author       = {solargis.com},
  title        = {Colombia solar resource maps, solargis.com},
  howpublished = {Download},
  year         = {2017},
  comment      = {Colombia Solar Irradiance Maps: DHI and DNI

Has other countries too and global (solarGis17worldSolarMap)},
  file         = {solarGis17colombiaSolarMap_GHI_en.png:solarGis17colombiaSolarMap_GHI_en.png:PNG image;solarGis17colombiaSolarMap_GHI_sp.png:solarGis17colombiaSolarMap_GHI_sp.png:PNG image;solarGis17colombiaSolarMap_DNI_en.png:solarGis17colombiaSolarMap_DNI_en.png:PNG image;solarGis17colombiaSolarMap_DNI_sp.png:solarGis17colombiaSolarMap_DNI_sp.png:PNG image},
  owner        = {sotterson},
  timestamp    = {2017.04.06},
  url          = {http://solargis.com/products/maps-and-gis-data/free/download/colombia},
}

@Misc{solarGis17worldSolarMap,
  author       = {Solargis},
  title        = {World solar resource maps, solargis.com},
  howpublished = {Download},
  year         = {2017},
  comment      = {Global Solar Irradiance Maps: DHI and DNI

Has other countries too e.g. Colombia: solarGis17colombiaSolarMap},
  file         = {solarGis17worldSolarMap_GHI.png:solarGis17worldSolarMap_GHI.png:PNG image;solarGis17worldSolarMap_DNI.png:solarGis17worldSolarMap_DNI.png:PNG image},
  owner        = {sotterson},
  timestamp    = {2017.04.06},
  url          = {http://solargis.com/products/maps-and-gis-data/free/download/world},
}

@Standard{Sontag12introMLclass,
  title       = {Introduction To Machine Learning (Class Notes)},
  institution = {Massachusetts Institute of Technology},
  author      = {David Sontag},
  month       = sep,
  year        = {2012},
  url         = {http://people.csail.mit.edu/dsontag/courses/ml12/},
  comment     = {Intro to ML course notes from MIT.  A good source of slides or a quick review
*  Interesting that he delayed linear regression to way at the end...
*  Decision Tree splitting quickly explained in Lecture 11.  Advocates splitting continuous values at midpoints

LECTURE TOPICS (see  syllabus in pdf for more details)
Overview
Introduction to learning (Loss functions, Perceptron algorithm)
Linear classifiers(Proof of perceptron mistake bound, introduction to Support vector machines)
Support vector machines
Derivation of SVM dual, introduction to kernels
Kernel methods
Kernel methods & optimization
Mercer's theorem, convexity
Learning theory
Generalization of finite hypothesis spaces
Learning theory
VC-dimension
Nearest neighbor methods
Also margin-based generalization
Decision trees
Ensemble methods, Boosting
Clustering
K-means, Agglomerative clustering
Spectral clustering
Introduction to Bayesian methods
Probability, decision theory
Naive Bayes
Logistic regression
Linear Regression
Mixture models, EM algorithm
EM algorithm (continued)
Hidden Markov models [Slides]
Factor analysis
Dimensionality reduction (incl. PCA)
Latent Dirichlet allocation
},
  file        = {:Sontag12introMLclass_slides.pdf:PDF},
}

@Article{Spruyt14geomInterpCovMat,
  author    = {Vincent Spruyt},
  title     = {A geometric interpretation of the covariance matrix},
  journal   = {Computer Vision for dummies blog},
  year      = {2014},
  month     = may,
  comment   = {Covers eigen values of covariance matrix.  But I was looking for info about partial correlation, linear regression and precision matrix.},
  file      = {Spruyt14geomInterpCovMat.pdf:Spruyt14geomInterpCovMat.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.06.15},
  url       = {http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/},
}

@Misc{StatsDirect19prospectVsRetroStudies,
  author       = {StatsDirect},
  title        = {Prospective vs. Retrospective Studies},
  howpublished = {www.statsdirect.com},
  year         = {2019},
  comment      = {    *Prospective*: 
    o define a group and then watch for future outcomes vs. factors
    o less prone to bias and confounders
    o stat. sig. requires many outcomes (big # confounders or common outcome)
    o usually Cohort
    o can estimate true risk vs. factors

    *Retrospecive*:
    o look backwards in time for outcomes and see what the factors were
    o prone to bias and confounding
    o can be cheaper
    o usually Case-Control
    o can't calc true risk vs. factors b/c 

    _Example (I think?)_

    Find people who do and don't have heart attacks.  

    See how many of each ate eggs.  Calc odds ratio:

    R = (# attack / # ate eggs) /(# no attack / # ate eggs)

    If R==1, then eggs have no effect.  
    If R<1 eggs eating eggs is protective; 
    If R>1 eating eggs increases risk of heart attack

},
  file         = {:StatsDirect19prospectVsRetroStudies.pdf:PDF},
  url          = {https://www.statsdirect.com/help/basics/prospective.htm},
}

@InBook{Steel08bayesTimeSeriesAn,
  chapter   = {{Bayes}ian time series analysis},
  title     = {The New Palgrave Dictionary of Economics, Second Edition},
  publisher = {Palgrave Macmillan},
  year      = {2008},
  author    = {Mark F.J. Steel},
  editor    = {Steven N. Durlauf and Lawrence E. Blume},
  comment   = {Switching markov models for forecasting, lots of other stuff},
  file      = {Steel08bayesTimeSeriesAn.pdf:Steel08bayesTimeSeriesAn.pdf:PDF;Steel08bayesTimeSeriesAn.pdf:Steel08bayesTimeSeriesAn.pdf:PDF},
  location  = {London},
  owner     = {sotterson},
  timestamp = {2009.02.10},
  url       = {http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic/steel/steel_homepage/publ},
}

@Misc{Sterner10pow2methSlides,
  author       = {Michael Sterner and M. Jentsch and Y-M. Saint-Drenan and N. Gerhardt and A. von Oehsen and M. Specht and F. Baumgart and B. Feigl and V. Frick and B. St?rmer and U. Zuberb?hler and G. Waldstein},
  title        = {Renewable (power to) methane: Storing renewables by linking power and gas grids},
  howpublished = {Fraunhofer IWES Presentation},
  year         = {2010},
  comment      = {Fraunhofer's renewable energy to methane storage idea.

Nice presentation slides.
* A little more than 60\% loss (power now to power in the future)
* Can get C02 for methanation from biogas, which also produces methane.

* greatly increases the per-hecatre energy yield of biofuel
-- biofuels don't have range problem of electric vehicles
-- says this makes it a carbon sink (but doesn't the methane get burned?)
* DE 80\% renewable energy scenario: must store at least 2 "bridge weeks" of energy.
* Current storage potential in DE:
-- current pumped hydro stores a few hours
-- E-mobility: a few hours
-- German gas network: 2 months
* hourly simulation for 2050 seems to show that it could work
* need to reduce cost
* Pilot plant was built in Stutgart; 10MW plant planned for 2012
* I could have used these for my REMENA class},
  file         = {Sterner10pow2methSlides.pdf:Sterner10pow2methSlides.pdf:PDF},
  groups       = {Read},
  owner        = {sotterson},
  timestamp    = {2012.06.22},
}

@InProceedings{Stevie12SpatiaFrcstLoad,
  author      = {Richard Stevie},
  title       = {Spatial Forecasting},
  booktitle   = {August EEI Load Forecasting Group Meeting},
  year        = {2012},
  publisher   = {Integral Analytics},
  comment     = {Presentation slides about grid planning load forecasts.  Uses spatial features that might also be good for short term (hour ahead, day ahead) load forecasts, especially if use a CNN aggregator, which would erase the exact location of loads, and would therefore need extra information like found here in order to distinguish spatial areas.

Actual link to slides: http://www.eei.org/meetings/Meeting_Documents/2012Aug-LoadForecasting-SpatialLoadForecasting_Stevie.pdf},
  file        = {Stevie12SpatiaFrcstLoad.pdf:Stevie12SpatiaFrcstLoad.pdf:PDF},
  institution = {Integral Analytics},
  owner       = {sotterson},
  timestamp   = {2017.03.13},
  url         = {http://www.eei.org/meetings/meeting.aspx?EEIEventId=107BEC41-6FEB-4543-8314-D3BF9B8DA7C6},
}

@Book{Strahler02physGeogBk,
  title     = {Physical geography},
  publisher = {John Wiley \& Sons},
  year      = {2002},
  author    = {Strahler, Alan and Strahler, Arthur},
  isbn      = {0-471-23800-7},
  comment   = {A reference for Yves-Marie's class on solar PV tech.  I used some slides from it for my GIZ Colombia class e.g. one on solar radiation.},
}

@Book{Sutton17reinfLrnIntroBook,
  title     = {Reinforcement learning: An introduction},
  publisher = {MIT Press},
  year      = {2017},
  author    = {Sutton, Richard S and Barto, Andrew G},
  address   = {Cambridge, MA. USA},
  edition   = {Second},
  note      = {Online draft, downloaded Dec. 24, 2016},
  comment   = {Frequently mentioned as a good entry point into Reinforcement Learning, is also the basis for a class.  Comes with
* book draft
* solutions
* course slides and tutorials
* code for the first edition of the book (see Sutton98reinfLrnIntroBook)

I believe the 2nd edition is supposed to be published in 2017; I've attached the draft from Dec. 24, 2017.  The name of the pdf was bookdraft2016sep.
* Recommened here: http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/
  along with Szepesvari10algReinfLrnBook
* old version of the book is Sutton98reinfLrnIntroBook},
  file      = {Sutton17reinfLrnIntroBook.pdf:Sutton17reinfLrnIntroBook.pdf:PDF},
  journal   = {MIT Press},
  owner     = {sotterson},
  timestamp = {2016.12.24},
  url       = {https://webdocs.cs.ualberta.ca/~sutton/book/the-book-2nd.html},
}

@Article{Sutton6BartoAG1998,
  author    = {Sutton, RS},
  title     = {Barto, AG (1998). Reinforcement learning: An introduction},
  journal   = {A Bradford Book},
  year      = {6},
  owner     = {sotterson},
  timestamp = {2016.12.24},
}

@InCollection{Sutton92Introductionchallengereinforcement,
  author    = {Sutton, Richard S},
  title     = {Introduction: The challenge of reinforcement learning},
  booktitle = {Reinforcement Learning},
  publisher = {Springer},
  year      = {1992},
  pages     = {1--3},
  owner     = {sotterson},
  timestamp = {2016.12.24},
}

@Book{Sutton98reinfLrnIntroBook,
  title     = {Reinforcement learning: An introduction},
  publisher = {MIT press Cambridge},
  year      = {1998},
  author    = {Sutton, Richard S and Barto, Andrew G},
  volume    = {1},
  number    = {1},
  edition   = {First},
  comment   = {A free reinforcement learning book w/ code.  Updated in 2017: Sutton17reinfLrnIntroBook

Matlab code here?
https://webdocs.cs.ualberta.ca/~sutton/book/code/code.html)

Python, MATLAB & C/Lisp Code mentioned here: https://github.com/aikorea/awesome-rl
},
  owner     = {sotterson},
  timestamp = {2016.12.24},
}

@Article{Sutton99MDPssemiMDPs,
  author    = {Sutton, Richard S and Precup, Doina and Singh, Satinder},
  title     = {Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  journal   = {Artificial intelligence},
  year      = {1999},
  volume    = {112},
  number    = {1},
  pages     = {181--211},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2016.12.24},
}

@InProceedings{Sutton99PolicyGradientMethods,
  author    = {Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay and others},
  title     = {Policy Gradient Methods for Reinforcement Learning with Function Approximation.},
  booktitle = {NIPS},
  year      = {1999},
  volume    = {99},
  pages     = {1057--1063},
  owner     = {sotterson},
  timestamp = {2016.12.24},
}

@InProceedings{Tarlow13knnStochDistLrn,
  author    = {Tarlow, Daniel and Swersky, Kevin and Charlin, Laurent and Sutskever, Ilya and Zemel, Rich},
  title     = {Stochastic k-Neighborhood Selection for Supervised and Unsupervised Learning},
  booktitle = {Proceedings of the 30\textsuperscript{th} International Conference on Machine Learning (ICML-13)},
  year      = {2013},
  pages     = {199--207},
  comment   = {Automatically learns a somehow-meaningful KNN distance function which is appropriate for the k in KNN.

This is probably a class-label-supervised algorithm, so for local linear QR, it wouldn't be good for clustering (right?). But once neighborhoods are picked, the labels could be used to define a new distance, useful for weighting in learning and in testing.

Maybe this is better than inserting a p-Gaussian into KNN, since the p-Gaussian isn't k sensitive.},
  file      = {Tarlow13knnStochDistLrn.pdf:Tarlow13knnStochDistLrn.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2014.04.04},
  url       = {http://machinelearning.wustl.edu/mlpapers/papers/icml2013_tarlow13},
}

@TechReport{Theis14cosomDEeps,
  author      = {S. Theis and C. Gebhardt and Z. Ben Bouall{\'e}gue},
  title       = {Beschreibung des COSMO-DE-EPS und seiner Ausgabe in die Datenbanken des DWD},
  institution = {Deutscher Wetterdienst},
  year        = {2014},
  number      = {V 2.0},
  month       = may,
  comment     = {The latest, as of 2015, description of COSMO DE EPS.},
  file        = {Theis14cosomDEeps.pdf:Theis14cosomDEeps.pdf:PDF},
  location    = {Deutscher Wetterdienst Gesch?aftsbereich Forschung und Entwicklung Postfach 100465 D-63004 Offenbach},
  url         = {https://www.dwd.de/SharedDocs/downloads/DE/modelldokumentationen/nwv/cosmo_de_eps/cosmo_de_eps_dbbeschr_201405.pdf?__blob=publicationFile&v=3},
}

@TechReport{Thevenard03valVerPVtlbx,
  author    = {Thevenard, D. and Ross, M. and Turcotte, D. and Sheriff, F.},
  title     = {Validation and Verification of Component Models and System Models for the {PV} Toolbox},
  year      = {2003},
  comment   = {Not really about "validation," at least not the prob. forecast type.

Might be a quick intro to PV forecasting, but it's maybe too nuts and bolts about the particular matlab toolbox they built. This paper is old, but maybe the matlab still exists.

Corresponding Matlab toolbox is here:
http://www.rerinfo.ca/english/publications/pubSESCI2003PVToolbox.html},
  file      = {Thevenard03valVerPVtlbx.pdf:Thevenard03valVerPVtlbx.pdf:PDF},
  groups    = {CitaviImport1, doReadNonWPV_2, PV},
  keywords  = {validation, verification, PV},
  owner     = {sotterson},
  publisher = {CETC-Varennes internal report},
  timestamp = {2013.09.26},
  url       = {http://www.rerinfo.ca/english/publications/pubReport2002PVToolboxValid.html},
}

@TechReport{Thomsen11frcstDongSpatTemp,
  author      = {Thomsen, {Sven Creutz} and Scott Otterson and Julija Tastu and Henrik Madsen},
  title       = {Spatio-temporal correction of DONG forecast errors},
  institution = {Technical University of Denmark (DTU)},
  year        = {2011},
  number      = {IMM-Technical Report-2011-09},
  comment     = {The tech report that has the polynomial regression thing for using windfarms as met masts. Also, has my Dong ramp stuff, I think.},
  location    = {Kgs. Lyngby, Denmark : DTU Informatics},
  owner       = {sotterson},
  publisher   = {DTU Informatics, Building 305},
  series      = {IMM-Technical Report-2011-09},
  timestamp   = {2013.04.12},
  url         = {http://orbit.dtu.dk/en/publications/spatiotemporal-correction-of-dong-forecast-errors%282566efd8-9c80-4778-ab8c-6f22ef8495ff%29.html},
}

@PhdThesis{Thorey17ensProbPVfrcstCRPSphdthesis,
  author        = {Thorey, Jean},
  title         = {{Ensemble forecasting using sequential aggregation for photovoltaic power applications}},
  school        = {{Universit{\'e} Pierre et Marie Curie - Paris VI}},
  year          = {2017},
  type          = {Theses},
  month         = Sep,
  __markedentry = {[Scott:1]},
  comment       = {Produces a probabilistic PV forecast by aggregates an ensemble of determinsitic NWP-based PV forecasts and optimizing something w.r.t. CRPS.  Could be used for PV portfolio/ensemble forecasting or for converting decision tree forecasts to a calibrated probabilstic forecast.

* notes that CRPS is not local, but justifies using it},
  file          = {:Thorey17ensProbPVfrcstCRPSphdthesis.pdf:PDF},
  hal_id        = {tel-01697133},
  hal_version   = {v2},
  keywords      = {Probabilistic forecasting ; Photovoltaic power ; Ensemble forecasting ; Sequential aggregation ; CRPS ; Machine learning ; Apprentissage statistique ; Photovolta{\"i}que ; Pr{\'e}vision d'ensemble ; Pr{\'e}vision probabilistique ; Ag{\'e}gation s{\'e}quentielle},
  number        = {2017PA066526},
  pdf           = {https://hal.inria.fr/tel-01697133/file/2017PA066526.pdf},
  url           = {https://hal.inria.fr/tel-01697133},
}

@Article{Thrun00Reinforcementlearningintroduction,
  author    = {Thrun, Sebastian and Littman, Michael L},
  title     = {Reinforcement learning: an introduction},
  journal   = {AI Magazine},
  year      = {2000},
  volume    = {21},
  number    = {1},
  pages     = {103--103},
  owner     = {sotterson},
  publisher = {American Association for Artificial Intelligence},
  timestamp = {2016.12.24},
}

@Book{Thrun12Learninglearn,
  title     = {Learning to learn},
  publisher = {Springer Science \& Business Media},
  year      = {2012},
  author    = {Thrun, Sebastian and Pratt, Lorien},
  owner     = {sotterson},
  timestamp = {2016.12.24},
}

@Article{Tibshirani06regRidgeLassoNotes,
  author    = {Rob Tibshirani},
  title     = {Regularization: Ridge Regression and the LASSO},
  journal   = {Course Notes: Statistics 305, Autumn Quarter, Standford U.},
  year      = {2006},
  comment   = {Course Slides:  Many, many ways to compute ridge regression and LASSO.},
  file      = {Tibshirani06regRidgeLassoNotes.pdf:Tibshirani06regRidgeLassoNotes.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.02.02},
  url       = {http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf},
}

@Book{Tibshirani15statLrnSprsLassoBk,
  title     = {Statistical learning with sparsity: the lasso and generalizations},
  publisher = {Chapman and Hall/CRC},
  year      = {2015},
  author    = {Tibshirani, Robert and Wainwright, Martin and Hastie, Trevor},
  edition   = {Dec. 19, 2016 Corrections},
  comment   = {Updates on how to to Lasso, etc., with sampling, sparsity, etc.

* Standardization is recommended for features in different units so that  lasso input selection doesn't depend upon input scaling.  Btu typically don't scale features in same units.
* Discusses interaction terms (cited paper: Lim15lrnInteractHierGrpLasso)},
  file      = {:Tibshirani15statLrnSprsLassoBk.pdf:PDF},
  url       = {http://web.stanford.edu/~hastie/StatLearnSparsity/},
}

@Article{Tizhoosh06OppositionBasedReinforcement,
  author    = {Tizhoosh, Hamid R},
  title     = {Opposition-Based Reinforcement Learning.},
  journal   = {JACIII},
  year      = {2006},
  volume    = {10},
  number    = {4},
  pages     = {578--585},
  owner     = {sotterson},
  timestamp = {2016.12.24},
}

@InProceedings{Tomasev11probKNNhubNaiv,
  author       = {Tomasev, Nenad and Radovanovi{\'c}, Miloa and Mladeni{\'c}, Dunja and Ivanovi{\'c}, Mirjana},
  title        = {A probabilistic approach to nearest-neighbor classification: naive hubness bayesian kNN},
  booktitle    = {Proceedings of the 20\textsuperscript{th} ACM international conference on Information and knowledge management},
  year         = {2011},
  pages        = {2173--2176},
  organization = {ACM},
  comment      = {A way of treating KNN as a probability, ref' to other prob KNN methods. Maybe good for mixed linear regression

But it's criticized in: Tomasev11knnHubsHighDim (same author)},
  file         = {Tomasev11probKNNhubNaiv.pdf:Tomasev11probKNNhubNaiv.pdf:PDF},
  owner        = {sotterson},
  timestamp    = {2014.04.04},
  url          = {http://dl.acm.org/citation.cfm?id=2063919},
}

@Article{Tong17probFrcstPowSysSlides,
  author  = {Lang Tong},
  title   = {Probabilistic Forecasting for Power System Operations},
  journal = {2017 Reliability \& Markets Peer Review: DOE/OE Transmission Reliability Program},
  year    = {2017},
  comment = {

Slides URL:  https://energy.gov/sites/prod/files/2017/07/f35/8.%20TongCERTS2017_final.pdf},
  file    = {:Tong17probFrcstPowSysSlides.pdf:PDF},
}

@TechReport{Trancik15imprvRinfSolWind,
  author      = {Trancik, Jessika E and Jean, Joel and Kavlak, Goksin and Klemun, Magdalena M and Edwards, Morgan R and McNerney, James and Miotti, Marco and Brown, Patrick R and Mueller, Joshua M and Needell, Zachary A},
  title       = {Technology improvement and emissions reductions as mutually reinforcing efforts: Observations from the global development of solar and wind energy},
  institution = {MIT},
  year        = {2015},
  comment     = {Has Wind and Solar component learning rates.  Separates out soft costs, etc.  Could get related data from: Barbose18trackSunTrendInstPVprice

I think it uses techniques in Nagy13basPredTechProg

Related evernote: https://www.evernote.com/l/AA1ii2xiLz5D45GB_kFXYD5JGp5rvYBvMtI/},
  file        = {:Trancik15imprvRinfSolWind.pdf:PDF},
  url         = {http://energy.mit.edu/publication/technology-improvement-and-emissions-reductions-as-mutually-reinforcing-efforts/},
}

@Misc{Tuohy06windUnitCommit,
  author       = {Aidan Tuohy},
  title        = {Wind power forecasting characteristics and their use in unit commitment},
  howpublished = {Seminar Slides, ESRI},
  month        = jul,
  year         = {2006},
  comment      = {How to use probabilistic forecasts. 100\% accuracy assumption is not least cost due to time correlations!},
  file         = {Tuohy06windUnitCommit.pdf:Tuohy06windUnitCommit.pdf:PDF},
  groups       = {Use, doReadWPV_2},
  owner        = {scotto},
  timestamp    = {2008.07.04},
  url          = {http://ee.ucd.ie/erc/events/ESRIJuly06/TuohyESRI06.pdf},
}

@Article{USDOE16solFrcstMaxGridVal,
  author    = {USDOE},
  title     = {Solar Forecasting: Maximizing its value for grid integration},
  journal   = {SunShot. U.S. Deptartment of Energy.},
  year      = {2016},
  comment   = {Value and overview of types of solar forecasting (not algorithms).  Good graph.

Value of probabilistic solar forecast
* Solar probablistic ("variability") forecast has value for reducing wear-and-tear of the On Load Tap Changers (OLTCs). Since the root cause of this  problem is the occasional extreme variability of solar resource at very high spatial resolutions
* section on large-scale, high cost events.

Time/space resolution:
*  solar irradiance
* 1-km spatial resolution
* temporal resolutions: 5 minutes to hourly,
* horizons between 0 and 72 hours , with 1-6 hour and day-ahead horizons being of particular importance;
* Figure 1 is a nice graph showing how these are done vsl time and space},
  file      = {USDOE16solFrcstMaxGridVal.pdf:USDOE16solFrcstMaxGridVal.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.13},
}

@Article{Vaisala17GlobalWindSpeedMap,
  author       = {Vaisala},
  title        = {Global Wind Speed Map},
  journal      = {Web Download},
  year         = {2017},
  comment      = {Global wind speed map.  Hub height?},
  file         = {Vaisala17GlobalWindSpeedMap.pdf:Vaisala17GlobalWindSpeedMap.pdf:PDF},
  howpublished = {Web Download},
  owner        = {sotterson},
  timestamp    = {2017.04.06},
  url          = {http://www.vaisala.com/Vaisala%20Documents/Scientific%20papers/Vaisala_global_wind_map.pdf},
}

@Article{Vaisala17horizRadianceMap,
  author       = {Vaisala},
  title        = {Global Horizontal Irradiance},
  journal      = {Web Download},
  year         = {2017},
  comment      = {Global Horizontal Irradiance},
  file         = {Vaisala17GlobalWindSpeedMap.pdf:Vaisala17horizRadianceMap:PDF},
  howpublished = {Web Download},
  owner        = {sotterson},
  timestamp    = {2017.04.06},
  url          = {http://www.vaisala.com/Vaisala%20Documents/Scientific%20papers/Vaisala_global_solar_map.pdf},
}

@Article{Venkatasubramanian03rvwProcFaultDet_IIIhist,
  author    = {Venkatasubramanian, Venkat and Rengaswamy, Raghunathan and Kavuri, Surya N and Yin, Kewen},
  title     = {A review of process fault detection and diagnosis: Part III: Process history based methods},
  journal   = {Computers \& chemical engineering},
  year      = {2003},
  volume    = {27},
  number    = {3},
  pages     = {327--346},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2016.12.19},
}

@Article{Venkatasubramanian03rvwProcFaultDet_Imodel,
  author    = {Venkatasubramanian, Venkat and Rengaswamy, Raghunathan and Yin, Kewen and Kavuri, Surya N},
  title     = {A review of process fault detection and diagnosis: Part I: Quantitative model-based methods},
  journal   = {Computers \& chemical engineering},
  year      = {2003},
  volume    = {27},
  number    = {3},
  pages     = {293--311},
  file      = {:Venkatasubramanian03rvwProcFaultDet_Imodel.pdf:PDF},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2016.12.19},
}

@Misc{Vincent17giniPurity,
  author       = {Picaud Vincent},
  title        = {A simple \& clear explanation of the Gini impurity?},
  howpublished = {Comment on StackExchange},
  month        = oct,
  year         = {2017},
  comment      = {Very simple explanation of the Gini Criterion used to do node splits in decision trees, better than the one in Breiman04consistRandForest or  Biau16randFrstGuideTour, if they are talking about the same thing.},
  file         = {:Vincent17giniPurity.pdf:PDF},
  url          = {https://stats.stackexchange.com/questions/308885/a-simple-clear-explanation-of-the-gini-impurity},
}

@Article{Voiland09sunFluctInstrmnt,
  author    = {Adam Voiland},
  title     = {New Sun-Watching Instrument to Monitor Sunlight Fluctuations},
  journal   = {NASA Goddard Space Flight Center},
  year      = {2009},
  comment   = {Sun cycle solar output variability.

Solar Ouput
* 11 year cycle
* 0.1% change

At Peak, More:
* Flares
* Ejections
* Spots},
  file      = {Voiland09sunFluctInstrmnt.pdf:Voiland09sunFluctInstrmnt.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.06},
  url       = {https://www.nasa.gov/topics/earth/features/glory_irradiance.html},
}

@Article{Wagner05Parietallobecontributions,
  author    = {Wagner, Anthony D and Shannon, Benjamin J and Kahn, Itamar and Buckner, Randy L},
  title     = {Parietal lobe contributions to episodic memory retrieval},
  journal   = {Trends in cognitive sciences},
  year      = {2005},
  volume    = {9},
  number    = {9},
  pages     = {445--453},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2016.12.19},
}

@InProceedings{Wagstaff04clustMissNoImp,
  author        = {Kiri Wagstaff},
  title         = {Clustering with Missing Values: No Imputation Required},
  booktitle     = {Classification, Clustering, and Data Mining Applications (Proceedings of the Meeting of the International Federation of Classification Societies)},
  year          = {2004},
  pages         = {649--658},
  organization  = {Jet Propulsion Laboratory, California Institute of Technology},
  bdsk-file-1   = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEFIuLi8uLi8uLi9QYXBlcnMvV2Fnc3RhZmYvQ2?1c3RlcmluZyB3aXRoIE1pc3NpbmcgVmFsdWVzIE5vIEltcHV0YXRpb24gUmVxdWlyZWQucGRm0hsPHB1XTlMuZGF0YU8RAkIAAAAAAkIAAgAACURvY3VtZW50cwAAAAAAAAAAAAAAAAAAAAAAAL7OeK5IKwAAAD+GUR9DbHVzdGVyaW5nIHdpdGggTWlzcyMzRjg2NjgucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP4ZowuDo/1BERiBTS0lNAAMAAwAACQAAAAAAAAAAAAAAAAAAAAAIV2Fnc3RhZmYAEAAIAAC+zlyOAAAAEQAIAADC4MzfAAAAAQAUAD+GUQA3G4AAALLyAAASxgAAEq0AAgBTRG9jdW1lbnRzOm5lbW86RG9jdW1lbnRzOlVuaXZlcnNpdGE6UGFwZXJzOldhZ3N0YWZmOkNsdXN0ZXJpbmcgd2l0aCBNaXNzIzNGODY2OC5wZGYAAA4AdAA5AEMAbAB1AHMAdABlAHIAaQBuAGcAIAB3AGkAdABoACAATQBpAHMAcwBpAG4AZwAgAFYAYQBsAHUAZQBzACAATgBvACAASQBtAHAAdQB0AGEAdABpAG8AbgAgAFIAZQBxAHUAaQByAGUAZAAuAHAAZABmAA8AFAAJAEQAbwBjAHUAbQBlAG4AdABzABIAZC9uZW1vL0RvY3VtZW50cy9Vbml2ZXJzaXRhL1BhcGVycy9XYWdzdGFmZi9DbHVzdGVyaW5nIHdpdGggTWlzc2luZyBWYWx1ZXMgTm8gSW1wdXRhdGlvbiBSZXF1aXJlZC5wZGYAEwASL1ZvbHVtZXMvRG9jdW1lbnRzABUAAgAX//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAP8BBAEMA1IDVANZA2IDbQNxA38DhgOPA5QDlwAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAOk},
  bdsk-url-1    = {http://www.litech.org/~wkiri/Papers/wagstaff-missing-ifcs04.pdf},
  comment       = {constraint based clustering for missing values: n^2 comp load, where n is num. of vectors w/ missing features (I think)
  * related to general area of constrained clustering http://books.google.com/books?hl=en&lr=&id=GMAkzEWlJzsC&oi=fnd&pg=PA1&dq=wagstaff+cluster+k-wagstaff&ots=ueDeZzAKrU&sig=hOlEHszIrYyCBEuqYn8Qwygf170\#PPA3,M1},
  date-added    = {2007-08-09 15:53:52 +0200},
  date-modified = {2007-08-10 16:37:17 +0200},
  file          = {Wagstaff04clustMissNoImp.pdf:Wagstaff04clustMissNoImp.pdf:PDF;Wagstaff04clustMissNoImp.pdf:Wagstaff04clustMissNoImp.pdf:PDF},
  keywords      = {clustering, missing values, astronomy},
  location      = {4800 Oak Grove Dr., Pasadena, CA 91109},
  owner         = {sotterson},
  timestamp     = {2009.01.30},
  url           = {http://www.litech.org/~wkiri/Papers/wagstaff-missing-ifcs04.pdf},
}

@Conference{Wang13lkAhdUCrobustOpt,
  author    = {Xing Wang and Peter Nieuwesteeg},
  title     = {Look-Ahead Unit Commitment With Robust Optimization},
  booktitle = {FERC Technical Conference Increasing Market and Planning Efficiency through Improved Software},
  year      = {2013},
  month     = jun,
  comment   = {Slides suggest suggest that MISO is using several scenarios in their planning -- not sure where they come from, or exactly what "scenario" means. Anyway, they seem to run unit commitment on each of them and then an operator manually picks one.

* related to Chen13robustOptMISO
* nice time series graph showing their current apprach vs. stochastic programming and robust optimization.

"Manage Uncertainties..." slide has a good list of major uncertainties, namely:
* Demand forecast error
* Wind forecast error
* Interchange swing
* Network topology
* Dispatchable resource performance
* Real-time offer adjustments


Robust UC
* allocates more fast-start units more often, is "immune" to future uncertainty
* avoids costly manual emergency dispatch possiblity, but slightly higher cost in "high demand scenario"
-- they don't say how it compares in other cases.
* has "acceptable" performance w/ uncertainty at 4 levels (4 stage optimization?)},
  file      = {Wang13lkAhdUCrobustOpt.pdf:Wang13lkAhdUCrobustOpt.pdf:PDF},
  groups    = {Use, doReadWPV_1},
  location  = {Washington DC, USA},
  owner     = {sotterson},
  timestamp = {2014.01.23},
  url       = {http://www.ferc.gov/CalendarFiles/20120627085905-Wednesday_SessionA_Wang.pdf},
}

@Misc{Ward18sumPoissonsCourseNts,
  author       = {Mark Daniel Ward},
  title        = {Sums of independent Poisson random variables are Poisson random variables},
  howpublished = {Course Notes: Probability STAT/MA 41600 (Fall). Purdue University},
  year         = {2018},
  comment      = {Proof that the sum of Poisson RV's is itself a Poisson RV.  Downloading the rest of these course notes might be worth it.},
  file         = {:Ward18sumPoissonsCourseNts.pdf:PDF},
  url          = {http://llc.stat.purdue.edu/2018/41600/},
}

@Conference{Watson12stochOptPowSys,
  author    = {Dr. Jean-Paul Watson},
  title     = {Stochastic Optimization of Power Systems Planning and Operations in High Penetration Renewables Scenarios},
  booktitle = {5\textsuperscript{th} International Conference on Integration of Renewable and Distributed Energy Resources},
  year      = {2012},
  comment   = {Overview slides of power system stochastic optimzation research, tools

* Argues that power systems are already using stochastic optimzation
* several types compared
* several applications overviewed, including transmission planning near wind farms
* recommends books on doing stoch opt. in Python.},
  file      = {Watson12stochOptPowSys.pdf:Watson12stochOptPowSys.pdf:PDF},
  groups    = {Use, doReadWPV_1},
  owner     = {sotterson},
  timestamp = {2014.01.23},
  url       = {http://www.conference-on-integration-2012.com/fileadmin/user_upload_COI-2012/RE_PDF/Watson_JP__verschluesselt_.pdf},
}

@Misc{Watt13specClustIntroBlog,
  author       = {Jeremy Watt},
  title        = {An intro to spectral clustering - complete with illustrative MATLAB toys},
  howpublished = {Blog Post},
  month        = jul,
  year         = {2013},
  note         = {Referencing blog page: http://neonwatty.wordpress.com/2013/07/03/an-intro-to-spectral-clustering-complete-with-illustrative-matlab-toys/},
  comment      = {Easy to understand explanation of spectral clustering w/ Matlab examples.

Kmeans clustering
* fails b/c it assumes clusters are in a big ball

Epsilon threshold on adjacency matrix
* epsilon thresholds on vertex weights can work better
* impossible to pick good epsilon when graph scale varies

K nearest neighbor clustering
* knn clustering is an improvement over epsilon thresh
* but it still has a problem w/ "sparse" a.k.a. false connections between truly isoloated graphs

Gaussian distance clustering
 -- also isn't so good
 -- "treats whold datatset as a completely connected graph"
 ---- I think he means that the adjacency weights never go to zero.

So, try looking at graph differently

Perfectly disjoint groups with (0,1) weighting
* vertice vectors of any group must be orthogonal to any other
* vertice vectors compared with itself must also sum to zero
* total vertex matrix sums sum to zero when compared

Imperfectly disjoint clusters
* optimization used for perfect is nearly identical to eigenvector problem
* chose K smallest eigen vectors of Laplace matrix to recover group vectors
* example w/ noisy block matrix (almost disjoint sets)
 -- lose literal vertex indices for each group
 -- but smallest eigenvector pattern is recognizably different for each group
 -- can recover cluster ID's by clustering the eigen vectors
 -- he uses kmeans but I don't know why, if knn is better?

IDEA: KNN local linear quantile regression weighting
* would loop over both k (in knn) and num. of clusters
* could avoid memorizing training set w/ "condensed nearest neighbors" See Wikipedia

Matlab and the notes are also in a zip file here:
https://drive.google.com/file/d/0B9LZEwqBZcp4LVVNbUxmRV82blU/edit?usp=sharing},
  file         = {Watt13specClustIntroBlog.pdf:Watt13specClustIntroBlog.pdf:PDF},
  groups       = {Read},
  owner        = {sotterson},
  timestamp    = {2014.03.10},
  url          = {http://neonwatty.files.wordpress.com/2013/07/spectral_clustering_draft_33.pdf},
}

@Misc{Weber16poissonExpClassNotesDS430,
  author       = {Bob Weber},
  title        = {Notes on the Poisson and exponential distributions},
  howpublished = {Course notes: Decision Sciences 430, Kellog School of Management},
  month        = apr,
  year         = {2016},
  file         = {:Weber16poissonExpClassNotesDS430.pdf:PDF},
  url          = {https://www.kellogg.northwestern.edu/faculty/weber/decs-430/},
}

@TechReport{Welch06kalmanIntro,
  author      = {Greg Welch and Gary Bishop},
  title       = {An Introduction to the {Kalman} Filter},
  institution = {Department of Computer Science, University of North Carolina at Chapel Hill},
  year        = {2006},
  number      = {TR 95-041},
  comment     = {Basic and Extended Kalman filter introduction Kalman filters
* an efficient, recursive, and online method of doing state estimation
* internally, it assumes states are Gaussian (stores mean and covariance)
* internally, it assumes that state updates are linear (or can be approximated as linear)
* looks like you can pick what your states are eg. they could be derivatives?
* measurements can be the "states" or they can be a linear or nonlinear (EKF) function of the states
* a bunch of initial states and parameters to set up

The Basic Kalman Filter Idea: Predictor-Corrector
1.) project states ahead in time, based on estimates from last step
2.) correct projection w/ new input
* not much theoretical justification but an explanation of how one works
* no explanation for how to use velocity, etc.
* extended version can handle non-linearities, but simple EKF has flaw of non-normality (other variants preserve)
* many ways of writing Kalman filter equations
* doesn't explain how to project into future (forecast)
* example showing how to estimate a DC value, and the effect of the},
  file        = {:Welch06kalmanIntro.pdf:PDF;Welch06kalmanIntro.pdf:Welch06kalmanIntro.pdf:PDF},
  groups      = {Read},
  owner       = {sotterson},
  timestamp   = {2008.09.25},
  url         = {http://www.cs.unc.edu/~welch/media/pdf/kalman_intro.pdf},
}

@Misc{Whyzar16diffRadIrradLumIllum,
  author       = {whyzar},
  title        = {Difference between irradiance and radiance, remote sensing reflectance and water leaving radiance},
  howpublished = {gis.stackexchange.com},
  month        = dec,
  year         = {2016},
  comment      = {Difference between radiance, irradiance; luminance, illuminance

Radiance:  Watts of radiation emitted in a solid angle from unit area of a source (W/m^2/sr)
Irradiance: radiation flux per area (W/m^2)
Luminance/Illuminance: seem to be flux projected on to a flat surface but this comment isn't too clear about that},
  file         = {:Whyzar16diffRadIrradLumIllum.pdf:PDF},
  url          = {https://gis.stackexchange.com/questions/222554/difference-between-irradiance-and-radiance-remote-sensing-reflectance-and-water},
}

@Article{Wicker13nwpEvrythng100min,
  author    = {Lou Wicker},
  title     = {Everything you need to know: Numerical Weather Prediction in about 100 minutes},
  journal   = {NSSL},
  year      = {2013},
  comment   = {I got some stuff out of this for GIZ colombia},
  file      = {Wicker13nwpEvrythng100min.pdf:Wicker13nwpEvrythng100min.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.04.25},
  url       = {http://weather.ou.edu/~scavallo/classes/metr_5004/f2013/lectures/NWP_LecturesFall2013.pdf},
}

@Misc{Wikipedia19bassDiffus,
  author  = {Wikipedia},
  title   = {Bass diffusion model},
  month   = apr,
  year    = {2019},
  comment = {A differential equation mdoel (Basic is Ricati w/ const. coeffs), assuming that there are two kinds of adopters: innovators and imitators (coeffs p and q)

- simplifies this to two types: innovators and imitators
- shows eqs. but doesn't explain
- get an S-curve (cdf) w/ pre-defined max market, m
- it fits well, w/ restricted range of innov. and imitation params
- more pessimistic than X and Y
- Super cited (>5K cites), recently used for spread of internet

Extensions
- include price: Generalized BDM
- factor in "brand loyalty" (sucessive generations: is that what it is?)

Relationship to other S-curves (their cdf?)
- q=0: is exponential distribution
- p=0: logistic distribution


},
  file    = {:Wikipedia19bassDiffus.pdf:PDF},
  url     = {https://en.wikipedia.org/wiki/Bass_diffusion_model},
}

@Article{Williams92Simplestatisticalgradient,
  author    = {Williams, Ronald J},
  title     = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  journal   = {Machine learning},
  year      = {1992},
  volume    = {8},
  number    = {3-4},
  pages     = {229--256},
  owner     = {sotterson},
  publisher = {Springer},
  timestamp = {2016.12.24},
}

@TechReport{Williams97predictLin2GaussProc,
  author      = {C. K. I. Williams},
  title       = {Prediction with {Gauss}ian processes: From linear regression to linear prediction and beyond},
  institution = {Neural Computing Research Group, Dept. of Computer Science \& Applied Mathematics, Aston University},
  year        = {1998},
  number      = {NCRG/97/012},
  booktitle   = {Learning and Inference in Graphical Models},
  comment     = {Bayesian linear regression derived from Gaussian processes. Relationship to ARMA and neural networks * GP's are like infinite neural nets * ARMA models are Gaussian processes! p. 8 * This is a tech report that appears in book},
  file        = {Williams97predictLin2GaussProc.pdf:Williams97predictLin2GaussProc.pdf:PDF;Williams97predictLin2GaussProc.pdf:Williams97predictLin2GaussProc.pdf:PDF},
  location    = {Birmingham, UK},
  owner       = {sotterson},
  pages       = {599--621},
  publisher   = {Kluwer},
  timestamp   = {2008.10.03},
  url         = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.878},
}

@Article{Worden03Experimentalvalidationstructural,
  author    = {Worden, Keith and Manson, Graeme and Allman, David},
  title     = {Experimental validation of a structural health monitoring methodology: Part I. Novelty detection on a laboratory structure},
  journal   = {Journal of Sound and Vibration},
  year      = {2003},
  volume    = {259},
  number    = {2},
  pages     = {323--343},
  owner     = {sotterson},
  publisher = {Elsevier},
  timestamp = {2016.12.19},
}

@Article{Wu07top10MachLrn,
  author    = {Wu, Xindong and Kumar, Vipin and Ross Quinlan, J. and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, Geoffrey J. and Ng, Angus and Liu, Bing and Yu, Philip S. and Zhou, Zhi-Hua and Steinbach, Michael and Hand, David J. and Steinberg, Dan},
  title     = {Top 10 algorithms in data mining},
  journal   = {Knowledge and Information Systems},
  year      = {2007},
  volume    = {14},
  number    = {1},
  pages     = {1--37},
  issn      = {0219-1377},
  comment   = {I should read this someday before it's too obsolete},
  doi       = {10.1007/s10115-007-0114-2},
  file      = {Wu07top10MachLrn.pdf:Wu07top10MachLrn.pdf:PDF},
  location  = {New York, NY, USA},
  owner     = {scot},
  publisher = {Springer-Verlag New York, Inc.},
  timestamp = {2010.07.28},
}

@Misc{Wu10gaussMargNotJoint,
  author       = {Dapeng Oliver Wu},
  title        = {Example: RVs Marginally {Gauss}ian but not Jointly {Gauss}ian},
  howpublished = {Author's home page: http://www.wu.ece.ufl.edu/books/math/probability/ProbabilityTheory.html},
  year         = {2010},
  comment      = {Example of two Gaussian RV's which are not jointly Gaussian. Relevant to spinning reserves and scenario generation: Pinson09probFrcstStatScenWind and Morales10scenarios both assume that Gaussianizing RV's with a transform makes their joint distribution a multi-variate Gaussian, which is not true in general.

So, I guess you could say that a Gaussian copula can't model every kind of dependence.  This is one example, where perfect Gaussian marginals can't be modeled by a covariance matrix.},
  file         = {Wu10gaussMargNotJoint.pdf:Wu10gaussMargNotJoint.pdf:PDF},
  groups       = {Read},
  owner        = {scot},
  timestamp    = {2010.11.17},
  url          = {http://www.wu.ece.ufl.edu/books/math/probability/jointlygaussian.pdf},
}

@Article{Xie08reviewrecentadvances,
  author    = {Xie, Xianghua},
  title     = {A review of recent advances in surface defect detection using texture analysis techniques},
  journal   = {ELCVIA Electronic Letters on Computer Vision and Image Analysis},
  year      = {2008},
  volume    = {7},
  number    = {3},
  owner     = {sotterson},
  timestamp = {2016.12.19},
}

@TechReport{Xu12flexRamp2ndFprpsl,
  author      = {Lin Xu and Donald Tretheway},
  title       = {Flexible Ramping Products (Second Revised Draft Final Proposal)},
  institution = {California ISO},
  year        = {2012},
  month       = oct,
  comment     = {Details of CAISO flexiramp thing.

also described in: AbdulRahman12flexRampCAISO and Xu12flexRamp2ndFprpsl},
  file        = {Xu12flexRamp2ndFprpsl.pdf:Xu12flexRamp2ndFprpsl.pdf:PDF},
  groups      = {Use, doReadWPV_2},
  location    = {California, USA},
  owner       = {sotterson},
  timestamp   = {2013.10.11},
}

@PhdThesis{Yang15UncertaintyForecastNetLd,
  author    = {Yang, Peizhi},
  title     = {Uncertainty in the Forecast of Net--Load Ramp in CAISO Region},
  school    = {Duke University},
  year      = {2015},
  comment   = {Measurures error of deterministic demand and RES forecasts, and calculates ramps from them.  These are fit to unconditional parametric Gaussian and Normal distributions, and it's not clear to me that the fiit that well.},
  file      = {Yang15UncertaintyForecastNetLd.pdf:Yang15UncertaintyForecastNetLd.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.05.02},
  url       = {https://dukespace.lib.duke.edu/dspace/bitstream/handle/10161/9622/Uncertainty%20in%20the%20Forecast%20of%20Net%20Load%20Ramp%20in%20CAISO-Peizhi.pdf?sequence=1},
}

@TechReport{Yano07timeVarStructAutoReg,
  author      = {Koiti Yano and Naoyuki Yoshino},
  title       = {{Japan}ese Monetary Policy Reaction Function and Time-Varying Structural Vector Autoregressions: A {Monte Carlo} Particle Filtering Approach},
  institution = {Financial Services Agency, Government of Japan.},
  year        = {2007},
  comment     = {Switching autoregressive models. Algorithm figures out which model is active by itself, so don't need to partition manually during training. Backbone is a markov chain.},
  file        = {Yano07timeVarStructAutoReg.pdf:Yano07timeVarStructAutoReg.pdf:PDF},
  owner       = {sotterson},
  timestamp   = {2008.12.19},
  url         = {http://www.fsa.go.jp/frtc/seika/discussion/2007/20080214-1.pdf},
}

@InProceedings{Yao10simplexQRfeatSel,
  author       = {Yonggang Yao},
  title        = {Simplex Techniques for Quantile Regression Model Selection.},
  booktitle    = {Conference on Nonparametric Statistics and Statistical Learning},
  year         = {2010},
  organization = {Ohio State University},
  comment      = {Quantile regression feature selection built into the simplex and other linear programming techniques. Conference slides, but quick overview.

* three ways to write QR optimization equation
* solution via linear programming and simplex
* AIC and other metrics for model/feature selection within simplex tableau
* also LASSO style selection

Also discusses updating partial models (adaptation)

Also has many refereneces at the end that could be relevant

Other relevant talks on his web site too. And some linear programming software.},
  file         = {Yao10simplexQRfeatSel.pdf:Yao10simplexQRfeatSel.pdf:PDF},
  groups       = {Read},
  location     = {Vancouver, Canada},
  owner        = {sotterson},
  timestamp    = {2014.05.11},
  url          = {http://www.stat.osu.edu/~yao/talks/},
}

@InProceedings{Yen-Nakafujinakafuji08windsManageChange,
  author    = {Dora Yen-Nakafujinakafuji},
  title     = {Managing for the Winds of Change},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  year      = {2008},
  comment   = {General overview: Lawrence Livermore Lab work for CA wind
* breakdown of MAE improvements to different model improvements (eg. 4.8\% improvement for power curve)
* lots of other system stuff},
  file      = {Yen-Nakafujinakafuji08windsManageChange.pdf:Yen-Nakafujinakafuji08windsManageChange.pdf:PDF;Yen-Nakafujinakafuji08windsManageChange.pdf:Yen-Nakafujinakafuji08windsManageChange.pdf:PDF},
  groups    = {IWFTM08, Read},
  owner     = {sotterson},
  timestamp = {2008.09.19},
  url       = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
}

@InProceedings{Yu17gnrcTrafFrcstExtrnRNNlstm,
  author    = {Yu, Rose and Li, Yaguang and Shahabi, Cyrus and Demiryurek, Ugur and Liu, Yan},
  title     = {Deep Learning: A Generic Approach for Extreme Condition Traffic Forecasting},
  booktitle = {Proceedings of the 2017 SIAM International Conference on Data Mining (SDM)},
  year      = {2017},
  comment   = {A mixture RNN LSTM that somehow predicts extreme traffic (just the peak traffic?).},
  file      = {Yu17gnrcTrafFrcstExtrnRNNlstm.pdf:Yu17gnrcTrafFrcstExtrnRNNlstm.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2017.03.08},
  url       = {http://infolab.usc.edu/DocsDemos/sdm2017_deep_learning_traffic.pdf},
}

@InCollection{Yuan09GaussProcMixVar,
  author    = {Chao Yuan and Claus Neubauer},
  title     = {Variational Mixture of {Gauss}ian Process Experts},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2009},
  editor    = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
  volume    = {21},
  comment   = {Fast way of computing a mixure of gaussian processes.

The mixture could be the mixture in BMA for ensemble density forecasting...},
  file      = {Yuan09GaussProcMixVar.pdf:Yuan09GaussProcMixVar.pdf:PDF;Yuan09GaussProcMixVar.pdf:Yuan09GaussProcMixVar.pdf:PDF},
  groups    = {Ensemble, doReadNonWPV_2},
  owner     = {sotterson},
  timestamp = {2008.12.15},
}

@InProceedings{Zack08truewindOverview,
  author    = {John W. Zack},
  title     = {An Overview of AWS Truewind's Approach and Experience in Providing Wind Power Production Forecasting Services to Utilities and Balancing Authorities in {North America}},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  year      = {2008},
  comment   = {Combine inputs w/ "statistical models"
* "statistical models": linear regression, neural nets, SVM's, ARIMA, state spaceetc.
* inputs
-- large scale physics models
--> regional physics-based atmospheric models
--> input -- regiona weather data
-- farm met data
-- farm power data
-- off-site met data
* model selection and dat types vary w/ lookahead period

Forecast Ensembles
* not necessarily different models (but they use many NWP's)
* just same model output w/ perturbed params and input data (within uncertainty range)
* composite (weighted on past perf, sometimes) often best
* spread provides range of uncertainty Why NWP?
* they list 6 available
* explain why use them instead of using gov't outputs (slide 8)

Factors affecting forecast accuracy (good list on p. 25)
* interesting to note that they also get better forecasts at greater aggregation
* also interesting that location of wind on power curve affects accuracy. (s/b factor in confidence bounds?)
* good graph onf wind speed distribution

FUTURE IMPROVEMENTS
* better physcis, modes
* satellite sensor data!
* more offsite data},
  file      = {Zack08truewindOverview.pdf:Zack08truewindOverview.pdf:PDF;Zack08truewindOverview.pdf:Zack08truewindOverview.pdf:PDF},
  groups    = {IWFTM08, Read, Ensemble, doReadWPV_2},
  owner     = {sotterson},
  timestamp = {2008.09.19},
  url       = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
}

@InCollection{Zhang09KernMeasIndepNonIID,
  author    = {Xinhua Zhang and Le Song and Arthur Gretton and Alex J. Smola},
  title     = {Kernel Measures of Independence for non-iid Data},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2009},
  editor    = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
  volume    = {21},
  comment   = {Measure independence between variables w/ time lagged dependence. Use for interturbine dependence detection (instead of multi-dime mutual information as in BPA SG II report). Also has refs for simpler, correlation methods. Could be used for selecting met towers, etc. See [18] * Example of use for time series clustering and segmentation},
  file      = {Zhang09KernMeasIndepNonIID.pdf:Zhang09KernMeasIndepNonIID.pdf:PDF},
  owner     = {sotterson},
  timestamp = {2008.12.15},
}

@Article{Zhu14SpatVarCoeffJump,
  author    = {Zhu, Hongtu and Fan, Jianqing and Kong, Linglong},
  title     = {Spatially Varying Coefficient Model for Neuroimaging Data with Jump Discontinuities},
  journal   = {Journal of the American Statistical Association},
  year      = {2014},
  number    = {just-accepted},
  comment   = {3D or 2D surface varying coefficient model with 3 step training procedure that includes functional PCA. Might be good for a lagged wind velocity basis?

I have only read this enough to get the general idea. I've put more scribbling on it on OneNote.

Attached is the Oct 22, 2013 Arxiv paper, not the journal paper
http://arxiv.org/abs/1310.1183

Related article: Zhu12multiVaryCfsFncResp},
  doi       = {10.1080/01621459.2014.881742},
  file      = {Zhu14SpatVarCoeffJump.pdf:Zhu14SpatVarCoeffJump.pdf:PDF},
  owner     = {sotterson},
  publisher = {Taylor \& Francis Group},
  timestamp = {2014.11.09},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectory:papers;}

@Comment{jabref-meta: fileDirectory-scot:papers;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Markings\;2\;1\;\;\;\;;
2 StaticGroup:Scott:6\;2\;1\;\;\;\;;
2 StaticGroup:[Scott:]\;2\;1\;\;\;\;;
1 StaticGroup:Markings\;2\;1\;\;\;\;;
2 StaticGroup:Scott:1\;2\;1\;\;\;\;;
1 StaticGroup:Markings\;2\;1\;\;\;\;;
2 StaticGroup:Scott Otterson:6\;2\;1\;\;\;\;;
2 StaticGroup:[Scott Otterson:]\;2\;1\;\;\;\;;
1 StaticGroup:Markings\;2\;1\;\;\;\;;
2 StaticGroup:Scott Otterson:6\;2\;1\;\;\;\;;
2 StaticGroup:[sotterson:]\;2\;1\;\;\;\;;
2 StaticGroup:sotterson:6\;2\;1\;\;\;\;;
1 StaticGroup:IWFTM08\;0\;1\;\;\;\;;
1 StaticGroup:Read\;0\;1\;\;\;\;;
1 StaticGroup:DOE-PNL09\;0\;1\;\;\;\;;
1 StaticGroup:ProbFrcst\;0\;1\;\;\;\;;
2 StaticGroup:Ensemble\;0\;1\;\;\;\;;
2 StaticGroup:PointDerived\;0\;1\;\;\;\;;
2 StaticGroup:Test\;0\;1\;\;\;\;;
2 StaticGroup:Use\;0\;1\;\;\;\;;
2 StaticGroup:ErrDistProps\;0\;1\;\;\;\;;
2 StaticGroup:Upscaling (prob)\;0\;1\;\;\;\;;
1 StaticGroup:Eweline\;0\;1\;\;\;\;;
2 StaticGroup:CitaviImport1\;0\;1\;\;\;\;;
2 StaticGroup:doReadWPV_1\;0\;1\;\;\;\;;
2 StaticGroup:doReadWPV_2\;0\;1\;\;\;\;;
2 StaticGroup:doReadNonWPV_1\;0\;1\;\;\;\;;
2 StaticGroup:doReadNonWPV_2\;0\;1\;\;\;\;;
1 StaticGroup:PtFrcst\;0\;1\;\;\;\;;
2 StaticGroup:PV\;0\;1\;\;\;\;;
2 StaticGroup:Wind\;0\;1\;\;\;\;;
2 StaticGroup:Use\;0\;1\;\;\;\;;
1 StaticGroup:recurNN\;0\;1\;\;\;\;;
1 StaticGroup:StochOpt\;0\;1\;\;\;\;;
}

@Comment{jabref-meta: protectedFlag:true;}

@Comment{jabref-meta: saveOrderConfig:original;abstract;false;abstract;false;abstract;false;}

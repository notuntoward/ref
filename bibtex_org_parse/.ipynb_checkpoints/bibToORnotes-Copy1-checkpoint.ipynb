{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: British spelling to American?\n",
    "I only found one example where this split a tag: \"optimization\".\n",
    "quick hack:\n",
    "https://stackoverflow.com/questions/42329766/python-nlp-british-english-vs-american-english\n",
    "https://stackoverflow.com/questions/18840640/python-2-7-find-and-replace-from-text-file-using-dictionary-to-new-text-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from bibtexparser.bparser import BibTexParser\n",
    "from dateutil.parser import parse\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "refDir = Path(\"C:/Users/scott/OneDrive - Clean Power Research/ref\")\n",
    "bibDirBase = refDir\n",
    "\n",
    "bibInNm = \"deepSolarDOE\"\n",
    "bibInNm = \"newTechAdopt\"\n",
    "bibInNm = \"energy\"\n",
    "\n",
    "bibInFNm = bibDirBase / f\"{bibInNm}.bib\"\n",
    "noteOutDir = Path(\"C:/Users/scott/tmp/bibNotesOR\")\n",
    "\n",
    "# for colapsing/replacing whitespace\n",
    "matchWhiteSpace_regexp = re.compile(r\"\\s+\")\n",
    "# for de-orgifying orig .bib comments\n",
    "matchLeadStars_regexp = re.compile(r\"\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read source .bib as string so can handle parsing in a separate step, avoiding\n",
    "# error throwing on unquoted .bib fields to right of '='.\n",
    "# (If bibtexparser were to write them out, they would be restored as in the\n",
    "# original bib file).\n",
    "\n",
    "with open(bibInFNm, encoding=\"utf8\") as bibtex_file:\n",
    "    bibtex_str = bibtex_file.read()\n",
    "\n",
    "bp = BibTexParser(interpolate_strings=False)\n",
    "bib_database = bp.parse(bibtex_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write bib fields to org-roam notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keeping 296 of 3249 unique keywords\n"
     ]
    }
   ],
   "source": [
    "# Collect all bibtex keys in the .bib file, and make roam_tags strings\n",
    "bibkeysAll = set()\n",
    "keywordsCntAll = defaultdict(lambda: 0)\n",
    "keywordsItem = defaultdict(lambda: [])\n",
    "remPunctTable = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# Collect all bibkeys and (normalized) keywords for each item\n",
    "for bibitem in bib_database.entries:\n",
    "    bibkey = bibitem['ID']\n",
    "    bibkeysAll.add(bibkey)\n",
    "\n",
    "    if \"keywords\" in bibitem:\n",
    "        # convert various .bib keyword list formats to OR format\n",
    "        for kw_phrase in re.split(', |; |,|;', bibitem['keywords']):\n",
    "            if len(kw_phrase) > 0:\n",
    "                # normalize to lowercase, no punctuation, singular, no spaces\n",
    "                kw_phrase = kw_phrase.translate(remPunctTable).lower().split()\n",
    "                for idx, word in enumerate(kw_phrase):\n",
    "                    if (word_singular := wordnet.morphy(word)) is not None:\n",
    "                        kw_phrase[idx] = word_singular\n",
    "\n",
    "                kw_phrase = \"_\".join(kw_phrase)\n",
    "                keywordsItem[bibkey] += [kw_phrase]\n",
    "                keywordsCntAll[kw_phrase] += 1\n",
    "\n",
    "# Remove rare keywords and then make a roam_tags string for each bibkey\n",
    "nOccurMin = 3\n",
    "keepKeywords = set({kw: count for kw, count in keywordsCntAll.items()\n",
    "                    if count >= nOccurMin}.keys())\n",
    "\n",
    "roam_tags_str = dict()\n",
    "for bibkey in bibkeysAll:\n",
    "    kws = keywordsItem[bibkey]\n",
    "    kwsItemKeep = keepKeywords.intersection(kws)\n",
    "    roam_tags_str[bibkey] = f'#+roam_tags: {\" \".join(kwsItemKeep)}'\n",
    "\n",
    "print(f\"keeping {len(keepKeywords)} of {len(keywordsCntAll)} unique keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Translate selected .bib entries to org-roam note syntax, and write notes\n",
    "\n",
    "def fix_comment_stars(commentStr):\n",
    "    \"\"\"Replace leading stars with 'o's.  Bib used '*'s which may\n",
    "    result in inadvertently collapsed .org headlines\"\"\"\n",
    "\n",
    "    outlns = []\n",
    "    for ln in commentStr.split('\\n'):\n",
    "        noLstars = ln.lstrip('*')\n",
    "        outlns += [\"o\" * (len(ln) - len(noLstars)) + noLstars]\n",
    "\n",
    "    return \"\\n\".join(outlns)\n",
    "\n",
    "\n",
    "def bibitem_to_OR_note(bibitem):\n",
    "    \"\"\"Makes org-roam note text from a .bib file item.  The note contains \n",
    "    the .bib comment, abstract and metadata.  The filename, OR title and key\n",
    "    are the .bib citekey.\"\"\"\n",
    "\n",
    "    bibkey = bibitem['ID']\n",
    "\n",
    "    noteLns = []\n",
    "    if \"title\" in bibitem:\n",
    "        noteLns += [f\"={bibitem['title']}=\"]\n",
    "\n",
    "    authChunks = []\n",
    "    if \"author\" in bibitem:\n",
    "        authChunks += [bibitem['author']]\n",
    "\n",
    "    if \"year\" in bibitem:\n",
    "        authChunks += [f\"({bibitem['year']})\"]\n",
    "\n",
    "    if len(authChunks) > 0:\n",
    "        authStr = \" \".join(authChunks)\n",
    "        noteLns += [f\"/{authStr}/\"]\n",
    "\n",
    "    if \"comment\" in bibitem:\n",
    "        commentTxt = bibitem['comment']\n",
    "        words = re.findall(r'\\w+', commentTxt)\n",
    "        words = set([w.translate(remPunctTable) for w in words])\n",
    "\n",
    "        for citedKey in words.intersection(bibkeysAll):\n",
    "            linkStr = f\"[[file:{citedKey}.org][{citedKey}]]\"\n",
    "            commentTxt = commentTxt.replace(citedKey, linkStr)\n",
    "\n",
    "        noteLns += [\"\", fix_comment_stars(commentTxt)]\n",
    "\n",
    "    if \"abstract\" in bibitem:\n",
    "        noteLns += [\"\", \"* Abstract\", bibitem['abstract']]\n",
    "\n",
    "    noteLns += [\"\", \"* Org-Roam Metadata\", f\"#+title:{bibkey}\"]\n",
    "\n",
    "    if \"timestamp\" in bibitem:\n",
    "        try:\n",
    "            dt = parse(bibitem['timestamp'])\n",
    "            timeStr = dt.strftime('%Y-%m-%d %a')\n",
    "        except:\n",
    "            timeStr = \"?\"\n",
    "\n",
    "        noteLns.append(f\"#+created: [{timeStr}]\")\n",
    "\n",
    "    noteLns += [f\"#+roam_key: cite:{bibkey}\",\n",
    "                roam_tags_str[bibkey]]\n",
    "\n",
    "    return noteLns\n",
    "\n",
    "\n",
    "# Write a separate org-roam note file for each .bib entry\n",
    "for bibitem in bib_database.entries:\n",
    "    noteLns = bibitem_to_OR_note(bibitem)\n",
    "\n",
    "    noteOutFNm = noteOutDir / f\"{bibitem['ID']}.org\"\n",
    "    with open(noteOutFNm, 'w', encoding=\"utf8\") as fh:\n",
    "        for ln in noteLns:\n",
    "            fh.write('%s\\n' % ln\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype code below.  Don't run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pybtex parser string examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# from pybtex.database.input import bibtex\n",
    "# bibitem['keywords']=\"CRPS, Diagnostic tools  , Evaluation framework, Ignorance Score, Probabilistic solar forecasting, Scoring rules\"\n",
    "# bibkws = bibitem['keywords'].split(\", \")\n",
    "# bibkws\n",
    "\n",
    "# s = 'Probabilistic    solar forecasting'\n",
    "# kws_underscore = []\n",
    "# for kw in bibitem['keywords'].split(\", \"):\n",
    "#     kws_underscore += [matchWhiteSpace_regexp.sub(\"_\", kw.strip())]\n",
    "\n",
    "# keywords = \" \".join(kws_underscore)\n",
    "# keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bibtex = \"\"\"@STRING{ jean = \"Jean\"}\n",
    "\n",
    "# @ARTICLE{Cesar2013,\n",
    "#   author = jean # { CÃ©sar},\n",
    "#   title = {An amazing title},\n",
    "#   year = {2013},\n",
    "#   month = jan,\n",
    "#   volume = {12},\n",
    "#   pages = {12--23},\n",
    "#   journal = {Nice Journal},\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# bp = BibTexParser(interpolate_strings=False)\n",
    "# bib_database = bp.parse(bibtex)\n",
    "# bib_database.entries[0]\n",
    "# as_text(bib_database.entries[0]['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BibTeX::Parser\n",
    "I wasn't able to get this to not collapse whitespace, so below is the hack to get around that.  But it turned out that bibtexparser could do the job w/o the hack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# bibIn = bibtex.Parser().parse_file(bibInFNm)\n",
    "\n",
    "# # get keys and whether or not it has a comment\n",
    "# bibkeyFields = dict()\n",
    "# for bibkey, bibitem in bibIn.entries.items():\n",
    "#     if 'comment' in bibitem.fields:\n",
    "#         bibkeyFields[bibkey] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keysWithComments = list(bibkeyFields.keys())\n",
    "# nextKey = keysWithComments.pop(0)\n",
    "# p = re.compile(f\"^@.*{nextKey},\")\n",
    "# lnum=1\n",
    "# with open(bibInFNm, encoding=\"utf8\") as fileHandler:\n",
    "#     try:\n",
    "#         for ln in fileHandler:\n",
    "#             ln = ln.rstrip()\n",
    "#             if p.match((ln)):\n",
    "#                 print(lnum, ln)\n",
    "#                 nextKey = keysWithComments.pop(0)\n",
    "#                 p = re.compile(f\"^@.*{nextKey},\")\n",
    "#             lnum += 1\n",
    "#     except:\n",
    "#         print(f\"trouble with line: {ln}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

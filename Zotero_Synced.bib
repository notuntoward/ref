@article{Perez87newSimpDiffIrradTilt,
  title = {A New Simplified Version of the Perez Diffuse Irradiance Model for Tilted Surfaces},
  author = {Perez, Richard and Seals, Robert and Ineichen, Pierre and Stewart, Ronald and Menicucci, David},
  year = {1987},
  month = jan,
  journal = {Solar Energy},
  volume = {39},
  number = {3},
  pages = {221--231},
  issn = {0038-092X},
  doi = {10.1016/S0038-092X(87)80031-2},
  url = {https://www.sciencedirect.com/science/article/pii/S0038092X87800312},
  urldate = {2022-09-25},
  abstract = {A new, more accurate and considerably simpler version of the Perez[1] diffuse irradiance model is presented. This model is one of those used currently to estimate short time step (hourly or less) irradiance on tilted planes based on global and direct (or diffuse) irradiance. It has been shown to perform more accurately than other models for a large number of locations worldwide. The key assumptions defining the model remain basically unchanged. These include (1) a description of the sky dome featuring a circumsolar zone and horizon zone superimposed over an isotropic background, and (2) a parameterization of insolation conditions (based on available inputs to the model), determining the value of the radiant power originating from these two zones. Operational modifications performed on the model are presented in a step by step approach. Each change is justified on the basis of increased ease of use and/or overall accuracy. Two years of hourly data on tilted planes from two climatically distinct sites in France are used to verify performance accuracy. The isotropic, Hay and Klucher models are used as reference. Major changes include (1) the simplification of the governing equation by use of reduced brightness coefficients; (2) the allowance for negative coefficients; (3) reduction of the horizon band to an arc-of-great-circle; (4) optimization of the circumsolar region width; and (5) optimization of insolation conditions parameterization.},
  langid = {english},
  annotation = {739 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {/Users/scott/OneDrive/share/ref/zotero/tmp_papers/Perez87newSimpDiffIrradTilt.pdf}
}

@article{MacKay92BayesianInterpolation,
  title = {Bayesian {{Interpolation}}},
  author = {MacKay, David J. C.},
  year = {1992},
  month = may,
  journal = {Neural Computation},
  volume = {4},
  number = {3},
  pages = {415--447},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.3.415},
  url = {https://doi.org/10.1162/neco.1992.4.3.415},
  urldate = {2022-06-16},
  abstract = {Although Bayesian analysis has been in use since Laplace, the Bayesian method of model-comparison has only recently been developed in depth. In this paper, the Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other data modeling problems. Regularizing constants are set by examining their posterior probability distribution. Alternative regularizers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. ``Occam's razor'' is automatically embodied by this process. The way in which Bayes infers the values of regularizing constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling.},
  annotation = {4243 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {The Bayesian Regularizaion backprop algorithm used by Matlab's 'trainbr'. See: http://www.mathworks.com/help/nnet/ref/trainbr.html},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\MacKay92BayesianInterpolation.pdf}
}

@inproceedings{Maryasin22priceFrcstElecMktNNcmpr,
  title = {Comparing {{Neural Networks}} in {{Forecasting Market Electricity Prices}} and {{Regional Energy Consumption}}},
  booktitle = {2022 {{International Conference}} on {{Industrial Engineering}}, {{Applications}} and {{Manufacturing}} ({{ICIEAM}})},
  author = {Maryasin, Oleg Yu. and Lukashov, Andrey I.},
  year = {2022},
  month = may,
  pages = {40--46},
  doi = {10.1109/ICIEAM54945.2022.9787216},
  abstract = {The paper compares popular architectures of artificial neural networks used in forecasting market electricity prices and the total regional energy consumption. Various structures of neural networks used in forecasting have been considered. These are multilayer perceptron and popular architectures of deep neural networks, as well as their combinations. The structures of input datasets for forecasting using chosen neural networks have been described. To compare various neural networks, the authors performed many numerical experiments, the results of which are presented in the paper. Based on these results, researchers may choose a neural network structure, which allows improving the reliability and accuracy of forecasting market electricity prices and the total regional energy consumption.},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]}
}

@article{Luo19priceFrcstElecDataSrc,
  title = {A Two-Stage Supervised Learning Approach for Electricity Price Forecasting by Leveraging Different Data Sources},
  author = {Luo, Shuman and Weng, Yang},
  year = {2019},
  month = may,
  journal = {Applied Energy},
  volume = {242},
  pages = {1497--1512},
  issn = {03062619},
  doi = {10.1016/j.apenergy.2019.03.129},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0306261919305380},
  urldate = {2022-05-17},
  abstract = {Over the years, the growing penetration of renewable energy into the electricity market has resulted in a significant change in the electricity market price. This change makes the existing forecasting method prone to error, decreasing the economic benefits. Hence, more precise forecasting methods need to be developed. This paper starts with a survey and benchmark of existing machine learning approaches for forecasting the real-time market (RTM) price. While these methods provide sufficient modeling capability via supervised learning, their accuracy is still limited due to the single data source, e.g., historical price information only. In this paper, a novel twostage supervised learning approach is proposed by diversifying the data sources such as highly correlated power data. This idea is inspired by the recent load forecasting methods that have shown extremely well performances. Specifically, the proposed two-stage method, namely the rerouted method, learns two types of mapping rules. The first one is the mapping between the historical wind power and the historical price. The second is the forecasting rule for wind generation. Based on the two rules, we forecast the price via the forecasted generation and the first learned mapping between power and price. Additionally, we observed that it is not the more training data the better, leading to our validation steps to quantify the best training intervals for different datasets. We conduct comparisons of numerical results between existing methods and the proposed methods based on datasets from the Electric Reliability Council of Texas (ERCOT). For each machine learning step, we examine different learning methods, such as polynomial regression, support vector regression, neural network, and deep neural network. The results show that the proposed method is significantly better than existing approaches when renewables are involved.},
  langid = {english},
  annotation = {44 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Luo19priceFrcstElecDataSrc.pdf}
}

@article{Kumar20AsymmetricalResponseCalifornia,
  title = {Asymmetrical Response of {{California}} Electricity Demand to Summer-Time Temperature Variation},
  author = {Kumar, Rohini and Rachunok, Benjamin and {Maia-Silva}, Debora and Nateghi, Roshanak},
  year = {2020},
  month = jul,
  journal = {Scientific Reports},
  volume = {10},
  pages = {10904},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-67695-y},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7331730/},
  urldate = {2022-05-27},
  abstract = {Current projections of the climate-sensitive portion of residential electricity demand are based on estimating the temperature response of the mean of the demand distribution. In this work, we show that there is significant asymmetry in the summer-time temperature response of electricity demand in the state of California, with high-intensity demand demonstrating a greater sensitivity to temperature increases. The greater climate sensitivity of high-intensity demand is found not only in the observed data, but also in the projections in the near future (2021--2040) and far future periods (2081--2099), and across all~(three) utility service regions in California. We illustrate that disregarding the asymmetrical climate sensitivity of demand can lead to underestimating high-intensity demand in a given period by 37--43\%. Moreover, the discrepancy in the projected increase in the climate-sensitive portion of demand based on the 50th versus 90{\textbackslash}documentclass[12pt]\{minimal\} 				{\textbackslash}usepackage\{amsmath\} 				{\textbackslash}usepackage\{wasysym\}  				{\textbackslash}usepackage\{amsfonts\}  				{\textbackslash}usepackage\{amssymb\}  				{\textbackslash}usepackage\{amsbsy\} 				{\textbackslash}usepackage\{mathrsfs\} 				{\textbackslash}usepackage\{upgreek\} 				{\textbackslash}setlength\{{\textbackslash}oddsidemargin\}\{-69pt\} 				{\textbackslash}begin\{document\}\$\$\{th\}\$\${\textbackslash}end\{document\}th quantile estimates could range from 18 to 40\% over the next 20 years.},
  pmcid = {PMC7331730},
  pmid = {32616812},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {/Users/scott/OneDrive/share/ref/zotero/tmp_papers/Kumar20AsymmetricalResponseCalifornia.pdf}
}

@article{Phu21NonlinearTemperatureResponse,
  title = {Nonlinear Temperature Response of Electricity Loads and Implications for Power Development Policies in {{Vietnam}}},
  author = {Phu, Le Viet},
  year = {2021},
  month = nov,
  journal = {Energy and Buildings},
  volume = {251},
  pages = {111339},
  issn = {0378-7788},
  doi = {10.1016/j.enbuild.2021.111339},
  url = {https://www.sciencedirect.com/science/article/pii/S037877882100623X},
  urldate = {2022-05-27},
  abstract = {Understanding how electricity demand responds to the ambient temperature is critical for the safe and efficient operation of the power system. Examining an extended series of daily electricity records from 2011 to 2020, we find that there is a significant nonlinear temperature-electricity load response approximating a cubic function of the temperature or the cooling degree days in the two major power subsystems in northern and southern Vietnam. Such a nonlinearity indicates a major concern for power supply infrastructure to meet increasingly more severe weather-induced load variations. We conclude the paper with several critical recommendations for future power development policies in Vietnam.},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Phu21NonlinearTemperatureResponse.pdf}
}

@article{Lin21ElectricityConsumptionForecast,
  title = {Electricity {{Consumption Forecast}} of {{High-Rise Office Buildings Based}} on the {{Long Short-Term Memory Method}}},
  author = {Lin, Xiaoyu and Yu, Hang and Wang, Meng and Li, Chaoen and Wang, Zi and Tang, Yin},
  year = {2021},
  month = aug,
  journal = {Energies},
  volume = {14},
  number = {16},
  pages = {4785},
  issn = {1996-1073},
  doi = {10.3390/en14164785},
  url = {https://www.mdpi.com/1996-1073/14/16/4785},
  urldate = {2022-05-27},
  abstract = {Various algorithms predominantly use data-driven methods for forecasting building electricity consumption. Among them, algorithms that use deep learning methods and, long and short-term memory (LSTM) have shown strong prediction accuracy in numerous fields. However, the LSTM algorithm still has certain limitations, e.g., the accuracy of forecasting the building air conditioning power consumption was not very high. To explore ways of improving the prediction accuracy, this study selects a high-rise office building in Shanghai to predict the air conditioning power consumption and lighting power consumption, respectively and discusses the influence of weather parameters and schedule parameters on the prediction accuracy. The results demonstrate that using the LSTM algorithm to accurately predict the electricity consumption of air conditioners is more challenging than predicting lighting electricity consumption. To improve the prediction accuracy of air conditioning power consumption, two parameters, relative humidity, and scheduling, must be added to the prediction model.},
  langid = {english},
  annotation = {12 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\linElectricityConsumptionForecast2021.pdf}
}

@article{Maia-Silva20CriticalRoleHumidity,
  title = {The Critical Role of Humidity in Modeling Summer Electricity Demand across the {{United States}}},
  author = {{Maia-Silva}, Debora and Kumar, Rohini and Nateghi, Roshanak},
  year = {2020},
  month = apr,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {1686},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-15393-8},
  url = {https://www.nature.com/articles/s41467-020-15393-8},
  urldate = {2022-05-27},
  abstract = {Cooling demand is projected to increase under climate change. However, most of the existing projections are based on rising air temperatures alone, ignoring that rising temperatures are associated with increased humidity; a lethal combination that could significantly increase morbidity and mortality rates during extreme heat events. We bridge this gap by identifying the key measures of heat stress, considering both air temperature and near-surface humidity, in characterizing the climate sensitivity of electricity demand at a national scale. Here we show that in many of the high energy consuming states, such as California and Texas, projections based on air temperature alone underestimates cooling demand by as much as 10--15\% under both present and future climate scenarios. Our results establish that air temperature is a necessary but not sufficient variable for adequately characterizing the climate sensitivity of cooling load, and that near-surface humidity plays an equally important role.},
  copyright = {2020 The Author(s)},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {/Users/scott/OneDrive/share/ref/zotero/tmp_papers/Maia-Silva20CriticalRoleHumidity.pdf}
}

@article{wangCoolingLoadForecastingbased2019,
  title = {Cooling Load Forecasting-Based Predictive Optimisation for Chiller Plants},
  author = {Wang, Lan and Lee, Eric Wai Ming and Yuen, Richard KK and Feng, Wei},
  year = {2019},
  journal = {Energy and Buildings},
  volume = {198},
  pages = {261--274},
  publisher = {Elsevier},
  file = {/Users/scott/OneDrive/share/ref/zotero/tmp_papers/Wang19CoolingLoadForecastingbased.pdf}
}

@misc{kaggleScikitoptimizeLightGBMTutorial2021,
  title = {Scikit-Optimize for {{LightGBM Tutorial}} with {{Luca Massaron}} {\textbar} {{Kaggle}}'s \#{{30daysofML}} 2021},
  author = {{Kaggle}},
  year = {2021},
  month = oct,
  url = {https://www.youtube.com/watch?v=AFtjWuwqpSQ},
  urldate = {2022-04-11},
  abstract = {SUBSCRIBE: https://www.youtube.com/c/kaggle?sub\_...    About Kaggle:  Kaggle is the world's largest community of data scientists. Join us to compete, collaborate, learn, and do your data science work. Kaggle's platform is the fastest way to get started on a new data science project. Spin up a Jupyter notebook with a single click. Build with our huge repository of free code and data. Stumped? Ask the friendly Kaggle community for help.    Follow Kaggle online:  Visit the WEBSITE: http://www.kaggle.com/?utm\_medium=you...  Like Kaggle on FACEBOOK: http://www.facebook.com/kaggle?utm\_me...  Follow Kaggle on TWITTER: http://twitter.com/kaggle?utm\_medium=...  Check out our BLOG: http://blog.kaggle.com/?utm\_medium=yo...  Connect with us on LINKEDIN: http://www.linkedin.com/company/kaggl...    Advance your data science skills:  Take our free online courses: http://www.kaggle.com/learn/overview?...  Get started with Kaggle Kernels: http://www.kaggle.com/docs/kernels?ut...  Download clean datasets from Kaggle: http://www.kaggle.com/docs/datasets?u...  Sign up for a Kaggle Competition: http://www.kaggle.com/docs/competitio...  Explore the Kaggle Public API: http://www.kaggle.com/docs/api?utm\_me...    Kaggle  https://www.youtube.com/c/kaggle}
}

@techreport{ArizeAI22mlObservability,
  title = {Machine {{Learning Observability}} 101},
  author = {{Arize AI}},
  year = {2022},
  url = {https://arize.com/resource/ebook-machine-learning-observability-101/},
  urldate = {2022-04-10},
  abstract = {What is ML Observability?  ML Observability is the practice of obtaining a deep understanding of a model's  performance across all stages of the model development cycle: as it's being built,  once it's been deployed, and long into its life in production. In practice, ML observability is often the key difference between a team that flies  blind after deploying a model and a team that can iterate and improve on their  models quickly.  Is Model Observability just a fancy word for ML monitoring? Model observability begins with the process of collecting model evaluations  in environments such as training, validation, and production, then tying them  together with analytics that allows one to connect these points to solve ML  engineering problems. These inferences are stored in a model evaluation store  (credit to Josh Tobin for this term), which hosts the raw inference data.   An evaluation store holds the response of the model, a signature of the model  decisions, to every piece of input data for every model version, in   every environment. An ML observability platform allows teams to analyze model degradation and  to root cause any issues that arise. This ability to diagnose the root cause of a  model's issues, by connecting points across validation and production, is what  differentiates model observability from traditional model monitoring. While  model monitoring consists of setting up alerts on key model performance  metrics such as accuracy, or drift, model observability implies a higher objective  of getting to the bottom of any regressions in performance or anomalous  behavior. We are interested in the why. Monitoring is interested in only  aggregates and alerts. Observability is interested in what we can infer from the  model's predictions, explainability insights, the production feature data, and  the training data, to understand the cause behind model actions and build  workflows to improve},
  langid = {american},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\ArizeAI22mlObservability.pdf}
}

@book{vogtSyntheticPhotovoltaicWind2022,
  title = {Synthetic {{Photovoltaic}} and {{Wind Power Forecasting Data}}},
  author = {Vogt, Stephan and Schreiber, Jens and Sick, Bernhard},
  year = {2022},
  month = apr,
  abstract = {Photovoltaic and wind power forecasts in power systems with a high share of renewable energy are essential in several applications. These include stable grid operation, profitable power trading, and forward-looking system planning. However, there is a lack of publicly available datasets for research on machine learning based prediction methods. This paper provides an openly accessible time series dataset with realistic synthetic power data. Other publicly and non-publicly available datasets often lack precise geographic coordinates, timestamps, or static power plant information, e.g., to protect business secrets. On the opposite, this dataset provides these. The dataset comprises 120 photovoltaic and 273 wind power plants with distinct sides all over Germany from 500 days in hourly resolution. This large number of available sides allows forecasting experiments to include spatial correlations and run experiments in transfer and multi-task learning. It includes side-specific, power source-dependent, non-synthetic input features from the ICON-EU weather model. A simulation of virtual power plants with physical models and actual meteorological measurements provides realistic synthetic power measurement time series. These time series correspond to the power output of virtual power plants at the location of the respective weather measurements. Since the synthetic time series are based exclusively on weather measurements, possible errors in the weather forecast are comparable to those in actual power data. In addition to the data description, we evaluate the quality of weather-prediction-based power forecasts by comparing simplified physical models and a machine learning model. This experiment shows that forecasts errors on the synthetic power data are comparable to real-world historical power measurements.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\vogtSyntheticPhotovoltaicWind2022.pdf}
}

@article{Dreher22storageOpReinfLrn,
  title = {{{AI}} Agents Envisioning the Future: {{Forecast-based}} Operation of Renewable Energy Storage Systems Using Hydrogen with {{Deep Reinforcement Learning}}},
  shorttitle = {{{AI}} Agents Envisioning the Future},
  author = {Dreher, Alexander and Bexten, Thomas and Sieker, Tobias and Lehna, Malte and Sch{\"u}tt, Jonathan and Scholz, Christoph and Wirsum, Manfred},
  year = {2022},
  month = mar,
  journal = {Energy Conversion and Management},
  pages = {115401},
  issn = {01968904},
  doi = {10.1016/j.enconman.2022.115401},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0196890422001972},
  urldate = {2022-03-16},
  abstract = {Hydrogen-based energy storage has the potential to compensate for the volatility of renewable power generation in energy systems with a high renewable penetration. The operation of these storage facilities can be optimized using automated energy management systems. This work presents a Reinforcement Learning-based energy management approach in the context of CO2-neutral hydrogen production and storage for an industrial combined heat and power application. The economic performance of the presented approach is compared to a rule-based energy management strategy as a lower benchmark and a Dynamic Programming-based unit commitment as an upper benchmark. The comparative analysis highlights both the potential benefits and drawbacks of the implemented Reinforcement Learning approach. The simulation results indicate a promising potential of Rein\- forcement Learning-based algorithms for hydrogen production planning, outperforming the lower benchmark. Furthermore, a novel approach in the scientific literature demonstrates that including energy and price forecasts in the Reinforcement Learning observation space significantly improves optimization results and allows the al\- gorithm to take variable prices into account. An unresolved challenge, however, is balancing multiple conflicting objectives in a setting with few degrees of freedom. As a result, no parameterization of the reward function could be found that fully satisfied all predefined targets, highlighting one of the major challenges for Reinforcement Learning -based energy management algorithms to overcome.},
  langid = {english},
  annotation = {17 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {/Users/scott/OneDrive/share/ref/zotero/tmp_papers/Dreher22storageOpReinfLrn.pdf}
}

@article{Mayer20optPVplantGenetic,
  title = {Techno-Economic Optimization of Grid-Connected, Ground-Mounted Photovoltaic Power Plants by Genetic Algorithm Based on a Comprehensive Mathematical Model},
  author = {Mayer, Martin J{\'a}nos and Gr{\'o}f, Gyula},
  year = {2020},
  month = may,
  journal = {Solar Energy},
  volume = {202},
  pages = {210--226},
  issn = {0038-092X},
  doi = {10.1016/j.solener.2020.03.109},
  url = {https://www.sciencedirect.com/science/article/pii/S0038092X20303558},
  urldate = {2022-03-14},
  abstract = {The increasing penetration of photovoltaic (PV) technology calls for the development of an effective method for optimization of grid-connected photovoltaic power plants. This paper presents a simultaneous optimization method of ten important design parameters of a PV plant, including the module power, inverter sizing, support structure dimensions, cable losses, module orientation and row spacing. A mathematical PV performance model taking into account the important effects and losses and an economic cost model were developed and presented in detail. The objective function is the internal rate of return and the optimization is performed by a genetic algorithm. The results show that the proposed models and method are capable to optimize the grid-connected PV plant and provide reliable results after a 6--7~min calculation time. The method was demonstrated in detail for a Hungarian location, including the losses and cost structure of the optimal plant configuration. The optimization was also performed for 5 additional sites around the world to assess the effect of location and meteorology. The impact of the decreasing PV module prices on the optimal design is calculated to identify the expected future trends in PV plant design. The presented optimization method can be utilized to facilitate the optimal design of commercial PV plants and for research purposes.},
  langid = {english},
  annotation = {38 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Mayer20optPVplantGenetic.pdf}
}

@article{Marion17aoiCorrectDiffusePV,
  title = {Numerical Method for Angle-of-Incidence Correction Factors for Diffuse Radiation Incident Photovoltaic Modules},
  author = {Marion, Bill},
  year = {2017},
  month = may,
  journal = {Solar Energy},
  volume = {147},
  pages = {344--348},
  issn = {0038-092X},
  doi = {10.1016/j.solener.2017.03.027},
  url = {https://www.sciencedirect.com/science/article/pii/S0038092X17301883},
  urldate = {2022-03-14},
  abstract = {A numerical method is provided for solving the integral equation for the angle-of-incidence (AOI) correction factor for diffuse radiation incident photovoltaic (PV) modules. The types of diffuse radiation considered include sky, circumsolar, horizon, and ground-reflected. The method permits PV module AOI characteristics to be addressed when calculating AOI losses associated with diffuse radiation. Pseudo code is provided to aid users in the implementation, and results are shown for PV modules with tilt angles from 0{$^\circ$} to 90{$^\circ$}. Diffuse AOI losses are greatest for small PV module tilt angles. Including AOI losses associated with the diffuse irradiance will improve predictions of PV system performance.},
  langid = {english},
  annotation = {37 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Marion17aoiCorrectDiffusePV.pdf}
}

@article{Reindl90hrlyTileIrradMdl,
  title = {Evaluation of Hourly Tilted Surface Radiation Models},
  author = {Reindl, D. T. and Beckman, W. A. and Duffie, J. A.},
  year = {1990},
  month = jan,
  journal = {Solar Energy},
  volume = {45},
  number = {1},
  pages = {9--17},
  issn = {0038-092X},
  doi = {10.1016/0038-092X(90)90061-G},
  url = {https://www.sciencedirect.com/science/article/pii/0038092X9090061G},
  urldate = {2022-03-14},
  abstract = {This study investigates the performance of the isotropic and four anisotropic hourly tilted surface radiation models by using monthly average hourly utilizable energy as a standard of measure. Utilizable energy is the radiation above a specified threshold level. Differences between the utilizable energy measured and the utilizable energy predicted are observed for various surface slope/azimuth orientations and critical radiation levels. Normalized root mean square difference and normalized mean bias difference statistics are formed to quantify the ability of each model to estimate the utilizable energy on a tilted surface. The influence of horizontal diffuse radiation on tilted surface model performance is examined by comparing the predicted utilizable energy on a tilted surface using both measured horizontal diffuse and estimated horizontal diffuse found from diffuse fraction correlations. On an overall basis, the isotropic sky model showed the poorest performance and is not recommended for estimating the hourly radiation on a tilted surface. The anisotropic models have comparable performance to each other. There was no significant degradation of tilted surface model performance when the diffuse radiation is estimated from a diffuse fraction correlation rather than obtained from measurements.},
  langid = {english},
  annotation = {584 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Reindl90hrlyTileIrradMdl.pdf}
}

@article{Starke18cloudResOnMinDirrad,
  title = {Resolution of the Cloud Enhancement Problem for One-Minute Diffuse Radiation Prediction},
  author = {Starke, Allan R. and Lemos, Leonardo F. L. and Boland, John and Cardemil, Jos{\'e} M. and Colle, Sergio},
  year = {2018},
  month = sep,
  journal = {Renewable Energy},
  volume = {125},
  pages = {472--484},
  issn = {0960-1481},
  doi = {10.1016/j.renene.2018.02.107},
  url = {https://www.sciencedirect.com/science/article/pii/S0960148118302593},
  urldate = {2022-03-14},
  abstract = {For design and simulation of solar energy systems, quality information about all components of solar irradiance is crucial. In cases when only global irradiance measurements are available, separation models are a useful method to estimate DNI and diffuse irradiation. Several of such models have been developed since the 1960s, most of them aiming to deliver estimates in hourly resolution. For higher data resolution, such as in minute data, those models are not able to describe fast transient and cloud enhancement phenomena commonly observed in data with smaller time-steps. This paper proposes an adaptation of the BRL separation model, making it capable of delivering more precise irradiance estimates for higher resolution data. Two models result from this adaptation: one for Brazil and other for Australia. The proposed models yield a more precise DNI and diffuse fraction estimates to their respective countries, compared to other separation models commonly used in the technical literature. For example, using the recommended Combined Performance Index (CPI) as a single statistical indicator, the proposed model yields DNI estimates with CPI from 230 to 350\% for Australia, and from 270 to 800\% for Brazil, while the Engerer model, recently recommended as a ``quasi-universal'' 1-min separation model, yields DNI estimates with CPI from 500 to 700\% for Australia, and from 600 to 1800\% for Brazil.},
  langid = {english},
  annotation = {39 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Starke18cloudResOnMinDirrad.pdf}
}

@inproceedings{Tabone18solarGenDisaggr,
  title = {Disaggregating Solar Generation behind Individual Meters in Real Time},
  booktitle = {Proceedings of the 5th {{Conference}} on {{Systems}} for {{Built Environments}}},
  author = {Tabone, Michaelangelo and Kiliccote, Sila and Kara, Emre Can},
  year = {2018},
  month = nov,
  pages = {43--52},
  publisher = {ACM},
  address = {Shenzen China},
  doi = {10.1145/3276774.3276776},
  url = {https://dl.acm.org/doi/10.1145/3276774.3276776},
  urldate = {2022-03-14},
  abstract = {Real-time photovoltaic (PV) generation information is crucial for distribution system operations such as switching, state-estimation, and voltage management. However, most behind-the-meter solar installations are not monitored. Typically, the only information available to the distribution system operator is the installed capacity of solar behind each meter; though in some cases even the presence of solar may be unknown. We present a method for disaggreagating behind-the-meter solar generation using only information that is already available in most distribution systems: advanced metering infrastructure, substation monitoring, and generation monitoring at a few PV systems nearby the circuit. The proposed method accurately predicts which homes have solar in over 90\% of cases, and recovers the 15-min resolution PV generation signals with root mean square errors between 20\% and 50\% of average daily PV generation both historically and real-time. A sensitivity analysis shows the method to be robust to the number of buildings and time span of data used to fit. However including more than 3 solar proxies can cause false positive of PV systems behind meters. We find that the proposed method performs better at homes that export electricity to the grid more often.},
  isbn = {978-1-4503-5951-1},
  langid = {english},
  annotation = {32 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Tabone18solarGenDisaggr.pdf}
}

@article{Quan20probIrradTranspos,
  title = {Probabilistic Solar Irradiance Transposition Models},
  author = {Quan, Hao and Yang, Dazhi},
  year = {2020},
  month = jun,
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {125},
  pages = {109814},
  issn = {1364-0321},
  doi = {10.1016/j.rser.2020.109814},
  url = {https://www.sciencedirect.com/science/article/pii/S136403212030109X},
  urldate = {2022-03-14},
  abstract = {Transposition models convert the solar irradiance received on a horizontal surface to in-plane irradiance. All transposition models to date, unfortunately, only produce deterministic (as oppose to probabilistic) estimates. In modern energy meteorology, having the entire predictive distribution is more desirable than relying only on deterministic estimates. To that end, this paper outlines two strategies for creating probabilistic transposition models (PTMs), that can quantify the various types of uncertainty involved in the modeling process. The first strategy seeks the analytic expressions of measurement, model, and parameter uncertainty, and the final predictive variance is the sum of these three types of uncertainty. On the other hand, the second strategy directly models the overall uncertainty as a whole, and uses ensemble model output statistics to estimate the predictive distribution through optimizing a loss function. Both strategies generate estimates of tilted irradiance with Gaussian predictive distributions. As compared to their deterministic counterparts, PTMs clearly offer more insights on uncertainty quantification, during solar energy system design, simulation, performance evaluation, and power output forecasting.},
  langid = {english},
  annotation = {17 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Quan20probIrradTranspos.pdf}
}

@inproceedings{Chan19PhotovoltaicSystemPerformance,
  title = {Photovoltaic {{System Performance Model}} for {{Output Power Forecasting}}},
  booktitle = {2019 {{IEEE PES Asia-Pacific Power}} and {{Energy Engineering Conference}} ({{APPEEC}})},
  author = {Chan, Aaron Keith Y. and Macabebe, Erees Queen B.},
  year = {2019},
  month = dec,
  pages = {1--6},
  doi = {10.1109/APPEEC45492.2019.8994607},
  abstract = {One of the challenges of using photovoltaic (PV) energy is its intermittent nature. Due to this, managing the electrical grid network becomes difficult. Hence, it is important to be able to predict the output power of a PV system. Being able to do so would help plant managers to effectively manage the supply of energy to meet the demand. In this study a photovoltaic system model composed of a PV array model, a DC cable model, and an inverter model was developed, and was used in conjunction with an irradiance forecasting algorithm to predict the output power of a 1 kW grid-connected photovoltaic system. A total of three system models were evaluated, each differing in the inverter model used. When using measured irradiance as input to the system models, the best model obtained a root mean square error (RMSE) of 32.957 W and a coefficient of determination (R2) of 0.978. When using predicted irradiance as input to the system models, the best model obtained an RMSE ofllS.041 W and an R2 of 0.710.},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chan19PhotovoltaicSystemPerformance.pdf}
}

@inproceedings{Driesse08curvesEfficPVinvert,
  title = {Beyond the Curves: {{Modeling}} the Electrical Efficiency of Photovoltaic Inverters},
  shorttitle = {Beyond the Curves},
  booktitle = {2008 33rd {{IEEE Photovoltaic Specialists Conference}}},
  author = {Driesse, Anton and Jain, Praveen and Harrison, Steve},
  year = {2008},
  month = may,
  pages = {1--6},
  issn = {0160-8371},
  doi = {10.1109/PVSC.2008.4922827},
  abstract = {It has been noted that the models typically used to represent inverters in simulation and design tools at the present are inadequate because they do not capture the variations in electrical efficiency over the full range of operating conditions. Data to develop more detailed models have been scarce in the past, but are now increasingly available from multiple sources, therefore it is time to rectify the situation. This paper examines efficiency measurements for a wide range of different inverter products at multiple power levels and input voltages. A model is developed that expresses efficiency as a function of both power and voltage, and it is demonstrated that this model can approximate the efficiency with an appropriate level of accuracy using a small number of parameters. This combination of accuracy and simplicity should facilitate implementation in software and dissemination of model parameters.},
  annotation = {62 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Driesse08curvesEfficPVinvert.pdf}
}

@article{Chen13DeterminingOptimumGridconnected,
  title = {Determining the Optimum Grid-Connected Photovoltaic Inverter Size},
  author = {Chen, Song and Li, Peng and Brady, David and Lehman, Brad},
  year = {2013},
  month = jan,
  journal = {Solar Energy},
  volume = {87},
  pages = {96--116},
  issn = {0038-092X},
  doi = {10.1016/j.solener.2012.09.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0038092X12003362},
  urldate = {2022-03-14},
  abstract = {This paper discusses the practical factors that may influence the inverter sizing strategy. Effects of various factors are studied separately by isolating parameters in the simulations. These factors include irradiance and temperature conditions of the photovoltaic (PV) installation location, PV incentives, electricity rates, and inverter intrinsic parameters such as overload protection schemes and efficiency curves. Specifically, examples of nine different geographic locations in the US are simulated and discussed with realistic parameters to show that the optimum inverter size varies notably by location and context.},
  langid = {english},
  annotation = {81 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chen13DeterminingOptimumGridconnected.pdf}
}

@techreport{King04pvArrayPerform,
  title = {Photovoltaic Array Performance Model.},
  author = {King, David and Boyson, William and Kratochvil, Jay},
  year = {2004},
  month = aug,
  number = {SAND2004-3535, 919131},
  pages = {SAND2004-3535, 919131},
  doi = {10.2172/919131},
  url = {https://www.osti.gov/servlets/purl/919131/},
  urldate = {2022-03-14},
  abstract = {This document summarizes the equations and applications associated with the photovoltaic array performance model developed at Sandia National Laboratories over the last twelve years. Electrical, thermal, and optical characteristics for photovoltaic modules are included in the model, and the model is designed to use hourly solar resource and meteorological data. The versatility and accuracy of the model has been validated for flat-plate modules (all technologies) and for concentrator modules, as well as for large arrays of modules. Applications include system design and sizing, `translation' of field performance measurements to standard reporting conditions, system performance optimization, and real-time comparison of measured versus expected system performance.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\King04pvArrayPerform.pdf}
}

@article{ShenLiang19crystSiliconBifacPV,
  title = {A Review of Crystalline Silicon Bifacial Photovoltaic Performance Characterisation and Simulation},
  author = {Shen~Liang, Tian and Pravettoni, Mauro and Deline, Chris and S.~Stein, Joshua and Kopecek, Radovan and Prakash~Singh, Jai and Luo, Wei and Wang, Yan and G.~Aberle, Armin and Sheng~Khoo, Yong},
  year = {2019},
  journal = {Energy \& Environmental Science},
  volume = {12},
  number = {1},
  pages = {116--148},
  publisher = {Royal Society of Chemistry},
  doi = {10.1039/C8EE02184H},
  url = {https://pubs.rsc.org/en/content/articlelanding/2019/ee/c8ee02184h},
  urldate = {2022-03-14},
  langid = {english},
  annotation = {107 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\ShenLiang19crystSiliconBifacPV.pdf}
}

@article{Loutzenhiser07validIrradIncline,
  title = {Empirical Validation of Models to Compute Solar Irradiance on Inclined Surfaces for Building Energy Simulation},
  author = {Loutzenhiser, P. G. and Manz, H. and Felsmann, C. and Strachan, P. A. and Frank, T. and Maxwell, G. M.},
  year = {2007},
  month = feb,
  journal = {Solar Energy},
  volume = {81},
  number = {2},
  pages = {254--267},
  issn = {0038-092X},
  doi = {10.1016/j.solener.2006.03.009},
  url = {https://www.sciencedirect.com/science/article/pii/S0038092X06000879},
  urldate = {2022-03-14},
  abstract = {Accurately computing solar irradiance on external facades is a prerequisite for reliably predicting thermal behavior and cooling loads of buildings. Validation of radiation models and algorithms implemented in building energy simulation codes is an essential endeavor for evaluating solar gain models. Seven solar radiation models implemented in four building energy simulation codes were investigated: (1) isotropic sky, (2) Klucher, (3) Hay--Davies, (4) Reindl, (5) Muneer, (6) 1987 Perez, and (7) 1990 Perez models. The building energy simulation codes included: EnergyPlus, DOE-2.1E, TRNSYS-TUD, and ESP-r. Solar radiation data from two 25 days periods in October and March/April, which included diverse atmospheric conditions and solar altitudes, measured on the EMPA campus in a suburban area in Duebendorf, Switzerland, were used for validation purposes. Two of the three measured components of solar irradiances -- global horizontal, diffuse horizontal and direct-normal -- were used as inputs for calculating global irradiance on a south-west fa{\c c}ade. Numerous statistical parameters were employed to analyze hourly measured and predicted global vertical irradiances. Mean absolute differences for both periods were found to be: (1) 13.7\% and 14.9\% for the isotropic sky model, (2) 9.1\% for the Hay--Davies model, (3) 9.4\% for the Reindl model, (4) 7.6\% for the Muneer model, (5) 13.2\% for the Klucher model, (6) 9.0\%, 7.7\%, 6.6\%, and 7.1\% for the 1990 Perez models, and (7) 7.9\% for the 1987 Perez model. Detailed sensitivity analyses using Monte Carlo and fitted effects for N-way factorial analyses were applied to assess how uncertainties in input parameters propagated through one of the building energy simulation codes and impacted the output parameter. The implications of deviations in computed solar irradiances on predicted thermal behavior and cooling load of buildings are discussed.},
  langid = {english},
  annotation = {379 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Loutzenhiser07validIrradIncline.pdf}
}

@article{Ineichen02airmassIndpTurbCoeff,
  title = {A New Airmass Independent Formulation for the {{Linke}} Turbidity Coefficient},
  author = {Ineichen, Pierre and Perez, Richard},
  year = {2002},
  month = sep,
  journal = {Solar Energy},
  volume = {73},
  number = {3},
  pages = {151--157},
  issn = {0038-092X},
  doi = {10.1016/S0038-092X(02)00045-2},
  url = {https://www.sciencedirect.com/science/article/pii/S0038092X02000452},
  urldate = {2022-03-14},
  abstract = {We propose a new formulation for the Linke turbidity coefficient with the objective of removing its dependence upon solar geometry. In the process, we also develop two new simple clear sky models for global and direct normal irradiance.},
  langid = {english},
  annotation = {467 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ineichen02airmassIndpTurbCoeff.pdf}
}

@article{Yang19SatelAugDiffuseSolar,
  title = {Satellite-Augmented Diffuse Solar Radiation Separation Models},
  author = {Yang, Dazhi and Boland, John},
  year = {2019},
  month = mar,
  journal = {Journal of Renewable and Sustainable Energy},
  volume = {11},
  number = {2},
  pages = {023705},
  publisher = {American Institute of Physics},
  doi = {10.1063/1.5087463},
  url = {https://aip.scitation.org/doi/abs/10.1063/1.5087463},
  urldate = {2022-03-14},
  annotation = {36 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Yang19SatelAugDiffuseSolar.pdf}
}

@inproceedings{Ross82FlatplatePVmod,
  title = {Flat-Plate Photovoltaic Module and Array Engineering},
  booktitle = {Proceedings of the 1982 {{Annual Meeting}} of the {{American Section}} of the {{International Solar Energy Society}}},
  author = {Ross, A. G.},
  year = {1982},
  pages = {909--914},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ross82FlatplatePVmod.pdf}
}

@article{Dolara15cmprPhysPVmodels,
  title = {Comparison of Different Physical Models for {{PV}} Power Output Prediction},
  author = {Dolara, Alberto and Leva, Sonia and Manzolini, Giampaolo},
  year = {2015},
  month = sep,
  journal = {Solar Energy},
  volume = {119},
  pages = {83--99},
  issn = {0038-092X},
  doi = {10.1016/j.solener.2015.06.017},
  url = {https://www.sciencedirect.com/science/article/pii/S0038092X15003254},
  urldate = {2022-03-13},
  abstract = {The electricity produced from renewable energy, in particular from wind and photovoltaic plants, has seen exponential rise in the last decade. Consequently, the prediction of power produced from these plants is fundamental for the reliability, safety and stability of the grid. This paper compares three physical models describing the PV cell (corresponding to three-, four- and five-parameter equivalent electric circuit) and two thermal models for the cell temperature estimation (NOCT and Sandia). The models were calibrated and tested towards ten monocrystalline and eight polycrystalline modules installed at SolarTechLab at Politecnico di Milano. The hourly error of the forecasted power output is usually lower than 15Wh, while NMAE\% and WMAE\% are in the range of 0.5\% and 10\%. Low errors, calculated with actual weather conditions, suggest that the implemented models are accurate, but they cannot be directly compared with other approaches which adopt weather forecasts. Results show that there is no clear advantage of using complex models, but the data used for the model calibration mostly affect the model accuracy. It was found that forecasted power output are more accurate using experimental data and Sandia's thermal model in monocrystalline cells type, while for the polycrystalline the data from the manufacturer and NOCT have lower errors.},
  langid = {english},
  annotation = {243 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Dolara15cmprPhysPVmodels.pdf}
}

@article{Shwartz-Ziv21deepVsTreeTabData,
  title = {Tabular {{Data}}: {{Deep Learning}} Is {{Not All You Need}}},
  shorttitle = {Tabular {{Data}}},
  author = {{Shwartz-Ziv}, Ravid and Armon, Amitai},
  year = {2021},
  month = nov,
  journal = {arXiv:2106.03253 [cs]},
  eprint = {2106.03253},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.03253},
  urldate = {2022-02-25},
  abstract = {A key element in solving real-life data science problems is selecting the types of models to use. Tree ensemble models (such as XGBoost) are usually recommended for classification and regression problems with tabular data. However, several deep learning models for tabular data have recently been proposed, claiming to outperform XGBoost for some use cases. This paper explores whether these deep models should be a recommended option for tabular data by rigorously comparing the new deep models to XGBoost on various datasets. In addition to systematically comparing their performance, we consider the tuning and computation they require. Our study shows that XGBoost outperforms these deep models across the datasets, including the datasets used in the papers that proposed the deep models. We also demonstrate that XGBoost requires much less tuning. On the positive side, we show that an ensemble of deep models and XGBoost performs better on these datasets than XGBoost alone.},
  archiveprefix = {arXiv},
  annotation = {371 citations (Semantic Scholar/arXiv) [2023-07-25]},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Shwartz-Ziv21deepVsTreeTabData.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Shwartz-Ziv21deepVsTreeTabData2.pdf}
}

@article{Li20solarAerosolSoiling,
  title = {Global Reduction of Solar Power Generation Efficiency Due to Aerosols and Panel Soiling},
  author = {Li, Xiaoyuan and Mauzerall, Denise L. and Bergin, Mike H.},
  year = {2020},
  month = sep,
  journal = {Nature Sustainability},
  volume = {3},
  number = {9},
  pages = {720--727},
  publisher = {Nature Publishing Group},
  issn = {2398-9629},
  doi = {10.1038/s41893-020-0553-2},
  url = {https://www.nature.com/articles/s41893-020-0553-2},
  urldate = {2022-02-02},
  abstract = {Air pollution and dust prevail over many regions that have rapid growth of solar photovoltaic (PV) electricity generation, potentially reducing PV generation. Here we combine solar PV performance modelling with long-term satellite-observation-constrained surface irradiance, aerosol deposition and precipitation rates to provide a global picture of the impact of particulate matter (PM) on PV generation. We consider attenuation caused by both atmospheric PM and PM deposition on panels (soiling) in calculating the overall effect of PM on PV generation, and include precipitation removal of soiling and the benefits of panel cleaning. Our results reveal that, with no cleaning and precipitation-only removal, PV generation in heavily polluted and desert regions is reduced by more than 50\% by PM, with soiling accounting for more than two-thirds of the total reduction. Our findings highlight the benefit of cleaning panels in heavily polluted regions with low precipitation and the potential to increase PV generation through air-quality improvements.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  annotation = {44 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Li20solarAerosolSoiling.pdf}
}

@inproceedings{Guen20probTseriesFrcstSTRIPE,
  title = {Probabilistic {{Time Series Forecasting}} with {{Structured Shape}} and {{Temporal Diversity}}},
  booktitle = {{{NeurIPS}} 2020},
  author = {Guen, Vincent Le and Thome, Nicolas},
  year = {2020},
  month = dec,
  pages = {14},
  address = {Vancouver, Canada},
  abstract = {Probabilistic forecasting consists in predicting a distribution of possible future outcomes. In this paper, we address this problem for non-stationary time series, which is very challenging yet crucially important. We introduce the STRIPE model for representing structured diversity based on shape and time features, ensuring both probable predictions while being sharp and accurate. STRIPE is agnostic to the forecasting model, and we equip it with a diversification mechanism relying on determinantal point processes (DPP). We introduce two DPP kernels for modeling diverse trajectories in terms of shape and time, which are both differentiable and proved to be positive semi-definite. To have an explicit control on the diversity structure, we also design an iterative sampling mechanism to disentangle shape and time representations in the latent space. Experiments carried out on synthetic datasets show that STRIPE significantly outperforms baseline methods for representing diversity, while maintaining accuracy of the forecasting model. We also highlight the relevance of the iterative sampling scheme and the importance to use different criteria for measuring quality and diversity. Finally, experiments on real datasets illustrate that STRIPE is able to outperform state-ofthe-art probabilistic forecasting approaches in the best sample prediction.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Guen20probTseriesFrcstSTRIPE.pdf}
}

@article{GuzmanRazo20geneticDigitalTwinPV,
  title = {A {{Genetic Algorithm Approach}} as a {{Self-Learning}} and {{Optimization Tool}} for {{PV Power Simulation}} and {{Digital Twinning}}},
  author = {Guzman Razo, Dorian Esteban and M{\"u}ller, Bj{\"o}rn and Madsen, Henrik and Wittwer, Christof},
  year = {2020},
  month = dec,
  journal = {Energies},
  volume = {13},
  number = {24},
  pages = {6712},
  issn = {1996-1073},
  doi = {10.3390/en13246712},
  url = {https://www.mdpi.com/1996-1073/13/24/6712},
  urldate = {2022-01-12},
  abstract = {A key aspect for achieving a high-accuracy Photovoltaic (PV) power simulation, and reliable digital twins, is a detailed description of the PV system itself. However, such information is not always accurate, complete, or even available. This work presents a novel approach to learn features of unknown PV systems or subsystems using genetic algorithm optimization. Based on measured PV power, this approach learns and optimizes seven PV system parameters: nominal power, tilt and azimuth angles, albedo, irradiance and temperature dependency, and the ratio of nominal module to nominal inverter power (DC/AC ratio). By optimizing these parameters, we create a digital twin that accurately reflects the actual properties and behaviors of the unknown PV systems or subsystems. To develop this approach, on-site measured power, ambient temperature, and satellite-derived irradiance of a PV system located in south-west Germany are used. The approach proposed here achieves a mean bias error of about 10\% for nominal power, 3{\textopenbullet} for azimuth and tilt angles, between 0.01\%/C and 0.09\%/C for temperature coefficient, and now-casts with an accuracy of around 6\%. In summary, we present a new solution to parametrize and simulate PV systems accurately with limited or no previous knowledge of their properties and features.},
  langid = {english},
  annotation = {8 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\GuzmanRazo20geneticDigitalTwinPV.pdf}
}

@inproceedings{Gruhl21probMultivarNoveltyDet,
  title = {The {{Problem}} with {{Real-World Novelty Detection}} - {{Issues}} in {{Multivariate Probabilistic Models}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Autonomic Computing}} and {{Self-Organizing Systems Companion}} ({{ACSOS-C}})},
  author = {Gruhl, Christian and Hannan, Abdul and Huang, Zhixin and Nivarthi, Chandana and Vogt, Stephan},
  year = {2021},
  month = sep,
  pages = {204--209},
  publisher = {IEEE},
  address = {DC, USA},
  doi = {10.1109/ACSOS-C52956.2021.00055},
  url = {https://ieeexplore.ieee.org/document/9599312/},
  urldate = {2021-12-08},
  abstract = {Novelty and anomaly detection in real-world data streams are becoming more and more important for IoT, industry 4.0 and digital-twin applications. However, most of these algorithms are designed in-vitro and usually not very resilient against the failure behaviour of real-world systems, that is, minor system faults (e.g. a failing sensor, small damage, or firmware updates). In most scenarios, such a minor fault leads to a total failure of the detection engine, resulting either in the constant reporting of an anomaly or a total inability for further detection. In this article we investigate this problem in more detail and present simple approaches to circumvent them.},
  isbn = {978-1-66544-393-7},
  langid = {english},
  annotation = {2 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gruhl21probMultivarNoveltyDet.pdf}
}

@inproceedings{Leprince21symbRgrssnPhysMdls,
  title = {Fifty Shades of Black: Uncovering Physical Models from Symbolic Regressions for Scalable Building Heat Dynamics Identification},
  shorttitle = {Fifty Shades of Black},
  booktitle = {Proceedings of the 8th {{ACM International Conference}} on {{Systems}} for {{Energy-Efficient Buildings}}, {{Cities}}, and {{Transportation}}},
  author = {Leprince, Julien and Miller, Clayton and Frei, Mario and Madsen, Henrik and Zeiler, Wim},
  year = {2021},
  month = nov,
  series = {{{BuildSys}} '21},
  pages = {345--348},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3486611.3491120},
  url = {https://doi.org/10.1145/3486611.3491120},
  urldate = {2021-11-29},
  abstract = {The rapid growth of machine learning (black-box) techniques and computing capacity has started to transform many research domains, including building performance analysis. However, physical interpretation of these models remains a challenge due to their opaque nature. This paper outlines an experiment to unveil analytical expressions from an open-source machine-learning-based algorithm, i.e., symbolic regression. From 241 residential buildings in the Netherlands, 50 unique analytical expressions were produced demonstrating overall better characterization accuracies than an XGBoost baseline, while providing a powerful mean of interpretability from model structures and coefficients. These insights present a starting point for further work towards highly scalable models yielding new characterizations of residential buildings.},
  isbn = {978-1-4503-9114-6},
  annotation = {3 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {/Users/scott/OneDrive/share/ref/zotero/tmp_papers/Leprince21symbRgrssnPhysMdls.pdf}
}

@article{Karniadakis21physInfMLreview,
  title = {Physics-Informed Machine Learning},
  author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  year = {2021},
  month = jun,
  journal = {Nature Reviews Physics},
  volume = {3},
  number = {6},
  pages = {422--440},
  publisher = {Nature Publishing Group},
  issn = {2522-5820},
  doi = {10.1038/s42254-021-00314-5},
  url = {https://www.nature.com/articles/s42254-021-00314-5},
  urldate = {2021-11-06},
  abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  annotation = {726 citations (Semantic Scholar/DOI) [2023-07-25]\\
Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Reviews\\
Subject\_term: Applied mathematics;Computational science\\
Subject\_term\_id: applied-mathematics;computational-science},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Karniadakis21physInfMLreview.pdf}
}

@inproceedings{Lahariya21physicsInfRCcktLSTM,
  title = {Physics-Informed {{Recurrent Neural Networks}} for {{The Identification}} of a {{Generic Energy Buffer System}}},
  booktitle = {2021 {{IEEE}} 10th {{Data Driven Control}} and {{Learning Systems Conference}} ({{DDCLS}})},
  author = {Lahariya, Manu and Karami, Farzaneh and Develder, Chris and Crevecoeur, Guillaume},
  year = {2021},
  month = may,
  pages = {1044--1049},
  publisher = {IEEE},
  address = {Suzhou, China},
  doi = {10.1109/DDCLS52934.2021.9455657},
  url = {https://ieeexplore.ieee.org/document/9455657/},
  urldate = {2021-11-06},
  abstract = {Energy storage is ubiquitous in industrial processes and comes in many forms such as material, chemical, electromechanical buffers. System identification of such energy buffers demands proper estimation/prediction of their physical quantities and unknown parameters. Once these parameters are determined, the identified model can be employed to predict the industrial process dynamics, which finally assist to build efficient control for these processes. This paper proposes physics-informed neural networks-based grey-box modeling methods for the identification of energy buffers. The underlying system dynamics are enforced on the neural network structure to ensure that the identified grey-box model follows the approximate physics. We define two novel grey-box models based on simple and recurrent neural network architectures and test these models for a generic energy buffer. Performance and training time for the proposed grey-box models are compared against a black-box baseline model. Results confirm that imposing the dynamic system's physics on the network improves the performance, and utilizing a recurrent architecture leads to a further improvement.},
  isbn = {978-1-66542-423-3},
  langid = {english},
  annotation = {2 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lahariya21physicsInfRCcktLSTM.pdf}
}

@inproceedings{Misyris20physicsInfNNpowSys,
  title = {Physics-{{Informed Neural Networks}} for {{Power Systems}}},
  booktitle = {2020 {{IEEE Power}} \& {{Energy Society General Meeting}} ({{PESGM}})},
  author = {Misyris, George S. and Venzke, Andreas and Chatzivasileiadis, Spyros},
  year = {2020},
  month = aug,
  pages = {1--5},
  publisher = {IEEE},
  address = {Montreal, QC, Canada},
  doi = {10.1109/PESGM41954.2020.9282004},
  url = {https://ieeexplore.ieee.org/document/9282004/},
  urldate = {2021-11-05},
  abstract = {This paper introduces for the first time, to our knowledge, a framework for physics-informed neural networks in power system applications. Exploiting the underlying physical laws governing power systems, and inspired by recent developments in the field of machine learning, this paper proposes a neural network training procedure that can make use of the wide range of mathematical models describing power system behavior, both in steady-state and in dynamics. Physics-informed neural networks require substantially less training data and can result in simpler neural network structures, while achieving high accuracy. This work unlocks a range of opportunities in power systems, being able to determine dynamic states, such as rotor angles and frequency, and uncertain parameters such as inertia and damping at a fraction of the computational time required by conventional methods. This paper focuses on introducing the framework and showcases its potential using a single-machine infinite bus system as a guiding example. Physics-informed neural networks are shown to accurately determine rotor angle and frequency up to 87 times faster than conventional methods.},
  isbn = {978-1-72815-508-1},
  annotation = {94 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Misyris20physicsInfNNpowSys.pdf}
}

@article{Kroposki20AutonEnergyGridsCtl,
  title = {Autonomous {{Energy Grids}}: {{Controlling}} the {{Future Grid With Large Amounts}} of {{Distributed Energy Resources}}},
  shorttitle = {Autonomous {{Energy Grids}}},
  author = {Kroposki, Benjamin and Bernstein, Andrey and King, Jennifer and Vaidhynathan, Deepthi and Zhou, Xinyang and Chang, Chin-Yao and Dall'Anese, Emiliano},
  year = {2020},
  month = nov,
  journal = {IEEE Power and Energy Magazine},
  volume = {18},
  number = {6},
  pages = {37--46},
  issn = {1558-4216},
  doi = {10.1109/MPE.2020.3014540},
  abstract = {Distributed energy resources (DERs)-which can include solar photovoltaic (PV), fuel cells, microturbines, gensets, distributed energy storage (e.g., batteries and ice storage), and new loads [e.g., electric vehicles (EVs), LED lighting, smart appliances, and electric heat pumps]-are being added to electric grids and causing bidirectional power flows and voltage fluctuations that can impact optimal control and system operation. Residential solar installations are expected to increase approximately 8\% annually through 2050. Customer battery systems are anticipated to reach almost 1.9 GW by 2024, and current forecasts project that approximately 18.7 million EVs will be on U.S. roads in 2030. With numbers like these, it is not unreasonable to imagine a residential electricity customer having at least five controllable DERs.AEGs are multilayer, or hierarchical, cellular -structured electric grid and control systems that enable resilient, reliable, and economic optimization. Supported by a scalable, reconfigurable, and self -organizing information and control infrastructure, AEGs are extremely secure and resilient, and they can operate in real time to ensure economic and reliable performance while systematically integrating energy in all forms. AEGs rely on cellular building blocks that can both self -optimize when isolated from a larger grid and participate in optimal operation when interconnected to a larger grid.},
  annotation = {23 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\share\ref\zotero\papers_to_add\Kroposki20AutonEnergyGridsCtl.pdf}
}

@techreport{Bernstein21rtOptCtrlTR,
  title = {Real-{{Time Optimization}} and {{Control}} of {{Next-Generation Distribution Infrastructure Project}}: {{Final Report}} for {{ARPA-E NODES}}},
  author = {Bernstein, Andrey},
  year = {2021},
  month = jan,
  number = {NREL/TP--5D00-78531, 1762447, MainId:32448},
  pages = {NREL/TP--5D00-78531, 1762447, MainId:32448},
  doi = {10.2172/1762447},
  url = {https://www.osti.gov/servlets/purl/1762447/},
  urldate = {2021-10-22},
  langid = {english},
  file = {C\:\\Users\\scott\\share\\ref\\zotero\\papers_to_add\\Bernstein21rtOptCtrlTR.pdf;C\:\\Users\\scott\\share\\ref\\zotero\\papers_to_add\\Bernstein21rtOptCtrlTR2.pdf}
}

@article{Yang19distribOptSurvey,
  title = {A Survey of Distributed Optimization},
  author = {Yang, Tao and Yi, Xinlei and Wu, Junfeng and Yuan, Ye and Wu, Di and Meng, Ziyang and Hong, Yiguang and Wang, Hong and Lin, Zongli and Johansson, Karl H.},
  year = {2019},
  month = jan,
  journal = {Annual Reviews in Control},
  volume = {47},
  pages = {278--305},
  issn = {1367-5788},
  doi = {10.1016/j.arcontrol.2019.05.006},
  url = {https://www.sciencedirect.com/science/article/pii/S1367578819300082},
  urldate = {2021-10-22},
  abstract = {In distributed optimization of multi-agent systems, agents cooperate to minimize a global function which is a sum of local objective functions. Motivated by applications including power systems, sensor networks, smart buildings, and smart manufacturing, various distributed optimization algorithms have been developed. In these algorithms, each agent performs local computation based on its own information and information received from its neighboring agents through the underlying communication network, so that the optimization problem can be solved in a distributed manner. This survey paper aims to offer a detailed overview of existing distributed optimization algorithms and their applications in power systems. More specifically, we first review discrete-time and continuous-time distributed optimization algorithms for undirected graphs. We then discuss how to extend these algorithms in various directions to handle more realistic scenarios. Finally, we focus on the application of distributed optimization in the optimal coordination of distributed energy resources.},
  langid = {english},
  annotation = {382 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\share\ref\zotero\papers_to_add\Yang19distribOptSurvey.pdf}
}

@article{Karpatne17theoryGuidedDS,
  title = {Theory-Guided {{Data Science}}: {{A New Paradigm}} for {{Scientific Discovery}} from {{Data}}},
  shorttitle = {Theory-Guided {{Data Science}}},
  author = {Karpatne, Anuj and Atluri, Gowtham and Faghmous, James and Steinbach, Michael and Banerjee, Arindam and Ganguly, Auroop and Shekhar, Shashi and Samatova, Nagiza and Kumar, Vipin},
  year = {2017},
  month = oct,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {29},
  number = {10},
  eprint = {1612.08544},
  pages = {2318--2331},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2017.2720168},
  url = {http://arxiv.org/abs/1612.08544},
  urldate = {2021-10-10},
  abstract = {Data science models, although successful in a number of commercial domains, have had limited applicability in scientific problems involving complex physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery. The overarching vision of TGDS is to introduce scientific consistency as an essential component for learning generalizable models. Further, by producing scientifically interpretable models, TGDS aims to advance our scientific understanding by discovering novel domain insights. Indeed, the paradigm of TGDS has started to gain prominence in a number of scientific disciplines such as turbulence modeling, material discovery, quantum chemistry, bio-medical science, bio-marker discovery, climate science, and hydrology. In this paper, we formally conceptualize the paradigm of TGDS and present a taxonomy of research themes in TGDS. We describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines. We also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science.},
  archiveprefix = {arXiv},
  annotation = {662 citations (Semantic Scholar/arXiv) [2023-07-25]\\
662 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Easy to read summary article based on this paper and similar:
\par
https://towardsdatascience.com/weaving-machine-learning-into-physics-a-data-scientists-point-of-view-63f60a95f5be},
  file = {/Users/scott/OneDrive/share/ref/zotero/tmp_papers/Karpatne17theoryGuidedDS.pdf}
}

@article{Elkholy19OptimalParametersEstimation,
  title = {Optimal Parameters Estimation and Modelling of Photovoltaic Modules Using Analytical Method},
  author = {Elkholy, A. and {Abou El-Ela}, A. A.},
  year = {2019},
  month = jul,
  journal = {Heliyon},
  volume = {5},
  number = {7},
  pages = {e02137},
  issn = {2405-8440},
  doi = {10.1016/j.heliyon.2019.e02137},
  url = {https://www.sciencedirect.com/science/article/pii/S2405844019357974},
  urldate = {2021-09-26},
  abstract = {This paper introduces a proposed approach to estimate the optimal parameters of the photovoltaic (PV) modules using in-field outdoor measurements and manufacturers' datasheet as well as employing the nonlinear least-squares fitting algorithm. The main goal is to determine the optimal parameter values of the implemented model which are: series resistance, reverse saturation current, photocurrent, ideality factor and shunt resistance in case of the five parameters model. A Microsoft Excel spreadsheet is developed in order to perform modeling and analysis of the parameters analytical initial values using manufacturer datasheet specifications regarding to the changing in solar irradiance and ambient temperature. Then, the sum of the squared residuals between in-field measured and simulated data are calculated and minimized using Excel solver in order to obtain the optimal values of the parameters simultaneously, to describe the best fit for the outdoor measured data. The proposed approach is used to find the optimal parameters of the PV module TRINA TSM-295 using an array tester. The convergence confidences of the estimated parameters are presented and assessed in an easy way. This approach allows all parameters to be optimized, simultaneously. The results are verified and compared with other research studies for different PV cell technologies. The obtained results are useful for the tested PV module manufacturer and assess the performance of the products in different weather conditions.},
  langid = {english},
  annotation = {40 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\tmp_papers\Elkholy19OptimalParametersEstimation.pdf}
}

@article{MdSabudin19ParameterEstimationMathematical,
  title = {Parameter {{Estimation}} in {{Mathematical Modelling}} for {{Photovoltaic Panel}}},
  author = {Md Sabudin, Siti Nurashiken and Jamil, Norazaliza Mohd},
  year = {2019},
  month = jun,
  journal = {IOP Conference Series: Materials Science and Engineering},
  volume = {536},
  pages = {012001},
  issn = {1757-899X},
  doi = {10.1088/1757-899X/536/1/012001},
  url = {https://iopscience.iop.org/article/10.1088/1757-899X/536/1/012001},
  urldate = {2021-09-26},
  abstract = {The demand for solar photovoltaic (PV) system is growing rapidly driven by new technology and strong economies of scale. PV systems directly convert solar energy into electricity without release any pollution into the environment and deplete natural resources. PV technology has matured and its reliability keeps improving. However, PV system is more expensive to produce than conventional sources of energy due in part to the cost of manufacturing PV devices and in part to the conversion efficiencies of the equipment. Besides, important attention in designing, developing and installing the PV systems is time-consuming. In this paper, we propose a mathematical model to predict the PV systems behaviour and performance by considering the plausible factors. The factors accept in this model are solar irradiance and manufacturers' information for the type of PV panel. A case study at the eastern part of Peninsular Malaysia was conducted to examine the effect of factors on the performance of PV. Through the comparative analysis, the results have a good agreement.},
  langid = {english},
  annotation = {7 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\MdSabudin19ParameterEstimationMathematical.pdf}
}

@article{PratapChandran21OptimalModelParameter,
  title = {Optimal Model Parameter Estimation of Solar and Fuel Cells Using Improved Estimation of Distribution Algorithm},
  author = {Pratap Chandran, Benin and Immanuel Selvakumar, A. and Shine Let, G. and Paul Sathiyan, S.},
  year = {2021},
  month = jun,
  journal = {Ain Shams Engineering Journal},
  volume = {12},
  number = {2},
  pages = {1693--1700},
  issn = {2090-4479},
  doi = {10.1016/j.asej.2020.07.034},
  url = {https://www.sciencedirect.com/science/article/pii/S2090447920302288},
  urldate = {2021-09-26},
  abstract = {Renewable energy through the use of fuelcellsandsolar cells is one of the popular developments in recent days that produce electricity. Accuratemodelling of fuel cellandsolarcellsareessentialinsimulation and analysis of energy systems with these sources. However, the systems are extremely nonlinear and complicated. The model needs to be optimized under distinct operating circumstances. Enhanced and streamlined Improved Estimation of Distribution (IED) Algorithm is suggested in this paper to estimate the parameter through optimization for solar cell models and fuel cell models. This is accomplished through the introduction of an ideal approach to improve population quality and the use of a local search to improve the efficiency of the finest globalsolution further. The design of an IED algorithm is much more straightforward and search efficiency is greatly improved compared with the fundamental optimization techniques from the literature.},
  langid = {english},
  annotation = {10 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\PratapChandran21OptimalModelParameter.pdf}
}

@article{Chen21physicalHardConstraintNN,
  title = {Theory-Guided Hard Constraint Projection ({{HCP}}): {{A}} Knowledge-Based Data-Driven Scientific Machine Learning Method},
  shorttitle = {Theory-Guided Hard Constraint Projection ({{HCP}})},
  author = {Chen, Yuntian and Huang, Dou and Zhang, Dongxiao and Zeng, Junsheng and Wang, Nanzhe and Zhang, Haoran and Yan, Jinyue},
  year = {2021},
  month = nov,
  journal = {Journal of Computational Physics},
  volume = {445},
  pages = {110624},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2021.110624},
  url = {https://www.sciencedirect.com/science/article/pii/S0021999121005192},
  urldate = {2021-09-26},
  abstract = {Machine learning models have been successfully used in many scientific and engineering fields. However, it remains difficult for a model to simultaneously utilize domain knowledge and experimental observation data. The application of knowledge-based symbolic artificial intelligence (AI) represented by expert systems is limited by the expressive ability of the model, and data-driven connectionism AI represented by neural networks is prone to produce predictions that might violate physical principles. In order to fully integrate domain knowledge with observations and make full use of the strong fitting ability of neural networks, this study proposes theory-guided hard constraint projection (HCP). This deep learning model converts physical constraints, such as governing equations, into a form that is easy to handle through discretization, and then implements hard constraint optimization through projection in a patch. Based on rigorous mathematical proofs, theory-guided HCP can ensure that model predictions strictly conform to physical mechanisms in the constraint patch. The training process of theory-guided HCP only needs a small amount of labeled data (sparse observation), and it can supervise the model by combining the coordinates (label-free data) with domain knowledge. The performance of the theory-guided HCP is verified by experiments based on a published heterogeneous subsurface flow problem. The experiments show that theory-guided HCP requires fewer data, and achieves higher prediction accuracy and stronger robustness to noisy observations, than the fully connected neural networks and soft constraint models. Furthermore, due to the application of domain knowledge, theory-guided HCP possesses the ability to extrapolate and can accurately predict points outside of the range of the training dataset.},
  langid = {english},
  annotation = {34 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chen21physicalHardConstraintNN.pdf}
}

@article{Kumar21DesignSimulationStandalone,
  title = {Design and Simulation of Standalone Solar {{PV}} System Using {{PVsyst Software}}: {{A}} Case Study},
  shorttitle = {Design and Simulation of Standalone Solar {{PV}} System Using {{PVsyst Software}}},
  author = {Kumar, Ravi and Rajoria, C. S. and Sharma, Amit and Suhag, Sathans},
  year = {2021},
  month = jan,
  journal = {Materials Today: Proceedings},
  series = {International {{Conference}} on {{Innovations}} in {{Clean Energy Technologies}} ({{ICET2020}})},
  volume = {46},
  pages = {5322--5328},
  issn = {2214-7853},
  doi = {10.1016/j.matpr.2020.08.785},
  url = {https://www.sciencedirect.com/science/article/pii/S2214785320366700},
  urldate = {2021-09-13},
  abstract = {Solar energy is quick creating source of energy in all over the world. The total installed solar power in India till 30th April 2020 is 34811.78\,MW. This paper presents the study of load requirement in mechanical department office in engineering college Bikaner and accordingly, designing and installation of stand-alone solar PV System. Analysis of performance ratio and losses has also been done using PVsyst simulation software. The average annual energy requirement in the department of mechanical engineering office is 1086.24 kWh and the energy available through solar panel is 1143.6 KWh, whereas energy supplied to the user is 1068.12 kWh a little less than the required load. The reduced power capacity of the system is because of different kinds of losses. The performance ratio analysis reveals that the highest PR was recordedin themonth of December is 86\% and lowest PR,64\% was obtained in the month of April, whereas the average PR for year is 72.8\%. The reduced power capacity of the system is because of different kinds of losses.},
  langid = {english},
  annotation = {36 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kumar21DesignSimulationStandalone.pdf}
}

@article{Ghosh18GaussianProcessEmulation,
  title = {Gaussian Process Emulation for Discontinuous Response Surfaces with Applications for Cardiac Electrophysiology Models},
  author = {Ghosh, Sanmitra and Gavaghan, David J. and Mirams, Gary R.},
  year = {2018},
  month = may,
  journal = {arXiv:1805.10020 [stat]},
  eprint = {1805.10020},
  primaryclass = {stat},
  url = {http://arxiv.org/abs/1805.10020},
  urldate = {2021-05-04},
  abstract = {Mathematical models of biological systems are beginning to be used for safety-critical applications, where large numbers of repeated model evaluations are required to perform uncertainty quantification and sensitivity analysis. Most of these models are nonlinear both in variables and parameters/inputs which has two consequences. First, analytic solutions are rarely available so repeated evaluation of these models by numerically solving differential equations incurs a significant computational burden. Second, many models undergo bifurcations in behaviour as parameters are varied. As a result, simulation outputs often contain discontinuities as we change parameter values and move through parameter/input space. Statistical emulators such as Gaussian processes are frequently used to reduce the computational cost of uncertainty quantification, but discontinuities render a standard Gaussian process emulation approach unsuitable as these emulators assume a smooth and continuous response to changes in parameter values. In this article, we propose a novel two-step method for building a Gaussian Process emulator for models with discontinuous response surfaces. We first use a Gaussian Process classifier to detect boundaries of discontinuities and then constrain the Gaussian Process emulation of the response surface within these boundaries. We introduce a novel `certainty metric' to guide active learning for a multi-class probabilistic classifier. We apply the new classifier to simulations of drug action on a cardiac electrophysiology model, to propagate our uncertainty in a drug's action through to predictions of changes to the cardiac action potential. The proposed two-step active learning method significantly reduces the computational cost of emulating models that undergo multiple bifurcations.},
  archiveprefix = {arXiv},
  annotation = {6 citations (Semantic Scholar/arXiv) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Ghosh18GaussianProcessEmulation.pdf}
}

@misc{Ornstein20GaussProceDiscontin,
  title = {Gaussian {{Process Regression Discontinuity}}},
  author = {Ornstein, Joseph T. and {Duck-Mayr}, {\relax Jb}randon},
  year = {2020},
  url = {/paper/Gaussian-Process-Regression-Discontinuity-Ornstein-Duck-Mayr/ff90d4584bd9e84ca918635c2c4cda654e6409a4},
  urldate = {2021-05-04},
  abstract = {In applied settings, regression discontinuity (RD) designs often suffer from noisy data and low power. This tends to produce exaggerated causal effect estimates, typified by implausibly large slope and/or concavity parameters. We propose a new method for estimating causal effects in RD designs called Gaussian Process Regression Discontinuity (GPRD). This approach overcomes the major disadvantages of global polynomial estimators and does so with lower variance than local linear estimators. When applied to several recent empirical examples from the published literature, GPRD yields more modest and plausible treatment effect estimates. We make this new method available through the R package gprd. {$\ast$}Postdoctoral Research Associate, Washington University in St. Louis {\dag}PhD Candidate, Washington University in St. Louis 1},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Ornstein20GaussProceDiscontin.pdf}
}

@article{Pope21GaussProcDiscontinClouds,
  title = {Gaussian {{Process Modeling}} of {{Heterogeneity}} and {{Discontinuities Using Voronoi Tessellations}}},
  author = {Pope, Christopher A. and Gosling, John Paul and Barber, Stuart and Johnson, Jill S. and Yamaguchi, Takanobu and Feingold, Graham and Blackwell, Paul G.},
  year = {2021},
  month = jan,
  journal = {Technometrics},
  volume = {63},
  number = {1},
  pages = {53--63},
  publisher = {Taylor \& Francis},
  issn = {0040-1706},
  doi = {10.1080/00401706.2019.1692696},
  url = {https://doi.org/10.1080/00401706.2019.1692696},
  urldate = {2021-05-04},
  abstract = {Many methods for modeling functions over high-dimensional spaces assume global smoothness properties; such assumptions are often violated in practice. We introduce a method for modeling functions that display heterogeneity or contain discontinuities. The heterogeneity is dealt with by using a combination of Voronoi tessellation, to partition the input space, and separate Gaussian processes to model the function over different regions of the partitioned space. The proposed method is highly flexible since it allows the Voronoi cells to combine to form regions, which enables nonconvex and disconnected regions to be considered. In such problems, identifying the borders between regions is often of great importance and we propose an adaptive sampling method to gain extra information along such borders. The method is illustrated by simulated examples and an application to real data, in which we see improvements in prediction error over the commonly used stationary Gaussian process and other nonstationary variations. In our application, a computationally expensive computer model that simulates the formation of clouds is investigated, the proposed method more accurately predicts the underlying process at unobserved locations than existing emulation methods. Supplementary materials for this article are available online.},
  annotation = {13 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Pope21GaussProcDiscontinClouds.pdf}
}

@article{Nguyen21TempLtntAutoEncMultiFrcst,
  title = {Temporal {{Latent Auto-Encoder}}: {{A Method}} for {{Probabilistic Multivariate Time Series Forecasting}}},
  shorttitle = {Temporal {{Latent Auto-Encoder}}},
  author = {Nguyen, Nam and Quanz, Brian},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.10460 [cs]},
  eprint = {2101.10460},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2101.10460},
  urldate = {2021-04-24},
  abstract = {Probabilistic forecasting of high dimensional multivariate time series is a notoriously challenging task, both in terms of computational burden and distribution modeling. Most previous work either makes simple distribution assumptions or abandons modeling cross-series correlations. A promising line of work exploits scalable matrix factorization for latent-space forecasting, but is limited to linear embeddings, unable to model distributions, and not trainable end-to-end when using deep learning forecasting. We introduce a novel temporal latent auto-encoder method which enables nonlinear factorization of multivariate time series, learned end-to-end with a temporal deep learning latent space forecast model. By imposing a probabilistic latent space model, complex distributions of the input series are modeled via the decoder. Extensive experiments demonstrate that our model achieves state-of-theart performance on many popular multivariate datasets, with gains sometimes as high as 50\% for several standard metrics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {obsLitNote},
  annotation = {26 citations (Semantic Scholar/arXiv) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nguyen21TempLtntAutoEncMultiFrcst.pdf}
}

@inproceedings{Anderson20EstimatingSubhourlyInverter,
  title = {Estimating {{Subhourly Inverter Clipping Loss From Satellite-Derived Irradiance Data}}},
  booktitle = {2020 47th {{IEEE Photovoltaic Specialists Conference}} ({{PVSC}})},
  author = {Anderson, Kevin and Perry, Kirsten},
  year = {2020},
  month = jun,
  pages = {1433--1438},
  publisher = {IEEE},
  address = {Calgary, AB, Canada},
  doi = {10.1109/PVSC45281.2020.9300750},
  url = {https://ieeexplore.ieee.org/document/9300750/},
  urldate = {2021-04-19},
  abstract = {Photovoltaic system production simulations are conventionally run using hourly weather datasets. Hourly simulations are su{\"y}ciently accurate to predict the majority of long-term system behavior but cannot resolve high-frequency eects like inverter clipping caused by short-duration irradiance variability. Direct modeling of this subhourly clipping error is only possible for the few locations with high-resolution irradiance datasets. This paper describes a method of predicting the magnitude of this error using a machine learning regressor ensemble model, comprised of a random forest and an XGBoost model, and 30-minute satellite irradiance data. The method predicts a correction for each 30-minute interval with the potential to roll up into 60-minute corrections to match an hourly energy model. The model is trained and validated at locations where the error can be directly simulated from 1-minute ground data. The validation shows low bias at most ground station locations. The model is also applied to gridded satellite irradiance to produce a heatmap of the estimated clipping error across the United States. Finally, the relative importance of each predictor satellite variable is retrieved from the model and discussed.},
  isbn = {978-1-72816-115-0},
  langid = {english},
  annotation = {4 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\andersonEstimatingSubhourlyInverter2020.pdf}
}

@incollection{Sun20CopulaMarkovModels,
  title = {Copula and {{Markov Models}}},
  booktitle = {Copula-{{Based Markov Models}} for {{Time Series}}: {{Parametric Inference}} and {{Process Control}}},
  author = {Sun, Li-Hsien and Huang, Xin-Wei and Alqawba, Mohammed S. and Kim, Jong-Min and Emura, Takeshi},
  editor = {Sun, Li-Hsien and Huang, Xin-Wei and Alqawba, Mohammed S. and Kim, Jong-Min and Emura, Takeshi},
  year = {2020},
  series = {{{SpringerBriefs}} in {{Statistics}}},
  pages = {7--28},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-15-4998-4_2},
  url = {https://doi.org/10.1007/978-981-15-4998-4_2},
  urldate = {2021-04-18},
  abstract = {This chapter introduces the basic concepts on copulas and Markov models. We review the formal definition of copulas with its fundamental properties. We then introduce Kendall's tau as a measure of dependence structure for a pair of random variables, and its relationship with a copula. Examples of copulas are reviewed, such as the Clayton copula, the Gaussian copula, the Frank copula, and the Joe copula. Finally, we introduce the copula-based Markov chain time series models and their fundamental properties.},
  isbn = {9789811549984},
  langid = {english}
}

@article{Jo19CopulabasedAlgorithmGenerating,
  title = {Copula-Based Algorithm for Generating Bursty Time Series},
  author = {Jo, Hang-Hyun and Lee, Byoung-Hwa and Hiraoka, Takayuki and Jung, Woo-Sung},
  year = {2019},
  month = aug,
  journal = {Physical Review E},
  volume = {100},
  number = {2},
  eprint = {1904.08795},
  pages = {022307},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.100.022307},
  url = {http://arxiv.org/abs/1904.08795},
  urldate = {2021-04-18},
  abstract = {Dynamical processes in various natural and social phenomena have been described by a series of events or event sequences showing non-Poissonian, bursty temporal patterns. Temporal correlations in such bursty time series can be understood not only by heterogeneous interevent times (IETs) but also by correlations between IETs. Modeling and simulating various dynamical processes requires us to generate event sequences with a heavy-tailed IET distribution and memory effects between IETs. For this, we propose a Farlie-Gumbel-Morgenstern copula-based algorithm for generating event sequences with correlated IETs when the IET distribution and the memory coefficient between two consecutive IETs are given. We successfully apply our algorithm to the cases with heavy-tailed IET distributions. We also compare our algorithm to the existing shuffling method to find that our algorithm outperforms the shuffling method for some cases. Our copula-based algorithm is expected to be used for more realistic modeling of various dynamical processes.},
  archiveprefix = {arXiv},
  annotation = {8 citations (Semantic Scholar/arXiv) [2023-07-25]\\
8 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Comment: 7 pages, 4 figures},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Jo19CopulabasedAlgorithmGenerating.pdf}
}

@article{Jo18LimitsMemoryCoefficient,
  title = {Limits of the Memory Coefficient in Measuring Correlated Bursts},
  author = {Jo, Hang-Hyun and Hiraoka, Takayuki},
  year = {2018},
  month = mar,
  journal = {Physical Review. E},
  volume = {97},
  number = {3-1},
  pages = {032121},
  issn = {2470-0053},
  doi = {10.1103/PhysRevE.97.032121},
  abstract = {Temporal inhomogeneities in event sequences of natural and social phenomena have been characterized in terms of interevent times and correlations between interevent times. The inhomogeneities of interevent times have been extensively studied, while the correlations between interevent times, often called correlated bursts, are far from being fully understood. For measuring the correlated bursts, two relevant approaches were suggested, i.e., memory coefficient and burst size distribution. Here a burst size denotes the number of events in a bursty train detected for a given time window. Empirical analyses have revealed that the larger memory coefficient tends to be associated with the heavier tail of the burst size distribution. In particular, empirical findings in human activities appear inconsistent, such that the memory coefficient is close to 0, while burst size distributions follow a power law. In order to comprehend these observations, by assuming the conditional independence between consecutive interevent times, we derive the analytical form of the memory coefficient as a function of parameters describing interevent time and burst size distributions. Our analytical result can explain the general tendency of the larger memory coefficient being associated with the heavier tail of burst size distribution. We also find that the apparently inconsistent observations in human activities are compatible with each other, indicating that the memory coefficient has limits to measure the correlated bursts.},
  langid = {english},
  pmid = {29776030},
  annotation = {5 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Jo18LimitsMemoryCoefficient.pdf}
}

@article{Jo20BursttreeDecompositionTime,
  title = {Burst-Tree Decomposition of Time Series Reveals the Structure of Temporal Correlations},
  author = {Jo, Hang-Hyun and Hiraoka, Takayuki and Kivel{\"a}, Mikko},
  year = {2020},
  month = jul,
  journal = {Scientific Reports},
  volume = {10},
  number = {1},
  pages = {12202},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-68157-1},
  abstract = {Comprehensive characterization of non-Poissonian, bursty temporal patterns observed in various natural and social processes is crucial for understanding the underlying mechanisms behind such temporal patterns. Among them bursty event sequences have been studied mostly in terms of interevent times (IETs), while the higher-order correlation structure between IETs has gained very little attention due to the lack of a proper characterization method. In this paper we propose a method of representing an event sequence by a burst tree, which is then decomposed into a set of IETs and an ordinal burst tree. The ordinal burst tree exactly captures the structure of temporal correlations that is entirely missing in the analysis of IET distributions. We apply this burst-tree decomposition method to various datasets and analyze the structure of the revealed burst trees. In particular, we observe that event sequences show similar burst-tree structure, such as heavy-tailed burst-size distributions, despite of very different IET distributions. This clearly shows that the IET distributions and the burst-tree structures can be separable. The burst trees allow us to directly characterize the preferential and assortative mixing structure of bursts responsible for the higher-order temporal correlations. We also show how to use the decomposition method for the systematic investigation of such correlations captured by the burst trees in the framework of randomized reference models. Finally, we devise a simple kernel-based model for generating event sequences showing appropriate higher-order temporal correlations. Our method is a tool to make the otherwise overwhelming analysis of higher-order correlations in bursty time series tractable by turning it into the analysis of a tree structure.},
  langid = {english},
  pmcid = {PMC7376115},
  pmid = {32699282},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Jo20BursttreeDecompositionTime.pdf}
}

@article{Jo19AnalyticallySolvableAutocorrelation,
  title = {Analytically Solvable Autocorrelation Function for Weakly Correlated Interevent Times},
  author = {Jo, Hang-Hyun},
  year = {2019},
  month = jul,
  journal = {Physical Review. E},
  volume = {100},
  number = {1-1},
  pages = {012306},
  issn = {2470-0053},
  doi = {10.1103/PhysRevE.100.012306},
  abstract = {Long-term temporal correlations observed in event sequences of natural and social phenomena have been characterized by algebraically decaying autocorrelation functions. Such temporal correlations can be understood not only by heterogeneous interevent times (IETs) but also by correlations between IETs. In contrast to the role of heterogeneous IETs on the autocorrelation function, little is known about the effects due to the correlations between IETs. To rigorously study these effects, we derive an analytical form of the autocorrelation function for the arbitrary IET distribution in the case with weakly correlated IETs, where the Farlie-Gumbel-Morgenstern copula is adopted for modeling the joint probability distribution function of two consecutive IETs. Our analytical results are confirmed by numerical simulations for exponential and power-law IET distributions. For the power-law case, we find a tendency of the steeper decay of the autocorrelation function for the stronger correlation between IETs. Our analytical approach enables us to better understand long-term temporal correlations induced by the correlations between IETs.},
  langid = {english},
  pmid = {31499919},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Jo19AnalyticallySolvableAutocorrelation.pdf}
}

@inproceedings{Zhuang20pvProbSkyState,
  title = {Towards a Simple and Robust Probabilistic Solar Variability Analysis Based on Transition Probability between Variability Classes of Sky State},
  booktitle = {{{EU PVSEC}}},
  author = {Zhuang, Fuqiang and Salvado, Philippe and Mangione, Jean-Philippe and Pitaval, S{\'e}bastien and Vernay, Christophe and Carriere, Thomas and Gschwind, Beno{\^i}t and {Saint-Drenan}, Yves-Marie and Blanc, Philippe},
  year = {2020},
  note = {A one-step-ahead prob forecast based on discrete sky states. Kind of a discrete copula. Maybe a sort-of continuous sky state could be used to make an analagous conditional copula for scenario forecasting.}
}

@article{Alexandrov19GluonTSProbabilisticTime,
  title = {{{GluonTS}}: {{Probabilistic Time Series Models}} in {{Python}}},
  shorttitle = {{{GluonTS}}},
  author = {Alexandrov, Alexander and Benidis, Konstantinos and {Bohlke-Schneider}, Michael and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim and Maddix, Danielle C. and Rangapuram, Syama and Salinas, David and Schulz, Jasper and Stella, Lorenzo and T{\"u}rkmen, Ali Caner and Wang, Yuyang},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.05264 [cs, stat]},
  eprint = {1906.05264},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.05264},
  urldate = {2020-06-03},
  abstract = {We introduce Gluon Time Series (GluonTS)1, a library for deep-learning-based time series modeling. GluonTS simplifies the development of and experimentation with time series models for common tasks such as forecasting or anomaly detection. It provides all necessary components and tools that scientists need for quickly building new models, for efficiently running and analyzing experiments and for evaluating model accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {76 citations (Semantic Scholar/arXiv) [2023-07-25]},
  note = {Comment: ICML Time Series Workshop 2019},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Alexandrov19GluonTSProbabilisticTime.pdf}
}

@article{Hooshmand19EnergyPredictiveModels,
  title = {Energy {{Predictive Models}} with {{Limited Data}} Using {{Transfer Learning}}},
  author = {Hooshmand, Ali and Sharma, Ratnesh},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.02646 [cs, eess]},
  eprint = {1906.02646},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/1906.02646},
  urldate = {2020-06-03},
  abstract = {In this paper, we consider the problem of developing predictive models with limited data for energy assets such as electricity loads, PV power generations, etc. We specifically investigate the cases where the amount of historical data is not sufficient to effectively train the prediction model. We first develop an energy predictive model based on convolutional neural network (CNN) which is well suited to capture the interaday, daily, and weekly cyclostationary patterns, trends and seasonalities in energy assets time series. A transfer learning strategy is then proposed to address the challenge of limited training data. We demonstrate our approach on a usecase of daily electricity demand forecasting. we show practicing the transfer learning strategy on the CNN model results in significant improvement to existing forecasting methods.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {40 citations (Semantic Scholar/arXiv) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hooshmand19EnergyPredictiveModels.pdf}
}

@article{Abdel-Nasser19pvPowFrcst-LSTM-RNN,
  title = {Accurate Photovoltaic Power Forecasting Models Using Deep {{LSTM-RNN}}},
  author = {{Abdel-Nasser}, Mohamed and Mahmoud, Karar},
  year = {2019},
  month = jul,
  journal = {Neural Computing and Applications},
  volume = {31},
  number = {7},
  pages = {2727--2740},
  issn = {1433-3058},
  doi = {10.1007/s00521-017-3225-z},
  url = {https://doi.org/10.1007/s00521-017-3225-z},
  urldate = {2020-06-01},
  abstract = {Photovoltaic (PV) is one of the most promising renewable energy sources. To ensure secure operation and economic integration of PV in smart grids, accurate forecasting of PV power is an important issue. In this paper, we propose the use of long short-term memory recurrent neural network (LSTM-RNN) to accurately forecast the output power of PV systems. The LSTM networks can model the temporal changes in PV output power because of their recurrent architecture and memory units. The proposed method is evaluated using hourly datasets of different sites for a year. We compare the proposed method with three PV forecasting methods. The use of LSTM offers a further reduction in the forecasting error compared with the other methods. The proposed forecasting method can be a helpful tool for planning and controlling smart grids.},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Abdel-Nasser19pvPowFrcst-LSTM-RNN.pdf}
}

@article{Tina12AnalysisForecastErrors,
  title = {Analysis of Forecast Errors for Irradiance on the Horizontal Plane},
  author = {Tina, G.M. and De Fiore, S. and Ventura, C.},
  year = {2012},
  month = dec,
  journal = {Energy Conversion and Management},
  volume = {64},
  pages = {533--540},
  issn = {01968904},
  doi = {10.1016/j.enconman.2012.05.031},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0196890412002786},
  urldate = {2021-04-17},
  langid = {english},
  annotation = {24 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Tina12AnalysisForecastErrors.pdf}
}

@article{Albulescu20CopulaBasedLocalDependence,
  title = {Copula-{{Based Local Dependence Between Energy}}, {{Agriculture}} and {{Metal Commodity Markets}}},
  author = {Albulescu, Claudiu and Tiwari, Aviral Kumar and Ji, Qiang},
  year = {2020},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3530758},
  url = {https://www.ssrn.com/abstract=3530758},
  urldate = {2021-04-17},
  abstract = {This paper studies the extreme dependencies between energy, agriculture and metal commodity markets, with a focus on local co-movements, allowing the identification of asymmetries and changing trend in the degree of co-movements. More precisely, starting from a non-parametric mixture copula, we use a novel copula-based local Kendall's tau approach to measure nonlinear local dependence in regions. In all pairs of commodity indexes, we find increased co-movements in extreme situations, a stronger dependence between energy and other commodity markets at lower tails, and a `V-type' local dependence for the energy-metal pairs. The three-dimensional Kendall's tau plot for upper tails in quantiles shows asymmetric comovements in the energy-metal pairs, which tend to become negative at peak returns. Therefore, we show that the energy market can offer diversification solutions for risk management in the case of extreme bull market events.},
  langid = {english},
  annotation = {42 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Albulescu20CopulaBasedLocalDependence.pdf}
}

@article{BenAlaya14ProbabilisticGaussianCopula,
  title = {Probabilistic {{Gaussian Copula Regression Model}} for {{Multisite}} and {{Multivariable Downscaling}}},
  author = {Ben Alaya, M. A. and Chebana, F. and Ouarda, T. B. M. J.},
  year = {2014},
  month = may,
  journal = {Journal of Climate},
  volume = {27},
  number = {9},
  pages = {3331--3347},
  issn = {0894-8755, 1520-0442},
  doi = {10.1175/JCLI-D-13-00333.1},
  url = {http://journals.ametsoc.org/doi/10.1175/JCLI-D-13-00333.1},
  urldate = {2021-04-17},
  abstract = {Atmosphere--ocean general circulation models (AOGCMs) are useful to simulate large-scale climate evolutions. However, AOGCM data resolution is too coarse for regional and local climate studies. Downscaling techniques have been developed to refine AOGCM data and provide information at more relevant scales. Among a wide range of available approaches, regression-based methods are commonly used for downscaling AOGCM data. When several variables are considered at multiple sites, regression models are employed to reproduce the observed climate characteristics at small scale, such as the variability and the relationship between sites and variables. This study introduces a probabilistic Gaussian copula regression (PGCR) model for simultaneously downscaling multiple variables at several sites. The proposed PGCR model relies on a probabilistic framework to specify the marginal distribution for each downscaled variable at a given day through AOGCM predictors, and handles multivariate dependence between sites and variables using a Gaussian copula. The proposed model is applied for the downscaling of AOGCM data to daily precipitation and minimum and maximum temperatures in the southern part of Quebec, Canada. Reanalysis products are used in this study to assess the potential of the proposed method. Results of the study indicate the superiority of the proposed model over classical regression-based methods and a multivariate multisite statistical downscaling model.},
  langid = {english},
  annotation = {29 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\BenAlaya14ProbabilisticGaussianCopula.pdf}
}

@inproceedings{huangTimespaceDependencyUtilityscale2019,
  title = {Time-Space Dependency of Utility-Scale Solar Photovoltaic Power Generation in the {{National Electricity Market}}},
  booktitle = {23rd {{International Congress}} on {{Modelling}} and {{Simulation}}},
  author = {Huang, Jing},
  year = {2019},
  pages = {575--581},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Huang19TimespaceDependencyUtilityscale.pdf}
}

@article{SilvaFilho12ModelingDependenceDynamics,
  title = {Modeling Dependence Dynamics through Copulas with Regime Switching},
  author = {da Silva Filho, Osvaldo Candido and Ziegelmann, Flavio Augusto and Dueker, Michael J.},
  year = {2012},
  month = may,
  journal = {Insurance: Mathematics and Economics},
  volume = {50},
  number = {3},
  pages = {346--356},
  issn = {0167-6687},
  doi = {10.1016/j.insmatheco.2012.01.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0167668712000029},
  urldate = {2021-04-17},
  abstract = {Measuring dynamic dependence between international financial markets has recently attracted great interest in financial econometrics because the observed correlations rose dramatically during the 2008--09 global financial crisis. Here, we propose a novel approach for measuring dependence dynamics. We include a hidden Markov chain (MC) in the equation describing dependence dynamics, allowing the unobserved time-varying dependence parameter to vary according to both a restricted ARMA process and an unobserved two-state MC. Estimation is carried out via the inference for the margins in conjunction with filtering/smoothing algorithms. We use block bootstrapping to estimate the covariance matrix of our estimators. Monte Carlo simulations compare the performance of regime switching and no switching models, supporting the regime-switching specification. Finally the proposed approach is applied to empirical data, through the study of the S\&P500 (USA), FTSE100 (UK) and BOVESPA (Brazil) stock market indexes.},
  langid = {english},
  annotation = {90 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\SilvaFilho12ModelingDependenceDynamics.pdf}
}

@article{Pircalabu17RegimeswitchingCopulaApproach,
  title = {A Regime-Switching Copula Approach to Modeling Day-Ahead Prices in Coupled Electricity Markets},
  author = {Pircalabu, A. and Benth, F. E.},
  year = {2017},
  month = oct,
  journal = {Energy Economics},
  volume = {68},
  pages = {283--302},
  issn = {0140-9883},
  doi = {10.1016/j.eneco.2017.10.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0140988317303468},
  urldate = {2021-04-17},
  abstract = {The recent price coupling of many European electricity markets has triggered a fundamental change in the interaction of day-ahead prices, challenging additionally the modeling of the joint behavior of prices in interconnected markets. In this paper we propose a regime-switching AR--GARCH copula to model pairs of day-ahead electricity prices in coupled European markets. While capturing key stylized facts empirically substantiated in the literature, this model easily allows us to 1) deviate from the assumption of normal margins and 2) include a more detailed description of the dependence between prices. We base our empirical study on four pairs of prices, namely Germany--France, Germany--Netherlands, Netherlands--Belgium and Germany--Western Denmark. We find that the marginal dynamics are better described by the flexible skew t distribution than the benchmark normal distribution. Also, we find significant evidence of tail dependence in all pairs of interconnected areas we consider. As a first application of the proposed model, we consider the pricing of financial transmission rights, and highlight how the choice of marginal distributions and copula impacts prices. As a second application we consider the forecasting of tail quantiles, and evaluate the out-of-sample performance of competing models.},
  langid = {english},
  annotation = {29 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Pircalabu17RegimeswitchingCopulaApproach.pdf}
}

@inproceedings{munkhammarCopulaCorrelationModeling14,
  title = {Copula Correlation Modeling of Aggregate Solar Irradiance in Spatial Networks},
  booktitle = {6th {{Solar Integration Workshop}}},
  author = {Munkhammar, Joakim and Widen, Joakim},
  year = {0014/2016-11-15},
  pages = {7},
  address = {Vienna, Austria},
  abstract = {Estimating solar irradiance over several locations in a spatial network is of interest for a wide variety of applications, in particular for simulations of distribution grid with high photovoltaic (PV) penetration. This paper presents a method for estimating the clear-sky index for N locations in any spatial network of locations. The model is based on the clear-sky index distribution for one location, and the cross-correlation of clear-sky index between all location pairs. The correlated clearsky index for each location is obtained from a copula model of correlation based on the station pair correlations and the clear-sky index for a single location. In this paper the clear-sky index for a single location is obtained by a bimodal mixture distribution model and the correlation between station pairs is modeled via an exponential model. The model bridges the gap between estimating the clear-sky index for adjacent and maximally dispersed locations. Applications of this model to simulations of aggregate photovoltaic power generation is also discussed.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Munkhammar14CopulaCorrelationModeling.pdf}
}

@article{Fu19TimeSeriesSimulation,
  title = {Time {{Series Simulation}} by {{Conditional Generative Adversarial Net}}},
  author = {Fu, Rao and Chen, Jie and Zeng, Shutian and Zhuang, Yiping and Sudjianto, Agus},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.11419 [cs, eess, stat]},
  eprint = {1904.11419},
  primaryclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/1904.11419},
  urldate = {2021-04-17},
  abstract = {Generative Adversarial Net (GAN) has been proven to be a powerful machine learning tool in image data analysis and generation. In this paper, we propose to use Conditional Generative Adversarial Net (CGAN) to learn and simulate time series data. The conditions can be both categorical and continuous variables containing different kinds of auxiliary information. Our simulation studies show that CGAN is able to learn different kinds of normal and heavy tail distributions, as well as dependent structures of different time series and it can further generate conditional predictive distributions consistent with the training data distributions. We also provide an in-depth discussion on the rationale of GAN and the neural network as hierarchical splines to draw a clear connection with the existing statistical method for distribution generation. In practice, CGAN has a wide range of applications in the market risk and counterparty risk analysis: it can be applied to learn the historical data and generate scenarios for the calculation of Value-at-Risk (VaR) and Expected Shortfall (ES) and predict the movement of the market risk factors. We present a real data analysis including a backtesting to demonstrate CGAN is able to outperform the Historic Simulation, a popular method in market risk analysis for the calculation of VaR. CGAN can also be applied in the economic time series modeling and forecasting, and an example of hypothetical shock analysis for economic models and the generation of potential CCAR scenarios by CGAN is given at the end of the paper.},
  archiveprefix = {arXiv},
  annotation = {31 citations (Semantic Scholar/arXiv) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Fu19TimeSeriesSimulation.pdf}
}

@article{Haupt19UseProbabilisticForecasts,
  title = {The {{Use}} of {{Probabilistic Forecasts}}: {{Applying Them}} in {{Theory}} and {{Practice}}},
  shorttitle = {The {{Use}} of {{Probabilistic Forecasts}}},
  author = {Haupt, Sue Ellen and Pestana, Rui and Zack, John and Garcia Casado, Mayte and Davidson, Michael and Dobschinski, Jan and Du, Pengwei and Lange, Matthias and Miller, Timothy and Mohrlen, Corinna and Motley, Amber},
  year = {2019},
  month = nov,
  journal = {IEEE Power and Energy Magazine},
  volume = {17},
  number = {6},
  pages = {46--57},
  issn = {1540-7977, 1558-4216},
  doi = {10.1109/MPE.2019.2932639},
  url = {https://ieeexplore.ieee.org/document/8878053/},
  urldate = {2021-04-17},
  langid = {english},
  annotation = {29 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Haupt19UseProbabilisticForecasts.pdf}
}

@article{Wen19DeepGenerativeQuantileCopula,
  title = {Deep {{Generative Quantile-Copula Models}} for {{Probabilistic Forecasting}}},
  author = {Wen, Ruofeng and Torkkola, Kari},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.10697 [cs, stat]},
  eprint = {1907.10697},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.10697},
  urldate = {2021-04-17},
  abstract = {We introduce a new category of multivariate conditional generative models and demonstrate its performance and versatility in probabilistic time series forecasting and simulation. Specifically, the output of quantile regression networks is expanded from a set of fixed quantiles to the whole Quantile Function by a univariate mapping from a latent uniform distribution to the target distribution. Then the multivariate case is solved by learning such quantile functions for each dimension's marginal distribution, followed by estimating a conditional Copula to associate these latent uniform random variables. The quantile functions and copula, together defining the joint predictive distribution, can be parameterized by a single implicit generative Deep Neural Network.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {19 citations (Semantic Scholar/arXiv) [2023-07-25]},
  note = {Comment: Published at the 36th International Conference on Machine Learning (ICML2019), Time Series Workshop, Long Beach, California, 2019},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Wen19DeepGenerativeQuantileCopula.pdf}
}

@techreport{meyerCopulabasedSyntheticData2021,
  type = {Preprint},
  title = {Copula-Based Synthetic Data Generation for Machine Learning Emulatorsin Weather and Climate: Application to a Simple Radiation Model},
  shorttitle = {Copula-Based Synthetic Data Generation for Machine Learning Emulatorsin Weather and Climate},
  author = {Meyer, David and Nagler, Thomas and Hogan, Robin J.},
  year = {2021},
  month = jan,
  institution = {{Earth and space science informatics}},
  doi = {10.5194/gmd-2020-427},
  url = {https://gmd.copernicus.org/preprints/gmd-2020-427/},
  urldate = {2021-04-17},
  abstract = {Abstract. Can we improve machine learning (ML) emulators with synthetic data? The use of real data for training ML models is often the cause of major limitations. For example, real data may be (a) only representative of a subset of situations and domains, (b) expensive to source, (c) limited to specific individuals due to licensing restrictions. Although the use of synthetic data is becoming increasingly popular in computer vision, the training of ML emulators in weather and climate still relies on the use of real data datasets. Here we investigate whether the use of copula-based synthetically-augmented datasets improves the prediction of ML emulators for estimating the downwelling longwave radiation. Results show that bulk errors are cut by up to 75\,\% for the mean bias error (from 0.08 to -0.02\,W\,m-2) and by up to 62\,\% (from 1.17 to 0.44\,W\,m-2) for the mean absolute error, thus showing potential for improving the generalization of future ML emulators.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Meyer21CopulabasedSyntheticData.pdf}
}

@article{Avramidis09EfficientCorrelationMatching,
  title = {Efficient {{Correlation Matching}} for {{Fitting Discrete Multivariate Distributions}} with {{Arbitrary Marginals}} and {{Normal-Copula Dependence}}},
  author = {Avramidis, Athanassios N. and Channouf, Nabil and L'Ecuyer, Pierre},
  year = {2009},
  month = feb,
  journal = {INFORMS Journal on Computing},
  volume = {21},
  number = {1},
  pages = {88--106},
  issn = {1091-9856, 1526-5528},
  doi = {10.1287/ijoc.1080.0281},
  url = {http://pubsonline.informs.org/doi/10.1287/ijoc.1080.0281},
  urldate = {2021-04-17},
  langid = {english},
  annotation = {46 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Avramidis09EfficientCorrelationMatching.pdf}
}

@article{Smith10ModelingLongitudinalData,
  title = {Modeling {{Longitudinal Data Using}} a {{Pair-Copula Decomposition}} of {{Serial Dependence}}},
  author = {Smith, Michael and Min, Aleksey and Almeida, Carlos and Czado, Claudia},
  year = {2010},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {105},
  number = {492},
  pages = {1467--1479},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/jasa.2010.tm09572},
  url = {https://www.tandfonline.com/doi/full/10.1198/jasa.2010.tm09572},
  urldate = {2021-04-17},
  abstract = {Copulas have proven to be very successful tools for the flexible modelling of cross-sectional dependence. In this paper we express the dependence structure of continuous-valued time series data using a sequence of bivariate copulas. This corresponds to a type of decomposition recently called a `vine' in the graphical models literature, where each copula is entitled a `pair-copula'. We propose a Bayesian approach for the estimation of this dependence structure for longitudinal data. Bayesian selection ideas are used to identify any independence pair-copulas, with the end result being a parsimonious representation of a time-inhomogeneous Markov process of varying order. Estimates are Bayesian model averages over the distribution of the lag structure of the Markov process. Using a simulation study we show that the selection approach is reliable and can improve the estimates of both conditional and unconditional pairwise dependencies substantially. We also show that a vine with selection out-performs a Gaussian copula with a flexible correlation matrix. The advantage of the pair-copula formulation is further demonstrated using a longitudinal model of intraday electricity load. Using Gaussian, Gumbel and Clayton pair-copulas we identify parsimonious decompositions of intraday serial dependence, which improve the accuracy of intraday load forecasts. We also propose a new diagnostic for measuring the goodness of fit of high-dimensional multivariate copulas. Overall, the pair-copula model is very general and the Bayesian method generalizes many previous approaches for the analysis of longitudinal data. Supplemental materials for the article are also available online.},
  langid = {english},
  annotation = {168 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Smith10ModelingLongitudinalData.pdf}
}

@article{VanDerMeer20ClearskyIndexSpacetime,
  title = {Clear-Sky Index Space-Time Trajectories from Probabilistic Solar Forecasts: {{Comparing}} Promising Copulas},
  shorttitle = {Clear-Sky Index Space-Time Trajectories from Probabilistic Solar Forecasts},
  author = {{van der Meer}, Dennis and Yang, Dazhi and Wid{\'e}n, Joakim and Munkhammar, Joakim},
  year = {2020},
  month = mar,
  journal = {Journal of Renewable and Sustainable Energy},
  volume = {12},
  number = {2},
  pages = {026102},
  issn = {1941-7012},
  doi = {10.1063/1.5140604},
  url = {http://aip.scitation.org/doi/10.1063/1.5140604},
  urldate = {2021-04-17},
  abstract = {Short-term probabilistic solar forecasts are an important tool in decision-making processes in which uncertainty plays a non-negligible role. Purely statistical models that produce temporal or spatiotemporal probabilistic solar forecasts are generally trained individually, and the combined forecasts therefore lack the temporal or spatiotemporal correlation present in the data. To recover the spatiotemporal dependence structure, a copula can be employed, which constructs a multivariate distribution from which spatially and temporally correlated uniform random numbers can be sampled, which in turn can be used to generate the so-called space-time trajectories via the inverse probability integral transform. In this study, we employ the recently introduced ultra-fast preselection algorithm to leverage the spatiotemporal information present in a pyranometer network and compare its accuracy to that of quantile regression forecasts that only consider temporal information. We show that the preselection algorithm improves both the calibration and sharpness of the predictive distributions. Furthermore, we employ four copulas, i.e., (1) Gaussian, (2) Student-t, (3) Clayton, and (4) empirical, to generate space-time trajectories. The results highlight the necessity to rigorously assess the calibration of the space-time trajectories and the correct modeling of the spatiotemporal dependence structure, which we show through techniques introduced in atmospheric sciences. The code used to generate the results in this study can be found at https://github.com/DWvanderMeer/SpaceTimeTrajectories.},
  langid = {english},
  annotation = {14 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\VanDerMeer20ClearskyIndexSpacetime.pdf}
}

@article{Qiu18LearningCorrelationSpace,
  title = {Learning {{Correlation Space}} for {{Time Series}}},
  author = {Qiu, Han and Lam, Hoang Thanh and Fusco, Francesco and Sinn, Mathieu},
  year = {2018},
  month = may,
  journal = {arXiv:1802.03628 [cs, stat]},
  eprint = {1802.03628},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.03628},
  urldate = {2021-04-17},
  abstract = {We propose an approximation algorithm for efficient correlation search in time series data. In our method, we use Fourier transform and neural network to embed time series into a low-dimensional Euclidean space. The given space is learned such that time series correlation can be effectively approximated from Euclidean distance between corresponding embedded vectors. Therefore, search for correlated time series can be done using an index in the embedding space for efficient nearest neighbor search. Our theoretical analysis illustrates that our method's accuracy can be guaranteed under certain regularity conditions. We further conduct experiments on real-world datasets and the results show that our method indeed outperforms the baseline solution. In particular, for approximation of correlation, our method reduces the approximation loss by a half in most test cases compared to the baseline solution. For top-k highest correlation search, our method improves the precision from 5\% to 20\% while the query time is similar to the baseline approach query time.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {obsLitNote},
  annotation = {2 citations (Semantic Scholar/arXiv) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Qiu18LearningCorrelationSpace.pdf}
}

@article{Ong20TemporalTensorTransformation,
  title = {Temporal {{Tensor Transformation Network}} for {{Multivariate Time Series Prediction}}},
  author = {Ong, Yuya Jeremy and Qiao, Mu and Jadav, Divyesh},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.01051 [cs, stat]},
  eprint = {2001.01051},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2001.01051},
  urldate = {2021-04-17},
  abstract = {Multivariate time series prediction has applications in a wide variety of domains and is considered to be a very challenging task, especially when the variables have correlations and exhibit complex temporal patterns, such as seasonality and trend. Many existing methods suffer from strong statistical assumptions, numerical issues with high dimensionality, manual feature engineering efforts, and scalability. In this work, we present a novel deep learning architecture, known as Temporal Tensor Transformation Network, which transforms the original multivariate time series into a higher order of tensor through the proposed Temporal-Slicing Stack Transformation. This yields a new representation of the original multivariate time series, which enables the convolution kernel to extract complex and non-linear features as well as variable interactional signals from a relatively large temporal region. Experimental results show that Temporal Tensor Transformation Network outperforms several state-of-theart methods on window-based predictions across various tasks. The proposed architecture also demonstrates robust prediction performance through an extensive sensitivity analysis.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {2 citations (Semantic Scholar/arXiv) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Ong20TemporalTensorTransformation.pdf}
}

@article{Zhu20LearningTemporalSpatial,
  title = {Learning {{Temporal}} and {{Spatial Correlations Jointly}}: {{A Unified Framework}} for {{Wind Speed Prediction}}},
  shorttitle = {Learning {{Temporal}} and {{Spatial Correlations Jointly}}},
  author = {Zhu, Qiaomu and Chen, Jinfu and Shi, Dongyuan and Zhu, Lin and Bai, Xiang and Duan, Xianzhong and Liu, Yilu},
  year = {2020},
  month = jan,
  journal = {IEEE Transactions on Sustainable Energy},
  volume = {11},
  number = {1},
  pages = {509--523},
  issn = {1949-3029, 1949-3037},
  doi = {10.1109/TSTE.2019.2897136},
  url = {https://ieeexplore.ieee.org/document/8633392/},
  urldate = {2021-04-17},
  abstract = {Leveraging both temporal and spatial correlations to predict wind speed remains one of the most challenging and less studied areas of wind speed prediction. In this paper, the problem of predicting wind speeds for multiple sites is investigated by using the spatio-temporal correlation. We proposed a deep architecture termed predictive spatio-temporal network (PSTN), which is a unified framework integrating a convolutional neural network (CNN) and a long short-term memory (LSTM). Initially, the spatial features are extracted from the spatial wind speed matrices (SWSMs) by the CNN at the bottom of the model. Then, the LSTM captures the temporal dependencies among the spatial features extracted from contiguous time points. Finally, the predicted wind speeds are given by the last state of the top layer of the LSTM, which are generated by using the spatial features and temporal dependencies. Though composed of two kinds of architectures, PSTN is trained with one loss function in an end-to-end (E2E) manner, which can learn temporal and spatial correlations jointly. Experiments for short-term predictions are conducted on real-world data, whose results demonstrate that PSTN outperforms prior methods.},
  langid = {english},
  keywords = {obsLitNote},
  annotation = {84 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhu20LearningTemporalSpatial.pdf}
}

@article{Munkhammar17AutocorrelationbasedCopulaModel,
  title = {An Autocorrelation-Based Copula Model for Generating Realistic Clear-Sky Index Time-Series},
  author = {Munkhammar, Joakim and Wid{\'e}n, Joakim},
  year = {2017},
  month = dec,
  journal = {Solar Energy},
  volume = {158},
  pages = {9--19},
  issn = {0038092X},
  doi = {10.1016/j.solener.2017.09.028},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0038092X17308009},
  urldate = {2021-04-17},
  abstract = {This study presents a method for using copulas to model the temporal variability of the clear-sky index, which in turn can be used to produce realistic timeseries of photovoltaic power generation. The method utilizes the autocorrelation function of a clear-sky index time-series, and based on that a correlation matrix is set up for the dependency between clear-sky indices at N time-steps. With the use of this correlation matrix an N -dimensional copula function is configured so that correlated samples for these N time-steps can be obtained. Results from the copula model are compared with the original data set and an uncorrelated model based on zero correlated clear-sky index data in terms of distribution, autocorrelation, step changes and distribution. The copula model is shown to be superior to the uncorrelated model in these aspects. The copula model is also applied to a set of bins of daily mean clear-sky index and the use of bins is shown to improve the results.},
  langid = {english},
  annotation = {18 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Munkhammar17AutocorrelationbasedCopulaModel.pdf}
}

@article{Cecaj20ComparingDeepLearning,
  title = {Comparing {{Deep Learning}} and {{Statistical Methods}} in {{Forecasting Crowd Distribution}} from {{Aggregated Mobile Phone Data}}},
  author = {Cecaj, Alket and Lippi, Marco and Mamei, Marco and Zambonelli, Franco},
  year = {2020},
  month = sep,
  journal = {Applied Sciences},
  volume = {10},
  number = {18},
  pages = {6580},
  issn = {2076-3417},
  doi = {10.3390/app10186580},
  url = {https://www.mdpi.com/2076-3417/10/18/6580},
  urldate = {2021-04-17},
  abstract = {Accurately forecasting how crowds of people are distributed in urban areas during daily activities is of key importance for the smart city vision and related applications. In this work we forecast the crowd density and distribution in an urban area by analyzing an aggregated mobile phone dataset. By comparing the forecasting performance of statistical and deep learning methods on the aggregated mobile data we show that each class of methods has its advantages and disadvantages depending on the forecasting scenario. However, for our time-series forecasting problem, deep learning methods are preferable when it comes to simplicity and immediacy of use, since they do not require a time-consuming model selection for each different cell. Deep learning approaches are also appropriate when aiming to reduce the maximum forecasting error. Statistical methods instead show their superiority in providing more precise forecasting results, but they require data domain knowledge and computationally expensive techniques in order to select the best parameters.},
  langid = {english},
  annotation = {23 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Cecaj20ComparingDeepLearning.pdf}
}

@article{Tsoukalas20SimulationNonGaussianCorrelated,
  title = {Simulation of {{Non-Gaussian Correlated Random Variables}}, {{Stochastic Processes}} and {{Random Fields}}: {{Introducing}} the {{anySim R-Package}} for {{Environmental Applications}} and {{Beyond}}},
  shorttitle = {Simulation of {{Non-Gaussian Correlated Random Variables}}, {{Stochastic Processes}} and {{Random Fields}}},
  author = {Tsoukalas, Ioannis and Kossieris, Panagiotis and Makropoulos, Christos},
  year = {2020},
  month = jun,
  journal = {Water},
  volume = {12},
  number = {6},
  pages = {1645},
  issn = {2073-4441},
  doi = {10.3390/w12061645},
  url = {https://www.mdpi.com/2073-4441/12/6/1645},
  urldate = {2021-04-17},
  abstract = {Stochastic simulation has a prominent position in a variety of scientific domains including those of environmental and water resources sciences. This is due to the numerous applications that can benefit from it, such as risk-related studies. In such domains, stochastic models are typically used to generate synthetic weather data with the desired properties, often resembling those of hydrometeorological observations, which are then used to drive deterministic models of the understudy system. However, generating synthetic weather data with the desired properties is not an easy task. This is due to the peculiarities of such processes, i.e., non-Gaussianity, intermittency, dependence, and periodicity, and the limited availability of open-source software for such purposes. This work aims to simplify the synthetic data generation procedure by providing an R-package called anySim, specifically designed for the simulation of non-Gaussian correlated random variables, stochastic processes at single and multiple temporal scales, and random fields. The functionality of the package is demonstrated through seven simulation studies, accompanied by code snippets, which resemble real-world cases of stochastic simulation (i.e., generation of synthetic weather data) of hydrometeorological processes and fields (e.g., rainfall, streamflow, temperature, etc.), across several spatial and temporal scales (ranging from annual down to 10-min simulations).},
  langid = {english},
  annotation = {25 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Tsoukalas20SimulationNonGaussianCorrelated.pdf}
}

@misc{Fowler11densitySumCorrRVs,
  title = {Density {{Function}} for the {{Sum}} of {{Correlated Random Variables}}},
  author = {{John W. Fowler}},
  year = {2011},
  month = dec,
  url = {http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/SumOfCorrelatedRVs.pdf},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Fowler11densitySumCorrRVs.pdf}
}

@article{Gordon83CumulativeDistributionFunction,
  title = {Cumulative Distribution Function of the Sum of Correlated Chi--- Squared Random Variables},
  author = {Gordon, N. H. and Ramig, P. F.},
  year = {1983},
  month = jan,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {17},
  number = {1},
  pages = {1--9},
  publisher = {Taylor \& Francis},
  issn = {0094-9655},
  doi = {10.1080/00949658308810633},
  url = {https://doi.org/10.1080/00949658308810633},
  urldate = {2021-03-29},
  abstract = {The cumulative distribution function of the sumS, of correlated random variables can be obtained by considering a multivariate generalization of a gamma distribution which occurs naturally within the context of a general multivariate normal model. By application of the inversion formula to the characteristic function of S, an accurate method for calculating the distribution of S was obtained. An explicit expression for this distribution is presented for certain parameter values in the case where the underlying multivariate normal model has the moving averages correlation structure. Agreement between the two methods was excellent.},
  annotation = {11 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gordon83CumulativeDistributionFunction.pdf}
}

@article{ElBouanani18sumCorrWeibulApprox,
  title = {Accurate {{Closed-Form Approximations}} for the {{Sum}} of {{Correlated Weibull Random Variables}}},
  author = {El Bouanani, Faissal and {da Costa}, Daniel Benevides},
  year = {2018},
  month = aug,
  journal = {IEEE Wireless Communications Letters},
  volume = {7},
  number = {4},
  pages = {498--501},
  issn = {2162-2337, 2162-2345},
  doi = {10.1109/LWC.2017.2789280},
  url = {https://ieeexplore.ieee.org/document/8245828/},
  urldate = {2021-03-29},
  abstract = {In this letter, a highly accurate approximate approach for the sum of arbitrary correlated Weibull random variables (CWRVs) is proposed. Specifically, closed-form approximate expressions for the probability density function, cumulative distribution function, and moment generating function are derived, and their accuracies are corroborated through Monte Carlo simulations for several representative examples.},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\ElBouanani18sumCorrWeibulApprox.pdf}
}

@article{RehmanLowComplexityNonIntrusive,
  title = {Low {{Complexity Non-Intrusive Disaggregation Approach}} for {{Air Conditioning Unit}} and {{Electric Vehicle Charging}}},
  author = {Rehman, Attique Ur and Lie, Tek Tjing and Vall{\`e}s, Brice and Tito, Shafiqur R},
  pages = {5},
  abstract = {Today energy monitoring is inevitable towards energy efficiency and conservation and load disaggregation is one of the techniques towards effective energy monitoring. In the said domain, Non-Intrusive Appliance Load Monitoring (NIALM) is an attractive method where aggregated load data is acquired at a single metering point and later-on segregated load consumption data has been acquired using different software techniques. This paper presents a low complexity event-based non-intrusive appliance load monitoring technique based on supervised machine learning. For disaggregation purposes, this paper emphases on air conditioning (AC) unit and Electric Vehicle (EV) charging load due to its high significance for the overall improvement of power grid stability. A comprehensive digital simulation has been carried out to validate the performance of the proposed approach and intended appliance are aptly classified having outcome of 97\% for same Data ID and 95\% for different Data ID in terms of precision, recall, and f-score performance metrics.},
  langid = {english}
}

@article{Barbieri17vshortPVfrcstCloudRev,
  title = {Very Short-Term Photovoltaic Power Forecasting with Cloud Modeling: {{A}} Review},
  shorttitle = {Very Short-Term Photovoltaic Power Forecasting with Cloud Modeling},
  author = {Barbieri, Florian and Rajakaruna, Sumedha and Ghosh, Arindam},
  year = {2017},
  month = aug,
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {75},
  pages = {242--263},
  issn = {1364-0321},
  doi = {10.1016/j.rser.2016.10.068},
  url = {https://www.sciencedirect.com/science/article/pii/S136403211630733X},
  urldate = {2021-02-10},
  abstract = {This paper endeavors to provide the reader with an overview of the various tools needed to forecast photovoltaic (PV) power within a very short-term horizon. The study focuses on the specific application of a large scale grid-connected PV farm. Solar resource is largely underexploited worldwide whereas it exceeds by far humans' energy needs. In the current context of global warming, PV energy could potentially play a major role to substitute fossil fuels within the main grid in the future. Indeed, the number of utility-scale PV farms is currently fast increasing globally, with planned capacities in excess of several hundred megawatts. This makes the cost of PV-generated electricity quickly plummet and reach parity with non-renewable resources. However, like many other renewable energy sources, PV power depends highly on weather conditions. This particularity makes PV energy difficult to dispatch unless a properly sized and controlled energy storage system (ESU) is used. An accurate power forecasting method is then required to ensure power continuity but also to manage the ramp rates of the overall power system. In order to perform these actions, the forecasting timeframe also called horizon must be first defined according to the grid operation that is considered. This leads to define both spatial and temporal resolutions. As a second step, an adequate source of input data must be selected. As a third step, the input data must be processed with statistical methods. Finally, the processed data are fed to a precise PV model. It is found that forecasting the irradiance and the cell temperature are the best approaches to forecast precisely swift PV power fluctuations due to the cloud cover. A combination of several sources of input data like satellite and land-based sky imaging also lead to the best results for very-short term forecasting.},
  langid = {english},
  annotation = {154 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Barbieri17vshortPVfrcstCloudRev.pdf}
}

@inproceedings{Zhang14trnFreeLoadMonLowSamp,
  title = {Training-Free Non-Intrusive Load Monitoring of Electric Vehicle Charging with Low Sampling Rate},
  booktitle = {{{IECON}} 2014 - 40th {{Annual Conference}} of the {{IEEE Industrial Electronics Society}}},
  author = {Zhang, Z. and Son, J. H. and Li, Y. and Trayer, M. and Pi, Z. and Hwang, D. Y. and Moon, J. K.},
  year = {2014},
  month = oct,
  pages = {5419--5425},
  issn = {1553-572X},
  doi = {10.1109/IECON.2014.7049328},
  abstract = {Non-intrusive load monitoring (NILM) is an important topic in smart-grid and smart-home. Many energy disaggregation algorithms have been proposed to detect various individual appliances from one aggregated signal observation. However, few works studied the energy disaggregation of plug-in electric vehicle (EV) charging in the residential environment since EVs charging at home has emerged only recently. Recent studies showed that EV charging has a large impact on smart-grid especially in summer. Therefore, EV charging monitoring has become a more important and urgent missing piece in energy disaggregation. In this paper, we present a novel method to disaggregate EV charging signals from aggregated real power signals. The proposed method can effectively mitigate interference coming from air-conditioner (AC), enabling accurate EV charging detection and energy estimation under the presence of AC power signals. Besides, the proposed algorithm requires no training, demands a light computational load, delivers high estimation accuracy, and works well for data recorded at the low sampling rate 1/60 Hz. When the algorithm is tested on real-world data recorded from 11 houses over about a whole year (total 125 months worth of data), the averaged error in estimating energy consumption of EV charging is 15.7 kwh/month (while the true averaged energy consumption of EV charging is 208.5 kwh/month), and the averaged normalized mean square error in disaggregating EV charging load signals is 0.19.},
  annotation = {37 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\zotero\\papers\\Zhang14trnFreeLoadMonLowSamp.pdf;C\:\\Users\\scott\\Zotero\\storage\\EAL7BEGC\\7049328.html}
}

@article{Teichgraeber19timeSeriesClustFrmWrk,
  title = {{{TimeSeriesClustering}}: {{An}} Extensible Framework in {{Julia}}},
  shorttitle = {{{TimeSeriesClustering}}},
  author = {Teichgraeber, Holger and Kuepper, Lucas and Brandt, Adam},
  year = {2019},
  month = sep,
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {41},
  pages = {1573},
  issn = {2475-9066},
  doi = {10.21105/joss.01573},
  url = {https://joss.theoj.org/papers/10.21105/joss.01573},
  urldate = {2021-03-15},
  abstract = {TimeSeriesClustering is a Julia implementation of unsupervised learning methods for time series datasets. It provides functionality for clustering and aggregating, detecting motifs, and quantifying similarity between time series datasets. The software provides a type system for temporal data, and provides an implementation of the most commonly used clustering methods and extreme value selection methods for temporal data. TimeSeriesClustering provides simple integration of multi-dimensional time-series data (e.g., multiple attributes such as wind availability, solar availability, and electricity demand) in a single aggregation process. The software is applicable to general time series datasets and lends itself well to a multitude of application areas within the field of time series data mining. TimeSeriesClustering was originally developed to perform time series aggregation for energy systems optimization problems. Because of the software's origin, many of the examples in this work stem from the field of energy systems optimization.},
  langid = {english},
  annotation = {9 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Teichgraeber19timeSeriesClustFrmWrk.pdf}
}

@inproceedings{Yang18QualityControlSolar,
  title = {Quality {{Control}} for {{Solar Irradiance Data}}},
  booktitle = {2018 {{IEEE Innovative Smart Grid Technologies}} - {{Asia}} ({{ISGT Asia}})},
  author = {Yang, Dazhi and Yagli, Gokhan Mert and Quan, Hao},
  year = {2018},
  month = may,
  pages = {208--213},
  publisher = {IEEE},
  address = {Singapore},
  doi = {10.1109/ISGT-Asia.2018.8467892},
  url = {https://ieeexplore.ieee.org/document/8467892/},
  urldate = {2021-01-12},
  abstract = {With the advent of sensing technology, highresolution data have become ubiquitous among various smart grid applications. Whereas the data provide valuable empirical evidence, they are often, if not always, subject to significant measurement uncertainty. However, nor is there any standardized procedure to quality control (QC) these data. In this regard, this paper investigates the well-known dataset from the Baseline Solar Radiation Network (BSRN). BSRN archives (mostly) 1-min solar irradiance data from 60 stations around the world for over two decades. A six-step QC procedure is demonstrated using the BSRN data.},
  isbn = {978-1-5386-4291-7},
  langid = {english},
  annotation = {20 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Describes irrad quality control},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Yang18QualCtrlSolarIrradDat.pdf}
}

@article{Aiad16disaggLdAdapt2wayInteract,
  title = {Non-Intrusive Load Disaggregation with Adaptive Estimations of Devices Main Power Effects and Two-Way Interactions},
  author = {Aiad, Misbah and Lee, Peng},
  year = {2016},
  month = oct,
  journal = {Energy and Buildings},
  volume = {130},
  pages = {131--139},
  doi = {10.1016/j.enbuild.2016.08.050},
  abstract = {Energy management and savings in residential homes are emerging concerns nowadays because of several challenges facing the energy sector such as energy sources limitations and environmental impacts. Non-intrusive load monitoring (NILM) was introduced as a set of methods and techniques that aim to decompose the total aggregate consumption measured by the smart meter into the consumptions by individual appliances present in the household. The detailed information on energy usage for each device were found to be a good influencing method for the residents to adopt better devices usage profiles which lead eventually to noticeable energy savings. Recent research had shown that the Hidden Markov Models (HMMs) and its extensions are effective models in the load disaggregation problem. The authors had introduced a new unsupervised approach for load disaggregation that includes the mutual devices interactions information into the Factorial Hidden Markov Model (FHMM) representation of the aggregate signal in an earlier work. In this paper, we introduce an adaptive approach for estimating devices main power consumptions and their two-way interactions during the disaggregation process. The adaptive approach is used to mimic the changes in devices consumptions and two-way interactions. The adaptive estimation process was carried out only for cases when there are four devices or less that are operating/ON instantaneously. The proposed approach was tested with data from the REDD public data set and it showed better performance in terms of energy disaggregation accuracy compared with the standard FHMM. The adaptive estimating of main factors effects (primary power consumptions) and two-way interactions during the disaggregation process provided higher disaggregation accuracy results, in general, than those with fixed factors and two-way interactions values.},
  annotation = {35 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Aiad16disaggLdAdapt2wayInteract.pdf}
}

@article{Xia20loadDisaggLSTM,
  title = {Non-Intrusive Load Disaggregation Based on Composite Deep Long Short-Term Memory Network},
  author = {Xia, Min and Liu, Wan'an and Wang, Ke and Song, Wenzhu and Chen, Chunling and Li, Yaping},
  year = {2020},
  month = dec,
  journal = {Expert Systems with Applications},
  volume = {160},
  pages = {113669},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2020.113669},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417420304930},
  urldate = {2021-03-15},
  abstract = {Non-invasive load monitoring (NILM) is a vital step to realize the smart grid. Although the existing various NILM algorithms have made significant progress in energy consumption feedback, there are still some problems need to further addressed, such as the exponential growth of state space with the increase of the number of multi-state devices, which leads to the dimension disaster; and it is difficult to capture the power fluctuation information effectively because of the neglect of time-dependency problem load disaggregation; traditional disaggregation involves a process of one sequence to one sequence optimization, which is inefficient. In our study, a composite deep LSTM is proposed for load disaggregation. The proposed algorithm considers the process of load disaggregation as a signal separation process and establishes regression learning from a single sequence to multiple sequences to avoid dimension disaster. In addition, an encoder-separation-decoder structure is introduced for load disaggregation. Encoder completes the effective encoding of the mains power and differential power information, the time-dependency of the encoding process implemented by a deep LSTM, separation realizes the disaggregation process by separating the encoded information, and decoder decode the separated signal into the sequences of corresponding electrical appliances. Compared with the one sequence to one sequence disaggregation method, the proposed method simplified disaggregation complexity and improves the efficiency of disaggregation. The experimental results on WikiEnergy and REDD datasets show that the proposed method can reduce the disaggregation error and improve the comprehensive performance of event detection. Besides, our study can provide conditions for the realization of the bidirectional interaction of the smart grid and the improvement of the smart grid scheduling.},
  langid = {english},
  annotation = {57 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Xia20loadDisaggLSTM.pdf}
}

@article{DAmico20windPowIndSemiMarkCpla,
  title = {Managing {{Wind Power Generation}} via {{Indexed Semi-Markov Model}} and {{Copula}}},
  author = {D'Amico, Guglielmo and Masala, Giovanni and Petroni, Filippo and Sobolewski, Robert Adam},
  year = {2020},
  month = aug,
  journal = {Energies},
  volume = {13},
  number = {16},
  pages = {4246},
  issn = {1996-1073},
  doi = {10.3390/en13164246},
  url = {https://www.mdpi.com/1996-1073/13/16/4246},
  urldate = {2021-03-13},
  abstract = {Because of the stochastic nature of wind turbines, the output power management of wind power generation (WPG) is a fundamental challenge for the integration of wind energy systems into either power systems or microgrids (i.e., isolated systems consisting of local wind energy systems only) in operation and planning studies. In general, a wind energy system can refer to both one wind farm consisting of a number of wind turbines and a given number of wind farms sited at the area in question. In power systems (microgrid) planning, a WPG should be quantified for the determination of the expected power flows and the analysis of the adequacy of power generation. Concerning this operation, the WPG should be incorporated into an optimal operation decision process, as well as unit commitment and economic dispatch studies. In both cases, the probabilistic investigation of WPG leads to a multivariate uncertainty analysis problem involving correlated random variables (the output power of either wind turbines that constitute wind farm or wind farms sited at the area in question) that follow different distributions. This paper advances a multivariate model of WPG for a wind farm that relies on indexed semi-Markov chains (ISMC) to represent the output power of each wind energy system in question and a copula function to reproduce the spatial dependencies of the energy systems' output power. The ISMC model can reproduce long-term memory effects in the temporal dependence of turbine power and thus understand, as distinct cases, the plethora of Markovian models. Using copula theory, we incorporate non-linear spatial dependencies into the model that go beyond linear correlations. Some copula functions that are frequently used in applications are taken into consideration in the paper; i.e., Gumbel copula, Gaussian copula, and the t-Student copula with different degrees of freedom. As a case study, we analyze a real dataset of the output powers of six wind turbines that constitute a wind farm situated in Poland. This dataset is compared with the synthetic data generated by the model thorough the calculation of three adequacy indices commonly used at the first hierarchical level of power system reliability studies; i.e., loss of load probability (LOLP), loss of load hours (LOLH) and loss of load expectation (LOLE). The results will be compared with those obtained using other models that are well known in the econometric field; i.e., vector autoregressive models (VAR).},
  langid = {english},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\DAmico20windPowIndSemiMarkCpla.pdf}
}

@inproceedings{Rehman19LowCmplxDisagg_AC_EV,
  title = {Low {{Complexity Non-Intrusive Load Disaggregation}} of {{Air Conditioning Unit}} and {{Electric Vehicle Charging}}},
  booktitle = {2019 {{IEEE Innovative Smart Grid Technologies}} - {{Asia}} ({{ISGT Asia}})},
  author = {Rehman, A. U. and Lie, T. Tjing and Vall{\`e}s, B. and Tito, S. R.},
  year = {2019},
  month = may,
  pages = {2607--2612},
  issn = {2378-8542},
  doi = {10.1109/ISGT-Asia.2019.8881113},
  abstract = {Energy monitoring is inevitable towards achieving energy efficiency and conservation. Load disaggregation is one of the techniques towards effective energy monitoring. In the said domain, Non-Intrusive Appliance Load Monitoring (NIALM) is an attractive method where aggregated load data are acquired from a single metering point and segregated appliance level load is estimated using effective software techniques. This paper presents a low complexity event-based NIALM technique based on supervised machine learning. In this paper, the emphasis is on the disaggregation of Air Conditioning (AC) unit and Electric Vehicle (EV) charging loads due to their high significance for the overall power grid stability improvement. A comprehensive digital simulation has been carried out to validate the performance of the proposed approach and intended appliances are aptly classified having an outcome of 97\% for same Data ID and 95\% for different Data ID in terms of precision, recall, and f-score performance metrics.},
  annotation = {12 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\zotero\\papers\\Rehman19LowCmplxDisagg_AC_EV.pdf;C\:\\Users\\scott\\Zotero\\storage\\7BURLJSR\\8881113.html}
}

@article{Salinas19hiDimFrcstRNNcpla,
  title = {High-{{Dimensional Multivariate Forecasting}} with {{Low-Rank Gaussian Copula Processes}}},
  author = {Salinas, David and {Bohlke-Schneider}, Michael and Callot, Laurent and Medico, Roberto and Gasthaus, Jan},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.03002 [cs, stat]},
  eprint = {1910.03002},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.03002},
  urldate = {2021-03-13},
  abstract = {Predicting the dependencies between observations from multiple time series is critical for applications such as anomaly detection, financial risk management, causal analysis, or demand forecasting. However, the computational and numerical difficulties of estimating time-varying and high-dimensional covariance matrices often limits existing methods to handling at most a few hundred dimensions or requires making strong assumptions on the dependence between series. We propose to combine an RNN-based time series model with a Gaussian copula process output model with a low-rank covariance structure to reduce the computational complexity and handle non-Gaussian marginal distributions. This permits to drastically reduce the number of parameters and consequently allows the modeling of time-varying correlations of thousands of time series. We show on several realworld datasets that our method provides significant accuracy improvements over state-of-the-art baselines and perform an ablation study analyzing the contributions of the different components of our model.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {117 citations (Semantic Scholar/arXiv) [2023-07-25]\\
Salinas19hiDimFrcstRNNcpla},
  note = {Reviewer comments:
\par
http://media.nips.cc/nipsbooks/nipspapers/paper\_files/nips32/reviews/3697.html},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Salinas19hiDimFrcstRNNcpla.pdf}
}

@article{Munshi18evChrgFlexDefExtrct,
  title = {Extracting and {{Defining Flexibility}} of {{Residential Electrical Vehicle Charging Loads}}},
  author = {Munshi, A. A. and Mohamed, Y. A. I.},
  year = {2018},
  month = feb,
  journal = {IEEE Transactions on Industrial Informatics},
  volume = {14},
  number = {2},
  pages = {448--461},
  issn = {1941-0050},
  doi = {10.1109/TII.2017.2724559},
  abstract = {The popularization of electric vehicles raises concerns about their negative impact on the electrical grid. Extracting electric vehicle charging load patterns is a key factor that allows smart grid operators to make intelligent and informed decisions about conserving energy and promoting the stability of the electrical grid. This paper presents an unsupervised algorithm to extract electric vehicle charging load patterns nonintrusively from the smart meter data. Furthermore, a method to define flexibility for the collective electric vehicle charging demand by analyzing the time-variable patterns of the aggregated electric vehicle charging behaviors is presented. Validation results on real residential loads have shown that the proposed approach is a promising solution to extract electric vehicle charging loads and that the approach can effectively mitigate the interference of other appliances that have similar load behaviors as electric vehicles. Furthermore, a case study on real residential data to analyze electric vehicle charging trends and quantify the flexibility achievable from the aggregated electric vehicle load in different time periods is presented.},
  annotation = {35 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Munshi18evChrgFlexDefExtrct.pdf;C\:\\Users\\scott\\Zotero\\storage\\4BHNM8LP\\7972985.html}
}

@article{Toledo20solarRadModelsEval,
  title = {Evaluation of {{Solar Radiation Transposition Models}} for {{Passive Energy Management}} and {{Building Integrated Photovoltaics}}},
  author = {Toledo, Carlos and Gracia Amillo, Ana Maria and Bardizza, Giorgio and Abad, Jose and Urbina, Antonio},
  year = {2020},
  month = feb,
  journal = {Energies},
  volume = {13},
  number = {3},
  pages = {702},
  issn = {1996-1073},
  doi = {10.3390/en13030702},
  url = {https://www.mdpi.com/1996-1073/13/3/702},
  urldate = {2021-01-12},
  abstract = {Incident solar radiation modelling has become of vital importance not only in architectural design considerations, but also in the estimation of the energy production of photovoltaic systems. This is particularly true in the case of buildings with integrated photovoltaics (PV) systems having a wide range of orientations and inclinations defined by the skin of the building. Since solar radiation data at the plane of interest is hardly ever available, this study presents the analysis of two of the most representative transposition models used to obtain the in-plane irradiance using as input data the global and diffuse irradiation on the horizontal plane, which can be obtained by satellite-based models or ground measurements. Both transposition models are validated with experimental measurements taken in Murcia (southeast of Spain) and datasets provided by the photovoltaic geographical information system (PVGIS) and the National Renewable Energy Laboratory (NREL) for vertical surfaces facing the four cardinal points. For the validation, the mean bias deviation, root mean square error and forecasted skill were used as indicators. Results show that the error rate decreases slightly for clear days. Better results are also obtained by dismissing data with low solar elevation angles so as to avoid shadowing effects from the surroundings in the early and late hours of the day, which affects mainly the performance of the transposition models for west and east surfaces. The results highlight the potential of equator-facing fa{\c c}ades in winter time when the received irradiation can be twice as much as the one collected by the horizontal plane. It is also noteworthy that the operating conditions of all fa{\c c}ades are mainly low irradiance and medium temperature at these locations.},
  langid = {english},
  annotation = {16 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Toledo20solarRadModelsEval.pdf}
}

@article{Wang19evDisaggLimActMatchPursuit,
  title = {Electric {{Vehicle Load Disaggregation Based}} on {{Limited Activation Matching Pursuits}}},
  author = {Wang, Shuangyuan and Li, Ran and Evans, Adrian and Li, Furong},
  year = {2019},
  month = feb,
  journal = {Energy Procedia},
  series = {Innovative {{Solutions}} for {{Energy Transitions}}},
  volume = {158},
  pages = {2611--2616},
  issn = {1876-6102},
  doi = {10.1016/j.egypro.2019.02.011},
  url = {https://www.sciencedirect.com/science/article/pii/S1876610219310884},
  urldate = {2021-03-15},
  abstract = {This paper proposes a novel limited activation matching pursuits (LAMP) method to monitor the number of eclectic vehicles (EVs) connected at a charging station and their charging activities. LAMP is able to reflect the two-dimensional characteristics of the number and charging time of the EVs. Constraints are entered on the activation coefficients of the matching pursuits to avoid over matching a particular type of EV. The method includes the development of the basis based on typical EV charging profiles and the improvement of matching pursuits. A case study is undertaken based on real EV data collected from London. The results show that 79.35\% EVs can be accurately detected in charging station load profile duration of one week. The number of EVs connected within each half hour can be identified with a relative mean absolute deviation (RMAD) of 21.18\%.},
  langid = {english},
  annotation = {4 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Wang19evDisaggLimActMatchPursuit.pdf}
}

@article{Zhang11evChrgRecogLdMon,
  title = {An {{Improved Non-Intrusive Load Monitoring Method}} for {{Recognition}} of {{Electric Vehicle Battery Charging Load}}},
  author = {Zhang, Peng and Zhou, Chengke and Stewart, Brian G. and Hepburn, Donald M. and Zhou, Wenjun and Yu, Jianhui},
  year = {2011},
  month = jan,
  journal = {Energy Procedia},
  series = {The {{Proceedings}} of {{International Conference}} on {{Smart Grid}} and {{Clean Energy Technologies}} ({{ICSGCE}} 2011},
  volume = {12},
  pages = {104--112},
  issn = {1876-6102},
  doi = {10.1016/j.egypro.2011.10.015},
  url = {https://www.sciencedirect.com/science/article/pii/S1876610211018418},
  urldate = {2021-03-15},
  abstract = {Non-intrusive load monitoring (NILM) is a convenient method to determine the electrical energy consumption and operation of individual appliances based on analysis of composite load measured at the entry of a building. It avoids installation of parallel sensors for monitoring individual appliances and could be applied in the smart metering system to obtain useful information for load management. This paper presents an improved NILM method that is capable of recognizing Electric Vehicle Battery (EVB) charging as a load type. Based on the proposed framework, a special pattern recognition algorithm is used to perform load disaggregation. A random switching simulator is developed to examine the performance of the improved NILM under various scenarios. The results demonstrate that the EVB charging load is recognized as well as other traditional appliances. The overall success rate of the disaggregation reaches 94.5\% at typical circumstance. Through sensitivity analysis it is also shown that the EVB charging load makes a small impact on the overall performance.},
  langid = {english},
  annotation = {25 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Zhang11evChrgRecogLdMon.pdf}
}

@inproceedings{Chang20EmpiricalStudyLowCost,
  title = {An {{Empirical Study}} with {{A Low-Cost Strategy}} for {{Improving}} the {{Energy Disaggregation}} via {{Questionnaire Survey}}},
  booktitle = {Computer {{Science}} \& {{Information Technology}} ({{CS}} \& {{IT}})},
  author = {Chang, Chun-peng and Ho, Wen-Jen and Hung, Yung-chieh and Chiang, Kuei-Chun and Zhao, Bill},
  year = {2020},
  month = oct,
  pages = {37--43},
  publisher = {AIRCC Publishing Corporation},
  doi = {10.5121/csit.2020.101204},
  url = {https://aircconline.com/csit/papers/vol10/csit101204.pdf},
  urldate = {2021-03-15},
  abstract = {Based on neural network and machine learning, we apply the energy disaggregation for both classification (prediction on usage time) and estimation (prediction on usage amount) on 150 AMI (Advanced Metering Infrastructure) smart meters and a small amount of HEMS (Home Energy Management System) smart plugs in a community in New Taipei City, Taiwan. The aim of this paper is to clarify how we lower the cost, obtain the model of appliance usage from only a small portion of households, improve it with simple questionnaire, and generalize it for prediction on collective households. Our investigation demonstrates the benefits and various possibilities for power suppliers and the government, and won the Elite Award in the Presidential Hackathon 2020, Taiwan.},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Chang20EmpiricalStudyLowCost.pdf}
}

@article{Liu20evChrgStnDisagSensSimular,
  title = {A {{Sensory Similarities Approach}} to {{Load Disaggregation}} of {{Charging Stations}} in {{Internet}} of {{Electric Vehicles}}},
  author = {Liu, Qi and Kamoto, Kondwani and Liu, Xiaodong and Zhang, Yonghong and Yang, Zaiqiang and Khosravi, Mohammad and Xu, Yanwei and Qi, Lianyong},
  year = {2020},
  month = sep,
  journal = {IEEE Sensors Journal},
  volume = {PP},
  pages = {1--1},
  doi = {10.1109/JSEN.2020.3027684},
  abstract = {Intelligent transportation systems (ITSs) have become popular in recent years as an essential requirement for safer and more efficient transportation systems. Internet of Electric vehicles (IoEV) as well as their hybrid forms provide an ideal means of supporting sustainability within an ITS. The control of charging/discharging of EV is still a challenge, despite the tremendous research progress to date in the field. In this paper, the use of charging station data and binary vectorization are proposed in order to provide timely insights on the dynamic behavior of charging processes. A Bag-of-Power-States model has been created for similarity measurement of charging stations within given time periods. The results of experimentations using synthetic data have shown that the proposed Bag-of-Power-States model is computationally feasible and provides useful results for optimizing the scheduling of power supply to charging stations that may be located across a wide range of distances, over the same period of time.},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Liu20evChrgStnDisagSensSimular.pdf}
}

@inproceedings{Wang19disaggEnrgySemiSupDpXfer,
  title = {A {{Semi-Supervised Deep Transfer Learning Architecture}} for {{Energy Disaggregation}}},
  booktitle = {2019 {{IEEE Power Energy Society General Meeting}} ({{PESGM}})},
  author = {Wang, S. and Du, L. and Zhou, Q.},
  year = {2019},
  month = aug,
  pages = {1--5},
  issn = {1944-9933},
  doi = {10.1109/PESGM40551.2019.8973556},
  abstract = {Energy disaggregation is the problem of estimating individual load consumption profiles from an aggregated waveform. Most existing methods ignore the discrepancy of load profile distributions within different sources, which may lead to robustness issues. Furthermore, limit amount of labeled data is a bottleneck for all existing literature. This paper proposed a deep transfer learning based method for energy disaggregation. Each load has its own disaggregtor, which consists of a feature extractor and a regressor. Unlike existing methods, a semi-supervised learning using deep domain adaptation is proposed, which can align the energy data distribution to some extent by utilizing both the labeled and the unlabeled data. Tests were carried out on a publicly available dataset. It can be shown that the proposed architecture can effectively improve energy disaggregation performance and enhance the robustness.},
  annotation = {3 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Wang19disaggEnrgySemiSupDpXfer.pdf;C\:\\Users\\scott\\Zotero\\storage\\6NGEXHA2\\8973556.html}
}

@article{Stainsby20pvEstGenNetMtrInstDate,
  title = {A Method to Estimate Residential {{PV}} Generation from Net-Metered Load Data and System Install Date},
  author = {Stainsby, Wendell and Zimmerle, Daniel and Duggan, Gerald P.},
  year = {2020},
  month = jun,
  journal = {Applied Energy},
  volume = {267},
  pages = {114895},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2020.114895},
  url = {https://www.sciencedirect.com/science/article/pii/S0306261920304074},
  urldate = {2021-03-15},
  abstract = {In the USA, residential photovoltaic (PV) systems are often configured for net metering ``behind-the-meter'', where PV energy generation and building energy demand are reported as a combined net load to advanced metering infrastructure (AMI) meters, impeding estimates of PV generation. This work presents a methodology for modeling individual array and system-wide PV generation using only weather data, premise AMI data, and the approximate date of PV installation -- information available to most distribution utilities. The study uses 36~months of data spanning nearly 850 homes with installed PV systems in Fort Collins, Colorado, USA. The algorithm estimates building energy consumption by comparing time periods before PV installation with similar periods after PV installation that have common weather and activity characteristics. Estimated building energy consumption is then compared with AMI meter data to estimate otherwise unobservable solar generation. To assess accuracy, modeled outputs are compared with directly metered PV generation and white-box physical models of PV production. Considering aggregate, utility-wide, generation estimates for the three year study period, the proposed method estimates over 75\% of all days to within {\textpm}20\% of established physical models. The method estimates more effectively in summer months when PV generation peaks and is of most interest to utilities. The model often outperforms physical models for days with snow cover and for arrays with shading or complex multi-roof implementations. The model also supports day-ahead PV prediction using forecasted weather data.},
  langid = {english},
  annotation = {17 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Stainsby20pvEstGenNetMtrInstDate.pdf}
}

@article{Zhang17disaggLoadCategResid,
  title = {Data Mining of Smart Meters for Load Category Based Disaggregation of Residential Power Consumption},
  author = {Zhang, Guanchen and Wang, G. Gary and Farhangi, Hassan and Palizban, Ali},
  year = {2017},
  journal = {Sustainable Energy, Grids and Networks},
  volume = {10},
  pages = {92--103},
  publisher = {Elsevier},
  abstract = {Non-Intrusive Load Monitoring (NILM) techniques have been leveraged by new instrument and machine learning algorithms to provide customers the breakdown of their energy usage. The state-of-art indicates a large amount of high-frequency measurement ({$>$}1 Hz) can lead to accurate disaggregation. This paper, however, proposes a disaggregation algorithm relies on hourly smart meter readings, aiming to extend the application of the low-frequency data that is accessible by both utilities and customers. The output of the disaggregation includes the breakdown of energy into load-category-based components that have different average power factors. The disaggregated data will support small-scale planning, e.g., in microgrid, by revealing the variance and patterns in different load categories. Our approach is built on a top-down structure that requires no prior knowledge or general models of individual loads. Using clustering and optimization techniques, we infer the load signatures of each category based on the active and reactive power from smart meters. The signatures are updated periodically using the most recent smart meter data. The results show that our disaggregation approach could be applied to random houses in different seasons and to single house and small neighborhood in both offline and quasi-real-time context.},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Zhang17disaggLoadCategResid.pdf}
}

@inproceedings{Sun15evAndBatt4loadHide,
  title = {Combining Electric Vehicle and Rechargeable Battery for Household Load Hiding},
  booktitle = {2015 {{IEEE International Conference}} on {{Smart Grid Communications}} ({{SmartGridComm}})},
  author = {Sun, Yanan and Lampe, Lutz and Wong, Vincent WS},
  year = {2015},
  pages = {611--616},
  publisher = {IEEE},
  abstract = {The transition from electromechanical to advanced smart meters offers great opportunities in load management, energy saving, and resource optimization. However, the fine- grained usage data collected by the smart meters also raises privacy concerns. The household load profile can be analyzed by non-intrusive load monitoring techniques to infer customer activity routines and behavioral preference. Several data ob- fuscation techniques based on controlling a local rechargeable battery or a controllable load have been proposed to mitigate the privacy leakage of the customers. In this paper, we introduce the use of electric vehicles (EVs) for load hiding. In particular, we propose an EV-assisted battery load hiding algorithm, which combines the use of an EV with a local rechargeable battery to achieve the dual purpose of optimal charging and measurement obfuscation. Since EVs are becoming increasingly popular and part of many households, their use renders the implementation of load hiding less costly compared to methods only using dedicated rechargeable batteries. Furthermore, EVs provide more flexibility than other household loads in terms of energy storage. We evaluate our proposed algorithm using real data for electricity price, household consumption, and EV parameters. Adopting mutual information as a measure for information leakage, nu- merical results show that the proposed algorithm can reduce th},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Sun15evAndBatt4loadHide.pdf}
}

@article{Oracle19evDetDisaggAMI,
  title = {{{AMI-based EV Detection}} \& {{Disaggregation}}},
  author = {{Oracle}},
  year = {2019},
  journal = {Oracle},
  pages = {2},
  url = {https://www.oracle.com/a/ocom/docs/industries/utilities/oua-ami-based-ev-detection-disaggregation.pdf},
  abstract = {The Oracle Utilities analytics team recently developed and deployed  advanced data-science-based EV detection and disaggregation capabilities  using 15-minute or hourly advanced metering infrastructure (AMI) data.  With billions of such data points being ingested for our roughly 100 clients  worldwide, this is a significant breakthrough that delivers on two promises:  * rolling out intuitive, user-friendly EV adoption customer journeys, and  * planning for and managing the operational impact of EVs as a DER on the grid.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Oracle19evDetDisaggAMI.pdf}
}

@article{Aiad16disaggLdInteract,
  title = {Unsupervised Approach for Load Disaggregation with Devices Interactions},
  author = {Aiad, Misbah and Lee, Peng Hin},
  year = {2016},
  journal = {Energy and Buildings},
  volume = {116},
  pages = {96--103},
  publisher = {Elsevier},
  abstract = {Energy savings is one of the hottest issues and concerns nowadays due to high oil prices and global  8 warming as a result of CO2 emissions. Non-intrusive appliances load monitoring (NIALM) is a  9 methodology that aim to breakdown the total power consumption measured by the smart meter in each  10 household into the power consumed by the individual appliances. These detailed information on  11 individual appliances consumptions can influence the users to follow better energy usage profiles so as to  12 achieve energy savings. We introduce a novel energy disaggregation model that consider mutual devices  13 interactions and embed the information on devices interactions into the Factorial Hidden Markov Models  14 (FHMM) representations of the aggregated data. The hidden states in the FHMM were inferred by means  15 of the Viterbi algorithm. Devices interactions is a power quality issue that affects the measured power  16 consumed by a device when there are other devices connected to the network. We tested our model using  17 a selected house from the REDD public data set. Our proposed approach showed enhanced results when  18 compared with the standard FHMM. Devices interactions, when observed, enabled us to disaggregate and  19 assign energy consumption for individual devices more accurately.  20 Keywords: non-intrusive appliance load monitoring, energy disaggregation, devices interactions,  21 Factorial Hidden Markov Models, Viterbi algorithm.},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Aiad16disaggLdInteract.pdf}
}

@inproceedings{Reinhardt20HowDoesLoad,
  title = {How Does Load Disaggregation Performance Depend on Data Characteristics? {{Insights}} from a Benchmarking Study},
  shorttitle = {How Does Load Disaggregation Performance Depend on Data Characteristics?},
  booktitle = {Proceedings of the {{Eleventh ACM International Conference}} on {{Future Energy Systems}}},
  author = {Reinhardt, Andreas and Klemenjak, Christoph},
  year = {2020},
  pages = {167--177},
  abstract = {Electrical consumption data contain a wealth of information, and their collection at scale is facilitated by the deployment of smart meters. Data collected this way is an aggregation of the power de- mands of all appliances within a building, hence inferences on the operation of individual devices cannot be drawn directly. By using methods to disaggregate data collected from a single measurement location, however, appliance-level detail can often be reconstructed. A major impediment to the improvement of such disaggregation algorithms lies in the way they are evaluated so far: Their per- formance is generally assessed using a small number of publicly available electricity consumption data sets recorded from actual buildings. As a result, algorithm parameters are often tuned to pro- duce optimal results for the used data sets, but do not necessarily generalize to different input data well. We propose to break this tradition by presenting a toolchain to create synthetic benchmark- ing data sets for the evaluation of disaggregation performance in this work. Generated synthetic data with a configurable amount of concurrent appliance activity is subsequently used to comparatively evaluate eight existing disaggregation algorithms. This way, we not only create a baseline for the comparison of newly developed disaggregation methods, but also point out the data characteristics that pose challenges for the state-of-the-art.},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Reinhardt20HowDoesLoad.pdf}
}

@inproceedings{Bidgely19amInsightsRprt,
  title = {{{AMI-Driven Insights Report}}},
  booktitle = {{{IDC Utilities Summit}}},
  author = {Bidgely},
  year = {2019},
  month = aug,
  publisher = {Bidgely, inc.},
  address = {European},
  url = {https://www.idcutilitiessummit.com/index/RESOURCES/ATTACHMENTS/bidgely_wp-am_i_driven_insights_engine_final_report_08022019.pdf},
  urldate = {2021-03-15},
  abstract = {Advanced Metering Infrastructure is becoming ubiquitous throughout  the utility industry, providing many sources of value for both utilities and  ratepayers. When it comes to analytics in particular, Bidgely's unique ability  to unlock previously unavailable insights to help customers understand  their consumption and take informed action has allowed the company  to serve as a pioneer in the use of AMI data and analytics for customer  engagement.  However, the full potential benefits of analyzing AMI data have not yet  been realized. With eight years of experience in load disaggregation and  applying artificial intelligence to household consumption data, no other  company is better equipped than Bidgely to help utilities unlock this  value. This report is focused on the residential sector, Bidgely's core area  of expertise, though some of the analyses can also be applied to other  sectors. The goal of this report is to describe and provide examples as to how AMI  data analysis can support utilities across core operational use cases. The  executive summary highlights five of these use cases: non-wires-solutions,  TOU rate optimization, EV adoption, rooftop PV analysis and demand  side management. The balance of the report provides additional detail on  each use case as well as anonymized, representative examples of analyses  performed for utilities around the world. These examples are built from  analytics provided by Bidgely's Insights Engine.},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Bidgely19amInsightsRprt.pdf}
}

@inproceedings{Mayhorn16disaggLdRealWrldPerf,
  title = {Load {{Disaggregation Technologies}}: {{Real World}} and {{Laboratory Performance}}},
  booktitle = {{{ACEEE Summer Study}} on {{Energy Efficiency}} in {{Buildings}}},
  author = {Mayhorn, Ebony T and Sullivan, Gregory P and Petersen, Joseph and Butner, Ryan S and Johnson, Erica M},
  year = {2016},
  pages = {13},
  abstract = {Over the past ten years, improvements in low-cost interval metering and communication technology have enabled load disaggregation through non-intrusive load monitoring (NILM) technologies, which estimate and report energy consumption of individual end-use loads. Given the appropriate performance characteristics, these technologies have the potential to enable many utility and customer facing applications. However, there has been skepticism concerning the ability of load disaggregation products to accurately identify and estimate energy consumption of end-uses; which has hindered wide-spread market adoption. A contributing factor is that common test methods and metrics are unavailable to evaluate performance without conducting large-scale field demonstrations and pilots, which can be costly. Without common and costeffective methods of evaluation, advanced NILM technologies will continue to be slow to market and potential users will remain uncertain about their capabilities. This paper reviews recent field studies and laboratory tests of NILM technologies. Several important factors are identified for consideration in test protocols so their results reflect real world performance. Potential metrics are examined to highlight their effectiveness in quantifying disaggregation performance. This analysis is then used to suggest performance metrics that are meaningful and of value to potential users and that will enable researchers/developers to identify beneficial ways to improve their technologies.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Mayhorn16disaggLdRealWrldPerf.pdf}
}

@article{Li21evMonRegionGraphSigProc,
  title = {Regional Non-Intrusive Electric Vehicle Monitoring Based on Graph Signal Processing},
  author = {Li, Jiahang and Li, Ran and Wang, Shuangyuan and Xiang, Yue and Gu, Yunjie},
  year = {2021},
  month = jan,
  journal = {IET Generation, Transmission \&amp; Distribution},
  volume = {14},
  number = {26},
  pages = {6512--6517},
  publisher = {IET Digital Library},
  issn = {1751-8695},
  doi = {10.1049/iet-gtd.2020.0845},
  url = {https://digital-library.theiet.org/content/journals/10.1049/iet-gtd.2020.0845},
  urldate = {2021-03-15},
  abstract = {Electricity network is leading to a low carbon future with high penetration of plug-in electric vehicles (EVs). However, it is extraordinarily difficult to acquire detailed information on regional EV electrification with an incomplete monitoring system for network operators. In this study, a flexible graph signal processing (GSP)-based non-intrusive monitoring on aggregated EVs is proposed to enhance the EVs visibility for operating power system safely and cost-efficiently. It can deduce the individual EV charging status with the highest possibility iteratively from the limited dataset using a GSP-based possibility calculation after processing a daytime EV characteristic charging patterns. The experiment is developed with realistic EV charging datasets collected in London, and the results show the daily EVs number in a specific region of 500 EVs daily aggregation can be estimated efficiently with an around 4.77\% value of relative mean absolute deviation applying the proposed method.},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Li21evMonRegionGraphSigProc.pdf}
}

@phdthesis{Hare18disaggHmLdDmdResp,
  type = {Thesis},
  title = {Disaggregation of Residential Home Energy via Non-Intrusive Load Monitoring for Energy Savings and Targeted Demand Response},
  author = {Hare, Jeremy (Jeremy B. )},
  year = {2018},
  url = {https://dspace.mit.edu/handle/1721.1/117983},
  urldate = {2021-03-15},
  abstract = {Residential energy disaggregation is a process by which the power usage of a home is broken down into the consumption of individual appliances. There are a number of different methods to perform energy disaggregation, from simulation models to installing "smart-plugs" at every outlet where an appliance is connected to the wall. Non-Intrusive Load Monitoring (NILM) is one such disaggregation option. NILM is widely recognized as one of the most cost-effective methods for gathering disaggregated energy data while maintaining a high level of accuracy. Although the technology has existed for many years, the adoption rate of NILM, and other devices that disaggregate energy, has been minimal. This thesis provides details on the potential benefits, both for the customer and utility provider, associated with furthering the adoption of NILM devices and obtaining the disaggregated appliance level energy-use. A broad overview of potential benefits is presented; however, the primary goal of this thesis will be to investigate two benefits of NILM in detail: overall household energy reduction and targeted demand response. First, installation of a NILM device can provide electricity customers information that allows them to become more aware of their energy consumption, and thereby, more energy efficient. A study was conducted that looked at the electricity consumption of 174 homes that were using a passive NILM device in their home. This NILM device provided immediate feedback on the power consumption for a portion of the home's appliances via smart-phone application. The homes reduced their monthly energy consumption by an average of 2.6 - 3.1\% after the NILM installation. This was validated by a number of analysis methods returning similar results. Aligned with this benefit comes a recommendation for an incentive structure that can reduce the price paid by the consumer and develop a higher adoption rate of NILM devices. Second, the wide-spread adoption of NILM devices can provide electric utilities information to reduce carbon intensity via targeted demand response. There is a significant opportunity for utilities to engage their customers based on the time of use of detailed appliances. Multiple metrics are presented in this thesis to quantify the deferrable load opportunity of specific appliances and individual households. Utility operational cost savings and greater customer incentives can be linked to the use of these metrics.},
  copyright = {MIT theses are protected by copyright. They may be viewed, downloaded, or printed from this source but further reproduction or distribution in any format is prohibited without written permission.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  annotation = {Accepted: 2018-09-17T15:52:35Z},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Hare18disaggHmLdDmdResp.pdf}
}

@inproceedings{Habte19solarDatSERIE-QC,
  title = {Data {{Quality Assessment Using SERI-QC}}},
  booktitle = {6th {{International Conference Energy}} \& {{Meteorology}} ({{ICEM}}),},
  author = {Habte, Aron and Sengupta, Manajit and Andreas, Afshin and Xie, Yu},
  year = {2019},
  month = jun,
  pages = {1},
  address = {Denmark},
  abstract = {High-quality solar resource data requires stringent calibration, operation and maintenance, and data quality assessment and control procedures. {$\bullet$} Strict protocols for calibration, measurement, modeling, operation and maintenance, and data quality assessment and control are used at NREL. {$\bullet$} NREL's SERI-QC software package is used to assess measurement data quality. {$\bullet$} SERI-QC uses three component data analyses in the K-space, as shown in Figure 1.},
  langid = {english},
  note = {Does 3 component test. Does solar QC for the OpenSolar R and Python package (Feng19openSolarDat) The manual came from here: https://www.nrel.gov/docs/legosti/old/5608.pdf and it has a flowshart of the original FORTRAN algorithm in section 6.3},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Habte19solarDatSERIE-QC.pdf}
}

@phdthesis{Rehman21LoadDisaggThesis,
  type = {Thesis},
  title = {Load {{Disaggregation}}: {{Towards Energy Efficient Systems}}},
  shorttitle = {Load {{Disaggregation}}},
  author = {Rehman, Attique Ur},
  year = {2021},
  url = {https://openrepository.aut.ac.nz/handle/10292/14002},
  urldate = {2021-03-15},
  abstract = {Electricity is one of the valuables and widely used forms of energy. However, with the fast-paced technological development in the electrical and electronics market, the electricity demand is on a constant rise. To tackle energy and sustainability issues, two paths can be followed by the world community. Either new generation plants to be established with an expense of millions of dollars or to explore the existing system by integrating innovative techniques that can lead to energy efficiency and conservation. The latter one is a more viable solution, and the research community is extensively working to propose and develop innovative techniques towards energy efficiency and conservation. In this context, energy monitoring is one of the key techniques that play a significant role in the field of sustainable energy.    Load disaggregation is one of these promising energy monitoring techniques, where a non-intrusive load disaggregation technique commonly referred to as non-intrusive load monitoring (NILM) is widely adopted to provide individual load profiles to the stakeholders. Appliance-level energy monitoring is not only beneficial for the consumers in terms of having valuable information regarding the operation status of their loads and corresponding consumption but also benefit the system operators, policymakers, and manufacturers in terms of analyzing the network's energy flow, creating policies/tariffs, and manufacturing of smart appliances, respectively.     This research work contributes to the existing research and development of non-invasive load disaggregation systems, by proposing and developing a robust event-based non-intrusive load disaggregation approach for low sampling data granularity. As a way forward, this research work contributes to different aspects of a NILM system. For NILM event detection, three new low complexity and computationally fast algorithms based on statistical parameters are proposed and validated on real-world datasets. In terms of electrical load features, a set of nine distinct load features based on statistical, geometrical, and power features is proposed. The extracted load features are further investigated in terms of significance using different feature selection methodologies and the extracted results are validated in the context of classification performance. For load classification, this research work investigated different supervised machine learning models towards an optimal learning model for the given conditions. In addition to standalone machine learning models, this research also presents a combinatorial learning model, i.e., ensemble learning, for load classification in the context of the NILM. Further, a comprehensive comparative evaluation of these techniques is also part of this thesis.     The entire digital simulations and corresponding analysis presented in this research work are based on real-world electricity datasets, originating from different geographical regions, i.e., New Zealand and the United States of America. Based on the low data granularity of the employed databases, three different appliances/circuits, i.e., air conditioning unit, electric vehicle charging, and water heating are successfully disaggregated using the proposed non-intrusive load disaggregation approach. Moreover, a proof of concept in terms of real-world deployment, i.e., the application of the proposed non-intrusive load disaggregation, is also proposed and validated in this research work.     Due to low data granularity nature, this research work is more relevant for the existing metering infrastructure. Therefore, the proposed methodologies and corresponding simulation studies presented in this research work will significantly contribute to the existing state of the art on low sampling NILM systems particularly in terms of event detection, electrical load features, and learning model selection. The study presented in this thesis will also facilitate future research in terms of real-world deployment of NILM systems and its broader applications. Concisely, this research work based on a non-invasive load disaggregation approach is a way forward for energy efficient systems.},
  copyright = {OpenAccess},
  langid = {english},
  school = {Auckland University of Technology},
  annotation = {Accepted: 2021-02-16T21:18:35Z},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Rehman21LoadDisaggThesis.pdf}
}

@article{Hoffmann19autoDetEVHrlyDat,
  title = {Automated {{Detection}} of {{Electric Vehicles}} in {{Hourly Smart Meter Data}}},
  author = {Hoffmann, Volker and Fesche, Bj{\o}rn Ingeberg and Ingebrigtsen, Karoline and Christie, Ingrid Nytun and Punnerud Engelstad, Morten},
  year = {2019},
  journal = {1531},
  publisher = {AIM},
  issn = {2032-9644},
  url = {https://sintef.brage.unit.no/sintef-xmlui/handle/11250/2618624},
  urldate = {2021-03-15},
  abstract = {Automated detection of EVs from smart meter data can provide important insights for DSOs about spatiotemporal EV charging patterns. However, smart meters typically provide only hourly measurements of consumption while most load disaggregation techniques require at least minute level data. We use machine and deep learning methods to detect EV signatures in hourly smart meter data. Models are trained and evaluated on labelled data, before being tested on unlabelled field data. While balanced models catch about 75\% of EVs at false positive rates of 35\%, tuned models detect up to 90\% of EVs with 10\% false positives. When using models to detect EVs on unlabelled Norwegian smart meter data, detections are in line with EV fractions from the national registry as well as expected spatiotemporal patterns. However, models may be confused by baseline consumption patterns. Collection and inclusion of labelled EVs is therefore the next step.},
  copyright = {Attribution-NonCommercial-NoDerivatives 4.0 Internasjonal},
  langid = {english},
  annotation = {Accepted: 2019-09-25T07:25:40Z},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Hoffmann19autoDetEVHrlyDat.pdf}
}

@article{Barsim18FeasibilityGenericDeep,
  title = {On the {{Feasibility}} of {{Generic Deep Disaggregation}} for {{Single-Load Extraction}}},
  author = {Barsim, Karim Said and Yang, B.},
  year = {2018},
  journal = {ArXiv},
  abstract = {Recently, and with the growing development of big energy datasets, data-driven learning techniques began to represent a potential solution to the energy disaggregation problem outperforming engineered and hand-crafted models. However, most proposed deep disaggregation models are load-dependent in the sense that either expert knowledge or a hyper-parameter optimization stage is required prior to training and deployment (normally for each load category) even upon acquisition and cleansing of aggregate and sub-metered data. In this paper, we present a feasibility study on the development of a generic disaggregation model based on data-driven learning. Specifically, we present a generic deep disaggregation model capable of achieving state-of-art performance in load monitoring for a variety of load categories. The developed model is evaluated on the publicly available UK-DALE dataset with a moderately low sampling frequency and various domestic loads.},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Barsim18FeasibilityGenericDeep.pdf}
}

@article{Tang19spatioTempSemiParaCpla,
  title = {Copula-Based Semiparametric Models for Spatiotemporal Data},
  author = {Tang, Yanlin and Wang, Huixia J. and Sun, Ying and Hering, Amanda S.},
  year = {2019},
  journal = {Biometrics},
  volume = {75},
  number = {4},
  pages = {1156--1167},
  issn = {1541-0420},
  doi = {10.1111/biom.13066},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13066},
  urldate = {2021-03-13},
  abstract = {The joint analysis of spatial and temporal processes poses computational challenges due to the data's high dimensionality. Furthermore, such data are commonly non-Gaussian. In this paper, we introduce a copula-based spatiotemporal model for analyzing spatiotemporal data and propose a semiparametric estimator. The proposed algorithm is computationally simple, since it models the marginal distribution and the spatiotemporal dependence separately. Instead of assuming a parametric distribution, the proposed method models the marginal distributions nonparametrically and thus offers more flexibility. The method also provides a convenient way to construct both point and interval predictions at new times and locations, based on the estimated conditional quantiles. Through a simulation study and an analysis of wind speeds observed along the border between Oregon and Washington, we show that our method produces more accurate point and interval predictions for skewed data than those based on normality assumptions.},
  copyright = {{\copyright} 2019 The International Biometric Society},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Tang19spatioTempSemiParaCpla.pdf}
}

@phdthesis{Linnet05toolsWindTrade,
  title = {Tools Supporting Wind Energy Trade in Deregulated Markets},
  author = {Linnet, U.},
  year = {2005},
  month = jul,
  address = {Lyngby, Denmark},
  abstract = {A large share of the wind energy produced in Scandinavia is sold at deregulated electricity markets. The main market, Elspot, is a day-ahead market where energy is sold up to 36 hours before delivery. Failure in delivering exactly the quantity which was sold results in a fine, called regulation cost. As wind energy comes from an uncontrollable energy source - the wind - producers can not always fulfil their sales obligations and must, therefore, often pay high regulation costs. In this thesis it is examined how producers can increase their profit by bidding on the market in such a way that the regulation cost is minimised. The methods developed rely on new production forecasts which provide better probabilistic information about the prediction uncertainty than many forecasting systems currently in use. The problem is formulated in two different ways. One, originally presented by John B. Bremnes, where only a part of the market is included, gives a simple method that can be applied using only statistical tools. The other method is more exible at the cost of complexity. It uses both statistics and stochastic programming. This method can be changed and applied in other markets with a structure different from that of the Scandinavian market, NordPool. Keywords: Electricity market, Wind energy, NordPool, Quantile Regression, Stochastic Programming.},
  owner = {sotterson},
  school = {Department of Informatics and Mathematical Modelling (IMM) of the Technical University of Denmark (DTU)},
  note = {Henrik's master's student describing how to reduce regulation costs on the nordpool markets. Uses probabilstic forecasts. Could be a good example of Nordpool use.},
  timestamp = {2013.09.26}
}

@book{Ackermann05WindPowSysBook,
  title = {Wind Power in Power Systems},
  author = {Ackermann, Thomas and others},
  year = {2005},
  volume = {140},
  publisher = {Wiley Online Library},
  url = {http://iie.fing.edu.uy/simsee/curso2010/wind_power_in_power_systems.pdf},
  abstract = {1 Introduction 1 Part A Theoretical Background and Technical Regulations 5 2 Historical Development and Current Status of Wind Power 7 Thomas Ackermann 2.1 Introduction 7 2.2 Historical Background 8 2.2.1 Mechanical power generation 8 2.2.2 Electrical power generation 9 2.3 Current Status of Wind Power Worldwide 11 2.3.1 Overview of grid-connected wind power generation 11 2.3.2 Europe 11 2.3.3 North America 13 2.3.4 South and Central America 16 2.3.5 Asia and Pacific 16 2.3.6 Middle East and Africa 17 2.3.7 Overview of stand-alone generation 18 2.3.8 Wind power economics 18 2.3.9 Environmental issues 20 2.4 Status of Wind Turbine Technology 21 2.4.1 Design approaches 22 2.5 Conclusions 23 Acknowledgements 23 References 23 3 Wind Power in Power Systems: An Introduction 25 Lennart So?der and Thomas Ackermann 3.1 Introduction 25 3.2 Power System History 25 3.3 Current Status of Wind Power in Power Systems 26 3.4 Network Integration Issues for Wind Power 28 3.5 Basic Electrical Engineering 29 3.6 Characteristics of Wind Power Generation 32 3.6.1 The wind 32 3.6.2 The physics 33 3.6.3 Wind power production 34 3.7 Basic Integration Issues Related to Wind Power 40 3.7.1 Consumer requirements 40 3.7.2 Requirements from wind farm operators 41 3.7.3 The integration issues 41 3.8 Conclusions 46 Appendix: A Mechanical Equivalent to Power System Operation with Wind Power 47 Introduction 47 Active power balance 48 Reactive power balance 49 References 50 4 Generators and Power Electronics for Wind Turbines 53 Anca D. Hansen 4.1 Introduction 53 4.2 State-of-the-art Technologies 53 4.2.1 Overview of wind turbine topologies 53 4.2.2 Overview of power control concepts 55 4.2.3 State-of-the-art generators 55 4.2.4 State-of-the-art power electronics 59 4.2.5 State-of-the-art market penetration 62 4.3 Generator Concepts 65 4.3.1 Asynchronous (induction) generator 66 4.3.2 The synchronous generator 69 4.3.3 Other types of generators 70 4.4 Power Electronic Concepts 72 4.4.1 Soft-starter 72 4.4.2 Capacitor bank 72 4.4.3 Rectifiers and inverters 73 4.4.4 Frequency converters 74 4.5 Power Electronic Solutions in Wind Farms 75 4.6 Conclusions 77 References 77 5 Power Quality Standards for Wind Turbines 79 John Olav Tande 5.1 Introduction 79 5.2 Power Quality Characteristics of Wind Turbines 80 viii Contents 5.2.1 Rated data 81 5.2.2 Maximum permitted power 81 5.2.3 Maximum measured power 81 5.2.4 Reactive power 81 5.2.5 Flicker coefficient 82 5.2.6 Maximum number of wind turbine switching operations 83 5.2.7 Flicker step factor 83 5.2.8 Voltage change factor 84 5.2.9 Harmonic currents 84 5.2.10 Summary power quality characteristics for various wind turbine types 84 5.3 Impact on Voltage Quality 85 5.3.1 General 85 5.3.2 Case study specifications 86 5.3.3 Slow voltage variations 87 5.3.4 Flicker 89 5.3.5 Voltage dips 91 5.3.6 Harmonic voltage 92 5.4 Discussion 93 5.5 Conclusions 94 References 95 6 Power Quality Measurements 97 Fritz Santjer 6.1 Introduction 97 6.2 Requirements for Power Quality Measurements 98 6.2.1 Guidelines 98 6.2.2 Specification 99 6.2.3 Future aspects 104 6.3 Power Quality Characteristics of Wind Turbines and Wind Farms 105 6.3.1 Power peaks 105 6.3.2 Reactive power 106 6.3.3 Harmonics 106 6.3.4 Flicker 108 6.3.5 Switching operations 109 6.4 Assessment Concerning the Grid Connection 111 6.5 Conclusions 112 References 113 7 Technical Regulations for the Interconnection of Wind Farms to the Power System 115 Julija Matevosyan, Thomas Ackermann and Sigrid M. Bolik 7.1 Introduction 115 7.2 Overview of Technical Regulations 115 7.2.1 Regulations for networks below 110 kV 117 7.2.2 Regulations for networks above 110 kV 119 7.2.3 Combined regulations 120 7.3 Comparison of Technical Interconnection Regulations 121 7.3.1 Active power control 122 7.3.2 Frequency control 123 Contents ix 7.3.3 Voltage control 124 7.3.4 Tap changers 128 7.3.5 Wind farm protection 128 7.3.6 Modelling information and verification 133 7.3.7 Communication and external control 133 7.3.8 Discussion of interconnection regulations 134 7.4 Technical Solutions for New Interconnection Rules 136 7.4.1 Absolute power constraint 136 7.4.2 Balance control 136 7.4.3 Power rate limitation control approach 136 7.4.4 Delta control 137 7.5 Interconnection Practice 138 7.6 Conclusions 140 References 140 8 Power System Requirements for Wind Power 143 Hannele Holttinen and Ritva Hirvonen 8.1 Introduction 143 8.2 Operation of the Power System 144 8.2.1 System reliability 145 8.2.2 Frequency control 146 8.2.3 Voltage management 147 8.3 Wind Power Production and the Power System 149 8.3.1 Production patterns of wind power 149 8.3.2 Variations of production and the smoothing effect 151 8.3.3 Predictability of wind power production 155 8.4 Effects of Wind Energy on the Power System 156 8.4.1 Short-term effects on reserves 156 8.4.2 Other short-term effects 160 8.4.3 Long-term effects on the adequacy of power capacity 162 8.4.4 Wind power in future power systems 164 8.5 Conclusions 164 References 165 9 The Value of Wind Power 169 Lennart So?der 9.1 Introduction 169 9.2 The Value of a Power Plant 169 9.2.1 Operating cost value 169 9.2.2 Capacity credit 170 9.2.3 Control value 170 9.2.4 Loss reduction value 170 9.2.5 Grid investment value 170 9.3 The Value of Wind Power 170 9.3.1 The operating cost value of wind power 171 9.3.2 The capacity credit of wind power 171 9.3.3 The control value of wind power 174 9.3.4 The loss reduction value of wind power 177 9.3.5 The grid investment value of wind power 180 x Contents 9.4 The Market Value of Wind Power 180 9.4.1 The market operation cost value of wind power 180 9.4.2 The market capacity credit of wind power 181 9.4.3 The market control value of wind power 182 9.4.4 The market loss reduction value of wind power 188 9.4.5 The market grid investment value of wind power 189 9.5 Conclusions 194 References 195 Part B Power System Integration Experience 197 10 Wind Power in the Danish Power System 199 Peter Borre Eriksen and Carl Hilger 10.1 Introduction 199 10.2 Operational Issues 203 10.2.1 The Nordic market model for electricity trading 205 10.2.2 Different markets 207 10.2.3 Interaction between technical rules and the market 209 10.2.4 Example of how Eltra handles the balance task 210 10.2.5 Balancing via Nord Pool: first step 211 10.2.6 The accuracy of the forecasts 213 10.2.7 Network controller and instantaneous reserves 215 10.2.8 Balancing prices in the real-time market 215 10.2.9 Market prices fluctuating with high wind production 217 10.2.10 Other operational problems 217 10.3 System Analysis and Modelling Issues 219 10.3.1 Future development of wind power 219 10.3.2 Wind regime 220 10.3.3 Wind power forecast models 221 10.3.4 Grid connection 223 10.3.5 Modelling of power systems with large-scale wind power production 224 10.3.6 Wind power and system analysis 226 10.3.7 Case study CO2 reductions according to the Kyoto Protocol 228 10.4 Conclusions and Lessons Learned 231 References 232 11 Wind Power in the German Power System: Current Status and Future Challenges of Maintaining Quality of Supply 233 Matthias Luther, Uwe Radtke and Wilhelm R. Winter 11.1 Introduction 233 11.2 Current Performance of Wind Energy in Germany 234 11.3 Wind Power Supply in the E.ON Netz Area 236 11.4 Electricity System Control Requirements 237 11.5 Network Planning and Connection Requirements 238 11.6 Wind Turbines and Dynamic Performance Requirements 241 11.7 Object of Investigation and Constraints 241 Contents xi 11.8 Simulation Results 244 11.8.1 Voltage quality 244 11.8.2 Frequency stability 248 11.9 Additional Dynamic Requirements of Wind Turbines 252 11.10 Conclusions 254 References 255 12 Wind Power on Weak Grids in California and the US Midwest 257 H. M. Romanowitz 12.1 Introduction 257 12.2 The Early Weak Grid: Background 259 12.2.1 Tehachapi 66 kV transmission 259 12.2.2 VARs 260 12.2.3 FACTS devices 260 12.2.4 Development of wind energy on the Tehachapi 66 kV grid 261 12.2.5 Reliable generation 262 12.2.6 Capacity factor improvement: firming intermittent wind generation 263 12.3 Voltage Regulation: VAR Support on a Wind-dominated Grid 264 12.3.1 Voltage control of a self-excited induction machine 264 12.3.2 Voltage regulated VAR control 264 12.3.3 Typical wind farm PQ operating characteristics 265 12.3.4 Local voltage change from VAR support 267 12.3.5 Location of supplying VARs within a wind farm 268 12.3.6 Self-correcting fault condition: VAR starvation 269 12.3.7 Efficient-to-use idle wind turbine component capacity for low-voltage VARs 270 12.3.8 Harmonics and harmonic resonance: location on grid 271 12.3.9 Islanding, self-correcting conditions and speed of response for VAR controls 274 12.3.10 Self-correcting fault condition: VAR starvation 275 12.3.11 Higher-speed grid events: wind turbines that stay connected through grid events 276 12.3.12 Use of advanced VAR support technologies on weak grids 278 12.3.13 Load flow studies on a weak grid and with induction machines 279 12.4 Private Tehachapi Transmission Line 280 12.5 Conclusions 281 References 282 13 Wind Power on the Swedish Island of Gotland 283 Christer Liljegren and Thomas Ackermann 13.1 Introduction 283 13.1.1 History 283 13.1.2 Description of the local power system 285 13.1.3 Power exchange with the mainland 286 13.1.4 Wind power in the South of Gotland 286 13.2 The Voltage Source Converter Based High-voltage Direct-current Solution 287 13.2.1 Choice of technology 287 13.2.2 Description 287 13.2.3 Controllability 288 xii Contents 13.2.4 Reactive power support and control 288 13.2.5 Voltage control 288 13.2.6 Protection philosophy 289 13.2.7 Losses 290 13.2.8 Practical experience with the installation 290 13.2.9 Tj?reborg Project 291 13.3 Grid Issues 291 13.3.1 Flicker 292 13.3.2 Transient phenomena 292 13.3.3 Stability issues with voltage control equipment 293 13.3.4 Validation 294 13.3.5 Power flow 295 13.3.6 Technical responsibility 296 13.3.7 Future work 296 13.4 Conclusions 296 Further Reading 297 References 297 14 Isolated Systems with Wind Power 299 Per Lundsager and E. Ian Baring-Gould 14.1 Introduction 299 14.2 Use of Wind Energy in Isolated Power Systems 300 14.2.1 System concepts and configurations 300 14.2.2 Basic considerations and constraints for wind?diesel power stations 305 14.3 Categorisation of Systems 310 14.4 Systems and Experience 311 14.4.1 Overview of systems 312 14.4.2 Hybrid power system experience 312 14.5 Wind Power Impact on Power Quality 315 14.5.1 Distribution network voltage levels 316 14.5.2 System stability and power quality 316 14.5.3 Power and voltage fluctuations 317 14.5.4 Power system operation 317 14.6 System Modelling Requirements 320 14.6.1 Requirements and applications 321 14.6.2 Some numerical models for isolated systems 322 14.7 Application Issues 322 14.7.1 Cost of energy and economics 324 14.7.2 Consumer demands in isolated communities 325 14.7.3 Standards, guidelines and project development approaches 325 14.8 Conclusions and Recommendations 327 References 328 15 Wind Farms in Weak Power Networks in India 331 Poul S?rensen 15.1 Introduction 331 15.2 Network Characteristics 334 15.2.1 Transmission capacity 334 15.2.2 Steady-state voltage and outages 335 Contents xiii 15.2.3 Frequency 337 15.2.4 Harmonic and interharmonic distortions 337 15.2.5 Reactive power consumption 338 15.2.6 Voltage imbalance 338 15.3 Wind Turbine Characteristics 338 15.4 Wind Turbine Influence on Grids 339 15.4.1 Steady-state voltage 339 15.4.2 Reactive power consumption 339 15.4.3 Harmonic and interharmonic emission 342 15.5 Grid Influence on Wind Turbines 343 15.5.1 Power performance 343 15.5.2 Safety 345 15.5.3 Structural lifetime 346 15.5.4 Stress on electric components 346 15.5.5 Reactive power compensation 346 15.6 Conclusions 347 References 347 16 Practical Experience with Power Quality and Wind Power 349 A ? ke Larsson 16.1 Introduction 349 16.2 Voltage Variations 349 16.3 Flicker 352 16.3.1 Continuous operation 352 16.3.2 Switching operations 354 16.4 Harmonics 358 16.5 Transients 360 16.6 Frequency 361 16.7 Conclusions 363 References 363 17 Wind Power Forecast for the German and Danish Networks 365 Bernhard Ernst 17.1 Introduction 365 17.2 Current Development and Use of Wind Power Prediction Tools 366 17.3 Current Wind Power Prediction Tools 367 17.3.1 Prediktor 367 17.3.2 Wind Power Prediction Tool 368 17.3.3 Zephyr 370 17.3.4 Previento 370 17.3.5 eWind 370 17.3.6 SIPREO?LICO 371 17.3.7 Advanced Wind Power Prediction Tool 372 17.3.8 HONEYMOON project 376 17.4 Conclusions and Outlook 377 17.4.1 Conclusions 377 17.4.2 Outlook 380 References 380 Useful websites 381 xiv Contents 18 Economic Aspects of Wind Power in Power Systems 383 Thomas Ackermann and Poul Erik Morthorst 18.1 Introduction 383 18.2 Costs for Network Connection and Network Upgrading 384 18.2.1 Shallow connection charges 384 18.2.2 Deep connection charges 387 18.2.3 Shallowish connection charges 388 18.2.4 Discussion of technical network limits 388 18.2.5 Summary of network interconnection and upgrade costs 389 18.3 System Operation Costs in a Deregulated Market 390 18.3.1 Primary control issues 391 18.3.2 Treatment of system operation costs 392 18.3.3 Secondary control issues 392 18.3.4 Electricity market aspects 395 18.4 Example: Nord Pool 395 18.4.1 The Nord Pool power exchange 396 18.4.2 Elspot pricing 397 18.4.3 Wind power and the power exchange 398 18.4.4 Wind power and the balancing market 403 18.5 Conclusions 408 References 409 Part C Future Concepts 411 19 Wind Power and Voltage Control 413 J. G. Slootweg, S. W. H. de Haan, H. Polinder and W. L. Kling 19.1 Introduction 413 19.2 Voltage Control 414 19.2.1 The need for voltage control 414 19.2.2 Active and reactive power 416 19.2.3 Impact of wind power on voltage control 417 19.3 Voltage Control Capabilities of Wind Turbines 420 19.3.1 Current wind turbine types 420 19.3.2 Wind turbine voltage control capabilities 421 19.3.3 Factors affecting voltage control 425 19.4 Simulation Results 425 19.4.1 Test system 425 19.4.2 Steady-state analysis 426 19.4.3 Dynamic analysis 428 19.5 Voltage Control Capability and Converter Rating 430 19.6 Conclusions 431 References 432 20 Wind Power in Areas with Limited Transmission Capacity 433 Julija Matevosyan 20.1 Introduction 433 20.2 Transmission Limits 434 20.2.1 Thermal limit 434 20.2.2 Voltage stability limit 435 Contents xv 20.2.3 Power output of wind turbines 438 20.2.4 Transient stability 439 20.2.5 Summary 439 20.3 Transmission Capacity: Methods of Determination 440 20.3.1 Determination of cross-border transmission capacity 440 20.3.2 Determination of transmission capacity within the country 441 20.3.3 Summary 442 20.4 Measures to Increase Transmission Capacity 442 20.4.1 ?Soft? measures 442 20.4.2 Possible reinforcement measures: thermal limit 443 20.4.3 Possible reinforcement measures: voltage stability limit 444 20.4.4 Converting AC transmission lines to DC for higher transmission ratings 444 20.5 Impact of Wind Generation on Transmission Capacity 445 20.6 Alternatives to Grid Reinforcement for the Integration of Wind Power 446 20.6.1 Regulation using existing generation sources 447 20.6.2 Wind energy spillage 447 20.6.3 Summary 457 20.7 Conclusions 458 References 458 21 Benefits of Active Management of Distribution Systems 461 Goran Strbac, Predrag Djapic?, Thomas Bopp and Nick Jenkins 21.1 Background 461 21.2 Active Management 462 21.2.1 Voltage-rise effect 462 21.2.2 Active management control strategies 464 21.3 Quantification of the Benefits of Active Management 465 21.3.1 Introduction 465 21.3.2 Case studies 466 21.4 Conclusions 476 References 476 22 Transmission Systems for Offshore Wind Farms 479 Thomas Ackermann 22.1 Introduction 479 22.2 General Electrical Aspects 481 22.2.1 Offshore substations 482 22.2.2 Redundancy 483 22.3 Transmission System to Shore 484 22.3.1 High-voltage alternating-current transmission 485 22.3.2 Line-commutated converter based high-voltage direct-current transmission 486 22.3.3 Voltage source converter based high-voltage direct-current transmission 488 22.3.4 Comparison 490 22.4 System Solutions for Offshore Wind Farms 497 22.4.1 Use of low frequency 497 22.4.2 DC solutions based on wind turbines with AC generators 498 22.4.3 DC solutions based on wind turbines with DC generators 498 22.5 Offshore Grid Systems 499 22.6 Alternative Transmission Solutions 500 22.7 Conclusions 500 xvi Contents Acknowledgement 501 References 501 23 Hydrogen as a Means of Transporting and Balancing Wind Power Production 505 Robert Steinberger-Wilckens 23.1 Introduction 505 23.2 A Brief Introduction to Hydrogen 506 23.3 Technology and Efficiency 507 23.3.1 Hydrogen production 507 23.3.2 Hydrogen storage 508 23.3.3 Hydrogen transport 509 23.4 Reconversion to Electricity: Fuel Cells 510 23.5 Hydrogen and Wind Energy 512 23.6 Upgrading Surplus Wind Energy 514 23.6.1 Hydrogen products 516 23.7 A Blueprint for a Hydrogen Distribution System 516 23.7.1 Initial cost estimates 518 23.8 Conclusions 519 References 519 Part D Dynamic Modelling of Wind Turbines for power System Studies 523 24 Introduction to the Modelling of Wind Turbines 525 Hans Knudsen and J?rgen Nyga?rd Nielsen 24.1 Introduction 525 24.2 Basic Considerations regarding Modelling and Simulations 526 24.3 Overview of Aerodynamic Modelling 526 24.3.1 Basic description of the turbine rotor 527 24.3.2 Different representations of the turbine rotor 532 24.4 Basic Modelling Block Description of Wind Turbines 534 24.4.1 Aerodynamic system 535 24.4.2 Mechanical system 536 24.4.3 Generator drive concepts 536 24.4.4 Pitch servo 539 24.4.5 Main control system 539 24.4.6 Protection systems and relays 541 24.5 Per Unit Systems and Data for the Mechanical System 541 24.6 Different Types of Simulation and Requirements for Accuracy 546 24.6.1 Simulation work and required modelling accuracy 546 24.6.2 Different types of simulation 547 24.7 Conclusions 552 References 553 25 Reduced-order Modelling of Wind Turbines 555 J. G. Slootweg, H. Polinder and W. L. Kling 25.1 Introduction 555 25.2 Power System Dynamics Simulation 556 25.3 Current Wind Turbine Types 557 25.4 Modelling Assumptions 557 Contents xvii 25.5 Model of a Constant-speed Wind Turbine 559 25.5.1 Model structure 559 25.5.2 Wind speed model 559 25.5.3 Rotor model 562 25.5.4 Shaft model 564 25.5.5 Generator model 565 25.6 Model of a Wind Turbine with a Doubly fed Induction Generator 567 25.6.1 Model structure 567 25.6.2 Rotor model 568 25.6.3 Generator model 568 25.6.4 Converter model 570 25.6.5 Protection system model 572 25.6.6 Rotor speed controller model 573 25.6.7 Pitch angle controller model 574 25.6.8 Terminal voltage controller model 575 25.7 Model of a Direct drive Wind Turbine 576 25.7.1 Generator model 577 25.7.2 Voltage controller model 578 25.8 Model Validation 579 25.8.1 Measured and simulated model response 579 25.8.2 Comparison of measurements and simulations 582 25.9 Conclusions 584 References 584 26 High-order Models of Doubly-fed Induction Generators 587 Eva Centeno Lo?pez and Jonas Persson 26.1 Introduction 587 26.2 Advantages of Using a Doubly-fed Induction Generator 588 26.3 The Components of a Doubly-fed Induction Generator 588 26.4 Machine Equations 589 26.4.1 The vector method 590 26.4.2 Notation of quantities 592 26.4.3 Voltage equations of the machine 592 26.4.4 Flux equations of the machine 594 26.4.5 Mechanical equations of the machine 595 26.4.6 Mechanical equations of the wind turbine 597 26.5 Voltage Source Converter 597 26.6 Sequencer 599 26.7 Simulation of the Doubly-fed Induction Generator 599 26.8 Reducing the Order of the Doubly-fed Induction Generator 600 26.9 Conclusions 601 References 602 27 Full-scale Verification of Dynamic Wind Turbine Models 603 Vladislav Akhmatov 27.1 Introduction 603 27.1.1 Background 604 27.1.2 Process of validation 605 27.2 Partial Validation 607 27.2.1 Induction generator model 607 27.2.2 Shaft system model 611 xviii Contents 27.2.3 Aerodynamic rotor model 613 27.2.4 Summary of partial validation 618 27.3 Full-scale Validation 619 27.3.1 Experiment outline 619 27.3.2 Measured behaviour 621 27.3.3 Modelling case 622 27.3.4 Model validation 623 27.3.5 Discrepancies between model and measurements 625 27.4 Conclusions 625 References 626 28 Impacts of Wind Power on Power System Dynamics 629 J. G. Slootweg and W. L. Kling 28.1 Introduction 629 28.2 Power System Dynamics 630 28.3 Actual Wind Turbine Types 631 28.4 Impact of Wind Power on Transient Stability 632 28.4.1 Dynamic behaviour of wind turbine types 632 28.4.2 Dynamic behaviour of wind farms 636 28.4.3 Simulation results 638 28.5 Impact of Wind Power on Small Signal Stability 645 28.5.1 Eigenvalue?frequency domain analysis 645 28.5.2 Analysis of the impact of wind power on small signal stability 646 28.5.3 Simulation results 647 28.5.4 Preliminary conclusions 648 28.6 Conclusions 650 References 651 29 Aggregated Modelling and Short-term Voltage Stability of Large Wind Farms 653 Vladislav Akhmatov 29.1 Introduction 653 29.1.1 Main outline 654 29.1.2 Area of application 655 29.1.3 Additional requirements 655 29.2 Large Wind Farm Model 656 29.2.1 Reactive power conditions 657 29.2.2 Faulting conditions 658 29.3 Fixed-speed Wind Turbines 658 29.3.1 Wind turbine parameters 661 29.3.2 Stabilisation through power ramp 661 29.4 Wind Turbines with Variable Rotor Resistance 663 29.5 Variable-speed Wind Turbines with Doubly-fed Induction Generators 665 29.5.1 Blocking and restart of converter 667 29.5.2 Response of a large wind farm 668 29.6 Variable-speed Wind Turbines with Permanent Magnet Generators 670 29.7 A Single Machine Equivalent 672 29.8 Conclusions 673},
  owner = {sotterson},
  note = {System level book on wind power: economics, electrical networks, storage, forecasting, markets, aeronautic and electro-mechanic design of turbines. Seems to have been used for some classes.},
  timestamp = {2015.01.28}
}

@inproceedings{Siddiqui19depIrradFrcstSkyImg,
  title = {A {{Deep Learning Approach}} to {{Solar-Irradiance Forecasting}} in {{Sky-Videos}}},
  booktitle = {2019 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Siddiqui, Talha Ahmad and Bharadwaj, Samarth and Kalyanaraman, Shivkumar},
  year = {2019},
  month = jan,
  pages = {2166--2174},
  issn = {1550-5790},
  doi = {10.1109/WACV.2019.00234},
  abstract = {Ahead-of-time forecasting of incident solar-irradiance on a panel is indicative of expected energy yield and is essential for efficient grid distribution and planning. Traditionally, these forecasts are based on meteorological physics models whose parameters are tuned by coarse-grained radiometric tiles sensed from geo-satellites. This research presents a novel application of deep neural network approach to observe and estimate short-term weather effects from videos. Specifically, we use time-lapsed videos (sky-videos) obtained from upward facing wide-lensed cameras (sky-cameras) to directly estimate and forecast solar irradiance. We introduce and present results on two large publicly available datasets obtained from weather stations in two regions of North America using relatively inexpensive optical hardware. These datasets contain over a million images that span for 1 and 12 years respectively, the largest such collection to our knowledge. Compared to satellite based approaches, the proposed deep learning approach significantly reduces the normalized mean-absolute-percentage error for both nowcasting, i.e. prediction of the solar irradiance at the instance the frame is captured, as well as forecasting, ahead-of-time irradiance prediction for a duration for upto 4 hours.},
  annotation = {31 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Siddiqui19depIrradFrcstSkyImg.pdf}
}

@article{Hur20optFlowDpAge,
  title = {Optical {{Flow Estimation}} in the {{Deep Learning Age}}},
  author = {Hur, Junhwa and Roth, Stefan},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.02853 [cs]},
  eprint = {2004.02853},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2004.02853},
  urldate = {2020-06-20},
  abstract = {Akin to many subareas of computer vision, the recent advances in deep learning have also significantly influenced the literature on optical flow. Previously, the literature had been dominated by classical energy-based models, which formulate optical flow estimation as an energy minimization problem. However, as the practical benefits of Convolutional Neural Networks (CNNs) over conventional methods have become apparent in numerous areas of computer vision and beyond, they have also seen increased adoption in the context of motion estimation to the point where the current state of the art in terms of accuracy is set by CNN approaches. We first review this transition as well as the developments from early work to the current state of CNNs for optical flow estimation. Alongside, we discuss some of their technical details and compare them to recapitulate which technical contribution led to the most significant accuracy improvements. Then we provide an overview of the various optical flow approaches introduced in the deep learning age, including those based on alternative learning paradigms (e.g., unsupervised and semi-supervised methods) as well as the extension to the multi-frame case, which is able to yield further accuracy improvements.},
  archiveprefix = {arXiv},
  annotation = {22 citations (Semantic Scholar/arXiv) [2023-07-25]},
  note = {Comment: To appear as a book chapter in Modelling Human Motion, N. Noceti, A. Sciutti and F. Rea, Eds., Springer, 2020},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hur20optFlowDpAge.pdf}
}

@article{Huang19estIrradSatReview,
  title = {Estimating Surface Solar Irradiance from Satellites: {{Past}}, Present, and Future Perspectives},
  shorttitle = {Estimating Surface Solar Irradiance from Satellites},
  author = {Huang, Guanghui and Li, Zhanqing and Li, Xin and Liang, Shunlin and Yang, Kun and Wang, Dongdong and Zhang, Yi},
  year = {2019},
  month = nov,
  journal = {Remote Sensing of Environment},
  volume = {233},
  pages = {111371},
  issn = {0034-4257},
  doi = {10.1016/j.rse.2019.111371},
  url = {http://www.sciencedirect.com/science/article/pii/S0034425719303906},
  urldate = {2020-06-19},
  abstract = {Surface Solar Irradiance (SSI) is a key parameter dictating surface-atmosphere interactions, driving radiative, hydrological, and land surface processes, and can thus impinge greatly upon weather and climate. It is thereby a prerequisite of many studies and applications. Estimating SSI from satellites began in the 1960s, and is currently the principal way to map SSI spatiotemporal distributions from regional to global scales. Starting from an overview of historical studies carried out in the past several decades, this paper reviews the progresses made in methodology, validation, and products over these years. First, the requirements of SSI in various studies or applications are presented along with the theoretical background of SSI satellite estimation. Methods to estimate SSI from satellites are then summarized as well as their advantages and limitations. Validations of satellite-based SSI on two typical spatial scales are discussed followed by a brief description of existing products and their accuracies. Finally, the challenges faced by current SSI satellite estimation are analyzed, and possible improvements to implement in the future are suggested. This review not only updates the review paper by Pinker et al. (1995) on satellite methods to derive SSI but also offers a more comprehensive summary of the related studies and applications.},
  langid = {english},
  annotation = {91 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Huang19estIrradSatReview.pdf}
}

@article{Heinemann06frcstSolRadCMV,
  title = {Forecasting of {{Solar Radiation}}},
  author = {Heinemann, Detlev},
  pages = {10},
  abstract = {Solar energy is expected to contribute major shares of the future global energy supply. Due to its fluctuating nature an efficient use will require reliable forecast information of its availability in various time and spatial scales depending on the application. The current status of forecasting solar irradiance for energy generation purposes is briefly reviewed with respect to very short-term forecasting (up to a few hours) and forecasts for up to two days mainly for use in utility applications.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Heinemann06frcstSolRadCMV.pdf}
}

@inproceedings{Cros14cloudPatPredSat,
  title = {Cloud Pattern Prediction from Geostationary Meteorological Satellite Images for Solar Energy Forecasting},
  booktitle = {Remote {{Sensing}} of {{Clouds}} and the {{Atmosphere XIX}}; and {{Optics}} in {{Atmospheric Propagation}} and {{Adaptive Systems XVII}}},
  author = {Cros, S. and S{\'e}bastien, N. and Liandrat, O. and Schmutz, N.},
  year = {2014},
  month = oct,
  volume = {9242},
  pages = {924202},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10.1117/12.2066853},
  url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9242/924202/Cloud-pattern-prediction-from-geostationary-meteorological-satellite-images-for-solar/10.1117/12.2066853.short},
  urldate = {2020-06-18},
  abstract = {Surface solar radiation forecasting permits to predict photovoltaic plant production for a massive and safe integration of solar energy into the electric network. For short-term forecasts (intra-day), methods using images from meteorological geostationary satellites are more suitable than numerical weather prediction models. Forecast schemes consist in assessing cloud motion vectors and in extrapolating cloud patterns from a given satellite image in order to predict cloud cover state above a PV plant. Atmospheric motion vectors retrieval techniques have been studied for several decades in order to improve weather forecasts. However, solar energy forecasting requires the extraction of cloud motion vectors on a finer spatial- and time-resolution than those provided for weather forecast applications. Even if motion vector retrieval is a wide research field in image processing related topics, only block-matching techniques are operationally used for solar energy forecasts via satellite images. In this paper, we propose two motion vectors extraction methods originating from video compression techniques (correlation phase and optical flow methods). We implemented them on a 6-day dataset of Meteosat-10 satellite diurnal images. We proceeded to cloud pattern extrapolation and compared predicted cloud maps against actual ones at different time horizons from 15 minutes to 4 hours ahead. Forecast scores were compared to the state-of-the-art (block matching) method. Correlation phase methods do not outperform block-matching but their computation time is about 25 times shorter. Optical flow based method outperforms all the methods with a satisfactory time computing.},
  annotation = {14 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Cros14cloudPatPredSat.pdf}
}

@article{Lago18irradFrcstSatDNN,
  title = {Short-Term Forecasting of Solar Irradiance without Local Telemetry: {{A}} Generalized Model Using Satellite Data},
  shorttitle = {Short-Term Forecasting of Solar Irradiance without Local Telemetry},
  author = {Lago, Jesus and De Brabandere, Karel and De Ridder, Fjo and De Schutter, Bart},
  year = {2018},
  month = oct,
  journal = {Solar Energy},
  volume = {173},
  pages = {566--577},
  issn = {0038-092X},
  doi = {10.1016/j.solener.2018.07.050},
  url = {http://www.sciencedirect.com/science/article/pii/S0038092X18307138},
  urldate = {2020-06-18},
  abstract = {Due to the increasing integration of solar power into the electrical grid, forecasting short-term solar irradiance has become key for many applications, e.g. operational planning, power purchases, reserve activation, etc. In this context, as solar generators are geographically dispersed and ground measurements are not always easy to obtain, it is very important to have general models that can predict solar irradiance without the need of local data. In this paper, a model that can perform short-term forecasting of solar irradiance in any general location without the need of ground measurements is proposed. To do so, the model considers satellite-based measurements and weather-based forecasts, and employs a deep neural network structure that is able to generalize across locations; particularly, the network is trained only using a small subset of sites where ground data is available, and the model is able to generalize to a much larger number of locations where ground data does not exist. As a case study, 25 locations in The Netherlands are considered and the proposed model is compared against four local models that are individually trained for each location using ground measurements. Despite the general nature of the model, it is shown show that the proposed model is equal or better than the local models: when comparing the average performance across all the locations and prediction horizons, the proposed model obtains a 31.31\% rRMSE (relative root mean square error) while the best local model achieves a 32.01\% rRMSE.},
  langid = {english},
  annotation = {48 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lago18irradFrcstSatDNN.pdf}
}

@inproceedings{Cros14cmvSatSolPow,
  title = {Extracting Cloud Motion Vectors from Satellite Images for Solar Power Forecasting},
  booktitle = {2014 {{IEEE Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Cros, S. and Liandrat, O. and S{\'e}bastien, N. and Schmutz, N.},
  year = {2014},
  month = jul,
  pages = {4123--4126},
  issn = {2153-7003},
  doi = {10.1109/IGARSS.2014.6947394},
  abstract = {The high temporal variability of solar power is a real issue to achieve a balanced production and consumption. Solar power forecasting is then necessary to better exploit this variability and to increase the penetration of photovoltaic power into the energy mix. Solar energy forecasting involves prediction of cloud property above a given point. For several hour ahead forecasts, using images from meteorological geostationary satellite is the most suitable solution. We propose a forecasting method based on a phase correlation algorithm for motion estimation between subsequent cloud maps derived from Meteosat-9 images. The method is assessed against state-of-the-art over a limited area over South of France for a 4-hour period. Cloud index maps are predicted. Our forecasting are 21 \% better than persistence in relative RMSE of cloud index. If state-of-the-art shows better results (23 \%), our algorithm reduces computing of 25 \% and then minimize operational solar forecasting constraints.},
  annotation = {29 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Cros14cmvSatSolPow.pdf}
}

@inproceedings{Lorenz07frcstEnsGridPV,
  title = {{{FORECAST OF ENSEMBLE POWER PRODUCTION BY GRID-CONNECTED PV SYSTEMS}}},
  author = {Lorenz, Elke and Heinemann, Detlev and Wickramarathne, Hashini and Beyer, Hans Georg and Bofinger, Stefan},
  year = {2007},
  abstract = {The contribution of power production by PV systems to the electricity supply is constantly increasing. An efficient use of the fluctuating solar power production will highly benefit from forecast information on the expected power production. This forecast information is necessary for the management of the electricity grids and for solar energy trading. This paper will present and evaluate an approach to forecast regional PV power production. The forecast quality was investigated for single systems and for ensembles of distributed PV systems. Due to spatial averaging effects the forecast for an ensemble of distributed systems shows higher quality than the forecast for single systems. Forecast errors are reduced to an RMSE of 0.05 Wh/Wp for an ensemble of the size of Germany compared to a RMSE of 0.13 Wh/Wp for single PV systems. Besides the forecast accuracy, also the specification of the forecast uncertainty is an important issue for an effective application. An approach to derive weather specific confidence intervals is presented that describe the maximum expected uncertainty of the forecast.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lorenz07frcstEnsGridPV.pdf}
}

@article{Perez10valOpIrradFrcst,
  title = {Validation of Short and Medium Term Operational Solar Radiation Forecasts in the {{US}}},
  author = {Perez, Richard and Kivalov, Sergey and Schlemmer, James and Hemker, Karl and Renn{\'e}, David and Hoff, Thomas E.},
  year = {2010},
  month = dec,
  journal = {Solar Energy},
  volume = {84},
  number = {12},
  pages = {2161--2172},
  issn = {0038-092X},
  doi = {10.1016/j.solener.2010.08.014},
  url = {http://www.sciencedirect.com/science/article/pii/S0038092X10002823},
  urldate = {2020-06-16},
  abstract = {This paper presents a validation of the short and medium term global irradiance forecasts that are produced as part of the US SolarAnywhere (2010) data set. The short term forecasts that extend up to 6-h ahead are based upon cloud motion derived from consecutive geostationary satellite images. The medium term forecasts extend up to 6-days-ahead and are modeled from gridded cloud cover forecasts from the US National Digital Forecast Database. The forecast algorithms are validated against ground measurements for seven climatically distinct locations in the United States for 1year. An initial analysis of regional performance using satellite-derived irradiances as a benchmark reference is also presented.},
  langid = {english},
  annotation = {411 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Perez10valOpIrradFrcst.pdf}
}

@article{Kloss18cmbnPhysLrnMdl,
  title = {Combining Learned and Analytical Models for Predicting Action Effects},
  author = {Kloss, Alina and Schaal, Stefan and Bohg, Jeannette},
  year = {2018},
  month = oct,
  journal = {arXiv:1710.04102 [cs]},
  eprint = {1710.04102},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1710.04102},
  urldate = {2020-06-15},
  abstract = {One of the most basic skills a robot should possess is predicting the effect of physical interactions with objects in the environment. This enables optimal action selection to reach a certain goal state. Traditionally, dynamics are approximated by physics-based analytical models. These models rely on specific state representations that may be hard to obtain from raw sensory data, especially if no knowledge of the object shape is assumed. More recently, we have seen learning approaches that can predict the effect of complex physical interactions directly from sensory input. It is however an open question how far these models generalize beyond their training data. In this work, we investigate the advantages and limitations of neural network based learning approaches for predicting the effects of actions based on sensory input and show how analytical and learned models can be combined to leverage the best of both worlds. As physical interaction task, we use planar pushing, for which there exists a well-known analytical model and a large real-world dataset. We propose to use a convolutional neural network to convert raw depth images or organized point clouds into a suitable representation for the analytical model and compare this approach to using neural networks for both, perception and prediction. A systematic evaluation of the proposed approach on a very large real-world dataset shows two main advantages of the hybrid architecture. Compared to a pure neural network, it significantly (i) reduces required training data and (ii) improves generalization to novel physical interaction.},
  archiveprefix = {arXiv},
  annotation = {75 citations (Semantic Scholar/arXiv) [2023-07-25]},
  note = {Comment: Submitted to IJRR, now includes experiments on learning error models on top of the analytical model and on using non-trivial camera viewpoints},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kloss18cmbnPhysLrnMdl.pdf}
}

@article{Ponsukcharoen12irradFrcstSat,
  title = {Solar {{Irradiance Forecast}} from Satellite Images},
  author = {Ponsukcharoen, Umnouy},
  year = {2012},
  journal = {Stanford Unviersity},
  pages = {16},
  url = {https://web.stanford.edu/class/cme334/docs/2012-12-08-ponsukcharoen_solar.pdf},
  langid = {english},
  note = {Overview of how satellite images are converted to ground irradiance. Explains why dynamic range is in the SA calculation, the cloud indexed image, and the basic astronomical geometry. Lecture notes so lots of pictures.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ponsukcharoen12irradFrcstSat.pdf}
}

@inproceedings{Perez18newSolFrcstSiteSpec,
  title = {A {{New Version}} of the {{SUNY Solar Forecast Model}}: {{A Scalable Approach}} to {{Site-Specific Model Training}}},
  booktitle = {{{WCPEC-7}}},
  author = {Perez, Richard and Schlemmer, James and Kivalov, Sergey and Dise, John and Keelin, Patrick and Grammatico, Mark and Hoff, Thomas and Tuohy, Aidan},
  year = {2018},
  pages = {6},
  publisher = {IEEE},
  address = {Waikoloa, HI},
  abstract = {This article introduces a new version of the SUNY solar forecast model, as implemented in the software SolarAnywhere{\textregistered}. Like the existing version, this new version is intended for direct, out-of-the-box application throughout North America without requiring training/feedback from measured data. The existing version was recently identified by EPRI as most accurate among thirteen operational models after an independent evaluation in two climatically distinct US regions. This new version shows further measurable performance improvements and operational functionality with capability of using historical satellite data for model training purposes. The advantage of incorporating historical satellite data is that it allows this forecast application to scale to all sizes of PV systems (large utility-scale down to individual rooftop) without the requirement of site-measured inputs. This new approach exhibits a 1-5\% root mean square error (RMSE) improvement over the forecast horizon up to two-days in advance.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Perez18newSolFrcstSiteSpec.pdf}
}

@article{Mishra19irradFrcstMultiTdp,
  title = {An {{Integrated Multi-Time-Scale Modeling}} for {{Solar Irradiance Forecasting Using Deep Learning}}},
  author = {Mishra, Sakshi and Palanisamy, Praveen},
  year = {2019},
  month = jun,
  journal = {arXiv:1905.02616 [cs, eess, stat]},
  eprint = {1905.02616},
  primaryclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/1905.02616},
  urldate = {2020-06-05},
  abstract = {For short-term solar irradiance forecasting, the traditional point forecasting methods are rendered less useful due to the non-stationary characteristic of solar power. The amount of operating reserves required to maintain reliable operation of the electric grid rises due to the variability of solar energy. The higher the uncertainty in the generation, the greater the operating-reserve requirements, which translates to an increased cost of operation. In this research work, we propose a unified architecture for multi-time-scale predictions for intra-day solar irradiance forecasting using recurrent neural networks (RNN) and long-short-term memory networks (LSTMs). This paper also lays out a framework for extending this modeling approach to intra-hour forecasting horizons thus, making it a multi-time-horizon forecasting approach, capable of predicting intra-hour as well as intra-day solar irradiance. We develop an end-to-end pipeline to effectuate the proposed architecture. The performance of the prediction model is tested and validated by the methodical implementation. The robustness of the approach is demonstrated with case studies conducted for geographically scattered sites across the United States. The predictions demonstrate that our proposed unified architecture-based approach is effective for multi-time-scale solar forecasts and achieves a lower root-mean-square prediction error when benchmarked against the best-performing methods documented in the literature that use separate models for each time-scale during the day. Our proposed method results in a 71.5\% reduction in the mean RMSE averaged across all the test sites compared to the ML-based best-performing method reported in the literature. Additionally, the proposed method enables multi-time-horizon forecasts with real-time inputs, which have a significant potential for practical industry applications in the evolving grid.},
  archiveprefix = {arXiv},
  annotation = {6 citations (Semantic Scholar/arXiv) [2023-07-25]},
  note = {Comment: 19 pages, 12 figures, 3 tables, under review for journal submission},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Mishra19irradFrcstMultiTdp.pdf}
}

@article{Szczotka00cnnIregNW,
  title = {Learning from {{Irregularly Sampled Data}} with {{Deep Nadaraya}}--{{Watson Kernel Regression Networks}} ({{NWNet}}): {{Application}} to {{Endomicroscopy Image Reconstruction}}},
  author = {Szczotka, Agnieszka Barbara and Rav{\`i}, Daniele and Shakir, Dzhoshkun Ismail},
  pages = {9},
  abstract = {Probe-based Confocal Laser Endomicroscopy (pCLE) enables more accurate diagnosis via optical biopsy. pCLE probes relay on of fibres bundles, which generate irregularly sampled signals. Current pCLE reconstruction is based on interpolating irregular signals onto an over-sampled Cartesian grid, using a sub-optimal Delaunay triangulation based linear interpolation scheme. High-quality reconstruction with improved information representation should be possible with the use of Deep Convolutional Neural Networks (CNNs). However, classical CNNs are limited to take as an input only Cartesian images, not irregular data. In this work, we propose to embed Nadaraya-Watson (NW) kernel regression into the CNN framework as a novel trainable CNN layer that allows for processing of irregularly sampled data represented as sparse data on a Cartesian grid. We design a new NWNet architecture in conjunction with examplar-based super-resolution CNN, which allows reconstructing high-quality pCLE images from the irregularly sampled input data. Models were trained on a database of 8806 images from 238 pCLE video sequences. The results were validated through an image quality assessment based on a composition of the following metrics: PSNR, SSIM, GCF. Our analysis indicates that the proposed solution unlocks the potential of CNNs for sparse data processing. NW layer is the main contribution of our end-to-end model performing pCLE image reconstruction directly from sparse imaging input to high-resolution cartesian images. Our method outperforms the reconstruction method in current clinical use.},
  langid = {english},
  note = {Could be used for Deep Solar forecasting with missing or irregularly spaced data.~ Advantage could be the simple NW kernel inside that interpolates data.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Szczotka00cnnIregNW.pdf}
}

@article{Alzahrani17irradFrcstDp,
  title = {Solar {{Irradiance Forecasting Using Deep Neural Networks}}},
  author = {Alzahrani, Ahmad and Shamsi, Pourya and Dagli, Cihan and Ferdowsi, Mehdi},
  year = {2017},
  month = jan,
  journal = {Procedia Computer Science},
  series = {Complex {{Adaptive Systems Conference}} with {{Theme}}: {{Engineering Cyber Physical Systems}}, {{CAS October}} 30 -- {{November}} 1, 2017, {{Chicago}}, {{Illinois}}, {{USA}}},
  volume = {114},
  pages = {304--313},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2017.09.045},
  url = {http://www.sciencedirect.com/science/article/pii/S1877050917318392},
  urldate = {2020-06-01},
  abstract = {Predicting solar irradiance has been an important topic in renewable energy generation. Prediction improves the planning and operation of photovoltaic systems and yields many economic advantages for electric utilities. The irradiance can be predicted using statistical methods such as artificial neural networks (ANN), support vector machines (SVM), or autoregressive moving average (ARMA). However, they either lack accuracy because they cannot capture long-term dependency or cannot be used with big data because of the scalability. This paper presents a method to predict the solar irradiance using deep neural networks. Deep recurrent neural networks (DRNNs) add complexity to the model without specifying what form the variation should take and allow the extraction of high-level features. The DRNN is used to predict the irradiance. The data utilized in this study is real data obtained from natural resources in Canada. The simulation of this method will be compared to several common methods such as support vector regression and feedforward neural networks (FNN). The results show that deep learning neural networks can outperform all other methods, as the performance tests indicate.},
  langid = {english},
  annotation = {178 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Alzahrani17irradFrcstDp.pdf}
}

@article{Wen19solDmdFrcstDp,
  title = {Optimal Load Dispatch of Community Microgrid with Deep Learning Based Solar Power and Load Forecasting},
  author = {Wen, Lulu and Zhou, Kaile and Yang, Shanlin and Lu, Xinhui},
  year = {2019},
  month = mar,
  journal = {Energy},
  volume = {171},
  pages = {1053--1065},
  issn = {0360-5442},
  doi = {10.1016/j.energy.2019.01.075},
  url = {http://www.sciencedirect.com/science/article/pii/S0360544219300775},
  urldate = {2020-06-01},
  abstract = {A deep recurrent neural network with long short-term memory units (DRNN-LSTM) model is developed to forecast aggregated power load and the photovoltaic (PV) power output in community microgrid. Meanwhile, an optimal load dispatch model for grid-connected community microgrid which includes residential power load, PV arrays, electric vehicles (EVs), and energy storage system (ESS), is established under three different scheduling scenarios. To promote the supply-demand balance, the uncertainties of both residential power load and PV power output are considered in the model by integrating the forecasting results. Two real-world data sets are used to test the proposed forecasting model, and the results show that the DRNN-LSTM model performs better than multi-layer perception (MLP) network and support vector machine (SVM). Finally, particle swarm optimization (PSO) algorithm is used to optimize the load dispatch of grid-connected community microgrid. The results show that EES and the coordinated charging mode of EVs can promote peak load shifting and reduce 8.97\% of the daily costs. This study contributes to the optimal load dispatch of community microgrid with load and renewable energy forecasting. The optimal load dispatch of community microgrid with deep learning based solar power and load forecasting achieves total costs reduction and system reliability improvement.},
  langid = {english},
  annotation = {166 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wen19solDmdFrcstDp.pdf}
}

@article{vanderMeer18netDmdFrcstProbGP,
  title = {Probabilistic Forecasting of Electricity Consumption, Photovoltaic Power Generation and Net Demand of an Individual Building Using {{Gaussian Processes}}},
  author = {{van der Meer}, D. W. and Shepero, M. and Svensson, A. and Wid{\'e}n, J. and Munkhammar, J.},
  year = {2018},
  month = mar,
  journal = {Applied Energy},
  volume = {213},
  pages = {195--207},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2017.12.104},
  url = {http://www.sciencedirect.com/science/article/pii/S0306261917318275},
  urldate = {2020-06-01},
  abstract = {This paper presents a study into the utilization of Gaussian Processes (GPs) for probabilistic forecasting of residential electricity consumption, photovoltaic (PV) power generation and net demand of a single household. The covariance function that encodes prior belief on the general shape of the time series plays a vital role in the performance of GPs and a common choice is the squared exponential (SE), although it has been argued that the SE is likely suboptimal for physical processes. Therefore, we thoroughly test various (combinations of) covariance functions. Furthermore, in order bypass the substantial learning and inference time accompanied with GPs, we investigate the potential of dynamically updating the hyperparameters using a moving training window and assess the consequences on predictive accuracy. We show that the dynamic GP produces sharper prediction intervals (PIs) than the static GP with significant lower computational burden, but at the cost of the ability to capture sharp peaks. In addition, we examine the difference in accuracy between a direct and indirect forecasting strategy in case of net demand forecasting and show that the latter is prone to producing wider PIs with higher coverage probability.},
  langid = {english},
  annotation = {97 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\vanderMeer18netDmdFrcstProbGP.pdf}
}

@article{Torres19solPowFrcstDp,
  title = {Big Data Solar Power Forecasting Based on Deep Learning and Multiple Data Sources},
  author = {Torres, Jos{\'e} F. and Troncoso, Alicia and Koprinska, Irena and Wang, Zheng and Mart{\'i}nez-{\'A}lvarez, Francisco},
  year = {2019},
  journal = {Expert Systems},
  volume = {36},
  number = {4},
  pages = {e12394},
  issn = {1468-0394},
  doi = {10.1111/exsy.12394},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12394},
  urldate = {2020-06-01},
  abstract = {In this paper, we consider the task of predicting the electricity power generated by photovoltaic solar systems for the next day at half-hourly intervals. We introduce DL, a deep learning approach based on feed-forward neural networks for big data time series, which decomposes the forecasting problem into several sub-problems. We conduct a comprehensive evaluation using 2 years of Australian solar data, evaluating accuracy and training time, and comparing the performance of DL with two other advanced methods based on neural networks and pattern sequence similarity. We investigate the use of multiple data sources (solar power and weather data for the previous days, and weather forecast for the next day) and also study the effect of different historical window sizes. The results show that DL produces competitive accuracy results and scales well, and is thus a highly suitable method for big data environments.},
  copyright = {{\copyright} 2019 John Wiley \& Sons, Ltd.},
  langid = {english},
  annotation = {41 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Torres19solPowFrcstDp.pdf}
}

@inproceedings{Mukhoty19irradFrcstDpSeq,
  title = {Sequence to Sequence Deep Learning Models for Solar Irradiation Forecasting},
  booktitle = {2019 {{IEEE Milan PowerTech}}},
  author = {Mukhoty, Bhaskar Pratim and Maurya, Vikas and Shukla, Sandeep Kumar},
  year = {2019},
  month = jun,
  pages = {1--6},
  doi = {10.1109/PTC.2019.8810645},
  abstract = {The energy output a photo voltaic(PV) panel is a function of solar irradiation and weather parameters like temperature and wind speed etc. A general measure for solar irradiation called Global Horizontal Irradiance (GHI), customarily reported in Watt/meter2, is a generic indicator for this intermittent energy resource. An accurate prediction of GHI is necessary for reliable grid integration of the renewable as well as for power market trading. While some machine learning techniques are well introduced along with the traditional time-series forecasting techniques, deep-learning techniques remains less explored for the task at hand. In this paper we give deep learning models suitable for sequence to sequence prediction of GHI. The deep learning models are reported for shortterm forecasting 1 - 24 hour along with the state-of-the art techniques like Gradient Boosted Regression Trees(GBRT) and Feed Forward Neural Networks(FFNN). We have checked that spatio-temporal features like wind direction, wind speed and GHI of neighboring location improves the prediction accuracy of the deep learning models significantly. Among the various sequence-to-sequence encoder-decoder models LSTM performed superior, handling short-comings of the stateof-the-art techniques.},
  annotation = {15 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Mukhoty19irradFrcstDpSeq.pdf}
}

@misc{Hobbs19rampRsrvProbSolar,
  title = {Coordinated {{Ramping Product}} and {{Regulation Reserve}}  {{Procurements}} in {{CAISO}} and {{MISO}} Using  {{Multi-Scale Probabilistic Solar Power Forecasts}} ({{Pro2R}})},
  author = {Hobbs, B.F.},
  year = {2019},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hobbs19rampRsrvProbSolar.pdf}
}

@article{Li19pvRcstRNN,
  title = {Recurrent {{Neural Networks Based Photovoltaic Power Forecasting Approach}}},
  author = {{Li} and {Wang} and {Zhang} and {Xin} and {Liu}},
  year = {2019},
  month = jul,
  journal = {Energies},
  volume = {12},
  number = {13},
  pages = {2538},
  issn = {1996-1073},
  doi = {10.3390/en12132538},
  url = {https://www.mdpi.com/1996-1073/12/13/2538},
  urldate = {2020-06-01},
  abstract = {The intermittency of solar energy resources has brought a big challenge for the optimization and planning of a future smart grid. To reduce the intermittency, an accurate prediction of photovoltaic (PV) power generation is very important. Therefore, this paper proposes a new forecasting method based on the recurrent neural network (RNN). At first, the entire solar power time series data is divided into inter-day data and intra-day data. Then, we apply RNN to discover the nonlinear features and invariant structures exhibited in the adjacent days and intraday data. After that, a new point prediction model is proposed, only by taking the previous PV power data as input without weather information. The forecasting horizons are set from 15 to 90 minutes. The proposed forecasting method is tested by using real solar power in Flanders, Belgium. The classical persistence method (Persistence), back propagation neural network (BPNN), radial basis function (RBF) neural network and support vector machine (SVM), and long short-term memory (LSTM) networks are adopted as benchmarks. Extensive results show that the proposed forecasting method exhibits a good forecasting quality on very short-term forecasting, which demonstrates the feasibility and effectiveness of the proposed forecasting model.},
  langid = {english},
  annotation = {84 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Li19pvRcstRNN.pdf}
}

@article{Ghimire19irradFrcstCNN-LSTM,
  title = {Deep Solar Radiation Forecasting with Convolutional Neural Network and Long Short-Term Memory Network Algorithms},
  author = {Ghimire, Sujan and Deo, Ravinesh C. and Raj, Nawin and Mi, Jianchun},
  year = {2019},
  month = nov,
  journal = {Applied Energy},
  volume = {253},
  pages = {113541},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2019.113541},
  url = {http://www.sciencedirect.com/science/article/pii/S0306261919312152},
  urldate = {2020-06-01},
  abstract = {This paper designs a hybridized deep learning framework that integrates the Convolutional Neural Network for pattern recognition with the Long Short-Term Memory Network for half-hourly global solar radiation (GSR) forecasting. The Convolution network is applied to robustly extract data input features from predictive variables (i.e., statistically significant antecedent inputs) while Long Short-Term Memory absorbs them for prediction. Half-hourly GSR for Alice Springs (Australia: 01 January 2006 to 31 August 2018) are extracted with stationarity checks applied via unit-root and mutual information test to capture antecedent GSR values required to forecast future GSR. The proposed hybrid model is benchmarked with standalone models as well as other Deep Learning, Single Hidden Layer and Tree based models. The results show that the benchmarked models are not able to generate satisfactory GSR predictions and the proposed hybrid model outperforms all other counterparts. The hybrid model registers superior results with over 70\% of predictive errors lying below {\textpm}10 Wm-2 and outperforms the benchmark model for 1-Day half-hourly GSR prediction with low Relative Root Mean Square Error ({$\approx$}1.515\%), Mean Absolute Percentage Error ({$\approx$}4.672\%) and Absolute Percentage Bias ({$\approx$}1.233\%). This study ascertains that a proposed hybrid model based on a convolution network framework can accurately predict GSR and enable energy availability to be regularly monitored over multi-step horizons when coupled with a low latency Long Short-Term Memory network. Furthermore, it also concludes that the proposed model can have practical implications in forecasting GSR, capitalizing its versatility as a stratagem in monitoring solar powered systems by integrating freely available solar radiation into a real power grid system.},
  langid = {english},
  annotation = {203 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ghimire19irradFrcstCNN-LSTM.pdf}
}

@article{Chung20frcstSolPowWth,
  title = {Estimating {{Solar Insolation}} and {{Power Generation}} of {{Photovoltaic Systems Using Previous Day Weather Data}}},
  author = {Chung, Min Hee},
  year = {2020},
  month = feb,
  journal = {Advances in Civil Engineering},
  volume = {2020},
  pages = {1--13},
  issn = {1687-8086, 1687-8094},
  doi = {10.1155/2020/8701368},
  url = {https://www.hindawi.com/journals/ace/2020/8701368/},
  urldate = {2020-06-03},
  abstract = {Day-ahead predictions of solar insolation are useful for forecasting the energy production of photovoltaic (PV) systems attached to buildings, and accurate forecasts are essential for operational efficiency and trading markets. In this study, a multilayer feed-forward neural network-based model that predicts the next day's solar insolation by taking into consideration the weather conditions of the present day was proposed. The proposed insolation model was employed to estimate the energy production of a real PV system located in South Korea. Validation research was performed by comparing the model's estimated energy production with the measured energy production data collected during the PV system operation. The accuracy indices for the optimal model, which included the root mean squared error, mean bias error, and mean absolute error, were 1.43\,kWh/m               2               /day, -0.09\,kWh/m               2               /day, and 1.15\,kWh/m               2               /day, respectively. These values indicate that the proposed model is capable of producing reasonable insolation predictions; however, additional work is needed to achieve accurate estimates for energy trading.},
  langid = {english},
  annotation = {14 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chung20frcstSolPowWth.pdf}
}

@article{Aslam19irradFrcstDpMicro,
  title = {Deep {{Learning Models}} for {{Long-Term Solar Radiation Forecasting Considering Microgrid Installation}}: {{A Comparative Study}}},
  shorttitle = {Deep {{Learning Models}} for {{Long-Term Solar Radiation Forecasting Considering Microgrid Installation}}},
  author = {Aslam, Muhammad and Lee, Jae-Myeong and Kim, Hyung-Seung and Lee, Seung-Jae and Hong, Sugwon},
  year = {2019},
  month = dec,
  journal = {Energies},
  volume = {13},
  number = {1},
  pages = {147},
  issn = {1996-1073},
  doi = {10.3390/en13010147},
  url = {https://www.mdpi.com/1996-1073/13/1/147},
  urldate = {2020-06-01},
  abstract = {Microgrid is becoming an essential part of the power grid regarding reliability, economy, and environment. Renewable energies are main sources of energy in microgrids. Long-term solar generation forecasting is an important issue in microgrid planning and design from an engineering point of view. Solar generation forecasting mainly depends on solar radiation forecasting. Long-term solar radiation forecasting can also be used for estimating the degradation-rate-influenced energy potentials of photovoltaic (PV) panel. In this paper, a comparative study of different deep learning approaches is carried out for forecasting one year ahead hourly and daily solar radiation. In the proposed method, state of the art deep learning and machine learning architectures like gated recurrent units (GRUs), long short term memory (LSTM), recurrent neural network (RNN), feed forward neural network (FFNN), and support vector regression (SVR) models are compared. The proposed method uses historical solar radiation data and clear sky global horizontal irradiance (GHI). Even though all the models performed well, GRU performed relatively better compared to the other models. The proposed models are also compared with traditional state of the art methods for long-term solar radiation forecasting, i.e., random forest regression (RFR). The proposed models outperformed the traditional method, hence proving their efficiency.},
  langid = {english},
  annotation = {83 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Aslam19irradFrcstDpMicro.pdf}
}

@article{Lima20deepSolarFrcstPortfolio,
  title = {Improving Solar Forecasting Using {{Deep Learning}} and {{Portfolio Theory}} Integration},
  author = {Lima, Marcello Anderson F. B. and Carvalho, Paulo C. M. and {Fern{\'a}ndez-Ram{\'i}rez}, Luis M. and Braga, Arthur P. S.},
  year = {2020},
  month = mar,
  journal = {Energy},
  volume = {195},
  pages = {117016},
  issn = {0360-5442},
  doi = {10.1016/j.energy.2020.117016},
  url = {http://www.sciencedirect.com/science/article/pii/S0360544220301237},
  urldate = {2020-06-01},
  abstract = {Solar energy has been consolidated as one of the main renewable energy sources capable of contributing to supply global energy demand. However, the solar resource has intermittent feature in electricity production, making it difficult to manage the electrical system. Hence, we propose the application of Deep Learning (DL), one of the emerging themes in the field of Artificial Intelligence (AI), as a solar predictor. To attest its capacity, the technique is compared with other consolidated solar forecasting strategies such as Multilayer Perceptron, Radial Base Function and Support Vector Regression. Additionally, integration of AI methods in a new adaptive topology based on the Portfolio Theory (PT) is proposed hereby to improve solar forecasts. PT takes advantage of diversified forecast assets: when one of the assets shows prediction errors, these are offset by another asset. After testing with data from Spain and Brazil, results show that the Mean Absolute Percentage Error (MAPE) for predictions using DL is 6.89\% and for the proposed integration (called PrevPT) is 5.36\% concerning data from Spain. For the data from Brazil, MAPE for predictions using DL is 6.08\% and 4.52\% for PrevPT. In both cases, DL and PrevPT results are better than the other techniques being used.},
  langid = {english},
  annotation = {40 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lima20deepSolarFrcstPortfolio.pdf}
}

@article{Kimura19ConvolutionalNeuralNetwork,
  title = {Convolutional {{Neural Network Coupled}} with a {{Transfer-Learning Approach}} for {{Time-Series Flood Predictions}}},
  author = {Kimura, Nobuaki and Yoshinaga, Ikuo and Sekijima, Kenji and Azechi, Issaku and Baba, Daichi},
  year = {2019},
  month = dec,
  journal = {Water},
  volume = {12},
  number = {1},
  pages = {96},
  issn = {2073-4441},
  doi = {10.3390/w12010096},
  url = {https://www.mdpi.com/2073-4441/12/1/96},
  urldate = {2020-06-03},
  abstract = {East Asian regions in the North Pacific have recently experienced severe riverine flood disasters. State-of-the-art neural networks are currently utilized as a quick-response flood model. Neural networks typically require ample time in the training process because of the use of numerous datasets. To reduce the computational costs, we introduced a transfer-learning approach to a neural-network-based flood model. For a concept of transfer leaning, once the model is pretrained in a source domain with large datasets, it can be reused in other target domains. After retraining parts of the model with the target domain datasets, the training time can be reduced due to reuse. A convolutional neural network (CNN) was employed because the CNN with transfer learning has numerous successful applications in two-dimensional image classification. However, our flood model predicts time-series variables (e.g., water level). The CNN with transfer learning requires a conversion tool from time-series datasets to image datasets in preprocessing. First, the CNN time-series classification was verified in the source domain with less than 10\% errors for the variation in water level. Second, the CNN with transfer learning in the target domain efficiently reduced the training time by 1/5 of and a mean error difference by 15\% of those obtained by the CNN without transfer learning, respectively. Our method can provide another novel flood model in addition to physical-based models.},
  langid = {english},
  annotation = {45 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kimura19ConvolutionalNeuralNetwork.pdf}
}

@article{Ribeiro18TransferLearningSeasonal,
  title = {Transfer Learning with Seasonal and Trend Adjustment for Cross-Building Energy Forecasting},
  author = {Ribeiro, Mauro and Grolinger, Katarina and ElYamany, Hany F. and Higashino, Wilson A. and Capretz, Miriam A.M.},
  year = {2018},
  month = apr,
  journal = {Energy and Buildings},
  volume = {165},
  pages = {352--363},
  issn = {03787788},
  doi = {10.1016/j.enbuild.2018.01.034},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0378778817329171},
  urldate = {2020-06-03},
  abstract = {Large scale smart meter deployments have resulted in popularization of sensor-based electricity forecasting which relies on historical sensor data to infer future energy consumption. Although those approaches have been very successful, they require significant quantities of historical data, often over extended periods of time, to train machine learning models and achieve accurate predictions. New buildings and buildings with newly installed meters have small historical datasets that are insufficient to create accurate predictions. Transfer learning methods have been proposed as a way to use cross-domain datasets to improve predictions. However, these methods do not consider the effects of seasonality within domains. Consequently, this paper proposes Hephaestus, a novel transfer learning method for cross-building energy forecasting based on time series multi-feature regression with seasonal and trend adjustment. This method enables energy prediction with merged data from similar buildings with different distributions and different seasonal profiles. Thus, it improves energy prediction accuracy for a new building with limited data by using datasets from other similar buildings. Hephaestus works in the pre- and post- processing phases and therefore can be used with any standard machine learning algorithm. The case study presented here demonstrates that the proposed approach can improve energy prediction for a school by 11.2\% by using additional data from other schools.},
  langid = {english},
  annotation = {122 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ribeiro18TransferLearningSeasonal.pdf}
}

@article{PotikyanTransferLearningTime,
  title = {Transfer {{Learning}} for {{Time Series Prediction}}},
  author = {Potikyan, Nshan},
  pages = {16},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\PotikyanTransferLearningTime2.pdf}
}

@article{StructuralTimeSeries,
  title = {Structural {{Time Series}} Modeling in {{TensorFlow Probability}}},
  pages = {3},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\StructuralTimeSeries.pdf}
}

@article{Laptev18ReconstructionRegressionLoss,
  title = {Reconstruction and {{Regression Loss}} for {{Time-Series Transfer Learning}}},
  author = {Laptev, Nikolay and Yu, Jiafan and Rajagopal, Ram},
  year = {2018},
  pages = {8},
  abstract = {Reliable and accurate time-series modeling is critical in many fields including energy, finance, and manufacturing. Many time-series tasks, however, suffer from a limited amount of clean training data resulting in poor forecasting, classification or clustering performance. Recently, convolutional neural networks (CNNs) have shown outstanding image classification performance even on tasks with small-scale training sets. The performance can be attributed to transfer learning through ability of CNNs to learn rich mid-level image representations. For time-series, however, no prior work exists on general transfer learning. In this paper, motivated by recent success of transfer learning in image-related tasks, we show its applicability for time-series and define a new architecture and a new loss function for time-series transfer learning that is able to outperform the baseline methods typically used in practice for transfer learning.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Laptev18ReconstructionRegressionLoss.pdf}
}

@inproceedings{Jamshidi17TransferLearningImproving,
  title = {Transfer {{Learning}} for {{Improving Model Predictions}} in {{Highly Configurable Software}}},
  booktitle = {2017 {{IEEE}}/{{ACM}} 12th {{International Symposium}} on {{Software Engineering}} for {{Adaptive}} and {{Self-Managing Systems}} ({{SEAMS}})},
  author = {Jamshidi, Pooyan and Velez, Miguel and Kastner, Christian and Siegmund, Norbert and Kawthekar, Prasad},
  year = {2017},
  month = may,
  pages = {31--41},
  publisher = {IEEE},
  address = {Buenos Aires, Argentina},
  doi = {10.1109/SEAMS.2017.11},
  url = {http://ieeexplore.ieee.org/document/7968130/},
  urldate = {2020-06-03},
  abstract = {Modern software systems are built to be used in dynamic environments using configuration capabilities to adapt to changes and external uncertainties. In a self-adaptation context, we are often interested in reasoning about the performance of the systems under different configurations. Usually, we learn a black-box model based on real measurements to predict the performance of the system given a specific configuration. However, as modern systems become more complex, there are many configuration parameters that may interact and we end up learning an exponentially large configuration space. Naturally, this does not scale when relying on real measurements in the actual changing environment. We propose a different solution: Instead of taking the measurements from the real system, we learn the model using samples from other sources, such as simulators that approximate performance of the real system at low cost. We define a cost model that transform the traditional view of model learning into a multi-objective problem that not only takes into account model accuracy but also measurements effort as well. We evaluate our cost-aware transfer learning solution using real-world configurable software including (i) a robotic system, (ii) 3 different stream processing applications, and (iii) a NoSQL database system. The experimental results demonstrate that our approach can achieve (a) a high prediction accuracy, as well as (b) a high model reliability.},
  isbn = {978-1-5386-1550-8},
  langid = {english},
  annotation = {88 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Jamshidi17TransferLearningImproving.pdf}
}

@article{Laptev18APPLIEDTIMESERIESTRANSFER,
  title = {{{APPLIED TIME-SERIES TRANSFER LEARNING}}},
  author = {Laptev, Nikolay and Yu, Jiafan and Rajagopal, Ram},
  year = {2018},
  pages = {4},
  abstract = {Reliable and accurate time-series modeling is critical in many fields including energy, finance, and manufacturing. Many time-series tasks, however, suffer from a limited amount of clean training data resulting in poor forecasting, classification or clustering performance. Recently, convolutional neural networks (CNNs) have shown outstanding image classification performance even on tasks with smallscale training sets. The performance can be attributed to transfer learning through ability of CNNs to learn rich mid-level image representations. For time-series, however, no prior work exists on general transfer learning. In this short paper, motivated by recent success of transfer learning in image-related tasks, we are the first to show that using an LSTM auto-encoder with attention trained on a largescale timeseries dataset with pre-processing we can effectively transfer time-series features across diverse domains.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Laptev18APPLIEDTIMESERIESTRANSFER.pdf}
}

@article{Kilimci19ImprovedDemandForecasting,
  title = {An {{Improved Demand Forecasting Model Using Deep Learning Approach}} and {{Proposed Decision Integration Strategy}} for {{Supply Chain}}},
  author = {Kilimci, Zeynep Hilal and Akyuz, A. Okay and Uysal, Mitat and Akyokus, Selim and Uysal, M. Ozan and Atak Bulbul, Berna and Ekmis, Mehmet Ali},
  year = {2019},
  month = mar,
  journal = {Complexity},
  volume = {2019},
  pages = {1--15},
  issn = {1076-2787, 1099-0526},
  doi = {10.1155/2019/9067367},
  url = {https://www.hindawi.com/journals/complexity/2019/9067367/},
  urldate = {2020-06-03},
  abstract = {Demand forecasting is one of the main issues of supply chains. It aimed to optimize stocks, reduce costs, and increase sales, profit, and customer loyalty. For this purpose, historical data can be analyzed to improve demand forecasting by using various methods like machine learning techniques, time series analysis, and deep learning models. In this work, an intelligent demand forecasting system is developed. This improved model is based on the analysis and interpretation of the historical data by using different forecasting methods which include time series analysis techniques, support vector regression algorithm, and deep learning models. To the best of our knowledge, this is the first study to blend the deep learning methodology, support vector regression algorithm, and different time series analysis models by a novel decision integration strategy for demand forecasting approach. The other novelty of this work is the adaptation of boosting ensemble strategy to demand forecasting system by implementing a novel decision integration model. The developed system is applied and tested on real life data obtained from SOK Market in Turkey which operates as a fast-growing company with 6700 stores, 1500 products, and 23 distribution centers. A wide range of comparative and extensive experiments demonstrate that the proposed demand forecasting system exhibits noteworthy results compared to the state-of-art studies. Unlike the state-of-art studies, inclusion of support vector regression, deep learning model, and a novel integration strategy to the proposed forecasting system ensures significant accuracy improvement.},
  langid = {english},
  annotation = {68 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kilimci19ImprovedDemandForecasting.pdf}
}

@article{LinTransferLearningTraffic,
  title = {Transfer {{Learning}} for {{Traffic Speed Prediction}}: {{A Preliminary Study}}},
  author = {Lin, Bill Y and Xu, Frank F and Liao, Eve Q and Zhu, Kenny Q},
  pages = {4},
  abstract = {Traffic speed prediction can benefit a wide range of IoT applications in intelligent transportation and smart city. Recent supervised machine learning approaches heavily leverage vast amount of historical time-series data. Consequently, they degrade dramatically in the areas where collecting a large traffic data is not quite feasible. With the aim of predicting the traffic speed of such urban areas, we propose a transfer learning framework that exploits historical data of some other dataabundant areas by utilizing various spatio-temporal semantic features. Experimental results show that classic regression models and our proposed kernel regression model can achieve competitive performance comparing to baseline methods that heavily rely on the historical data of target areas.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\LinTransferLearningTraffic2.pdf}
}

@article{Pircalabu17priceVolRiskWndTrdCpla,
  title = {Joint Price and Volumetric Risk in Wind Power Trading: {{A}} Copula Approach},
  author = {Pircalabu, A. and Hvolby, T. and Jung, J. and H??g, E.},
  year = {2017},
  journal = {Energy Economics},
  volume = {62},
  pages = {139--154},
  issn = {0140-9883},
  doi = {10.1016/j.eneco.2016.11.023},
  url = {http://www.sciencedirect.com/science/article/pii/S0140988316303450},
  abstract = {Abstract This paper examines the dependence between wind power production and electricity prices and discusses its implications for the pricing and the risk distributions associated with contracts that are exposed to joint price and volumetric risk. We propose a copula model for the joint behavior of prices and wind power production, which is estimated to data from the Danish power market. We find that the marginal behavior of the individual variables is best described by ARMA???GARCH models with non-Gaussian error distributions, and the preferred copula model is a time-varying Gaussian copula. As an application of our joint model, we consider the case of an energy trading company entering into longer-term agreements with wind power producers, where the fluctuating future wind power production is bought at a predetermined fixed price. We find that assuming independence between prices and wind power production leads to an underestimation of risk, as the profit distribution becomes left-skewed when the negative dependence that we find in the data is accounted for. By performing a simple static hedge in the forward market, we show that the risk can be significantly reduced. Furthermore, an out-of-sample study shows that the choice of copula influences the price of correlation risk, and that time-varying copulas are superior to the constant ones when comparing actual profits generated with different models.},
  owner = {sotterson},
  note = {* A copula model is proposed for electricity spot prices and wind power production. * Evidence of time-varying dependence between prices and production is found. * The pricing of contracts exposed to joint price and volumetric risk is investigated. * We show that assuming independence leads to an underestimation of risk. * We find that the choice of copula model influences the price of correlation risk. * 2D conditional copula with Markov dependendence upon previous time step cdf. * Marginal models: seems to be a fixed cdf of the error of a time-variying ARMA-GARCH deterministic prediction},
  timestamp = {2017.05.23},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Pircalabu17priceVolRiskWndTrdCpla.pdf}
}

@article{Dionne15LiquidAdjIntraday,
  title = {Liquidity-Adjusted {{Intraday Value}} at {{Risk}} Modeling and Risk Management: {{An}} Application to Data from {{Deutsche B{\"o}rse}}},
  author = {Dionne, Georges and Pacurar, Maria and Zhou, Xiaozhou},
  year = {2015},
  journal = {Journal of Banking \& Finance},
  volume = {59},
  pages = {202--219},
  issn = {0378-4266},
  doi = {10.1016/j.jbankfin.2015.06.005},
  abstract = {Abstract This paper develops a high-frequency risk measure: the Liquidity-adjusted Intraday Value at Risk (LIVaR). Our objective is to explicitly consider the endogenous liquidity dimension associated with order size. By reconstructing the open Limit Order Book of Deutsche B??rse, changes in the tick-by-tick (ex-ante) frictionless return and actual return are modeled jointly. The risk related to the ex-ante liquidity premium is then quantified. Our model can be used to identify the impact of ex-ante liquidity risk on total risk, and to provide an estimation of the VaR for the actual return at a point in time. In our sample, liquidity risk can account for up to 32\% of total risk depending on order size.},
  note = {Maybe how to model liquidity risk in German Intraday Electricity market trading?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Dionne15LiquidAdjIntraday.pdf}
}

@article{Zilko16mixedDscrtContinCopula,
  title = {Copula in a Multivariate Mixed Discrete -- Continuous Model},
  author = {Zilko, Aurelius A. and Kurowicka, Dorota},
  year = {2016},
  journal = {Computational Statistics \& Data Analysis},
  volume = {103},
  pages = {28--55},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2016.02.017},
  url = {http://www.sciencedirect.com/science/article/pii/S0167947316300895},
  abstract = {Abstract The use of different copula-based models to represent the joint distribution of an eight-dimensional mixed discrete and continuous problem consisting of five discrete and three continuous variables is investigated. The discussion starts with the theoretical properties of the copula-based models. Four different models are constructed for the data collected for the purpose of predicting the length of disruption caused by problems with the train detection system in the Dutch railway network and their performance is tested. The more complex models turn out to represent the data better. Nevertheless, it is shown that the simpler eight dimensional Normal copula still constitutes a statistically sound model for the data.},
  owner = {sotterson},
  note = {A normal copula is a good model for 8-d mixed discrete and continuous inputs! They have a fancier copula that also works.},
  timestamp = {2017.05.23},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zilko16mixedDscrtContinCopula.pdf}
}

@article{Kaur16benefitsSolPowFrcstImbalMkt,
  title = {Benefits of Solar Forecasting for Energy Imbalance Markets},
  author = {Kaur, Amanpreet and Nonnenmacher, Lukas and Pedro, Hugo T.C. and Coimbra, Carlos F.M.},
  year = {2016},
  journal = {Renewable Energy},
  volume = {86},
  pages = {819--830},
  issn = {0960-1481},
  doi = {10.1016/j.renene.2015.09.011},
  url = {http://www.sciencedirect.com/science/article/pii/S0960148115302901},
  abstract = {Abstract Short term electricity trading to balance generation and demand provides an economic opportunity to integrate larger shares of variable renewable energy sources in the power grid. Recently, many regulatory market environments are reorganized to allow short term electricity trading. This study seeks to quantify the benefits of solar forecasting for energy imbalance markets (EIM). State-of-the-art solar forecasts, covering forecast horizons ranging from 24??h to 5??min are proposed and compared against the currently used benchmark models, persistence (P) and smart persistence (SP). The implemented reforecast of numerical weather prediction time series achieves a skill of 14.5\% over the smart persistence model. Using the proposed forecasts for a forecast horizon of up to 75??min for a single 1??MW power plant reduces required flexibility reserves by 21\% and 16.14\%, depending on the allowed trading intervals (5 and 15??min). The probability of an imbalance, caused through wrong market bids from \{PV\} solar plants, can be reduced by 19.65\% and 15.12\% (for 5 and 15??min trading intervals). All \{EIM\} stakeholders benefit from accurate forecasting. Previous estimates on the benefits of EIMs, based on persistence model are conservative. It is shown that the design variables regulating the market time lines, the bidding and the binding schedules, drive the benefits of forecasting.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kaur16benefitsSolPowFrcstImbalMkt.pdf}
}

@article{Pinson07frcstEval,
  title = {Non-Parametric Probabilistic Forecasts of Wind Power: Required Properties and Evaluation},
  author = {Pinson, Pierre and Nielsen, Henrik Aa. and M{\o}ller, Jan K. and Madsen, Henrik and Kariniotakis, George N.},
  year = {2007},
  month = may,
  journal = {Wind Energy},
  volume = {10},
  number = {6},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/we.230},
  pages = {497--516},
  doi = {10.1002/we.230},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/we.230},
  abstract = {Abstract Predictions of wind power production for horizons up to 48--72 h ahead comprise a highly valuable input to the methods for the daily management or trading of wind generation. Today, users of wind power predictions are not only provided with point predictions, which are estimates of the conditional expectation of the wind generation for each look-ahead time, but also with uncertainty estimates given by probabilistic forecasts. In order to avoid assumptions on the shape of predictive distributions, these probabilistic predictions are produced from non-parametric methods, and then take the form of a single or a set of quantile forecasts. The required and desirable properties of such probabilistic forecasts are defined and a framework for their evaluation is proposed. This framework is applied for evaluating the quality of two statistical methods producing full predictive distributions from point predictions of wind power. These distributions are defined by a number of quantile forecasts with nominal proportions spanning the unit interval. The relevance and interest of the introduced evaluation framework are discussed. Copyright {\copyright} 2007 John Wiley \& Sons, Ltd.},
  annotation = {288 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {General purpose metrics for probabilistic forecasts; quantile regression a bit better than adapt. samp? Generally, the metrics are: * sharpness * reliability * resolution But there are a lot of variants in this paper I kind of skimmed it quickly and didn't read much at the end. Explains Interval score, among other things. I've put most of my markups on the technote, but I'm storing this so I can refer to a journal paper instead of tech note. Tech note came from: http://www2.imm.dtu.dk/pubdb/p.php?5024},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Pinson07frcstEval.pdf}
}

@article{Lydia13powCurvModelAdvncd,
  title = {Advanced Algorithms for Wind Turbine Power Curve Modeling},
  author = {Lydia, M. and Selvakumar, A.I. and Kumar, S.S. and Kumar, G.E.P.},
  year = {2013},
  journal = {IEEE Transactions on Sustainable Energy},
  volume = {4},
  number = {3},
  pages = {827--835},
  doi = {10.1109/TSTE.2013.2247641},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6491505},
  abstract = {A wind turbine power curve essentially captures the performance of the wind turbine. The power curve depicts the relationship between the wind speed and output power of the turbine. Modeling of wind turbine power curve aids in performance monitoring of the turbine and also in forecasting of power. This paper presents the development of parametric and nonparametric models of wind turbine power curves. Parametric models of the wind turbine power curve have been developed using four and five parameter logistic expressions. The parameters of these expressions have been solved using advanced algorithms like genetic algorithm (GA), evolutionary programming (EP), particle swarm optimization (PSO), and differential evolution (DE). Nonparametric models have been evolved using algorithms like neural networks, fuzzy c-means clustering, and data mining. The modeling of wind turbine power curve is done using five sets of data; one is a statistically generated set and the others are real-time data sets. The results obtained have been compared using suitable performance metrics and the best method for modeling of the power curve has been obtained.},
  owner = {sotterson},
  annotation = {188 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Simple physical model of power curve, including blade pitch angle. Improved on this w/ machine learning methods. Physical model could be logged to make angle, etc. additive, dep on speed could still be a 1D spline if but in log domain. Could be good for ReWP.},
  timestamp = {2014.11.10},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lydia13powCurvModelAdvncd.pdf}
}

@article{Cutler07rampWPPT,
  title = {Detecting, Categorizing and Forecasting Large Ramps in Wind Farm Power Output Using Meteorological Observations and {{WPPT}}},
  author = {Cutler, Nicholas and Kay, Merlinde and Jacka, Kieran and Nielsen, Torben Skov},
  year = {2007},
  journal = {Wind Energy},
  volume = {10},
  number = {5},
  pages = {453--470},
  publisher = {John Wiley \& Sons, Ltd.},
  issn = {1099-1824},
  doi = {10.1002/we.235},
  abstract = {Abstract 10.1002/we.235.abs The Wind Power Prediction Tool (WPPT) has been installed in Australia for the first time, to forecast the power output from the 65MW Roaring 40s Renewable Energy P/L Woolnorth Bluff Point wind farm. This article analyses the general performance of WPPT as well as its performance during large ramps (swings) in power output. In addition to this, detected large ramps are studied in detail and categorized. WPPT combines wind speed and direction forecasts from the Australian Bureau of Meteorology regional numerical weather prediction model, MesoLAPS, with real-time wind power observations to make hourly forecasts of the wind farm power output. The general performances of MesoLAPS and WPPT are evaluated over 1 year using the root mean square error (RMSE). The errors are significantly lower than for basic benchmark forecasts but higher than for many other WPPT installations, where the site conditions are not as complicated as Woolnorth Bluff Point. Large ramps are considered critical events for a wind power forecast for energy trading as well as managing power system security. A methodology is developed to detect large ramp events in the wind farm power data. Forty-one large ramp events are detected over 1 year and these are categorized according to their predictability by MesoLAPS, the mechanical behaviour of the wind turbine, the power change observed on the grid and the source weather event. During these events, MesoLAPS and WPPT are found to give an RMSE only roughly equivalent to just predicting the mean (climatology forecast). Copyright ?? 2007 John Wiley \& Sons, Ltd.},
  owner = {scot},
  annotation = {85 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Ramp detection heuristic with some tuning. Could use for phase error detection (only detect on ramps) Doesn't really forecast ramps. Just evaluates WPPT performance on them. Also categorizes what happened during ramps.},
  timestamp = {2010.11.23},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Cutler07rampWPPT.pdf}
}

@article{Garnier14blncErrIntraday,
  title = {Balancing Forecast Errors in Continuous-Trade Intraday Markets},
  author = {Garnier, Ernesto and Madlener, Reinhard},
  year = {2014},
  publisher = {FCN Working Paper},
  url = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2463199},
  abstract = {Forecasting the production of photovoltaic (PV) and wind power systems inevitably implies inaccuracies. Therefore, sales made based on forecasts almost always require the vendor to make balancing efforts. In the absence of resources available within their own portfolios, operators can turn towards the intraday market in order to avoid an engagement in the imbalance market with the resulting surcharges and regulatory penalties. In this paper, we combine a novel trade value concept with options valuation and dynamic programming to optimize volume and timing decisions of an individual operator without market power when compensating PV or wind power forecast errors in the market. The model employs a multi-dimensional binomial lattice, with trade value maximized at every node to help formulating bids in view of correlated, uncertain production forecast and price patterns. Inspired by the German electricity market's characteristics, we test the sensitivity of the model's output ? namely trade timing and trade volume ? to changing uncertainty and transaction cost parameters in 50 different setups. It shows that the model effectively outbalances price against volumetric risks. Trades are executed early and with large batch sizes in the case of price volatility. In contrast, increasing forecast error uncertainty leads to trade delays. High transaction costs trigger batch size reductions and ultimately further trade delays. Running 10,000 simulations across ten scenarios, we find that the model translates its flexible trade execution into a competitive advantage vis-?-vis static bidding strategy alternatives. Keywords: Bidding strategy, Production forecast, Renewable energy, Options, Intraday market},
  owner = {sotterson},
  note = {Dynamic programming in intraday trading. Good for Eweline/IRPWIND "use of forecast" work, as it is taylored to German market. Also considers error correlations. * German laws don't allow speculating against balancing power. Recently, enforcement of this rule has gotten stricter * Double trading undesirable because: 1. transaction cost 2. "bid ask spread" for small volumes (people charge more for less?) 3. may buy high and sell low 4. risk of not finding a trade.},
  timestamp = {2015.03.31},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Garnier14blncErrIntraday.pdf}
}

@article{Pinson09ensembProbFrcst,
  title = {Ensemble-Based Probabilistic Forecasting at Horns Rev},
  author = {Pinson, Pierre and Madsen, Henrik},
  year = {2009},
  journal = {Wind Energy},
  volume = {12},
  number = {2},
  pages = {137--155},
  doi = {10.1002/we.309},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/we.309},
  abstract = {For management and trading purposes, information on short-term wind generation (from a few hours to a few days ahead) is crucial at large offshore wind farms, since they concentrate a large capacity at a single location. The most complete information that can be provided today consists of probabilistic forecasts, the resolution of which may be maximized by using meteorological ensemble predictions as input. The paper concentrates on the test case of the Horns Rev wind farm over a period of approximately 1 year, in order to describe, apply and discuss a complete ensemble-based probabilistic forecasting methodology. In a first stage, ensemble forecasts of meteorological variables are converted to power through a suitable power curve model. This model employs local polynomial regression, and is adaptively estimated with an orthogonal fitting method. The obtained ensemble forecasts of wind power are then converted into predictive distributions with an original adaptive kernel dressing method. The shape of the kernels is driven by a mean-variance model, the parameters of which are recursively estimated in order to maximize the overall skill of obtained predictive distributions. Such a methodology has the benefit of yielding predictive distributions that are of increased reliability (in a probabilistic sense) in comparison with the raw ensemble forecasts, at the same time taking advantage of their high resolution.},
  owner = {sotterson},
  annotation = {93 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Pinson09ensembProbFrcst
\par
About NWP ensembles -- wind power quantiles. Malte said the orthogonal interpolation used here might be good for either point or ensemble forecasts.
\par
test biblink: (Dina Genkina, 2024)
\par
\textbf{Some test highlight copies:}
\par
``e others, one may consider the mean of all ensemble members,{\= } xt+k{\textbar}t = 1 m m {$\sum$} j=1 {\textasciicircum} x(j) t+k{\textbar}t, {$\forall$}t, k (2) as the best forecast that can be extracte'' (Pinson and Madsen, 2009, p. 4)
\par
``issues related'' (Pinson and Madsen, 2009, p. 2)},
  timestamp = {2009.03.04},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Pinson09ensembProbFrcst.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Pinson09ensembProbFrcst.html}
}

@techreport{Pape15fndmntlPriceVarDEpowMkt,
  title = {Are Fundamentals Enough? {{Explaining}} Price Variations in the German Day-Ahead and Intraday Power Market},
  author = {Pape, Christian and Weber, Christoph and others},
  year = {2015},
  institution = {{University of Duisburg-Essen, Chair for Management Science and Energy Economics}},
  url = {http://www.econstor.eu/bitstream/10419/113275/1/822013509.pdf},
  abstract = {European electricity market participants are encouraged to balance intraday deviations from their day-ahead schedules via trades in the intraday market. Together with the increasing produc- tion of variable renewable energy sources, the intraday market is gaining importance. We inves- tigate the explanatory power of a fundamental modeling approach explicitly accounting for must- run operations of combined heat and power plants (CHP) and intraday peculiarities such as a shortened intraday supply stack. The fundamental equilibria between every hour?s supply stack and aggregated demand in 2012 and 2013 are modeled to yield hourly price estimates. The major benefits of a fundamental modeling approach are the ability to account for non-linearities in the supply stack and the ability to combine time-varying information consistently. The empirical re- sults show that fundamental modeling explains a considerable share of spot price variance. How- ever, differences between the fundamental and actual prices persist and are explored using re- gression models. The main differences can be attributed to (avoided) start up-costs, market states and trading behavior. Keywords: Intraday market for electricity, fundamental price modeling},
  note = {Useful for DE intraday prob. frcst experiments?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Pape15fndmntlPriceVarDEpowMkt.pdf}
}

@article{Kim22ImperfectForesightDERrenum,
  title = {Impact of Imperfect Foresight on the Optimal {{DER}} Deployment, Remuneration and Policy},
  author = {Kim, Jip and Bialek, Sylwia and {\"U}nel, Bur{\c c}in and Dvorkin, Yury},
  year = {2022},
  month = nov,
  journal = {Applied Energy},
  volume = {326},
  pages = {119885},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2022.119885},
  url = {https://www.sciencedirect.com/science/article/pii/S0306261922011485},
  urldate = {2022-10-04},
  abstract = {This paper proposes a decision-making framework to optimize electricity tariffs and remuneration policy for renewable energy sources operating in transmission- and distribution-level (T\&D) marketplaces. We develop perfect and imperfect foresight models with a multi-level structure to investigate the effects of the inability of actors to correctly predict future remuneration on the efficiency of the decisions made by policymakers. The important advantage of the proposed models is that while they are capturing conflicting interests and different information availability of stakeholders involved, they also account for techno-economic constraints on operations of generation, transmission and distribution assets. The presented models can accommodate net metering, value stack, and distribution locational marginal price (DLMP)-based remuneration policies, which allows for their numerical comparison in terms of the effect on the optimal roll-out of distributed energy resources (DERs). The case study carried out for the NYISO and Manhattan T\&D networks reveals that inaccurate foresight on the DER remuneration diminishes the social welfare under all DER remuneration policies, with the maximum loss of 1.6\% across all considered scenarios.},
  langid = {english},
  annotation = {2 citations (Semantic Scholar/DOI) [2023-07-25]}
}

@inproceedings{Pinson06ProbFrcstPropsEval,
  title = {Properties of Quantile and Interval Forecasts of Wind Generation and Their Evaluation},
  booktitle = {Proceedings of the European Wind Energy Conference \& Exhibition},
  author = {Pinson, P. and Kariniotakis, G. and Nielsen, H. A. and Nielsen, T. S. and Madsen, H.},
  year = {2006},
  address = {Athens},
  abstract = {Either for managing or trading wind power generation, it is recognized today that forecasting is a cornerstone. Traditionally, methods that are developed and implemented are point forecasting methods, i.e. they provide a single estimated value for a given horizon. As errors are unavoidable, several research teams have recently proposed uncertainty estimation methods in order to optimize the decision-making process (reserve quantification, bidding strategy definition, etc.). Here, focus is given to methods that quote quantiles or intervals from predictive distributions of wind generation. The paper describes what the required properties of appropriate uncertainty estimation methods are and how they can be evaluated. Finally, it is shown how the introduced evaluation criteria may be used for highlighting or optimizing the performance of current probabilistic forecasting methodologies. Keywords: Wind power, short-term forecasting, uncertainty estimation, probabilistic forecasting, evaluation methods},
  ncite = {19},
  owner = {sotterson},
  note = {Not super highly cited, but maybe tells you how to make cost functions for tuning forecast methods. Talks about unique skill score. Maybe Brocker07properScore is more thorough?},
  timestamp = {2013.09.26}
}

@article{Pinson07tradeWindProbFrcst,
  title = {Trading Wind Generation from Short-Term Probabilistic Forecasts of Wind Power},
  author = {Pinson, P. and Chevallier, C. and Kariniotakis, G.N.},
  year = {2007},
  month = aug,
  journal = {Power Systems, IEEE Transactions on},
  volume = {22},
  number = {3},
  pages = {1148--1156},
  issn = {0885-8950},
  doi = {10.1109/TPWRS.2007.901117},
  abstract = {Due to the fluctuating nature of the wind resource, a wind power producer participating in a liberalized electricity market is subject to penalties related to regulation costs. Accurate forecasts of wind generation are therefore paramount for reducing such penalties and thus maximizing revenue. Despite the fact that increasing accuracy in spot forecasts may reduce penalties, this paper shows that, if such forecasts are accompanied with information on their uncertainty, i.e., in the form of predictive distributions, then this can be the basis for defining advanced strategies for market participation. Such strategies permit to further increase revenues and thus enhance competitiveness of wind generation compared to other forms of dispatchable generation. This paper formulates a general methodology for deriving optimal bidding strategies based on probabilistic forecasts of wind generation, as well as on modeling of the sensitivity a wind power producer may have to regulation costs. The benefits resulting from the application of these strategies are clearly demonstrated on the test case of the participation of a multi-MW wind farm in the Dutch electricity market over a year.},
  ncite = {190},
  owner = {sotterson},
  annotation = {573 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Quantile forecasts used to optimally offer day ahead wind power so that regulation costs are minimized. A min/max risk averse method is also mentioned but not developed or tested. Finds that optimal bidding doesn't decrease totla balancing energy but reduces cost. Paper argues this is OK b/c asymmetric balancing costs reflect true costs (but later, I think Pierre said that this would increase GHG's ?) Offering schemes * quantile is picked by simple ratio of regulation prices (eq. 8 of Bremnes04windLocQR) -- ratio --{\textquestiondown} tau --{\textquestiondown} pick power at closest tau (or interpolation across quantiles) -- crossover could be a problem, but not as bad if needed density calculated from derivative of a (crossed) quantile forecast. * can also do a min/max risk averse offer but not evaluated -- like TSOs? -- e.g. minimize so don't ever exceed the fraction of reserve allocated to EEG? * also has a scheme for offering when have some storage. Could be useful for IWES virtual power plant? Market model * Nordpool model showed lots of wind depressed spot prices but but from 2002-3, didn't see it. Note that I found that Forecast errors do affect market prices in DE. * Assume bidder can't affect spot or balance prices! * says Unknown reg. cost could be frcsted e.g. from mkt. close price frcst? - In DE, would this also work? - unlikely given comments in Moehrlen12tradeStrtgyRESeeg2012 - Here, assume annual, quarterly forecast is possible (but don't actually test this?) * but don't do that: instead cheat using anticausal yearly or quarterly averages Pierre says this suggests that poor market design can cause bad bidding that increases GHG's Is generalized in Zugno13tradeWindGenMrktQs, where stochastic market prices are used. Tryggvi had a 2013 price forecasting paper (Jonsson13elecPriceQR). Did it use this idea? * Marco had a 2010 paper modeling market feedback (Giabardo10feedbackElecMkt). Would that help this paper RESULTS * 39\% cost reg cost reduction w/ quarterly avg. price forecast. * increase in balance energy, which paper says is OK},
  timestamp = {2009.03.03},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Pinson07tradeWindProbFrcst.pdf}
}

@techreport{Crawford16loadFrcstERCOT,
  title = {{{ERCOT}} Load Forecasting},
  author = {Crawford, Todd},
  year = {2016},
  institution = {The Weather Company},
  url = {https://business.weather.com/resource/ercot-load-forecasting-white-paper},
  abstract = {Good load forecasts are strongly dependent upon good weather forecasts, and The Weather Company?s weather forecasting engine is unsurpassed in that regard. Learn more in this white paper. When a company?s profitability is dependent on weather, accuracy and insight can be critical to success. The Weather Company, an IBM Business (Weather) has recently made significant investments in both (a) an improved weather forecasting system and (b) data science capabilities. The former allows for the most accurate, timely, and spatially resolute weather forecasts in the industry, while expertise in the latter allows us to convert these accurate weather forecasts into user-friendly products for our clients in the utility and energy trading businesses. One of these exciting new products is a load forecasting module for the ERCOT region, with new forecasts produced each hour, at hourly resolution out to 15 days. The Weather Company load forecasting algorithm exhibits errors (expressed with the MAPE metric) of generally less than 2\% (for the aggregate ERCOT region) during the first three days of the forecast, rising to 3-4\% by day 6 and 4-5\% by day 9. A comparison with archived ERCOT-produced load forecasts out to 180 hours indicates that The Weather Company forecasts had lower errors for 98\% of the forecast hours, with relative improvements ranging from 5-20\%.},
  owner = {sotterson},
  note = {IBM's new load forecast for Texas ERCOT uses NWP grid and 2600 NN's. Seems like a CNN and maybe RNN could be better? Goes from 1 hour (?) to 15 days in 1 hour increments. I suppose the huge \# of NN's is partly b/c they have a separate NN per horizon. There does not seem to be a spatial component in the NN's so would miss fronts, etc. But I haven't read this whitepaper yet. Wang16loadFrcstRcncyBigDat thinks that separate models may not be better than single model w/ interactions.},
  timestamp = {2017.03.13}
}

@techreport{Botterud11windPowFrcstOpUse,
  title = {Use of Wind Power Forecasting in Operational Decisions},
  author = {Botterud, Audun and Zhou, Zhi and Wang, Jianhui and Bessa, Ricardo J. and Keko, Hnroje and Mendes, Joana and Sumaili, Jean and Miranda, Vladimiro},
  year = {2011},
  month = sep,
  number = {ANL/DIS-11-8},
  address = {Oak Ridge, TN, USA},
  institution = {Argonne National Laboratories},
  url = {http://www.dis.anl.gov/projects/windpowerforecasting.html},
  abstract = {In this project, we have also developed and successfully tested several methodologies and modeling tools for the use of wind power forecasting in operational decisions, from the perspectives of the system operator as well as the wind power producers. We have investigated how the different objectives of system operators and market participants may lead to different opinions on what constitutes a good wind power forecast, which in turn may influence the training criteria used for wind power point forecasting. We have also focused on the use probabilistic wind power forecasts in electricity markets. We have investigated the representation of wind power forecasting uncertainty in the unit commitment problem. Traditional deterministic unit commitment models use a point forecast for wind power output. In contrast, we have developed a stochastic alternative that represents forecast uncertainty by using scenarios that capture cross-temporal dependencies in the predicted wind power. Furthermore, we have investigated the use of probabilistic wind power forecasts to estimate dynamic operating reserve requirements. We have tested the new algorithms on several case studies on a small-scale hypothetical power system as well as on realistic data for the power system of Illinois. A comparison of a diversity of unit commitment and operating reserve strategies illustrate the potential advantages of using probabilistic forecasts in the scheduling of energy and operating reserves compared to the traditional deterministic approach. We have also developed a model for optimal trading of wind power under uncertainty in wind power and prices. The model has been tested on several case studies on both hypothetical and real-world data. The results show that the model can control the trade-offs between risk and return for wind farm owners depending on their risk preferences. We have also used the model to investigate how market design, in the form of potential deviation penalties, influences optimal bidding decisions, system imbalances, and wind power?s profitability.},
  owner = {sotterson},
  note = {Illinois test case (simulation w/ real data inputs) show benefits of stochastic optimization Journal paper: Botterud13dispUCprobWndFrcst Another Journal paper may be: Wang11windFrcstUncertUC},
  timestamp = {2013.10.01}
}

@article{Foresi95condDistRetDR,
  title = {The Conditional Distribution of Excess Returns: {{An}} Empirical Analysis},
  author = {Foresi, Silverio and Peracchi, Franco},
  year = {1995},
  journal = {Journal of the American Statistical Association},
  volume = {90},
  number = {430},
  pages = {451--466},
  publisher = {Taylor \& Francis},
  url = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1299462},
  abstract = {In this paper we describe the cumulative distribution function of excess returns conditional on a broad set of predictors that summarize the state of the economy. We do so by estimating a sequence of conditional logit models over a grid of values of the response variable. Our method uncovers higher-order multidimensional structure that cannot be found by modeling only the first two moments of the distribution. We compare two approaches to modeling: one based on a conventional linear logit model, the other an additive logit. The second approach avoids the ??Scurse of dimensionality??? problem of fully nonparametric methods while retaining both interpretability and the ability to let the data determine the shape of the relationship between the response variable and the predictors. We find that additive logit fits better and reveals aspects of the data that remain undetected by the linear logit. The additive model retains its superiority even in out-of-sample prediction and portfolio selection performance, suggesting that this model captures genuine features of the data which seem to be important to guide investors" optimal portfolio choices. Keywords: Asset pricing, generalized additive models, nonparametric methods},
  owner = {sotterson},
  keywords = {todo},
  note = {Foresi95condDistRetDR
\par
Distribution regression predicts cdf probabilities instead of quantile dependent variable levels. Has same crossing problems as quantile regression, but may be better for extreme quantiles (if somehow coverted to them). This is the original distribution regression paper. As far as I can tell, it's clearer than the later papers. For ReWP, and other post-bidding operations like redispatch, where a fixed level of power has already been promised, Distributional regression could be more appropriate than quantile regression: you pick a level, and DR tells you the probability that it will be exceeded. This could be used for intraday correction of DA forecasts, or reserve activation. Prediction is done best w/ an additive logistic regression model (on trading test), but it's not clear that logistic regression is necessarily the best method. Anyway crossover is an issue w/ this method, just as w/ quantile regression. DR vs. QR * quantile regression: given a c.d.f. probability, predict the value of the dependent variable * distribution regression: given a value of a dependent variable, predict the c.d.f. probability * Performance (from Koenker13DistributionalvsQuantile) - DR perf. QR perf, but DR might be better in extremes: - both QR and DR have crossover problems (they give up on it in this paper) Predicting the c.d.f. probabilities * given a level threshold, create a binary indicator vector w/ a 1 where the dep. var. is {\textexclamdown}= the threshold * try to predict the indicator using the independent variables, x * prediction done w/ variants of logistic regression - predicts the mean value of indicator - the mean value prediction, not the distribution assumed by logistic regression, is the goal * use many thresholds * Result: for every test, x, you get a set of predicted cdf probs at a constant set of levels * somehow build an ordered c.d.f. out of this (can have crossover) - think you could turn probs into quantiles byr training a model that uses an (x,prob) input to predict the levels * Simple logistic regression details can be found in, for example: Shalizi15datAnalElemViewBk * but I don't think that logistic regression matters: could be an NN if it's using RMSE training * it is not clear to me how the cdf probs are used in their studies, however!! Logistic regression models: 3 types tested 1. linear logistic regression - best on squared prediction error metrics - not best on trading simulation b/c of transaction costs from frequent trading 2. additive logistic ref. model, which can accommodate splines - 2{$^{nd}$} best on pred. error - clearly the best in trading experiment 3. semi-additive L.R. - generally doesn't work Crossover avoidance approaches (in cdf invversion) 1. Do nothing - montonicity not guaranteed - max quantile prob {\textexclamdown}= 1 2. Ordered logit - monotonicity guaranteed - 0 {\textexclamdown}= quantile prob {\textexclamdown}=1 guaranteed - too restrictive, only models location shift (but see location+shift model in Koenker13DistributionalvsQuantile ?) 3. Probability spacing - monotonicity guaranteed - max prob {\textexclamdown}=1 not guaranteed -- Schmidt13noCrossQRspacing does spacing but this works b/c they're spacing quantiles, which aren't bounded 4. Survivor recursion - montonicity guaranteed - 0 {\textexclamdown}= p {\textexclamdown}=1 guaranteed - not interpretable (and hard to estimate?). - but maybe this is no less interpretable than a NN? Their log-odds ratio model * could use projection-persuit logistic regression * but they use a semi-additive model (could be spline smoothing based) * estimated w/ some kind of local scoring Feature selection * heuristic * have 7 of them MY NOTES: Regression function * xx says the threshold error is gaussian? * an NN with an LSQ cost function predicts the conditional mean e.g. https://books.google.de/books?id=U\textsubscript{b}2BwAAQBAJ\&pg=PA5\&lpg=PA5\&dq=neural+network+for+estimating+conditional+mean\&source=bl\&ots=hLZiI0M6Tp\&sig=hljm-efcyj4eiRoXF9KVHzsgZP0\&hl=en\&sa=X\&redir\textsubscript{e}sc=y\#v=onepage\&q=neural\%20network\%20for\%20estimating\%20conditional\%20mean\&f=false
\par
Also see
\par
\begin{itemize}

\item  (Hu et al., 2024)
\item (Marcjasz et al., 2023)

\end{itemize}},
  timestamp = {2015.07.11},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Foresi95condDistRetDR.pdf}
}

@article{Lee14PowerCurveEstimation,
  title = {Power Curve Estimation with Multivariate Environmental Factors for Inland and Offshore Wind Farms},
  author = {Lee, Giwhyun and Ding, Yu and Genton, Marc G and Xie, Le},
  year = {2014},
  journal = {Journal of the American Statistical Association},
  number = {just-accepted},
  pages = {00--00},
  publisher = {Taylor \& Francis},
  doi = {10.1080/01621459.2014.977385},
  abstract = {In the wind industry, a power curve refers to the functional relationship between the power output generated by a wind turbine and the wind speed at the time of power generation. Power curves are used in practice for a number of important tasks including predicting wind power production and assessing a turbine's energy production efficiency. Nevertheless, actual wind power data indicate that the power output is affected by more than just wind speed. Several other environmental factors, such as wind direction, air density, humidity, turbulence intensity, and wind shears, have potential impact. Yet, in industry practice, as well as in the literature, current power curve models primarily consider wind speed and, sometimes, wind speed and direction. We propose an additive multivariate kernel method that can include the aforementioned environmental factors as a new power curve model. Our model provides, conditional on a given environmental condition, both the point estimation and density estimation of power output. It is able to capture the nonlinear relationships between environmental factors and the wind power output, as well as the high-order interaction effects among some of the environmental factors. Using operational data associated with four turbines in an inland wind farm and two turbines in an offshore wind farm, we demonstrate the improvement achieved by our kernel method. Keywords: Additive multivariate kernel regression, Nonparametric estimation, Turbine performance assessment, Wind power forecast},
  owner = {sotterson},
  annotation = {75 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Physical power curve equation used to justify a multiplicative power curve Kernel function. But only generally: the things that are most important, the general nonlinear, multiplicative relationship.... =={\textquestiondown} interaction terms are important. * says ordinary GAM's are unlikely to work well b/c true physical law as multiplicative relationship - so needs interaction terms * so, possibilities are - Bayesian additive regression trees - smoothing spline anova - kernel based methods * They like conditional kernel density (CKD) estimators - this leads to a product kernel, which I guess is the multiplicative relationship they're after * 3 most important terms are - wind speed - direction - air density (explanation for conversion to it from pressure and temp is on p. 11) * 3 possible inputs listed on p. 16 (other than wind speed, direction). Also in Fig 3. - air density - turbulence intensity - wind shear},
  timestamp = {2014.11.14},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lee14PowerCurveEstimation.pdf}
}

@article{Zhao18resPriceEvoImpBass,
  title = {The Evolution of Renewable Energy Price Policies Based on Improved Bass Model: {{A}} System Dynamics ({{SD}}) Analysis},
  author = {Zhao, Xin-gang and Zhang, Yu-zhuo and Li, Yan-bin},
  year = {2018},
  journal = {Sustainability},
  volume = {10},
  number = {6},
  pages = {1748},
  publisher = {Multidisciplinary Digital Publishing Institute},
  url = {https://www.mdpi.com/2071-1050/10/6/1748},
  abstract = {Many countries in the world have implemented many price support policies to promote the development of renewable energy, and there are evolutionary processes between different policies at different stages of national development. Existing literature has less research on the internal mechanism and alternative process of renewable energy price policies' evolution process. In view of this, this paper innovatively introduces the classic model of innovation diffusion theory, the Bass model, into the renewable energy price mechanism, and improves it on the basis of the traditional Bass model, and then proposes a system dynamics (SD) simulation based on the improved Bass model to study the evolution process of the renewable energy price policies. This paper mainly studies the evolution process of the policies from feed-in tariff (FIT) to renewable portfolio standard (RPS), and takes China's wind power industry as an example to simulate the model. The results show that FIT can effectively and quickly evolve to RPS based on the internal influence of the interaction among power generation enterprises and the external influence of government behaviors. All the power generation enterprises will implement RPS, and the amount of green power enterprises eventually grows steadily and slowly. In addition, increasing the decline rate of FIT subsidy and RPS unit fine can effectively promote the evolution of RPS policy, and also improve the amount of green power enterprises and the activity of the tradable green certificates (TGC) trading market. Keywords: renewable energy; feed-in tariff; renewable portfolio standard; bass model; system dynamics},
  note = {Energy price mechanism integrated with traditional Bass model, creating and improved Bass model, it seems. Is used to predict the response of Chinese wind power adoption in response to a feedin tariff. This paper might be a hint about how to modify Bass to other specific uses, e.g. EV adoption forecasting when EV's are being sold, used, and then repurchased.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhao18resPriceEvoImpBass.pdf}
}

@techreport{Monteiro09windFrcstStArt,
  type = {Tech Report},
  title = {Wind Power Forecasting: State-of-the-Art 2009.},
  author = {Monteiro, C and Bessa, R and Miranda, V and Botterud, A and Wang, J and Conzelmann, G and others},
  year = {2009},
  month = nov,
  number = {ANL/DIS-10-1},
  institution = {Argonne National Laboratory (ANL)},
  doi = {DOI 10.2172/968212},
  url = {http://www.osti.gov/energycitations/product.biblio.jsp?osti_id=968212},
  abstract = {Many countries and regions are introducing policies aimed at reducing the environmental footprint from the energy sector and increasing the use of renewable energy. In the United States, a number of initiatives have been taken at the state level, from renewable portfolio standards (RPSs) and renewable energy certificates (RECs), to regional greenhouse gas emission control schemes. Within the U.S. Federal government, new energy and environmental policies and goals are also being crafted, and these are likely to increase the use of renewable energy substantially. The European Union is pursuing implementation of its ambitious 20/20/20 targets, which aim (by 2020) to reduce greenhouse gas emissions by 20\% (as compared to 1990), increase the amount of renewable energy to 20\% of the energy supply, and reduce the overall energy consumption by 20\% through energy efficiency. With the current focus on energy and the environment, efficient integration of renewable energy into the electric power system is becoming increasingly important. In a recent report, the U.S. Department of Energy (DOE) describes a model-based scenario, in which wind energy provides 20\% of the U.S. electricity demand in 2030. The report discusses a set of technical and economic challenges that have to be overcome for this scenario to unfold. In Europe, several countries already have a high penetration of wind power (i.e., in the range of 7 to 20\% of electricity consumption in countries such as Germany, Spain, Portugal, and Denmark). The rapid growth in installed wind power capacity is expected to continue in the United States as well as in Europe. A large-scale introduction of wind power causes a number of challenges for electricity market and power system operators who will have to deal with the variability and uncertainty in wind power generation when making their scheduling and dispatch decisions. Wind power forecasting (WPF) is frequently identified as an important tool to address the variability and uncertainty in wind power and to more efficiently operate power systems with large wind power penetrations. Moreover, in a market environment, the wind power contribution to the generation portofolio becomes important in determining the daily and hourly prices, as variations in the estimated wind power will influence the clearing prices for both energy and operating reserves. With the increasing penetration of wind power, WPF is quickly becoming an important topic for the electric power industry. System operators (SOs), generating companies (GENCOs), and regulators all support efforts to develop better, more reliable and accurate forecasting models. Wind farm owners and operators also benefit from better wind power prediction to support competitive participation in electricity markets against more stable and dispatchable energy sources. In general, WPF can be used for a number of purposes, such as: generation and transmission maintenance planning, determination of operating reserve requirements, unit commitment, economic dispatch, energy storage optimization (e.g., pumped hydro storage), and energy trading. The objective of this report is to review and analyze state-of-the-art WPF models and their application to power systems operations. We first give a detailed description of the methodologies underlying state-of-the-art WPF models. We then look at how WPF can be integrated into power system operations, with specific focus on the unit commitment problem. Subject:17 WIND ENERGY; 24 POWER TRANSMISSION AND DISTRIBUTION; 29 ENERGY PLANNING, POLICY AND ECONOMY; ECONOMICS; ELECTRIC POWER INDUSTRY; ELECTRICITY; ENERGY CONSUMPTION; ENERGY EFFICIENCY; ENERGY SOURCES; ENERGY STORAGE; FORECASTING; GREENHOUSE GASES; IMPLEMENTATION; MAINTENANCE; MARKET; NATIONAL GOVERNMENT; OPTIMIZATION; PLANNING; POWER SYSTEMS; STORAGE; WIND POWER; WIND TURBINE ARRAYS},
  owner = {sotterson},
  note = {A big section on regime learning/detection and the effect of regimes on forecast errors. I think the forecasting test case of Alberta is mentioned. Wasn't this the contest where 3TIER lost badly? Says that Bremnes says that there's no big diffreence between prob wind forecast algs, Nadaraya-Watson estimator has the advantage that it's simple and easy to implement.},
  timestamp = {2013.03.13},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Monteiro09windFrcstStArt.pdf}
}

@article{Giabardo10feedbackElecMkt,
  title = {Feedback, Competition and Stochasticity in a Day Ahead Electricity Market},
  author = {Giabardo, Paolo and Zugno, Marco and Pinson, Pierre and Madsen, Henrik},
  year = {2010},
  journal = {Energy Economics},
  volume = {32},
  number = {2},
  pages = {292--301},
  issn = {0140-9883},
  doi = {10.1016/j.eneco.2009.09.006},
  url = {http://www.sciencedirect.com/science/article/pii/S0140988309001625},
  abstract = {Major recent changes in electricity markets relate to the process for their deregulation, along with increasing participation of renewable (stochastic) generation e.g. wind power. Our general objective is to model how feedback, competition and stochasticity (on the production side) interact in electricity markets, and eventually assess what their effects are on both the participants and the society. For this, day ahead electricity markets are modeled as dynamic closed loop systems, in which the feedback signal is the market price. In parallel, the Cournot competition model is considered. Mixed portfolios with significant share of renewable energy are based on stochastic threshold cost functions. Regarding trading strategies, it is assumed that generators are looking at optimizing their individual profits. The point of view of the society is addressed by analyzing market behavior and stability. The performed simulations show the beneficial effects of employing long term bidding strategies for both generators and society. Sensitivity analyses are performed in order to evaluate the effects of demand elasticity. It is shown that increase in demand elasticity reduces the possibility for the generators to exploit their market power. Furthermore, the results suggest that introduction of wind power generation in the market is beneficial both for the generators and the society.},
  owner = {sotterson},
  annotation = {26 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {A market model the simulates the feedback loop where energy sold effects price, which effects the amount sold,.... Tryggvi had a 2013 price forecasting paper (Jonsson13elecPriceQR). Did it use this idea?},
  timestamp = {2015.03.12},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Giabardo10feedbackElecMkt.pdf}
}

@inproceedings{Sack12windEns2ProbFrcst,
  title = {From {{NWP}} Ensembles to Probabilistic Wind Energy Production Forecasts},
  booktitle = {{{EWEA}}},
  author = {Sack, J. and Bremen, L. and Kyrianzis, A. and Donelly, R.},
  year = {2012},
  abstract = {Probabilistic wind power forecast has gained increasing importance in the recent years. The advantages of probabilistic information, for the integration of wind power to the electrical network, are discussed in numerous publications which mainly address advantages in energy trading and electricity network regulation. The value of the probabilistic wind power forecast is based on the spread of wind power forecast members originating either from a number of NWP models or ensem- ble products of global models (such as ECMWF, UKMO, Meteo France, NCEP and others). Wind power members provide an estimate of the expected uncertainty of power predictions. However, calibration is needed for reliable probabilistic information that is consistent with observations. The paper addresses the development and assesses the quality of the 100m ECMWF EPS (ensemble prediction system winds) which were introduced by ECMWF beginning of 2010 in two operational wind farms (onshore and onshore). In addition, the paper compares differerent calibration method- ologies of Ensembles (in wind and in wind power mode) and addresses the implications for the operational environment. Results show that the probabilistic calibration methodologies examined in wind power mode success- fully optimise the quantiles ranging from 10\% to 90\% with some outliers outside of these limits, mainly due to limited amount of available observational data. The paper compares the different approaches by means of resolution, sharpness, reliability and skill score. We address the training window of the calibration methodologies in the operational forecast environment and show that sim- ple models x yield equivalent accurate results in the operational forecast environment with training periods of less than a month compared with more complex models with longer training periods. Ensembles that have been calibrated in wind mode ((u,v) components at the same time) are still well calibrated when transformed into wind power and show good improvements in probabilistic skill compared to the raw ensemble. The benefit from calibration is less when verification is done with observed wind power instead of using simulated wind power for Germany. The spread of the calibrated ensemble is underestimated with respect to real observations mostly because},
  ncite = {0},
  owner = {sotterson},
  note = {NWP ensembles --{\textquestiondown} wind power quantiles},
  timestamp = {2013.09.26}
}

@article{Chen11Online24hSolFrcstWeathTyp,
  title = {Online 24-h Solar Power Forecasting Based on Weather Type Classification Using Artificial Neural Network},
  author = {Chen, Changsong and Duan, Shanxu and Cai, Tao and Liu, Bangyin},
  year = {2011},
  journal = {Solar Energy},
  volume = {85},
  number = {11},
  pages = {2856--2870},
  publisher = {Elsevier},
  url = {http://www.sciencedirect.com/science/article/pii/S0038092X11003008},
  abstract = {Power forecasting is an important factor for planning the operations of photovoltaic (PV) system. This paper presents an advanced statistical method for solar power forecasting based on artificial intelligence techniques. The method requires as input past power measurements and meteorological forecasts of solar irradiance, relative humidity and temperature at the site of the photovoltaic power system. A self-organized map (SOM) is trained to classify the local weather type of 24 h ahead provided by the online meteorological services. A unique feature of the method is that following a preliminary weather type classification, the neural networks can be well trained to improve the forecast accuracy. The proposed method is suitable for operational planning of transmission system operator, i.e. forecasting horizon of 24 h ahead and for PV power system operators trading in electricity markets. Application of the forecasting method on the power production of an actual PV power system shows the validity of the method. Keywords: Power forecasting; Solar power; Neural network; Weather type; Photovoltaic power system},
  owner = {sotterson},
  note = {Unsupervised SOM weather categories are fed into a day-ahead PV forecast system. NN regression, which is adaptive.},
  timestamp = {2015.02.19}
}

@article{Pinson09skillEnsWndPow,
  title = {Skill Forecasting from Ensemble Predictions of Wind Power},
  author = {Pinson, Pierre and Nielsen, H Aa and Madsen, Henrik and Kariniotakis, Georges},
  year = {2009},
  journal = {Applied Energy},
  volume = {86},
  number = {7},
  pages = {1326--1334},
  publisher = {Elsevier},
  url = {http://www.sciencedirect.com/science/article/pii/S0306261908002602},
  abstract = {Optimal management and trading of wind generation calls for the providing of uncertainty estimates along with the commonly provided short-term wind power point predictions. Alternative approaches for the use of probabilistic forecasting are introduced. More precisely, focus is given to prediction risk indices aiming to give a comprehensive signal on the expected level of forecast uncertainty. Ensemble predictions of wind generation are used as input. A proposal for the definition of prediction risk indices is given. Such skill forecasts are based on the spread of ensemble forecasts (i.e. a set of alternative scenarios for the coming period) for a single prediction horizon or over a look-ahead period. It is shown on the test case of a Danish offshore wind farm how these prediction risk indices may be related to several levels of forecast uncertainty (and potential energy imbalances). Wind power ensemble predictions are derived from the conversion of ECMWF and NCEP ensemble forecasts of meteorological variables to wind power ensemble forecasts, as well as by a lagged average approach alternative. The ability of prediction risk indices calculated from the various types of ensembles forecasts to resolve among situations with different levels of uncertainty is discussed.},
  owner = {sotterson},
  note = {Instantaneous ensemble spread not useful, but can use it over time (I think it was said in Geibel05*, which referenced a much older paper of Pierre's on the same topic as this one). That old one was on poor man's ensembles (I think Gregor said), and this one is on real ensembles (I think).},
  timestamp = {2014.07.09}
}

@techreport{Brauns14RegelWindLeist,
  title = {Regelenergie Durch Windkraftanlagen: {{Abschlussbericht}}},
  author = {Brauns, Steffen and Stobrawe, Markus and Jansen, Malte and Bohlen, Werner and Jost, Dominik and Erdmann, Eike and Siefert, Malte and Just, Ren{\'e} and Speckmann, Markus and Netzel, Niklas and Widdel, Martin},
  year = {2014},
  month = mar,
  institution = {Fraunhofer IWES},
  abstract = {Objectives Due to the forecast inaccuracy and Dargebotsabh?ngigkeit the wind energy is unclear, as the quotation and the verification at the Regelleistungsbe- provision can be done by wind turbines, because the previous supplier of control power are not faced with these uncertainties. Therefore, it was to develop the overarching goal of the project, a quotation and a detection method for the control power supply through wind turbines and related ICT solutions. Quotation ? Using probabilistic forecasts for the wind supply wind can park pools control power with the same reliability as current provider to offer. ? In the project different offer strategies for a day before auction and a sub-tags auction with a lead time of one hour have been developed and investigated the associated potential. ? show potential studies that a single wind farm can offer virtually no control power at a day before the auction. In the investigated wind park pool with a size of 1 GW and 30 GW on the other hand results in an appreciable range potential that is performance at 30 GW wind park pool by far the largest in terms of nominal. * In the case of a sub-tags auction result for 1 GW and 30 GW wind park pools in relation to the nominal power on the other hand almost equal appropriateness botspotenziale. This calls for the introduction of an energy price market with a sub-tags auction, since this would allow, in contrast to a previous day auction, many vendors with wind farm pools participation. * Further, the potential are increased by the pooling of controllable systems. * A year-round supply of control power by However, wind farm pools is not possible, since there are phases in which the wind volume is not sufficient. detection methods * In the project the two detection methods "schedule" and "potential supply" were defined and criteria based compared in order to identify the best of overall system view method. * In the process of "potential supply" the provision of spect is relative to potential supply. The possible supply of a wind farm is the power that would have produced the wind farm, if he had not been governed{\textbar}. Summary * In the process of "roadmap" the rule of performance is relative to a previously reported schedule, in turn the result of trading activities in the energy market is. This method is currently used for the provision of balancing power in Germany. In the case of wind energy de would the timetable based on a probabilistic forecast can be created, which the wind park pool allows the feed with the indication of a certain reliability to predict.not * The tested under the project method for the determination of the possible supply have a sufficient rule for the service provision accuracy. Future research activities and Ausgleichsef- effects through the pooling of wind farms, however, have a sufficiently accurate determination of the possible feeding hope. The detection method in the process "roadmap" has already speed over a sufficient accuracy. * As the partner weight the criteria differently, recommend the Fraunhofer IWES, energy source and ENERCON the process "possible infeed" and Amprion and Tennet TSO the process "roadmap". The recommendation of the process "potential supply" presupposes, however, that this is technically also accurate enough to implement. ICT solutions and field test results * In field tests it was shown that the control of wind farms with a three-second clocking possible is. * The results show that the wind farm control in the case of minute reserve is already sufficiently accurate to wind farm level, ignoring the inaccuracy of the determination of the possible supply. This does not apply in the case of primary and secondary control, but can probably be achieved structurally through a pooling of several wind parks and the optimization of regulatory{\textbar}.},
  owner = {sotterson},
  note = {1\textsuperscript{s}\textsuperscript{t} Tech Report for regulation of wind turbines Follow-on project ReWP: IWES14totProjDescReWPR},
  timestamp = {2014.11.05}
}

@article{Notton19irradFrcst3comp,
  title = {Forecasting of {{Three Components}} of {{Solar}} Irradiation for {{Building Applications}}},
  author = {Notton, Gilles and Voyant, Cyril and Fouilloy, Alexis and Duchaud, Jean Laurent and Nivet, Marie Laure},
  editor = {Tanabe, S.I and Zhang, H. and Kurnitski, J. and {Gameiro da Silva}, M.C. and Nastase, I. and Wargocki, P. and Cao, G. and Mazzarela, L. and Inard, C.},
  year = {2019},
  journal = {E3S Web of Conferences},
  volume = {111},
  pages = {05012},
  issn = {2267-1242},
  doi = {10.1051/e3sconf/201911105012},
  abstract = {Solar energy and the concept of passive architecture and Net Zero Energy buildings are being increased. For an optimal management of the building energy, a Model Predictive Control is generally used but requires an accurate building model and weather forecast. For a more reliable modelling, the knowledge of the global solar irradiation is not sufficient; three methods, smart persistence, artificial neural network and random forest, are compared to forecast the three components of solar irradiation measured on the site with a high meteorological variability. Hourly solar irradiations are forecasted for time horizons from h+1 to h+6. The random forest method (RF) is the most efficient and the accuracy of forecasts are in term of nRMSE, from 19.65\% for h+1 to 27.78\% for h+6 for global horizontal irradiation, from 34.11\% for h+1 to 49.08\% for h+6 for beam normal irradiation, from 35.08\% for h+1 to 49.14\% for h+6 for diffuse horizontal irradiation. The improvement brought by the use of RF compared to the two other methods increases with the forecasting horizon. A seasonal study is realized and shows that the forecasting during spring and autumn is less reliable than during winter and summer due to a higher meteorological variability.},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {The smart persistence used here is a similar idea to the clear sky power normlization in Bacher09pvFrcstOnlineClrSky, where stationarity-assuing classical regression, instead of persistence" is applied to clearsky-normalized power},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\papers\Notton19solarIrradFrcst3comps.pdf}
}

@inproceedings{Juban07ProbWndFrcstFeatSelKDE,
  title = {Probabilistic Short-Term Wind Power Forecasting Based on Kernel Density Estimators},
  booktitle = {Proceedings, European Wind Energy Conference and Exhibition, {{EWEC}} 2007},
  author = {Juban, J{\'e}r{\'e}mie and Fugon, Lionel and Kariniotakis, Georges and others},
  year = {2007},
  url = {http://hal.archives-ouvertes.fr/hal-00526011/},
  abstract = {Short-term wind power forecasting tools have been developed for some time. The majority of such tools usually provide single-valued (spot) predictions. Such predictions are however often not adequate when the aim is decision-making under uncertainty. In that case there is a clear requirement by end-users to have additional information on the uncertainty of the predictions for performing efficiently functions such as reserves estimation, unit commitment, trading in electricity markets, a.o. In this paper, we propose a method for producing the complete predictive probability density function (PDF) for each time step of the prediction horizon based on the kernel density estimation technique. The performance of the proposed approach is demonstrated using real data from several wind farms. Comparisons to state-of-the-art methods from both outside and inside the wind power forecasting community are presented illustrating the performances of the proposed method.},
  owner = {sotterson},
  note = {Select short term wind power features with mutual information. Then does Nadaraya -Watson KDE quantile forecast. Features other than the usual speed/dir were tested. Interesting b/c * featsel is based on mutual information * multivariate kernel distribution is based on matrix inversion smoothing, something like covariance matrix smoothing, I guess. This is different and maybe better than Bessa12adaptQuantCopulaFrcst \& others, which do multivariate KDE with a product kernel.},
  timestamp = {2014.11.10},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Juban07ProbWndFrcstFeatSelKDE.pdf}
}

@article{Zhou11conjFrcstMkts,
  title = {Short-Term Congestion Forecasting in Wholesale Power Markets},
  author = {Zhou, Q. and Tesfatsion, L. and Liu, C. C.},
  year = {2011},
  month = nov,
  journal = {IEEE Transactions on Power Systems},
  volume = {26},
  number = {4},
  pages = {2185--2196},
  issn = {0885-8950},
  doi = {10.1109/TPWRS.2011.2123118},
  abstract = {Short-term congestion forecasting is highly important for market participants in wholesale power markets that use locational marginal prices (LMPs) to manage congestion. Accurate congestion forecasting facilitates market traders in bidding and trading activities and assists market operators in system planning. This study proposes a new short-term forecasting algorithm for congestion, LMPs, and other power system variables based on the concept of system patterns - combinations of status flags for generating units and transmission lines. The advantage of this algorithm relative to standard statistical forecasting methods is that structural aspects underlying power market operations are exploited to reduce forecast error. The advantage relative to previously proposed structural forecasting methods is that data requirements are substantially reduced. Forecasting results based on a NYISO case study demonstrate the feasibility and accuracy of the proposed algorithm.},
  annotation = {66 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhou11conjFrcstMkts.pdf}
}

@article{Pinson09ProbabilisticForecastsStatistical,
  title = {From Probabilistic Forecasts to Statistical Scenarios of Short-Term Wind Power Production},
  author = {Pinson, P. and Papaefthymiou, G. and Kl{\"o}ckl, B. and Nielsen, H. {\relax Aa}. and Madsen, H.},
  year = {2009},
  journal = {Wind Energy},
  volume = {12},
  number = {1},
  pages = {51--62},
  doi = {10.1002/we.284},
  abstract = {Short-term (up to 2-3 days ahead) probabilistic forecasts of wind power provide forecast users with a highly valuable information on the uncertainty of expected wind generation. Whatever the type of these probabilistic forecasts, they are produced on a per horizon basis, and hence do not inform on the development of the forecast uncertainty through forecast series. However, this additional informationmay be paramount for a large class of time-dependent and multi-stage decision-making problems e.g. optimal operation of combined wind-storage systems or multiple-market trading with different gate closures. This issue is addressed here by describing a method that permits the generation of statistical scenarios of short-term wind generation that accounts for both the interdependence structure of prediction errors and the predictive distributions of wind power production. The method is based on the conversion of series of prediction errors to a multivariate Gaussian random variable, the interdependence structure of which can then be summarized by a unique covariance matrix. Such matrix is recursively estimated in order to accommodate long-term variations in the prediction error characteristics. The quality and interest of the methodology are demonstrated with an application to the test case of a multi-MW wind farm over a period of more than two years.},
  owner = {sotterson},
  annotation = {483 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {May be basis for new DTU ramp forecasting Preprint (attached) has easier to see colored graphs. I have the .tex file for the preprint too. * Assumes that Gaussianizing RV's with a scalar transform makes their joint distribution a multi-variate Gaussian, which is NOT TRUE in general. See Wu10gaussMargNotJoint. * Use true multivariate Gaussianization (see energytop.org)? * Pierre says this is a copula approach. But I thought correlations were done rank space, and the correlations here are done after the uniform dist. has been converted to gaussian (what space is that?) . On the other hand, p. 19 and Eq. 10, p. l0 of Schmidt07copulasCope seems to indicate that you do indeed xfer to Gaussian before doing the correlation. compared against NWP ensembles in Girard10trajEvalWind (ensembles seem better?) * Used in: Soares17distGridMgmtPowFlw * expanded to spatial covariance in Tastu15spcTimeTrajGaussCpla},
  timestamp = {2009.04.17},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Pinson09probFrcstStatScenWind.pdf}
}

@inproceedings{Shenoy15StochOptPowMktRgrssn,
  title = {Stochastic Optimization of Power Market Forecast Using Non-Parametric Regression Models},
  booktitle = {{{IEEE POWER}} \& {{ENERGY SOCIETY GENERAL MEETING}}},
  author = {Shenoy, Saahil and Gorinevsky, Dimitry},
  year = {2015},
  month = jul,
  address = {Denver, CO},
  url = {http://web.stanford.edu/~gorin/papers/PES15_QRopt.pdf},
  abstract = {The paper considers stochastic optimization of the electricity procurement in the day-ahead power market. The novelty is in addressing the random errors of time series forecasting of electrical power loads and prices in the procurement. This problem is currently important because of the increased random variability in the power grid that is caused by growing integration of renewable generation. This paper presents a methodology for stochastic optimization using data-driven models. We consider non-parametric models of multivariate distributions based on multiple quantile regressions, built from historical data sets. The statistics, such as cost expectation, required for the stochastic optimization are computed numerically using these models. Applying the methodology to utility data shows that 2\% improvement of the costs is feasible.},
  owner = {sotterson},
  note = {Day ahead power market bidding optimization using trained forecast error distributions. Saved 2\%. Maybe good for German TSO trading (intraday?)? Forecasting method is here: Shenoy15stochOptLdFrcstRgrssn (I think)},
  timestamp = {2015.04.17},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Shenoy15StochOptPowMktRgrssn.pdf}
}

@article{Moehrlen12tradeStrtgyRESeeg2012,
  title = {Investigation of Various Trading Strategies for Wind and Solar Power Developed for the New {{EEG}} 2012 Law},
  author = {M{\"o}hrlen, Corinna and Pahlow, Markus and J{\o}rgensen, Jess U},
  year = {2012},
  journal = {Zeitschrift fur Energiewirtschaft},
  volume = {36},
  number = {1},
  pages = {9},
  url = {http://download.weprog.com/WEPROG_Trading_strategies_EEG2012_ZEFE_71-2012-01_en.pdf},
  abstract = {The new EEG 2012 law opens up for more parties to participate in the trading of wind and solar power, because of the bonus system that now compensates everybody for all market relevant costs, not only the Transmission System Operators. Therefore it can be expected, that the trading of renewable energies by private parties will increase. One of the central questions to be answered is how efficient does a balance responsible party have to be to stay competitive also with a small pool. The quantification of balance costs for different trading strategies is however complex and non-trivial. We propose a methodology in this study that accounts for this fact. Additionally, we analyse and show the requirement and the monetary value of Intra-Day trading for the handling of wind and solar power. The trading strategies proposed in this article make use of an uncertainty band around the forecasts used in the Intra-Day in order to avoid double trading and thereby reducing the total balancing volume and the associated costs.},
  owner = {sotterson},
  note = {Argument that should use probabilistic forecasts on DE intraday markets after EEG 2012. * TSO's have small trading costs, but later they want to avoid double trading (b/c of costs or b/c of risk of buy high, sell low?) * "spot market" 24 hrs ahead * intraday "shorter times," but I found that there were bids w/ horizons over 30 hrs * No incentive or rqt. to increase EEG wind plant operational efficiency, (since get fixed price, but wouldn't you still want to sell more?) * tests done w/ WEPROG 75 member ensembles made at 48 hr. horizon. -- says ensembles provide decent error coverage up to 18 hrs ahead (no test, though. Seems that QR would be better then?) -- the prob forecasts can be used to improve point forecasts * their upscaling algorithm used an Ensemble Kalman Filter (see the paper) -- this iEnKF has 3D feedback from measurements at different locations -- sounds like it's a wind speed correcter, not a power correcter * Must use ensemble forecasts for good extreme event performance. Stat methods (QR?) don't work. Market properties * lose money by trading wind energy on intraday market * reason: more expensive to buy intraday error correction power than to trade on a bias-free day-ahead forecast -- so DA forecast lack of bias is important! -- Matthias Stark complained about the bias in our forecast, said it should be MAE, but that's not necessary -- should probably do adaptive bias correction on DA RMSE-optimized forecasts, at the least * claim that multiple trades somehow activates extra control power (WHY?) * Balancing power prices are extremely difficult to forecast Market simulation * they simulated trading for all of DE * simulated up to 13 hours ahead in intraday trading * did hourly intraday (not 15 min periods) * did six combos of trading w/ different data * didn't simulate reserve activation cost. This seems wrong. Trading algorithm: * only fix the error that the quantiles say is outside of confidence interval * only fix it enough to bring it just inside the interval (I think) * Spread-based uncertainty estimation -- estimation from correlation of ensemble spread and forecast error -- not quantile regression, so should be able to do better than this * seems like intraday forecast correction equation (6) is wrong, is missing sum of prev. corrections * don't trade if forecast error estimated to be {\textexclamdown} 2\% of installed cap. * price forecast: -- didn't use one -- couldn't predict price from error volumes, especially failed to predict large errors. -- note that Pinson07tradeWindProbFrcst assumes you \textsubscript{c}an\textsubscript{f}orecast this but doesn't prove/demonstrate that this is true RESULTS * alg makes the least number of corrections, and they are small * seems to work (I didn't read this too carefully)},
  timestamp = {2015.04.30},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Moehrlen12tradeStrtgyRESeeg2012.pdf}
}

@article{Barski14qntHdgShrtfllCtl,
  title = {On the Shortfall Risk Control-a Refinement of the Quantile Hedging Method},
  author = {Barski, Micha{\l}},
  year = {2014},
  journal = {arXiv preprint arXiv:1402.3725},
  eprint = {1402.3725},
  url = {http://arxiv.org/abs/1402.3725},
  abstract = {The issue of constructing a risk minimizing hedge with additional constraints on the shortfall risk is examined. Several classical risk minimizing problems have been adapted to the new setting and solved. The existence and specific forms of optimal strategies in a general semimartingale market model with the use of conditional statistical tests have been proven. The quantile hedging method applied in [4] and [5] as well as the classical Neyman-Pearson lemma have been generalized. Optimal hedging strategies with shortfall constraints in the Black-Scholes and exponential Poisson model have been explicitly determined.},
  archiveprefix = {arXiv},
  owner = {sotterson},
  annotation = {3 citations (Semantic Scholar/arXiv) [2023-07-25]},
  note = {How to hedge using quantiles with contraints on shortfall. Maybe related to ReWP interday and intraday offer stochastic optimization? Note that traditional quantile hedging -- while optimal in the long term -- can put you out of business in the short-term via shortfalls. This technique is designed to avoid that, but I'm not sure if it's exactly related to power trading.},
  timestamp = {2015.01.26},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Barski14qntHdgShrtfllCtl.pdf}
}

@article{Braun18priceSenseFeatsDEmkt,
  title = {Price Sensitivity of Hourly Day-Ahead and Quarter-Hourly Intraday Auctions in Germany},
  author = {Braun, Sebastian M. and Brunner, Christoph},
  year = {2018-04-18, 2018-04},
  journal = {Zeitschrift f{\"u}r Energiewirtschaft},
  pages = {1},
  publisher = {Springer},
  issn = {1866-2765},
  doi = {10.1007/s12398-018-0228-0},
  url = {http://dx.doi.org/10.1007/s12398-018-0228-0},
  abstract = {The introduction of the quarter-hourly intraday auction in 2014 for the German market confirms a tendency towards short-term energy markets. The reason for the new market was the need to trade shorter periods than just hours a day-ahead to minimize open positions in the more volatile continuous intraday trading. The increased production capacity of solar power boosted this requirement for new short-term power products. The quarter-hourly market shows a distinctive zigzag price formation. We identify two influencing factors: first, the solar residual that combines the trading of solar power ramps around midday as well as the gradients of consumption and thermal power plant ramps throughout the course of the day, and second, a characteristic two stage market design with higher liquidity for the hourly than for the quarter-hourly auction. Therefore, demand, solar generation and inflexible ramps of thermal power plants are hedged at the hourly day-ahead auction and use the quarter-hourly auction only to balance the remaining differences. To prove this argument the price sensitivities of the hourly day-ahead and quarter-hourly intraday auctions in Germany are compared based on actual bid and ask curves from 2015 and 2016. Finally, the development of an adequate design of future spot markets is discussed.ZusammenfassungDie Einf{\"u}hrung der viertelst{\"u}ndlichen Intraday-Auktion im Jahre 2014 f{\"u}r den deutschen Strommarkt untermauert die Bedeutung der kurzfristigen Energiem{\"a}rkte. Der Grund f{\"u}r die Einf{\"u}hrung war der Bedarf, bereits vort{\"a}gig, viertelst{\"u}ndliche Mengen auszugleichen und offene Positionen im volatilen kontinuierlichen Intraday-Handel zu minimieren. Die gestiegene Stromproduktion aus Solarenergie verst{\"a}rkte die Nachfrage nach neuen kurzfristigen Produkten. Der Viertelstundenmarkt zeigt eine ausgepr{\"a}gte Zickzack-Preisbildung. Wir identifizieren zwei Einflussfaktoren: erstens, den Handel von Solarrampen um die Mittagszeit, sowie die Gradienten von Nachfrage und thermischer Erzeugung im Laufe des Tages und zweitens, ein charakteristisches zweistufiges Marktdesign mit h{\"o}herer Liquidit{\"a}t an den st{\"u}ndlichen als auf der viertelst{\"u}ndlichen Auktion. Die unterschiedliche Liquidit{\"a}t l{\"a}sst darauf schlie{\ss}en, dass bei der st{\"u}ndlichen Day-Ahead-Auktion die solare Erzeugung und die unflexiblen Rampen thermischer Kraftwerke abgesichert und die viertelst{\"u}ndliche Auktion genutzt, um die verbleibenden Differenzen auszugleichen. Um diese Aussage argumentativ zu verifizieren, werden die Preissensitivit{\"a}ten der st{\"u}ndlichen Day-Ahead- und viertelst{\"u}ndlichen Intraday-Auktionen in Deutschland anhand der tats{\"a}chlichen Angebots- und Nachfragekurven von 2015 und 2016 verglichen. Abschlie{\ss}end wird die Entwicklung einer ad{\"a}quaten Gestaltung zuk{\"u}nftiger Spotm{\"a}rkte diskutiert.},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Another measure of liquidity paper on German market. ID's features that affect the market.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Braun18priceSenseFeatsDEmkt.pdf}
}

@inproceedings{Moller13windFrcstSDE,
  title = {Probabilistic Forecasts of Wind Power Generation by Stochastic Differential Equation Models},
  booktitle = {59{{{\textsuperscript{th}}}} {{ISI}} World Statistics Congress},
  author = {M{\o}ller, Jan Kloppenborg and Pinson, Pierre and Madsen, Henrik},
  year = {2013},
  month = aug,
  address = {Hong Kong, China},
  url = {http://www.statistics.gov.hk/wsc/STS019-P5-A.pdf},
  abstract = {The increasing production of renewable energy, and in particular wind energy, introduces highly volatile sources of energy in the total production. This implies that methods for reliable probabilistic forecasts of future wind power production are essential. Today there exist numerous methods and tools for providing point forecasts of wind power generation. However, for efficient and safe regulation, and for harvesting optimal trading strategies reliable information on the uncertainty is also needed. In this paper we focus on forecasts on the 1-48 hour horizon. It is well-known that the form of the conditional density for the wind power production is highly dependent on the level of predicted wind power in addition to the prediction horizon. This paper describes a new approach for wind power forecasting based on state dependent stochastic differential equations (SDEs). Specifically we will use a logistic type stochastic differential equation to account for the natural restrictions (wind power cannot exceed installed capacity and cannot be below zero). The SDE is driven by a widely used point predictor for wind power forecast, and the SDE formulation allows us to calculate both state dependent conditional uncertainties as well as correlation structures. Evaluation and optimization of the model is obtained by evaluating the likelihood of a 48-dimensional random vector when accounting for the correlation structure defined by the SDE-formulation. We explore the correlation parameters and skewness of the model and input-variables (prediction horizon and point predictions), by a non-parametric (spline based) model for the parameters. Keywords: nonlinear forecasting, state space model, stochastic differential equations, wind power prediction.},
  owner = {sotterson},
  note = {SDE's used for wind power forecasting using SDE's. It's done for solar in: Iversen13solarFrcstSDE},
  timestamp = {2014.02.04},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Moller13windFrcstSDE.pdf}
}

@phdthesis{Martin17intrdyMktThesis,
  title = {A Limit Order Book Model for the German Intraday Electricity Market},
  author = {Martin, Henry},
  year = {2017},
  abstract = {The importance of the German intraday electricity market increased significantly over the past years, however there is limited research about intraday market models in Germany. In this thesis we examine the possibility of applying limit order book market models designed for stock markets to the German intraday electricity market. We find that the general approach may be suitable for modeling characteristics like liquidity, but that the specific models are a poor fit. Therefore we propose new models that are tailored to our application. We proceed by preparing the EPEX SPOT M7 order flow data, a data set containing all orders submitted to the German intraday electricity market. We present prepro- cessing steps and decision rules necessary to infer the identity of individual orders, information that is not directly available. Based on these rules, we implement a historical market simulator that reconstructs the complete state of the market for every point in time. The historic market simulator itself can be used as a simple, limit order book based market model. It allows modeling the influence of trade size and trading time on the available market price. Thus, this model can, when limited to small trade sizes, represent important dimensions of liquidity. We find that the historical EPEX SPOT order flow simulator can reproduce independent transaction records with a near-exact accuracy. The order flow is then used to create a stochastic market model. First, we introduce a horizon dependent power law model for the order submission intensity, which provides a very good fit to empirical data. We show that trading size is strongly clustered around integers, and propose a mixture model that can reproduce the preference of traders for rounded values. Order cancellation arrival intensity is indirectly modeled by a distribution of order lifetimes. We show that order lifetime does not follow the exponential distribution used for stock markets so we instead model it using a log-logistic model. Finally, we simulate the market model for the last two hours of trading for one month of unseen test data. For most of the variables, the simulated outcome matches the time independent distributions of the empirical data. The fit is especially close for the order arrival rate, order prices, the mid-price, the price at the best ask and the price of the best bid.},
  school = {Technical University Munich},
  note = {Henry's Masters Thesis}
}

@book{Ullrich09frcstHdgMkt,
  title = {Forecasting and Hedging in the Foreign Exchange Markets},
  author = {Ullrich, Christian},
  year = {2009},
  volume = {623},
  publisher = {Springer},
  url = {http://www.springer.com/economics/financial+economics/book/978-3-642-00494-0?token=gbgen&wt_mc=Google-_-Book+Search-_-Springer-_-EN&otherVersion=978-3-642-00495-7},
  abstract = {The growing complexity of many real world problems is one of the biggest challenges of our time. The area of international finance is one prominent example where decision making is often fraud to mistakes, and tasks such as forecasting, trading and hedging exchange rates seem to be too difficult to expect correct or at least adequate decisions. From the high complexity of the foreign exchange market and related decision problems, the author derives the necessity to use tools from Machine Learning and Artificial Intelligence, e.g. Support Vector Machines, and to combine such methods with sophisticated financial modelling techniques. The suitability of this combination of ideas is demonstrated by an empirical study and by simulation. Content Level ? Professional/practitioner Keywords ? Artificial Intelligence - Computational Finance - Decision-making - Foreign Exchange Market Complexity - Machine Learning Related subjects ? Artificial Intelligence - Business Information Systems - Finance \& Banking - Financial Economics - Quantitative Finance},
  owner = {sotterson},
  note = {Says that, in high dimensions, p-Gaussians "lead to better generalization than Gaussians" also more computation b/c of fractional kernel (p. 103, as viewed on Google books) He calls them "exotic"! p-Gaussians: Francois05locKernHiDim},
  timestamp = {2014.03.28}
}

@incollection{Jaimungal09KernCopulaPhD,
  title = {Kernel-Based Copula Processes},
  booktitle = {Machine Learning and Knowledge Discovery in Databases: {{European}} Conference, {{ECML PKDD}} 2009, Bled, Slovenia, September 7-11, 2009, Proceedings, Part {{I}}},
  author = {Jaimungal, Sebastian and Ng, Eddie K. H.},
  editor = {Buntine, Wray and Grobelnik, Marko and Mladeni{\'c}, Dunja and {Shawe-Taylor}, John},
  year = {2009},
  pages = {628--643},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-04180-8_58},
  url = {http://dx.doi.org/10.1007/978-3-642-04180-8_58},
  abstract = {The field of time-series analysis has made important contributions to a wide spectrum of ap- plications such as tide-level studies in hydrology, natural resource prospecting in geo-statistics, speech recognition, weather forecasting, financial trading, and economic forecasts and analy- sis. Nevertheless, the analysis of the non-Gaussian and non-stationary features of time-series remains challenging for the current state-of-art models. This thesis proposes an innovative framework that leverages the theory of copula, combined with a probabilistic framework from the machine learning community, to produce a versatile tool for multiple time-series analysis. I coined this new model Kernel-based Copula Processes (KCPs). Under the new proposed framework, various idiosyncracies can be modeled compactly via a kernel function for each individual time-series, and long-range dependency can be cap- tured by a copula function. The copula function separates the marginal behavior and serial dependency structures, thus allowing them to be modeled separately and with much greater flexibility. Moreover, the codependent structure of a large number of time-series with poten- tially vastly different characteristics can be captured in a compact and elegant fashion through the notion of a binding copula. This feature allows a highly heterogeneous model to be built, breaking free from the homogeneous limitation of most conventional models. The KCPs have demonstrated superior predictive power when used to forecast a multitude of data sets from meteorological and financial areas. Finally, the versatility of the KCP model is exemplified when it was successfully applied to non-trivial classification problems unaltered.},
  isbn = {978-3-642-04180-8},
  owner = {sotterson},
  timestamp = {2017.06.06},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Jaimungal09KernCopulaPhD.pdf}
}

@techreport{Giebel05windEnsemble,
  title = {Wind Power Prediction Using Ensembles},
  author = {Giebel, Gregor and Badger, Jake and Landberg, Lars and Nielsen, Henrik Aalborg and Nielsen, Torben Skov and Madsen, Henrik and Sattler, Kai and Feddersen, Henrik and Vedel, Henrik and T{\o}fting, John and Kruse, Lars and Voulund, Lars},
  year = {2005},
  month = sep,
  number = {Ris{\o}-R-1527(EN)},
  institution = {Ris{\o} National Laboratory},
  url = {http://www.risoe.dtu.dk/knowledge_base/publications/reports/ris-r-1527.aspx?sc_lang=en},
  abstract = {The Ensemble project investigated the use of meteorological ensemble fore-casts for the prognosis of uncertainty of the forecasts, and found a good method to make use of ensemble forecasts. This method was then tried based on ensembles from ECMWF in form of a demo application for both the Nysted offshore wind farm and the whole Jutland/Funen area. The utilities used these forecasts for maintenance planning, fuel consumption estimates and over-the-weekend trading on the Leipzig power exchange. Other notable scientific results include the better accuracy of forecasts made up from a simple superposition of two NWP provider (in our case, DMI and DWD), an investigation of the merits of a parameterisation of the turbulent kinetic energy within the delivered wind speed forecasts, and the finding that a ?na?ve? downscaling of each of the coarse ECMWF ensemble members with higher resolution HIRLAM did not improve the error scores or the result space enough to warrant the computational effort.},
  owner = {scotto},
  note = {* instantanious time-lagged ensemble spread doesn't predict uncertainty (ref 6( -- but apparenty their temporal development does give a clue about ensembles papers (refs 2,3) ---- Pinson09skillEnsWndPow may be an update of ref 2 ---- ref 3: Lange02windPowShrtTrmMetCond * NWP errors don't depend upon wind speed! -- but do depend upon location in power curve * Interesting KNN forecasting idea (ref 11) -- look back through history for K=10 days with forecasts like the one that has been forecasted -- use (the actually measured?) wind speeds during those days to build an empirical quantile forecast -- I guess it worked, but I can't find the paper. -- this is a kind of KNN regime learning * tried TKE (turbulent kinetic energy) as a forecast input -- several types -- one of them slightly improved a point forecast -- but they didn't check to see if it could improve a quantile forecast (maybe I should try this?) Ensembles --{\textquestiondown} power * first learn a logarithmic transform of power -- separation of power and speed makes it possible to always cover full power range on test, if it wasn't in train -- do it this way so that it's not biased (I gather from the Conclusions on p. 39, although I missed this in the text) * Predict transformed power with nonlinear local function -- a function of NWP u,v and horizon -- horizon dependence is affected only by speed and dir -- trained on ensembles? Doesn't say. -- power curve is a deterministic fit (local linear and local constant) -- trained by S+ (and R?) function LFLM * Foreach ensemble, predict the power * At each time, calc raw pow probs from ranks of ensemble probs -- means that you can't predict quantiles outside of (25-75\%)! -- is badly calibrated, as shown in QQ plots -- but can use the QQ plots with measurement quantiles to adjust the raw probabilities * Predict quantiles from raw power probs -- a spline fit of the raw power prob, predicting a log transformed version of the adjusted raw probs ---- does this keep it bounded or something? -- horizon has a multiplicative affected on quantile prob. (and is modeled w/ a local 2{$^{nd}$} order poly) * Adaptation -- they say it's really important to adapt, or at least regularly recalibrate * autocorrelation is ignored in this method * NWP: HIRLAM and COSMODE, I think Use of forecasts * for Elsam (Danish pow Co.?) control room, they're used in both planning and operation phase * in demo, find that users just use the point forecast (ensemble mean), not the ensemble * users aren't able to judge quality of forecasts b/c don't have enough data * for some power plants need at least a 7 day ahead forecast! * Theoretical uses -- Electricity trade over weekend -- Power station failure -- Fuel demand predictions RESULTS * the usual reliability, sharpness, and resolution plots * Spread/skill relationship: they do find one -- the forecast interquantile range does predict the pt. forecast MAE (pt. forecast is the ensemble median) -- I'm not sure if it exists below 24 hours (they "disregard" this range) * get slightly better results using NWP grid point offset in the direction of offshore winds. * Multi-model: avg. better than either NWP by itself, as usual *},
  timestamp = {2008.07.04}
}

@book{Lange05physShortWindPredBook,
  title = {Physical Approach to Short-Term Wind Power Prediction},
  author = {Lange, Matthias and Focken, Ulrich},
  year = {2006},
  publisher = {Springer},
  url = {http://www.springer.com/engineering/power+engineering/book/978-3-540-25662-5},
  abstract = {The effective integration of wind energy into the overall electricity supply is a technical and economical challenge because the availability of wind power is determined by fluctuating meteorological conditions. This book offers an approach to the ultimate goal of the short-term prediction of the power output of winds farms. Starting from basic aspects of atmospheric fluid dynamics, the authors discuss the structure of winds fields, the available forecast systems and the handling of the intrinsic, weather-dependent uncertainties in the regional prediction of the power generated by wind turbines. This book addresses scientists and engineers working in wind energy related R and D and industry, as well as graduate students and nonspecialists researchers in the fields of atmospheric physics and meteorology.},
  owner = {sotterson},
  note = {good chapter: Overview of Wind Power Prediction Systems},
  timestamp = {2008.07.03}
}

@inproceedings{Otterson16stochTrdTSO,
  title = {Stochastic Trading across Time under {{German TSO}} Constraints},
  booktitle = {15th International Workshop on Large-Scale Integration of Wind Power into Power Systems},
  author = {Otterson, Scott and Jost, Dominik and Jansen, Malte and Siefert, Malte},
  year = {2016},
  month = nov,
  address = {Vienna},
  url = {http://windintegrationworkshop.org/vienna2016},
  abstract = {The uncertainty of wind power forecasts generally decreases with forecast horizon, making it possible to profitably sell large amounts of this power on a day-ahead interday market, and then to make small corrections closer to delivery time by trading on a short-horizon intraday market. Such multistage intraday buy/sell decisions are often made using deterministic power forecasts, even though probabilistic forecasts describe the highly relevant uncertainty vs. time relationship. This is especially true for German transmission system operators (TSOs), who are allowed to consider only the expected value -- i.e. a point forecast -- when selling on the interday market, and who have other restrictions on the intraday market. Simulations suggest that certain German TSOs, or direct marketers which have smaller installed capacities, may benefit on the intraday market by trading while using probabilistic forecasts. A much larger benefit may also accrue with the use of a price forecast, also developed for this work. One limitation this study is that the market model used may have the effect of smoothing out price volatility and illiquidity. A next step is to re-evaluate probabilistic trading on a market model which simulates such illiquidity. Index Terms -- wind power, probabilistic forecasting, price forecasting, electricity markets, grid integration},
  owner = {sotterson},
  note = {My Wind Integration Workshop (Vienna, 2016) paper.},
  timestamp = {2016.11.18}
}

@article{Kiesel15econAnl15minIntradayPrice,
  title = {Econometric Analysis of 15-Minute Intraday Electricity Prices},
  author = {Kiesel, Ruediger and Paraschiv, Florentina},
  year = {2015},
  journal = {Available at SSRN},
  url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2671379},
  abstract = {The trading activity in the German intraday electricity market has increased significantly over the last years. This is partially due to an increasing share of renewable energy, wind and photovoltaic, which requires power generators to balance out the forecasting errors in their production. We investigate the bidding behaviour in the intraday market by looking at both last prices and continuous bidding, in the context of a fundamental model. A unique data set of 15-minute intraday prices and intraday-updated forecasts of wind and photovoltaic has been employed and price bids are modelled by prior information on fundamentals. We show that intraday prices adjust asymmetrically to both forecasting errors in renewables and to the volume of trades dependent on the threshold variable demand quote, which reflects the expected demand covered by the planned traditional capacity in the day-ahead market. The location of the threshold can be used by market participants to adjust their bids accordingly, given the latest updates in the wind and photovoltaic forecasting errors and the forecasts of the control area},
  owner = {sotterson},
  note = {German intraday 15 minute price behavior. Shows 15 min. sawtooth shape due to day ahead market product length of one hour. {\textquestiondown}{\textquestiondown}{\textquestiondown} Henry Martin 11/4/2016 2:20 pm {\textquestiondown}{\textquestiondown}{\textquestiondown} Hi Scott, Hi Volker, here is the graph I told both of you about. It shows the dependency of the 15min intraday price to solar ramping. Notice how the price spikes occur in the first 15 minutes when solar feed in is on the rise, and in the last 15 minutes when it is decreasing. It is also interesting to see, that the amplitude of the price spikes depends on the slope of the solar generation. Reason is, that energy on the day ahead market can only be traded hourly, which means that if generation is increasing, generators sold to much energy in the first half of the hour and to few in the second half. See you soon, Henry},
  timestamp = {2016.11.21},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kiesel15econAnl15minIntradayPrice.pdf}
}

@inproceedings{Martin18germanMktLOB,
  title = {German Intraday Electricity Market Analysis and Modeling Based on the Limit Order Book},
  booktitle = {International Conf. on the European Energy Market.},
  author = {Martin, Henry and Otterson, Scott},
  year = {2018},
  address = {{\L}{\'o}d{\'z}, PL.},
  abstract = {This paper presents a market model for the SPOT German continuous intraday market for electric trading based on the limit order book (LOB). We use EPEX SPOT M7 order book data, which contains all submitted to the German continuous intraday market, to the historic course of the market. Thereby, we reconstruct complete state of the LOB at every point in (trading We validate our simulation by comparing the transactions our simulation generated with the actual historical transactions available from a different data set. The LOB based market can be used to include price volatility risk and illiquidity when simulating trading at the EPEX SPOT continuous market. Furthermore, we present all preprocessing steps decision rules necessary to correctly identify orders from often ambiguous EPEX SPOT M7 order book data.},
  note = {Conf. paper on Henry's Master's thesis}
}

@article{Hagemann15priceDetDEintraday,
  title = {Price Determinants in the {{German}} Intraday Market for Electricity: An Empirical Analysis},
  author = {Hagemann, Simon},
  year = {2015},
  month = jun,
  journal = {The Journal of Energy Markets},
  volume = {8},
  number = {2},
  pages = {21--45},
  publisher = {Incisive Media},
  issn = {1756-3607},
  doi = {10.21314/jem.2015.128},
  url = {http://dx.doi.org/10.21314/jem.2015.128},
  abstract = {This paper presents a first investigation of hourly price determinants in the German intraday market for electricity. The influence of power plant outages, forecast errors of wind and solar power produc- tion, load forecast errors and foreign demand and supply on intraday prices are explained from a theoretical perspective. Furthermore the influences of the non-linear merit-order shape, ramping costs and strategic market behavior are discussed. The empirical results from different regression analysis with data from 2010 and 2011 show that most price determinants increase and decrease intraday prices as expected. Nevertheless, only a minor share of power plant outages and solar power forecast errors are traded on the electronic intraday trading platform, thus influencing prices not as strongly as expected. Furthermore the price determinants influence intraday prices differently over the course of the day which may be explained by an alternating liquidity provision. Keywords: Intraday market for electricity, price modeling, price determinants. This was the 2013 working paper abstract},
  owner = {sotterson},
  note = {Use for price forecasting feature selection? 2013 working paper is from:},
  timestamp = {2016.11.28},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hagemann15priceDetDEintraday.pdf}
}

@inproceedings{Gronwald10carbonCopula,
  title = {The Dependence Structure between Carbon Emission Allowances and Financial Markets - a Copula Analysis},
  booktitle = {Global Finance Conference},
  author = {Gronwald, Marc and Ketterer, Janina and Tr{\"u}ck, Stefan},
  year = {2010},
  month = jun,
  address = {Poznan, Poland},
  url = {http://www.glofin.org/2010GFC/gfc2010papers.htm},
  abstract = {This paper investigates the relationship between CO2 emission allowance prices and those of various other financial variables and commodities. Different copulas are applied in order to model the complex dependence structure between the return series of carbon emission allowance prices and those of various commodities as well as other financial series. As suggested in the literature, the use of correlation as the only measure of dependence can lead to an underestimation of the risk of joint extreme price movements. What is more, copula models represent a more flexible method for deriving the nature of dependence and provide an appropriate fit also for the tails of multivariate distributions. The findings suggest that the relationship between EU emission allowance (EUA) future returns and those of the other commodities - in particular gas and oil markets - is relatively weak. However, we find some dependence between EUA futures and equity or energy index returns. These results at least somehow contradict earlier studies that report no statistically significant or even negative correlations between returns of emission allowances and other financial variables. Regarding the nature of dependence, we also find some evidence of weak symmetric tail dependence for most of the considered series. Key words: CO2 Emission Trading, Commodity Markets, Copula Models, Dependence Structure},
  organization = {Global Finance Association},
  owner = {scot},
  note = {spinning reserves, a good overview of copulas that explains how to use them on a real (bivariate) problem},
  timestamp = {2010.12.06}
}

@article{Karpatne17physNNlakeT,
  title = {Physics-Guided Neural Networks ({{PGNN}}): {{An}} Application in Lake Temperature Modeling},
  author = {Karpatne, Anuj and Watkins, William and Read, Jordan and Kumar, Vipin},
  year = {2017},
  journal = {arXiv preprint arXiv:1710.11431},
  eprint = {1710.11431},
  url = {https://arxiv.org/abs/1710.11431},
  abstract = {This paper introduces a novel framework for combin- ing scientific knowledge of physics-based models with neural networks to advance scientific discovery. This framework, termed as physics-guided neural network (PGNN), leverages the output of physics-based model simulations along with observational features to gen- erate predictions using a neural network architecture. Further, this paper presents a novel framework for us- ing physics-based loss functions in the learning objective of neural networks, to ensure that the model predic- tions not only show lower errors on the training set but are also scientifically consistent with the known physics on the unlabeled set. We illustrate the effectiveness of PGNN for the problem of lake temperature modeling, where physical relationships between the temperature, density, and depth of water are used to design a physics- based loss function. By using scientific knowledge to guide the construction and learning of neural networks, we are able to show that the proposed framework en- sures better generalizability as well as scientific consis- tency of results.},
  archiveprefix = {arXiv},
  annotation = {353 citations (Semantic Scholar/arXiv) [2023-07-25]},
  note = {NN model of lake temp with physical model derived input features and a simple penalty term for physical inconsistency. Works better than plain NN or plain physical model. Could be useful for forecasting w/ mixed physical/NN models e.g. the PV model here: Marquez13solarFrcstSatGrndNN Authors have other related papers too. Was covered here: https://towardsdatascience.com/physics-guided-neural-networks-pgnns-8fe9dbad9414},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Karpatne17physNNlakeT.pdf}
}

@article{Zhou12prbFrcstMkts,
  title = {Application of Probabilistic Wind Power Forecasting in Electricity Markets},
  author = {Zhou, Z. and Botterud, A. and Wang, J. and Bessa, R.J. and Keko, H. and Sumaili, J. and Miranda, V.},
  year = {2012},
  journal = {Wind Energy},
  volume = {16},
  number = {3},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {1099-1824},
  doi = {10.1002/we.1496},
  abstract = {This paper discusses the potential use of probabilistic wind power forecasting in electricity markets, with focus on the scheduling and dispatch decisions of the system operator. We apply probabilistic kernel density forecasting with a quantile-copula estimator to forecast the probability density function, from which forecasting quantiles and scenarios with temporal dependency of errors are derived. We show how the probabilistic forecasts can be used to schedule energy and operating reserves to accommodate the wind power forecast uncertainty. We simulate the operation of a two-settlement electricity market with clearing of day-ahead and real-time markets for energy and operating reserves. At the day-ahead stage, a deterministic point forecast is input to the commitment and dispatch procedure. Then a probabilistic forecast is used to adjust the commitment status of fast-starting units closer to real time, on the basis of either dynamic operating reserves or stochastic unit commitment. Finally, the real-time dispatch is based on the realized availability of wind power. To evaluate the model in a large-scale real-world setting, we take the power system in Illinois as a test case and compare different scheduling strategies. The results show better performance for dynamic compared with fixed operating reserve requirements. Furthermore, although there are differences in the detailed dispatch results, dynamic operating reserves and stochastic unit commitment give similar results in terms of cost. Overall, we find that probabilistic forecasts can contribute to improve the performance of the power system, both in terms of cost and reliability.},
  owner = {sotterson},
  annotation = {60 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Useful for TSO's in Ewelina? See also: Zhou13probFrcstElecMkts},
  timestamp = {2013.03.13},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhou12prbFrcstMkts.pdf}
}

@article{Zhao16nnSkewTcopulaPortfolioOpt,
  title = {Neural Network Copula Portfolio Optimization for Exchange Traded Funds},
  author = {Zhao, Yang and Stasinakis, Charalampos and Sermpinis, Georgios and Shi, Yukun},
  year = {2016},
  journal = {SSRN Electronic Journal},
  publisher = {Elsevier BV},
  doi = {10.2139/ssrn.2877966},
  abstract = {This paper attempts to investigate if adopting accurate forecasts from Neural Network (NN) models can lead to statistical and economically significant benefits in portfolio management decisions. In order to achieve that, three NNs, namely the Multi-Layer Perceptron (MLP), Recurrent Neural Network (RNN) and the Psi Sigma Network (PSN), are applied to the task of forecasting the daily returns of three Exchange Traded Funds (ETFs). The statistical and trading performance of the NNs is benchmarked with the traditional Auto-Regressive Moving Average (ARMA) models. Next, a novel dynamic asymmetric copula model (NNC) is introduced in order to capture the dependence structure across ETF returns. Based on the above, weekly re-balanced portfolios are obtained and compared by using the traditional mean-variance and the mean-CVaR portfolio optimization approach. In terms of the results, PSN outperforms all models in statistical and trading terms. Additionally, the asymmetric skewed t copula statistically outperforms symmetric copulas when it comes to modelling ETF returns dependence. The proposed NNC model leads to significant improvements in the portfolio optimization process, while forecasting covariance accounting for asymmetric dependence between the ETFs also improves the performance of obtained portfolios. Keywords Copulas, Neural Networks, Portfolio Optimization, ETF},
  note = {NN's produce forecasts and they are combined with a skew-T copula. These are better than T-copulas because they can model asymmetry (and have better performance). I don't know about robustness or dimensionality, though... Also, a Psi Sigma Network (PSN) is found to be a better forecasting network thatn MLP's or RNN's.},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Zhao16nnSkewTcopulaPortfolioOpt.pdf;C\:\\Users\\scott\\Zotero\\storage\\UPT7D3JL\\Zhao et al.2016 - Neural network copul.pdf}
}

@techreport{Hochloff14GermanPowerMarket,
  type = {Tech Report},
  title = {German {{Power Market}}: {{Introduction}} to Markets, Products and Processes},
  author = {Hochloff, Patrick and Jansen, Malte and Jost, Dominik and Bofinger, Dr. Stefan},
  year = {2014},
  institution = {Fraunhofer Institute Of Wind Energy And Energy System Technology},
  abstract = {This document will give the reader an introduction to the German power market. The aim is to provide a comprehensive overview over the market rules and products at the German wholesale and ancillary service markets. The wholesale power market is introduced in chapter 3. The basic principles of electricity trading on derivative and spot markets are similar to other commodity markets. Commodity trading is typical for metals, coal, agricultural products, crude oil and natural gas. The purpose of derivative market is to hedge risks against unforeseen price developments. The seller of a commodity agrees upfront to pay a price for a specified good at a specified delivery date and the seller agrees to deliver the good at the specified time. These deals can be closed at exchanges or bilaterally between the buyer and the seller, the so-called over-the-counter trade (OTC). The European Energy Exchange (EEX) operates the power derivative market for the market area Germany. An introduction to the derivative markets is given in chapter 3.2 and to OTC trading in chapter 3.3. A closer matching of supply and demand occurs on short-term spot markets that are operated by EPEX SPOT for the German market area. Spot markets establish power prices for every hour and even for every quarter of an hour. Chapter 3.1 introduces to the EPEX SPOT market. However forecast of generation and consumption are not perfect but supply and demand must be in balance every time to keep a constant frequency. For this reason the TSOs procure ancillary services and especially frequency control reserve. Control reserve is an extra market that enables market participants to provide services. This market is explained in chapter 4. On the other hand, imbalances of a market participant are charged with a balancing energy price. The organisational framework of market participants and the charging of imbalances are introduced in chapter 1.},
  owner = {sotterson},
  note = {Overview of how German power market works in 2014 Day ahead * last bid is placed at noon; * time covered by the bidding starts 12 hours after close * last if time covered is 36 hours after close * i.e. the possible range covered by the noon bids is 00:00-24 of the next day.},
  timestamp = {2014.10.14}
}

@article{Weber10AdqtMktDsngEur,
  title = {Adequate Intraday Market Design to Enable the Integration of Wind Energy into the {{European}} Power Systems},
  author = {Weber, Christoph},
  year = {2010},
  journal = {Energy Policy},
  volume = {38},
  number = {7},
  pages = {3155--3163},
  issn = {0301-4215},
  doi = {10.1016/j.enpol.2009.07.040},
  url = {http://www.sciencedirect.com/science/article/pii/S0301421509005564},
  abstract = {This contribution analyses the European electricity markets with respect to their aptitude to absorb large amounts of wind energy. Thereby in a first step the market designs of the major European power markets in France, Germany, Scandinavia, Spain and \{UK\} are reviewed, with a particular focus on liquidity in the spot and intraday markets. Then some key features of the short-term adjustments required by wind energy are discussed and the necessity of sufficient liquidity in intraday markets is highlighted. For the example of the German market subsequently the discrepancy between the physical short-term adjustment needs and the traded volumes on the intraday market is analyzed. This leads to an evaluation of proposals for improving the liquidity on the short-term market, including the use of continuous spot trading like in \{UK\} or the use of intraday auctions like in Spain.},
  owner = {sotterson},
  annotation = {251 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Large-scale wind power in electricity markets with Regular Papers
\par
Says that intraday market not used much in Europe to satisfy day-ahead forecast errors (according cite in Zugno13tradeWindGenMrktQs)},
  timestamp = {2015.03.05},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Weber10AdqtMktDsngEur.pdf}
}

@inproceedings{Mahler09sp500lagLasso,
  title = {Modeling the {{S}}\&{{P}} 500 Index Using the {{Kalman}} Filter and the {{LagLasso}}},
  booktitle = {Advances in Machine Learning for Computational Finance, International Workshop},
  author = {Mahler, Nicolas},
  year = {2009},
  month = jul,
  address = {London, UK},
  url = {http://web.me.com/davidrh/AMLCF09/Schedule.html},
  abstract = {This article introduces a method to predict upward and downward monthly variations of the S\&P 500 index by using a pool of macro-economic and financial explicative variables. The method is based on the combination of a denoising step, performed by Kalman filtering, with a variable selection step, performed by a Lasso-type procedure. In particular, we propose an implementation of the Lasso method called LagLasso which includes selection of lags for individual factors. We provide promising backtesting results of the prediction model based on a naive trading rule.},
  owner = {sotterson},
  note = {Use lasso to select predictor lag for stock uptick/downtick (use for wind ramps?) * selects one lag per variable using lasso * is predicting upward/downard S\&P 500 movement, so good for RAMP FORECASTING? * needs Kalman filtering to detrend (economics, but this might make sense anyway) * contrasted to a LARS approach, which selects a block of lags},
  timestamp = {2009.08.13}
}

@article{Lorenz11RegionalPVpowFrcst,
  title = {Regional {{PV}} Power Prediction for Improved Grid Integration},
  author = {Lorenz, Elke and Scheidsteger, Thomas and Hurka, Johannes and Heinemann, Detlev and Kurz, Christian},
  year = {2011},
  journal = {Progress in Photovoltaics: Research and Applications},
  volume = {19},
  number = {7},
  pages = {757--771},
  publisher = {Wiley Online Library},
  doi = {10.1002/pip.1033/abstract},
  abstract = {The contribution of power production from PV systems to the electricity supply is constantly increasing. An efficient use of the fluctuating solar power production will highly benefit from forecast information on the expected power production, as a basis for management of the electricity grids and trading on the energy market. We present and evaluate the regional PV power prediction system of University of Oldenburg and Meteocontrol GmbH providing forecasts of up to 2 days ahead with hourly resolution. The proposed approach is based on forecasts of the global model of the European Centre for Medium-Range Forecasts (ECMWF). It includes a post-processing procedure to derive optimised, site-specific irradiance forecasts and explicit physical modelling steps to convert the predicted irradiances to PV power. Finally, regional power forecasts are derived by up-scaling from a representative set of PV systems. The investigation of proper up-scaling is a special focus of this paper. We introduce a modified up-scaling approach, modelling the spatial distribution of the nominal power with a resolution of 18 18. The operational PV power prediction system is evaluated in comparison to the modified up-scaling approach for the control areas of the two German transmission system operators ?transpower? and ?50 Hertz? for the period 2.7.2009?30.4.2010. rmse values of the operational forecasts are in the range of 4?5\% with respect to the nominal power for intra-day and day-ahead forecast horizons. Further improvement is achieved with the modified upscaling approach. KEYWORDS PV power prediction; grid integration; irradiance prediction; PV simulation},
  owner = {sotterson},
  note = {How to do PV upscaling for horizons up to two days ahead.},
  timestamp = {2015.02.19},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lorenz11RegionalPVpowFrcst.pdf}
}

@inproceedings{Jin19physInvNN,
  title = {A Physics-Driven Deep Learning Network for Subsurface Inversion},
  booktitle = {Proc. {{United}} States National Committee of {{URSI}} National Radio Science Meeting ({{USNC-URSI NRSM}})},
  author = {Jin, Y. and Wu, X. and Chen, J. and Huang, Y.},
  year = {2019},
  month = jan,
  pages = {1--2},
  issn = {null},
  doi = {10.23919/USNC-URSI-NRSM.2019.8712940},
  abstract = {Subsurface inversion is an essential technique for many applications including seismic processing, oilfield well logging an geosteering. Conventional inverse methods based on optimization are time-consuming and sensitive to initial values. The traditional lookup table approach which is limited by the table size could reduce the computational time but only achieves low accuracy. To solve these issues, we propose a physics-driven Deep Neural Network (PhDNN) for solving non-linear inverse problems. In this framework, the physical forward model is utilized to produce a data misfit. Both the model misfit and data misfit are used to train the network. As an example, we use this framework to solve a geosteering problem which enables the drilling direction adjusted by collected resistivity well logging measurements. Numerical tests indicate that the proposed network could improve the quality of the prediction significantly.},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Invertible model, useable maybe for both quantile and distribution regression, or for some kind of probabilistic forecasting assuming a parsimonious parametric distribution model, like the metalog (Keelin16metalogDist)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Jin19physInvNN.pdf}
}

@article{Pineda10scenarioRedRiskAvrs,
  title = {Scenario Reduction for Risk-Averse Electricity Trading},
  author = {Pineda, S. and Conejo, A.J.},
  year = {2010},
  month = jun,
  journal = {Generation, Transmission Distribution, IET},
  volume = {4},
  number = {6},
  pages = {694--705},
  issn = {1751-8687},
  doi = {10.1049/iet-gtd.2009.0376},
  url = {http://ieeexplore.ieee.org.globalproxy.cvt.dk/search/srchabstract.jsp?tp=&arnumber=5473192&queryText%3DScenario+reduction+for+risk-averse+electricity+trading%26openedRefinements%3D*%26searchField%3DSearch+All},
  abstract = {Stochastic optimisation models used to identify risk-averse decisions in electricity futures markets are usually hard to solve because of the large number of scenarios representing the uncertain parameters involved. A novel scenario reduction technique is proposed to select those scenarios that, considering the risk aversion of the decision maker, best represent the original scenario set and make the optimisation problem tractable. Two case studies illustrate the performance of the proposed technique to reduce scenarios pertaining to both continuous and discrete uncertain parameters. The advantage of the proposed technique against the existing ones is apparent in highly risk-averse cases.},
  owner = {scot},
  annotation = {62 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Reducing stochastic programming scenarios explicitly considering risk (variance of return) sensitivity},
  timestamp = {2010.11.22},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Pineda10scenarioRedRiskAvrs.pdf}
}

@techreport{Jonsson10spinRsrv,
  title = {Allocation and Deployment of Spinning Reserves},
  author = {J{\'o}nsson, Tryggvi and Gonz{\'a}lez, Juan Miguel Morales and Zugno, Marco and Madsen, Henrik and Otterson, Scott},
  year = {2010},
  month = oct,
  address = {Lyngby, Denmark},
  institution = {Technical University of Denmark (DTU)},
  abstract = {This report introduces an optimization model to determine the reserve needs in a power system with a high penetration of wind power. The reserve required to accommodate the net load forecast error is estimated using a probabilistic criterion based on the expected load not served. The net load is defined as the total system demand minus the wind generation. The proposed reserve determination model is specially tailored to the Danish pool market organization, which comprises several power and energy trading arenas sequentially arranged from one day ahead to real time. The uncertainty associated with the system load and wind production is dealt with by means of the multivariate treatment of the forecast errors. Specifically, the cross-correlogram of load and wind power forecast errors is analyzed with a view to identifying a structure easily exploitable.},
  owner = {scot},
  note = {First spining reserves simulation result; Tryggvi's first report, delivered to energinet.dk, I think. This is a net demand forecast.},
  timestamp = {2010.11.05}
}

@article{Perez90ModelingDaylightAvailability,
  title = {Modeling Daylight Availability and Irradiance Components from Direct and Global Irradiance},
  author = {Perez, Richard and Ineichen, Pierre and Seals, Robert and Michalsky, Joseph and Stewart, Ronald},
  year = {1990},
  journal = {Solar Energy},
  volume = {44},
  number = {5},
  pages = {271--289},
  issn = {0038-092X},
  doi = {http://dx.doi.org/10.1016/0038-092X(90)90055-H},
  url = {http://www.sciencedirect.com/science/article/pii/0038092X9090055H},
  abstract = {This paper presents the latest versions of several models developed by the authors to predict short time-step solar energy and daylight availability quantities needed by energy system modelers or building designers. The modeled quantities are global, direct and diffuse daylight illuminance, diffuse irradiance and illuminance impinging on tilted surfaces of arbitrary orientation, sky zenith luminance and sky luminance angular distribution. All models are original except for the last one which is extrapolated from current standards. All models share a common operating structure and a common set of input data: Hourly (or higher frequency) direct (or diffuse) and global irradiance plus surface dew point temperature. Key experimental observations leading to model development are briefly reviewed. Comprehensive validation results are presented. Model accuracy, assessed in terms of root-mean-square and mean bias errors, is analyzed both as a function of insolation conditions and site climatic environment.},
  owner = {sotterson},
  note = {How IWES models irradiation on inclined surface. Referred to in: Saint-Drenan14commentsGenPVpow Alternative seems to be Klucher79Evaluationmodelspredict},
  timestamp = {2017.04.30},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Perez90ModelingDaylightAvailability.pdf}
}

@techreport{Hagemann15volIntradayEurope,
  type = {{{EWL}} Working Paper No. [03/15]},
  title = {Trading Volumes in Intraday Markets-Theoretical Reference Model and Empirical Observations in Selected European Markets},
  author = {Hagemann, Simon and Weber, Christoph and others},
  year = {2015},
  institution = {University of Duisburg-Essen},
  url = {http://www.ewl.wiwi.uni-due.de/fileadmin/fileupload/BWL-ENERGIE/Arbeitspapiere/RePEc/pdf/wp1503_TradingVolumesInIntradayMarketsTheoreticalReferenceModelAndEmpiricalObservationsInSelectedEuropeanMarkets.pdf},
  abstract = {This paper presents an analytical benchmark model for national intraday adjustment needs under consideration of fundamental drivers, market concentration and portfolio internal netting. The benchmark model is used to calculate the intraday market outcomes if (i) large and small players as well as transmissions operators trade and (ii) only large players and transmission system operators trade. Transaction costs may prevent the competitive fringe from intraday market participation. The theoretical national intraday trading volumes are calculated with market data from three European countries with auction-based intraday markets (Italy, Portugal, Spain) and four countries with continuous intraday markets (Denmark, France, Germany, United Kingdom). The model results allow two main conclusions: The competitive fringe is not trading on exchanges in Denmark and France but in Germany. The second conclusion is that the high observed volumes in auction-based intraday markets cannot be explained by fundamentals or the auction-based design but are mainly caused by market peculiarities. The same result applies to the UK. Keywords: Renewables market integration, Liquidity modeling, continuous and auction-based intraday markets.},
  note = {Only Germany has a "competitive fringe" and volumes are "too high" in auction based markets.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hagemann15volIntradayEurope.pdf}
}

@article{Hagemann13intradayMktLiquid,
  title = {An Empirical Analysis of Liquidity and Its Determinants in the {{German}} Intraday Market for Electricity},
  author = {Hagemann, Simon and Weber, Christoph},
  year = {2013},
  month = oct,
  journal = {EWL Working Paper No. 17/2013},
  doi = {10.2139/ssrn.2349565},
  abstract = {This paper presents a theoretical and empirical analysis of liquidity in the German intraday market for electricity. Two models that aim at explaining intraday liquidity are developed. The first model considers the fundamental merit-order and intraday adjustment needs as the drivers of liquidity in a perfectly competitive market. The second model relaxes the assumption of perfect competition in the intraday market and assumes that the trading behavior of profit maximizing market participants influences the liquidity provision. The relevance of commonly used liquidity indicators like the bid ask-spread, resiliency, market depth, price variance, delay and search costs as well as trading volume and the number of trades are analyzed with respect to both models of liquidity. The empirical findings indicate that liquidity in the German intraday market can be explained by the trading model while the purely fundamental model is rejected. Keywords: Intraday market, electricity, liquidity, fundamental model, trading model},
  howpublished = {EWL Working Paper No. 17/2013},
  owner = {sotterson},
  annotation = {42 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {EWL Working Paper No. 17/2013 Review: Maybe good price forecast features. German EPEX Intraday market doesn't behave in the fundamental/merit order economist sense because traders are not risk averse, and will delay trading to get a better deal, increasing liquidity (I think). Study discusses several metrics for market liquidity and then deduces how they would behave under classic, "fundamental" assumption and under a more complicated "trader model." It then checks the metrics to see which assumption is more likely to be true; it is overwhelmingly the trader model. The dimensions of liquidity and their correlations might be good for a price forecast, since low liquidity prediction for the future would suggest that the price will go up if you buy a lot of electricity. If TSO's can't consider price, they still must consider market liquidity in intraday market. This is a paper about that, and maybe that will lead to a "liquidity" forecast (trade when you forecast that enough electricity will be available, or maybe make this decision on every step). If do this, then will have to also figure out what the true TSO forecast was (not normalized) and must somehow remove that from the historical data. This paper also talks about merit order stuff, maybe related to the work the other group did on UC price forecasting. Says there are no German grid-wide load forecasts. Really? Possible features from, Fig II, p. 19 and the neighborhood 1. Time of day 2. Day of week (binary?) 3. Bid Ask spread, a liquidity indicator(would need DA load curves. Available) 4. intraday (I think) price variance and high-low spread, a liquidity indicator (from past hour or something, for the delivery time in question?) 5. number of trades and volume, a liquidity indicator (for the delivery time in question?)},
  timestamp = {2015.07.18},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hagemann13intradayMktLiquid.pdf}
}

@article{Morales10shortWindTrade,
  title = {Short-Term Trading for a Wind Power Producer},
  author = {Morales, J.M. and Conejo, A.J. and {P{\'e}rez-Ruiz}, J.},
  year = {2010},
  month = feb,
  journal = {Power Systems, IEEE Transactions on},
  volume = {25},
  number = {1},
  pages = {554--564},
  issn = {0885-8950},
  doi = {10.1109/TPWRS.2009.2036810},
  abstract = {This paper presents a technique to derive the best offering strategy for a wind power producer in an electricity market that includes various trading floors. Uncertainty pertaining to wind availability, market prices at the different trading stages, and balancing energy needs are properly taken into account. Risk on profit variability is suitably controlled at the cost of a small reduction in expected profit. The proposed technique translates into a linear programming problem of moderate size, which is readily solvable using commercially available software. A variety of numerical case studies demonstrate the interest and effectiveness of the proposed technique. Appropriate conclusions are duly drawn.},
  owner = {scot},
  note = {Relevant to optimal spinning reserve project},
  timestamp = {2010.07.05},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Morales10shortWindTrade.pdf}
}

@article{Zugno13tradeWindGenMrktQs,
  title = {Trading Wind Energy on the Basis of Probabilistic Forecasts Both of Wind Generation and of Market Quantities},
  author = {Zugno, M. and J{\'o}nsson, T. and Pinson, P.},
  year = {2013},
  journal = {Wind Energy},
  volume = {16},
  number = {6},
  pages = {909--926},
  issn = {1099-1824},
  doi = {10.1002/we.1531},
  abstract = {Wind power is not easily predictable and non-dispatchable. Nevertheless, wind power producers are increasingly urged to participate in electricity market auctions in the same manner as conventional power producers. The aim of this paper is to propose an operational strategy for trading wind energy in liberalized electricity markets and to assess its performance. At first, the so-called optimal quantile strategy is revisited. It is proved that without market power, i.e. under the price-taker assumption, this strategy maximizes expected market revenues. Forecasts of wind power production, of day-ahead and real-time market prices and of the system imbalance are inputs to this strategy. Subsequently, constraining of the bid that maximizes the expected revenues is proposed as a way to overcome the strategy's disregard of practical limitations and, at the same time, of risk. Two constraining techniques are introduced: constraining in the decision space and in the probability space. Finally, the trade of a wind power producer is simulated in a test case for the Eastern Danish (DK-2) price area of the Nordic Power Exchange (Nord Pool) during a 10 month period in 2008. The results of the test case show the financial benefits of the aforementioned strategy as well as the consequent interaction with the electricity market. This study will support a demonstration in the framework of the EU project ANEMOS.plus.},
  owner = {sotterson},
  annotation = {60 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Optimal quantile trading given price and wind generation forecasts for Danish Nordpool. The paper that Pierre-Julien will base his IRPWIND 82.4 code upon. * day ahead forecast errors fixed mostly in regulation ("realtime") markets, not intraday (cite to Weber10AdqtMktDsngEur) * quantiles are optimal: many cites, theory discussed in Gneiting11quantOptFrcst (but does this require asymmetric penalties?) * stoch. mkt. prices generalize Pinson07tradeWindProbFrcst and Bremnes04windLocQR * Tryggvi had a 2013 price forecasting paper (Jonsson13elecPriceQR). Did it use this idea? * Marco had a 2010 paper modeling market feedback (Giabardo10feedbackElecMkt). Does this paper use that idea?},
  timestamp = {2015.03.04},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zugno13tradeWindGenMrktQs.pdf}
}

@article{Mahoney12ensKalQntlFrcst,
  title = {A Wind Power Forecasting System to Optimize Grid Integration},
  author = {Mahoney, W.P. and Parks, K. and Wiener, G. and Liu, Yubao and Myers, W.L. and Sun, Juanzhen and Delle Monache, L. and Hopson, T. and Johnson, D. and Haupt, S.E.},
  year = {2012},
  journal = {Sustainable Energy, IEEE Transactions on},
  volume = {3},
  number = {4},
  pages = {670--682},
  issn = {1949-3029},
  doi = {10.1109/TSTE.2012.2201758},
  abstract = {Wind power forecasting can enhance the value of wind energy by improving the reliability of integrating this variable resource and improving the economic feasibility. The National Center for Atmospheric Research (NCAR) has collaborated with Xcel Energy to develop a multifaceted wind power prediction system. Both the day-ahead forecast that is used in trading and the short-term forecast are critical to economic decision making. This wind power forecasting system includes high resolution and ensemble modeling capabilities, data assimilation, now-casting, and statistical postprocessing technologies. The system utilizes publicly available model data and observations as well as wind forecasts produced from an NCAR-developed deterministic mesoscale wind forecast model with real-time four-dimensional data assimilation and a 30-member model ensemble system, which is calibrated using an Analogue Ensemble Kalman Filter and Quantile Regression. The model forecast data are combined using NCAR's Dynamic Integrated Forecast System (DICast). This system has substantially improved Xcel's overall ability to incorporate wind energy into their power mix.},
  owner = {sotterson},
  annotation = {126 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {A fancy algorithm that converts wind ensembles to wind power. I haven't read yet if it had individual ensemble forecasts or what. The Kalman filter is an "analogue" KF -- don't know if that has anything to do with analog ensemble forecasting...},
  timestamp = {2013.10.10},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Mahoney12ensKalQntlFrcst.pdf}
}

@article{Pinson06qualValProbFrcst,
  title = {On the Quality and Value of Probabilistic Forecasts of Wind Generation},
  author = {Pinson, P. and Juban, J. and Kariniotakis, G. N.},
  year = {2006},
  month = jun,
  journal = {Probabilistic Methods Applied to Power Systems (PMAPS)},
  pages = {1--7},
  doi = {10.1109/PMAPS.2006.360290},
  abstract = {While most of the current forecasting methods provide single estimates of future wind generation, some methods now allow one to have probabilistic predictions of wind power. They are often given in the form of prediction intervals or quantile forecasts. Such forecasts, since they include the uncertainty information, can be seen as optimal for the management or trading of wind generation. This paper explores the differences and relations between the quality (i.e. statistical performance) and the operational value of these forecasts. An application is presented on the use of probabilistic predictions for bidding in a European electricity market. The benefits of a probabilistic view of wind power forecasting are clearly demonstrated},
  ncite = {18},
  owner = {sotterson},
  annotation = {40 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Found that distribution-based trading more profitable than point forecast based; can be worse for C02, though. Probabilistic forecasts * build quantiles by stacking center intervals of increasing width * produced by adative resampling, which I think is worse than quantile regression in Pinson07frcstEval Forecast quality * reliability: average match of predicted and actual distributions across time (at some horizon) * sharpness: average central interval width over time (at some horizon, can't be calibrated) * resolution: variance of central intervals (at some horizon, can't be calibrated) * skill: a single number, not clear what it is * 3 distribution forecasts (based on 3 different point forecasts) have roughly same quality. One a little better. Profitability of bidding strategies based on point and dist. forecasts compared * around half of energy on Dutch markets is traded in regulation! (in this simulation) * bidding strategy -- point forecast: promise to deliver the expected value (point forecast) -- distribution forecast: Probabilistic Choice (PC), explained in reference 20 * bidding from distribution forecasts better than and point-based bidding strategy * most accurate distribution produces most profitable bidding * profit comes from underpromsing electricity, INCREASING bid/production error -- increases profit b/c on these markets, positive deviations more expensive than negative -- My comment: this is not so good: ---- underpromising increases the amt. of regulated energy. ---- Usually, more regulated means more society cost ---- fast reacting plants produce more C02 (I think) so this is worse for global warming!},
  timestamp = {2009.03.04},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Pinson06qualValProbFrcst.pdf}
}

@article{Aid15optTradeIntraday,
  title = {An Optimal Trading Problem in Intraday Electricity Markets},
  author = {A{\"i}d, Ren{\'e} and Gruet, Pierre and Pham, Huy{\^e}n},
  year = {2015},
  journal = {arXiv preprint arXiv:1501.04575},
  eprint = {1501.04575},
  url = {http://arxiv.org/abs/1501.04575},
  abstract = {We consider the problem of optimal trading for a power producer in the context of intraday electricity markets. The aim is to minimize the imbalance cost induced by the random residual demand in electricity, i.e. the consumption from the clients minus the production from renewable energy. For a simple linear price impact model and a quadratic criterion, we explicitly obtain approximate optimal strategies in the intraday market and thermal power generation, and exhibit some remarkable properties of the trading rate. Furthermore, we study the case when there are jumps on the demand forecast and on the intraday price, typically due to error in the prediction of wind power generation. Finally, we solve the problem when taking into account delay constraints in thermal power production. Key words: Optimal trading, power plant, intraday electricity markets, renewable energy, market impact, linear-quadratic control problem, jumps, delay.},
  archiveprefix = {arXiv},
  owner = {sotterson},
  annotation = {48 citations (Semantic Scholar/arXiv) [2023-07-25]},
  note = {Optimal trading on EEX intraday market. Was eventually published in Jan 2016 here: http://link.springer.com/article/10.1007\%2Fs11579-015-0150-8},
  timestamp = {2015.03.31},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Aid15optTradeIntraday.pdf}
}

@article{Ortega09scenarioDyn,
  title = {A Dynamic Approach for Scenario Generation in {{Risk}} Management},
  author = {Ortega, Juan-Pablo and Pullirsch, Rainer and Teichmann, Josef and Wergieluk, Julian},
  year = {2009},
  month = aug,
  url = {http://arxiv.org/abs/0904.0624v2},
  abstract = {We provide a new dynamic approach to scenario generation for the purposes of risk management in the banking industry. We connect ideas from conventional techniques -- like historical and Monte Carlo simulation -- and we come up with a hybrid method that shares the advantages of standard procedures but eliminates several of their drawbacks. Instead of considering the static problem of constructing one or ten day ahead distributions for vectors of risk factors, we embed the problem into a dynamic framework, where any time horizon can be consistently simulated. Additionally, we use standard models from mathematical finance for each risk factor, whence bridging the worlds of trading and risk management. Our approach is based on stochastic differential equations (SDEs), like the HJM-equation or the Black-Scholes equation, governing the time evolution of risk factors, on an empirical calibration method to the market for the chosen SDEs, and on an Euler scheme (or high-order schemes) for the numerical evaluation of the respective SDEs. The empirical calibration procedure presented in this paper can be seen as the SDE-counterpart of the so called Filtered Historical Simulation method; the behavior of volatility stems in our case out of the assumptions on the underlying SDEs. Furthermore, we are able to easily incorporate "middle-size" and "large-size" events within our framework always making a precise distinction between the information obtained from the market and the one coming from the necessary a-priori intuition of the risk manager. Results of one concrete implementation are provided.},
  owner = {scot},
  refid = {http://www.scientificcommons.org/43868052},
  annotation = {7 citations (Semantic Scholar/arXiv) [2023-07-25]},
  note = {arXiv:0904.0624v2 [q-fin.RM]
\par
spinning reservers},
  timestamp = {2010.11.24},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ortega09scenarioDyn.pdf}
}

@article{Lauret19VerificationSolarIrradiance,
  title = {Verification of Solar Irradiance Probabilistic Forecasts},
  author = {Lauret, Philippe and David, Mathieu and Pinson, Pierre},
  year = {2019},
  month = dec,
  journal = {Solar Energy},
  volume = {194},
  pages = {254--271},
  publisher = {Elsevier BV},
  doi = {10.1016/j.solener.2019.10.041},
  abstract = {We propose a framework for evaluating the quality of solar irradiance probabilistic forecasts. The verification framework is based on visual diagnostic tools and a set of scoring rules mostly originating from the weather forecast verification community. Two types of probabilistic forecasts are used as a basis to illustrate the ap- plication of these verification approaches. The first one consists in ensemble forecasts commonly provided by national or international meteorological centres. The second one originates from statistical methods and pro- duces a set of discrete quantile forecasts, the nominal proportions of which span the unit interval. These probabilistic forecasts are evaluated for two selected sites that experience very different climatic conditions. The first site is located in the continental US while the second one is situated on La R{\'e}union Island. Although visual diagnostic tools can help identify deficiencies in generated forecasts, it is recommended that a set of numerical scores be used to assess the quality of probabilistic forecasts. In particular, the Continuous Ranked Probability Score (CRPS) seems to have all the features needed to evaluate a probabilistic forecasting system and, as such, may become a standard for verifying solar irradiance probabilistic forecasts and by extension probabilistic forecasts of solar power generation.},
  annotation = {73 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Proposes CRPS, I think, for error metric. Also looks at forms of ignorance score. Vogt18multiDistLrnAnomDet compares DRNN prob forecasts using CRPS and ignorance, seems like ignorance is better?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lauret19VerificationSolarIrradiance.pdf}
}

@article{Zhang01multiWvltFrcst,
  title = {Multiresolution Forecasting for Futures Trading Using Wavelet Decompositions},
  author = {Zhang, B.-L. and Coggins, R. and Jabri, M.A. and Dersch, D. and Flower, B.},
  year = {2001},
  month = jul,
  journal = {Neural Networks, IEEE Transactions on},
  volume = {12},
  number = {4},
  pages = {765--775},
  issn = {1045-9227},
  doi = {10.1109/72.935090},
  abstract = {We investigate the effectiveness of a financial time-series forecasting strategy which exploits the multiresolution property of the wavelet transform. A financial series is decomposed into an over complete, shift invariant scale-related representation. In transform space, each individual wavelet series is modeled by a separate multilayer perceptron (MLP). We apply the Bayesian method of automatic relevance determination to choose short past windows (short-term history) for the inputs to the MLPs at lower scales and long past windows (long-term history) at higher scales. To form the overall forecast, the individual forecasts are then recombined by the linear reconstruction property of the inverse transform with the chosen autocorrelation shell representation, or by another perceptron which learns the weight of each scale in the prediction of the original time series. The forecast results are then passed to a money management system to generate trades},
  owner = {sotterson},
  annotation = {151 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Basic intro to wavelet frcsting, modwt and a trous arrangment. A 2005 tech report seems to be largely copied from it: Ahmad05multiWvltFrcst},
  timestamp = {2013.03.15},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhang01multiWvltFrcst.pdf}
}

@incollection{Bocharov13tseriesClustGeom,
  title = {Segmentation of Nonstationary Time Series with Geometric Clustering},
  booktitle = {Pattern Recognition - Applications and Methods},
  author = {Bocharov, Alexei and Thiesson, Bo},
  editor = {Latorre Carmona, Pedro and S{\'a}nchez, J. Salvador and Fred, Ana L.N.},
  year = {2013},
  series = {Advances in Intelligent Systems and Computing},
  volume = {204},
  pages = {93--107},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-642-36530-0_8},
  abstract = {We introduce a non-parametric method for segmentation in regimeswitching time-series models. The approach is based on spectral clustering of target-regressor tuples and derives a switching regression tree, where regime switches are modeled by oblique splits. Such models can be learned efficiently from data, where clustering is used to propose one single split candidate at each split level. We use the class of ART time series models to serve as illustration, but because of the non-parametric nature of our segmentation approach, it readily generalizes to a wide range of time-series models that go beyond the Gaussian error assumption in ART models. Experimental results on S and P 1500 financial trading data demonstrates dramatically improved predictive accuracy for the exemplifying ART models. Keywords: Regime-switching time series, Spectral clustering, Regression tree, Oblique split, Financial markets.},
  isbn = {978-3-642-36529-4},
  owner = {sotterson},
  timestamp = {2014.03.07},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bocharov13tseriesClustGeom.pdf}
}

@article{Eisenbrand09optMethFin_CrsNotes,
  title = {Optimization Methods in Finance Course Notes: {{Part}} 4 Mean-Variance Portfolio Theory},
  author = {Eisenbrand, Friedrich},
  year = {2009},
  month = oct,
  url = {http://disopt.epfl.ch/page-10555-en.html},
  howpublished = {Course Notes},
  note = {Financial optimzation course slides. Includes linear stochastic programming, portfolio optimization (or the start of it). Relevant for ReWP portfolio selection. Also general optimization and maybe optimal TSO trading in Eweline. The "variance of a sum" slides, "PART 4 MEAN-VARIANCE PORTFOLIO THEORY" give a simple way to compute the variance of any sum, given the pairwise covariances. Useful for ReWP pool selection: having only computed a small number of pairwise covariances, can compute the variance of the sum of any pool. Since forecasting accuracy improves with decreasing variance this is an approximate way to select a pool with the best forecastability. Could verify this by looking at the empirical results of EERA DTOC, and seeing if this approximation is true on that data. The idea would be: 1.) compute pairwise FORECAST ERROR covariances of all farms 2.) for a given power target compute all power sum combinations that produce enough power (at some quantile?) 3.) find the cost of power from all power sum combinations 4.) find the error variance for all combinations (computationally cheap) For all combos satisfying 2.) somehow start from lowest cost/lowest error variance combination. Possibly compute the real quantile forecast. If it fails to meet the power target, compute the sum forecast for the next best combination. Repeat until satisfied. Or something... is related to Jan's PhD thesis forecastability stuff, and to EERA DTOC. Or, alternatively, use the covariance matrix to cluster farms into "groups with the minimum summed covarariance" and do something else...}
}

@inproceedings{Kariniotakis08anemosWindForecast,
  title = {{{ANEMOS}} Leading {{European}} Union Research on Wind Power Forecasting},
  booktitle = {International Wind Forecast Techniques and Methodologies Workshop},
  author = {Kariniotakis, George},
  year = {2008},
  url = {http://www.bpa.gov/corporate/business/innovation/iwftm.cfm},
  owner = {sotterson},
  note = {ANEMOS history and technology overview: current and future directions, pretty good * explanation of who is in ANEMOS and history * for both onshore and offshore New models they've developed * power curve * upscaling of regional forecasts * prediction combination * NWP filtering * online uncertainty estimation -- adapted resampling -- quantile regression -- prediction risk indices * handling incomplete information Modeling Techniques * Neural and fuzzy nets * regression * Kalman, etc. Change from ACCURACY driven forecasts to VALUE driven * accuracy: RMS, etc. * value: money, risk? They don't say Move to Optimal Power System Management * want to develop tools based on stochastic paradigm * uses -- reserve est. -- congestion mgmt -- scheduling -- storage -- trading Safewind: * focus on extreme event prediction},
  timestamp = {2008.09.19}
}

@book{Jemberie04infoTheoryHydro,
  title = {Information Theory and Artificial Intelligence to Manage Uncertainty in Hydrodynamic and Hydrological Models},
  author = {Jemberie, Abebe ANDUALEM},
  year = {2004},
  publisher = {Taylor \& Francis Ltd},
  url = {http://books.google.com/books?id=eoSwda604XYC&printsec=frontcover&source=gbs_summary_r&cad=0},
  owner = {sotterson},
  note = {Combined physical and data driven models. Neural Nets, etc. capture model error statistics. Looks applicable to wind too.},
  timestamp = {2008.07.03}
}

@article{Hodge16IntroductionWindSolar,
  title = {An Introduction to Wind and Solar Power Forecasting},
  author = {Hodge, Bri-Mathias},
  year = {2016},
  journal = {Clean Energy Solutions Center: Greening the Grid.},
  url = {https://cleanenergysolutions.org/sites/default/files/documents/greening-the-grid-webinar-intro-to-re-forecasting-21april2016.pdf},
  owner = {sotterson},
  note = {Developing country solar forecasting training slides from NREL. Wind, Solar, value of forecasts.},
  timestamp = {2017.04.13},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hodge16IntroductionWindSolar.pdf}
}

@inproceedings{Ruiz12frcstUncertTrade,
  title = {Forecast Uncertainty and Trading Decision-Making},
  booktitle = {Charles River Associates},
  author = {Ruiz, Pablo A.},
  year = {2012},
  month = feb,
  publisher = {UWIG Workshop on Variable Generation Forecasting Applications},
  address = {Tucson, AZ, USA},
  owner = {sotterson},
  note = {Friendly consultant slides on use of prob. forecasts and scenarios in power systems. Would be worth tracking down this company to find published examples of its customers using such forecasts.},
  timestamp = {2013.10.01}
}

@article{Toyabe22stochDurIntradayLOB,
  title = {Stochastic {{Conditional Duration Model}} with {{Intraday Seasonality}} and {{Limit Order Book Information}}},
  author = {Toyabe, Tomoki and Nakatsuma, Teruo},
  year = {2022},
  month = oct,
  journal = {Journal of Risk and Financial Management},
  volume = {15},
  number = {10},
  pages = {470},
  issn = {1911-8074},
  doi = {10.3390/jrfm15100470},
  url = {https://www.mdpi.com/1911-8074/15/10/470},
  urldate = {2022-10-20},
  abstract = {It is a widely known fact that the intraday seasonality of trading intervals for financial transactions such as stocks is short at the beginning of business hours and long in the middle of the day. In this paper, we extend the stochastic conditional duration (SCD) model to capture the pattern of intraday trading intervals and propose a new Markov chain Monte Carlo method to estimate this intraday seasonality simultaneously. To efficiently generate the Monte Carlo sample, we used a hybrid of the Gibbs/Metropolis--Hastings (MH) sampling scheme and also applied generalized Gibbs sampling. In addition to capturing this intraday seasonality, this paper also considers limit order book information. Three-day tick data for three stocks obtained from Nikkei NEEDS are used for estimation, and model selection is performed on smooth parameters, Weibull distribution and Gamma distribution. The typical intraday regularity of frequent trading immediately after the start of trading is confirmed, and the spread of the limit order book information is also found to affect the trading time interval.},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Toyabe22stochDurIntradayLOB.pdf}
}

@article{Alkawaz22priceFrcstDAhybrid,
  title = {Day-{{Ahead Electricity Price Forecasting Based}} on {{Hybrid Regression Model}}},
  author = {Alkawaz, Ali Najem and Abdellatif, Abdallah and Kanesan, Jeevan and Khairuddin, Anis Salwa Mohd and Gheni, Hassan Muwafaq},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {108021--108033},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3213081},
  abstract = {Since the deregulation of the power markets, accurate short term Electricity Price Forecasting (EPF) has become crucial in maximizing economic benefits and mitigating power market risks. Due to the challenging characteristics of electricity price, which comprise high volatility, rapid spike, and seasonality, developing robust machine learning prediction tools becomes cumbersome. This work proposes a new hybrid machine learning method for a day-ahead EPF, which involves linear regression Automatic Relevance Determination (ARD) and ensemble bagging Extra Tree Regression (ETR) models. Considering that each model of EPF has its own strengths and weaknesses, combining several models gives more accurate predictions and overcomes the limitations of an individual model. Therefore, the linear ARD model is applied because it can efficiently deal with trend and seasonality variations; on the other hand, the ensemble ETR model is employed to learn from interactions, and thus combining ARD with ETR produces robust forecasting outcomes. The effectiveness of the proposed method was validated using a data set from the Nord Pool electricity market. The proposed model is compared with other models to demonstrate its superiority using performance matrices, such as Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). Experiment results show that the proposed method achieves lower forecasting errors than other individual and hybrid models. Additionally, a comparative study has been performed against previous works, where forecasting measurement of the proposed method outperforms previous works' accuracy in forecasting electricity price.},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Alkawaz22priceFrcstDAhybrid
\par
Multimodel via bagging. ~The Extra Tree Model (ETR) improves DAM spike forecasting e.g. on Nordpool (and I think others).},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Alkawaz22priceFrcstDAhybrid.pdf}
}

@article{Macedo22elecFlowRenewPriceSpain,
  title = {The Role of Electricity Flows and Renewable Electricity Production in the Behaviour of Electricity Prices in {{Spain}}},
  author = {Macedo, Daniela Pereira and Marques, Ant{\'o}nio Cardoso and Damette, Olivier},
  year = {2022},
  month = oct,
  journal = {Economic Analysis and Policy},
  issn = {0313-5926},
  doi = {10.1016/j.eap.2022.10.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0313592622001679},
  urldate = {2022-10-20},
  abstract = {Electricity markets are becoming increasingly interconnected, in part, to deal with unpredictability and imbalances in electricity prices. This paper assesses how electricity flows and generation from wind and solar photovoltaic (PV) power impact the volatility and mean of day-ahead electricity prices in Spain. It finds there is a meritorder effect from wind and solar PV power at most times of the day, which varies in magnitude for each of the 24 h. Electricity production from wind power is also found to play a clear role in increasing price volatility at all hours. Electricity inflow is shown to decrease both the mean and volatility of the electricity prices. The volatility of electricity prices is revealed to be highly persistent, and thus prone to large transmission shocks. This volatility is also susceptible to new shocks, which may indicate a tendency by market participants to overreact to unexpected shocks in electricity prices, possibly due to the relatively small size of the Iberian electricity market.},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Macedo22elecFlowRenewPriceSpain.pdf}
}

@article{Bello16paramDensRecalElecPrice,
  title = {Parametric {{Density Recalibration}} of a {{Fundamental Market Model}} to {{Forecast Electricity Prices}}},
  author = {Bello, Antonio and Bunn, Derek and Reneses, Javier and Mu{\~n}oz, Antonio},
  year = {2016},
  month = nov,
  journal = {Energies},
  volume = {9},
  number = {11},
  pages = {959},
  issn = {1996-1073},
  doi = {10.3390/en9110959},
  url = {http://www.mdpi.com/1996-1073/9/11/959},
  urldate = {2022-10-28},
  abstract = {This paper proposes a new approach to hybrid forecasting methodology, characterized as the statistical recalibration of forecasts from fundamental market price formation models. Such hybrid methods based upon fundamentals are particularly appropriate to medium term forecasting and in this paper the application is to month-ahead, hourly prediction of electricity wholesale prices in Spain. The recalibration methodology is innovative in seeking to perform the recalibration into parametrically defined density functions. The density estimation method selects from a wide diversity of general four-parameter distributions to fit hourly spot prices, in which the first four moments are dynamically estimated as latent functions of the outputs from the fundamental model and several other plausible exogenous drivers. The proposed approach demonstrated its effectiveness against benchmark methods across the full range of percentiles of the price distribution and performed particularly well in the tails.},
  langid = {english},
  annotation = {21 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {/Users/scott/OneDrive/share/ref/zotero/tmp_papers/Bello16paramDensRecalElecPrice.pdf}
}

@misc{Shao17TailDependenceModification,
  type = {{{SSRN Scholarly Paper}}},
  title = {Tail {{Dependence Modification}} of the {{Gaussian Copula}}: {{A Distorted Mix Method}}},
  shorttitle = {Tail {{Dependence Modification}} of the {{Gaussian Copula}}},
  author = {Shao, Hui},
  year = {2017},
  month = apr,
  number = {2956293},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.2956293},
  url = {https://papers.ssrn.com/abstract=2956293},
  urldate = {2022-10-29},
  abstract = {The inability of the Gaussian copula to describe the tail dependence among risks has been sharply criticized since the 2008 financial crisis. In this paper, we extend the classic Gaussian copula to the distorted GAB copula with the distorted mix method. Similar with the Gaussian copula, the distorted GAB copula can be uniquely determined by all its two-dimensional marginal copulas, which provides a great convenience for high-dimensional statistical estimations in practice, and at the same time, the distorted GAB copula is able to describe the tail dependence.},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Shao17TailDependenceModification.pdf}
}

@article{Soares18ActiveDistributionGrid,
  title = {Active {{Distribution Grid Management Based}} on {{Robust AC Optimal Power Flow}}},
  author = {Soares, Tiago and Bessa, Ricardo J. and Pinson, Pierre and Morais, Hugo},
  year = {2018},
  month = nov,
  journal = {IEEE Transactions on Smart Grid},
  volume = {9},
  number = {6},
  pages = {6229--6241},
  issn = {1949-3061},
  doi = {10.1109/TSG.2017.2707065},
  abstract = {Further integration of distributed renewable energy sources in distribution systems requires a paradigm change in grid management by the distribution system operators (DSOs). DSOs are currently moving to an operational planning approach based on activating flexibility from distributed energy resources in day/hour-ahead stages. This paper follows the DSO trends by proposing a methodology for active grid management by which robust optimization is applied to accommodate spatial-temporal uncertainty. The proposed method entails the use of a multi-period AC-OPF, ensuring a reliable solution for the DSO. Wind and PV uncertainty is modeled based on spatial-temporal trajectories, while a convex hull technique to define uncertainty sets for the model is used. A case study based on real generation data allows illustration and discussion of the properties of the model. An important conclusion is that the method allows the DSO to increase system reliability in the real-time operation. However, the computational effort grows with increases in system robustness.},
  annotation = {65 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Use of scenarios for DSO power flow optimization. Related to net Demand forecast scenarios, since net demand is what flows. Uses Scenario Generator in: Pinson09probFrcstStatScenWind},
  file = {/Users/scott/OneDrive/share/ref/zotero/tmp_papers/Soares18ActiveDistributionGrid.pdf}
}

@misc{Awadalla22InfluenceStochasticDependence,
  title = {Influence of {{Stochastic Dependence}} on {{Network Constraints Screening}} for {{Unit Commitment}}},
  author = {Awadalla, Mohamed and Bouffard, Fran{\c c}ois},
  year = {2022},
  month = aug,
  number = {arXiv:2208.12139},
  eprint = {2208.12139},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2208.12139},
  urldate = {2022-11-03},
  abstract = {The deepening penetration of renewable energy is challenging how power system operators cope with the associated variability and uncertainty in the unit commitment problem. Given its computational complexity, several optimization-based methods have been proposed to lighten the full unit commitment formulation by removing redundant line flow constraints. These approaches often ignore the spatial couplings of multi-side renewable generation and demand. To address this pitfall, we rule out redundant constraints over a tightened linear programming relaxation of the original unit commitment feasibility region by adding a constraint that efficiently models the correlation of residual demand variations. We set forth a novel, tractable and robust polyhedral uncertainty envelope induced by a given set of scenarios to characterize the tightening constraint. We propose a data-driven umbrella constraint discovery problem formulation that substantially increase the network constraints filtration in unit commitment. Numerical tests are performed on standard IEEE test networks to substantiate the effectiveness of the approach.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/arXiv) [2023-07-25]},
  note = {Comment: In proceedings of the 11th Bulk Power Systems Dynamics and Control Symposium (IREP 2022), July 25-30, 2022, Banff, Canada},
  file = {/Users/scott/OneDrive/share/ref/zotero/tmp_papers/Awadalla22InfluenceStochasticDependence.pdf}
}

@article{Tan20EstimatingRobustPQ,
  title = {Estimating the {{Robust P-Q Capability}} of a {{Technical Virtual Power Plant Under Uncertainties}}},
  author = {Tan, Zhenfei and Zhong, Haiwang and Xia, Qing and Kang, Chongqing and Wang, Xuanyuan Sharon and Tang, Honghai},
  year = {2020},
  month = nov,
  journal = {IEEE Transactions on Power Systems},
  volume = {35},
  number = {6},
  pages = {4285--4296},
  issn = {0885-8950, 1558-0679},
  doi = {10.1109/TPWRS.2020.2988069},
  url = {https://ieeexplore.ieee.org/document/9072317/},
  urldate = {2022-11-03},
  abstract = {The technical virtual power plant (TVPP) is a promising paradigm to facilitate the integration of distributed energy resources (DERs) while incorporating operational constraints of both DERs and networks. Due to the volatility and limited predictability of DER generation and electric loads, the output capability of the TVPP is uncertain. In this regard, this paper proposes the robust capability curve (RCC) of the TVPP, which explicitly characterizes the allowable range of the scheduled power output that is executable for the TVPP under uncertainties. Implementing the RCC can secure the scheduling of the TVPP against unexpected fluctuations of operating conditions when the TVPP participates in the transmission-level dispatch. Mathematically, the RCC is the first-stage feasible set of an adjustable robust optimization problem. An uncertainty set model incorporating the variable correlation and uncertainty budget is employed, which makes the robustness and conservatism of the RCC adjustable. A novel methodology is proposed to estimate the RCC by the convex hull of several points on its perimeter. These perimeter points are obtained by solving a series of multi scenario-optimal power flow problems with worst-case uncertainty realizations identified based on a linearized network configuration. Case studies based on the IEEE-13 test feeder validate the effectiveness of the RCC to ensure the scheduling feasibility while hedging against uncertainties. The computational efficiency of the proposed RCC estimation method is also verified based on larger-scale test systems.},
  langid = {english},
  annotation = {33 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Tan20EstimatingRobustPQ.pdf}
}

@article{Lorca17multiRobustDynUncertSet,
  title = {Multistage {{Robust Unit Commitment With Dynamic Uncertainty Sets}} and {{Energy Storage}}},
  author = {Lorca, {\'A}lvaro and Sun, Xu Andy},
  year = {2017},
  month = may,
  journal = {IEEE Transactions on Power Systems},
  volume = {32},
  number = {3},
  pages = {1678--1688},
  issn = {1558-0679},
  doi = {10.1109/TPWRS.2016.2593422},
  abstract = {The deep penetration of wind and solar power is a critical component of the future power grid. However, the intermittency and stochasticity of these renewable resources bring significant challenges to the reliable and economic operation of power systems. Motivated by these challenges, we present a multistage adaptive robust optimization model for the unit commitment (UC) problem, which models the sequential nature of the dispatch process and utilizes a new type of dynamic uncertainty sets to capture the temporal and spatial correlations of wind and solar power. The model also considers the operation of energy storage devices. We propose a simplified and effective affine policy for dispatch decisions, and develop an efficient algorithmic framework using a combination of constraint generation and duality-based reformulation with various improvements. Extensive computational experiments show that the proposed method can efficiently solve multistage robust UC problems on the Polish 2736-bus system under high dimensional uncertainty of 60 wind farms and 30 solar farms. The computational results also suggest that the proposed model leads to significant benefits in both costs and reliability over robust models with traditional uncertainty sets as well as deterministic models with reserve rules.},
  annotation = {166 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {/Users/scott/OneDrive/share/ref/zotero/tmp_papers/Lorca17MultistageRobustUnit.pdf}
}

@article{Bouallegue16scenGenEnsCopulaCpl,
  title = {Generation of {{Scenarios}} from {{Calibrated Ensemble Forecasts}} with a {{Dual-Ensemble Copula-Coupling Approach}}},
  author = {Bouall{\`e}gue, Zied Ben and Heppelmann, Tobias and Theis, Susanne E. and Pinson, Pierre},
  year = {2016},
  month = dec,
  journal = {Monthly Weather Review},
  volume = {144},
  number = {12},
  pages = {4737--4750},
  publisher = {American Meteorological Society},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/MWR-D-15-0403.1},
  url = {https://journals.ametsoc.org/view/journals/mwre/144/12/mwr-d-15-0403.1.xml},
  urldate = {2022-11-17},
  abstract = {Abstract Probabilistic forecasts in the form of ensembles of scenarios are required for complex decision-making processes. Ensemble forecasting systems provide such products but the spatiotemporal structures of the forecast uncertainty is lost when statistical calibration of the ensemble forecasts is applied for each lead time and location independently. Nonparametric approaches allow the reconstruction of spatiotemporal joint probability distributions at a small computational cost. For example, the ensemble copula coupling (ECC) method rebuilds the multivariate aspect of the forecast from the original ensemble forecasts. Based on the assumption of error stationarity, parametric methods aim to fully describe the forecast dependence structures. In this study, the concept of ECC is combined with past data statistics in order to account for the autocorrelation of the forecast error. The new approach, called d-ECC, is applied to wind forecasts from the high-resolution Consortium for Small-Scale Modeling (COSMO) ensemble prediction system (EPS) run operationally at the German Weather Service (COSMO-DE-EPS). Scenarios generated by ECC and d-ECC are compared and assessed in the form of time series by means of multivariate verification tools and within a product-oriented framework. Verification results over a 3-month period show that the innovative method d-ECC performs as well as or even outperforms ECC in all investigated aspects.},
  chapter = {Monthly Weather Review},
  langid = {english},
  annotation = {29 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {/Users/scott/OneDrive/share/ref/zotero/tmp_papers/Bouallegue16GenerationScenariosCalibrateda.pdf}
}

@article{Kudryashova22ParametricCopulaGPModel,
  title = {Parametric {{Copula-GP}} Model for Analyzing Multidimensional Neuronal and Behavioral Relationships},
  author = {Kudryashova, Nina and Amvrosiadis, Theoklitos and Dupuy, Nathalie and Rochefort, Nathalie and Onken, Arno},
  year = {2022},
  month = jan,
  journal = {PLoS Computational Biology},
  volume = {18},
  number = {1},
  pages = {e1009799},
  issn = {1553-734X},
  doi = {10.1371/journal.pcbi.1009799},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8827448/},
  urldate = {2022-11-15},
  abstract = {One of the main goals of current systems neuroscience is to understand how neuronal populations integrate sensory information to inform behavior. However, estimating stimulus or behavioral information that is encoded in high-dimensional neuronal populations is challenging. We propose a method based on parametric copulas which allows modeling joint distributions of neuronal and behavioral variables characterized by different statistics and timescales. To account for temporal or spatial changes in dependencies between variables, we model varying copula parameters by means of Gaussian Processes (GP). We validate the resulting Copula-GP framework on synthetic data and on neuronal and behavioral recordings obtained in awake mice. We show that the use of a parametric description of the high-dimensional dependence structure in our method provides better accuracy in mutual information estimation in higher dimensions compared to other non-parametric methods. Moreover, by quantifying the redundancy between neuronal and behavioral variables, our model exposed the location of the reward zone in an unsupervised manner (i.e., without using any explicit cues about the task structure). These results demonstrate that the Copula-GP framework is particularly useful for the analysis of complex multidimensional relationships between neuronal, sensory and behavioral variables., Understanding the relationship between a set of variables is a common problem in many fields, such as weather forecast or stock market data. In neuroscience, one of the main challenges is to characterize the dependencies between neuronal activity, sensory stimuli and behavioral outputs. A method of choice for modeling such statistical dependencies is based on copulas, which disentangle dependencies from single variable statistics. To account for changes in dependencies, we model changes in copula parameters by means of Gaussian Processes, conditioned on a task-related variable. The novelty of our approach includes 1) explicit modeling of the dependencies; and 2) combining different copulas to describe experimentally observed variability. We validate the goodness-of-fit as well as information estimates on synthetic data and on recordings from the visual cortex of mice performing a behavioral task. Our parametric model demonstrates significantly better performance in describing high dimensional dependencies compared to other commonly used techniques. We demonstrate that our model can estimate information and predict behaviorally-relevant parameters of the task without providing any explicit cues to the model. Our results indicate that our model is interpretable in the context of neuroscience applications, scalable to large datasets and suitable for accurate statistical modeling and information estimation.},
  pmcid = {PMC8827448},
  pmid = {35089913},
  annotation = {3 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {/Users/scott/OneDrive/share/ref/zotero/tmp_papers/Kudryashova22ParametricCopulaGPModel.pdf}
}

@phdthesis{Loesch22prosumFlexSpotBalMkt,
  title = {Utilization of {{Electric Prosumer Flexibility Incentivized}} by {{Spot}} and {{Balancing Markets}}},
  author = {L{\"o}sch, Manuel},
  year = {2022},
  address = {Karlsruhe, DE},
  doi = {10.5445/IR/1000152126},
  url = {https://publikationen.bibliothek.kit.edu/1000152126},
  urldate = {2022-12-21},
  abstract = {The use of energy flexibility to balance electricity demand and supply is becoming increasingly important due to the growing share of fluctuating energy sources. Electric flexibility regarding time or magnitude of consumption can be offered in the form of different products on electricity spot and balancing power markets. In the wake of the energy transition and because of new possibilities provided by digitalization, the decision intervals on these markets are becoming shorter and the controllability of electricity consumption and generation more small-scale. This evolution opens up new chances for formerly passive energy consumers. This thesis shows how electric flexibility can be monetized using the application example of commercial sites. These are often multimodal energy systems coupling electricity, heat, and gas, and thus deliver high flexibility potential. To leverage this potential, a comprehensive picture of demand-side flexibilization is provided and used to propose an energy management system and optimization for cost-optimized device schedules. The cost-optimization considers two simultaneous incentives: variable day-ahead spot market prices and revenues for offering possible schedule adjustments to the automatic Frequency Restoration Reserve (aFRR) balancing market. To solve the formulated optimization problem, a genetic algorithm is presented, tailored to the specific needs of consumers. In addition to addressing the trade-off between the two competing markets, the algorithm inherently considers the uncertain activation of aFRR bids and related catch-up effects. An analysis of the activation behavior of aFRR balancing market bids, based on a developed ex-post simulation, forms an important decision basis for the optimization. Finally, a simulation study concentrating on battery energy storage systems and combined heat and power plants on the consumer side enables the quantitative discussion of the optimization potential. The results show that consumers considering both markets simultaneously can achieve cost benefits that are up to multiples of those for pure day-ahead price optimization, despite the stochastic nature of aFRR balancing power activations. In conclusion, this thesis enables formerly passive electricity consumers to assume the role of alternative balancing service providers, hence contributing to the economic and reliable operation of power grids characterized by a high share of renewable energy sources.},
  langid = {english},
  school = {Karlsruher Institut f{\"u}r Technologie (KIT)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Loesch22prosumFlexSpotBalMkt.pdf}
}

@phdthesis{Bertrand21scarcityPriceFlexGenMkt,
  title = {Enhancing the Value of Flexible Generation Capacity through Continuous Intraday Markets and Scarcity Pricing},
  author = {Bertrand, Gilles},
  year = {2021},
  url = {https://dial.uclouvain.be/pr/boreal/object/boreal:250192},
  urldate = {2022-12-21},
  abstract = {The increasing integration of intermittent renewable production requires more flexible as- sets in power systems. There are two potential paths in order to increase the remuneration of these flexible assets. The first one is to improve trading strategies in existing markets. The second one is to introduce new market mechanisms. This thesis studies both aspects by developing trading strategies for the Continuous Intraday Market, and by analyzing dif- ferent options for implementing a ``scarcity pricing mechanism'' in the European electricity market design. The contributions of the dissertation are organized in three chapters. Chapter 2 presents a method for trading the production of a storage unit in the Continuous In- traday Market. We model this problem in the Markov Decision Process framework. We present an approach based on Policy Function Approximation for tackling the problem. We provide relevant parameters for defining our policy, and demonstrate the effectiveness of our approach by comparing it to a method commonly employed in the industry on real historical data. In chapter 3, we are interested in the problem of a renewable unit covering its position in the Continuous Intraday Market. As a starting point for tackling this problem, we characterize an optimal policy for trading a fixed quantity in a simplified version of the Continuous Intraday Market. We use this analytical solution as a basis for developing a Value Function Approximation algorithm and an alternative Stochastic Dual Dynamic Programming algorithm that can trade under a more realistic set of assumptions. Chapter 4 proposes a methodology for analysing different market design options for imple- menting scarcity pricing in the European markets. The methodology relies on analytical insights that can be derived under an assumption of price-taking behavior. These insights are validated by a simulation model which represents the European balancing market as a Markov Decision Process. Our results highlight the benefit of introducing a market in the European market design for trading balancing capacity in real time.},
  langid = {english},
  school = {UCL - Universit{\'e} Catholique de Louvain},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bertrand21scarcityPriceFlexGenMkt.pdf}
}

@article{Kuppelwieser21liquidCostsPowMktcontinAuct,
  title = {Liquidity Costs on Intraday Power Markets: {{Continuous}} Trading versus Auctions},
  shorttitle = {Liquidity Costs on Intraday Power Markets},
  author = {Kuppelwieser, Thomas and Wozabal, David},
  year = {2021},
  month = jul,
  journal = {Energy Policy},
  volume = {154},
  pages = {112299},
  issn = {0301-4215},
  doi = {10.1016/j.enpol.2021.112299},
  url = {https://www.sciencedirect.com/science/article/pii/S0301421521001683},
  urldate = {2022-12-21},
  abstract = {We analyze liquidity costs on continuous and auction-based intraday power markets using a cost-of-round-trip measure that works for both market designs. We use data from the Italian auction-based intraday market and the German continuous market and present descriptive statistics as well as multivariate regression models to analyze determinants of liquidity costs in both markets. To test for differences in liquidity due to market design, we employ a double machine learning technique controlling for several confounding variables. We show that weekly patterns, yearly seasonality, electricity demand, as well as the influence of temperatures significantly affect liquidity costs. Comparing liquidity costs in both market, we find that, overall, liquidity costs are lower on the Italian market. However, Italian costs increase towards later auctions, while the costs on the German continuous intraday market decrease and reach their low close to physical delivery, where costs are lower than on the last Italian market trading the corresponding products.},
  langid = {english},
  annotation = {4 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kuppelwieser21liquidCostsPowMktcontinAuct.pdf}
}

@article{Lehna22ReinfLrnTradeDEwind,
  title = {A {{Reinforcement Learning}} Approach for the Continuous Electricity Market of {{Germany}}: {{Trading}} from the Perspective of a Wind Park Operator},
  shorttitle = {A {{Reinforcement Learning}} Approach for the Continuous Electricity Market of {{Germany}}},
  author = {Lehna, Malte and Hoppmann, Bj{\"o}rn and Scholz, Christoph and Heinrich, Ren{\'e}},
  year = {2022},
  month = may,
  journal = {Energy and AI},
  volume = {8},
  pages = {100139},
  issn = {2666-5468},
  doi = {10.1016/j.egyai.2022.100139},
  url = {https://www.sciencedirect.com/science/article/pii/S2666546822000039},
  urldate = {2022-12-21},
  abstract = {With the rising extension of renewable energies, the intraday electricity markets have recorded a growing popularity amongst traders as well as electric utilities to cope with the induced volatility of the energy supply. Through their short trading horizon and continuous nature, the intraday markets offer the ability to adjust trading decisions from the day-ahead market or reduce trading risk in a short-term notice. Producers of renewable energies utilize the intraday market to lower their forecast risk, by modifying their provided capacities based on current forecasts. However, the market dynamics are complex due to the fact that the power grids have to remain stable and electricity is only partly storable. Consequently, robust and intelligent trading strategies are required that are capable to operate in the intraday market. In this work, we propose a novel autonomous trading approach based on Deep Reinforcement Learning (DRL) algorithms as a possible solution. For this purpose, we model the intraday trade as a Markov Decision Process (MDP) and employ the Proximal Policy Optimization (PPO) algorithm as our DRL approach. A simulation framework is introduced that enables the trading of the continuous intraday price in a resolution of one minute steps. We test our framework in a case study from the perspective of a wind park operator. We include next to general trade information both price and wind forecasts. On a test scenario of German intraday trading results from 2018, we are able to outperform multiple baselines with at least 45.24\% improvement, showing the advantage of the DRL algorithm. However, we also discuss limitations and enhancements of the DRL agent, in order to increase the performance in future works.},
  langid = {english},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lehna22ReinfLrnTradeDEwind.pdf}
}

@article{Kuppelwieser22weathFrcstRacePowTrade,
  title = {Intraday Power Trading: Toward an Arms Race in Weather Forecasting?},
  shorttitle = {Intraday Power Trading},
  author = {Kuppelwieser, Thomas and Wozabal, David},
  year = {2022},
  month = nov,
  journal = {OR Spectrum},
  issn = {1436-6304},
  doi = {10.1007/s00291-022-00698-5},
  url = {https://doi.org/10.1007/s00291-022-00698-5},
  urldate = {2022-12-21},
  abstract = {We propose the first speculative weather-based algorithmic trading strategy on a continuous intraday power market. The strategy uses neither production assets nor power demand and generates profits purely based on superior information about aggregate output of weather-dependent renewable production. We use an optimized parametric policy based on state-of-the-art intraday updates of renewable production forecasts and evaluate the resulting decisions out-of-sample for one year of trading based on detailed order book level data for the German market. Our strategies yield significant positive profits, which suggests that intraday power markets are not semi-strong efficient. Furthermore, sizable additional profits could be made using improved forecasts of renewable output, which implies that the quality of forecasts is an important factor for profitable trading strategies. This has the potential to trigger an arms race for more frequent and more accurate forecasts, which would likely lead to increased market efficiency, more reliable price signals, and more liquidity.},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {/Users/scott/OneDrive/share/ref/zotero/tmp_papers/Kuppelwieser22intradayTradeWeathFrcst.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Kuppelwieser22weathFrcstRacePowTrade.pdf}
}

@article{Baule22flexShortCertTradeIntraMkd,
  title = {Flexible {{Short-Term Electricity Certificates}}---{{An Analysis}} of {{Trading Strategies}} on the {{Continuous Intraday Market}}},
  author = {Baule, Rainer and Naumann, Michael},
  year = {2022},
  month = jan,
  journal = {Energies},
  volume = {15},
  number = {17},
  pages = {6344},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1996-1073},
  doi = {10.3390/en15176344},
  url = {https://www.mdpi.com/1996-1073/15/17/6344},
  urldate = {2022-12-21},
  abstract = {The most important price for short-term electricity trading in Germany is the day-ahead auction price, which is provided by EPEX SPOT. Basically, short-term fluctuating electricity prices allow cost-optimized production planning by shifting electricity-intensive processes to times of favorable electricity prices. However, the day-ahead price as the outcome of an auction is not directly tradeable afterwards. We propose short-term flexible electricity certificates that pass on the day-ahead auction prices plus a premium for the supplier, enabling users to plan electricity consumption based on realized day-ahead auction prices. We analyze the supplier's problem of delivering electricity based on such certificates. The supplier can adjust the required electricity volume after the close of the day-ahead auction on the continuous intraday market. We analyze the price fluctuations in this market in relation to the day-ahead price and propose different trading strategies. Using the order book history of EPEX SPOT, we analyze the trading success and risk of these strategies. Furthermore, we investigate to what extent trading success can be explained by changes in market conditions, and, in particular, we identify renewable forecast errors as a driver.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Baule22flexShortCertTradeIntraMkd.pdf}
}

@article{Subramanya21vppAggPVnEurFreq,
  title = {A {{Virtual Power Plant Solution}} for {{Aggregating Photovoltaic Systems}} and {{Other Distributed Energy Resources}} for {{Northern European Primary Frequency Reserves}}},
  author = {Subramanya, Rakshith and {Yli-Ojanper{\"a}}, Matti and Sierla, Seppo and H{\"o}ltt{\"a}, Taneli and Valtakari, Jori and Vyatkin, Valeriy},
  year = {2021},
  month = jan,
  journal = {Energies},
  volume = {14},
  number = {5},
  pages = {1242},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1996-1073},
  doi = {10.3390/en14051242},
  url = {https://www.mdpi.com/1996-1073/14/5/1242},
  urldate = {2022-12-21},
  abstract = {Primary frequency reserves in Northern Europe have traditionally been provided with hydro plants and fossil fuel-burning spinning reserves. Recently, smart distributed energy resources have been equipped with functionality needed to participate on frequency reserves. Key categories of such resources include photovoltaic systems, batteries, and smart loads. Most of these resources are small and cannot provide the minimum controllable power required to participate on frequency reserves. Thus, virtual power plants have been used to aggregate the resources and trade them on the frequency reserves markets. The information technology aspects of virtual power plants are proprietary and many of the details have not been made public. The first contribution of this article is to propose a generic data model and application programming interface for a virtual power plant with the above-mentioned capabilities. The second contribution is to use the application programming interface to cope with the unpredictability of the frequency reserve capacity that the photovoltaic systems and other distributed energy resources are able to provide to the frequency reserves markets in the upcoming bidding period. The contributions are demonstrated with an operational virtual power plant installation at a Northern European shopping center, aggregating photovoltaic Primary Frequency Reserves resources.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  annotation = {10 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Subramanya21vppAggPVnEurFreq.pdf}
}

@article{Candra18econOptDEmktVPP,
  title = {Economic {{Optimal Implementation}} of {{Virtual Power Plants}} in the {{German Power Market}}},
  author = {Candra, Dodiek Ika and Hartmann, Kilian and Nelles, Michael},
  year = {2018},
  month = sep,
  journal = {Energies},
  volume = {11},
  number = {9},
  pages = {2365},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1996-1073},
  doi = {10.3390/en11092365},
  url = {https://www.mdpi.com/1996-1073/11/9/2365},
  urldate = {2022-12-21},
  abstract = {The burden of excess energy from the high renewable energy sources (RES) share creates a significant reduction of residual load for the future, resulting in reduced market prices. The higher the share of stochastic RES, the more often the price will be 0 {\texteuro}/MWh. The power market needs new methods to solve these problems. The development of virtual power plants (VPPs) is aimed at solving techno-economic problems with an increasing share of RES in the power market. This study analyses a possible implementation of stochastic and deterministic RES in a VPP to generate secured power, which can be implemented in the European Power Exchange (EPEX)/European Energy Exchange (EEX) power market using existing market products. In this study, the optimal economic VPP configuration for an RES-based power plant is investigated and implemented into standard power market products. The results show that the optimal economic VPP configuration for different market products varies, depending on the energy availability and the marginal costs of the VPP components. The size of the VPP components is positively correlated to the components' share of the energy generated. It was also found that projecting or implementing VPPs in Germany at current market prices (EPEX/EEX prices) is not yet economically feasible for a small share of market products. However, the secured power can be marketed on the SPOT and in the futures market with higher and more stable prices compared with the status quo.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  annotation = {20 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Candra18econOptDEmktVPP.pdf}
}

@misc{EPEX22basicsPowMkt,
  title = {Basics of the {{Power Market}} {\textbar} {{EPEX SPOT}}},
  author = {{EPEX}},
  url = {https://www.epexspot.com/en/basicspowermarket},
  urldate = {2022-12-21},
  abstract = {Efficient and liquid wholesale markets are a prerequisite for competitive retail markets, and hence an advantage for the final consumer. The larger supply and demand, the more relevant and competitive the price signal. Therefore, increasing the liquidity of the European market is a means to maximise social welfare for all European citizens.},
  file = {C:\Users\scott\Zotero\storage\QCQ4TRTC\basicspowermarket.html}
}

@article{Fridgen20alignPowTradeMkt,
  title = {The Search for the Perfect Match: {{Aligning}} Power-Trading Products to the Energy Transition},
  shorttitle = {The Search for the Perfect Match},
  author = {Fridgen, Gilbert and Michaelis, Anne and Rinck, Maximilian and Sch{\"o}pf, Michael and Weibelzahl, Martin},
  year = {2020},
  month = sep,
  journal = {Energy Policy},
  volume = {144},
  pages = {111523},
  issn = {0301-4215},
  doi = {10.1016/j.enpol.2020.111523},
  url = {https://www.sciencedirect.com/science/article/pii/S0301421520302688},
  urldate = {2022-12-21},
  abstract = {Given the growing share of uncertain renewable energy production, the energy transition challenges modern power systems and especially calls for increased flexibility. However, relevant information on the highly asset-specific flexibility potential is typically only known to plant operators themselves and not, e.g., to transmission system operators. Therefore, liberalized electricity markets use prices that set explicit monetary incentives to disclose the relevant private information about the market participants' assets. In this way, information asymmetries may be reduced. Given the different challenges of an integration of renewables, we argue that the associated new forms of volatile power profiles require new forms of power-trading products. In particular, based on recent advances in technical power measurement and billing, individual and market participant-specific power profiles may be superior to the current trading of average volumes. Against this background, we first outline various evolutionary adjustments of existing power-trading products and their underlying product parameters including (1) strengthening local pricing, (2) finer temporal granularity, (3) smaller minimum volume, and (4) shorter gate-closure time. Second, we open up a new perspective in form of a more disruptive shift towards power-profile trade, where market participants define their trading product using the actual power profile as a new product parameter.},
  langid = {english},
  annotation = {10 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Fridgen20alignPowTradeMkt.pdf}
}

@misc{EPEX22tradeProducts,
  title = {Trading {{Products}} {\textbar} {{EPEX SPOT}}},
  author = {{EPEX}},
  url = {https://www.epexspot.com/en/tradingproducts},
  urldate = {2022-12-21},
  abstract = {EPEX SPOT operates organized short-term electricity markets for Central Western Europe, Great Britain, the Nordics and Poland. We offer a large range of products, to trade across the entire value chain of electricity.},
  file = {C:\Users\scott\Zotero\storage\8BBDY56R\tradingproducts.html}
}

@techreport{ChunXiaBauer21bizModelVPPdeutschlnd,
  title = {Business {{Models}} of {{Virtual Power Plants}} ({{VPPs}}) in {{Germany}}},
  author = {{Chun Xia-Bauer} and {Felix Gro{\ss}e-Kreul} and {Stefan Thomas}},
  year = {2021},
  month = nov,
  pages = {19},
  address = {Berlin, Germany},
  institution = {Deutsche Gesellschaft f{\"u}r Internationale Zusammenarbeit (GIZ) GmbH},
  url = {https://www.energypartnership.cn/fileadmin/user_upload/china/media_elements/publications/Business_Models_of_Virtual_Power_Plants__VPPs__in_Germany-EN.pdf},
  abstract = {Germany aims to achieve climate neutrality by 2045.  Expanding renewable energy is a key pillar to achieve  this ambitious target, in which wind energy and PV will  play a dominant role. Given their intermittent charac- ter, the high penetration of wind and PV in the power  system imposes significant challenges for grid opera- tors and power markets to balance supply and demand.  At the same time, decarbonisation of all end use sectors  is associated with increasing electrification, which  would result in a substantial growth in gross electricity  demand and peak demand.  Thus, power system flexi- bility is key for achieving climate neutrality in Germany.  Virtual Power Plants (VPPs) have emerged as a promis- ing solution here. It is a collection of distributed energy  resources (DERs) that are centrally coordinated. DERs  consist of small- to medium- scale resources that are  connected mainly to the distribution grids or near the  end users, including distributed generation, demand- side resources, and energy storage. Despite their poten- tial of flexibility, they are often too small, scattered,  and/or their power generation is too fluctuating to di- rectly provide system service. A VPP, acting as an aggre- gator, will bundle these DERs into a sizable portfolio as  a single resource. It monitors, forecasts, optimizes, and  dispatches their generation, storage/release, or flexible  consumption through a central IT system to enable  them to participate in power markets and/or deliver the  flexibility to system operators.    The study 1) offers key insights about the development  of VPPs in Germany and its contribution to power sys- tem flexibility, through conducting case studies of VPP  business models and analysing the key power market  and regulatory framework, in which VPPs are embed- ded; 2) reflects on key enabling factors for VPPs in par- ticular, for its flexibility provision, in Germany.      What are the requirements for VPP busi- ness models?  {$\bullet$} Given the large pool of energy assets, VPP  needs an advanced ICT software platform, which  allows a large number of DERs to connect. The  platform should be able to capture data, enable  secure and fast communication between VPP,  individual assets, the grid operators, and elec- tricity market, and automatically regulate the  DERs to participate in electricity markets and  provide grid congestion service. It relies on ad- vanced forecasting algorithms to develop an op- timised dispatch schedule.  {$\bullet$} Hardware: Real-time data capture from, com- munication with, and controlling of DERs in  the portfolio requires smart meters, remote con- trol, and automation systems.   {$\bullet$} DERs in the portfolio: In order to increase rev- enue, it is important to build a diverse set of  DERs (generation, demand, and storage) and ex- plore business opportunities on different markets.  Biomass/ biogas power and hydropower plants  are considered as indispensable resources for  VPPs in Germany due to their high flexibility.  Small-scale DERs, such as BEVs, heat pumps,  home storage, rooftop PV, will play an increas- ingly important role in flexibility of the future  power system. Green hydrogen electrolysis  plants can also provide short- and long-term  flexibility similarly to biogas power plants.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\ChunXiaBauer21bizModelVPPdeutschlnd.pdf}
}

@article{Boano-Danquah20AnalysisExtremePeak,
  title = {Analysis of {{Extreme Peak Loads Using Point Processes}}: {{An Application Using South African Data}}},
  shorttitle = {Analysis of {{Extreme Peak Loads Using Point Processes}}},
  author = {{Boano-Danquah}, Jerry and Sigauke, Caston and Kyei, Kwabena A.},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {146105--146115},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3015259},
  abstract = {Extreme value modelling of peak load process is critical to the reliable specification of power generation, distribution and maintenance purposes during both peak and off-peak periods. In this study, a frequency assessment of extreme peak electricity demand for the four seasons of the year using South African data for the period, January 1997 to December 2013 is carried out. A point process approach from extreme value theory is proposed as an ingenious extreme value theory approach. The data are made stationary by using a time-varying threshold which has a positive shift factor. The non-linear detrended datasets are then grouped into summer, spring, winter and autumn according to the calendar dates in the Southern Hemisphere. The datasets were declustered to keep the series relatively independent using Ferro and Segers automatic declustering method. A stationary point process model is then fitted to each of the cluster maxima. The modelling framework, which is easily extensible to other peak load parameters, assumes that peak power follows a Poisson point process. The parameters of the developed model are estimated using the maximum likelihood method. Empirical results show that daily peak electricity demand could be experienced approximately 27, 16, 7 and 15 days per year in winter, spring, summer and autumn, respectively. The modelling approach could assist system operators of utility companies in scheduling maintenance of generating units including long term planning.},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Boano-Danquah20AnalysisExtremePeak.pdf}
}

@misc{Ying22MTGBMMultiTaskGradient,
  title = {{{MT-GBM}}: {{A Multi-Task Gradient Boosting Machine}} with {{Shared Decision Trees}}},
  shorttitle = {{{MT-GBM}}},
  author = {Ying, ZhenZhe and Xu, Zhuoer and Li, Zhifeng and Wang, Weiqiang and Meng, Changhua},
  year = {2022},
  month = jan,
  number = {arXiv:2201.06239},
  eprint = {2201.06239},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.06239},
  url = {http://arxiv.org/abs/2201.06239},
  urldate = {2023-01-20},
  abstract = {Despite the success of deep learning in computer vision and natural language processing, Gradient Boosted Decision Tree (GBDT) is yet one of the most powerful tools for applications with tabular data such as e-commerce and FinTech. However, applying GBDT to multi-task learning is still a challenge. Unlike deep models that can jointly learn a shared latent representation across multiple tasks, GBDT can hardly learn a shared tree structure. In this paper, we propose Multi-task Gradient Boosting Machine (MT-GBM), a GBDT-based method for multi-task learning. The MT-GBM can find the shared tree structures and split branches according to multi-task losses. First, it assigns multiple outputs to each leaf node. Next, it computes the gradient corresponding to each output (task). Then, we also propose an algorithm to combine the gradients of all tasks and update the tree. Finally, we apply MT-GBM to LightGBM. Experiments show that our MT-GBM improves the performance of the main task significantly, which means the proposed MT-GBM is efficient and effective.},
  archiveprefix = {arXiv},
  annotation = {1 citations (Semantic Scholar/arXiv) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ying22MTGBMMultiTaskGradient.pdf}
}

@article{Yukseltan20HourlyElectricityDemand,
  title = {Hourly Electricity Demand Forecasting Using {{Fourier}} Analysis with Feedback},
  author = {Yukseltan, Ergun and Yucekaya, Ahmet and Bilge, Ayse Humeyra},
  year = {2020},
  month = sep,
  journal = {Energy Strategy Reviews},
  volume = {31},
  pages = {100524},
  issn = {2211-467X},
  doi = {10.1016/j.esr.2020.100524},
  url = {https://www.sciencedirect.com/science/article/pii/S2211467X20300778},
  urldate = {2023-02-09},
  abstract = {Whether it be long-term, like year-ahead, or short-term, such as hour-ahead or day-ahead, forecasting of electricity demand is crucial for the success of deregulated electricity markets. The stochastic nature of the demand for electricity, along with parameters such as temperature, humidity, and work habits, eventually causes deviations from expected demand. In this paper, we propose a feedback-based forecasting methodology in which the hourly prediction by a Fourier series expansion is updated by using the error at the current hour for the forecast at the next hour. The proposed methodology is applied to the Turkish power market for the period 2012--2017 and provides a powerful tool to forecasts the demand in hourly, daily and yearly horizons using only the past demand data. The hourly forecasting errors in the demand, in the Mean Absolute Percentage Error (MAPE) norm, are 0.87\% in hour-ahead, 2.90\% in day-ahead, and 3.54\% in year-ahead horizons, respectively. An autoregressive (AR) model is also applied to the predictions by the Fourier series expansion to obtain slightly better results. As predictions are updated on an hourly basis using the already realized data for the current hour, the model can be considered as reliable and practical in circumstances needed to make bidding and dispatching decisions.},
  langid = {english},
  annotation = {25 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Yukseltan20HourlyElectricityDemand.pdf}
}

@article{Fontana19FunctionalDataAnalysis,
  title = {Functional {{Data Analysis}} of High-Frequency Load Curves Reveals Drivers of Residential Electricity Consumption},
  author = {Fontana, Matteo and Tavoni, Massimo and Vantini, Simone},
  year = {2019},
  month = jun,
  journal = {PLoS ONE},
  volume = {14},
  number = {6},
  pages = {e0218702},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0218702},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6592564/},
  urldate = {2023-02-09},
  abstract = {Smart energy meters generate real time, high frequency data which can foster demand management and response of consumers and firms, with potential private and social benefits. However, proper statistical techniques are needed to make sense of this large amount of data and translate them into usable recommendations. Here, we apply Functional Data Analysis (FDA), a novel branch of Statistics that analyses functions---to identify drivers of residential electricity load curves. We evaluate a real time feedback intervention which involved about 1000 Italian households for a period of three years. Results of the FDA modelling reveal, for the first time, daytime-indexed patterns of residential electricity consumption which depend on the ownership of specific clusters of electrical appliances and an overall reduction of consumption after the introduction of real time feedback, unrelated to appliance ownership characteristics.},
  pmcid = {PMC6592564},
  pmid = {31237923},
  annotation = {3 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Fontana19FunctionalDataAnalysis.pdf}
}

@inproceedings{Gaur16AnalysingElectricityDemand,
  title = {Analysing the Electricity Demand Pattern},
  booktitle = {2016 {{National Power Systems Conference}} ({{NPSC}})},
  author = {Gaur, Kajal and Kumar, Harish and Agarwal, Rathour P. K. and Baba, K. V. S. and Soonee, S. K.},
  year = {2016},
  month = dec,
  pages = {1--6},
  publisher = {IEEE},
  address = {Bhubaneswar, India},
  doi = {10.1109/NPSC.2016.7858969},
  url = {http://ieeexplore.ieee.org/document/7858969/},
  urldate = {2023-02-09},
  abstract = {The electricity demand of a nation speaks of its social standard, pace of economic growth, geographical variations and demography of the population at large. Understanding and forecasting of load characteristics have been complex due to its dependency on large number of factors which affects it i.e. weather condition, geographical diversity, sunrise/sunset times, seasonal diversity etc. The detailed study of the electricity consumption invokes a knowledge of its trend and seasonality which can be exploited to extrapolate the demand characteristics.},
  isbn = {978-1-4673-9968-5},
  langid = {english},
  annotation = {35 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gaur16AnalysingElectricityDemand.pdf}
}

@misc{Schuurmans23MemoryAugmentedLarge,
  title = {Memory {{Augmented Large Language Models}} Are {{Computationally Universal}}},
  author = {Schuurmans, Dale},
  year = {2023},
  month = jan,
  number = {arXiv:2301.04589},
  eprint = {2301.04589},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2301.04589},
  urldate = {2023-03-24},
  abstract = {We show that transformer-based large language models are computationally universal when augmented with an external memory. Any deterministic language model that conditions on strings of bounded length is equivalent to a finite automaton, hence computationally limited. However, augmenting such models with a read-write memory creates the possibility of processing arbitrarily large inputs and, potentially, simulating any algorithm. We establish that an existing large language model, Flan-U-PaLM 540B, can be combined with an associative read-write memory to exactly simulate the execution of a universal Turing machine, U15 2. A key aspect of the finding is that it does , not require any modification of the language model weights. Instead, the construction relies solely on designing a form of stored instruction computer that can subsequently be programmed with a specific set of prompts.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {15 citations (Semantic Scholar/arXiv) [2023-07-25]},
  note = {Comment: 23 pages, 0 figures},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Schuurmans23MemoryAugmentedLarge.pdf}
}

@inproceedings{Li19ZeroShotLearningIntrusion,
  title = {Zero-{{Shot Learning}} for {{Intrusion Detection}} via {{Attribute Representation}}},
  booktitle = {Neural {{Information Processing}}},
  author = {Li, Zhipeng and Qin, Zheng and Shen, Pengbo and Jiang, Liu},
  editor = {Gedeon, Tom and Wong, Kok Wai and Lee, Minho},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {352--364},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-36708-4_29},
  abstract = {Network intrusion detection is an important network security infrastructure. Although numerous studies based on machine learning have explored how to enable intrusion detection to detect unknown novel attack types, so called anomaly detection, little work focuses on using attribute learning methods. An important application of attribute learning is zero-shot learning, which can be used to solve the anomaly detection problem. In this paper, we propose an attribute learning method. A pipeline framework using random forest feature selection and DBSCAN clustering attribute conversion is introduced to convert raw network data into attributes. A comprehensive empirical evaluation demonstrates that our proposed framework sustains the data information effectively and outperforms the state-of-the-art approaches. An extra zero-shot learning experiment show that our attribute approach works well in zero-shot learning scenario.},
  isbn = {978-3-030-36708-4},
  langid = {english},
  keywords = {obsLitNote},
  annotation = {3 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Li19ZeroShotLearningIntrusion.pdf}
}

@misc{Galib22DeepExtremaDeepLearning,
  title = {{{DeepExtrema}}: {{A Deep Learning Approach}} for {{Forecasting Block Maxima}} in {{Time Series Data}}},
  shorttitle = {{{DeepExtrema}}},
  author = {Galib, Asadullah Hill and McDonald, Andrew and Wilson, Tyler and Luo, Lifeng and Tan, Pang-Ning},
  year = {2022},
  month = may,
  number = {arXiv:2205.02441},
  eprint = {2205.02441},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.02441},
  url = {http://arxiv.org/abs/2205.02441},
  urldate = {2023-03-24},
  abstract = {Accurate forecasting of extreme values in time series is critical due to the significant impact of extreme events on human and natural systems. This paper presents DeepExtrema, a novel framework that combines a deep neural network (DNN) with generalized extreme value (GEV) distribution to forecast the block maximum value of a time series. Implementing such a network is a challenge as the framework must preserve the inter-dependent constraints among the GEV model parameters even when the DNN is initialized. We describe our approach to address this challenge and present an architecture that enables both conditional mean and quantile prediction of the block maxima. The extensive experiments performed on both real-world and synthetic data demonstrated the superiority of DeepExtrema compared to other baseline methods.},
  archiveprefix = {arXiv},
  annotation = {2 citations (Semantic Scholar/arXiv) [2023-07-25]\\
2 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Comment: 7 pages, 6 figures, accepted to IJCAI'22},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Galib22DeepExtremaDeepLearning.pdf}
}

@book{JimenezVillalonga21UncoveringCorrelationsTwo,
  title = {Uncovering {{Correlations Between Two UMAP Hyperparameters}} and the {{Input Dataset}}},
  author = {Jimenez Villalonga, Federico},
  year = {2021},
  url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-306045},
  urldate = {2023-03-31},
  abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\JimenezVillalonga21UncoveringCorrelationsTwo.pdf}
}

@article{Schoen05newModelTHI,
  title = {A New Empirical Model of the Temperature--Humidity Index},
  author = {Schoen, Carl},
  year = {2005},
  journal = {Journal of Applied Meteorology and Climatology},
  volume = {44},
  number = {9},
  pages = {1413--1420},
  publisher = {American Meteorological Society},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Schoen05newModelTHI.pdf}
}

@article{Rothfusz90HeatIndexEquation,
  title = {The Heat Index Equation (or, More than You Ever Wanted to Know about Heat Index)},
  author = {Rothfusz, Lans P. and Headquarters, NWS Southern Region},
  year = {1990},
  journal = {Fort Worth, Texas: National Oceanic and Atmospheric Administration, National Weather Service, Office of Meteorology},
  volume = {9023},
  pages = {640},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Rothfusz90HeatIndexEquation.PDF}
}

@article{King01LogisticRegressionRare,
  title = {Logistic {{Regression}} in {{Rare Events Data}}},
  author = {King, Gary and Zeng, Langche},
  year = {2001},
  langid = {english},
  note = {It's OK to inflate the training data to include more rare events if you do a statistical correction afterwards. Maybe relaxes limit in: Peduzzi96numEVvarLogRgrssn Does this eliminate the need for exact? (King02logistRgrssnMLvsExact)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\King01LogisticRegressionRare.pdf}
}

@inproceedings{Pozzolo15CalibratingProbabilityUndersampling,
  title = {Calibrating {{Probability}} with {{Undersampling}} for {{Unbalanced Classification}}},
  booktitle = {2015 {{IEEE Symposium Series}} on {{Computational Intelligence}}},
  author = {Pozzolo, Andrea Dal and Caelen, Olivier and Johnson, Reid A. and Bontempi, Gianluca},
  year = {2015},
  month = dec,
  pages = {159--166},
  publisher = {IEEE},
  address = {Cape Town, South Africa},
  doi = {10.1109/SSCI.2015.33},
  url = {http://ieeexplore.ieee.org/document/7376606/},
  urldate = {2023-04-03},
  abstract = {Undersampling is a popular technique for unbalanced datasets to reduce the skew in class distributions. However, it is well-known that undersampling one class modifies the priors of the training set and consequently biases the posterior probabilities of a classifier [9]. In this paper, we study analytically and experimentally how undersampling affects the posterior probability of a machine learning model. We formalize the problem of undersampling and explore the relationship between conditional probability in the presence and absence of undersampling. Although the bias due to undersampling does not affect the ranking order returned by the posterior probability, it significantly impacts the classification accuracy and probability calibration. We use Bayes Minimum Risk theory to find the correct classification threshold and show how to adjust it after undersampling. Experiments on several real-world unbalanced datasets validate our results.},
  isbn = {978-1-4799-7560-0},
  langid = {english},
  annotation = {366 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Pozzolo15CalibratingProbabilityUndersampling.pdf}
}

@misc{Posocco21EstimatingExpectedCalibration,
  title = {Estimating {{Expected Calibration Errors}}},
  author = {Posocco, Nicolas and Bonnefoy, Antoine},
  year = {2021},
  month = sep,
  number = {arXiv:2109.03480},
  eprint = {2109.03480},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2109.03480},
  urldate = {2023-04-03},
  abstract = {Uncertainty in probabilistic classifiers predictions is a key concern when models are used to support human decision making, in broader probabilistic pipelines or when sensitive automatic decisions have to be taken. Studies have shown that most models are not intrinsically well calibrated, meaning that their decision scores are not consistent with posterior probabilities. Hence being able to calibrate these models, or enforce calibration while learning them, has regained interest in recent literature. In this context, properly assessing calibration is paramount to quantify new contributions tackling calibration. However, there is room for improvement for commonly used metrics and evaluation of calibration could benefit from deeper analyses. Thus this paper focuses on the empirical evaluation of calibration metrics in the context of classification. More specifically it evaluates different estimators of the Expected Calibration Error (ECE), amongst which legacy estimators and some novel ones, proposed in this paper. We build an empirical procedure to quantify the quality of these ECE estimators, and use it to decide which estimator should be used in practice for different settings.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {5 citations (Semantic Scholar/arXiv) [2023-07-25]},
  note = {Comment: 12 pages, 2 figures, ICANN2021},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Posocco21EstimatingExpectedCalibration.pdf}
}

@misc{Nixon20MeasuringCalibrationDeep,
  title = {Measuring {{Calibration}} in {{Deep Learning}}},
  author = {Nixon, Jeremy and Dusenberry, Mike and Jerfel, Ghassen and Nguyen, Timothy and Liu, Jeremiah and Zhang, Linchuan and Tran, Dustin},
  year = {2020},
  month = aug,
  number = {arXiv:1904.01685},
  eprint = {1904.01685},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1904.01685},
  urldate = {2023-04-03},
  abstract = {Overconfidence and underconfidence in machine learning classifiers is measured by calibration: the degree to which the probabilities predicted for each class match the accuracy of the classifier on that prediction.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {252 citations (Semantic Scholar/arXiv) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nixon20MeasuringCalibrationDeep.pdf}
}

@inproceedings{Guo17CalibrationModernNeural,
  title = {On Calibration of Modern Neural Networks},
  booktitle = {International Conference on Machine Learning},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  year = {2017},
  pages = {1321--1330},
  publisher = {PMLR},
  file = {/Users/scott/OneDrive/share/ref/zotero/tmp_papers/Guo17CalibrationModernNeural.pdf}
}

@article{Hu22ConformalizedTemporalConvolutional,
  title = {Conformalized Temporal Convolutional Quantile Regression Networks for Wind Power Interval Forecasting},
  author = {Hu, Jianming and Luo, Qingxi and Tang, Jingwei and Heng, Jiani and Deng, Yuwen},
  year = {2022},
  month = jun,
  journal = {Energy},
  volume = {248},
  pages = {123497},
  issn = {0360-5442},
  doi = {10.1016/j.energy.2022.123497},
  url = {https://www.sciencedirect.com/science/article/pii/S0360544222004005},
  urldate = {2023-04-18},
  abstract = {Wind power interval prediction is an effective technique for quantifying forecasting uncertainty caused by the intermittent and fluctuant characteristics of wind energy. Valid coverage and short interval length are the two most critical targets in interval prediction to attain reliable and accurate information, providing effective support for decision-makers to better control the risks in the power planning. This paper proposes a novel interval prediction approach named conformalized temporal convolutional quantile regression networks (CTCQRN) which combines the conformalized quantile regression (CQR) algorithm with a temporal convolutional network (TCN), without making any distributional assumptions. The proposed model inherits the advantages of quantile regression and conformal prediction that is fully adaptive to heteroscedasticity implicated in data, and meets the theoretical guarantee of valid coverage. As opposed to conventional RNN-based approaches, the adopted TCN architecture frees from suffering iterative propagation and gradient vanishing/explosion, and can handle very long sequences in a parallel manner. Case studies on two different geographical wind power datasets show that the proposed model has a distinct edge over benchmark models in goals of valid coverage and narrow interval bandwidth, which can help to ensure the economic and secure operation of the electric power system.},
  langid = {english},
  keywords = {obsLitNote},
  annotation = {16 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\S0360544222004005.html}
}

@article{Beinert23PowerFlowForecasts,
  title = {Power Flow Forecasts at Transmission Grid Nodes Using {{Graph Neural Networks}}},
  author = {Beinert, Dominik and Holzh{\"u}ter, Clara and Thomas, Josephine M. and Vogt, Stephan},
  year = {2023},
  month = apr,
  journal = {Energy and AI},
  pages = {100262},
  issn = {2666-5468},
  doi = {10.1016/j.egyai.2023.100262},
  url = {https://www.sciencedirect.com/science/article/pii/S2666546823000344},
  urldate = {2023-04-18},
  abstract = {The increasing share of renewable energy in the electricity grid and progressing changes in power consumption have led to fluctuating, and weather-dependent power flows. To ensure grid stability, grid operators rely on power forecasts which are crucial for grid calculations and planning. In this paper, a Multi-Task Learning approach is combined with a Graph Neural Network (GNN) to predict vertical power flows at transformers connecting high and extra-high voltage levels. The proposed method accounts for local differences in power flow characteristics by using an Embedding Multi-Task Learning approach. The use of a Bayesian embedding to capture the latent node characteristics allows to share the weights across all transformers in the subsequent node-invariant GNN while still allowing the individual behavioral patterns of the transformers to be distinguished. At the same time, dependencies between transformers are considered by the GNN architecture which can learn relationships between different transformers and thus take into account that power flows in an electricity network are not independent from each other. The effectiveness of the proposed method is demonstrated through evaluation on two real-world data sets provided by two of four German Transmission System Operators, comprising large portions of the operated German transmission grid. The results show that the proposed Multi-Task Graph Neural Network is a suitable representation learner for electricity networks with a clear advantage provided by the preceding embedding layer. It is able to capture interconnections between correlated transformers and indeed improves the performance in power flow prediction compared to standard Neural Networks. A sign test shows that the proposed model reduces the test RMSE on both datasets compared to the benchmark models significantly.},
  langid = {english},
  keywords = {obsLitNote},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Beinert23PowerFlowForecasts.pdf}
}

@article{Lin19DataefficientMutualInformation,
  title = {Data-Efficient Mutual Information Neural Estimator},
  author = {Lin, Xiao and Sur, Indranil and Nastase, Samuel A. and Divakaran, Ajay and Hasson, Uri and Amer, Mohamed R.},
  year = {2019},
  journal = {arXiv preprint arXiv:1905.03319},
  eprint = {1905.03319},
  archiveprefix = {arXiv},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lin19DataefficientMutualInformation.pdf}
}

@inproceedings{Belghazi18MutualInformationNeural,
  title = {Mutual Information Neural Estimation},
  booktitle = {International Conference on Machine Learning},
  author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  year = {2018},
  pages = {531--540},
  publisher = {PMLR},
  abstract = {We argue that the estimation of mutual informa- tion between high dimensional continuous ran- dom variables can be achieved by gradient descent over neural networks. We present a Mutual Infor- mation Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or max- imize mutual information. We apply MINE to im- prove adversarially trained generative models. We also use MINE to implement the Information Bot- tleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.},
  note = {I'm just making up a note, here. ~Does LSQ-SDO copy it and keep it updated?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Belghazi18MutualInformationNeural.pdf}
}

@article{Khatun21TestingPairsContinuous,
  title = {Testing Pairs of Continuous Random Variables for Independence: {{A}} Simple Heuristic},
  shorttitle = {Testing Pairs of Continuous Random Variables for Independence},
  author = {Khatun, Mahfuza and Siddiqui, Sikandar},
  year = {2021},
  month = sep,
  journal = {Journal of Computational Mathematics and Data Science},
  volume = {1},
  pages = {100012},
  issn = {2772-4158},
  doi = {10.1016/j.jcmds.2021.100012},
  url = {https://www.sciencedirect.com/science/article/pii/S2772415821000067},
  urldate = {2023-05-02},
  abstract = {Detection and examination of pairwise dependence patterns between continuous variables is among the central tasks in the fields of business and economic statistics. To perform this analysis, practitioners frequently resort to Pearson's (1895) product--moment correlation coefficient and the related significance tests. However, the use of such tests in isolation involves the risk of missing the nonlinear and particularly non-monotonic associations between the variables. This problem is also relevant in the cases where the dependence prevails between higher-order moments, e.g., variances, rather than means. We present a simple, computationally inexpensive heuristic by which this problem can be addressed and demonstrate its usefulness in a small number of example cases.},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Khatun21TestingPairsContinuous.pdf}
}

@inproceedings{Lolla11SelNumBinsHist,
  title = {On Selecting the Number of Bins for a Histogram},
  booktitle = {Proceedings of the {{International Conference}} on {{Data Science}} ({{ICDATA}})},
  author = {Lolla, Sai Venu Gopal and Hoberock, Lawrence L.},
  year = {2011},
  pages = {1},
  publisher = {Citeseer},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lolla11SelNumBinsHist.pdf}
}

@article{Gupta21entropyEstEquiProb,
  title = {Computing {{Accurate Probabilistic Estimates}} of {{One-D Entropy}} from {{Equiprobable Random Samples}}},
  author = {Gupta, Hoshin V. and Ehsani, Mohammad Reza and Roy, Tirthankar and {Sans-Fuentes}, Maria A. and Ehret, Uwe and Behrangi, Ali},
  year = {2021},
  month = jun,
  journal = {Entropy},
  volume = {23},
  number = {6},
  pages = {740},
  issn = {1099-4300},
  doi = {10.3390/e23060740},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8231182/},
  urldate = {2023-05-04},
  abstract = {We develop a simple Quantile Spacing (QS) method for accurate probabilistic estimation of one-dimensional entropy from equiprobable random samples, and compare it with the popular Bin-Counting (BC) and Kernel Density (KD) methods. In contrast to BC, which uses equal-width bins with varying probability mass, the QS method uses estimates of the quantiles that divide the support of the data generating probability density function (pdf) into equal-probability-mass intervals. And, whereas BC and KD each require optimal tuning of a hyper-parameter whose value varies with sample size and shape of the pdf, QS only requires specification of the number of quantiles to be used. Results indicate, for the class of distributions tested, that the optimal number of quantiles is a fixed fraction of the sample size (empirically determined to be {\textasciitilde}0.25--0.35), and that this value is relatively insensitive to distributional form or sample size. This provides a clear advantage over BC and KD since hyper-parameter tuning is not required. Further, unlike KD, there is no need to select an appropriate kernel-type, and so QS is applicable to pdfs of arbitrary shape, including those with discontinuous slope and/or magnitude. Bootstrapping is used to approximate the sampling variability distribution of the resulting entropy estimate, and is shown to accurately reflect the true uncertainty. For the four distributional forms studied (Gaussian, Log-Normal, Exponential and Bimodal Gaussian Mixture), expected estimation bias is less than 1\% and uncertainty is low even for samples of as few as 100 data points; in contrast, for KD the small sample bias can be as large as -10\% and for BC as large as -50\%. We speculate that estimating quantile locations, rather than bin-probabilities, results in more efficient use of the information in the data to approximate the underlying shape of an unknown data generating pdf.},
  pmcid = {PMC8231182},
  pmid = {34208344},
  annotation = {4 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gupta21entropyEstEquiProb.pdf}
}

@inproceedings{Lapedriza07HierarchicalApproachMultitask,
  title = {A {{Hierarchical Approach}} for {{Multi-task Logistic Regression}}},
  booktitle = {Pattern {{Recognition}} and {{Image Analysis}}},
  author = {Lapedriza, {\`A}gata and Masip, David and Vitri{\`a}, Jordi},
  editor = {Mart{\'i}, Joan and Bened{\'i}, Jos{\'e} Miguel and Mendon{\c c}a, Ana Maria and Serrat, Joan},
  year = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {258--265},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-72849-8_33},
  abstract = {In the statistical pattern recognition field the number of samples to train a classifier is usually insufficient. Nevertheless, it has been shown that some learning domains can be divided in a set of related tasks, that can be simultaneously trained sharing information among the different tasks. This methodology is known as the multi-task learning paradigm. In this paper we propose a multi-task probabilistic logistic regression model and develop a learning algorithm based in this framework, which can deal with the small sample size problem. Our experiments performed in two independent databases from the UCI and a multi-task face classification experiment show the improved accuracies of the multi-task learning approach with respect to the single task approach when using the same probabilistic model.},
  isbn = {978-3-540-72849-8},
  langid = {english},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lapedriza07HierarchicalApproachMultitask.pdf}
}

@article{Lall21StableFeatureSelection,
  title = {Stable Feature Selection Using Copula Based Mutual Information},
  author = {Lall, Snehalika and Sinha, Debajyoti and Ghosh, Abhik and Sengupta, Debarka and Bandyopadhyay, Sanghamitra},
  year = {2021},
  month = apr,
  journal = {Pattern Recognition},
  volume = {112},
  pages = {107697},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2020.107697},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320320305008},
  urldate = {2023-05-09},
  abstract = {Feature selection is a key step in many machine learning tasks. A majority of the existing methods of feature selection address the problem by devising some scoring function while treating the features independently, thereby overlooking their interdependencies. We leverage the scale invariance property of copula to construct a greedy, supervised feature selection algorithm that maximizes the feature relevance while minimizing the redundant information content. Multivariate copula is used in the proposed copula Based Feature Selection (CBFS) to discover the dependence structure between features. The incorporation of copula-based multivariate dependency in the formulation of mutual information helps avoid averaging over multiple instances of bivariate dependencies, thus eliminating the average estimation error introduced when bivariate dependency is used between a pair of feature variables. Under a controlled setting, our algorithm outperformed the existing best practice methods in warding off the noise in data. On several real and synthetic datasets, the proposed algorithm performed competitively in maximizing classification accuracy. CBFS also outperforms the other methods in terms of its noise tolerance property.},
  langid = {english},
  annotation = {23 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lall21StableFeatureSelection.pdf}
}

@article{Ye18NovelTransferLearning,
  title = {A Novel Transfer Learning Framework for Time Series Forecasting},
  author = {Ye, Rui and Dai, Qun},
  year = {2018},
  month = sep,
  journal = {Knowledge-Based Systems},
  volume = {156},
  pages = {74--99},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2018.05.021},
  url = {https://www.sciencedirect.com/science/article/pii/S095070511830251X},
  urldate = {2023-05-09},
  abstract = {Recently, many excellent algorithms for time series prediction issues have been proposed, most of which are developed based on the assumption that sufficient training data and testing data under the same distribution are available. However, in reality, time-series data usually exhibit some kind of time-varying characteristic, which may lead to a wide variability between old data and new data. Hence, how to transfer knowledge over a long time span, when addressing time series prediction issues, poses serious challenges. To solve this problem, in this paper, a hybrid algorithm based on transfer learning, Online Sequential Extreme Learning Machine with Kernels (OS-ELMK), and ensemble learning, abbreviated as TrEnOS-ELMK, is proposed, along with its precise mathematic derivation. It aims to make the most of, rather than discard, the adequate long-ago data, and constructs an algorithm framework for transfer learning in time series forecasting, which is groundbreaking. Inspired by the preferable performance of models ensemble, ensemble learning scheme is also incorporated into our proposed algorithm, where the weights of the constituent models are adaptively updated according to their performances on fresh samples. Compared to many existing time series prediction methods, the newly proposed algorithm takes long-ago data into consideration and can effectively leverage the latent knowledge implied in these data for current prediction. In addition, TrEnOS-ELMK naturally inherits merits of both OS-ELMK and ensemble learning due to its incorporation of the two techniques. Experimental results on three synthetic and six real-world datasets demonstrate the effectiveness of the proposed algorithm.},
  langid = {english},
  annotation = {74 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ye18NovelTransferLearning.pdf}
}

@article{Zellinger20MultisourceTransferLearning,
  title = {Multi-Source Transfer Learning of Time Series in Cyclical Manufacturing},
  author = {Zellinger, Werner and Grubinger, Thomas and Zwick, Michael and Lughofer, Edwin and Sch{\"o}ner, Holger and Natschl{\"a}ger, Thomas and {Saminger-Platz}, Susanne},
  year = {2020},
  month = mar,
  journal = {Journal of Intelligent Manufacturing},
  volume = {31},
  number = {3},
  pages = {777--787},
  issn = {1572-8145},
  doi = {10.1007/s10845-019-01499-4},
  url = {https://doi.org/10.1007/s10845-019-01499-4},
  urldate = {2023-05-09},
  abstract = {This paper describes a new transfer learning method for modeling sensor time series following multiple different distributions, e.g. originating from multiple different tool settings. The method aims at removing distribution specific information before the modeling of the individual time series takes place. This is done by mapping the data to a new space such that the representations of different distributions are aligned. Domain knowledge is incorporated by means of corresponding parameters, e.g. physical dimensions of tool settings. Results on a real-world problem of industrial manufacturing show that our method is able to significantly improve the performance of regression models on time series following previously unseen distributions.},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zellinger20MultisourceTransferLearning.pdf}
}

@inproceedings{He19TransferLearningFinancial,
  title = {Transfer {{Learning}} for {{Financial Time Series Forecasting}}},
  booktitle = {{{PRICAI}} 2019: {{Trends}} in {{Artificial Intelligence}}},
  author = {He, Qi-Qiao and Pang, Patrick Cheong-Iao and Si, Yain-Whar},
  editor = {Nayak, Abhaya C. and Sharma, Alok},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {24--36},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-29911-8_3},
  abstract = {Time-series are widely used for representing non-stationary data such as weather information, health related data, economic and stock market indexes. Many statistical methods and traditional machine learning techniques are commonly used for forecasting time series. With the development of deep learning in artificial intelligence, many researchers have adopted new models from artificial neural networks for forecasting time series. However, poor performance of applying deep learning models in short time series hinders the accuracy in time series forecasting. In this paper, we propose a novel approach to alleviate this problem based on transfer learning. Existing work on transfer learning uses extracted features from a source dataset for prediction task in a target dataset. In this paper, we propose a new training strategy for time-series transfer learning with two source datasets that outperform existing approaches. The effectiveness of our approach is evaluated on financial time series extracted from stock markets. Experiment results show that transfer learning based on 2 data sets is superior than other base-line methods.},
  isbn = {978-3-030-29911-8},
  langid = {english},
  keywords = {obsLitNote},
  annotation = {21 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\He19TransferLearningFinancial.pdf}
}

@book{Huang20ModelingComplexSpatial,
  title = {Modeling {{Complex Spatial Patterns}} with {{Temporal Features}} via {{Heterogenous Graph Embedding Networks}}},
  author = {Huang, Yida and Xu, Haoyan and Duan, Ziheng and Ren, Anni and Feng, Jie and Wang, Xiaoqian},
  year = {2020},
  month = aug,
  abstract = {Multivariate time series (MTS) forecasting is an important problem in many fields. Accurate forecasting results can effectively help decision-making. Variables in MTS have rich relations among each other and the value of each variable in MTS depends both on its historical values and on other variables. These rich relations can be static and predictable or dynamic and latent. Existing methods do not incorporate these rich relational information into modeling or only model certain relation among MTS variables. To jointly model rich relations among variables and temporal dependencies within the time series, a novel end-to-end deep learning model, termed Multivariate Time Series Forecasting via Heterogenous Graph Neural Networks (MTHetGNN) is proposed in this paper. To characterize rich relations among variables, a relation embedding module is introduced in our model, where each variable is regarded as a graph node and each type of edge represents a specific relationship among variables or one specific dynamic update strategy to model the latent dependency among variables. In addition, convolutional neural network (CNN) filters with different perception scales are used for time series feature extraction, which is used to generate the feature of each node. Finally, heterogenous graph neural networks are adopted to handle the complex structural information generated by temporal embedding module and relation embedding module. Three benchmark datasets from the real world are used to evaluate the proposed MTHetGNN and the comprehensive experiments show that MTHetGNN achieves state-of-the-art results in MTS forecasting task.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Huang20ModelingComplexSpatial.pdf}
}

@article{Cui12SemiparametricGaussianVarianceMean,
  title = {Semiparametric {{Gaussian Variance-Mean Mixtures}} for {{Heavy-Tailed}} and {{Skewed Data}}},
  author = {Cui, Kai},
  year = {2012},
  month = dec,
  journal = {International Scholarly Research Notices},
  volume = {2012},
  pages = {e345784},
  publisher = {Hindawi},
  doi = {10.5402/2012/345784},
  url = {https://www.hindawi.com/journals/isrn/2012/345784/},
  urldate = {2023-05-09},
  abstract = {There is a need for new classes of flexible multivariate distributions that can capture heavy tails and skewness without being so flexible as to fully incur the curse of dimensionality intrinsic to nonparametric density estimation. We focus on the family of Gaussian variance-mean mixtures, which have received limited attention in multivariate settings beyond simple special cases. By using a Bayesian semiparametric approach, we allow the data to infer about the unknown mixing distribution. Properties are considered and an approach to posterior computation is developed relying on Markov chain Monte Carlo. The methods are evaluated through simulation studies and applied to a variety of applications, illustrating their flexible performance in characterizing heavy tails, tail dependence, and skewness.},
  langid = {english},
  annotation = {3 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Cui12SemiparametricGaussianVarianceMean.pdf}
}

@article{Babic23TailCoRNewSimple,
  title = {{{TailCoR}}: {{A}} New and Simple Metric for Tail Correlations That Disentangles the Linear and Nonlinear Dependencies That Cause Extreme Co-Movements},
  shorttitle = {{{TailCoR}}},
  author = {Babi{\'c}, Sladana and Ley, Christophe and Ricci, Lorenzo and Veredas, David},
  year = {2023},
  month = jan,
  journal = {PLOS ONE},
  volume = {18},
  number = {1},
  pages = {e0278599},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0278599},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0278599},
  urldate = {2023-05-09},
  abstract = {Economic and financial crises are characterised by unusually large events. These tail events co-move because of linear and/or nonlinear dependencies. We introduce TailCoR, a metric that combines (and disentangles) these linear and non-linear dependencies. TailCoR between two variables is based on the tail inter quantile range of a simple projection. It is dimension-free, and, unlike competing metrics, it performs well in small samples and no optimisations are needed. Indeed, TailCoR requires a few lines of coding and it is very fast. A Monte Carlo analysis confirms the goodness of the metric, which is illustrated on a sample of 21 daily financial market indexes across the globe and for 20 years. The estimated TailCoRs are in line with the financial and economic events, such as the 2008 great financial crisis and the 2020 pandemic.},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Babic23TailCoRNewSimple.pdf}
}

@article{Fang02MetaellipticalDistributionsGiven,
  title = {The {{Meta-elliptical Distributions}} with {{Given Marginals}}},
  author = {Fang, Hong-Bin and Fang, Kai-Tai and Kotz, Samuel},
  year = {2002},
  month = jul,
  journal = {Journal of Multivariate Analysis},
  volume = {82},
  number = {1},
  pages = {1--16},
  issn = {0047-259X},
  doi = {10.1006/jmva.2001.2017},
  url = {https://www.sciencedirect.com/science/article/pii/S0047259X01920172},
  urldate = {2023-05-09},
  abstract = {Based on an analysis of copulas of elliptically contoured distributions, joint densities of continuous variables with given strictly increasing marginal distributions are constructed. A method utilized for this procedure is to embed the spherical distribution quantile transformation of each variable into an elliptically contoured distribution. The new class of distributions is then called meta-elliptical distributions. The corresponding analytic forms of the density, conditional distribution functions, and dependence properties are derived. This new class of distributions has the same Kendall's rank correlation coefficient as meta-Gaussian distributions. As an extension of elliptically contoured distributions, some new classes of distributions are also obtained.},
  langid = {english},
  annotation = {372 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Proof of arcsin relationship between Kendall's tau and Pearson'ts Rho for Guassian Copula According to: https://stats.stackexchange.com/questions/133460/proof-of-the-relation-between-kendalls-tau-and-pearsons-rho-for-the-gaussian-c It is proven as Theorem 3.2 in Fang, Fang, \& Kotz, The Meta-elliptical Distributions with Given Marginals Journal of Multivariate Analysis, Elsevier, 2002, 82, 1?16 but that relies on Theorem 2.22 in [K. T. Fang, Kotz, and Ng, "Symmetric Multivariate and Related Distribution," Chapman \& Hall, London, 1990.] (to which I do not have access).},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Fang02MetaellipticalDistributionsGiven.pdf;C\:\\Users\\scott\\tmp\\tmp_orphan_papers\\Fang02metaElipDistMargKendalPears.pdf;C\:\\Users\\scott\\Zotero\\storage\\57TLBKXA\\Fang et al. - 2002 - The Meta-elliptical Distributions with Given Margi.pdf;C\:\\Users\\scott\\Zotero\\storage\\5GYPZTSR\\Fang et al. - 2002 - The Meta-elliptical Distributions with Given Margi.pdf}
}

@article{Lee09CopulabasedMultivariateGARCH,
  title = {Copula-Based Multivariate {{GARCH}} Model with Uncorrelated Dependent Errors},
  author = {Lee, Tae-Hwy and Long, Xiangdong},
  year = {2009},
  month = jun,
  journal = {Journal of Econometrics},
  series = {Recent {{Development}} in {{Financial Econometrics}}},
  volume = {150},
  number = {2},
  pages = {207--218},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2008.12.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0304407608002194},
  urldate = {2023-05-09},
  abstract = {Multivariate GARCH (MGARCH) models are usually estimated under multivariate normality. In this paper, for non-elliptically distributed financial returns, we propose copula-based multivariate GARCH (C-MGARCH) model with uncorrelated dependent errors, which are generated through a linear combination of dependent random variables. The dependence structure is controlled by a copula function. Our new C-MGARCH model nests a conventional MGARCH model as a special case. The aim of this paper is to model MGARCH for non-normal multivariate distributions using copulas. We model the conditional correlation (by MGARCH) and the remaining dependence (by a copula) separately and simultaneously. We apply this idea to three MGARCH models, namely, the dynamic conditional correlation (DCC) model of Engle [Engle, R.F., 2002. Dynamic conditional correlation: A simple class of multivariate generalized autoregressive conditional heteroskedasticity models. Journal of Business and Economic Statistics 20, 339--350], the varying correlation (VC) model of Tse and Tsui [Tse, Y.K., Tsui, A.K., 2002. A multivariate generalized autoregressive conditional heteroscedasticity model with time-varying correlations. Journal of Business and Economic Statistics 20, 351--362], and the BEKK model of Engle and Kroner [Engle, R.F., Kroner, K.F., 1995. Multivariate simultaneous generalized ARCH. Econometric Theory 11, 122--150]. Empirical analysis with three foreign exchange rates indicates that the C-MGARCH models outperform DCC, VC, and BEKK in terms of in-sample model selection and out-of-sample multivariate density forecast, and in terms of these criteria the choice of copula functions is more important than the choice of the volatility models.},
  langid = {english},
  annotation = {151 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lee09CopulabasedMultivariateGARCH.pdf}
}

@article{Archambeau08MixturesRobustProbabilistic,
  title = {Mixtures of Robust Probabilistic Principal Component Analyzers},
  author = {Archambeau, C{\'e}dric and Delannay, Nicolas and Verleysen, Michel},
  year = {2008},
  month = mar,
  journal = {Neurocomputing},
  series = {Progress in {{Modeling}}, {{Theory}}, and {{Application}} of {{Computational Intelligenc}}},
  volume = {71},
  number = {7},
  pages = {1274--1282},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2007.11.029},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231208000623},
  urldate = {2023-05-09},
  abstract = {Mixtures of probabilistic principal component analyzers model high-dimensional nonlinear data by combining local linear models. Each mixture component is specifically designed to extract the local principal orientations in the data. An important issue with this generative model is its sensitivity to data lying off the low-dimensional manifold. In order to address this problem, the mixtures of robust probabilistic principal component analyzers are introduced. They take care of atypical points by means of a long tail distribution, the Student-t. It is shown that the resulting mixture model is an extension of the mixture of Gaussians, suitable for both robust clustering and dimensionality reduction. Finally, we briefly discuss how to construct a robust version of the closely related mixture of factor analyzers.},
  langid = {english},
  annotation = {64 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Archambeau08MixturesRobustProbabilistic.pdf}
}

@article{Ma11MutualInformationCopula,
  title = {Mutual Information Is Copula Entropy},
  author = {Ma, Jian and Sun, Zengqi},
  year = {2011},
  month = feb,
  journal = {Tsinghua Science and Technology},
  volume = {16},
  number = {1},
  pages = {51--54},
  issn = {1007-0214},
  doi = {10.1016/S1007-0214(11)70008-6},
  abstract = {Mutual information (MI) is a basic concept in information theory. Therefore, estimates of the MI are fundamentally important in most information theory applications. This paper provides a new way of understanding and estimating the MI using the copula function. First, the entropy of the copula, named the copula entropy, is defined as a measure of the dependence uncertainty represented by the copula function and then the MI is shown to be equivalent to the negative copula entropy. With this equivalence, the MI can be estimated by first estimating the empirical copula and then estimating the entropy of the empirical copula. Thus, the MIestimate is an estimation of the entropy, which reduces the complexity and computational requirements. Tests show that the method is more effective than the traditional method.},
  annotation = {117 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Equiv. to Kraskov04EstMutInfKNN on a test signal but far less computationally intensive. Total entropy, not pairwise. What's the diff between this and Poczos10rankEstRenInfo ? Is this another way to estimate McGill Interaction entropy?},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Ma11MutualInformationCopula.pdf;C\:\\Users\\scott\\Zotero\\storage\\JZG7UVD3\\6077935.html}
}

@article{Ashrafi22MultivariateGaussianCopula,
  title = {Multivariate {{Gaussian Copula Mutual Information}} to {{Estimate Functional Connectivity}} with {{Less Random Architecture}}},
  author = {Ashrafi, Mahnaz and {Soltanian-Zadeh}, Hamid},
  year = {2022},
  month = may,
  journal = {Entropy},
  volume = {24},
  number = {5},
  pages = {631},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e24050631},
  url = {https://www.mdpi.com/1099-4300/24/5/631},
  urldate = {2023-05-09},
  abstract = {Recognition of a brain region's interaction is an important field in neuroscience. Most studies use the Pearson correlation to find the interaction between the regions. According to the experimental evidence, there is a nonlinear dependence between the activities of different brain regions that is ignored by Pearson correlation as a linear measure. Typically, the average activity of each region is used as input because it is a univariate measure. This dimensional reduction, i.e., averaging, leads to a loss of spatial information across voxels within the region. In this study, we propose using an information-theoretic measure, multivariate mutual information (mvMI), as a nonlinear dependence to find the interaction between regions. This measure, which has been recently proposed, simplifies the mutual information calculation complexity using the Gaussian copula. Using simulated data, we show that the using this measure overcomes the mentioned limitations. Additionally using the real resting-state fMRI data, we compare the level of significance and randomness of graphs constructed using different methods. Our results indicate that the proposed method estimates the functional connectivity more significantly and leads to a smaller number of random connections than the common measure, Pearson correlation. Moreover, we find that the similarity of the estimated functional networks of the individuals is higher when the proposed method is used.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ashrafi22MultivariateGaussianCopula.pdf}
}

@misc{Crimi23GrowingLibrariesPythona,
  title = {5 {{Growing Libraries}} in {{Python}} for {{Causality Analysis}}},
  author = {Crimi, Dr Alessandro},
  year = {2023},
  month = feb,
  journal = {Medium},
  url = {https://pub.towardsai.net/5-growing-libraries-in-python-for-causality-analysis-39266d19edea},
  urldate = {2023-05-09},
  abstract = {not only for neuroimaging but other network AI analysis},
  langid = {english}
}

@misc{Simple23AreTransformersEffective,
  title = {Are {{Transformers}} Effective for {{Time Series Forecasting}}},
  author = {Simple, Devansh-Machine Learning Made},
  year = {2023},
  month = apr,
  journal = {Geek Culture},
  url = {https://medium.com/geekculture/are-transformers-effective-for-time-series-forecasting-6e2194338ff0},
  urldate = {2023-05-09},
  abstract = {How well does Machine Learning's Favorite Architecture stack up against simple autoregressive models},
  langid = {english},
  keywords = {obsLitNote}
}

@inproceedings{Nivarthi22UnifiedAutoencoderTask,
  title = {Unified {{Autoencoder}} with {{Task Embeddings}} for {{Multi-Task Learning}} in {{Renewable Power Forecasting}}},
  booktitle = {2022 21st {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Nivarthi, Chandana Priya and Vogt, Stephan and Sick, Bernhard},
  year = {2022},
  month = dec,
  pages = {1530--1536},
  doi = {10.1109/ICMLA55696.2022.00240},
  abstract = {Renewable power generation forecasts using machine learning are typically implemented as single-task learning models, where a separate model is trained for each photovoltaic or wind park. In recent years, transfer learning is gaining popularity in these systems, as it can be used to transfer the knowledge gained from source parks to a target park. However, for transferring the knowledge to a target park, there is a need to determine the most similar source park(s) among the existing parks. This similarity determination using historical power measurements is challenging when the target park has limited to no historical data samples. Therefore, we propose a simple multi-task learning architecture that initially learns a common representation of input weather features among the tasks, using a Unified Autoencoder (UAE) and then learns the task specific information utilizing a Task Embedding layer in a Neural Network (TENN). This proposed architecture, UAE-TENN, can be easily extended to new parks with or without historical data. An elaborate performance comparison of single and multi-task learning models is performed on six photovoltaic and wind farm datasets comprising a total of 529 parks. UAE-TENN significantly improves the performance of power forecasting by 10 to 19\% for photovoltaic parks and 5 to 22\% for wind parks compared to the baseline models. Even in the zero-shot learning scenario, when there is no historical data, we successfully demonstrate that the UAE-TENN improves the forecast accuracy for a new park by 19\% for photovoltaic parks.},
  keywords = {obsLitNote},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nivarthi22UnifiedAutoencoderTask.pdf}
}

@inproceedings{Schreiber21TaskEmbeddingTemporal,
  title = {Task {{Embedding Temporal Convolution Networks}} for {{Transfer Learning Problems}} in {{Renewable Power Time Series Forecast}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}. {{Applied Data Science Track}}},
  author = {Schreiber, Jens and Vogt, Stephan and Sick, Bernhard},
  editor = {Dong, Yuxiao and Kourtellis, Nicolas and Hammer, Barbara and Lozano, Jose A.},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {118--134},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-86514-6_8},
  abstract = {Task embeddings in multi-layer perceptrons (MLP) for multi-task learning and inductive transfer learning in renewable power forecasts is an exciting new technique. In many cases, this approach improves the forecast error and reduces the required training data. However, it does not take the periodic influences in power forecasts within a day into account, i.e., the diurnal cycle. Therefore, we extended this idea to temporal convolutional networks to consider those in tasks of day-ahead power forecasts for renewables. We propose transforming the embedding space, which contains the latent similarities between tasks, through convolution and providing these results to the network's residual block. The proposed architecture significantly improves the forecast accuracy up~to \$\$25{\textbackslash}\%\$\$25\%for multi-task learning for power forecasts on the open EuropeWindFarm and GermanSolarFarm datasets compared to the MLP approach. Based on the same data, we achieve a ten percent improvement for the wind datasets and more than \$\$20{\textbackslash}\%\$\$20\%in most cases for the solar dataset for inductive transfer learning without catastrophic forgetting. Finally, we are the first to propose zero-shot learning for renewable power forecasts. The proposed architecture achieves an error as good as the task embedding MLP with a full year of training data in the respective experiments.},
  isbn = {978-3-030-86514-6},
  langid = {english},
  keywords = {obsLitNote},
  annotation = {7 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Schreiber21TaskEmbeddingTemporal.pdf}
}

@article{Schreiber23ModelSelectionAdaptation,
  title = {Model Selection, Adaptation, and Combination for Transfer Learning in Wind and Photovoltaic Power Forecasts},
  author = {Schreiber, Jens and Sick, Bernhard},
  year = {2023},
  month = oct,
  journal = {Energy and AI},
  volume = {14},
  pages = {100249},
  issn = {2666-5468},
  doi = {10.1016/j.egyai.2023.100249},
  url = {https://www.sciencedirect.com/science/article/pii/S2666546823000216},
  urldate = {2023-05-15},
  abstract = {There is recent interest in using model hubs -- a collection of pre-trained models -- in computer vision tasks. To employ a model hub, we first select a source model and then adapt the model for the target to compensate for differences. There still needs to be more research on model selection and adaption for renewable power forecasts. In particular, none of the related work examines different model selection and adaptation strategies for neural network architectures. Also, none of the current studies investigates the influence of available training samples and considers seasonality in the evaluation. We close these gaps by conducting the first thorough experiment for model selection and adaptation for transfer learning in renewable power forecast, adopting recent developments from the field of computer vision on 667 wind and photovoltaic parks from six datasets. We simulate different amounts of training samples for each season to calculate informative forecast errors. We examine the marginal likelihood and forecast error for model selection for those amounts. Furthermore, we study four adaption strategies. As an extension of the current state of the art, we utilize a Bayesian linear regression for forecasting the response based on features extracted from a neural network. This approach outperforms the baseline with only seven days of training data and shows that fine-tuning is not beneficial with less than three months of data. We further show how combining multiple models through ensembles can significantly improve the model selection and adaptation approach such that we have a similar mean error with only 30 days of training data which is otherwise only possible with an entire year of training data. We achieve a mean error of 9.8 and 14 percent for the most realistic dataset for PV and wind with only seven days of training data.},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {This is a note.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Schreiber23ModelSelectionAdaptation.pdf}
}

@article{Zhang20UltraShortTermElectricalLoad,
  title = {An {{Ultra-Short-Term Electrical Load Forecasting Method Based}} on {{Temperature-Factor-Weight}} and {{LSTM Model}}},
  author = {Zhang, Dengyong and Tong, Haixin and Li, Feng and Xiang, Lingyun and Ding, Xiangling},
  year = {2020},
  journal = {Energies},
  volume = {13},
  number = {18},
  pages = {4875},
  publisher = {MDPI},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhang20UltraShortTermElectricalLoad.pdf}
}

@inproceedings{Zhao19featSelMaxRelMinRedund,
  title = {Maximum {{Relevance}} and {{Minimum Redundancy Feature Selection Methods}} for a {{Marketing Machine Learning Platform}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Data Science}} and {{Advanced Analytics}} ({{DSAA}})},
  author = {Zhao, Zhenyu and Anand, Radhika and Wang, Mallory},
  year = {2019},
  month = oct,
  pages = {442--452},
  doi = {10.1109/DSAA.2019.00059},
  abstract = {In machine learning applications for online product offerings and marketing strategies, there are often hundreds or thousands of features available to build such models. Feature selection is one essential method in such applications for multiple objectives: improving the prediction accuracy by eliminating irrelevant features, accelerating the model training and prediction speed, reducing the monitoring and maintenance workload for feature data pipeline, and providing better model interpretation and diagnosis capability. However, selecting an optimal feature subset from a large feature space is considered as an NP-complete problem. The mRMR (Minimum Redundancy and Maximum Relevance) feature selection framework solves this problem by selecting the relevant features while controlling for the redundancy within the selected features. This paper describes the approach to extend, evaluate, and implement the mRMR feature selection methods for classification problem in a marketing machine learning platform at Uber that automates creation and deployment of targeting and personalization models at scale. This study first extends the existing mRMR methods by introducing a non-linear feature redundancy measure and a model-based feature relevance measure. Then an extensive empirical evaluation is performed for eight different feature selection methods, using one synthetic dataset and three real-world marketing datasets at Uber to cover different use cases. Based on the empirical results, the selected mRMR method is implemented in production for the marketing machine learning platform. A description of the production implementation is provided and an online experiment deployed through the platform is discussed.},
  annotation = {63 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhao19featSelMaxRelMinRedund.pdf}
}

@article{Borisov22DeepNeuralNetworks,
  title = {Deep {{Neural Networks}} and {{Tabular Data}}: {{A Survey}}},
  shorttitle = {Deep {{Neural Networks}} and {{Tabular Data}}},
  author = {Borisov, Vadim and Leemann, Tobias and Se{\ss}ler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
  year = {2022},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  eprint = {2110.01889},
  primaryclass = {cs},
  pages = {1--21},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2022.3229161},
  url = {http://arxiv.org/abs/2110.01889},
  urldate = {2023-06-18},
  abstract = {Heterogeneous tabular data are the most commonly used form of data and are essential for numerous critical and computationally demanding applications. On homogeneous data sets, deep neural networks have repeatedly shown excellent performance and have therefore been widely adopted. However, their adaptation to tabular data for inference or data generation tasks remains challenging. To facilitate further progress in the field, this work provides an overview of state-of-the-art deep learning methods for tabular data. We categorize these methods into three groups: data transformations, specialized architectures, and regularization models. For each of these groups, our work offers a comprehensive overview of the main approaches. Moreover, we discuss deep learning approaches for generating tabular data, and we also provide an overview over strategies for explaining deep models on tabular data. Thus, our first contribution is to address the main research streams and existing methodologies in the mentioned areas, while highlighting relevant challenges and open research questions. Our second contribution is to provide an empirical comparison of traditional machine learning methods with eleven deep learning approaches across five popular real-world tabular data sets of different sizes and with different learning objectives. Our results, which we have made publicly available as competitive benchmarks, indicate that algorithms based on gradient-boosted tree ensembles still mostly outperform deep learning models on supervised learning tasks, suggesting that the research progress on competitive deep learning models for tabular data is stagnating. To the best of our knowledge, this is the first in-depth overview of deep learning approaches for tabular data; as such, this work can serve as a valuable starting point to guide researchers and practitioners interested in deep learning with tabular data.},
  archiveprefix = {arXiv},
  keywords = {intro},
  annotation = {162 citations (Semantic Scholar/arXiv) [2023-07-25]\\
162 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Borisov22DeepNeuralNetworks.pdf}
}

@misc{Kadra21WelltunedSimpleNets,
  title = {Well-Tuned {{Simple Nets Excel}} on {{Tabular Datasets}}},
  author = {Kadra, Arlind and Lindauer, Marius and Hutter, Frank and Grabocka, Josif},
  year = {2021},
  month = nov,
  number = {arXiv:2106.11189},
  eprint = {2106.11189},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.11189},
  url = {http://arxiv.org/abs/2106.11189},
  urldate = {2023-06-18},
  abstract = {Tabular datasets are the last "unconquered castle" for deep learning, with traditional ML methods like Gradient-Boosted Decision Trees still performing strongly even against recent specialized neural architectures. In this paper, we hypothesize that the key to boosting the performance of neural networks lies in rethinking the joint and simultaneous application of a large set of modern regularization techniques. As a result, we propose regularizing plain Multilayer Perceptron (MLP) networks by searching for the optimal combination/cocktail of 13 regularization techniques for each dataset using a joint optimization over the decision on which regularizers to apply and their subsidiary hyperparameters. We empirically assess the impact of these regularization cocktails for MLPs in a large-scale empirical study comprising 40 tabular datasets and demonstrate that (i) well-regularized plain MLPs significantly outperform recent state-of-the-art specialized neural network architectures, and (ii) they even outperform strong traditional ML methods, such as XGBoost.},
  archiveprefix = {arXiv},
  annotation = {70 citations (Semantic Scholar/arXiv) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kadra21WelltunedSimpleNets.pdf}
}

@inproceedings{Shavitt18RegularizationLearningNetworks,
  title = {Regularization {{Learning Networks}}: {{Deep Learning}} for {{Tabular Datasets}}},
  shorttitle = {Regularization {{Learning Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Shavitt, Ira and Segal, Eran},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2018/hash/500e75a036dc2d7d2fec5da1b71d36cc-Abstract.html},
  urldate = {2023-06-18},
  abstract = {Despite their impressive performance, Deep Neural Networks (DNNs) typically underperform Gradient Boosting Trees (GBTs) on many tabular-dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use of the more relevant inputs. However, this will lead to an intractable number of hyperparameters. Here, we introduce Regularization Learning Networks (RLNs), which overcome this challenge by introducing an efficient hyperparameter tuning scheme which minimizes a new Counterfactual Loss. Our results show that RLNs significantly improve DNNs on tabular datasets, and achieve comparable results to GBTs, with the best performance achieved with an ensemble that combines GBTs and RLNs. RLNs produce extremely sparse networks, eliminating up to 99.8\% of the network edges and 82\% of the input features, thus providing more interpretable models and reveal the importance that the network assigns to different inputs. RLNs could efficiently learn a single network in datasets that comprise both tabular and unstructured data, such as in the setting of medical imaging accompanied by electronic health records. An open source implementation of RLN can be found at https://github.com/irashavitt/regularizationlearningnetworks.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Shavitt18RegularizationLearningNetworks.pdf}
}

@article{Januschowski22ForecastingTrees,
  title = {Forecasting with Trees},
  author = {Januschowski, Tim and Wang, Yuyang and Torkkola, Kari and Erkkil{\"a}, Timo and Hasson, Hilaf and Gasthaus, Jan},
  year = {2022},
  month = oct,
  journal = {International Journal of Forecasting},
  series = {Special {{Issue}}: {{M5}} Competition},
  volume = {38},
  number = {4},
  pages = {1473--1481},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2021.10.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0169207021001679},
  urldate = {2023-06-18},
  abstract = {The prevalence of approaches based on gradient boosted trees among the top contestants in the M5 competition is potentially the most eye-catching result. Tree-based methods out-shone other solutions, in particular deep learning-based solutions. The winners in both tracks of the M5 competition heavily relied on them. This prevalence is even more remarkable given the dominance of other methods in the literature and the M4 competition. This article tries to explain why tree-based methods were so widely used in the M5 competition. We see possibilities for future improvements of tree-based models and then distill some learnings for other approaches, including but not limited to neural networks.},
  langid = {english},
  keywords = {forecast,intro,obsLitNote},
  annotation = {28 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Januschowski22ForecastingTrees.pdf}
}

@article{Lainder22frcstGBTaugTuneCV,
  title = {Forecasting with Gradient Boosted Trees: Augmentation, Tuning, and Cross-Validation Strategies: {{Winning}} Solution to the {{M5 Uncertainty}} Competition},
  shorttitle = {Forecasting with Gradient Boosted Trees},
  author = {Lainder, A. David and Wolfinger, Russell D.},
  year = {2022},
  month = oct,
  journal = {International Journal of Forecasting},
  series = {Special {{Issue}}: {{M5}} Competition},
  volume = {38},
  number = {4},
  pages = {1426--1433},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2021.12.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0169207021002090},
  urldate = {2023-06-18},
  abstract = {Deep neural networks and gradient boosted tree models have swept across the field of machine learning over the past decade, producing across-the-board advances in performance. The ability of these methods to capture feature interactions and nonlinearities makes them exceptionally powerful and, at the same time, prone to overfitting, leakage, and a lack of generalization in domains with target non-stationarity and collinearity, such as time-series forecasting. We offer guidance to address these difficulties and provide a framework that maximizes the chances of predictions that generalize well and deliver state-of-the-art performance. The techniques we offer for cross-validation, augmentation, and parameter tuning have been used to win several major time-series forecasting competitions---including the M5 Forecasting Uncertainty competition and the Kaggle COVID19 Forecasting series---and, with the proper theoretical grounding, constitute the current best practices in time-series forecasting.},
  langid = {english},
  keywords = {forecast,intro,obsLitNote},
  file = {/Users/scott/Library/CloudStorage/OneDrive-Personal/share/ref/zotero/Lainder22frcstGBTaugTuneCV.pdf}
}

@article{Bennasar15FeatureSelectionUsing,
  title = {Feature Selection Using {{Joint Mutual Information Maximisation}}},
  author = {Bennasar, Mohamed and Hicks, Yulia and Setchi, Rossitza},
  year = {2015},
  month = dec,
  journal = {Expert Systems with Applications},
  volume = {42},
  number = {22},
  pages = {8520--8532},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2015.07.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417415004674},
  urldate = {2023-06-21},
  abstract = {Feature selection is used in many application areas relevant to expert and intelligent systems, such as data mining and machine learning, image processing, anomaly detection, bioinformatics and natural language processing. Feature selection based on information theory is a popular approach due its computational efficiency, scalability in terms of the dataset dimensionality, and independence from the classifier. Common drawbacks of this approach are the lack of information about the interaction between the features and the classifier, and the selection of redundant and irrelevant features. The latter is due to the limitations of the employed goal functions leading to overestimation of the feature significance. To address this problem, this article introduces two new nonlinear feature selection methods, namely Joint Mutual Information Maximisation (JMIM) and Normalised Joint Mutual Information Maximisation (NJMIM); both these methods use mutual information and the `maximum of the minimum' criterion, which alleviates the problem of overestimation of the feature significance as demonstrated both theoretically and experimentally. The proposed methods are compared using eleven publically available datasets with five competing methods. The results demonstrate that the JMIM method outperforms the other methods on most tested public datasets, reducing the relative average classification error by almost 6\% in comparison to the next best performing method. The statistical significance of the results is confirmed by the ANOVA test. Moreover, this method produces the best trade-off between accuracy and stability.},
  langid = {english},
  annotation = {481 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Feature selection with Joint Mutual Information -- a one-to-one MI low dim MI technique like Deng\&Peng mRMR but better. * Is not the same as "JMI" method mentioned in Vergara14revFeatSelMutInfo, and found to be best in Brown12condLikInfoFeatSel Is tested on classification problems, many algorithms compared. * Overall, it seems that JMIM or JMI are the best, * JMI better on some problems but oveall JMIM is more accurate * JMIM possibly having a better accuracy/stability tradeoff. Weaknesses * Does not use high dim MI like Frenzel07partMutInfo and later use. * Is a Filter method so doesn't consider learning algorithm interaction (like a wrapper, I suppose)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bennasar15FeatureSelectionUsing.pdf}
}

@misc{Kafrouni23threshInPipeline,
  title = {How to Add {{Decision Threshold}} Tuning to Your End to End {{ML}} Pipelines},
  author = {Kafrouni, Jerome},
  year = {2023},
  month = jan,
  journal = {Medium},
  url = {https://towardsdatascience.com/how-to-add-decision-threshold-tuning-to-your-end-to-end-ml-pipelines-7077b82b71a},
  urldate = {2023-06-26},
  abstract = {In this article, I will introduce you to two packages I use when working with imbalanced data (imblearn~, although I also use it a lot, is{\dots}},
  langid = {english}
}

@misc{Draelos20avgPrecAUC,
  title = {The {{Complete Guide}} to {{AUC}} and {{Average Precision}}: {{Simulations}} and {{Visualizations}}},
  shorttitle = {The {{Complete Guide}} to {{AUC}} and {{Average Precision}}},
  year = {2020},
  month = jul,
  journal = {Glass Box},
  url = {https://glassboxmedicine.com/2020/07/14/the-complete-guide-to-auc-and-average-precision-simulations-and-visualizations/},
  urldate = {2023-07-05},
  abstract = {This post offers the clearest explanation on the web for how the popular metrics AUC (AUROC) and average precision can be used to understand how a classifier performs on balanced data, with the nex{\dots}},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Draelos20avgPrecAUC.pdf}
}

@inproceedings{Zhao22DatadrivenCyclecalendarCombined,
  title = {Data-Driven Cycle-Calendar Combined Battery Degradation Modeling for Grid Applications},
  booktitle = {2022 {{IEEE Power}} \& {{Energy Society General Meeting}} ({{PESGM}})},
  author = {Zhao, Chunyang and Andersen, Peter Bach and Tr{\ae}holt, Chresten and Hashemi, Seyedmostafa},
  year = {2022},
  pages = {1--5},
  publisher = {IEEE},
  abstract = {Battery degradation is the main uncertainty that  hedges the development of battery projects. In this work, we  build a data-driven battery degradation model assessing the  impact of the complex state of charge (SOC) operation  condition. Both cycle life and calendar life are incorporated,  based on the available lab testing data of battery cells. Various  degradation modeling functions are compared to acquire the  best fitting results under different depths of discharge (DOD)  ranges calculated by rainflow counting algorithm. The  statistical relation between shallow cycles and calendar time is  used to address the calendar degradation in the cycling scope. A  battery frequency regulation service case study is carried out  based on a one-year frequency record in the Nordic synchronous  area. Our work bridges the battery cell testing datasets to the  battery degradation modeling in grid applications and proposes  a new perspective to address the calendar life by the analysis of  the shallow cycles for the state of health (SOH) estimation,  which improves applicability and accuracy.  Keywords---Battery degradation, grid service, data-driven  model, cycle life, calendar life},
  keywords = {battery,battSOH},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhao22DatadrivenCyclecalendarCombined.pdf}
}

@article{Zhao23battSOHpartialDisch,
  title = {Data-Driven Battery Health Prognosis with Partial-Discharge Information},
  author = {Zhao, Chunyang and Andersen, Peter Bach and Tr{\ae}holt, Chresten and Hashemi, Seyedmostafa},
  year = {2023},
  journal = {Journal of Energy Storage},
  volume = {65},
  pages = {107151},
  publisher = {Elsevier},
  abstract = {The unpredictability of battery degradation behavior is a challenging issue impeding the development of battery  applications, due to the complexity of the degradation and the limitation of state measurement methods.  Nowadays, with accessible battery aging datasets and machine learning algorithms, there are opportunities for  data-driven battery health prognosis. However, most of the previous work is restricted in the scope of full-  discharge capacity records extrapolation, which has insufficient prospects in real-life applications. In this  work, we propose using partial discharge information for degradation estimation and prediction. Our Gaussian  process regression model achieves good performance by limited partial discharge information without re- quirements of feature selection. The accurate battery health prognosis in 300 cycles can be carried out by one  partial-discharge cycle at any degradation stage. The capacity estimation gives around 1 \% root mean square  error (RMSE) when using 30 \% information on the discharge process. As full-cycle discharge is not required, the  proposed model can diagnose the battery state of health (SOH) with a limited portion of battery operation in- formation extracted during the discharge process and reduce the effort of capacity tests. Further development of  this method brings opportunities for battery state evaluation and prediction in real applications with better  applicability and accuracy.},
  keywords = {battery,battSOH},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhao23battSOHpartialDisch.pdf}
}

@article{Hou22ReviewCriticalState,
  title = {A Review of Critical State Joint Estimation Methods of Lithium-Ion Batteries in Electric Vehicles},
  author = {Hou, Junjian and Li, Tong and Zhou, Fang and Zhao, Dengfeng and Zhong, Yudong and Yao, Lei and Zeng, Li},
  year = {2022},
  journal = {World Electric Vehicle Journal},
  volume = {13},
  number = {9},
  pages = {159},
  publisher = {MDPI},
  abstract = {Battery state of charge (SOC), state of health (SOH), and state of power (SOP) are decisive factors that influence the energy-management system (EMS) performance of electric vehicles. However, the accurate estimation of SOC, SOH, and SOP remains a challenge due to the high nonlinearity of the battery dynamic characteristics and the strong coupling among the states. In this paper, different methods of single-state and two-state joint estimation are classified and discussed, including SOC/SOH and SOC/SOP joint estimation methods, and their advantages and limitations are analyzed. On this basis, key issues of joint multi-state estimation are discussed, and suggestions for future work are made. Keywords: electric vehicle; Lithium-ion battery; core state; joint estimation; fusion technology},
  keywords = {battery,battFire,battSOC,battSOH,battSOP,intro,priorityRead},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hou22ReviewCriticalState.pdf}
}

@article{Ardiansyah22Seq2SeqRegressionLearningbased,
  title = {{{Seq2Seq}} Regression Learning-Based Multivariate and Multistep {{SOC}} Forecasting of {{BESS}} in Frequency Regulation Service},
  author = {{Ardiansyah} and Masood, Zaki and Choi, Deokjai and Choi, Yonghoon},
  year = {2022},
  month = dec,
  journal = {Sustainable Energy, Grids and Networks},
  volume = {32},
  pages = {100939},
  issn = {23524677},
  doi = {10.1016/j.segan.2022.100939},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2352467722001849},
  urldate = {2023-07-07},
  abstract = {Battery Energy Storage Systems (BESS) has a fast response time and high ramping capability, making it suitable for smart grid frequency regulation service. However, providing the required regulation capacity without a problem at any point in time is bound with the BESS state of charge (SOC) development periodically. Moreover, knowing SOC variation in advance will help the BESS producers keep SOC at the desired level. The BESS SOC can be maintained in the admissible operation area to avoid the mismatch penalties while following regulation signals. Therefore, the maximum service reward can also be achieved. Unfortunately, forecasting the SOC of BESS in frequency regulation service is not a straightforward problem. The forecasting method should deal with multiple dependent variables that periodically determine SOC's development. Moreover, developing a one-time manner multistep SOC forecasting model is also a challenge. To solve both problems, a sequence-to-sequence (seq2seq) regression learning architecture that has been proven to deal with sequentially interdependent data is adopted in our proposed forecasting framework. Various state-of-the-art memory cells in deep regression learning, i.e., long short-term memory (LSTM), gated recurrent unit (GRU), bi-directional (bi)-LSTM, and bi-GRU, were utilized and evaluated. The evaluation result shows that the developed models outperform the existing machine learning-based forecasting methods. The Bi-GRU cells provide the best performance in root mean square error (RMSE) and mean absolute error (MAE) evaluation metrics.},
  langid = {english},
  keywords = {battery,battSOC},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Same SOC definition as (Park et al., 2020)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ardiansyah22Seq2SeqRegressionLearningbased.pdf}
}

@article{Yao21ReviewLithiumionBattery,
  title = {A Review of Lithium-Ion Battery State of Health Estimation and Prediction Methods},
  author = {Yao, Lei and Xu, Shiming and Tang, Aihua and Zhou, Fang and Hou, Junjian and Xiao, Yanqiu and Fu, Zhijun},
  year = {2021},
  journal = {World Electric Vehicle Journal},
  volume = {12},
  number = {3},
  pages = {113},
  publisher = {MDPI},
  abstract = {Lithium-ion power batteries have been widely used in transportation due to their advan- tages of long life, high specific power, and energy. However, the safety problems caused by the inaccurate estimation and prediction of battery health state have attracted wide attention in academic circles. In this paper, the degradation mechanism and main definitions of state of health (SOH) were described by summarizing domestic and foreign literatures. The estimation and prediction methods of lithium-ion power battery SOH were discussed from three aspects: model-based methods, data-driven methods, and fusion technology methods. This review summarizes the advantages and disadvantages of the current mainstream SOH estimation and prediction methods. This paper believes that more innovative feature parameter extraction methods, multi-algorithm coupling, com- bined with cloud platform and other technologies will be the development trend of SOH estimation and prediction in the future, which provides a reference for health state estimation and prediction of lithium-ion power battery. Keywords: lithium-ion power battery; state of health; data-driven methods; fusion technology},
  keywords = {battery,battSOH,intro},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Yao21ReviewLithiumionBattery.pdf}
}

@article{Li19DatadrivenHealthEstimation,
  title = {Data-Driven Health Estimation and Lifetime Prediction of Lithium-Ion Batteries: {{A}} Review},
  shorttitle = {Data-Driven Health Estimation and Lifetime Prediction of Lithium-Ion Batteries},
  author = {Li, Yi and Liu, Kailong and Foley, Aoife M. and Z{\"u}lke, Alana and Berecibar, Maitane and {Nanini-Maury}, Elise and Van Mierlo, Joeri and Hoster, Harry E.},
  year = {2019},
  journal = {Renewable and sustainable energy reviews},
  volume = {113},
  pages = {109254},
  publisher = {Elsevier},
  abstract = {Accurate health estimation and lifetime prediction of lithium-ion batteries are crucial for  durable electric vehicles. Early detection of inadequate performance facilitates timely  maintenance of battery systems. This reduces operational costs and prevents accidents and  malfunctions. Recent advancements in ``Big Data'' analytics and related  statistical/computational tools raised interest in data-driven battery health estimation.  Here, we will review these in view of their feasibility and cost-effectiveness in dealing with  battery health in real-world applications. We categorise these methods according to their  underlying models/algorithms and discuss their advantages and limitations. In the final  section we focus on challenges of real-time battery health management and discuss potential  next-generation techniques. We are confident that this review will inform commercial  technology choices and academic research agendas alike, thus boosting progress in data- driven battery health estimation and prediction on all technology readiness levels.  Keywords: Lithium-ion battery, Data-driven approach, Ageing mechanism, Battery health  diagnostics and prognostics, Electric vehicle, Sustainable energy},
  keywords = {battery,battSOH},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Li19DatadrivenHealthEstimation.pdf}
}

@article{Che21PredictiveBatteryHealth,
  title = {Predictive Battery Health Management with Transfer Learning and Online Model Correction},
  author = {Che, Yunhong and Deng, Zhongwei and Lin, Xianke and Hu, Lin and Hu, Xiaosong},
  year = {2021},
  journal = {IEEE Transactions on Vehicular Technology},
  volume = {70},
  number = {2},
  pages = {1269--1277},
  publisher = {IEEE},
  abstract = {Significant progress has been made in  transportation electrification in recent years. As the main  energy storage device, lithium-ion batteries are one of the  key components that need to be properly managed. The  remaining useful life, which represents battery health, has  attracted increasing attention. Because accurate and  robust predictions provide important information for  predictive maintenance and cascade utilization. This paper  proposes a novel method to predict remaining useful life  based on the optimized health indicators and online model  correction with transfer learning. Gaussian process  regression is used to optimize the threshold for health  indicators to determine the end of life, and a usefulness  evaluation strategy is proposed to assess the health  indicators. Then, a combination of transfer learning and  gated recurrent neural network is designed to predict the  remaining useful life based on the optimized health  indicators directly, which can promote online applications.  The prediction model initially trained based on a relevant  battery is further fine-tuned according to the early  degradation cycling data of the test battery to provide  accurate predictions. Moreover, a self-correction strategy  is proposed to retrain the regression models so that the  models can gradually reach the optimal prediction  performance during the operating cycles, which could not  be achieved by traditional methods. The recommended  input sequence lengths for potential applications are  discussed. The method is verified by experiments of a  batch of batteries under fast charging conditions, and the  results show that, after fine-tuning, the proposed method  predicts remaining useful life with an error of fewer than 5  cycles.},
  keywords = {battery,battSOH},
  note = {This is a note from zotero},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Che21PredictiveBatteryHealth.pdf}
}

@article{Hu20BatteryHealthPrediction,
  title = {Battery Health Prediction Using Fusion-Based Feature Selection and Machine Learning},
  author = {Hu, Xiaosong and Che, Yunhong and Lin, Xianke and Onori, Simona},
  year = {2020},
  journal = {IEEE Transactions on Transportation Electrification},
  volume = {7},
  number = {2},
  pages = {382--398},
  publisher = {IEEE},
  abstract = {State of health (SOH) is a key parameter to assess  lithium-ion battery feasibility for secondary usage applications.  SOH estimation based on machine learning has attracted great  attention in recent years, and holds potentials for battery  informatization and cloud battery management techniques. In this  paper, a comprehensive study of the data-driven SOH estimation  methods is conducted. A new classification for health indicators  (HIs) is proposed where the HIs are divided into the measured  variables and calculated variables. To illustrate the significance of  data preprocessing, four noise reduction methods are assessed in  the HIs extraction process; different feature selection methods,  including filter-based method, wrapper-based method, and fusion- based method, are applied to select HIs subsets. The four widely  used machine learning algorithms, including artificial neural  network, support vector machine, relevance vector machine, and  Gaussian process regression, are applied and compared. In order  to evaluate the estimation performance in potential real usages  under future big data era, the three HIs selection methods and four  machine learning methods are evaluated using three public data  sets and two estimation strategies. The results show that the  combination of the fusion-based selection method and Gaussian  process regression has an overall superior estimation performance  in terms of both accuracy and computational efficiency.   Index Terms---lithium-ion batteries, state of health, feature  extraction, feature selection, machine learning, comprehensive  comparison},
  keywords = {battery,battSOH},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hu20BatteryHealthPrediction.pdf}
}

@article{Ryu23QuantileMixerNovelDeep,
  title = {Quantile-{{Mixer}}: {{A Novel Deep Learning Approach}} for {{Probabilistic Short-term Load Forecasting}}},
  shorttitle = {Quantile-{{Mixer}}},
  author = {Ryu, Seunghyoung and Yu, Yonggyun},
  year = {2023},
  journal = {IEEE Transactions on Smart Grid},
  pages = {1--1},
  issn = {1949-3061},
  doi = {10.1109/TSG.2023.3290180},
  abstract = {As the power grid becomes more complex and dynamic, accurate short-term load forecasting (STLF) with probabilistic information is a prerequisite for various smart grid applications. For doing this, various deep learning models have been proposed, and recent models increase model size and complexity to achieve better accuracy which could also increase the burden on model design, computation time, and resources. To this end, we propose a novel deep learning model for accurate and efficient probabilistic STLF (PSTLF). First, we develop an STLF model utilizing the multi-layer perceptron (MLP)-mixer structure, i.e., MLP-mixer for STLF (MM-STLF), that has an advantage in forecasting accuracy and efficiency compared to the other deep learning models. Then, we propose a random quantile regression (RQR) method that takes a cumulative probability {$\tau$} as an input to the model and is trained on random {$\tau$}s. By combining MM-STLF and RQR, we develop a novel deep-PSTLF model, namely quantile-mixer (Q-mixer). We evaluate the overall performance of the proposed model with seven load datasets in terms of prediction error, model size, and inference time, respectively. Through experiments, various STLF models and probabilistic forecasting methods are compared, and the experimental results demonstrate the effectiveness of Q-mixer in load forecasting.},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\Zotero\storage\5TR7W4MU\10171210.html}
}

@article{Chicco20AdvantagesMatthewsCorrelation,
  title = {The Advantages of the {{Matthews}} Correlation Coefficient ({{MCC}}) over {{F1}} Score and Accuracy in Binary Classification Evaluation},
  author = {Chicco, Davide and Jurman, Giuseppe},
  year = {2020},
  month = jan,
  journal = {BMC Genomics},
  volume = {21},
  number = {1},
  pages = {6},
  issn = {1471-2164},
  doi = {10.1186/s12864-019-6413-7},
  url = {https://doi.org/10.1186/s12864-019-6413-7},
  urldate = {2023-07-07},
  abstract = {To evaluate binary classifications and their confusion matrices, scientific researchers can employ several statistical rates, accordingly to the goal of the experiment they are investigating. Despite being a crucial issue in machine learning, no widespread consensus has been reached on a unified elective chosen measure yet. Accuracy and F1 score computed on confusion matrices have been (and still are) among the most popular adopted metrics in binary classification tasks. However, these statistical measures can dangerously show overoptimistic inflated results, especially on imbalanced datasets.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chicco20AdvantagesMatthewsCorrelation.pdf}
}

@article{Khaleghi19DevelopingRealtimeDatadriven,
  title = {Developing a Real-Time Data-Driven Battery Health Diagnosis Method, Using Time and Frequency Domain Condition Indicators},
  author = {Khaleghi, S. and Firouz, Y. and Van Mierlo, J. and Van Den Bossche, P.},
  year = {2019},
  month = dec,
  journal = {Applied Energy},
  volume = {255},
  pages = {113813},
  issn = {03062619},
  doi = {10.1016/j.apenergy.2019.113813},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0306261919315004},
  urldate = {2023-07-07},
  abstract = {Lithium-ion batteries are considered as promising electric energy storage systems. However, identification of battery health is a critical issue. Furthermore, battery aging extremely depends on operating conditions. Therefore, monitoring and analysis of battery health degradation in real-time systems such as electric vehicles, in which a variety of stress factors may come into play, are demanded. This paper proposes a data-driven algorithm based on multiple condition indicator to estimate battery health using application-based load profiles. In this regard, battery cells have been cycled under a worldwide light duty driving test cycle (WLTC) load profile in laboratory to acquire real-world driving data. Time-domain and frequency-domain condition indicators are extracted from measured on-board data like voltage and current within certain time intervals, enabling real-time investigation of battery health degradation. The condition indicators have been fed into a Gaussian process estimator to track the real-time state of health (SoH). As degradation strongly depends on magnitude of input current, it is important that the proposed method can predict health of the cell regardless of current amplitude and aging pattern. Therefore, to assess accuracy and robustness of the proposed method, it is validated using a different load profile with distinct depth of discharge, current amplitude, and distinctive aging pattern. Results reveal the proposed approach is highly precise and is capable of estimating battery SoH with low computational costs and a relative error of less than 1\%. The proposed technique is promising for online diagnostics of battery health thanks to its high accuracy and robustness.},
  langid = {english},
  keywords = {battery,battSOH},
  annotation = {46 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {review
\par
also uses spectrum: (Lyu et al., 2022)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Khaleghi19DevelopingRealtimeDatadriven.pdf}
}

@article{Chicco21MatthewsCorrelationCoefficient,
  title = {The {{Matthews}} Correlation Coefficient ({{MCC}}) Is More Reliable than Balanced Accuracy, Bookmaker Informedness, and Markedness in Two-Class Confusion Matrix Evaluation},
  author = {Chicco, Davide and T{\"o}tsch, Niklas and Jurman, Giuseppe},
  year = {2021},
  month = feb,
  journal = {BioData Mining},
  volume = {14},
  number = {1},
  pages = {13},
  issn = {1756-0381},
  doi = {10.1186/s13040-021-00244-z},
  url = {https://doi.org/10.1186/s13040-021-00244-z},
  urldate = {2023-07-11},
  abstract = {Evaluating binary classifications is a pivotal task in statistics and machine learning, because it can influence decisions in multiple areas, including for example prognosis or therapies of patients in critical conditions. The scientific community has not agreed on a general-purpose statistical indicator for evaluating two-class confusion matrices (having true positives, true negatives, false positives, and false negatives) yet, even if advantages of the Matthews correlation coefficient (MCC) over accuracy and F1 score have already been shown.In this manuscript, we reaffirm that MCC is a robust metric that summarizes the classifier performance in a single value, if positive and negative cases are of equal importance. We compare MCC to other metrics which value positive and negative cases equally: balanced accuracy (BA), bookmaker informedness (BM), and markedness (MK). We explain the mathematical relationships between MCC and these indicators, then show some use cases and a bioinformatics scenario where these metrics disagree and where MCC generates a more informative response.Additionally, we describe three exceptions where BM can be more appropriate: analyzing classifications where dataset prevalence is unrepresentative, comparing classifiers on different datasets, and assessing the random guessing level of a classifier. Except in these cases, we believe that MCC is the most informative among the single metrics discussed, and suggest it as standard measure for scientists of all fields. A Matthews correlation coefficient close to +1, in fact, means having high values for all the other confusion matrix metrics. The same cannot be said for balanced accuracy, markedness, bookmaker informedness, accuracy and F1 score.},
  annotation = {12 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chicco21MatthewsCorrelationCoefficient.pdf}
}

@article{Ward22PrinciplesBatteryData,
  title = {Principles of the Battery Data Genome},
  author = {Ward, Logan and Babinec, Susan and Dufek, Eric J. and Howey, David A. and Viswanathan, Venkatasubramanian and Aykol, Muratahan and Beck, David AC and Blaiszik, Benjamin and Chen, Bor-Rong and Crabtree, George},
  year = {2022},
  journal = {Joule},
  volume = {6},
  number = {10},
  pages = {2253--2271},
  publisher = {Elsevier},
  abstract = {Batteries are central to modern society. They are no longer just a convenience but a critical enabler of the transition to a resilient, low-carbon economy. Battery development capabilities are provided by communities spanning materials discovery, battery chemistry and electrochemistry, cell and pack design, scale-up, manufacturing, and deployments. Despite their relative maturity, data-science practices among these diverse groups are far behind the state of the art in other fields, which have demonstrated an abil- ity to significantly improve innovation and economic impact. The negative consequences of the present paradigm include incremen- tal improvements but few breakthroughs, significant manufacturing uncertainties, and cascading investment risks that collectively slow deployments. The primary roadblock to a battery-data-science re- naissance is the requirement for large amounts of high-quality data, which are not available in the current fragmented ecosystem. Here, we identify gaps and propose principles that enable the solu- tion by building a robust community of data hubs with standardized practices and flexible sharing options that will seed advanced tools spanning innovation to deployment. Precedents are offered that demonstrate that both public good and immense economic gains will arise from sharing valuable battery data. The proposed Battery Data Genome looks to broadly transform innovations and revolu- tionize their translation from research to societal impact.},
  keywords = {battery,battFire,battSOC,dataSrc,intro},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ward22PrinciplesBatteryData.pdf}
}

@article{Kunz21EarlyBatteryPerformance,
  title = {Early Battery Performance Prediction for Mixed Use Charging Profiles Using Hierarchal Machine Learning},
  author = {Kunz, M. Ross and Dufek, Eric J. and Yi, Zonggen and Gering, Kevin L. and Shirk, Matthew G. and Smith, Kandler and Chen, Bor-Rong and Wang, Qiang and Gasper, Paul and Bewley, Randy L.},
  year = {2021},
  journal = {Batteries \& Supercaps},
  volume = {4},
  number = {7},
  pages = {1186--1196},
  publisher = {Wiley Online Library},
  keywords = {battery,battSOH},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kunz21EarlyBatteryPerformancea.pdf}
}

@article{Gasper20IdentificationLifeModels,
  title = {Identification of {{Life Models}} for {{Li-Ion Batteries Using Penalized Regression}} and {{Bilevel Optimization}}},
  author = {Gasper, Paul and Smith, Kandler},
  year = {2020},
  month = nov,
  journal = {ECS Meeting Abstracts},
  volume = {MA2020-02},
  number = {6},
  pages = {1055},
  publisher = {IOP Publishing},
  issn = {2151-2043},
  doi = {10.1149/MA2020-0261055mtgabs},
  url = {https://iopscience.iop.org/article/10.1149/MA2020-0261055mtgabs/meta},
  urldate = {2023-07-20},
  langid = {english},
  keywords = {battery,battSOH},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gasper20IdentificationLifeModels.pdf}
}

@article{Finegan21ApplicationDataDrivenMethods,
  title = {The {{Application}} of {{Data-Driven Methods}} and {{Physics-Based Learning}} for {{Improving Battery Safety}}},
  author = {Finegan, Donal P. and Zhu, Juner and Feng, Xuning and Keyser, Matt and Ulmefors, Marcus and Li, Wei and Bazant, Martin Z. and Cooper, Samuel J.},
  year = {2021},
  month = feb,
  journal = {Joule},
  volume = {5},
  number = {2},
  pages = {316--329},
  publisher = {Elsevier},
  issn = {2542-4785, 2542-4351},
  doi = {10.1016/j.joule.2020.11.018},
  url = {https://www.cell.com/joule/abstract/S2542-4351(20)30562-6},
  urldate = {2023-07-20},
  abstract = {Enabling accurate prediction of battery failure will lead to safer bat- tery systems, as well as accelerating cell design and manufacturing processes for increased consistency and reliability. Data-driven pre- diction methods have shown promise for accurately predicting cell behaviors with low computational cost, but they are expensive to train. Furthermore, given that the risk of battery failure is already very low, gathering enough relevant data to facilitate data-driven predictions is extremely challenging. Here, a perspective for designing experiments to facilitate a relatively low number of tests, handling the data, applying data-driven methods, and improving our understanding of behavior-dictating physics is outlined. This perspective starts with effective strategies for experimentally repli- cating rare failure scenarios and thus reducing the number of exper- iments, and proceeds to describe means to acquire high-quality datasets, apply data-driven prediction techniques, and to extract physical insights into the events that lead to failure by incorporating physics into data-driven approaches.},
  langid = {english},
  keywords = {battery,battFire,intro},
  annotation = {84 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Finegan21ApplicationDataDrivenMethods.pdf}
}

@article{Gasper22battLifeMdlRedcOrder,
  title = {Machine-{{Learning Assisted Identification}} of {{Accurate Battery Lifetime Models}} with {{Uncertainty}}},
  author = {Gasper, Paul and Collath, Nils and Hesse, Holger C. and Jossen, Andreas and Smith, Kandler},
  year = {2022},
  month = aug,
  journal = {Journal of The Electrochemical Society},
  volume = {169},
  number = {8},
  pages = {080518},
  publisher = {IOP Publishing},
  issn = {1945-7111},
  doi = {10.1149/1945-7111/ac86a8},
  url = {https://dx.doi.org/10.1149/1945-7111/ac86a8},
  urldate = {2023-07-20},
  abstract = {Reduced-order battery lifetime models, which consist of algebraic expressions for various aging modes, are widely utilized for extrapolating degradation trends from accelerated aging tests to real-world aging scenarios. Identifying models with high accuracy and low uncertainty is crucial for ensuring that model extrapolations are believable, however, it is difficult to compose expressions that accurately predict multivariate data trends; a review of cycling degradation models from literature reveals a wide variety of functional relationships. Here, a machine-learning assisted model identification method is utilized to fit degradation in a stand-out LFP-Gr aging data set, with uncertainty quantified by bootstrap resampling. The model identified in this work results in approximately half the mean absolute error of a human expert model. Models are validated by converting to a state-equation form and comparing predictions against cells aging under varying loads. Parameter uncertainty is carried forward into an energy storage system simulation to estimate the impact of aging model uncertainty on system lifetime. The new model identification method used here reduces life-prediction uncertainty by more than a factor of three (86\% {\textpm} 5\% relative capacity at 10 years for human-expert model, 88.5\% {\textpm} 1.5\% for machine-learning assisted model), empowering more confident estimates of energy storage system lifetime.},
  langid = {english},
  keywords = {battery,battSOH},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gasper22battLifeMdlRedcOrder.pdf}
}

@misc{Baru23predMaintLife3ways,
  title = {Three {{Ways}} to {{Estimate Remaining Useful Life}} for {{Predictive Maintenance}}},
  shorttitle = {Mathworks},
  author = {Baru, Aditya and Johnson, Rachael},
  year = {2023},
  journal = {Three Ways to Estimate Remaining Useful Life for Predictive Maintenance},
  url = {https://www.mathworks.com/company/newsletters/articles/three-ways-to-estimate-remaining-useful-life-for-predictive-maintenance.html},
  urldate = {2023-07-20},
  abstract = {This article presents models for estimating RUL from three types of data: lifetime data, run-to-failure data, and threshold values.},
  langid = {english},
  keywords = {battery,battSOH,intro,noted},
  note = {\subsubsection{Baru23predMaintLife3ways}

\par
The kind of data you have determines how you can predict the remaining useful life (RUL) of something
\par
\section{\textbf{The Three RUL Methods}}

\subsection{\textbf{I. Lifetime Data}}

\par
\textbf{Data}: Just the single~\emph{lifetime number}~of a bunch of similar historic batteries
\par
\textbf{Algorithm}: Use a~\href{app://obsidian.md/Proportional\%20Hazard\%20Model}{Proportional Hazard Model}~to predict the prob. dist. of failure times, usually conditional upon some covariates e.g. given battery charging cycles, you predict the fraction of life remaining -- a~\href{app://obsidian.md/Survival\%20Function}{Survival Function}~plot which looks to me like 1 - cdf to me:
\par
(Baru and Johnson, 2023, p. 1)
\par
In this graph, it looks like they've got actual failures in their data; can this be done by extrapolation, as I've seen either done or mentioned in other papers that do extrapolation. \#Battery/SOH/Extrapolation
\par
\subsection{\textbf{II. Run-to-Failure Data}}

\par
\textbf{Algorithm}: To forecast remaining life for some test device, you pick the historical time series that looks most like the test device's (which includes multiple covariates, not just SOC or whatever); your forecast is failure time of the historical device with the best time series match.
\par
(Baru and Johnson, 2023, p. 2)
\par
\subsection{\textbf{III. Threshold Data}}

\par
\textbf{Data}: You have~\emph{no end-of-life data}, but you do know that, when certain~\emph{measurements}~are above~\emph{known thresholds}, the device will certainly fail.\\
\textbf{Algorithm}~So again, you do a sort of~\textbf{\href{app://obsidian.md/index.html\#Battery/SOH/Extrapolation}{\#Battery/SOH/Extrapolation}}, for the future of the current measurement values. My impression is that this~\emph{extrapolation method}~is physics based or follows a chosen statistical distributions e.g. (Hou et al., 2022)
\par
You can simplify by compressing the set of measurement values to a low dimension:
\par
\begin{itemize}

\item e.g. PCA, probably to the 1st princ comp.
\item PCA can also transforms the thresholds linearly, so you then have a scalar (or low dim) threshold to check against
\item 
\par
A physics based method, like in (Shah et al., 2016)
\par
\end{itemize}

\section{\textbf{Matlab Tools and Video Tutorials}}

\begin{itemize}

\item Matlab has a a Predictive Maintenance toolbox
\item 
\par
Matlab Videos
\par
\begin{itemize}

\item \href{https://www.youtube.com/watch?v=Dd_4rbWYgI4}{Estimating Remaining Useful Life (RUL) {\textbar} Predictive Maintenance}
\item \href{https://www.mathworks.com/videos/predictive-maintenance-part-1-introduction-1545827554336.html?s_tid=vid_pers_recs}{Introduction {\textbar} Predictive Maintenance, Part 1}
\item \href{https://www.mathworks.com/videos/predictive-maintenance-part-2-feature-extraction-for-identifying-condition-indicators-1547125501847.html?s_tid=vid_pers_recs}{Feature Extraction for Identifying Condition Indicators {\textbar} Predictive Maintenance, Part 2}
\item \href{https://www.mathworks.com/videos/predictive-maintenance-part-3-remaining-useful-life-estimation-1549881037621.html}{Remaining Useful Life Estimation {\textbar} Predictive Maintenance, Part 3}
\item \href{https://www.mathworks.com/videos/predictive-maintenance-part-4-how-to-use-diagnostic-feature-designer-for-feature-extraction-1554458327719.html?s_tid=vid_pers_recs}{How to Use Diagnostic Feature Designer for Feature Extraction {\textbar} Predictive Maintenance, Part 4}

\end{itemize}

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Baru23predMaintLife3ways.pdf}
}

@inproceedings{Zhang21EarlyPredictionLithiumion,
  title = {The Early Prediction of Lithium-Ion Battery Remaining Useful Life Using a Novel {{Long Short-Term Memory}} Network},
  booktitle = {2021 {{IEEE}} 16th {{Conference}} on {{Industrial Electronics}} and {{Applications}} ({{ICIEA}})},
  author = {Zhang, Meng and Wu, Lifeng and Peng, Zhen},
  year = {2021},
  month = aug,
  pages = {1364--1371},
  issn = {2158-2297},
  doi = {10.1109/ICIEA51954.2021.9516254},
  url = {https://ieeexplore.ieee.org/document/9516254},
  abstract = {Accurate prediction of the lithium-ion battery remaining useful life can effectively manage the lithium-ion battery health. Using the early cycle data to predict the remaining useful life can reduce consumption and detect battery failures earlier, but it is still a great challenge due to weak and high dimensional nonlinear feature data of the early cycle. In order to solve this issue, this paper proposes a Long Short-Term Memory (LSTM) model that combines the idea of Broad Learning System (BLS), called BLS-LSTM, to accurately forecast the lithium-ion battery remaining useful life by using early cycle data. Firstly, according to the BLS idea, more effective feature nodes are obtained by performing mapping operations and enhancement operations on input features. Secondly, the characteristic nodes are input into the LSTM as new input nodes to predict the remaining useful life of the lithium-ion battery. Finally, the proposed model is validated with different early cycle data and compared with other methods. The results show that the BLS-LSTM model has better prediction performance and higher accuracy in the early prediction of the remaining useful life.},
  keywords = {battery,battSOH},
  annotation = {2 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\Zotero\storage\D63IKHED\9516254.html}
}

@inproceedings{Gupta21TransferLearningLSTM,
  title = {Transfer Learning {{LSTM}} Model for Battery Useful Capacity Fade Prediction},
  booktitle = {2021 24th {{International Conference}} on {{Mechatronics Technology}} ({{ICMT}})},
  author = {Gupta, Aniruddha and Sheikh, Muhammad and Tripathy, Yashraj and Widanage, W. Dhammika},
  year = {2021},
  month = dec,
  pages = {1--6},
  doi = {10.1109/ICMT53429.2021.9687230},
  abstract = {Lithiumion (Li-ion) batteries have become increasingly useful within the automotive industry and modern life applications due to high energy and power densities. However, these batteries suffer capacity loss due to different ageing mechanisms in various applications. Despite several existing models, lack of accurate predictability of capacity degradation limits the advancement of Li-ion batteries. The present work focuses on prediction of battery useful capacity degradation using long-short term memory (LSTM) transfer learning neural network model. At first, a base model was developed and trained using all the (100\%) degradation data available at 0{$^\circ$}C and 10{$^\circ$}C environmental temperatures. Thereafter, the training of the base model was fixed, and additional hidden layers were added on top of the base model to further fine tune it with only the initial 30\% degradation data available at 25{$^\circ$}C environmental temperature. The remaining (70\%) data of the 25{$^\circ$}C case was tested for model prediction. To decide the number of fixed hidden layers to be transferred from base model to transfer model and the number of additional hidden layers on top, an optimization for minimum cross validation error was performed. It was found that the resulting model was able to forecast the remaining battery degradation with 96\% accuracy. The model prediction was also compared with LSTM deep learning architecture without using transfer learning. The LSTM with transfer learning model was found to be 17\% higher in prediction accuracy than that without utilizing transfer learning.},
  keywords = {battery,battSOH},
  annotation = {2 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\Zotero\storage\PBNY5PVF\9687230.html}
}

@article{SanthiraSekeran22TransferabilityBatteryCell,
  title = {Transferability of a {{Battery Cell End-of-Life Prediction Model Using Survival Analysis}}},
  author = {Santhira Sekeran, Maya and {\v Z}ivadinovi{\'c}, Milan and Spiliopoulou, Myra},
  year = {2022},
  month = jan,
  journal = {Energies},
  volume = {15},
  number = {8},
  pages = {2930},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1996-1073},
  doi = {10.3390/en15082930},
  url = {https://www.mdpi.com/1996-1073/15/8/2930},
  urldate = {2023-07-22},
  abstract = {Electric vehicles are increasingly becoming the vehicle of choice in today's environmentally conscious society, and the heart of an electric vehicle is its battery. Today, lithium-ion batteries are mainly used to power electric vehicles for its increased energy storage density and longevity. However, in order to estimate battery life, long and costly battery testing is required. Therefore, there is a need to investigate efficient ways that could reduce the amount of testing required by reusing existing knowledge of aging patterns from different kinds of battery chemistry. This work aims to answer two research questions. The first addresses the challenge of battery cell testing data that contain battery cells that do not reach the End-of-Life (EOL) threshold by the time the testing has been completed. For this challenge, we propose to implement survival analysis that is able to handle incomplete data or what is referred to as censored data. The second addresses how to reuse a model trained on one type of battery cell chemistry to predict the EOL of another battery cell chemistry by implementing transfer learning. We develop a workflow to implement a prediction model for one type of battery cell chemistry and to reuse this pre-trained model to predict the EOL for another type of battery cell chemistry.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {battery,battSOH},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\SanthiraSekeran22TransferabilityBatteryCell.pdf}
}

@article{Wang21CriticalReviewOnline,
  title = {A {{Critical Review}} of {{Online Battery Remaining Useful Lifetime Prediction Methods}}},
  author = {Wang, Shunli and Jin, Siyu and Deng, Dan and Fernandez, Carlos},
  year = {2021},
  journal = {Frontiers in Mechanical Engineering},
  volume = {7},
  issn = {2297-3079},
  url = {https://www.frontiersin.org/articles/10.3389/fmech.2021.719718},
  urldate = {2023-07-22},
  abstract = {Lithium-ion batteries play an important role in our daily lives. The prediction of the remaining service life of lithium-ion batteries has become an important issue. This article reviews the methods for predicting the remaining service life of lithium-ion batteries from three aspects: machine learning, adaptive filtering, and random processes. The purpose of this study is to review, classify and compare different methods proposed in the literature to predict the remaining service life of lithium-ion batteries. This article first summarizes and classifies various methods for predicting the remaining service life of lithium-ion batteries that have been proposed in recent years. On this basis, by selecting specific criteria to evaluate and compare the accuracy of different models, find the most suitable method. Finally, summarize the development of various methods. According to the research in this article, the average accuracy of machine learning is 32.02\% higher than the average of the other two methods, and the prediction cycle is 9.87\% shorter than the average of the other two methods.},
  keywords = {battery,battSOH,intro},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wang21CriticalReviewOnline.pdf}
}

@article{Edge21LithiumIonBattery,
  title = {Lithium Ion Battery Degradation: What You Need to Know},
  shorttitle = {Lithium Ion Battery Degradation},
  author = {Edge, Jacqueline S. and O'Kane, Simon and Prosser, Ryan and D.~Kirkaldy, Niall and N.~Patel, Anisha and Hales, Alastair and Ghosh, Abir and Ai, Weilong and Chen, Jingyi and Yang, Jiang and Li, Shen and Pang, Mei-Chin and Diaz, Laura Bravo and Tomaszewska, Anna and Waseem~Marzook, M. and N.~Radhakrishnan, Karthik and Wang, Huizhi and Patel, Yatish and Wu, Billy and J.~Offer, Gregory},
  year = {2021},
  journal = {Physical Chemistry Chemical Physics},
  volume = {23},
  number = {14},
  pages = {8200--8221},
  publisher = {Royal Society of Chemistry},
  doi = {10.1039/D1CP00359C},
  url = {https://pubs.rsc.org/en/content/articlelanding/2021/cp/d1cp00359c},
  urldate = {2023-07-22},
  abstract = {The expansion of lithium-ion batteries from consumer electronics to larger-scale transport and energy storage applications has made understanding the many mechanisms responsible for battery degradation increasingly important. The literature in this complex topic has grown considerably; this perspective aims to distil current knowledge into a succinct form, as a reference and a guide to understanding battery degradation. Unlike other reviews, this work emphasises the coupling between the different mechanisms and the different physical and chemical approaches used to trigger, identify and monitor various mechanisms, as well as the various computational models that attempt to simulate these interactions. Degradation is separated into three levels: the actual mechanisms themselves, the observable consequences at cell level called modes and the operational effects such as capacity or power fade. Five principal and thirteen secondary mechanisms were found that are generally considered to be the cause of degradation during normal operation, which all give rise to five observable modes. A flowchart illustrates the different feedback loops that couple the various forms of degradation, whilst a table is presented to highlight the experimental conditions that are most likely to trigger specific degradation mechanisms. Together, they provide a powerful guide to designing experiments or models for investigating battery degradation.},
  langid = {english},
  keywords = {battery,battFire,battSOH,intro},
  annotation = {147 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Edge21LithiumIonBattery.pdf}
}

@article{Susilo18StateHealthEstimation,
  title = {State of {{Health Estimation}} of {{Lithium-Ion Batteries Based}} on {{Combination}} of {{Gaussian Distribution Data}} and {{Least Squares Support Vector Machines Regression}}},
  author = {Susilo, Didik Djoko and Widodo, Achmad and Prahasto, Toni and Nizam, Muhammad},
  year = {2018},
  journal = {Materials Science Forum},
  volume = {929},
  pages = {93--102},
  publisher = {Trans Tech Publications Ltd},
  issn = {1662-9752},
  doi = {10.4028/www.scientific.net/MSF.929.93},
  url = {https://www.scientific.net/MSF.929.93},
  urldate = {2023-07-22},
  abstract = {Lithium-ion batteries play a critical role in the reliability and safety of a system. Battery health monitoring and remaining useful life (RUL) prediction are needed to prevent catastrophic failure of the battery. The aim of this research is to develop a data-driven method to monitor the batteries state of health and predict their RUL by using the battery capacity degradation data. This paper also investigated the effect of prediction starting point to the RUL prediction error. One of the data-driven method drawbacks is the need of a large amount of data to obtain accurate prediction. This paper proposed a method to generate a series of degradation data that follow the Gaussian distribution based on limited battery capacity degradation data. The prognostic model was constructed from the new data using least square support vector machine (LSSVM) regression. The remaining useful life prediction was carried out by extrapolating the model until reach the end of life threshold. The method was applied to three differences lithium-ion batteries capacity data. The results showed that the proposed method has good performance. The method can predict the lithium-ion batteries RUL with a small error, and the optimal RUL starting point was found at the point where the battery has experienced the highest capacity recovery due to the self-recharge phenomenon.},
  langid = {english},
  keywords = {battery,battSOH},
  annotation = {5 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Susilo18StateHealthEstimation.pdf}
}

@article{Chen18SimplifiedAnalysisPredict,
  title = {A Simplified Analysis to Predict the Fire Hazard of Primary Lithium Battery},
  author = {Chen, Mingyi and Liu, Jiahao and Dongxu, Ouyang and Cao, Shuchao and Wang, Zhi and Wang, Jian},
  year = {2018},
  journal = {Applied Sciences},
  volume = {8},
  number = {11},
  pages = {2329},
  publisher = {MDPI},
  keywords = {battery,battFire},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chen18SimplifiedAnalysisPredict.pdf}
}

@misc{Kuipers23mechBattLifeMdl,
  title = {Blog -- {{Mechanistic Modeling}} in {{Batteries}} to {{Influence Lifetime}}},
  author = {Kuipers, Matthias},
  url = {https://www.accure.net/battery-knowledge/mechanistic-modeling-battery-aging?utm_campaign=Evergreen%20Newsletter&utm_medium=email&_hsmi=267213651&_hsenc=p2ANqtz-9h-lLWTVx7YPk0ElW5ZzzFrbfvhTL4sC8hPG2bMuWgYAsw7ykq2OChF8tp1LVgesxH0WB8gPXTPxakhTheL7zq4ekrOTjuh0fQBaRwA3BTbbiE6DQ&utm_content=267214897&utm_source=hs_email},
  urldate = {2023-07-22},
  abstract = {Experts use ``aging models'' to replicate how a battery degrades throughout its lifetime. Mechanistic modeling focuses on what is happening within the battery.},
  langid = {english},
  keywords = {battery,battSOH,intro},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kuipers23mechBattLifeMdl.pdf}
}

@article{Liang21sleepStagePred,
  title = {A {{Multi-Level Classification Approach}} for {{Sleep Stage Prediction With Processed Data Derived From Consumer Wearable Activity Trackers}}},
  author = {Liang, Zilu and {Chapa-Martell}, Mario Alberto},
  year = {2021},
  journal = {Frontiers in Digital Health},
  volume = {3},
  issn = {2673-253X},
  url = {https://www.frontiersin.org/articles/10.3389/fdgth.2021.665946},
  urldate = {2023-07-24},
  abstract = {Consumer wearable activity trackers, such as Fitbit are widely used in ubiquitous and longitudinal sleep monitoring in free-living environments. However, these devices are known to be inaccurate for measuring sleep stages. In this study, we develop and validate a novel approach that leverages the processed data readily available from consumer activity trackers (i.e., steps, heart rate, and sleep metrics) to predict sleep stages. The proposed approach adopts a selective correction strategy and consists of two levels of classifiers. The level-I classifier judges whether a Fitbit labeled sleep epoch is misclassified, and the level-II classifier re-classifies misclassified epochs into one of the four sleep stages (i.e., light sleep, deep sleep, REM sleep, and wakefulness). Best epoch-wise performance was achieved when support vector machine and gradient boosting decision tree (XGBoost) with up sampling were used, respectively at the level-I and level-II classification. The model achieved an overall per-epoch accuracy of 0.731 {\textpm} 0.119, Cohen's Kappa of 0.433 {\textpm} 0.212, and multi-class Matthew's correlation coefficient (MMCC) of 0.451 {\textpm} 0.214. Regarding the total duration of individual sleep stage, the mean normalized absolute bias (MAB) of this model was 0.469, which is a 23.9\% reduction against the proprietary Fitbit algorithm. The model that combines support vector machine and XGBoost with down sampling achieved sub-optimal per-epoch accuracy of 0.704 {\textpm} 0.097, Cohen's Kappa of 0.427 {\textpm} 0.178, and MMCC of 0.439 {\textpm} 0.180. The sub-optimal model obtained a MAB of 0.179, a significantly reduction of 71.0\% compared to the proprietary Fitbit algorithm. We highlight the challenges in machine learning based sleep stage prediction with consumer wearables, and suggest directions for future research.},
  note = {Liang21sleepStagePred
\par
Says Matthew's Corr coeff is
\par
``one of the best performance measure for multiclass imbalanced classification'' (Liang and Chapa-Martell, 2021, p. 6)
\par
However, Straube14infreqPerfEstImbal says that it's affected by imbalance (my notes in energy.bib)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Liang21sleepStagePred.pdf}
}

@article{Grewal20LithiumBatteryRisks,
  title = {Rechargeable Lithium Battery Fire Risks},
  author = {Grewal, Devinder},
  year = {2020},
  month = dec,
  journal = {Fathom Engineering},
  url = {https://www.marinwater.org/sites/default/files/2020-12/rechargeable%20lithium%20battery%20fire%20risk%20presentation.pdf},
  langid = {english},
  keywords = {battery,battFire},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Grewal20LithiumBatteryRisks.pdf}
}

@article{Wang19FireLiBattHeatSOC,
  title = {Fire Behavior of Lithium-Ion Battery with Different States of Charge Induced by High Incident Heat Fluxes},
  author = {Wang, Zhi and Ouyang, Dongxu and Chen, Mingyi and Wang, Xuehui and Zhang, Zheng and Wang, Jian},
  year = {2019},
  month = jun,
  journal = {Journal of Thermal Analysis and Calorimetry},
  volume = {136},
  number = {6},
  pages = {2239--2247},
  issn = {1588-2926},
  doi = {10.1007/s10973-018-7899-y},
  url = {https://doi.org/10.1007/s10973-018-7899-y},
  urldate = {2023-07-24},
  abstract = {Numerous accidents including fires and explosions occurring in transportation and storage of batteries enhance the need of studying the fire performances on batteries. In the current work, series bench-scale tests are conducted using a cone calorimeter to explore the fire behavior of lithium-ion battery. The influence of two key factors, namely state of charge (SOC) and incident external heat flux, on the battery fire characteristics is especially investigated. Combustion behavior, time to ignition (TTI), heat release rate (HRR) and fire risk assessment are obtained. The battery with higher SOC under high incident heat flux presents a fierce combustion process and higher surface temperature than the others. It is noteworthy that the time to ignition and time to peak HRR (TTP) decrease with the SOC, whereas the peak HRR (pHRR) and total heat release (THR) increase overall. On the whole, the increment of incident heat flux causes the decrease in TTI and TTP, while the pHRR and THR increase at higher incident heat flux. And the values of TTI agree well with the classical ignition model. Finally, the fire risk of battery with different SOCs under various incident heat fluxes is also highlighted.},
  langid = {english},
  keywords = {battery,battFire},
  annotation = {43 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wang19FireLiBattHeatSOC.pdf}
}

@article{Lee19IncrementalCapacityCurve,
  title = {Incremental {{Capacity Curve Peak Points-Based Regression Analysis}} for the {{State-of-Health Prediction}} of a {{Retired LiNiCoAlO2 Series}}/{{Parallel Configured Battery Pack}}},
  author = {Lee, Hyunjun and Park, Jounghu and Kim, Jonghoon},
  year = {2019},
  month = oct,
  journal = {Electronics},
  volume = {8},
  number = {10},
  pages = {1118},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-9292},
  doi = {10.3390/electronics8101118},
  url = {https://www.mdpi.com/2079-9292/8/10/1118},
  urldate = {2023-07-24},
  abstract = {To recycle retired series/parallel battery packs, it is necessary to know their state-of-health (SOH) correctly. Unfortunately, voltage imbalances between the cells occur repeatedly during discharging/charging. The voltage ranges for the discharge/charge of a retired series/parallel battery pack are reduced owing to the voltage imbalances between the cells. To determine the accurate SOH of a retired series/parallel battery pack, it is necessary to calculate the total discharge capacity using fully discharging/charging tests. However, a fully discharging/charging test is impossible owing to the reduced voltage range. The SOH of a retired series/parallel battery pack with a voltage imbalance should be estimated within the reduced discharging/charging voltage range. This paper presents a regression analysis of the peak point in the incremental capacity (IC) curve from the fresh state to a 100-cycle aging state. Moreover, the SOH of the considered retired series/parallel battery pack was estimated using a regression analysis model. The error in the SOHs of the retired series/parallel battery pack and linear regression analysis model was within 1\%, and hence a good accuracy is achieved.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {battery,battSOH},
  annotation = {11 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lee19IncrementalCapacityCurve.pdf}
}

@article{Meng19ReviewPrognosticsHealth,
  title = {A Review on Prognostics and Health Management ({{PHM}}) Methods of Lithium-Ion Batteries},
  author = {Meng, Huixing and Li, Yan-Fu},
  year = {2019},
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {116},
  pages = {109405},
  publisher = {Elsevier},
  abstract = {Batteries are prevalent energy providers for modern systems. They can also be regarded as storage units for renewable and sustainable energy. Failures of batteries can bring huge losses in terms of personnel, facility, environment, and reputation aspects. Therefore the accurate health estimation and high availability of batteries is urgently required by corresponding users, distributors, and manufactures. Fortunately, prognostics and health management (PHM) technique has been demonstrated the capability of supporting the improvement of the availability and reliability of batteries. In this paper, we gave a review on the state-of-the-art of the PHM study on batteries. We observed the increase of publication related to battery PHM in the past decade (2009-2018), especially in the past five years. Approaches related to battery performance prognostics are categorized into physics-based, data-driven and hybrid classes. Selection of the battery PHM approach requires to take the user requirement, data availability and degradation mechanisms attainability into consideration. Based on the survey, we also proposed research and development perspectives to conduct further studies on the battery PHM ,including the approach selection, health management, performance evaluation, uncertainty treatment, application economics, as well as environmental issues. We focused on PHM of lithium-ion batteries, given the fact that several publications discussed other types of batteries (e.g., lead-acid batteries).},
  keywords = {battery,battSOH,intro},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Meng19ReviewPrognosticsHealth.pdf}
}

@article{Feng20MitigatingThermalRunaway,
  title = {Mitigating {{Thermal Runaway}} of {{Lithium-Ion Batteries}}},
  author = {Feng, Xuning and Ren, Dongsheng and He, Xiangming and Ouyang, Minggao},
  year = {2020},
  month = apr,
  journal = {Joule},
  volume = {4},
  number = {4},
  pages = {743--770},
  issn = {25424351},
  doi = {10.1016/j.joule.2020.02.010},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S254243512030088X},
  urldate = {2023-07-24},
  abstract = {This paper summarizes the mitigation strategies for the thermal runaway of lithium-ion batteries. The mitigation strategies function at the material level, cell level, and system level. A time-sequence map with states and flows that describe the evolution of the physical and/or chemical processes has been pro- posed to interpret the mechanisms, both at the cell level and at the system level. At the cell level, the time-sequence map helps clarify the relationship between thermal runaway and fire. At the system level, the time-sequence map depicts the relationship between the expected thermal runaway propagation and the undesired fire pathway. Mitigation strategies are fulfilled by cutting off a spe- cific transformation flow between the states in the time sequence map. The abuse conditions that may trigger thermal runaway are also summarized for the complete protection of lithium-ion batteries. This perspective provides di- rections for guaranteeing the safety of lithium-ion batteries for electrical energy storage applications in the future.},
  langid = {english},
  keywords = {battery,battFire,priorityRead},
  annotation = {413 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {\textbf{Feng20 paper test notes gahbidge}
\par
``of Figure 3A. The ve''
\par
Changing this to see exported markdown changes. ~NO, it doesn't
\par
``I'm editing this. ~It's no longer a quote from the pdf. ~But I can still jump to the pdf page. ~I can also hide the citation, which is nice, in fact, I wish the citation was off by default. ~At least, you can hide them in bulk using the ellipsis ~menu. If I delete the quotation marks, or if I add a newline, this text turns into regular text and I can't jump to the line in the pdf. ~But I can undo and retrieve jump capability. ~If I click the elipsisis, I can show the annotation colors, which makes it easy to ID notes -- and this works even after adding a newline. ~But the jump still doesn't work. '' (Feng et al., 2020, p. 743)
\par
``safety problem, known as ``thermal runaway (TR),''4 must be overcome. Solutions to this problem ar'' Here's a comment on what I marked with a green marker and then~ changed in here to blue.~ annoying that it's smushed up to the marked text with no newline or space between it.

Are tags unique to this note only?~ When I try to add them, I don't see options for tags on the general Zotero list of papers.

Note that when I add to note, the comments are added too, but they're not updated in the note if I change my comment here.
\par
\begin{itemize}

\item Yes! you can move them around, but they turn into numbered lists, if you're not careful to keep them in the same set of bullets. ~Annoying b/c can't split them up and put text between them. ~Well, you can if you just hit a few blank lines, but rearranging is messier this way.
\item can I emded a highlight in the middle of a note, like here (``Figure 1.''). Yes, with some effort.

\end{itemize}

\par
asdfsadf
\par
\begin{itemize}

\item 
\par
Yes, there are bullets!
\par
\begin{itemize}

\item Yes, then indeint
\item adlskfj

\end{itemize}

\item 
\par
dslkfjalsjkfd
\par
\begin{itemize}

\item asfldkjalsf
\item alsdfkjlafjd

\end{itemize}

\end{itemize}

\par
aldsfjaldfsj ad;lfjsdflj ~I'm going to move bullets here:
\par
(Fioravanti et al., 2020)
\par
\href{https://www.zotero.org/users/60638/items/793AW775}{https://www.zotero.org/users/60638/items/793AW775}
\par
\section{Two References}

\par
(Baru and Johnson, 2023)
\par
(Pinson and Madsen, 2009)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Feng20MitigatingThermalRunaway.pdf}
}

@article{Ma18TemperatureEffectThermal,
  title = {Temperature Effect and Thermal Impact in Lithium-Ion Batteries: {{A}} Review},
  shorttitle = {Temperature Effect and Thermal Impact in Lithium-Ion Batteries},
  author = {Ma, Shuai and Jiang, Modi and Tao, Peng and Song, Chengyi and Wu, Jianbo and Wang, Jun and Deng, Tao and Shang, Wen},
  year = {2018},
  month = dec,
  journal = {Progress in Natural Science: Materials International},
  volume = {28},
  number = {6},
  pages = {653--666},
  issn = {1002-0071},
  doi = {10.1016/j.pnsc.2018.11.002},
  url = {https://www.sciencedirect.com/science/article/pii/S1002007118307536},
  urldate = {2023-07-24},
  abstract = {Lithium-ion batteries, with high energy density (up to 705\,Wh/L) and power density (up to 10,000\,W/L), exhibit high capacity and great working performance. As rechargeable batteries, lithium-ion batteries serve as power sources in various application systems. Temperature, as a critical factor, significantly impacts on the performance of lithium-ion batteries and also limits the application of lithium-ion batteries. Moreover, different temperature conditions result in different adverse effects. Accurate measurement of temperature inside lithium-ion batteries and understanding the temperature effects are important for the proper battery management. In this review, we discuss the effects of temperature to lithium-ion batteries at both low and high temperature ranges. The current approaches in monitoring the internal temperature of lithium-ion batteries via both contact and contactless processes are also discussed in the review.},
  langid = {english},
  keywords = {battery,battFire,battSOH,intro},
  annotation = {500 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ma18TemperatureEffectThermal.pdf}
}

@article{Diaz20ReviewMetaReviewFire,
  title = {Review---{{Meta-Review}} of {{Fire Safety}} of {{Lithium-Ion Batteries}}: {{Industry Challenges}} and {{Research Contributions}}},
  shorttitle = {Review---{{Meta-Review}} of {{Fire Safety}} of {{Lithium-Ion Batteries}}},
  author = {Diaz, Laura Bravo and He, Xuanze and Hu, Zhenwen and Restuccia, Francesco and Marinescu, Monica and Barreras, Jorge Varela and Patel, Yatish and Offer, Gregory and Rein, Guillermo},
  year = {2020},
  month = aug,
  journal = {Journal of The Electrochemical Society},
  volume = {167},
  number = {9},
  pages = {090559},
  publisher = {IOP Publishing},
  issn = {1945-7111},
  doi = {10.1149/1945-7111/aba8b9},
  url = {https://dx.doi.org/10.1149/1945-7111/aba8b9},
  urldate = {2023-07-24},
  abstract = {The Lithium-ion battery (LIB) is an important technology for the present and future of energy storage, transport, and consumer electronics. However, many LIB types display a tendency to ignite or release gases. Although statistically rare, LIB fires pose hazards which are significantly different to other fire hazards in terms of initiation route, rate of spread, duration, toxicity, and suppression. For the first time, this paper collects and analyses the safety challenges faced by LIB industries across sectors, and compares them to the research contributions found in all the review papers in the field. The comparison identifies knowledge gaps and opportunities going forward. Industry and research efforts agree on the importance of understanding thermal runaway at the component and cell scales, and on the importance of developing prevention technologies. But much less research attention has been given to safety at the module and pack scales, or to other fire protection layers, such as compartmentation, detection or suppression. In order to close the gaps found and accelerate the arrival of new LIB safety solutions, we recommend closer collaborations between the battery and fire safety communities, which, supported by the major industries, could drive improvements, integration and harmonization of LIB safety across sectors.},
  langid = {english},
  keywords = {battery,battFire,intro,priorityRead},
  annotation = {71 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Diaz20ReviewMetaReviewFire.pdf}
}

@article{Yang22FastAccurateHealth,
  title = {Fast and {{Accurate Health Assessment}} of {{Lithium-Ion Batteries Based}} on {{Typical Voltage Segments}}},
  author = {Yang, Ning and Yu, Tao and Luo, Qingquan and Wang, Keying},
  year = {2022},
  journal = {Frontiers in Energy Research},
  volume = {10},
  issn = {2296-598X},
  url = {https://www.frontiersin.org/articles/10.3389/fenrg.2022.925947},
  urldate = {2023-07-25},
  abstract = {Lithium-ion batteries are widely employed in industries and daily life. Research on the state of health (SOH) of batteries is essential for grasping the performance of batteries, better guiding battery health management, and avoiding safety mishaps caused by battery aging. Nowadays, most research adopts a data-driven artificial intelligence approach to assess SOH. However, the majority of approaches are based on entire voltage, current, or temperature curves. In reality, voltage, current, and temperature are frequently presented in segments, leading to the limited flexibility and slow analysis speed of the traditional techniques. This study solves the problem by dividing the whole voltage curve into many typical kinds of segments with equal timescales based on different typical voltage beginning points. On this foundation, the temporal convolution network (TCN) is used to create a sub-model of SOH estimation for several typical kinds of segments. In addition, the sub-models are fused using the bootstrap aggregating (Bagging) approach to boost accuracy. Finally, this research uses a publicly available dataset from Oxford to demonstrate the effectiveness of the suggested strategy.},
  keywords = {battery,battSOH},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Yang22FastAccurateHealth.pdf}
}

@article{Prosser21LithiumIonDiagnosticsFirst,
  title = {Lithium-{{Ion Diagnostics}}: {{The First Quantitative In-Operando Technique}} for {{Diagnosing Lithium Ion Battery Degradation Modes}} under {{Load}} with {{Realistic Thermal Boundary Conditions}}},
  shorttitle = {Lithium-{{Ion Diagnostics}}},
  author = {Prosser, Ryan and Offer, Gregory and Patel, Yatish},
  year = {2021},
  month = mar,
  journal = {Journal of The Electrochemical Society},
  volume = {168},
  number = {3},
  pages = {030532},
  publisher = {IOP Publishing},
  issn = {1945-7111},
  doi = {10.1149/1945-7111/abed28},
  url = {https://dx.doi.org/10.1149/1945-7111/abed28},
  urldate = {2023-07-25},
  abstract = {A diagnostic technique capable of quantitatively estimating degradation modes in-operando, including loss of lithium inventory and loss of active material, which operates under charge and discharge loads with realistic thermal boundary conditions is presented for the first time. The technique uses a zero-dimensional heat generation model with only three parameters requiring estimation, a simple heat transfer model, and requires just three temperature measurements per cell, voltage and current. The technique has been demonstrated to work for pouch cells with tab cooling and a constant coolant temperature and for charge and discharge rates of C/2, 1C and 2C. Compared to state-of-the-art open circuit voltage (OCV) model methods, the technique predicts electrode capacities and offset of a fresh cell with accuracies of 3\% and 6\% respectively. Further the technique has been shown to predict loss of lithium and loss of active material in the positive and negative electrodes with accuracies of 0.18\%, 0.22\% and 1.99\% respectively. The technique can therefore provide information of the same quality as the current state-of-the-art techniques but works under application relevant conditions and due to its simplicity is suitable for implementation on-line in a battery management system (BMS).},
  langid = {english},
  keywords = {battery,battSOH},
  annotation = {4 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Prosser21LithiumIonDiagnosticsFirst.pdf}
}

@article{Wang21FlexibleMethodStateofhealth,
  title = {A Flexible Method for State-of-Health Estimation of Lithium Battery Energy Storage System},
  author = {Wang, Zhenpo and Song, Chunbao and Yuan, Changgui and Li, Xiaoyu},
  year = {2021},
  month = nov,
  journal = {Energy Reports},
  volume = {7},
  pages = {6375--6383},
  issn = {2352-4847},
  doi = {10.1016/j.egyr.2021.09.054},
  url = {https://www.sciencedirect.com/science/article/pii/S235248472100860X},
  urldate = {2023-07-25},
  abstract = {Prognostics of battery health conditions are regarded as significant tools for ensuring safety and stability of battery energy storage systems. Meanwhile, flexible and practical techniques are promising and innovative for achieving accurate and cost-effective state estimation. Herein, a novel curvature-based method is proposed to track battery degradation conditions. The proposed technique can ingeniously capture the health features of an aged battery from incremental capacity curves. Specifically, an advanced filter is employed to smooth the incremental capacity curves. Secondly, a self-definition sine function is applied to fit the smoothed incremental capacity curves. The curvature-based technique provides an essential manner for extracting the health features including the peak value and position from the fitted incremental capacity curves. On this basis, a flexible cost-effective numerical aging model is constructed by mapping the relationship between the feature indicators and battery capacity for battery health prognostic. Finally, for verification of the generalization and accuracy, the developed model is to estimate battery health conditions with the same type of battery under different experimental conditions. Experimental results of two types of batteries manifest that the proposed algorithm can provide accurate and credible battery health conditions with maximum relative errors of less than 4\%.},
  langid = {english},
  keywords = {battery,battSOH},
  annotation = {5 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wang21FlexibleMethodStateofhealth.pdf}
}

@article{Yang18OnlineStateofhealthEstimation,
  title = {Online State-of-Health Estimation for Lithium-Ion Batteries Using Constant-Voltage Charging Current Analysis},
  author = {Yang, Jufeng and Xia, Bing and Huang, Wenxin and Fu, Yuhong and Mi, Chris},
  year = {2018},
  month = feb,
  journal = {Applied Energy},
  volume = {212},
  pages = {1589--1600},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2018.01.010},
  url = {https://www.sciencedirect.com/science/article/pii/S0306261918300102},
  urldate = {2023-07-25},
  abstract = {Battery state-of-health (SoH) estimation is a critical function in a well-designed battery management system (BMS). In this paper, the battery SoH is detected based on the dynamic characteristic of the charging current during the constant-voltage (CV) period. Firstly, according to the preliminary analysis of the battery test data, the time constant of CV charging current is proved to be a robust characteristic parameter related to the battery aging. Secondly, the detailed expression of the current time constant is derived based on the first order equivalent circuit model (ECM). Thirdly, the quantitative correlation between the normalized battery capacity and the current time constant is established to indicate the battery SoH. Specifically, for the uncompleted CV charging process, the logarithmic function-based current time constant prediction model and the reference correlation curve are established to identify the battery capacity fading. At last, experimental results showed that regardless of the adopted data size, the correlation identified from one battery can be used to indicate the SoH of other three batteries within 2.5\% error bound except a few outliers.},
  langid = {english},
  keywords = {battery,battSOH},
  annotation = {121 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Yang18OnlineStateofhealthEstimation.pdf}
}

@article{Naha20IncrementalVoltageDifference,
  title = {An {{Incremental Voltage Difference Based Technique}} for {{Online State}} of {{Health Estimation}} of {{Li-ion Batteries}}},
  author = {Naha, Arunava and Han, Seongho and Agarwal, Samarth and Guha, Arijit and Khandelwal, Ashish and Tagade, Piyush and Hariharan, Krishnan S. and Kolake, Subramanya Mayya and Yoon, Jongmoon and Oh, Bookeun},
  year = {2020},
  month = jun,
  journal = {Scientific Reports},
  volume = {10},
  number = {1},
  pages = {9526},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-66424-9},
  url = {https://www.nature.com/articles/s41598-020-66424-9},
  urldate = {2023-07-25},
  abstract = {Accurate state of health (SOH) estimation of rechargeable batteries is important for the safe and reliable operation of electric vehicles (EVs), smart phones, and other battery operated systems. We propose a novel method for accurate SOH estimation which does not necessarily need full charging data. Using only partial charging data during normal usage, 10 derived voltage values (\$\$\{v\}\_\{sei\}\$\$) are collected. The initial \$\$\{v\}\_\{sei\}\$\$ point is fixed and then for every 1.5\% increase in the Coulomb counting, other points are selected. The difference between the \$\$\{v\}\_\{sei\}\$\$ values (\$\${\textbackslash}Delta \{v\}\_\{sei\}\$\$) and the average temperature during the charging form the feature vector at different SOH levels. The training data set is prepared by extrapolating the charging voltage curves for the complete SOH range using initial 400 cycles of data. The trained artificial neural network (ANN) based on the feature vector and SOH values can be used in any battery management system (BMS) with a time complexity of only \$\$O(\{n\}{\textasciicircum}\{4\})\$\$. Less than 1\% mean absolute error (MAE) for the test cases has been achieved. The proposed method has a moderate training data requirement and does not need any knowledge of previous SOH, state of charge (SOC) vs. OCV relationship, and absolute SOC value.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {battery,battFire,battSOC,battSOH,priorityRead},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {Definitions of SAFE and the relationship between SOC and SOH (and co-estimates them)
\par
``unsafe if the SOH falls below 80\%'' (Naha et al., 2020, p. 1)
\par
``charge left is directly proportional to the product of the SOC and SOH'' (Naha et al., 2020, p. 1)
\par
Is this the same SOC/SOH definition as in (Park et al., 2020)},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Naha20IncrementalVoltageDifference.pdf;C\:\\Users\\scott\\tmp\\zot_mdnote_export\\Naha20IncrementalVoltageDifference - Definitions of SAFE and the relationship between SOC and SOH (and co-estimates them).md;C\:\\Users\\scott\\tmp\\zot_mdnote_export\\Naha20IncrementalVoltageDifference-zotero.md;C\:\\Users\\scott\\tmp\\zot_mdnote_export\\Naha20IncrementalVoltageDifference.md}
}

@article{Nuroldayeva23StateHealthEstimation,
  title = {State of {{Health Estimation Methods}} for {{Lithium-Ion Batteries}}},
  author = {Nuroldayeva, Gulzat and Serik, Yerkin and Adair, Desmond and Uzakbaiuly, Berik and Bakenov, Zhumabay},
  year = {2023},
  month = mar,
  journal = {International Journal of Energy Research},
  volume = {2023},
  pages = {e4297545},
  publisher = {Hindawi},
  issn = {0363-907X},
  doi = {10.1155/2023/4297545},
  url = {https://www.hindawi.com/journals/ijer/2023/4297545/},
  urldate = {2023-07-25},
  abstract = {Contemporary lithium-ion batteries (LIBs) are one of the main components of energy storage systems that need effective management to extend service life and increase reliability and safety. Their characteristics depend highly on internal and external conditions (ageing, temperature, and chemistry). Currently, the state of batteries is determined using two parameters: the state of charge (SOC) and the state of health (SOH). Applying these two parameters makes it possible to calculate the expected battery life and a battery's performance. There are many methods for estimating the SOH of batteries, including experimental, model-based, and machine learning methods. By comparing model-based estimations with experimental techniques, it can be concluded that the use of experimental methods is not applicable for commercial cases. The electrochemical model-based SOH estimation method clearly explains processes in the battery with the help of multidifferential equations. The machine learning method is based on creating a program trained to predict the battery's state of health with the help of past ageing data. In this review paper, we analyze the research available in the literature in this direction. It is found that all methods used to assess the SOH of an LIB play an essential role, and each method has its pros and cons.},
  langid = {english},
  keywords = {battery,battSOH,intro,priorityRead},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nuroldayeva23StateHealthEstimation.pdf}
}

@article{Cho22FireRiskAssessment,
  title = {A Fire Risk Assessment Method for High-Capacity Battery Packs Using Interquartile Range Filter},
  author = {Cho, Inho and Park, Seongyun and Kim, Jonghoon},
  year = {2022},
  month = jun,
  journal = {Journal of Energy Storage},
  volume = {50},
  pages = {104663},
  issn = {2352-152X},
  doi = {10.1016/j.est.2022.104663},
  url = {https://www.sciencedirect.com/science/article/pii/S2352152X22006764},
  urldate = {2023-07-25},
  abstract = {Lithium-ion batteries are chosen as the most suitable device for energy storage system (ESS) due to their high energy density. However, lithium-ion batteries have high chemical reactivity, which increase the fire risk of products using them. Accordingly, various studies have been conducted to prevent lithium-ion battery-fire accidents but the main purpose of the conventional studies was to prevent the spread of fire after it started rather than predicting the risk of fire. Thus, there is a limit to reducing the maintenance time and cost of ESS. In this paper, a new method for real-time monitoring of the fire risk during operation of the battery pack is proposed. It combines with the electrochemical theory while using the real-time data from the electrically measured factors to select the aged cells. Also, a method of assessing the fire risk of battery packs by applying the interquartile range (IQR) filter to real-time data obtained from the electrically measured factors of battery packs is proposed. The feasibility of the proposed method is verified by the experiment using the specifications of the high-capacity battery packs used for railway vehicles.},
  langid = {english},
  keywords = {battery,battFire}
}

@article{Park20ReviewStateoftheartBattery,
  title = {Review of State-of-the-Art Battery State Estimation Technologies for Battery Management Systems of Stationary Energy Storage Systems},
  author = {Park, Seongyun and Ahn, Jeongho and Kang, Taewoo and Park, Sungbeak and Kim, Youngmi and Cho, Inho and Kim, Jonghoon},
  year = {2020},
  journal = {Journal of Power Electronics},
  volume = {20},
  pages = {1526--1540},
  publisher = {Springer},
  keywords = {battery,battSOC,battSOH,intro,priorityRead},
  note = {Defines SOC. ~Different than (Gu et al., 2023) or (Ardiansyah et al., 2022)?
\par
But SOC sounds like this SOH definition:
\par
``SOH is given by the normalization of charge/discharge capacity, that is SOH = Ct/C0, where Ct is the current capacity and C0 is the rated capacity.'' (Chou et al., 2023, p. 3)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Park20ReviewStateoftheartBattery.pdf}
}

@article{Wei23PredictionStateHealth,
  title = {Prediction of {{State}} of {{Health}} of {{Lithium-Ion Battery Using Health Index Informed Attention Model}}},
  author = {Wei, Yupeng},
  year = {2023},
  month = feb,
  journal = {Sensors (Basel, Switzerland)},
  volume = {23},
  number = {5},
  pages = {2587},
  issn = {1424-8220},
  doi = {10.3390/s23052587},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10007287/},
  urldate = {2023-07-25},
  abstract = {State-of-health (SOH) is a measure of a battery's capacity in comparison to its rated capacity. Despite numerous data-driven algorithms being developed to estimate battery SOH, they are often ineffective in handling time series data, as they are unable to utilize the most significant portion of a time series while predicting SOH. Furthermore, current data-driven algorithms are often unable to learn a health index, which is a measurement of the battery's health condition, to capture capacity degradation and regeneration. To address these issues, we first present an optimization model to obtain a health index of a battery, which accurately captures the battery's degradation trajectory and improves SOH prediction accuracy. Additionally, we introduce an attention-based deep learning algorithm, where an attention matrix, referring to the significance level of a time series, is developed to enable the predictive model to use the most significant portion of a time series for SOH prediction. Our numerical results demonstrate that the presented algorithm provides an effective health index and can precisely predict the SOH of a battery.},
  pmcid = {PMC10007287},
  pmid = {36904789},
  keywords = {battery,battSOH},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wei23PredictionStateHealth.pdf}
}

@article{Yu23BatterySOHPrediction,
  title = {Battery {{SOH Prediction Based}} on {{Multi-Dimensional Health Indicators}}},
  author = {Yu, Zhilong and Liu, Na and Zhang, Yekai and Qi, Lihua and Li, Ran},
  year = {2023},
  month = feb,
  journal = {Batteries},
  volume = {9},
  number = {2},
  pages = {80},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2313-0105},
  doi = {10.3390/batteries9020080},
  url = {https://www.mdpi.com/2313-0105/9/2/80},
  urldate = {2023-07-25},
  abstract = {Battery capacity is an important metric for evaluating and predicting the health status of lithium-ion batteries. In order to determine the answer, the battery's capacity must be, with some difficulty, directly measured online with existing methods. This paper proposes a multi-dimensional health indicator (HI) battery state of health (SOH) prediction method involving the analysis of the battery equivalent circuit model and constant current discharge characteristic curve. The values of polarization resistance, polarization capacitance, and initial discharge resistance are identified as the health indicators reflective of the battery's state of health. Moreover, the retention strategy genetic algorithm (e-GA) selects the optimal voltage drop segment, and the corresponding equal voltage drop discharge time is also used as a health indicator. Based on the above health indicator selection strategy, a battery SOH prediction model based on particle swarm optimization (PSO) and LSTM neural network is constructed, and its accuracy is validated. The experimental results demonstrate that the suggested strategy is accurate and generalizable. Compared with the prediction model with single health indicator input, the accuracy is increased by 0.79\%.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {battery,battSOH,priorityRead},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Yu23BatterySOHPrediction.pdf}
}

@article{Zhang22EvaluationStateHealth,
  title = {Evaluation of the {{State}} of {{Health}} of {{Lithium-Ion Battery Based}} on the {{Temporal Convolution Network}}},
  author = {Zhang, Dan and Zhao, Weihua and Wang, Long and Chang, Xucheng and Li, Xiang and Wu, Peng},
  year = {2022},
  journal = {Frontiers in Energy Research},
  volume = {10},
  issn = {2296-598X},
  url = {https://www.frontiersin.org/articles/10.3389/fenrg.2022.929235},
  urldate = {2023-07-25},
  abstract = {The state of health (SOH) of lithium-ion batteries is an important part of the battery management system (BMS). Accurately grasping the SOH of the lithium-ion battery will help replace the battery in time, to avoid accidents. Aiming at the problems of complex BMS management and high calculation cost caused by too many inputs/attributes, this study used feature engineering to mine the higher temperature variety rate associated with degraded capacity as the input of temporal convolutional networks (TCNs) and SOH as the output to establish the TCN model. On this basis, three lithium-ion batteries, namely, as B0005, B0007, and B0018 are verified, and the mean absolute error (MAE) and root mean square error (RMSE) of predicted SOH are not more than 1.455\% and 1.800\%, respectively. To further obtain the uncertain expression of predicted SOH, this study adopts the sampling method to obtain the confidence interval of lithium-ion battery SOH prediction results.},
  keywords = {battery,battSOH},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhang22EvaluationStateHealth.pdf}
}

@misc{Electropaedia23LithiumBatteryFailures,
  title = {Lithium {{Battery Failures}}},
  author = {{Electropaedia}},
  year = {2023},
  journal = {Electropaedia},
  url = {https://mpoweruk.com/lithium_failures.htm},
  urldate = {2023-07-25},
  abstract = {Lithium Battery Failures The performance of Lithium Ion cells is dependent on both the temperature and the operating voltage. The diagram below shows that, at all times, the cell operating voltage and temperature must be kept within the limits indicated by the green b},
  keywords = {battery,battFire,battSOH,intro,priorityRead},
  note = {This is a threshold technique like in Baru23},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Electropaedia23LithiumBatteryFailures.pdf}
}

@techreport{Keslar22analysisSOCfaaTN,
  title = {An {{Analysis}} of {{State}} of {{Charge}} in {{Lithium-ion Batteries}}},
  author = {Keslar, Daniel},
  year = {2022},
  institution = {United States. Department of Transportation. Federal Aviation Administration {\dots}},
  keywords = {battery,battFire,battSOC},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Keslar22analysisSOCfaaTN.pdf}
}

@misc{Saha07battDatLiNASA,
  title = {Battery {{Data Set}}, {{NASA Ames Prognostics Data Repository}}},
  shorttitle = {Battery {{Data Set}}, {{NASA Ames Prognostics Data Repository}}; {{NASA Ames Research Center}}},
  author = {Saha, B. and Goebel, K.},
  year = {2007},
  publisher = {NASA Ames Research Center: Moffett Field, CA, USA, 2007},
  url = {https://phm-datasets.s3.amazonaws.com/NASA/5.+Battery+Data+Set.zip},
  abstract = {Experiments on Li-Ion batteries. Charging and discharging at different temperatures. Records the impedance as the damage criterion. The data set was provided by the NASA Prognostics Center of Excellence (PCoE).},
  keywords = {battery,battSOH,dataSrc}
}

@article{Tian22battSOHpred,
  title = {State-of-{{Health Prediction}} of {{Lithium-Ion Batteries Based}} on {{CNN-BiLSTM-AM}}},
  author = {Tian, Yukai and Wen, Jie and Yang, Yanru and Shi, Yuanhao and Zeng, Jianchao},
  year = {2022},
  month = oct,
  journal = {Batteries},
  volume = {8},
  number = {10},
  pages = {155},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2313-0105},
  doi = {10.3390/batteries8100155},
  url = {https://www.mdpi.com/2313-0105/8/10/155},
  urldate = {2023-07-25},
  abstract = {State-of-Health (SOH) prediction of lithium-ion batteries is crucial in battery management systems. In order to guarantee the safe operation of lithium-ion batteries, a hybrid model based on convolutional neural network (CNN)-bidirectional long short-term memory (BiLSTM) and attention mechanism (AM) is developed to predict the SOH of lithium-ion batteries. By analyzing the charging and discharging process of batteries, the indirect health indicator (HI), which is highly correlated with capacity, is extracted in this paper. HI is taken as the input of CNN, and the convolution and pooling operations of CNN layers are used to extract the features of battery time series data. On this basis, a BiLSTM depth model is built in this paper to collect the data coming from CNN forward and reverse dependencies and further emphasize the correlation between the serial data by AM to obtain an accurate SOH estimate. Experimental results based on NASA PCoE lithium-ion battery data demonstrate that the proposed hybrid model outperforms other single models, with the root mean square error (RMSE) of SOH prediction results all less than 0.01, and can accurately predict the SOH of lithium-ion batteries.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {battery,battSOH},
  annotation = {2 citations (Semantic Scholar/DOI) [2023-07-25]},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Tian22StateofHealthPredictionLithiumIon.pdf}
}

@article{Lu22battDegrPredUncertFut,
  title = {Battery Degradation Prediction against Uncertain Future Conditions with Recurrent Neural Network Enabled Deep Learning},
  author = {Lu, Jiahuan and Xiong, Rui and Tian, Jinpeng and Wang, Chenxu and Hsu, Chia-Wei and Tsou, Nien-Ti and Sun, Fengchun and Li, Ju},
  year = {2022},
  month = sep,
  journal = {Energy Storage Materials},
  volume = {50},
  pages = {139--151},
  issn = {2405-8297},
  doi = {10.1016/j.ensm.2022.05.007},
  url = {https://www.sciencedirect.com/science/article/pii/S2405829722002446},
  urldate = {2023-07-25},
  abstract = {Accurate degradation trajectory and future life are the key information of a new generation of intelligent battery and electrochemical energy storage systems. It is very challenging to obtain accurate predictions against uncertain application conditions by using only a few known historical data. In this article, we extend the widely studied remaining useful life (RUL) prediction to the prediction of charge and discharge capacity trajectories under both fixed and random future operating conditions. This is achieved by developing a general deep learning framework cored by recurrent neural network (RNN) which integrates future current plan and few early capacity-voltage data as inputs. As a case study, we have experimented with 77 commercial batteries cycled under fixed and random operating conditions. We demonstrate that the median root mean square error (RMSE) of prediction can be within 2.4\% for NMC/graphite batteries and 2.3\% for LFP/graphite batteries by using 3.8\% of the whole life data only. Compared with the existing methods, the proposed framework predicts more accurately and has a very balanced performance for both fixed and random future conditions. This work highlights the promise of actively forecasting the future of batteries based on RNN.},
  langid = {english},
  keywords = {battery,battSOH},
  annotation = {30 citations (Semantic Scholar/DOI) [2023-07-25]}
}

@techreport{Hamann17MultiscaleMultimodelMachinelearning,
  title = {A Multi-Scale, Multi-Model, Machine-Learning Solar Forecasting Technology},
  author = {Hamann, Hendrik F.},
  year = {2017},
  institution = {IBM, Yorktown Heights, NY (United States). Thomas J. Watson Research Center},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hamann17MultiscaleMultimodelMachinelearning.pdf}
}

@article{Chou23NovelFineTuningModel,
  title = {A {{Novel Fine-Tuning Model Based}} on {{Transfer Learning}} for {{Future Capacity Prediction}} of {{Lithium-Ion Batteries}}},
  author = {Chou, Jia-Hong and Wang, Fu-Kwun and Lo, Shih-Che},
  year = {2023},
  month = jun,
  journal = {Batteries},
  volume = {9},
  number = {6},
  pages = {325},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2313-0105},
  doi = {10.3390/batteries9060325},
  url = {https://www.mdpi.com/2313-0105/9/6/325},
  urldate = {2023-07-25},
  abstract = {Future capacity prediction of lithium-ion batteries is a highly researched topic in the field of battery management systems, owing to the gradual degradation of battery capacity over time due to various factors such as chemical changes within the battery, usage patterns, and operating conditions. The accurate prediction of battery capacity can aid in optimizing its usage, extending its lifespan, and mitigating the risk of unforeseen failures. In this paper, we proposed a novel fine-tuning model based on a deep learning model with a transfer learning approach comprising of two key components: offline training and online prediction. Model weights and prediction parameters were transferred from offline training using source data to the online prediction stage. The transferred Bi-directional Long Short-Term Memory with an Attention Mechanism model weights and prediction parameters were utilized to fine-tune the model by partial target data in the online prediction phase. Three battery batches with different charging policy were used to evaluate the proposed approach's robustness, reliability, usability, and accuracy for the three charging policy batteries' real-world data. The experiment results show that the proposed method's efficacy improved, with an increase in the cycle number of the starting point, exhibiting a linear relationship with the starting point. The proposed method yields relative error values of 8.70\%, 6.38\%, 9.52\%, 7.58\%, 1.94\%, and 2.29\%, respectively, for the six target batteries in online prediction. Thus, the proposed method is effective in predicting the future capacity of lithium-ion batteries and holds potential for use in predictive maintenance applications.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {battery,battSOH,obsLitNote},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-25]},
  note = {SOH definition:
\par
``SOH is given by the normalization of charge/discharge capacity, that is SOH = Ct/C0, where Ct is the current capacity and C0 is the rated capacity.'' (Chou et al., 2023, p. 3)
\par
This is similar to (Ardiansyah et al., 2022)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chou23NovelFineTuningModel.pdf}
}

@article{Xiong23EarlyPredictionLithiumion,
  title = {Early Prediction of Lithium-Ion Battery Cycle Life Based on Voltage-Capacity Discharge Curves},
  author = {Xiong, Wei and Xu, Gang and Li, Yumei and Zhang, Feng and Ye, Peng and Li, Ben},
  year = {2023},
  month = jun,
  journal = {Journal of Energy Storage},
  volume = {62},
  pages = {106790},
  issn = {2352-152X},
  doi = {10.1016/j.est.2023.106790},
  url = {https://www.sciencedirect.com/science/article/pii/S2352152X23001871},
  urldate = {2023-07-26},
  abstract = {Accurately predicting the lifetime of lithium-ion batteries is critical for accelerating technological advancements and applications. Nevertheless, the complex aging mechanisms and dynamic operating conditions of lithium-ion batteries have remained major challenges. This paper proposes a method for early predicting lithium-ion batteries cycle life based on weighted least squares support vector machine (WLS-SVM) with health indicators (HIs) as input. The HIs are extracted from lithium-ion batteries voltage-capacity discharge curves, since these curves are easy to measure and strongly correlate to battery cycle life. Taking into account the nonlinearity of batteries cycle life, a support vector machine (SVM) that is capable of strong generalization is used to predict cycle life. As a solution to the problem of misleading results from outlier data in SVM, error square term combined with weight functions is used in this study to improve robustness and prediction accuracy. The datasets of 41 cells are used to verify the proposed early prediction method. The results show that the root mean square error (RMSE) and mean absolute error (MAE) of cycle life prediction results by the proposed method are much lower, and the cycle life early prediction errors of the test cells are all {$<$}9~\%, which notes that the proposed method is accurate and valid for cycle life early prediction.},
  langid = {english},
  keywords = {battery,battSOH},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Xiong23EarlyPredictionLithiumiona.pdf}
}

@inproceedings{Xie22RemainingUsefulLife,
  title = {Remaining {{Useful Life Prediction}} of {{Lithium-ion Batteries Based}} on {{Particle Filter}} with {{Generalized Exponential Distribution}}},
  booktitle = {2022 2nd {{International Conference}} on {{Intelligent Technology}} and {{Embedded Systems}} ({{ICITES}})},
  author = {Xie, Xuan and Li, Xi-Feng and Bi, Dong-Jie and Xie, Yong-Le and Tang, Shen-Jie and Peng, Li-Biao},
  year = {2022},
  month = sep,
  pages = {97--101},
  doi = {10.1109/ICITES56274.2022.9943657},
  url = {https://ieeexplore.ieee.org/document/9943657},
  abstract = {A remaining useful life (RUL) prediction method is proposed, which uses a particle filter (PF) with the generalized exponential distribution (GE). There are three parameters in the adopted GE and the maximum likelihood estimation is used to estimate the parameters of GE. Due to the parameters in GE, more prior information in historical data can be absorbed and taken into PF for RUL prediction. The battery data from NASA Prognostics Center of Excellence Data Set Repository (PCoE) are used to compare the performance of the proposed method and that of using PF with the exponential function. The experiment results show that the performance of the proposed method is better.},
  keywords = {battery,battSOH}
}

@article{Penrose13LogNormStockPrice,
  title = {The Log-Normal Model for the Stock Price},
  author = {Penrose, Mathew},
  year = {2013},
  journal = {University of Bath},
  volume = {7.5},
  url = {https://people.bath.ac.uk/masmdp/findir.bho/lec7b.pdf},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Penrose13LogNormStockPrice.pdf}
}

@article{Tegner18volatilityLogNormNot,
  title = {Volatility {{Is Log-Normal}}---{{But Not}} for the {{Reason You Think}}},
  author = {Tegn{\'e}r, Martin and Poulsen, Rolf},
  year = {2018},
  month = jun,
  journal = {Risks},
  volume = {6},
  number = {2},
  pages = {46},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-9091},
  doi = {10.3390/risks6020046},
  url = {https://www.mdpi.com/2227-9091/6/2/46},
  urldate = {2023-07-26},
  abstract = {It is impossible to discriminate between the commonly used stochastic volatility models of Heston, log-normal, and 3-over-2 on the basis of exponentially weighted averages of daily returns---even though it appears so at first sight. However, with a 5-min sampling frequency, the models can be differentiated and empirical evidence overwhelmingly favours a fast mean-reverting log-normal model.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Tegner18volatilityLogNormNot.pdf}
}

@incollection{Savian20OpticalFlowEstimation,
  title = {Optical {{Flow Estimation}} with {{Deep Learning}}, a {{Survey}} on {{Recent Advances}}},
  author = {Savian, Stefano and Elahi, Mehdi and Tillo, Tammam},
  year = {2020},
  month = jan,
  pages = {257--287},
  doi = {10.1007/978-3-030-32583-1_12},
  abstract = {One of the many components used in biometrics is optical flow estimation. This could be due to the fact that motion is an inseparable attribute of our (visual) world and hence it is a valuable resource of data needed to tackle many real-world problems. Indeed, technologies that use object detection, motion detection, object tracking, gait recognition as well as video compression heavily rely on optical flow estimation. This chapter explores recent advances in optical flow estimation, while mainly focusing on estimation techniques based on deep learning (DL). In fact, recent advancements in deep learning are seemingly making a shift in the optical flow estimation research field. This chapter begins with reviewing traditional (handcrafted) approaches, then introduces the more recent approaches, and finally gets concluded with surveying deep learning approaches.},
  isbn = {978-3-030-32582-4},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Savian20OpticalFlowEstimation.pdf}
}

@techreport{Zhou16surveyUSAncillaryMkt,
  title = {Survey of {{US}} Ancillary Services Markets},
  author = {Zhou, Zhi and Levin, Todd and Conzelmann, Guenter},
  year = {2016},
  institution = {Argonne National Lab.(ANL), Argonne, IL (United States)},
  abstract = {In addition to providing energy to end-consumers, power system operators are also responsible for ensuring system reliability. To this end, power markets maintain an array of ancillary services to ensure it is always possible to balance the supply and demand for energy in real-time. A subset of these ancillary services are commonly procured through market-based mechanisms: namely, Regulation, Spinning, and Non-spinning Reserves.  Regulation Reserves are maintained to respond to supply/demand imbalances over much shorter time frames, typically on the order of one to several seconds. Resources that provide Regulation Reserves adjust their generation or load levels in response to automatic generation control (AGC) signals provided by the system operator.  Operating reserves are maintained to provide additional generation capacity in the event that load increases or other supply side resources reduce their output or are taken offline. The reserves are typically segmented into two categories, 1) Spinning or Synchronized Reserves that are provided by generation units that are actively generating and have the ability to increase or decrease their output, 2) Non-spinning or Non-synchronized Reserves that are provided by generation resources that are not actively generating, but are able to start up and provide generation within a specified timeframe.  Operating reserves typically have response times on the order of ten to 30 minutes and can similarly be provided by supply-side resources that are capable of reducing their load.  There are seven distinct power markets in the United States, each operated by a Regional Transmission Operator (RTO) or Independent System Operator (ISO) that manages transmission infrastructure in its territory, operates markets for energy and ancillary services, and maintains system reliability. Each power market offers its own set of ancillary services, and precise definitions, requirements, and market mechanisms differ between markets.  Despite the differences between markets, both in terms of services offered and system requirements, some broad trends generally apply. Regulation Reserves typically have the highest market prices, followed by Spinning Reserves and Non-spinning Reserves. Prices for Regulation Reserves have been the highest in the PJM market, since it opened in October 2012. This is partially because PJM experienced large price spikes during the period of extreme weather conditions in early 2014. ERCOT has traditionally had the highest prices for Spinning Reserves (called Responsive Reserves in ERCOT), including several periods of sustained high prices between 2010 and 2012. This can be explained in part by the relatively high penetration of variable wind resources and a similarly high requirement relative to peak load.  ERCOT has also traditionally had the highest price for Non-spinning Reserves, followed by the NYISO East region. Both experienced several periods of prolonged high prices since their inception, an occurrence that has not been regularly seen in other markets.  Market size (in terms of capacity) typically follows the reverse order of prices, as systems maintain the most Non-spinning Reserves capacity followed by Spinning Reserves and Regulation Reserves. PJM has the largest market for Regulation Reserves both in terms of revenue and capacity. Total revenue in PJM has grown since the market opened in 2012 due to increased prices. The size of most Regulation Survey of U.S. Ancillary Services Markets Reserves markets in terms of capacity stay relatively constant year-to-year, as this is dictated largely by system requirements. PJM also has the largest Spinning Reserves market in terms of capacity, but due to low prices in 2012 and 2013, total market revenue was relatively small prior to 2014. SPP, MISO, ISO-NE and SPP (beginning in 2014) all have Spinning Reserve markets with similar average capacity levels.  When combined, the markets for Non-spinning and Operating reserves in ISO-NE have a comparable capacity to the market for Primary Reserves in PJM. SPP, MISO, and CAISO all have smaller markets for their respective Non-spinning Reserves products that are roughly the same size as each other in terms of capacity.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhou16surveyUSAncillaryMkt.pdf}
}

@article{Ziel16priceFrcstElecXmodel,
  title = {Electricity {{Price Forecasting}} Using {{Sale}} and {{Purchase Curves}}: {{The X-Model}}},
  shorttitle = {Electricity {{Price Forecasting}} Using {{Sale}} and {{Purchase Curves}}},
  author = {Ziel, Florian and Steinert, Rick},
  year = {2016},
  month = sep,
  journal = {Energy Economics},
  volume = {59},
  pages = {435--454},
  doi = {10.1016/j.eneco.2016.08.008},
  abstract = {Our paper aims to model and forecast the electricity price by taking a completely new perspective on the data. It will be the first approach which is able to combine the insights of market structure models with extensive and modern econometric analysis. Instead of directly modeling the electricity price as it is usually done in time series or data mining approaches, we model and utilize its true source: the sale and purchase curves of the electricity exchange. We will refer to this new model as X-Model, as almost every deregulated electricity price is simply the result of the intersection of the electricity supply and demand curve at a certain auction. Therefore we show an approach to deal with a tremendous amount of auction data, using a subtle data processing technique as well as dimension reduction and lasso based estimation methods. We incorporate not only several known features, such as seasonal behavior or the impact of other processes like renewable energy, but also completely new elaborated stylized  facts of the bidding structure. Our model is able to capture the non-linear behavior of the electricity price, which is especially useful for predicting huge price spikes. Using simulation methods we show how to derive prediction intervals for probabilistic forecasting. We describe and show the proposed methods for the day-ahead EPEX spot price of Germany and Austria.},
  keywords = {Electricity markets,obsLitNote,Price forecasting},
  note = {Ziel16priceFrcstElecXmodel
\par
\begin{itemize}

\item Seems like a predecessor to (Kulakov and Ziel, 2019)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ziel16priceFrcstElecXmodel.pdf}
}

@article{Mahler22elecPriceDynStructModel,
  title = {Data-Driven Structural Modeling of Electricity Price Dynamics},
  author = {Mahler, Valentin and Girard, Robin and Kariniotakis, Georges},
  year = {2022},
  month = mar,
  journal = {Energy Economics},
  volume = {107},
  pages = {105811},
  issn = {0140-9883},
  doi = {10.1016/j.eneco.2022.105811},
  url = {https://www.sciencedirect.com/science/article/pii/S0140988322000032},
  urldate = {2023-07-28},
  abstract = {In many countries, electricity prices on day-ahead auction markets result from a market clearing designed to maximize social welfare. For each hour of the day, the market price can be represented as the intersection of a supply and demand curve. Structural market models reflect this price formation mechanism and are widely used in prospective studies guiding long-term decisions (e.g. investments and market design). However, simulating the supply curve in these models proves challenging since estimating the sell orders it comprises (i.e. offer prices and corresponding quantities) typically requires formulating numerous techno-economic hypotheses about power system assets and the behaviors of market participants. Due to imperfect competition, real market prices differ from the theoretical optimum, but modeling this difference is not straightforward. The objective of this work is to propose a model to simulate prices on day-ahead markets that account for the optimal economic dispatch of generation units, while also making use of historical day-ahead market prices. Inferring from historical data is especially important when not all information is made public (e.g. bidding strategies) or due to difficulty in accurately accounting for qualitative notions in quantitative models (e.g. market power). In this paper we propose a method for the parametrization of sell orders associated with production units. The estimation algorithm for this parametrization makes it possible to mitigate the requirement for analytic formulation of all of the above-mentioned aspects and to take advantage of the ever-increasing volume of available data on power systems (e.g. technical and market data). Parametrized orders also offer the possibility to account for various factors in a modular fashion, such as the strategic behavior of market participants. The proposed approach is validated using data related to the French day-ahead market and power system, for the period from 2015 to 2018.},
  langid = {english},
  keywords = {Electricity markets,obsLitNote,Price forecasting},
  note = {Mahler22elecPriceDynStructModel
\par
A day ahead electricity market supply curve that's constructed based on real-life ``techno-economic constraints'' using simulated, parameterized market orders.
\par
\begin{itemize}

\item says X-model e.g. (Kulakov, 2020), doesn't include availiability and production cost (availability would be a big deal in AEMO's transmission-constrained system)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Mahler22elecPriceDynStructModel.pdf}
}

@inproceedings{Shah16frcstElecMktVariables,
  title = {Modeling and {{Forecasting Electricity Market Variables}}},
  author = {Shah, Ismail},
  year = {2016},
  month = jan,
  url = {https://www.semanticscholar.org/paper/Modeling-and-Forecasting-Electricity-Market-Shah/3df284fd3749422e3f5a6610a4716aa933e0c60d},
  urldate = {2023-07-28},
  abstract = {In deregulated electricity markets, accurate modeling and forecasting of different variables, e.g. demand, prices, production etc. have obtained increasing importance in recent years. As in most electricity markets, the daily demand and prices are determined the day before the physical delivery by means of (semi-) hourly concurrent auctions, accurate forecasts are necessary for the efficient management of power systems. However, it is well known that electricity (demand/price) data exhibit some specific features, among which, daily, weekly and annual periodic patterns as well as non-constant mean and variance, jumps and dependency on calendar effects. Modeling and forecasting, thus, is a challenging task. This thesis tackles these two issues, and to do this, two approaches are followed.    In the first case, we address the issue of modeling and out-of-sample forecasting electricity demand and price time series. For this purpose, an additive component model was considered that includes some deterministic and a stochastic residual components. The deterministic components include a long-term dynamics, annual and weekly periodicities and calendar effects. The first three components were estimated using splines while the calendar effects were modeled using dummy variables. The residual component is instead treated as stochastic and different univariate and multivariate models have been considered with increasing level of complexity. In both cases, linear parametric and nonlinear nonparametric models, as well as functional based models, have been estimated and compared in a one day-ahead out-of-sample forecast framework.    The class of univariate models includes parametric autoregressive models (AR), nonparametric and nonlinear regression models based on splines (NPAR) and scalar-response functional models, that in turns can be formulated parametrically (FAR) or non parametrically (NPFAR). The multivariate models are vector autoregressive models (VAR) and functionalresponse, parametric (FFAR) and nonparametric (NPFFAR), models. For this issue, five different electricity markets, namely, British electricity market (APX Power UK), Nord Pool electricity market (NP), Italian electricity market (IPEX), Pennsylvania-New Jersey-Maryland electricity market (PJM) and Portuguese electricity market (OMIE(Po)) were considered for the period 2009 to 2014. The first five years were used for model estimation while the year 2014 was left for one-day-ahead forecasts. Predictive performances are first evaluated by means of descriptive indicators and then through a test to assess the significance of the differences. The analyses suggest that the multivariate approach leads to better results than the univariate one and that, within the multivariate framework, functional models are the most accurate, with VAR being a competitive model in some cases. The results also lead to another important finding concerning to the performance of parametric and nonparametric approach that showed strong linkage with underlying process. Finally the obtained results were compared with other works in the literature that suggest our forecasting errors are smaller compared with the state-of-art prediction techniques used in the literature.    In the second part of this thesis the issue of electricity price forecasting is revisited following a completely different approach. The main idea of this approach is that of modeling the daily supply and demand curves, predicting them and finding the intersection of the predicted curves in order to find the predicted market clearing price and volume. In this approach, the raw bids/offers data for demand and supply, corresponding to each (half-) hour is first aggregated in a specific order. The functional approach converts the resulted piece wise curves into smooth functions. For this issue, parametric functional model (FFAR) and the nonlinear nonparametric counterpart (NPFFAR) were considered. As benchmark, an ARIMA model was fitted to the scalar time series corresponding to the market clearing prices obtained from the crossing points of supply and demand curves. Data from Italian electricity market were used for this issue and the results are summarized by different descriptive indicators. As in the first case, results show superior forecasting performance of our functional approach compare to ARIMA. Among different models, the nonparametric functional model produces better results compared to parametric models. Apart from the improvement in forecasting accuracy, it is important to stress that this approach can be used for optimizing bidding strategies. As forecasting the whole curves gives deep insight into the market, our analysis showed that this strategy can significantly improve bidding strategies and maximize traders profit.},
  keywords = {Electricity markets,obsLitNote,Price forecasting},
  note = {Shah16frcstElecMktVariables
\par
Time series price forecasting on several markets. ~I coulln't find the conference paper but did find the PhD
\par
Maybe of special interest: 
\par
Chapter 6: Modeling and Forecasting Supply and Demand Curves},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Shah16frcstElecMktVariables.pdf}
}

@article{Kulakov19supplyDmdCurveElecMkt,
  title = {Determining Fundamental Supply and Demand Curves in a Wholesale Electricity Market},
  author = {Kulakov, Sergei and Ziel, Florian},
  year = {2019},
  journal = {arXiv preprint arXiv:1903.11383},
  eprint = {1903.11383},
  archiveprefix = {arXiv},
  keywords = {Electricity markets,obsLitNote,Price forecasting},
  note = {Kulakov19supplyDmdCurveElecMkt
\par
Decomposes demand and supply curves to eliminate arbitrage orders, categorize groups of participants (retailers, suppliers, utilities). ~Result is fundamentals model of DE wholesale electricity market
\par
\begin{itemize}

\item Ancestor seems to be (Ziel and Steinert, 2016)
\item modified in (Kulakov, 2020)
\item ``conclude that many demand elasticities were transferred to the supply side at the spike points.'' (Kulakov and Ziel, 2019, p. 23)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kulakov19supplyDmdCurveElecMkt.pdf}
}

@article{Kulakov20xmodelElecMkts,
  title = {X-Model: Further Development and Possible Modifications},
  shorttitle = {X-Model},
  author = {Kulakov, Sergei},
  year = {2020},
  journal = {Forecasting},
  volume = {2},
  number = {1},
  pages = {20--35},
  publisher = {MDPI},
  abstract = {The main goal of the present paper is to improve the X-model used for day-ahead electricity price and volume forecasting. The key feature of the X-model is that it makes a day-ahead forecast for the entire wholesale supply and demand curves. The intersection of the predicted curves yields the forecast for equilibrium day-ahead prices and volumes. We take advantage of a technique for auction curves' transformation to improve the original X-model. Instead of using actual wholesale supply and demand curves, we rely on transformed versions of these curves with perfectly inelastic demand. As a result, the computational requirements of our X-model are reduced and its forecasting power increases. Moreover, our X-model is more robust towards outliers present in the initial auction curves' data. Keywords: energy economics; energy forecasting; econometric modeling; electricity supply; electricity demand},
  keywords = {Electricity markets,obsLitNote,Price forecasting},
  note = {Kulakov20xmodelElecMkts
\par
Modified X-model separately forecasts supply and demand curves, without time series techniques that forecast the price directly
\par
\begin{itemize}

\item 
\par
(Mahler et al., 2022)
\par
says X-model doesn't include availiability and production cost (availability would be a big deal in AEMO's transmission-constrained system)
\par
\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kulakov20xmodelElecMkts.pdf}
}

@article{Yildirim23supplyCurveDynMdl,
  title = {Supply Curves in Electricity Markets: {{A}} Framework for Dynamic Modeling and Monte Carlo Forecasting},
  author = {Y{\i}ld{\i}r{\i}m, Sinan and Khalafi, Mohammad and G{\"u}zel, Tayyar and Sat{\i}k, Halil and Y{\i}lmaz, Murat},
  year = {2023},
  month = jul,
  journal = {IEEE Transactions on Power Systems},
  volume = {38},
  number = {4},
  pages = {3056--3069},
  issn = {1558-0679},
  doi = {10.1109/TPWRS.2022.3208765},
  abstract = {We introduce a new time series model for supply curves in a short-term electricity market. The model accounts for the contribution of different resources to the aggregate supply curve, as well as the impact of external factors on the price and amount of supplies provided by each of those resources. We equip the proposed model with a unified Monte Carlo methodology for tracking the latent variables, forecasting, and hyperparameter estimation. Specifically, we present a sequential Markov chain Monte Carlo (S-MCMC) algorithm for tracking the latent variables of the model, which in turn enables forecasting day-ahead supply curves. We present two stochastic variants of the expectation-maximization (EM) algorithm for estimating the hyperparameters of the proposed model. Both variants of EM employ S-MCMC in their expectation steps. We apply the proposed framework to the Turkish electricity market and show its performance on a real dataset from that market.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Yildirim23supplyCurveDynMdl.pdf}
}

@article{Ma22RealtimePersonalizedHealth,
  title = {Real-Time Personalized Health Status Prediction of Lithium-Ion Batteries Using Deep Transfer Learning},
  author = {Ma, Guijun and Xu, Songpei and Jiang, Benben and Cheng, Cheng and Yang, Xin and Shen, Yue and Yang, Tao and Huang, Yunhui and Ding, Han and Yuan, Ye},
  year = {2022},
  month = oct,
  journal = {Energy \& Environmental Science},
  volume = {15},
  number = {10},
  pages = {4083--4094},
  publisher = {The Royal Society of Chemistry},
  issn = {1754-5706},
  doi = {10.1039/D2EE01676A},
  url = {https://pubs.rsc.org/en/content/articlelanding/2022/ee/d2ee01676a},
  urldate = {2023-07-29},
  abstract = {Real-time and personalized lithium-ion battery health management is conducive to safety improvement for end-users. However, personalized prognostic of the battery health status is still challenging due to diverse usage interests, dynamic operational patterns and limited historical data. We generate a comprehensive dataset consisting of 77 commercial cells (77 discharge protocols) with over 140 000 charge--discharge cycles---the largest dataset to our knowledge of its kind, and develop a transfer learning framework to realize real-time personalized health status prediction for unseen battery discharge protocols, at any charge--discharge cycle. Our method can achieve mean testing errors of 0.176\% and 8.72\% for capacity estimation and remaining useful life (RUL) prediction, respectively. Additionally, the proposed framework can leverage the knowledge from two other well-known battery datasets, with a variety of charge configurations and a different battery chemistry respectively, to reliably estimate the capacity (0.328\%/0.193\%) and predict the RUL (9.80\%/9.90\%) of our cells. This study allows end users to tailor battery consumption plans and motivates manufacturers to improve battery designs.},
  langid = {english},
  keywords = {battery,battSOH},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Ma22RealtimePersonalizedHealth.pdf;C\:\\Users\\scott\\Zotero\\storage\\R9NKCUHH\\Ma et al. - 2022 - Real-time personalized health status prediction of.pdf}
}

@article{Berghout23battSOHcollabNN,
  title = {Lithium-Ion {{Battery State}} of {{Health Prediction}} with a {{Robust Collaborative Augmented Hidden Layer Feedforward Neural Network Approach}}},
  author = {Berghout, Tarek and Benbouzid, Mohamed and Amirat, Yassine and Yao, Gang},
  year = {2023},
  journal = {IEEE Transactions on Transportation Electrification},
  pages = {1--1},
  issn = {2332-7782},
  doi = {10.1109/TTE.2023.3237726},
  abstract = {Lithium-ion (Li-ion) batteries play an important role in providing necessary energy when acting as a main or backup source of electricity. Indeed, the unavailability of battery aging discharge data in most real-world applications makes the State of Health (SoH) assessment very challenging. Alternatively, accelerated aging is therefore adopted to emulate the degradation process and to achieve an SoH estimate. However, accelerated aging generates limited deterioration patterns suffering from a higher level of complexity due to the non-linearity and non-stationarity imposed by harsh conditions. In this context, this paper aims to provide a predictive model capable of solving incomplete data problems by providing two main solutions for each of the problems of complexity and missing patterns, respectively. First, to overcome the problem of lack of patterns, a robust collaborative feature extractor (RCFE) is designed by collaborating between a set of improved restricted Boltzmann machines (I-RBMs) to be able to share learning knowledge among different locally trained I-RBMs to create a more generalized global extraction model. Second, a set of RCFEs is then evolved through a neural network with an augmented hidden layer (NAHL) to enhance the predictive ability by further exploring representation learning to overcome pattern complexity issues. The designed RCFE-NAHL is trained to predict SoH using constant current (CC) discharge characteristics by implying multiple characteristics recorded through the constant voltage (CV) charging process as indicators of health. The proposed SoH prediction approach performances are evaluated on a set of battery life cycles from the well-known NASA database. In this context, the achieved results clearly highlight the higher accuracy and robustness of the proposed learning model.},
  keywords = {battery,battSOH},
  note = {uses NASA batt data},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Berghout23battSOHcollabNN.pdf}
}

@article{Che23HealthPrognosticsLithiumion,
  title = {Health Prognostics for Lithium-Ion Batteries: Mechanisms, Methods, and Prospects},
  shorttitle = {Health Prognostics for Lithium-Ion Batteries},
  author = {Che, Yunhong and Hu, Xiaosong and Lin, Xianke and Guo, Jia and Teodorescu, Remus},
  year = {2023},
  journal = {Energy \& Environmental Science},
  volume = {16},
  number = {2},
  pages = {338--371},
  publisher = {Royal Society of Chemistry},
  doi = {10.1039/D2EE03019E},
  url = {https://pubs.rsc.org/en/content/articlelanding/2023/ee/d2ee03019e},
  urldate = {2023-07-29},
  langid = {english},
  keywords = {battery,battSOH,intro},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Che23HealthPrognosticsLithiumion.pdf}
}

@article{Roman21MachineLearningPipeline,
  title = {Machine Learning Pipeline for Battery State-of-Health Estimation},
  author = {Roman, Darius and Saxena, Saurabh and Robu, Valentin and Pecht, Michael and Flynn, David},
  year = {2021},
  month = may,
  journal = {Nature Machine Intelligence},
  volume = {3},
  number = {5},
  pages = {447--456},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-021-00312-3},
  url = {https://www.nature.com/articles/s42256-021-00312-3},
  urldate = {2023-07-29},
  abstract = {Lithium-ion batteries are ubiquitous in applications ranging from portable electronics to electric vehicles. Irrespective of the application, reliable real-time estimation of battery state of health (SOH) by on-board computers is crucial to the safe operation of the battery, ultimately safeguarding asset integrity. In this Article, we design and evaluate a machine learning pipeline for estimation of battery capacity fade---a metric of battery health---on 179 cells cycled under various conditions. The pipeline estimates battery SOH with an associated confidence interval by using two parametric and two non-parametric algorithms. Using segments of charge voltage and current curves, the pipeline engineers 30 features, performs automatic feature selection and calibrates the algorithms. When deployed on cells operated under the fast-charging protocol, the best model achieves a root-mean-squared error of 0.45\%. This work provides insights into the design of scalable data-driven models for battery SOH estimation, emphasizing the value of confidence bounds around the prediction. The pipeline methodology combines experimental data with machine learning modelling and could be applied to other critical components that require real-time estimation of SOH.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {battery,battSOH},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Roman21MachineLearningPipeline.pdf}
}

@article{Wang23EfficientFederatedTransfer,
  title = {An Efficient Federated Transfer Learning Framework for Collaborative Monitoring of Wind Turbines in {{IoE-enabled}} Wind Farms},
  author = {Wang, Lijin and Fan, Weipeng and Jiang, Guoqian and Xie, Ping},
  year = {2023},
  month = jul,
  journal = {Energy},
  pages = {128518},
  issn = {0360-5442},
  doi = {10.1016/j.energy.2023.128518},
  url = {https://www.sciencedirect.com/science/article/pii/S0360544223019126},
  urldate = {2023-07-29},
  abstract = {Wind turbine (WT) condition monitoring has gained increasing interests in the era of the Internet of Energy (IoE), and existing monitoring approaches mainly focus on training a reliable model in a centralized manner. However, there are three main challenges: data islands due to data privacy and strict confidentiality of wind farm owners, domain shift due to distribution difference of different WTs, and computing and communication burden due to a large number of model parameters. To address these challenges, we propose an efficient federated transfer learning framework (EFTLWT) for collaborative monitoring of WTs in IoE-enabled wind farms, which integrates the adversarial domain adaptation into the federated framework to address the domain shift issue across multiple WTs from different wind farms. It is the first attempt to apply federated transfer learning into the field of WT monitoring. Specifically, we design a lightweight multi-scale neural network model to reduce the computation and communication cost between the client and the server. Furthermore, we propose a partial aggregation strategy to aggregate partial model parameters to reduce the model weight during the uploading and downloading, thus further reducing the burden of communication bandwidth and speeding up the response of the monitoring system. We carry out extensive experiments on real operating datasets of WTs and the results show that our EFTLWT can effectively reduce the domain shift and communication cost, decrease the response time, and greatly improve the performance of the local and federated model while maintaining the favorably comparable performance as the centralized model.},
  langid = {english},
  keywords = {battery,battSOH}
}

@article{Vermeer22rvwLiBattAgingMdl,
  title = {A {{Comprehensive Review}} on the {{Characteristics}} and {{Modeling}} of {{Lithium-Ion Battery Aging}}},
  author = {Vermeer, Wiljan and Chandra Mouli, Gautham Ram and Bauer, Pavol},
  year = {2022},
  month = jun,
  journal = {IEEE Transactions on Transportation Electrification},
  volume = {8},
  number = {2},
  pages = {2205--2232},
  issn = {2332-7782},
  doi = {10.1109/TTE.2021.3138357},
  abstract = {Battery aging is one of the critical problems to be tackled in battery research, as it limits the power and energy capacity during the battery's life. Therefore, optimizing the design of battery systems requires a good understanding of aging behavior. Due to their simplicity, empirical and semiempirical models (EMs) are frequently used in smart charging studies, feasibility studies, and cost analyses studies, among other uses. Unfortunately, these models are prone to significant estimation errors without appropriate knowledge of their inherent limitations and the interdependence between stress factors. This article presents a review of empirical and semiempirical modeling techniques and aging studies, focusing on the trends observed between different studies and highlighting the limitations and challenges of the various models. First, we summarize the main aging mechanisms in lithium-ion batteries. Next, empirical modeling techniques are reviewed, followed by the current challenges and future trends, and a conclusion. Our results indicate that the effect of stress factors is easily oversimplified, and their correlations are often not taken into account. The provided knowledge in this article can be used to evaluate the limitations of aging models and improve their accuracy for various applications.},
  keywords = {battery,battSOH,intro},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Vermeer22rvwLiBattAgingMdl.pdf}
}

@article{Liao16HybridFrameworkCombining,
  title = {A Hybrid Framework Combining Data-Driven and Model-Based Methods for System Remaining Useful Life Prediction},
  author = {Liao, Linxia and K{\"o}ttig, Felix},
  year = {2016},
  month = jul,
  journal = {Applied Soft Computing},
  volume = {44},
  pages = {191--199},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2016.03.013},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494616301223},
  urldate = {2023-07-30},
  abstract = {Remaining useful life prediction is one of the key requirements in prognostics and health management. While a system or component exhibits degradation during its life cycle, there are various methods to predict its future performance and assess the time frame until it does no longer perform its desired functionality. The proposed data-driven and model-based hybrid/fusion prognostics framework interfaces a classical Bayesian model-based prognostics approach, namely particle filter, with two data-driven methods in purpose of improving the prediction accuracy. The first data-driven method establishes the measurement model (inferring the measurements from the internal system state) to account for situations where the internal system state is not accessible through direct measurements. The second data-driven method extrapolates the measurements beyond the range of actually available measurements to feed them back to the model-based method which further updates the particles and their weights during the long-term prediction phase. By leveraging the strengths of the data-driven and model-based methods, the proposed fusion prognostics framework can bridge the gap between data-driven prognostics and model-based prognostics when both abundant historical data and knowledge of the physical degradation process are available. The proposed framework was successfully applied on lithium-ion battery remaining useful life prediction and achieved a significantly better accuracy compared to the classical particle filter approach.},
  langid = {english},
  keywords = {battery,battSOH},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Liao16HybridFrameworkCombining.pdf}
}

@techreport{Yang20EmpiricalAgingModel,
  type = {{{SAE Technical Paper}}},
  title = {An {{Empirical Aging Model}} for {{Lithium-Ion Battery}} and {{Validation Using Real-Life Driving Scenarios}}},
  author = {Yang, Zhuo and Mamun, Abdullah-Al and Makam, Sandeep and Okma, Carrie},
  year = {2020},
  month = apr,
  number = {2020-01-0449},
  address = {Warrendale, PA},
  institution = {SAE International},
  issn = {0148-7191, 2688-3627},
  doi = {10.4271/2020-01-0449},
  url = {https://www.sae.org/publications/technical-papers/content/2020-01-0449/},
  urldate = {2023-07-30},
  abstract = {Lithium-ion batteries (LIBs) have been widely used as the energy storage system in plug-in hybrid electric vehicles (PHEVs) and battery electric vehicles (BEVs) due to their high power and energy density and long cycle life compared to other chemistries. However, LIBs are sensitive to operating cond},
  langid = {english},
  keywords = {battery,battSOH}
}

@article{Gasper22PredictingBatteryCapacitya,
  title = {Predicting Battery Capacity from Impedance at Varying Temperature and State of Charge Using Machine Learning},
  author = {Gasper, Paul and Schiek, Andrew and Smith, Kandler and Shimonishi, Yuta and Yoshida, Shuhei},
  year = {2022},
  month = dec,
  journal = {Cell Reports Physical Science},
  volume = {3},
  number = {12},
  pages = {101184},
  issn = {2666-3864},
  doi = {10.1016/j.xcrp.2022.101184},
  url = {https://www.sciencedirect.com/science/article/pii/S2666386422005021},
  urldate = {2023-07-30},
  abstract = {Prediction of battery health from electrochemical impedance spectroscopy (EIS) data can enable rapid measurement of battery state in real-world applications without using additional sensors or time-consuming performance measurements. However, deconvoluting the effect of capacity, state of charge, and temperature on EIS response is complicated analytically. Here, various machine-learning models, such as linear, Gaussian process, random forest, and artificial neural network regression, are utilized to predict capacity from EIS using hundreds of capacity, direct current (DC) resistance, and EIS measurements recorded under varying conditions of health, temperature, and state of charge (SOC). Several feature extraction and selection methods from traditional electrochemical analysis and statistical modeling are explored using machine-learning pipelines. EIS data from just two frequencies can accurately predict capacity, and interrogation shows that the optimal set of frequencies is not usually intuitive. Best results are achieved with an ensemble model, which predicts battery capacity with a mean absolute error of 1.9\% on data from unobserved cells.},
  langid = {english},
  keywords = {battery,battSOC,battSOH},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gasper22PredictingBatteryCapacitya.pdf}
}

@article{Pannala22ImpactDataWindow,
  title = {Impact of {{Data Window}} on {{Prediction}} of {{Battery Aging}} and {{Swelling}}},
  author = {Pannala, Sravan and Siegel, Jason and Stefanopoulou, Anna},
  year = {2022},
  month = oct,
  journal = {ECS Meeting Abstracts},
  volume = {MA2022-02},
  number = {3},
  pages = {180},
  publisher = {IOP Publishing},
  issn = {2151-2043},
  doi = {10.1149/MA2022-023180mtgabs},
  url = {https://iopscience.iop.org/article/10.1149/MA2022-023180mtgabs/meta},
  urldate = {2023-07-30},
  abstract = {Lithium-ion batteries degrade over time, based on their usage and these changes can be observed in the terminal voltage and cell expansion curves with respect to SOC. Physics-based battery models augmented with degradation modes can describe the changes in battery electrical and mechanical response as the battery ages and can be tuned to predict the observed rate of change. Tuning these models is computationally intensive, because it requires multiple simulations of the battery aging process over the entire lifetime. Accelerated simulations that use an adaptive inter-cycle extrapolation algorithm [1] can reduce simulation time and thus enable use of optimization algorithms to automatically tune degradation models [2]. In this work, we investigate how the amount of aging data used to train the model impacts the prediction of future battery aging assuming the battery continues to operate under the same conditions. The electrode-specific state of health (eSOH) parameters are computed from the reference performance testing performed at periodic intervals throughout the battery's lifetime. These four parameters include the capacities of positive and negative electrodes Cp and Cn, and the corresponding electrode stoichiometric windows y0 and x100. These 4 parameters extracted from a cell aged at C/5 constant current charging rate were used for training the model. The aging model includes SEI growth in the negative electrodes which causes a loss of cyclable lithium/loss of lithium inventory (LLI) and loss of electrode active material (LAM) due to particle cracking at both the negative and positive electrodes. Simulation results show us that a shorter training window of three eSOH reference points (every 25 cycles) over (Cycles 41 to 98) over-predicts battery degradation because the rate of degradation during these initial cycles is larger than the degradation which happens later in the battery's life. The battery degradation parameters that are predicted (and shown in the figure) include battery capacity, maximum reversible swelling in the battery, loss of active material in the negative electrode and loss of lithium inventory. If we increase the model training window to five eSOH data points (Cycles 41 to 175), the prediction of battery degradation is much better because the model now accounts for both the initial higher rate of degradation and subsequent lower rate of degradation.},
  langid = {english},
  keywords = {battery,battSOH}
}

@article{Gasper21battAlgebraLifeID,
  title = {Challenging {{Practices}} of {{Algebraic Battery Life Models}} through {{Statistical Validation}} and {{Model Identification}} via {{Machine-Learning}}},
  author = {Gasper, Paul and Gering, Kevin and Dufek, Eric and Smith, Kandler},
  year = {2021},
  month = feb,
  journal = {Journal of The Electrochemical Society},
  volume = {168},
  number = {2},
  pages = {020502},
  publisher = {IOP Publishing},
  issn = {1945-7111},
  doi = {10.1149/1945-7111/abdde1},
  url = {https://dx.doi.org/10.1149/1945-7111/abdde1},
  urldate = {2023-07-30},
  abstract = {Various modeling techniques are used to predict the capacity fade of Li-ion batteries. Algebraic reduced-order models, which are inherently interpretable and computationally fast, are ideal for use in battery controllers, technoeconomic models, and multi-objective optimizations. For Li-ion batteries with graphite anodes, solid-electrolyte-interphase (SEI) growth on the graphite surface dominates fade. This fade is often modeled using physically informed equations, such as square-root of time for predicting solvent-diffusion limited SEI growth, and Arrhenius and Tafel-like equations predicting the temperature and state-of-charge rate dependencies. In some cases, completely empirical relationships are proposed. However, statistical validation is rarely conducted to evaluate model optimality, and only a handful of possible models are usually investigated. This article demonstrates a novel procedure for automatically identifying reduced-order degradation models from millions of algorithmically generated equations via bi-level optimization and symbolic regression. Identified models are statistically validated using cross-validation, sensitivity analysis, and uncertainty quantification via bootstrapping. On a LiFePO4/Graphite cell calendar aging data set, automatically identified models utilizing square-root, power law, stretched exponential, and sigmoidal functions result in greater accuracy and lower uncertainty than models identified by human experts, and demonstrate that previously known physical relationships can be empirically ``rediscovered'' using machine learning.},
  langid = {english},
  keywords = {battery,battSOH,intro},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gasper21battAlgebraLifeID.pdf}
}

@article{Attia22ReviewKneesLithiumIon,
  title = {Review---``{{Knees}}'' in {{Lithium-Ion Battery Aging Trajectories}}},
  author = {Attia, Peter M. and Bills, Alexander and Planella, Ferran Brosa and Dechent, Philipp and dos Reis, Gon{\c c}alo and Dubarry, Matthieu and Gasper, Paul and Gilchrist, Richard and Greenbank, Samuel and Howey, David and Liu, Ouyang and Khoo, Edwin and Preger, Yuliya and Soni, Abhishek and Sripad, Shashank and Stefanopoulou, Anna G. and Sulzer, Valentin},
  year = {2022},
  month = jun,
  journal = {Journal of The Electrochemical Society},
  volume = {169},
  number = {6},
  pages = {060517},
  publisher = {IOP Publishing},
  issn = {1945-7111},
  doi = {10.1149/1945-7111/ac6d13},
  url = {https://dx.doi.org/10.1149/1945-7111/ac6d13},
  urldate = {2023-08-03},
  abstract = {Lithium-ion batteries can last many years but sometimes exhibit rapid, nonlinear degradation that severely limits battery lifetime. In this work, we review prior work on ``knees'' in lithium-ion battery aging trajectories. We first review definitions for knees and three classes of ``internal state trajectories'' (termed snowball, hidden, and threshold trajectories) that can cause a knee. We then discuss six knee ``pathways'', including lithium plating, electrode saturation, resistance growth, electrolyte and additive depletion, percolation-limited connectivity, and mechanical deformation---some of which have internal state trajectories with signals that are electrochemically undetectable. We also identify key design and usage sensitivities for knees. Finally, we discuss challenges and opportunities for knee modeling and prediction. Our findings illustrate the complexity and subtlety of lithium-ion battery degradation and can aid both academic and industrial efforts to improve battery lifetime.},
  langid = {english},
  keywords = {battery,battSOH,intro,priorityRead},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Attia22ReviewKneesLithiumIon.pdf}
}

@techreport{Gasper21IdentificationLifeModels,
  title = {Identification of {{Life Models}} for {{Li-Ion Batteries Using Penalized Regression}} and {{Bilevel Optimization}}},
  author = {Gasper, Paul and Smith, Kandler},
  year = {2021},
  month = mar,
  number = {NREL/PR-5400-77973},
  institution = {National Renewable Energy Lab. (NREL), Golden, CO (United States)},
  url = {https://www.osti.gov/biblio/1774844},
  urldate = {2023-08-03},
  abstract = {Reduced-order physics-based life models are extremely useful for rapidly predicting battery state-of-health and for simulating battery lifetime in arbitrary aging conditions. However, identification of well-parameterized models is difficult. This is because, for maximum usefulness in predicting lifetime under a variety of conditions, aging test data exhibits many degradation mechanisms, which all need to be accurately modeled. However, because aging tests are time-consuming and expensive, especially for large-format batteries, a minimum of tests are conducted while probing many stress factors. Building a well-parameterized model is then very challenging: an under-parameterized model will neglect critical degradation modes, and an over-parameterized model will extrapolate poorly to new testing conditions. To complicate this matter, the functional form of the model for any individual degradation rate can be very difficult to identify. In this work, the statistical tools of penalized regression and bilevel optimization are used to help identify both the functional forms of and optimize the parameters of reduced-order life models, accelerating identification of robust models. Model robustness is demonstrated through traditional statistics methods of cross-validation and sensitivity analysis, uncertainty quantification through bootstrap resampling and Monte-Carlo simulation, and simulation of real-world use cases.},
  langid = {english},
  keywords = {battery,battSOH,intro},
  file = {C:\Users\scott\Zotero\storage\R6TNCG9B\Gasper21IdentificationLifeModels.pdf}
}

@misc{Birkl17OxfordBatteryDegradation,
  title = {Oxford {{Battery Degradation Dataset}} 1},
  author = {Birkl, Christoph},
  year = {2017},
  publisher = {University of Oxford},
  doi = {10.5287/BODLEIAN:KO2KDMYGG},
  url = {http://ora.ox.ac.uk/objects/uuid:03ba4b01-cfed-46d3-9b1a-7d4a7bdf6fac},
  urldate = {2023-08-04},
  abstract = {Lithium-ion (Li-ion) batteries are the most popular energy storage technology in consumer electronics and electric vehicles and are increasingly applied in stationary storage systems. Yet, concerns about safety and reliability remain major obstacles, which must be addressed in order to improve the acceptance of this technology. The gradual degradation of Li-ion cells over time lies at the heart of this problem. Time, usage and environmental conditions lead to performance deterioration and cell failures, which, in rare cases, can be catastrophic due to fires or explosions. The physical and chemical mechanisms responsible for degradation are numerous, complex and interdependent. Our understanding of degradation and failure of Li-ion cells is still very limited and more limited yet are reliable and practical methods for the detection and prediction of these phenomena. This dataset contains the results of long term cycling of 8 lithium-ion cells in our lab in Oxford. The full details are given in the readme.txt file.},
  collaborator = {Howey, David},
  copyright = {ODC Attribution Share-Alike for data/databases (ODC-ODbL)},
  keywords = {battery,battSOH,dataSrc},
  note = {fasf}
}

@phdthesis{Birkl17DiagnosisPrognosisDegradation,
  title = {Diagnosis and Prognosis of Degradation in Lithium-Ion Batteries},
  author = {Birkl, Christoph},
  year = {2017},
  url = {https://howey.eng.ox.ac.uk/data-and-code/},
  school = {University of Oxford},
  keywords = {battery,battSOH,dataSrc},
  note = {Reference to the oxford battery aging dataset (and code)}
}

@article{Hasib21ComprehensiveReviewAvailable,
  title = {A {{Comprehensive Review}} of {{Available Battery Datasets}}, {{RUL Prediction Approaches}}, and {{Advanced Battery Management}}},
  author = {Hasib, Shahid A. and Islam, S. and Chakrabortty, Ripon K. and Ryan, Michael J. and Saha, D. K. and Ahamed, Md H. and Moyeen, S. I. and Das, Sajal K. and Ali, Md F. and Islam, Md R. and Tasneem, Z. and Badal, Faisal R.},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {86166--86193},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3089032},
  abstract = {Battery ensures power solutions for many necessary portable devices such as electric vehicles, mobiles, and laptops. Owing to the rapid growth of Li-ion battery users, unwanted incidents involving Li-ion batteries have also increased to some extent. In particular, the sudden breakdown of industrial and lightweight machinery due to battery failure causes a substantial economic loss for the industry. Consequently, battery state estimation, management system, and estimation of the remaining useful life (RUL) have become a topic of interest for researchers. Considering this, appropriate battery data acquisition and proper information on available battery data sets may require. This review paper is mainly focused on three parts. The first one is battery data acquisitions with commercially and freely available Li-ion battery data set information. The second is the estimation of the states of battery with the battery management system. And third is battery RUL estimation. Various RUL prognostic methods applied for Li-ion batteries are classified, discussed, and reviewed based on their essential performance parameters. Information on commercially and publicly available data sets of many battery models under various conditions is also reviewed. Various battery states are reviewed considering advanced battery management systems. To that end, a comparative study of Li-ion battery RUL prediction is provided together with the investigation of various RUL prediction algorithms and mathematical modelling.},
  keywords = {battery,battSOH,dataSrc,intro},
  file = {C:\Users\scott\Zotero\storage\6AAV3ZIQ\Hasib et al.2021 - A Comprehensive Revi.pdf}
}

@article{Li22TemperaturePredictionLithiumion,
  title = {Temperature Prediction of Lithium-Ion Batteries Based on Electrochemical Impedance Spectrum: {{A}} Review},
  shorttitle = {Temperature Prediction of Lithium-Ion Batteries Based on Electrochemical Impedance Spectrum},
  author = {Li, Dezhi and Wang, Licheng and Duan, Chongxiong and Li, Qiang and Wang, Kai},
  year = {2022},
  journal = {International Journal of Energy Research},
  volume = {46},
  number = {8},
  pages = {10372--10388},
  issn = {1099-114X},
  doi = {10.1002/er.7905},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/er.7905},
  urldate = {2023-08-04},
  abstract = {With the rapid development of global electric vehicles, artificial intelligence, and aerospace, lithium-ion batteries (LIBs) have become more and more widely used due to their high property. More and more disasters are caused by battery combustion. Among them, the temperature prediction of LIBs is the key to prevent the occurrence of fire. At present, using surface temperature sensor to measure the temperature of LIBs is the main method. High-capacity LIB packs used in electric vehicles and grid-tied stationary energy storage system essentially consist of thousands of individual LIB cells. Therefore, installing a physical sensor at each cell, especially at the cell core, is not practically feasible from the solution cost, space, and weight point of view. So developing a new method for battery temperature prediction has become an urgent problem to be solved. Electrochemical impedance spectroscopy (EIS) is a widely applied non-destructive method of characterization of LIBs. In recent years, methods of predicting LIBs temperature by EIS have been developed. The prediction of LIBs temperature based on EIS has the advantages of high real-time performance and prediction accuracy, and the device is simple and practical. The proposed method has a good development prospect in electric vehicles and other fields and can effectively solve the current problems of LIBs temperature prediction. Therefore, it is urgent to summarize these works to promote the next development. This review summarizes the main methods of using EIS to predict the temperature of LIBs in recent years, including the methods based on the impedance, phase shift, and intercept frequency. The principle and application of various methods are reviewed. The advantages and disadvantages of different methods and the future development direction are discussed. Highlights Use EIS to quickly and effectively predict the internal temperature changes of LIBs. No hardware temperature sensors and thermal model are required. The methods to predict battery temperature based on impedance, phase shift, and intercept frequency are reviewed.},
  copyright = {{\copyright} 2022 John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {battery,battFire}
}

@article{Li23MachineLearningAssisted,
  title = {Machine Learning Assisted Advanced Battery Thermal Management System: {{A}} State-of-the-Art Review},
  shorttitle = {Machine Learning Assisted Advanced Battery Thermal Management System},
  author = {Li, Ao and Weng, Jingwen and Yuen, Anthony Chun Yin and Wang, Wei and Liu, Hengrui and Lee, Eric Wai Ming and Wang, Jian and Kook, Sanghoon and Yeoh, Guan Heng},
  year = {2023},
  month = apr,
  journal = {Journal of Energy Storage},
  volume = {60},
  pages = {106688},
  issn = {2352-152X},
  doi = {10.1016/j.est.2023.106688},
  url = {https://www.sciencedirect.com/science/article/pii/S2352152X23000853},
  urldate = {2023-08-04},
  abstract = {With an increasingly wider application of the lithium-ion battery (LIB), specifically the drastic increase of electric vehicles in cosmopolitan cities, improving the thermal and fire resilience of LIB systems is inevitable. Thus, in-depth analysis and performance-based study on battery thermal management system (BTMs) design have arisen as a popular research topic in energy storage systems. Among the LIB system parameters, such as battery temperature distribution, battery heat generation rate, cooling medium properties, electrical properties, physical dimension design, etc., multi-factor design optimisation is one of the most difficult experimental tasks. Computational simulations deliver a holistic solution to the BTMs design, yet it demands an immense amount of computational power and time, which is often not practical for the design optimisation process. Therefore, machine learning (ML) models play a non-substitute role in the safety management of battery systems. ML models aid in temperature prediction and safety diagnosis, thereby assisting in the early warning of battery fire and its mitigation. In this review article, we summarise extensive lists of literature on BTMs employing ML models and identify the current state-of-the-art research, which is expected to serve as a much-needed guideline and reference for future design optimisation. Following that, the application of various ML models in battery fire diagnosis and early warning is illustrated. Finally, the authors propose improved approaches to advanced battery safety management with ML. This review paper aims to bring new insights into the application of ML in the LIB thermal safety issue and BTMs design and anticipate boosting further advanced battery system design not limited to the thermal management system, as well as proposing potential digital twin modelling for BTMs.},
  langid = {english},
  keywords = {battery,battFire,intro}
}

@article{Chombo22PredictionOnsetThermal,
  title = {Prediction of the Onset of Thermal Runaway and Its Thermal Hazards in 18650 Lithium-Ion Battery Abused by External Heating},
  author = {Chombo, Pius Victor and Laoonual, Yossapong},
  year = {2022},
  month = may,
  journal = {Fire Safety Journal},
  volume = {129},
  pages = {103560},
  issn = {0379-7112},
  doi = {10.1016/j.firesaf.2022.103560},
  url = {https://www.sciencedirect.com/science/article/pii/S0379711222000388},
  urldate = {2023-08-05},
  abstract = {This work developed empirical equations to predict the onset of thermal runaway (TR) and its related thermal hazards under external heating abuse. A set of Lithium Nickel Cobalt Aluminum Oxide (NCA), Lithium Cobalt Oxide (LCO) and Lithium Manganese Oxide (LMO) Li-ion batteries (LIBs) with 25--100\% state of charge (SOC) was externally heated-to-fire. From the experimental results, we generated empirical equations to predict the onset of thermal runaway (TRonset) and hazards such as maximum surface temperature (Tsurf), and flame temperature (Tflame) except for LMO LIBs due to their violent explosions. To demonstrate the accuracy of predictions, we compared the results predicted with another set of test data from the same LIBs but different SOCs and evaluated the accuracy using percentage error (\%Er). The developed empirical equations achieved higher prediction accuracy, with the TRonset reaching the \%Er of less than 10\% for both NCA and LCO LIBs. The lower SOC (20\%) showed the greatest discrepancy, with the maximum \%Er of 2.7 and 7.4\% in LCO and NCA, respectively. A large variation of \%Er in Tsurf was observed regardless of SOC due to violent explosion, and \%Er reached the maximum of 15.4\% and 24.0\% in NCA and LCO, respectively. In Tflame, the highest \%Er observed for NCA and LCO are 25.6\% and 29.9\%, respectively. Overall, \%Er show no correlation with SOC in any of the TRonset, Tsurf and Tflame predictions. LMO as the most energy dense of the three LIB samples, many of them ruptured violently during the test. In light of the growing demand for energy dense LIBs, new empirical equations should include ruptured samples as well.},
  langid = {english},
  keywords = {battery,battFire}
}

@article{MdSaid19PredictionLithiumionBattery,
  title = {Prediction of {{Lithium-ion Battery Thermal Runaway Propagation}} for {{Large Scale Applications Fire Hazard Quantification}}},
  author = {Md Said, Mohamad Syazarudin and Mohd Tohir, Mohd Zahirasri},
  year = {2019},
  month = oct,
  journal = {Processes},
  volume = {7},
  number = {10},
  pages = {703},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-9717},
  doi = {10.3390/pr7100703},
  url = {https://www.mdpi.com/2227-9717/7/10/703},
  urldate = {2023-08-05},
  abstract = {The high capacity and voltage properties demonstrated by lithium-ion batteries render them as the preferred energy carrier in portable electronic devices. The application of the lithium-ion batteries which previously circulating and contained around small-scale electronics is now expanding into large scale emerging markets such as electromobility and stationary energy storage. Therefore, the understanding of the risk involved is imperative. Thermal runaway is the most common failure mode of lithium-ion battery which may lead to safety incidents. Transport process of immense amounts of heat released during thermal runaway of lithium-ion battery to neighboring batteries in a module can lead to cascade failure of the whole energy storage system. In this work, a model is developed to predict the propagation of lithium-ion battery in a module for large scale applications. For this purpose, kinetic of material thermal decomposition is combined with heat transfer modelling. The simulation is built based on chemical kinetics at component level of a singular cell and energy balance that accounts for conductive and convective heat transfer.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {battery,battFire},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\MdSaid19PredictionLithiumionBattery.pdf}
}

@article{Fioravanti20PredictiveMaintenancePracticesOperational,
  title = {Predictive-{{Maintenance Practices}}: {{For Operational Safety}} of {{Battery Energy Storage Systems}}},
  shorttitle = {Predictive-{{Maintenance Practices}}},
  author = {Fioravanti, Richard and Kumar, Kiran and Nakata, Shinobu and Chalamala, Babu and Preger, Yuliya},
  year = {2020},
  month = nov,
  journal = {IEEE Power and Energy Magazine},
  volume = {18},
  number = {6},
  pages = {86--97},
  issn = {1558-4216},
  doi = {10.1109/MPE.2020.3014542},
  abstract = {Changes in the Demand Profile and a growing role for renewable and distributed generation are leading to rapid evolution in the electric grid. These changes are beginning to considerably strain the transmission and distribution infrastructure. Utilities are increasingly recognizing that the integration of energy storage in the grid infrastructure will help manage intermittency and improve grid reliability. This recognition, coupled with the proliferation of state-level renewable portfolio standards and rapidly declining lithium-ion (Li-ion) battery costs, has led to a surge in the deployment of battery energy storage systems (BESSs). Although BESSs represented less than 1\% of grid-scale energy storage in the United States in 2019, they are the preferred technology to meet growing demand because they are modular, scalable, and easy to deploy across diverse use cases and geographic locations.},
  keywords = {battery,battFire,intro},
  note = {\section{Fioravanti20PredictiveMaintenancePracticesOperational}

\par
``Predict'' (Fioravanti et al., 2020, p. 86)
\par
``outfitted for battery'' (Fioravanti et al., 2020, p. 90)
\par
(Fioravanti et al., 2020, p. 91) a comment in a stcky},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Fioravanti20PredictiveMaintenancePracticesOperational.pdf}
}

@article{Eom07LifePredictionReliability,
  title = {Life Prediction and Reliability Assessment of Lithium Secondary Batteries},
  author = {Eom, Seung-Wook and Kim, Min-Kyu and Kim, Ick-Jun and Moon, Seong-In and Sun, Yang-Kook and Kim, Hyun-Soo},
  year = {2007},
  month = dec,
  journal = {Journal of Power Sources},
  series = {13th {{International Meeting}} on {{Lithium Batteries}}},
  volume = {174},
  number = {2},
  pages = {954--958},
  issn = {0378-7753},
  doi = {10.1016/j.jpowsour.2007.06.208},
  url = {https://www.sciencedirect.com/science/article/pii/S0378775307013389},
  urldate = {2023-08-05},
  abstract = {Reliability assessment of lithium secondary batteries was mainly considered. Shape parameter ({$\beta$}) and scale parameter ({$\eta$}) were calculated from experimental data based on cycle life test. We also examined safety characteristics of lithium secondary batteries. As proposed by IEC 62133 (2002), we had performed all of the safety/abuse tests such as `mechanical abuse tests', `environmental abuse tests', `electrical abuse tests'. This paper describes the cycle life of lithium secondary batteries, FMEA (failure modes and effects analysis) and the safety/abuse tests we had performed.},
  langid = {english},
  keywords = {battery,battSOH},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Eom07LifePredictionReliability.pdf}
}

@article{Li19LithiumionBatteryThermal,
  title = {Lithium-Ion {{Battery Thermal Safety}} by {{Early Internal Detection}}, {{Prediction}} and {{Prevention}}},
  author = {Li, Bing and Parekh, Mihit H. and Adams, Ryan A. and Adams, Thomas E. and Love, Corey T. and Pol, Vilas G. and Tomar, Vikas},
  year = {2019},
  month = sep,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {13255},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-49616-w},
  url = {https://www.nature.com/articles/s41598-019-49616-w},
  urldate = {2023-08-09},
  abstract = {Temperature rise in Lithium-ion batteries (LIBs) due to solid electrolyte interfaces breakdown, uncontrollable exothermic reactions in electrodes and Joule heating can result in the catastrophic failures such as thermal runaway, which is calling for reliable real-time electrode temperature monitoring. Here, we present a customized LIB setup developed for early detection of electrode temperature rise during simulated thermal runaway tests incorporating a modern additive manufacturing-supported resistance temperature detector (RTD). An advanced RTD is embedded in a 3D printed polymeric substrate and placed behind the electrode current collector of CR2032 coin cells that can sustain harsh electrochemical operational environments (acidic electrolyte without Redox, short-circuiting, leakage etc.) without participating in electrochemical reactions. The internal RTD measured an average 5.8\,{$^\circ$}C higher temperature inside the cells than the external RTD with almost 10 times faster detection ability, prohibiting thermal runaway events without interfering in the LIBs' operation. A temperature prediction model is developed to forecast battery surface temperature rise stemming from measured internal and external RTD temperature signatures.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {battery,battFire},
  note = {Is about internal sensor, but has a simple linear diffyQ for battery temp that might be adaptable to our case. ~Can predict rate of change, so maybe enough for runaway prediction w/o much knowledge of battery.
\par
Fancier model: (Chen et al., 2021)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Li19LithiumionBatteryThermal2.pdf}
}

@article{Chen21SimplifiedMathematicalModel,
  title = {A {{Simplified Mathematical Model}} for {{Heating-Induced Thermal Runaway}} of {{Lithium-Ion Batteries}}},
  author = {Chen, Haodong and Buston, Jonathan E. H. and Gill, Jason and Howard, Daniel and Williams, Rhiannon C. E. and Read, Elliott and Abaza, Ahmed and Cooper, Brian and Wen, Jennifer X.},
  year = {2021},
  month = jan,
  journal = {Journal of The Electrochemical Society},
  volume = {168},
  number = {1},
  pages = {010502},
  publisher = {IOP Publishing},
  issn = {1945-7111},
  doi = {10.1149/1945-7111/abd64c},
  url = {https://dx.doi.org/10.1149/1945-7111/abd64c},
  urldate = {2023-08-09},
  abstract = {The present study aims to develop a simplified mathematical model for the evolution of heating-induced thermal runaway (TR) of lithium-ion batteries (LIBs). This model only requires a minimum number of input parameters, and some of these unknown parameters can be obtained from accelerating rate calorimeter (ARC) tests and previous studies, removing the need for detailed measurements of heat flow of cell components by differential scanning calorimetry. The model was firstly verified by ARC tests for a commercial cylindrical 21700 cell for the prediction of the cell surface temperature evolution with time. It was further validated by uniform heating tests of 21700 cells conducted with flexible and nichrome-wire heaters, respectively. The validated model was finally used to investigate the critical ambient temperature that triggers battery TR. The predicted critical ambient temperature is between 127 {$^\circ$}C and 128 {$^\circ$}C. The model has been formulated as lumped 0D, axisymmetric 2D and full 3D to suit different heating and geometric arrangements and can be easily extended to predict the TR evolution of other LIBs with different geometric configurations and cathode materials. It can also be easily implemented into other computational fluid dynamics (CFD) code.},
  langid = {english},
  keywords = {battery,battFire,priorityRead},
  note = {Simple compared to other methods diffy Q for cylindrical batteries, has 12 params. ~Predicts runaway temp. ~Simpler than diffyQ in (Li et al., 2019)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chen21SimplifiedMathematicalModel.pdf}
}

@article{Li23TimelyThermalRunaway,
  title = {Timely {{Thermal Runaway Prognosis}} for {{Battery Systems}} in {{Real-World Electric Vehicles Based}} on {{Temperature Abnormality}}},
  author = {Li, Da and Zhang, Zhaosheng and Wang, Zhenpo and Liu, Peng and Liu, Zhicheng and Lin, Ni},
  year = {2023},
  month = feb,
  journal = {IEEE Journal of Emerging and Selected Topics in Power Electronics},
  volume = {11},
  number = {1},
  pages = {120--130},
  issn = {2168-6785},
  doi = {10.1109/JESTPE.2022.3153337},
  abstract = {Hundreds of thermal-runaway-induced battery fire accidents have been occurring to real-world electric vehicles (EVs) in recent years, exposing life to danger and causing property losses. Timely and fast battery thermal runaway prognosis is essential but restricted by limited parameters and complex influencing factors during real-world operation of EVs, i.e., environment, driving behavior, and weather. To cope with the issue, several data-driven methods are combined, and the thermal runaway prognosis is realized by two steps, i.e., temperature prediction by the modified extreme gradient boosting (XGBoost) and then abnormality detection by the principal component analysis (PCA) and density-based spatial clustering of applications with noise (DBSCAN). The XGBoost is modified and trained by data of real-world EVs to couple the influencing factors during the real-world operation of EVs. For parameter optimization, the ``pretraining and adjacent grid optimizing method'' (P-AGOM) and the ``adjacent grid optimizing method'' (AGOM) are proposed to achieve locally optimal hyperparameters for XGBoost and DBSCAN. Verified results showcase that the XGBoost-PCA-DBSCAN achieves accurate 5-min-forward temperature prediction, and the mean square errors (mses) of four seasons are only 0.0729, 0.0594, 0.0747, and 0.0523, respectively. By modification of XGBoost, the mse of temperature prediction is reduced by 31.2\%. In addition, the 35-min-forward thermal runaway prognosis by the XGBoost-PCA-DBSCAN will provide the driver sufficient response time to minimize the loss of life and property.},
  keywords = {battery,battFire}
}

@article{Jiang21DatadrivenFaultDiagnosis,
  title = {Data-Driven Fault Diagnosis and Thermal Runaway Warning for Battery Packs Using Real-World Vehicle Data},
  author = {Jiang, Lulu and Deng, Zhongwei and Tang, Xiaolin and Hu, Lin and Lin, Xianke and Hu, Xiaosong},
  year = {2021},
  month = nov,
  journal = {Energy},
  volume = {234},
  pages = {121266},
  issn = {0360-5442},
  doi = {10.1016/j.energy.2021.121266},
  url = {https://www.sciencedirect.com/science/article/pii/S0360544221015140},
  urldate = {2023-08-09},
  abstract = {Battery fault diagnosis is essential to ensure the safe and reliable operation of electric vehicles. Early detection of battery faults can reduce battery incidents and property losses. However, early warning of battery thermal runaway is still a challenging task. This paper proposes a novel data-driven method for lithium-ion battery pack fault diagnosis and thermal runaway warning based on state representation methodology. The normalized battery voltages are used to achieve accurate identification of battery early faults. The proposed method calculates the real-time state of each cell to characterize the internal characteristics of the battery cell, and the state changes are recorded to achieve battery fault diagnosis. The fault detection time is compared with the alarm time of real vehicles to verify the effectiveness of the proposed method. The real-world operation data of four electric vehicles with different specifications are used to verify the feasibility, robustness, and reliability of the proposed method. The results show that the method can achieve not only the accurate identification of the faulty cells and accurate determination of the voltage fault type but also the early detection of faults and early warning of thermal runaway.},
  langid = {english},
  keywords = {battery,battFire},
  note = {Uses ML on operating EVs to predict faults that warn of battery fire. ~Knowing what those faults are might be useful for utility scale, as might the ML techniques.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Jiang21DatadrivenFaultDiagnosis2.pdf}
}

@article{Li19DBSCANBasedThermalRunaway,
  title = {{{DBSCAN-Based Thermal Runaway Diagnosis}} of {{Battery Systems}} for {{Electric Vehicles}}},
  author = {Li, Da and Zhang, Zhaosheng and Liu, Peng and Wang, Zhenpo},
  year = {2019},
  month = jan,
  journal = {Energies},
  volume = {12},
  number = {15},
  pages = {2977},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1996-1073},
  doi = {10.3390/en12152977},
  url = {https://www.mdpi.com/1996-1073/12/15/2977},
  urldate = {2023-08-09},
  abstract = {Battery system diagnosis and prognosis are essential for ensuring the safe operation of electric vehicles (EVs). This paper proposes a diagnosis method of thermal runaway for ternary lithium-ion battery systems based on the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) clustering. Two-dimensional fault characteristics are first extracted according to battery voltage, and DBSCAN clustering is used to diagnose the potential thermal runaway cells (PTRC). The periodic risk assessing strategy is put forward to evaluate the fault risk of battery cells. The feasibility, reliability, stability, necessity, and robustness of the proposed algorithm are analyzed, and its effectiveness is verified based on datasets collected from real-world operating electric vehicles. The results show that the proposed method can accurately predict the locations of PTRC in the battery pack a few days before the thermal runaway occurrence.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {battery,battFire},
  note = {Predicts individual cell runaway up to a couple weeks early. ~This is on cars, using 2D DB SCAN, so simple features.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Li19DBSCANBasedThermalRunaway2.pdf}
}

@article{Zhang22DatadrivenEarlyWarning,
  title = {Data-Driven Early Warning Strategy for Thermal Runaway Propagation in {{Lithium-ion}} Battery Modules with Variable State of Charge},
  author = {Zhang, Wencan and Ouyang, Nan and Yin, Xiuxing and Li, Xingyao and Wu, Weixiong and Huang, Liansheng},
  year = {2022},
  month = oct,
  journal = {Applied Energy},
  volume = {323},
  pages = {119614},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2022.119614},
  url = {https://www.sciencedirect.com/science/article/pii/S0306261922009187},
  urldate = {2023-08-09},
  abstract = {Thermal runaway (TR) propagation is triggered in a battery pack by abnormalities such as a cell fire or explosion, which leads to severe consequences. Predicting the TR propagation is challenging due to the complex, high non-linearity, and uncertain disturbances of TR. This paper establishes an electro-thermal coupling simulation model of TR propagation to supplement experimental data and public datasets for model training and verification. Then, a data-driven fusion model named Multi-Mode and Multi-Task Thermal Propagation Forecasting Neural Network (MMTPFNN) is established quantitative advance multi-step prediction of TR propagation in Li-ion battery modules, and a temperature-based TR propagation grading warning strategy is proposed. The TR propagation is mainly influenced by the thermal characteristics of surrounding batteries, and the temperature distribution in the entire battery module is of great significance to the prediction of TR propagation. Herein, the model is presented by using the thermal image and the discrete operating data of cells. Furthermore, because TR is a small probability event, obtaining the thermal image of the battery module requires additional system memory and computational resources. A switching strategy of the prediction model is established to improve the applicability of the model with the temperature threshold of 60~{$^\circ$}C. When the battery is in a safe temperature range (below 60~{$^\circ$}C), the long short-term memory (LSTM) model is run to predict the battery temperature. Once the battery temperature is detected above 60~{$^\circ$}C, the thermal image is captured, and the MMTPFNN model is run to predict the TR propagation. In the validation section, different network structures are discussed, and different time resolutions and different window settings of the MMTPFNN are compared. Finally, the early warning strategy with three alert levels is introduced, and the effectiveness of the warning strategy with different window settings and initial SoCs is further discussed.},
  langid = {english},
  keywords = {battery,battFire},
  note = {Says thermal properties of neighboring batteries influence thermal runaway, so uses a ``thermal image'' to (apparently) learn that with ML. ~Also, does model switching at 60 C threshold. ~No paper, as of yet.},
  file = {C:\Users\scott\Zotero\storage\E3WGNR75\Zhang et al. - 2022 - Data-driven early warning strategy for thermal run.pdf}
}

@article{Liao19SurveyMethodsMonitoring,
  title = {A Survey of Methods for Monitoring and Detecting Thermal Runaway of Lithium-Ion Batteries},
  author = {Liao, Zhenghai and Zhang, Shen and Li, Kang and Zhang, Guoqiang and Habetler, Thomas G.},
  year = {2019},
  month = oct,
  journal = {Journal of Power Sources},
  volume = {436},
  pages = {226879},
  issn = {0378-7753},
  doi = {10.1016/j.jpowsour.2019.226879},
  url = {https://www.sciencedirect.com/science/article/pii/S0378775319308729},
  urldate = {2023-08-09},
  abstract = {Lithium-ion batteries have many advantages such as the high specific energy, the high specific power, the long calendar life, being environmentally friendly, and can be used without the memory effect. Thus this type of battery is widely used as the core component in many applications such as electric vehicles, portable electronic devices, and distributed energy storage systems. However, lithium-ion batteries can easily develop into thermal runaways due to the stress and abuse from mechanical, electrical, and thermal perspectives, posing a major threat to the overall safety of many battery systems. On the premise of passing the manufacturer's safety inspections, a variety of methods for monitoring and detecting thermal runaway events are developed to enhance the safety and robustness of lithium-ion batteries in different application scenarios. This paper thus summarizes the existing literature on this topic and presents a comparative study on the sensitivity of various monitoring and detection methods. Potential future research directions are also discussed in detail to further enhance the safety and robustness of lithium-ion battery systems.},
  langid = {english},
  keywords = {battery,battFire,intro},
  note = {Good b/c it's a survey paper. ~Read (Tran et al., 2022) instead?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Liao19SurveyMethodsMonitoring2.pdf}
}

@article{Wu22DimensionlessNormalizedConcentration,
  title = {Dimensionless Normalized Concentration Based Thermal-Electric Regression Model for the Thermal Runaway of Lithium-Ion Batteries},
  author = {Wu, Hang and Chen, Siqi and Chen, Jie and Jin, Changyong and Xu, Chengshan and Rui, Xinyu and Hsu, Hungjen and Zheng, Yuejiu and Feng, Xuning},
  year = {2022},
  month = feb,
  journal = {Journal of Power Sources},
  volume = {521},
  pages = {230958},
  issn = {0378-7753},
  doi = {10.1016/j.jpowsour.2021.230958},
  url = {https://www.sciencedirect.com/science/article/pii/S0378775321014415},
  urldate = {2023-08-09},
  abstract = {Thermal runaway (TR) has become a critical issue for lithium-ion battery-based electric vehicles (EVs) and energy storage stations. Accurate thermal-electric estimation is significant for the safety of lithium-ion batteries (LIB). To predict the highly correlated behavior of temperature and voltage variation characteristics during the TR process, a thermal-electric coupled TR model within the full range of State of Charge (SOC) is constructed in this study. The electric quantity normalized concentration ceq is defined and used to construct the corresponding numerical relationship between temperature and voltage in the model. This model can effectively fit the battery TR temperature and voltage variation behavior under both adiabatic TR test and oven test results. Furthermore, the model-based analysis proves that the temperature and voltage changes during the TR can be mutually obtained through the stable numerical coupling relationship. Besides, the relationship between SOC and the concentration field is established. Moreover, the feasibility of using the normalized concentration to couple the voltage and temperature signal is verified. This study provides perspectives for the coupling relationships of TR characteristic signals and multi-signal coupled modeling of lithium-ion battery TR.},
  langid = {english},
  keywords = {battery,battFire,battSOC},
  note = {Thermal runaway depends upon SOC. ~Claims this model is the 1st to predict TR across full SOC range.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wu22DimensionlessNormalizedConcentration2.pdf}
}

@article{Wu23ThermalRunawayBehaviors,
  title = {Thermal Runaway Behaviors of {{Li-ion}} Batteries after Low Temperature Aging: {{Experimental}} Study and Predictive Modeling},
  shorttitle = {Thermal Runaway Behaviors of {{Li-ion}} Batteries after Low Temperature Aging},
  author = {Wu, Senming and Wang, Chang and Luan, Weiling and Zhang, Yulong and Chen, Ying and Chen, Haofeng},
  year = {2023},
  month = aug,
  journal = {Journal of Energy Storage},
  volume = {66},
  pages = {107451},
  issn = {2352-152X},
  doi = {10.1016/j.est.2023.107451},
  url = {https://www.sciencedirect.com/science/article/pii/S2352152X23008484},
  urldate = {2023-08-09},
  abstract = {Li-ion batteries are widely used as a substitute for fossil energy in various fields. However, fire and explosion accidents caused by thermal runaway have attracted widespread attention. Studies have shown that lithium plating of Li-ion batteries during low-temperature aging can seriously affect their thermal stability. Still, there is no corresponding mathematical model to analyze and predict the thermal runaway behavior of batteries during the whole life cycle after low-temperature aging. In this paper, the evolution of thermal runaway behaviors and kinetics caused by the low-temperature aging is analyzed by the stage division and analysis of the thermal runaway process based on the adiabatic thermal runaway tests. The results show that the low-temperature aging significantly affects the thermal kinetics of the battery and accelerates the thermal runaway process. By investigating several groups of experimental results, a prediction method for the kinetic parameters of side reactions of batteries with arbitrary state of health (SOH) is proposed, and a numerical model for the prediction of thermal runaway during the whole life cycle is established based on the reaction kinetics. Furthermore, the proposed model is experimentally verified to accurately predict the thermal runaway behaviors of Li-ion batteries during the whole life cycle after low-temperature aging.},
  langid = {english},
  keywords = {battery,battFire},
  note = {Batteries aged in low temperatures are more vulnerable to thermal runaway. ~This paper makes a physical TR prediction model that works across full range of battery health, esp. after low temp aging.},
  file = {C\:\\Users\\scott\\tmp\\zot_mdnote_export\\Wu23ThermalRunawayBehaviors - Batteries aged in low temperatures are more vulnerable to thermal runaway. This paper makes a physical TR prediction mo.md;C\:\\Users\\scott\\tmp\\zot_mdnote_export\\Wu23ThermalRunawayBehaviors-zotero.md;C\:\\Users\\scott\\tmp\\zot_mdnote_export\\Wu23ThermalRunawayBehaviors.md}
}

@article{Yeardley20StudyThermalRunaway,
  title = {A Study of the Thermal Runaway of Lithium-Ion Batteries: {{A Gaussian Process}} Based Global Sensitivity Analysis},
  shorttitle = {A Study of the Thermal Runaway of Lithium-Ion Batteries},
  author = {Yeardley, Aaron S. and Bugryniec, Peter J. and Milton, Robert A. and Brown, Solomon F.},
  year = {2020},
  month = apr,
  journal = {Journal of Power Sources},
  volume = {456},
  pages = {228001},
  issn = {0378-7753},
  doi = {10.1016/j.jpowsour.2020.228001},
  url = {https://www.sciencedirect.com/science/article/pii/S0378775320303049},
  urldate = {2023-08-09},
  abstract = {A particular safety issue with Lithium-ion (Li-ion) cells is thermal runaway (TR), which is the exothermic decomposition of cell components creating an uncontrollable temperature rise leading to fires and explosions. The modelling of TR is difficult due to the broad range of cell properties and potential conditions. Understanding the effect that thermo-physical and heat transfer characteristics have on the TR abuse model output is essential to develop more accurate and robust TR models. This study uses global sensitivity analysis (GSA) to investigate the effect of the cell parameters on the outcome of TR events. Using a Gaussian Process (GP) surrogate model to calculate the Sobol' indices, it is shown that the emissivity value is the dominant thermo-characteristic throughout the overall abuse scenario. Further analysis, investigating three key TR features shows the conductivity coefficient to be the most important with respect to the maximum temperature reached during TR. Results demonstrate that researchers can confidently estimate some thermo-characteristics but require accurate characterisation of the emissivity and conductivity coefficient to ensure robust predictions. Given the importance of battery technology to aid in global de-carbonisation, these findings are key to increasing their safe design and operation.},
  langid = {english},
  keywords = {battery,battFire,intro},
  note = {Good estimates of Emissivity and Conductivity are required to predict the max temp during thermal runaway.
\par
(Zhai et al., 2021) uses location and temperature only, I think.
\par
Decision tree temp predictor: (Tran et al., 2022)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Yeardley20StudyThermalRunaway2.pdf}
}

@article{Shah16ExperimentalTheoreticalAnalysis,
  title = {Experimental and Theoretical Analysis of a Method to Predict Thermal Runaway in {{Li-ion}} Cells},
  author = {Shah, Krishna and Chalise, Divya and Jain, Ankur},
  year = {2016},
  month = oct,
  journal = {Journal of Power Sources},
  volume = {330},
  pages = {167--174},
  issn = {0378-7753},
  doi = {10.1016/j.jpowsour.2016.08.133},
  url = {https://www.sciencedirect.com/science/article/pii/S0378775316311533},
  urldate = {2023-08-09},
  abstract = {Thermal runaway is a well-known safety concern in Li-ion cells. Methods to predict and prevent thermal runaway are critically needed for enhanced safety and performance. While much work has been done on understanding the kinetics of various heat generation processes during thermal runaway, relatively lesser work exists on understanding how heat removal from the cell influences thermal runaway. Through a unified analysis of heat generation and heat removal, this paper derives and experimentally validates a non-dimensional parameter whose value governs whether or not thermal runaway will occur in a Li-ion cell. This parameter is named the Thermal Runaway Number (TRN), and comprises contributions from thermal transport within and outside the cell, as well as the temperature dependence of heat generation rate. Experimental data using a 26650 thermal test cell are in good agreement with the model, and demonstrate the dependence of thermal runaway on various thermal transport and heat generation parameters. This parameter is used to predict the thermal design space in which the cell will or will not experience thermal runaway. By combining all thermal processes contributing to thermal runaway in a single parameter, this work contributes towards a unified understanding of thermal runaway, and provides the fundamental basis for design tools for safe, high-performance Li-ion batteries.},
  langid = {english},
  keywords = {battery,battFire},
  note = {Computes a scalar Thermal Runaway Number (TRN) which determines whether or not runaway will occur. ~Could be used in a threshold method, as in (Baru and Johnson, 2023)
\par
Other similar scalars:
\par
(Shah et al., 2016; Hell and Kim, 2022)
\par
Other scalar: (Lyu et al., 2022)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Shah16ExperimentalTheoreticalAnalysis2.pdf}
}

@article{Ouyang23DatadrivenMethodPredicting,
  title = {A Data-Driven Method for Predicting Thermal Runaway Propagation of Battery Modules Considering Uncertain Conditions},
  author = {Ouyang, Nan and Zhang, Wencan and Yin, Xiuxing and Li, Xingyao and Xie, Yi and He, Hancheng and Long, Zhuoru},
  year = {2023},
  month = jun,
  journal = {Energy},
  volume = {273},
  pages = {127168},
  issn = {0360-5442},
  doi = {10.1016/j.energy.2023.127168},
  url = {https://www.sciencedirect.com/science/article/pii/S0360544223005625},
  urldate = {2023-08-09},
  abstract = {Thermal Runaway Propagation (TRP) of lithium-ion battery packs has serious hazards. However, the TRP prediction is challenging because of the substantial uncertainty and hard-to-acquire data. To solve this problem, a fuzzy system and multi-task CNN-LSTM method are proposed to predict TRP multiple steps ahead. The TRP dataset is constructed by 25 sets of experiments and 130 sets of simulations. The uncertain SoC, charging and discharging conditions, and thermal runaway (TR) trigger points are considered in both experiments and simulations. Then, the fuzzy system is introduced to reason about the TR probability of the battery and optimized by a sparrow search algorithm (SSA). A multi-task CNN-LSTM model is proposed to extract fuzzy and physical information by employing a convolutional neural network (CNN) and multiple long short-term memory (LSTM) neural networks, respectively, and output the temperature of multiple cells simultaneously. Finally, the models are evaluated in the simulation and experimental validation sets with different window lengths and time resolutions. The results show that the fuzzy information significantly improves the prediction accuracy of the method, with a coefficient of determination (R2) of 98.48\% for the 3s prediction horizon and 97.27\% for the 18s prediction horizon in the experimental validation set.},
  langid = {english},
  keywords = {battery,battFire},
  note = {Predicts thermal runaway ``multiple steps ahead'' (horiz=3 s: 98.5\% acc; h=18s: 97.3\%) using fuzzy multitask CNN-LSTM. ~Are they really using ``accuracy?'' ~What kind of thershold tuning did they do? ~Required a bunch of simulations, physical, I guess.}
}

@article{Zhang23UnderstandingThermalRunaway,
  title = {Understanding of Thermal Runaway Mechanism of {{LiFePO4}} Battery In-Depth by Three-Level Analysis},
  author = {Zhang, Yue and Cheng, Siyuan and Mei, Wenxin and Jiang, Lihua and Jia, Zhuangzhuang and Cheng, Zhixiang and Sun, Jinhua and Wang, Qingsong},
  year = {2023},
  month = apr,
  journal = {Applied Energy},
  volume = {336},
  pages = {120695},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2023.120695},
  url = {https://www.sciencedirect.com/science/article/pii/S0306261923000594},
  urldate = {2023-08-09},
  abstract = {The complex chemical composition and material interactions of lithium-ion batteries challenge the in-depth understanding of thermal runaway reactions and failure mechanisms. In this study, detailed analysis and implementation have been made from three levels to further explain the thermal failure mechanism, from material interactions to cell-level experiments and applications. The LiFePO4 thermal runaway mechanism is put forward to characterize exothermic peaks from differential analysis of differential scanning calorimetry (DSC) and Accelerating Rate Calorimetry (ARC) data. Furthermore, the development, parameterization, and application of the thermal runaway prediction model are also discussed. Multi-heating rate data is a prerequisite to kinetic analysis and modeling work and provides valuable data set for LiFePO4 thermal failure. And the unraveled mechanism is believed to provide a profound understanding of the thermal failure mechanism, strengthening interactions between material characterization and thermal runaway modeling.},
  langid = {english},
  keywords = {battery,battFire},
  note = {Detailed physical explanation of thermal runaway. ~Over my head, but a good cite? ~Would (Tang et al., 2021) be a good substitute?},
  file = {C\:\\Users\\scott\\tmp\\zot_mdnote_export\\Zhang23UnderstandingThermalRunaway - Detailed physical explanation of thermal runaway. Over my head, but a good cite Would (Tang et al., 2021) be a good s.md;C\:\\Users\\scott\\tmp\\zot_mdnote_export\\Zhang23UnderstandingThermalRunaway-zotero.md;C\:\\Users\\scott\\tmp\\zot_mdnote_export\\Zhang23UnderstandingThermalRunaway.md}
}

@article{Tang21InvestigatingCriticalCharacteristics,
  title = {Investigating the Critical Characteristics of Thermal Runaway Process for {{LiFePO4}}/Graphite Batteries by a Ceased Segmented Method},
  author = {Tang, Xuan and Zhang, Guangxu and Wang, Xueyuan and Wei, Gang and Han, Guangshuai and Zhu, Jiangong and Wei, Xuezhe and Dai, Haifeng},
  year = {2021},
  month = oct,
  journal = {iScience},
  volume = {24},
  number = {10},
  pages = {103088},
  issn = {2589-0042},
  doi = {10.1016/j.isci.2021.103088},
  url = {https://www.sciencedirect.com/science/article/pii/S2589004221010567},
  urldate = {2023-08-09},
  abstract = {Lithium-ion batteries (LIBs) are widely used as the energy carrier in our daily life. However, the higher energy density of LIBs results in poor safety performance. Thermal runaway (TR) is the critical problem which hinders the further application of LIBs. Clarifying the mechanism of TR evolution is beneficial to safer cell design and safety management. In this paper, liquid nitrogen spray is proved to be an effective way to stop the violent reaction of LIBs during the TR process. Based on extended-volume accelerating rate calorimetry, the liquid nitrogen ceasing combined with non-atmospheric exposure analysis is used to investigate the TR evolution about LiFePO4/graphite batteries at critical temperature. Specifically, the geometrical shape, voltage, and impedance change are monitored during the TR process on the cell level. The morphologies/constitution of electrodes and separators are presented on the component level. Utilizing the gas analysis, the failure mechanism of the prismatic LiFePO4/graphite battery is studied comprehensively.},
  langid = {english},
  keywords = {battery,battFire,intro},
  note = {Physical study of thermal runaway. ~Maybe I could understaqnd this instead of (Zhang et al., 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Tang21InvestigatingCriticalCharacteristics2.pdf}
}

@article{Tran22ReviewLithiumIonBattery,
  title = {A {{Review}} of {{Lithium-Ion Battery Thermal Runaway Modeling}} and {{Diagnosis Approaches}}},
  author = {Tran, Manh-Kien and Mevawalla, Anosh and Aziz, Attar and Panchal, Satyam and Xie, Yi and Fowler, Michael},
  year = {2022},
  month = jun,
  journal = {Processes},
  volume = {10},
  number = {6},
  pages = {1192},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-9717},
  doi = {10.3390/pr10061192},
  url = {https://www.mdpi.com/2227-9717/10/6/1192},
  urldate = {2023-08-09},
  abstract = {Lithium-ion (Li-ion) batteries have been utilized increasingly in recent years in various applications, such as electric vehicles (EVs), electronics, and large energy storage systems due to their long lifespan, high energy density, and high-power density, among other qualities. However, there can be faults that occur internally or externally that affect battery performance which can potentially lead to serious safety concerns, such as thermal runaway. Thermal runaway is a major challenge in the Li-ion battery field due to its uncontrollable and irreversible nature, which can lead to fires and explosions, threatening the safety of the public. Therefore, thermal runaway prognosis and diagnosis are significant topics of research. To efficiently study and develop thermal runaway prognosis and diagnosis algorithms, thermal runaway modeling is also important. Li-ion battery thermal runaway modeling, prediction, and detection can help in the development of prevention and mitigation approaches to ensure the safety of the battery system. This paper provides a comprehensive review of Li-ion battery thermal runaway modeling. Various prognostic and diagnostic approaches for thermal runaway are also discussed.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {battery,battFire,intro},
  note = {A review. ~Compare with / choose between (Liao et al., 2019)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Tran22ReviewLithiumIonBattery.pdf}
}

@article{Jindal21CoupledElectrochemicalabuseheattransferModel,
  title = {Coupled Electrochemical-Abuse-Heat-Transfer Model to Predict Thermal Runaway Propagation and Mitigation Strategy for an {{EV}} Battery Module},
  author = {Jindal, Puneet and Kumar, Banoth Sravan and Bhattacharya, Jishnu},
  year = {2021},
  month = jul,
  journal = {Journal of Energy Storage},
  volume = {39},
  pages = {102619},
  issn = {2352-152X},
  doi = {10.1016/j.est.2021.102619},
  url = {https://www.sciencedirect.com/science/article/pii/S2352152X21003613},
  urldate = {2023-08-09},
  abstract = {Safe and reliable design of lithium-ion battery packs necessitates prevention of thermal runaway and its propagation. To characterize the propagation behaviour in a battery module, a three-mode heat transfer model coupled with electrochemical and abuse-reaction-kinetics models is developed here. The efficacy of an active thermal management system in preventing damage-cascade for a standard EV module geometry (as used in Tesla) is elucidated. The model effectively captures the temperature profile, heat generation rate, and the extent of resulting decomposition of cell-components as validated against the literature. A temperature spike is applied to a cell within a standard battery module to emulate abuse driven runaway. The propagation of damage to other cells due to the temperature-spike in the triggered cell is simulated through the model as developed. A counter-intuitive non-monotonic trend (increasing first, decreasing later) is observed for the damage-severity as a function of coolant flow rate. Through a unified analysis of cumulative heat generation and heat dissipation through coolant, a mitigation strategy for runaway propagation is obtained in terms of a generalized non-dimensional parameter, Runway Mitigation Number (RMN), which predicts runaway propagation beyond a value of {\textasciitilde}1.8 for similar module designs.},
  langid = {english},
  keywords = {battery,battFire},
  note = {Predicts a scalar Runaway Mitigation Number. ~If it's bigger than -1.8, ~then a runaway is predicted. ~Says Tesla's actively thermal management in EVs produces counterintuitive temperature profile.
\par
Other scalar: (Lyu et al., 2022)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Jindal21CoupledElectrochemicalabuseheattransferModel2.pdf}
}

@article{Zhang23CriticalReviewThermal,
  title = {A {{Critical Review}} of {{Thermal Runaway Prediction}} and {{Early-Warning Methods}} for {{Lithium-Ion Batteries}}},
  author = {Zhang, Xi and Chen, Shun and Zhu, Jingzhe and Gao, Yizhao},
  year = {2023},
  month = jan,
  journal = {Energy Material Advances},
  volume = {4},
  pages = {0008},
  publisher = {American Association for the Advancement of Science},
  doi = {10.34133/energymatadv.0008},
  url = {https://spj.science.org/doi/full/10.34133/energymatadv.0008},
  urldate = {2023-08-09},
  abstract = {Lithium-ion batteries are widely used in electric vehicles because of their high energy density and long cycle life. However, the spontaneous combustion accident of electric vehicles caused by thermal runaway of lithium-ion batteries seriously threatens passengers' personal and property safety. This paper expounds on the internal mechanism of lithium-ion battery thermal runaway through many previous studies and summarizes the proposed lithium-ion battery thermal runaway prediction and early warning methods. These methods can be classified into battery electrochemistry-based, battery big data analysis, and artificial intelligence methods. In this paper, various lithium-ion thermal runaway prediction and early warning methods are analyzed in detail, including the advantages and disadvantages of each method, and the challenges and future development directions of the intelligent lithium-ion battery thermal runaway prediction and early warning methods are discussed.},
  keywords = {battery,battFire,intro,priorityRead},
  note = {The newest Survey I've found. ~Compare with (Liao et al., 2019; Diaz et al., 2020; Tran et al., 2022; Shahid and Agelin-Chaab, 2022; Fioravanti et al., 2020)
\par
(Tang et al., 2021)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhang23CriticalReviewThermal.pdf}
}

@article{Kim23ModelingPredictionLithiumion,
  title = {Modeling and Prediction of Lithium-Ion Battery Thermal Runaway via Multiphysics-Informed Neural Network},
  author = {Kim, Sung Wook and Kwak, Eunji and Kim, Jun-Hyeong and Oh, Ki-Yong and Lee, Seungchul},
  year = {2023},
  month = apr,
  journal = {Journal of Energy Storage},
  volume = {60},
  pages = {106654},
  issn = {2352-152X},
  doi = {10.1016/j.est.2023.106654},
  url = {https://www.sciencedirect.com/science/article/pii/S2352152X23000518},
  urldate = {2023-08-09},
  abstract = {In this study, a multiphysics-informed neural network (MPINN) is proposed for the estimation and prediction of thermal runaway (TR) in lithium-ion batteries (LIBs). MPINNs are encoded with the governing laws of physics, including the energy balance equation and Arrhenius law, ensuring accurate estimation of time and space-dependent temperature and dimensionless concentration in comparison to a purely data-driven approach. Specifically, the network is trained using data from a high-fidelity model of an LIB, which describes TR by addressing several coupled partial differential equations. Quantitative analysis reveals that the mean absolute error (MAE) and root mean squared error (RMSE) of the MPINN for TR estimation are less than an artificial neural network (ANN) by 0.71 and 1.57, respectively, when using fully labeled data for training. It outperforms the ANN in terms of MAE and RMSE by 90.56 and 118.64, when only a small portion of labeled data (semi-supervision) are used for TR prediction. Importantly, it predicts TR without any labeled data when the decomposition of reactive species is modeled in the positive electrode. The MPINN exhibits promising results in surrogate modeling, implying it can be successfully implemented in practical scenarios and stimulate further research related to TR modeling using physics-informed deep learning.},
  langid = {english},
  keywords = {battery,battFire},
  note = {NN/physical model of TR. ~Is it single cell or a full battery pack? ~ (Meng et al., 2023) does something similar w/ DBN},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kim23ModelingPredictionLithiumion.pdf}
}

@article{Meng23IntegratedMethodologyDynamic,
  title = {An Integrated Methodology for Dynamic Risk Prediction of Thermal Runaway in Lithium-Ion Batteries},
  author = {Meng, Huixing and Yang, Qiaoqiao and Zio, Enrico and Xing, Jinduo},
  year = {2023},
  month = mar,
  journal = {Process Safety and Environmental Protection},
  volume = {171},
  pages = {385--395},
  issn = {0957-5820},
  doi = {10.1016/j.psep.2023.01.021},
  url = {https://www.sciencedirect.com/science/article/pii/S0957582023000228},
  urldate = {2023-08-09},
  abstract = {The risk of thermal runaway in lithium-ion battery (LIB) attracts significant attention from domains of society, industry, and academia. However, the thermal runaway prediction in the framework of system safety requires further efforts. In this paper, we propose a methodology for dynamic risk prediction by integrating fault tree (FT), dynamic Bayesian network (DBN) and support vector regression (SVR). FT graphically describes the logic of mechanism of thermal runaway. DBN allows considering multiple states and uncertain inference for providing quantitative results of the risk evolution. SVR is subsequently utilized for predicting the risk from the DBN estimation. The proposed methodology can be applied for risk early warning of LIB thermal runaway.},
  langid = {english},
  keywords = {battery,battFire},
  note = {Dynamic Bayesian Net models logic of thermal runaway. ~(Kim et al., 2023) does something similar w/ physics NN.}
}

@article{Hell22DevelopmentDataDrivenMethod,
  title = {Development of a {{Data-Driven Method}} for {{Online Battery Remaining-Useful-Life Prediction}}},
  author = {Hell, Sebastian Matthias and Kim, Chong Dae},
  year = {2022},
  month = oct,
  journal = {Batteries},
  volume = {8},
  number = {10},
  pages = {192},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2313-0105},
  doi = {10.3390/batteries8100192},
  url = {https://www.mdpi.com/2313-0105/8/10/192},
  urldate = {2023-08-09},
  abstract = {Remaining-useful-life (RUL) prediction of Li-ion batteries is used to provide an early indication of the expected lifetime of the battery, thereby reducing the risk of failure and increasing safety. In this paper, a detailed method is presented to make long-term predictions for the RUL based on a combination of gated recurrent unit neural network (GRU NN) and soft-sensing method. Firstly, an indirect health indicator (HI) was extracted from the charging processes using a soft-sensing method that can accurately describe power degradation instead of capacity. Then, a GRU NN with a sliding window was applied to learn the long-term performance development. The method also uses a dropout and early stopping method to prevent overfitting. To build the models and validate the effectiveness of the proposed method, a real-world NASA battery data set with various battery measurements was used. The results show that the method can produce a long-term and accurate RUL prediction at each position of the degradation progression based on several historical battery data sets.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {battery,battFire},
  note = {Estimates a scalar indirect ``health indicator'' HI and uses it to predict remaining useful life. ~Similar to ideas for thermal runaway: (Jindal et al., 2021; Shah et al., 2016)
\par
Other scalar: (Lyu et al., 2022)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hell22DevelopmentDataDrivenMethod.pdf}
}

@article{Zhai21ExperimentalbasedDominoPrediction,
  title = {An Experimental-Based {{Domino}} Prediction Model of Thermal Runaway Propagation in 18,650 Lithium-Ion Battery Modules},
  author = {Zhai, Hongju and Li, Huang and Ping, Ping and Huang, Zonghou and Wang, Qingsong},
  year = {2021},
  month = dec,
  journal = {International Journal of Heat and Mass Transfer},
  volume = {181},
  pages = {122024},
  issn = {0017-9310},
  doi = {10.1016/j.ijheatmasstransfer.2021.122024},
  url = {https://www.sciencedirect.com/science/article/pii/S0017931021011303},
  urldate = {2023-08-09},
  abstract = {Currently, the thermal safety issue of lithium-ion battery (LIB) has become a major challenge to restrict its development. In this work, the thermal runaway propagation (TRP) process of the 18,650-type LIB module is studied by experimental and modeling methods. A novel experimental-based Domino prediction model is proposed, which can predict the TRP path and its probability. The calculation part of the model is realized with the Matlab software. This model for the first time proposes that whether the battery thermal runaway (TR) is a probability event, and the probability is a function of its temperature. To verify the feasibility of the model, the TRP process in a 4~{\texttimes}~4 arrangement battery module with three different first TR battery locations is detailed analyzed. The results show that the dangerous level ranking of cell locations from low to high is the corner location, the edge location, and the location near the module center. Higher dangerous level means more maximum probability TRP paths and higher probability. Moreover, it was found that the whole TRP process can be divided into four stages: the TRP trigger stage, the heat accumulation stage, the Domino effect stage, and the TRP stop stage. The proposed model can effectively predict the TRP process in modules, and the results have important reference value for the design of the battery thermal management system and the research on the method of blocking TRP.},
  langid = {english},
  keywords = {battery,battFire},
  note = {probability of thermal runaway for a 4x4 cell battery. ~Considers spatial location. ~As recommended in: (Yeardley et al., 2020)
\par
Decision tree temp predictor: (Tran et al., 2022)}
}

@article{Lyu22RealTimeOverchargeWarning,
  title = {Real-{{Time Overcharge Warning}} and {{Early Thermal Runaway Prediction}} of {{Li-Ion Battery}} by {{Online Impedance Measurement}}},
  author = {Lyu, Nawei and Jin, Yang and Xiong, Rui and Miao, Shan and Gao, Jinfeng},
  year = {2022},
  month = feb,
  journal = {IEEE Transactions on Industrial Electronics},
  volume = {69},
  number = {2},
  pages = {1929--1936},
  issn = {1557-9948},
  doi = {10.1109/TIE.2021.3062267},
  abstract = {A lithium-ion battery has advantages such as high energy density and long calendar life, but it suffers from the risk of thermal runaway. Overcharge-induced thermal runaway accidents hold a considerable percentage. This article discovers that the slope of the dynamic impedance in the frequency band of 30--90 Hz turns positive from negative when the cell just starts to overcharge and proposes the theoretical explanation. Taking 70 Hz impedance as an example, the thermal runaway accident can be successfully avoided by cutting off the charging when the slope turns positive from negative during charging. The warning time is 580 s ahead of the thermal runaway. This feature is easy to identify and requires no complex mathematical models and parameters. Besides, the prediction method based on this feature can be conducted by using an online dynamic impedance measurement device designed by us, which is suitable for large-scale applications. Thus, the overcharge-induced thermal runaway accidents can be avoided.},
  keywords = {battery,battFire},
  note = {impedance in 30-90 Hz predicts thermal runaway 580s ahead of time. ~
\par
Other scalar predictors: (Jindal et al., 2021; Hell and Kim, 2022; Shah, 2016)
\par
Decision tree temp predictor: (Tran et al., 2022)
\par
Also uses spectrum: (Khaleghi et al., 2019)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lyu22RealTimeOverchargeWarning2.pdf}
}

@article{Chen18TemperatureRisePrediction,
  title = {Temperature Rise Prediction of Lithium-Ion Battery Suffering External Short Circuit for All-Climate Electric Vehicles Application},
  author = {Chen, Zeyu and Xiong, Rui and Lu, Jiahuan and Li, Xinggang},
  year = {2018},
  month = mar,
  journal = {Applied Energy},
  volume = {213},
  pages = {375--383},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2018.01.068},
  url = {https://www.sciencedirect.com/science/article/pii/S0306261918300771},
  urldate = {2023-08-09},
  abstract = {External short circuit (ESC) is a severe fault that can cause the large current and high temperature of lithium-ion batteries (LiBs) immediately. Temperature rise prediction is crucial for LiB safety management in an all-climate electric vehicles application because many disastrous consequences are caused by high temperature. This study mainly investigates the ESC-caused temperature rise characteristics of LiB, and proposes an online prediction approach of the maximum temperature rise. Three original contributions are made: (1) Abusing tests of LiBs under ESC are conducted at varying ambient temperatures, and the influences of battery state of charge (SOC) and ambient temperature on the maximum temperature rise are revealed. (2) Characteristics of temperature rises are analysed, therein finding that the heat generation of LiBs caused by ESC presents two modes: Joule heat-dominant mode and reaction heat/Joule heat blended mode; leakage is an external manifestation of the latter. (3) Two heat generation modes are proved to be linearly separable at temperature rise discharge capacity plane, and then a two-step prediction approach of maximum temperature rise is proposed based on support vector machine. Finally, the presented approach is validated by the experimental data. The maximum temperature rise can be predicted up to 22.3\,s ahead of time and very precise prediction results are obtained, where the mean prediction error for the eight test cells is 3.05\%.},
  langid = {english},
  keywords = {battery,battFire},
  note = {linearly separable heating effects feed a two-step max temp rise prediction prediction. ~can be predicted 22.3 s ahead of time, w/ a 3.05\% error rate.
\par
Also uses temperature: (Chen et al., 2021; Zhai et al., 2021; Zhang et al., 2022)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chen18TemperatureRisePrediction2.pdf}
}

@article{Tran22PythonbasedScikitlearnMachine,
  title = {Python-Based Scikit-Learn Machine Learning Models for Thermal and Electrical Performance Prediction of High-Capacity Lithium-Ion Battery},
  author = {Tran, Manh-Kien and Panchal, Satyam and Chauhan, Vedang and Brahmbhatt, Niku and Mevawalla, Anosh and Fraser, Roydon and Fowler, Michael},
  year = {2022},
  journal = {International Journal of Energy Research},
  volume = {46},
  number = {2},
  pages = {786--794},
  issn = {1099-114X},
  doi = {10.1002/er.7202},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/er.7202},
  urldate = {2023-08-09},
  abstract = {With the increasing popularity of electric vehicles (EVs), the demands for rechargeable and high-performance batteries like lithium-ion (Li-ion) batteries have soared. Li-ion battery systems require the use of a battery management system (BMS) to perform safely and efficiently. Accurate and reliable battery modeling is important for the BMS to function properly. Currently, many BMS applications use the equivalent circuit model due to its simplicity. However, with the development of a cloud BMS, machine learning battery models can be utilized, which can potentially improve the accuracy and reliability of the BMS. This work investigates the performance of four different machine learning models used to predict the thermal (temperature) and electrical (voltage) behaviors of Li-ion battery cells. A prismatic Li-ion battery cell with a capacity of 25 Ah was cycled under a constant current profile at three different ambient temperatures, and the surface temperature and voltage of the battery were measured. The four machine learning regression models---linear regression, k-nearest neighbors, random forest, and decision tree---were developed using the scikit-learn library in Python and validated with experimental data. The results of their performance were reported and compared using the R2 metric. The decision tree-based model, with an R2 score of 0.99, was determined to be the best model in this case study.},
  copyright = {{\copyright} 2021 John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {battery,battFire,priorityRead},
  note = {ML methods predicting temp and voltage in python. ~Decision tree best.
\par
How related to DBN, as in (Meng et al., 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Tran22PythonbasedScikitlearnMachine2.pdf}
}

@article{Sohn22TwostageDeepLearning,
  title = {Two-Stage Deep Learning for Online Prediction of Knee-Point in {{Li-ion}} Battery Capacity Degradation},
  author = {Sohn, Suyeon and Byun, Ha-Eun and Lee, Jay H.},
  year = {2022},
  month = dec,
  journal = {Applied Energy},
  volume = {328},
  pages = {120204},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2022.120204},
  url = {https://www.sciencedirect.com/science/article/pii/S0306261922014611},
  urldate = {2023-08-11},
  abstract = {Accurate monitoring of capacity degradation of a lithium-ion battery is important as it enables the user to manage the battery usage for optimal performance/lifetime and to take preemptive measures against any potential explosion or fire. Battery capacity fades gradually through repetitive charging and discharging until it reaches the so called `knee-point', after which it goes through rapid and irreversible deterioration to reach its end-of-life. It is crucial to forecast the knee-point early and accurately for safety and economic use of the battery. Machine learning based methods have been used to predict the knee-point with early cycles cell data. Despite some notable progress made, the existing methods make the unrealistic assumption of constant cycle-to-cycle charge/discharge operation. In this study, a novel two-stage deep learning method is proposed for online knee-point prediction under variable battery usage. A CNN-based model extracts temporal features across past and current cycles to sort out those that should be monitored closely for near-term failures, and then predict the number of cycles left to reach the knee-point for them. The proposed method extracts features from time-series data and thus reflects dynamic changes in battery properties, resulting in improved prediction performance under realistic scenarios.},
  langid = {english},
  keywords = {battSOH}
}

@article{Gu23TimeseriesWassersteinGAN,
  title = {A Time-Series {{Wasserstein GAN}} Method for State-of-Charge Estimation of Lithium-Ion Batteries},
  author = {Gu, Xinyu and See, K. W. and Liu, Yanbin and Arshad, Bilal and Zhao, Liang and Wang, Yunpeng},
  year = {2023},
  month = oct,
  journal = {Journal of Power Sources},
  volume = {581},
  pages = {233472},
  issn = {0378-7753},
  doi = {10.1016/j.jpowsour.2023.233472},
  url = {https://www.sciencedirect.com/science/article/pii/S0378775323008480},
  urldate = {2023-08-11},
  abstract = {Estimating the state-of-charge (SOC) of lithium-ion batteries is essential for maintaining secure and reliable battery operation while minimizing long-term service and maintenance expenses. In this work, we present a novel Time-Series Wasserstein Generative Adversarial Network (TS-WGAN) approach for SOC estimation of lithium-ion batteries, characterized by a well-designed data preprocessing process and a distinctive WGAN-GP architecture. In the data preprocessing stage, we employ the Pearson correlation coefficient (PCC) to identify strongly associated features and apply feature scaling techniques for data normalization. Moreover, we leverage polynomial regression to expand the original features and utilize principal component analysis (PCA) to reduce the computational load and retain essential information by projecting features into a lower-dimensional subspace. Within the WGAN-GP architecture, we originally devise a Transformer as the generator and a Convolution Neural Network (CNN) as the critic to make the most of local (CNN) and global (Transformer) variables. The overall model is trained with the WGAN, incorporating gradient penalty loss for training purposes. Simulation outcomes using real-road dataset and laboratory dataset reveal that TS-WGAN surpasses all baseline methods with enhanced accuracy, stability, and robustness. The coefficient of determination (R2) for both datasets exceeds 99.50\%, demonstrating its potential for practical application.},
  langid = {english},
  keywords = {battSOC,battSOH},
  note = {SOC definition same as (Park et al., 2020) ?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gu23TimeseriesWassersteinGAN.pdf}
}

@article{Qu22AdaptiveNoiseReduction,
  title = {An {{Adaptive Noise Reduction Approach}} for {{Remaining Useful Life Prediction}} of {{Lithium-Ion Batteries}}},
  author = {Qu, Wenyu and Chen, Guici and Zhang, Tingting},
  year = {2022},
  month = jan,
  journal = {Energies},
  volume = {15},
  number = {19},
  pages = {7422},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1996-1073},
  doi = {10.3390/en15197422},
  url = {https://www.mdpi.com/1996-1073/15/19/7422},
  urldate = {2023-08-11},
  abstract = {Lithium-ion batteries are widely used in the electric vehicle industry due to their recyclability and long life. However, a failure of lithium-ion batteries can cause some catastrophic accidents, such as electric car battery explosion fires and so on. To prevent such harm from occurring, it is essential to monitor the remaining useful life of lithium-ion batteries and give early warning. In this paper, an adaptive noise reduction approach is proposed to predict the RUL (Remaining Useful Life) of lithium-ion batteries, which uses CEEMDAN (Complete Ensemble Empirical Mode Decomposition with Adaptive Noise) combined with wavelet decomposition to achieve adaptive noise reduction decomposition, and then inputs the obtained IMF (Intrinsic Mode Function) components into LS--RVM (Least Square Relevance Vector Machine) for training, prediction, and reconstruction, so as to achieve high-precision prediction of RUL. Moreover, in order to verify the validity of the model, the model in this paper is compared with other common models. The results demonstrate that the RMSE, MAPE, and MAE of the proposed model are 0.008678, 0.005002, and 0.006894, and that it has higher accuracy than the other common prediction models.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {battSOH},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Qu22AdaptiveNoiseReduction.pdf}
}

@article{Li22BatteryThermalRunaway,
  title = {Battery {{Thermal Runaway Fault Prognosis}} in {{Electric Vehicles Based}} on {{Abnormal Heat Generation}} and {{Deep Learning Algorithms}}},
  author = {Li, Da and Liu, Peng and Zhang, Zhaosheng and Zhang, Lei and Deng, Junjun and Wang, Zhenpo and Dorrell, David G. and Li, Weihan and Sauer, Dirk Uwe},
  year = {2022},
  month = jul,
  journal = {IEEE Transactions on Power Electronics},
  volume = {37},
  number = {7},
  pages = {8513--8525},
  issn = {1941-0107},
  doi = {10.1109/TPEL.2022.3150026},
  abstract = {Efficient battery thermal runaway prognosis is of great importance for ensuring safe operation of electric vehicles (EVs). This presents formidable challenges under widely varied and ever-changing driving conditions in real-world vehicular operations. In this article, an enabling thermal runaway prognosis model based on abnormal heat generation (AHG) is proposed by combining the long short-term memory neural network (LSTM) and the convolutional neural network (CNN). The memory cell of the LSTM is modified and the resultant modified LSTM-CNN serves to provide accurate battery temperature prediction. The principal component analysis is used to optimize the model input factors to improve prediction accuracy and to reduce computing time. A random adjacent optimization method is employed to automatically optimize the hyperparameters. Finally, a model-based scheme is presented to achieve AHG-based thermal runaway prognosis. Real-world EV operating data are used to verify the effectiveness and robustness of the proposed scheme. The verification results indicate that the presented scheme exhibits accurate 48-time-step battery temperature prediction with a mean-relative-error of 0.28\% and can realize 27-min-ahead thermal runaway prognosis.},
  keywords = {battFire}
}

@article{Shahid22ReviewThermalRunawaya,
  title = {A Review of Thermal Runaway Prevention and Mitigation Strategies for Lithium-Ion Batteries},
  author = {Shahid, Seham and {Agelin-Chaab}, Martin},
  year = {2022},
  month = dec,
  journal = {Energy Conversion and Management: X},
  volume = {16},
  pages = {100310},
  issn = {2590-1745},
  doi = {10.1016/j.ecmx.2022.100310},
  url = {https://www.sciencedirect.com/science/article/pii/S2590174522001337},
  urldate = {2023-08-11},
  abstract = {Lithium-ion batteries are widely considered the leading candidate energy source for powering electric vehicles due to their high energy and power densities. The thermal runaway of lithium-ion batteries is the phenomenon of chain exothermic reactions within the battery. These reactions cause a sharp rise in the internal battery temperature causing the inner structures of the battery to destabilize and degrade, which eventually leads to the failure of the battery. This paper provides a comprehensive review of the key aspects of the thermal runaway processes, which consists of thermal runaway initiation mechanisms, thermal runaway propagation, and the characterization of vented gases during the thermal runaway process. Thermal runaway is a major safety concern; therefore, the development of mathematical and numerical models to predict thermal runaway is reviewed, which provides useful data to design and develop battery packs with thermal runaway safety features. Furthermore, the development of effective battery thermal management systems is discussed, which is essential to prevent thermal runaway initiation. Finally, mitigation strategies are reviewed, which are developed to contain and minimize damages when thermal runaway occurs.},
  langid = {english},
  keywords = {battery,battFire,intro,priorityRead},
  note = {\textbf{Contents}
\par
\begin{itemize}

\item \href{zotero://open-pdf/0_N8CS9ZX2/1}{1 Introduction}
\item \href{zotero://open-pdf/0_N8CS9ZX2/3}{2 Thermal runaway characterization}

\begin{itemize}

\item \href{zotero://open-pdf/0_N8CS9ZX2/3}{2.1 Thermal runaway mechanism}

\begin{itemize}

\item \href{zotero://open-pdf/0_N8CS9ZX2/3}{2.1.1 Mechanical abuse}
\item \href{zotero://open-pdf/0_N8CS9ZX2/4}{2.1.2 Electrical abuse}
\item \href{zotero://open-pdf/0_N8CS9ZX2/4}{2.1.3 Thermal abuse}

\end{itemize}

\item \href{zotero://open-pdf/0_N8CS9ZX2/5}{2.2 Thermal runaway propagation}
\item \href{zotero://open-pdf/0_N8CS9ZX2/6}{2.3 Thermal runaway fumes}

\end{itemize}

\item \href{zotero://open-pdf/0_N8CS9ZX2/6}{3 Thermal runaway models and prediction}

\begin{itemize}

\item \href{zotero://open-pdf/0_N8CS9ZX2/6}{3.1 Numerical modeling of BTMS - background}
\item \href{zotero://open-pdf/0_N8CS9ZX2/9}{3.2 Thermal runaway models}
\item \href{zotero://open-pdf/0_N8CS9ZX2/9}{3.3 Thermal runaway prediction}

\end{itemize}

\item \href{zotero://open-pdf/0_N8CS9ZX2/10}{4 Thermal runaway prevention strategies}

\begin{itemize}

\item \href{zotero://open-pdf/0_N8CS9ZX2/10}{4.1 Air cooled BTMS}
\item \href{zotero://open-pdf/0_N8CS9ZX2/12}{4.2 Liquid cooled BTMS}
\item \href{zotero://open-pdf/0_N8CS9ZX2/13}{4.3 Phase change material (PCM) based BTMS}
\item \href{zotero://open-pdf/0_N8CS9ZX2/14}{4.4 Hybrid BTMS}

\end{itemize}

\item \href{zotero://open-pdf/0_N8CS9ZX2/15}{5 Thermal runaway mitigation}

\begin{itemize}

\item \href{zotero://open-pdf/0_N8CS9ZX2/15}{5.1 Thermal runaway mitigation mechanism}
\item \href{zotero://open-pdf/0_N8CS9ZX2/16}{5.2 Thermal runaway mitigation strategies}

\end{itemize}

\item \href{zotero://open-pdf/0_N8CS9ZX2/17}{6 Summary and conclusions}
\item \href{zotero://open-pdf/0_N8CS9ZX2/17}{7 New directions of research}
\item \href{zotero://open-pdf/0_N8CS9ZX2/18}{Declaration of Competing Interest}
\item \href{zotero://open-pdf/0_N8CS9ZX2/18}{Data availability}
\item \href{zotero://open-pdf/0_N8CS9ZX2/18}{Acknowledgement}
\item \href{zotero://open-pdf/0_N8CS9ZX2/18}{References}

\end{itemize}

\par
Discusses runaway mechanisms and preventative measures. ~Could be a good starting point?
\par
One other survey (of many): (Zhang et al., 2023; Zhang et al., 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Shahid22ReviewThermalRunawaya.pdf}
}

@misc{Podili23HowCountBattery,
  type = {Forum Post},
  title = {How to Count the Battery Cycles Using Rain Flow Counting Using Python},
  author = {{podili}, aparna},
  year = {2023},
  month = jan,
  journal = {Stack Overflow},
  url = {https://stackoverflow.com/q/74990147/2591097},
  urldate = {2023-08-11},
  keywords = {battery,battSOC,battSOH,hasCode},
  note = {Simple python for counting battery cycles.
\par
(Huang et al., 2021)},
  file = {C:\Users\scott\Zotero\storage\RFME4UUW\StackoverFlow Rainbow.html}
}

@article{Huang21NovelAutoregressiveRainflow,
  title = {A {{Novel Autoregressive Rainflow}}---{{Integrated Moving Average Modeling Method}} for the {{Accurate State}} of {{Health Prediction}} of {{Lithium-Ion Batteries}}},
  author = {Huang, Junhan and Wang, Shunli and Xu, Wenhua and Shi, Weihao and Fernandez, Carlos},
  year = {2021},
  month = may,
  journal = {Processes},
  volume = {9},
  number = {5},
  pages = {795},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-9717},
  doi = {10.3390/pr9050795},
  url = {https://www.mdpi.com/2227-9717/9/5/795},
  urldate = {2023-08-11},
  abstract = {The accurate estimation and prediction of lithium-ion battery state of health are one of the important core technologies of the battery management system, and are also the key to extending battery life. However, it is difficult to track state of health in real-time to predict and improve accuracy. This article selects the ternary lithium-ion battery as the research object. Based on the cycle method and data-driven idea, the improved rain flow counting algorithm is combined with the autoregressive integrated moving average model prediction model to propose a new prediction for the battery state of health method. Experiments are carried out with dynamic stress test and cycle conditions, and a confidence interval method is proposed to fit the error range. Compared with the actual value, the method proposed in this paper has a maximum error of 5.3160\% under dynamic stress test conditions, a maximum error of 5.4517\% when the state of charge of the cyclic conditions is used as a sample, and a maximum error of 0.7949\% when the state of health under cyclic conditions is used as a sample.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {battery,battSOC},
  note = {Fancy algorithm for counting battery cycles, SOH maybe SOC?
\par
(podili, 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Huang21NovelAutoregressiveRainflow.pdf}
}

@techreport{Mouchaileh21interventPriceMeth,
  title = {Intervention {{Pricing Methodology}}},
  author = {Mouchaileh, Peter GeersViolette},
  year = {2021},
  institution = {AEMO},
  url = {https://aemo.com.au/-/media/files/stakeholder_consultation/consultations/nem-consultations/2020/wholesale-demand-response/final-stage/intervention-pricing-methodology.pdf?la=en#:~:text=This%20Methodology%20is%20designed%20to,not%20intervened%20in%20the%20market.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Mouchaileh21interventPriceMeth.pdf}
}

@techreport{Aemo20interventPrcGuide,
  title = {Guide to {{Intervention Pricing}}},
  author = {AEMO},
  year = {2019},
  month = jun,
  institution = {AEMO},
  url = {https://aemo.com.au/-/media/files/stakeholder_consultation/consultations/nem-consultations/%E2%80%8C2019/%E2%80%8Cdispatch/guide-to-intervention-pricing.pdf?la=en&hash=4EB7C0EA39AB0103A5674182F58C64B8#:~:text=Intervention%20pricing%20does%20not%20trigger,exercised%20under%20the%20RERT%20provisions.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Aemo20interventPrcGuide.pdf}
}

@techreport{MMSDataModela,
  title = {{{MMS Data Model Package Summary}} v5.2},
  year = {2023},
  month = mar,
  institution = {AEMO},
  abstract = {These documents explain the packages, tables, and reports in the Electricity and Gas Data Models.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\MMSDataModela.pdf}
}

@techreport{AEMO24MMSDataModelV5_2,
  title = {{{MMS Data Model Report}} v5.2},
  author = {{AEMO}},
  year = {2023},
  month = may,
  number = {v5.2},
  institution = {AEMO},
  url = {https://visualisations.aemo.com.au/aemo/di-help/Content/Data_Model/MMS_Data_Model.htm#MMSDataModelv52},
  abstract = {These documents explain the packages, tables, and reports in the Electricity and Gas Data Models. The MMS Data Model is the definition of the interface to participants of data published by AEMO from  the NEM system. A database conforming to the MMS Data Model can contain a local copy of all  current participant-specific data recorded in the main NEM production database. The target databases  have been called such names as the Participant Database, the Participant InfoServer and the Replica  Database.  The MMS Data Model includes database tables, indexes and primary keys. The model is currently  exposed as a physical model, so is different in presentation for each RDBMS. However, the same  logical model underlies all the physical models published by AEMO.  The MMS Data Model is the target model for products transferring data from AEMO to each  participant. Current product supplied by AEMO for data transfer is Participant Data Replication  (PDR), with some support for the superseded Parser.  Compatibility of the transfer products with the MMS Data Model is the responsibility of those  products and their configuration. AEMO's intention is to supply the data transfer products pre- configured to deliver data consistent with the MMS Data Model, noting differences where they occur  (e.g. for historical reasons).  Entity Diagrams  The entity diagrams show the key columns. Relationships have now been included in many cases.  Note:  The National Electricity Market registration classification of Yarwun Power Station Unit 1  (dispatchable unit ID: YARWUN\_1) is market non-scheduled generating unit. However, it is a  condition of the registration of this unit that the Registered Participant complies with some of the  obligations of a Scheduled Generator. This unit is dispatched as a scheduled generating unit with  respect to its dispatch offers, targets and generation outputs. Accordingly, information about  YARWUN\_1 is reported as scheduled generating unit information},
  keywords = {obsLitNote},
  note = {AEMO24MMSDataModelV5\_2
\par
The tables in this pdf are also available online, \href{https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS\%20Data\%20Model\%20Report.htm}{here}.
\par
\section{Demand}

\par
Package Name: DEMAND\_FORECASTS
\par
Regional Demand Forecasts (For some reasion, 30 minute/8 day intermittent forecasts seem to be listed below the ``DEMAND'' name, yet they are not regional, but per generator). ~See my annotations
\par
There are also UIGF forecasts (5 minute ahead intermittend forecasts): (Harper, 2023),
\par
\subsection{Demand Forecasts}

\par
Table Name: DEMANDOPERATIONALFORECAST
\par
Shows Forecast Operational Demand for a particular date time interval
\par
\subsection{Demand Actuals}

\par
Table Name: DEMANDOPERATIONALACTUAL
\par
Shows Actual Operational Demand for a particular date time interval
\par
\section{Intermittent Generation}

\par
These seem to be per-generator, not summed to region. ~Interesting tables are: ~Identifying record for a given forecast of an intermittent generation. This table is the version table for the INTERMITTENT\_GEN\_FCST\_DATA table which stores the individual forecast values.
\par
The forecasts are
\par
``INTERMITTENT\_GEN\_FCST\_DATA updates every 30 minutes when AEMO issues a new 30minute forecast of intermittent generation out to 8 days ahead.'' (AEMO, 2023, p. 234)
\par
INTERMITTENT\_GEN\_FCST
\par
INTERMITTENT\_GEN\_FCST\_DATA
\par
Stores the forecast generation (MW) for each interval within a given forecast of an intermittent generator.
\par
INTERMITTENT\_GEN\_LIMIT
\par
A submission of Upper MW Limit for an intermittent generating unit, by Trading Day and Trading Interval},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\AEMO24MMSDataModelV5_2.pdf}
}

@techreport{MMSDataModelc,
  title = {{{MMS Data Model Upgrade Report}} v5.2},
  year = {2023},
  month = mar,
  institution = {AEMO},
  abstract = {These documents explain the packages, tables, and reports in the Electricity and Gas Data Models.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\MMSDataModelc.pdf}
}

@techreport{AEMO21nemFactSheet,
  title = {The {{National Electricity Market}} Fact Sheet},
  author = {AEMO},
  year = {2021},
  month = dec,
  address = {Australia},
  institution = {AEMO},
  url = {https://aemo.com.au/en/energy-systems/electricity/national-electricity-market-nem/about-the-national-electricity-market-nem},
  abstract = {The NEM spans Australia's eastern and south-eastern coasts  and comprises of five interconnected states that also act  as price regions: Queensland, New South Wales (including  the Australian Capital Territory), South Australia, Victoria,  and Tasmania. Western Australia and the Northern Territory are not  connected to the NEM, primarily due to the distance between  networks. The NEM's transmission network carries power from  electricity generators to large industrial energy users and local  electricity distributors across the five regions. These assets  are owned and operated by state governments, or private  businesses.  The NEM commenced operation as a wholesale spot market for electricity in December 1998. The NEM incorporates around 40,000 km  of transmission lines and cables. The NEM supplies about 204 terawatt  hours of electricity to businesses and  households each year. \$11.5  billion was traded in the NEM in  FY 2020-21. The NEM supplies approximately  10.7 million customers. The NEM has a total electricity generating  capacity of 65,252  MW  (as at December 2021). The NEM has approximately 14 GW  of distributed solar (as at Dec 2021).  Collectively the largest generator in the NEM.  The National Electricity Market  (NEM) operates on one of the  world's longest interconnected  power systems, stretching from  Port Douglas in Queensland to  Port Lincoln in South Australia  and across the Bass Strait to  Tasmania -- a distance of around  5,000 kilometres.},
  keywords = {noted},
  note = {AEMO21nemFactSheet
\par
\begin{itemize}

\item Solar has biggest capacity in NEM but in energy, is only 6\% of coal; wind is 16\% of coal
\item 504 market participants: generators, transmission and distribution services, customers
\item Bid stacking like ERCOT or EPEX: cheapest energy dispatched first
\item Dispatch price determined every 5 minutes, so a power/price curve every 5 minutes (5MS settlement)
\item price cap: \$15000
\item price floor: -\$1000 (negative prices increasing over time)
\item AEMO must recover costs from customers
\item In addition to price, customers pay a commercial tarrif
\item 
\par
Price types
\par
\begin{itemize}

\item NEM market: supply/demand curve determination, every 5 mins
\item 
\par
Financial market ~
\par
\begin{itemize}

\item contracts entered
\item smooth price volatility
\item done with derivatives: swaps, hedges, options, futures:

\end{itemize}

\end{itemize}

\item RERT (reliability and emergency reserve training) contracts, where customers allow their load to be shed in emergency.
\item 
\par
Retail electricity markets
\par
\begin{itemize}

\item AEMO runs these too
\item allow retail competition
\item choice of suppliers

\end{itemize}

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\AEMO21nemFactSheet.pdf}
}

@techreport{AEMO20sysStrengthNEM,
  title = {System Strength in the {{NEM}} Explained},
  author = {AEMO},
  year = {2020},
  month = mar,
  address = {Australia},
  url = {https://aemo.com.au/en/energy-systems/electricity/national-electricity-market-nem/about-the-national-electricity-market-nem},
  abstract = {The Australian Energy Market Commission (AEMC) made a rule on managing power system fault levels in  2017 1 (Fault Level Rule), which created a new framework in the National Electricity Rules (NER) for the  management of system strength.  The Fault Level Rule responded to a need to facilitate the safe and secure  operation of the National Electricity Market (NEM) power system with increasing levels of inverter-based  resources (IBR) 2.   The Fault Level Rule sought to ensure that adequate sources of system strength would remain available (or  be made available) to assist in the management of power system security and enable the secure operation of  IBR.  There are two parallel mechanisms in the NER to achieve this outcome:  minimum three-phase fault  levels to be maintained by transmission network service providers (TNSPs) for the power system to be in a  secure operating state, and obligations on connection applicants (primarily Generators) to remediate adverse  system strength impacts resulting from their connection (or alteration).  System strength is generally not well understood.  This document seeks to address some common  misunderstandings by presenting AEMO's views on:  {$\bullet$} The meaning of system strength.  {$\bullet$} The importance of system strength.  {$\bullet$} Relationship between system strength and other power system stability phenomena.  {$\bullet$} Assessing system strength.  {$\bullet$} Impact on system strength from new connections, and the application of system strength remediation.  {$\bullet$} System strength from a planning perspective},
  keywords = {AEMO},
  note = {AEMO20sysStrengthNEM
\par
alksdfj
\par
\begin{itemize}

\item Which of these problems are solved by grid forming inverters (Witze, 2023)?

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\AEMO20sysStrengthNEM.pdf}
}

@techreport{AEMO20powSysRqts,
  title = {Power {{System Requirements}}},
  author = {AEMO},
  year = {2020},
  month = jul,
  url = {https://aemo.com.au/en/energy-systems/electricity/national-electricity-market-nem/about-the-national-electricity-market-nem},
  abstract = {Modern power systems are giant, multi-faceted machines. To operate the  complex `system of systems' in Australia's National Electricity Market (NEM),  AEMO oversees in aggregate millions of separate electricity supply and demand  decisions in real time, all day, every day.   The NEM, like power systems worldwide, is being transformed from a system dominated by large thermal  power stations, to a system including a multitude of power generation resources and technologies of various  sizes 1,2. At the same time, customers are engaging with their electricity supply in new ways.   The energy transformation involves a shift from:  {$\bullet$} Firm to variable energy sources.  {$\bullet$} Synchronous to inverter-based resource (IBR) generation.  {$\bullet$} A centralised to a decentralised system.  {$\bullet$} Passive to active consumers.  AEMO's challenge is to continually meet the needs of the power system, in the  face of major structural changes and the resulting uncertainty across investment and operational timeframes.   While the power system is being transformed, the laws of physics that determine electrical flows do not  change. To maintain a secure and reliable system, a range of interdependent technical and operational needs  must be met at all times.   Physically, the NEM operates on one of the world's longest interconnected power systems, stretching from  Port Douglas in Queensland to Port Lincoln in South Australia and across the Bass Strait to Tasmania -- a  distance of around 5,000 kilometres. By international standards, the NEM is unusually long and sparse, which  affects power system dynamics.  Interactions in any power system are highly complex and dynamic. Operating a power system involves a  continuum of decisions. AEMO needs to know what is happening in real time, and anticipate what is likely to  happen in the coming seconds, minutes, hours, days, weeks, years, even decades.   This work culminates in the continuous matching of supply with demand and constant provision of essential  voltage and frequency management services, ensuring sufficient reserves so the power system is robust  enough to cope with unexpected events and stay within the power system operational design limits.},
  keywords = {AEMO},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\AEMO20powSysRqts.pdf}
}

@article{Sturm23DatasetFireTests,
  title = {Dataset of Fire Tests with Lithium-Ion Battery Electric Vehicles in Road Tunnels},
  author = {Sturm, Peter and F{\"o}{\ss}leitner, Patrik and Fruhwirt, Daniel and Heindl, Simon Franz and Heger, Oliver and Galler, Robert and Wenighofer, Robert and Krausbar, Stefan},
  year = {2023},
  month = feb,
  journal = {Data in Brief},
  volume = {46},
  pages = {108839},
  issn = {23523409},
  doi = {10.1016/j.dib.2022.108839},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2352340922010423},
  urldate = {2023-08-18},
  langid = {english},
  keywords = {battFire,data},
  file = {C:\Users\scott\Zotero\storage\ZIXDWCED\Sturm et al. - 2023 - Dataset of fire tests with lithium-ion battery ele.pdf}
}

@misc{Bansal22DataShortCircuit,
  title = {Data: {{Short Circuit}} 666 {{Wh Li-Ion Battery}}},
  shorttitle = {Data},
  author = {Bansal, Abhishek},
  year = {2022},
  month = nov,
  publisher = {IEEE},
  url = {https://ieee-dataport.org/documents/data-short-circuit-666-wh-li-ion-battery},
  urldate = {2023-08-18},
  abstract = {This dataset is in support of my research paper - Short Circuit Analysis of 666 Wh Li-Ion NMC~Faults and datasets can be copied to submit in fire cause investigation reports or thesis.~The simulation is run for 20 hours (72000 seconds) of simulation time~for each fault of 100 faults.~PrePrint : (Make sure you have read Caution.)},
  langid = {english},
  keywords = {battFire,data}
}

@misc{zotero-6285,
  title = {About the {{National Electricity Market}} ({{NEM}})},
  author = {AEMO},
  url = {https://aemo.com.au/en/energy-systems/electricity/national-electricity-market-nem/about-the-national-electricity-market-nem},
  abstract = {The NEM commenced operation as wholesale spot market in December 1998. It interconnects five regional market jurisdictions -- Queensland, New South Wales (including the Australian Capital Territory), Victoria, South Australia, and Tasmania. Western Australia and the Northern Territory are not connected to the NEM.}
}

@misc{AEMO23aboutNEM,
  title = {National {{Electricity Market}} ({{NEM}})},
  author = {AEMO},
  year = {2023},
  month = aug,
  url = {https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem/about-the-national-electricity-market-nem},
  urldate = {2023-08-21},
  abstract = {Landing page for the NEM information.},
  langid = {english},
  keywords = {noted},
  note = {AEMO23aboutNEM
\par
AEMO National Electricity Market (NEM) connects power producers via HV transmission to large energy users and distributors. ~It's a wholesale Spot Market, or ``pool'' that controls aggregate power at 5 minute intervals.
\par
The 5 regions it interconnects are:
\par
\begin{enumerate}

\item Queensland
\item New South Wales, incl. Australian Capital Territory
\item Victoria
\item South Australia
\item Tasmania

\end{enumerate}

\par
Not included: Western AUS and the Northern Territory
\par
Reports this page links to:
\par
\begin{itemize}

\item (AEMO, 2021)
\item (AEMO, 2020)
\item (AEMO, 2020)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\AEMO23aboutNEM.html}
}

@misc{Witze23gridFormInvert,
  title = {How One Device Could Help Transform Our Power Grid},
  author = {Witze, Alexandra},
  year = {2023},
  month = aug,
  url = {https://www.sciencenews.org/article/one-device-transform-power-electical-grid-inverter},
  urldate = {2023-08-24},
  abstract = {As coal-fired power plants are retired, grid-forming inverters may be key to a future that relies on solar and wind power.},
  chapter = {Climate},
  langid = {american},
  keywords = {battery,noted},
  note = {Witze23gridFormInvert
\par
Grid forming inverters (GFI) control the flow of RES into grid, and help with blackstart, mimicking large power plants. ~They can inject voltage to help adjust frequency, and have been used for years in isolated grids. ~Most RES is connected only with grid-following inverter, which (AFIK from this article) follow the AC waveform, at whatever frequency it happens to be. ~GFI is not ``necessarily more expensive'' so recommendation to just always use them.
\par
Australia may be the world leader in large RES grid-forming inverters. ~In 2023, 3 already existed, with 3 ``in the works'', for a total of 480 MW. ~A couple are for S AUS wind plants, and 8 more RES/grid-forming projects are planned (2 GW more, 10X current GFI capacity).
\par
The UK is starting 5 new GFI projects, while the US is 10 years behind the curve.
\par
Do GFIs solve AEMO grid strength problems (AEMO, 2020)?
\par
\subsubsection{}

\subsubsection{Research Goals}

\begin{itemize}

\item 
\par
US (UNIFI) GFI
\par
\begin{itemize}

\item 2nd rev. of standards (end 2023)
\item demo programming required to integrate GFIs from multiple makers (millions of them, eventually)

\end{itemize}

\item 
\par
realtime diagnose and reaction to grid problems
\par
\begin{itemize}

\item e.g. GFI can't tolerate large short-circuit currents that standard plants can handle. ~Sounds like battery discharge is a possible solution to this
\item ML optimization across inverters for virtual power plant

\end{itemize}

\end{itemize}

\subsubsection{How many GFIs are needed?}

\begin{itemize}

\item depends upon grid age, stability, geometry, interconnectedness
\item dense nets like in EU need fewer than AUS (or US, I suppose)
\item some grids OK w/ 60-70\% RES connected to GFI
\item others ``might crash'' at 20\% (so, say, 30\% is OK? ~Not clear)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Witze23gridFormInvert.html}
}

@article{Stulp13TallClaimsSense,
  title = {Tall Claims? {{Sense}} and Nonsense about the Importance of Height of {{US}} Presidents},
  shorttitle = {Tall Claims?},
  author = {Stulp, Gert and Buunk, Abraham P. and Verhulst, Simon and Pollet, Thomas V.},
  year = {2013},
  month = feb,
  journal = {The Leadership Quarterly},
  volume = {24},
  number = {1},
  pages = {159--171},
  issn = {1048-9843},
  doi = {10.1016/j.leaqua.2012.09.002},
  url = {https://www.sciencedirect.com/science/article/pii/S1048984312000884},
  urldate = {2023-08-28},
  abstract = {According to both the scientific literature and popular media, all one needs to win a US presidential election is to be taller than one's opponent. Yet, such claims are often based on an arbitrary selection of elections, and inadequate statistical analysis. Using data on all presidential elections, we show that height is indeed an important factor in the US presidential elections. Candidates that were taller than their opponents received more popular votes, although they were not significantly more likely to win the actual election. Taller presidents were also more likely to be reelected. In addition, presidents were, on average, much taller than men from the same birth cohort. The advantage of taller candidates is potentially explained by perceptions associated with height: taller presidents are rated by experts as `greater', and having more leadership and communication skills. We conclude that height is an important characteristic in choosing and evaluating political leaders.},
  keywords = {battFire},
  file = {C\:\\Users\\scott\\Zotero\\storage\\FFZWBSK9\\Stulp et al. - 2013 - Tall claims Sense and nonsense about the importan.pdf;C\:\\Users\\scott\\Zotero\\storage\\SFLVDXPU\\stulp2013.pdf.pdf}
}

@article{Hong23FaultProgIsolEVbatt,
  title = {Fault {{Prognosis}} and {{Isolation}} of {{Lithium-Ion Batteries}} in {{Electric Vehicles Considering Real-Scenario Thermal Runaway Risks}}},
  author = {Hong, Jichao and Wang, Zhenpo and Qu, Changhui and Ma, Fei and Xu, Xiaoming and Yang, Jue and Zhang, Jinghan and Zhou, Yangjie and Shan, Tongxin and Hou, Yankai},
  year = {2023},
  month = feb,
  journal = {IEEE Journal of Emerging and Selected Topics in Power Electronics},
  volume = {11},
  number = {1},
  pages = {88--99},
  issn = {2168-6785},
  doi = {10.1109/JESTPE.2021.3097827},
  abstract = {Advanced safe battery storage systems with health prognostic performance are vital for electric vehicles. Various faults of lithium-ion batteries are usually undetectable in their early stage due to their concealment and graduality. This article presents a real-time fault diagnosis and isolation scheme for real-scenario batteries using the normalized discrete wavelet decomposition. The early frequency-domain features of the fault signals are extracted utilizing the high-frequency detail wavelet components, and a multilevel fault prognosis strategy is developed considering complex charging/driving characteristics under real-vehicle operating conditions. The verification results, implemented on loose wire connection batteries and real-scenario thermal runaway batteries, demonstrate that the proposed method can accurately extract and locate the hidden fault signals even under small magnitudes and effectively detecting and isolating battery faults before thermal runaway. Furthermore, significant reliability and stability of the proposed method are verified on more real-vehicle operation data, enabling online monitorable and traceable of battery faults before triggering thermal runaway, safeguarding drivers and passengers in real-world vehicular operation.},
  keywords = {battFire,reading},
  note = {Hong23FaultProgIsolEVbatt
\par
The paper that Steven thought could run at T=1s, with a forecast horizon of but in fact, it needs Ts {$<$}= 0.48 ms: 
\par
(Hong et al., 2023, p. 94) Alg requires Fs {$>$}= 2048 Hz
\par
I think he also thought the forecast horizon was ``several days ahead'' but in fact, this is a realtime algorithm.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hong23FaultProgIsolEVbatt.pdf}
}

@article{Severson19battCycleLifeCapDegrd,
  title = {Data-Driven Prediction of Battery Cycle Life before Capacity Degradation},
  author = {Severson, Kristen A. and Attia, Peter M. and Jin, Norman and Perkins, Nicholas and Jiang, Benben and Yang, Zi and Chen, Michael H. and Aykol, Muratahan and Herring, Patrick K. and Fraggedakis, Dimitrios},
  year = {2019},
  journal = {Nature Energy},
  volume = {4},
  number = {5},
  pages = {383--391},
  publisher = {Nature Publishing Group UK London},
  keywords = {battery,battSOC,battSOH,dataSrc,reading},
  note = {Severson19battCycleLifeCapDegrd
\par
Has data. ~Steven says:
\par
\textbf{Steven Sun}~
\par
Hi Scott, this is the dataset can be use for cycle life prediction and SOC prediction, it is also the largest open source li-ion battery dataset in the world at this moment.\\
\href{https://data.matr.io/1/projects/5c48dd2bc625d700019f3204}{https://data.matr.io/1/projects/5c48dd2bc625d700019f3204}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Severson19battCycleLifeCapDegrd.pdf}
}

@article{Farmann16reviewSOCpredEV,
  title = {A Comprehensive Review of On-Board {{State-of-Available-Power}} Prediction Techniques for Lithium-Ion Batteries in Electric Vehicles},
  author = {Farmann, Alexander and Sauer, Dirk Uwe},
  year = {2016},
  month = oct,
  journal = {Journal of Power Sources},
  volume = {329},
  pages = {123--137},
  issn = {03787753},
  doi = {10.1016/j.jpowsour.2016.08.031},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0378775316310308},
  urldate = {2023-09-13},
  abstract = {This study provides an overview of available techniques for on-board State-of-Available-Power (SoAP) prediction of lithium-ion batteries (LIBs) in electric vehicles. Different approaches dealing with the onboard estimation of battery State-of-Charge (SoC) or State-of-Health (SoH) have been extensively discussed in various researches in the past. However, the topic of SoAP prediction has not been explored comprehensively yet. The prediction of the maximum power that can be applied to the battery by discharging or charging it during acceleration, regenerative braking and gradient climbing is definitely one of the most challenging tasks of battery management systems. In large lithium-ion battery packs because of many factors, such as temperature distribution, cell-to-cell deviations regarding the actual battery impedance or capacity either in initial or aged state, the use of efficient and reliable methods for battery state estimation is required. The available battery power is limited by the safe operating area (SOA), where SOA is defined by battery temperature, current, voltage and SoC. Accurate SoAP prediction allows the energy management system to regulate the power flow of the vehicle more precisely and optimize battery performance and improve its lifetime accordingly. To this end, scientific and technical literature sources are studied and available approaches are reviewed.},
  langid = {english},
  keywords = {battery,battSOP,intro},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Farmann16reviewSOCpredEV.pdf}
}

@article{Yang20battExtShortCircuitNN,
  title = {Characterization of External Short Circuit Faults in Electric Vehicle {{Li-ion}} Battery Packs and Prediction Using Artificial Neural Networks},
  author = {Yang, Ruixin and Xiong, Rui and Ma, Suxiao and Lin, Xinfan},
  year = {2020},
  month = feb,
  journal = {Applied Energy},
  volume = {260},
  pages = {114253},
  issn = {03062619},
  doi = {10.1016/j.apenergy.2019.114253},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0306261919319403},
  urldate = {2023-09-20},
  abstract = {To investigate the characteristics of lithium-ion battery packs under the condition that one cell is short-circuited when the whole battery pack is being discharged or charged, systematic battery external short circuit (ESC) experiments are conducted. Since not all battery cells are equipped with current sensors because of the space limitation and manufacturing cost, an artificial neural network (ANN)-based method is proposed to estimate the current of the short-circuited cell using only the voltage information, which is the feasible practice in electric vehicle application. Furthermore, the estimated current is used to predict maximum temperature increase as well as internal and surface temperature distribution of the ESC cell based on a 3D electro-thermal coupling model. Two experimental groups under constant current charging condition and constant power discharging condition are employed to validate the stability and accuracy of the proposed method. The results indicate that the rootmean-square-error between the estimated and measured current are 3.72 A and 6.61 A under the two validation experiments respectively, and the maximum estimation errors of temperature increase are 4.9 {$^\circ$}C and 7.3 {$^\circ$}C respectively.},
  langid = {english},
  keywords = {battery,battFire,priorityRead},
  note = {Yang20battExtShortCircuitNN
\par
Steve says this one doesn't need per-cell, when I asked if min/max of cells was usable. 
\par
My quick skim: seems like can predict max temp},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Yang20battExtShortCircuitNN.pdf}
}

@article{Zhao22battEVfailPred,
  title = {Data-Driven Prediction of Battery Failure for Electric Vehicles},
  author = {Zhao, Jingyuan and Ling, Heping and Wang, Junbin and Burke, Andrew F. and Lian, Yubo},
  year = {2022},
  month = apr,
  journal = {iScience},
  volume = {25},
  number = {4},
  pages = {104172},
  issn = {25890042},
  doi = {10.1016/j.isci.2022.104172},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2589004222004424},
  urldate = {2023-09-21},
  abstract = {Despite great progress in battery safety modeling, accurately predicting the evolution of multiphysics systems is extremely challenging. The question on how to ensure safety of billions of automotive batteries during their lifetime remains unanswered. In this study, we overcome the challenge by developing machine learning techniques based on the recorded data that are uploaded to the cloud. Using charging voltage and temperature curves from early cycles that are yet to exhibit symptoms of battery failure, we apply data-driven models to both predict and classify the sample data by health condition based on the observational, empirical, physical, and statistical understanding of the multiscale systems. The best well-integrated machine learning models achieve a verified classification accuracy of 96.3\% (exhibiting an increase of 20.4\% from initial model) and an average misclassification test error of 7.7\%. Our findings highlight the need for cloud-based artificial intelligence technology tailored to robustly and accurately predict battery failure in real-world applications.},
  langid = {english},
  keywords = {battery},
  note = {Zhao22battEVfailPred
\par
Steven's paper on how BYD's battery prediction system runs in the cloud.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhao22battEVfailPred.pdf}
}

@inproceedings{Figgener22predBattCycleAccure,
  title = {How to Use {{AI}} \& Machine Learning to Predict Battery Lifecycles},
  author = {Figgener, Jan and Shustack, Matt and Lee, Yuan},
  year = {2022},
  month = dec,
  publisher = {Accure},
  langid = {english},
  keywords = {battery,battFire,battSOC,battSOH,battSOP,intro},
  note = {Figgener22predBattCycleAccure
\par
Accure's battery lifecycle webinar slides},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Figgener22predBattCycleAccure.pdf}
}

@misc{DNV23BatteryAIweb,
  title = {Battery {{AI}}},
  author = {DNV},
  year = {2023},
  url = {https://store.veracity.com/battery-ai},
  urldate = {2023-09-22},
  abstract = {Battery AI Modelling battery health using AI and laboratory testing data.},
  langid = {english},
  keywords = {battery,battFire,battSOC,battSOH,battSOP,intro},
  note = {DNV23BatteryAIweb
\par
DNV's product, which is both apparently hybrid physical/ML, transfer learning, etc. ~Also uses generic battery model
\par
TODO: annotate highlights in this note},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\DNV23BatteryAIweb.html}
}

@article{Liu17reliabLargeBatts,
  title = {Reliability {{Evaluation}} of {{Large Scale Battery Energy Storage Systems}}},
  author = {Liu, Mingjun and Li, Wenyuan and Wang, Caisheng and Polis, Michael P. and Wang, Le Yi and Li, Jian},
  year = {2017},
  month = nov,
  journal = {IEEE Transactions on Smart Grid},
  volume = {8},
  number = {6},
  pages = {2733--2743},
  issn = {1949-3053, 1949-3061},
  doi = {10.1109/TSG.2016.2536688},
  url = {http://ieeexplore.ieee.org/document/7433464/},
  urldate = {2023-09-22},
  abstract = {This paper analyzes the reliability of large scale battery storage systems consisting of multiple battery modules. The whole system reliability assessment is based on the reliability evaluation of system components including individual battery modules and power electronic converters. In order to evaluate the reliability of a battery module, a reliability model based on the state of health of individual battery cells is introduced. The state of health of a battery cell is calculated based on the capacity fade of the cell using a weighted Ampere-hour (Ah) throughput method. A universal generating function (UGF) based method is then introduced to evaluate the reliability of the battery module. The reliability model of DC/AC power electronic converters is also presented in the paper. The reliability analysis is conducted for battery storage systems with different system configurations and management strategies; and the influence of system configuration on the reliability of battery system is studied. Comparative studies are conducted for a classic battery energy storage system (BESS) and a reconfigurable battery energy storage system (RBESS) to demonstrate the advantages of having a reconfigurable system topology. The comparison results show that the proposed RBESS has higher system reliability and more power outputs than the classic BESS.},
  langid = {english},
  keywords = {battery},
  note = {Liu17reliabLargeBatts
\par
Probabilistic models of battery bank reliability vs. architecture. ~Point was to show the reliability advantage of reconfigurable batteries, but maybe the probabilistic models used here are useful by themselves, either for predictive maintenance, or for multi-year simulations -- if you know what the model coefficients are.
\par
I sent this one to Purboday Ghosh, who wanted to simulate battery outages. ~Maybe it'll help?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Liu17reliabLargeBatts.pdf}
}

@article{Seger23StorageDegradationModel,
  title = {A Storage Degradation Model of {{Li-ion}} Batteries to Integrate Ageing Effects in the Optimal Management and Design of an Isolated Microgrid},
  author = {Seger, Pedro VH and {Rigo-Mariani}, R{\'e}my and Thivel, Pierre-Xavier and Riu, Delphine},
  year = {2023},
  journal = {Applied Energy},
  volume = {333},
  pages = {120584},
  publisher = {Elsevier},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Seger23StorageDegradationModel.pdf}
}

@article{Moa23largeBattSafetyRIsk,
  title = {Large-Scale Energy Storage System: Safety and Risk Assessment},
  shorttitle = {Large-Scale Energy Storage System},
  author = {Moa, Ernest Hiong Yew and Go, Yun Ii},
  year = {2023},
  month = sep,
  journal = {Sustainable Energy Research},
  volume = {10},
  number = {1},
  pages = {13},
  issn = {2731-9237},
  doi = {10.1186/s40807-023-00082-z},
  url = {https://doi.org/10.1186/s40807-023-00082-z},
  urldate = {2023-09-22},
  abstract = {The International Renewable Energy Agency predicts that with current national policies, targets and energy plans, global renewable energy shares are expected to reach 36\% and 3400~GWh of stationary energy storage by 2050. However, IRENA Energy Transformation Scenario forecasts that these targets should be at 61\% and 9000~GWh to achieve net zero carbon emissions by 2050 and limit the global temperature rise within the twenty-first century to under 2~{$^\circ$}C. Despite widely known hazards and safety design of grid-scale battery energy storage systems, there is a lack of established risk management schemes and models as compared to the chemical, aviation, nuclear and the petroleum industry. Incidents of battery storage facility fires and explosions are reported every year since 2018, resulting in human injuries, and millions of US dollars in loss of asset and operation. Traditional risk assessment practices such as ETA, FTA, FMEA, HAZOP and STPA are becoming inadequate for accident prevention and mitigation of complex energy power systems. This work describes an improved risk assessment approach for analyzing safety designs in the battery energy storage system incorporated in large-scale solar to improve accident prevention and mitigation, via incorporating probabilistic event tree and systems theoretic analysis. The causal factors and mitigation measures are presented. The risk assessment framework presented is expected to benefit the Energy Commission and Sustainable Energy Development Authority, and Department of Standards in determining safety engineering guidelines and protocols for future large-scale renewable energy projects. Stakeholders and Utility companies will benefit from improved safety and reliability by avoiding high-cost asset damages and downtimes due to accident events.},
  keywords = {battery,battFire,intro},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Moa23largeBattSafetyRIsk.pdf}
}

@misc{Angenendt23battFngrprntOCV,
  title = {The {{Fingerprint}} of the {{Battery}}: {{Understanding Open-Circuit Voltage}}},
  shorttitle = {Blog -- {{The Fingerprint}} of the {{Battery}}},
  author = {Angenendt, Georg},
  year = {2023},
  month = sep,
  journal = {Accure Bog},
  url = {https://www.accure.net/battery-knowledge/battery-open-circuit-voltage?utm_campaign=Evergreen%20Newsletter&utm_medium=email&_hsmi=275605600&utm_content=275605905&utm_source=hs_email},
  urldate = {2023-09-26},
  abstract = {Analyzing the battery open-circuit voltage (OCV) curve can help predict battery lifetime, estimate the battery's state of health, and detect capacity anomalies.},
  langid = {english},
  keywords = {battery,battSOC,battSOH,battSOP,intro},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Angenendt23battFngrprntOCV.html}
}

@misc{Dolan23ScientistsUseMachine,
  title = {Scientists Use Machine Learning to Predict Narcissistic Traits Based on Neural and Psychological Features},
  author = {Dolan, Eric W.},
  year = {2023},
  month = sep,
  journal = {PsyPost},
  url = {https://www.psypost.org/2023/09/scientists-use-machine-learning-to-predict-narcissistic-traits-based-on-neural-and-psychological-features-189528},
  urldate = {2023-09-26},
  abstract = {In a new study published in the journal Social Neuroscience, researchers employed machine learning techniques to predict individual differences in narcissistic personality traits using distinct structural brain features. The study represents the first-ever attempt to harness machine learning for deciphering the neural underpinnings of narcissism. ...},
  langid = {american}
}

@misc{Simon18tensorflowAWSaemoPriceFrcst,
  title = {{{AWS}} Re:{{Invent}} 2018: {{Learning Applications Using TensorFlow}}, {{Advanced Microgrid Solutions}} ({{AIM401-R2}})},
  shorttitle = {{{AWS}} Re},
  year = {2018},
  month = dec,
  url = {https://www.youtube.com/watch?v=VIEp4GR9BRc},
  urldate = {2023-09-27},
  abstract = {The TensorFlow deep learning framework is used for developing diverse artificial intelligence (AI) applications, including computer vision, natural language, speech, and translation. In this session, learn how to use TensorFlow within the Amazon SageMaker machine learning platform. Then, hear from Advanced Microgrid Solutions about how they implemented a deep neural network architecture with Keras and TensorFlow to forecast energy prices in near real time. Complete Title: AWS re:Invent 2018: [REPEAT 2] Deep Learning Applications Using TensorFlow, ft. Advanced Microgrid Solutions (AIM401-R2)},
  collaborator = {Simon, Julien and Cliffod, Kevin and Martinez, Andrew},
  keywords = {AEMO,noted,obsLitNote},
  note = {Simon18tensorflowAWSaemoPriceFrcst
\par
An AEMO price forecast in some kind of TensorFlow. ~Minute 46:20 reports the errors.
\par
\textbf{See} \href{obsidian://open?vault=Obsidian\%20Share\%20Vault&file=work\%2FpriceFrcstAEMO\%2FSimon18tensorflowAWSaemoPriceFrcst\%20YouTube\%20Notes}{my video notes in Obsidian}
\par
\href{obsidian://open?vault=Obsidian\%20Share\%20Vault&file=priceFrcstAEMO\%2FAEMO\%20Market\%20Timing}{test link 2}}
}

@misc{Burba23localVsGlobalLGBMfrcst,
  title = {Local vs {{Global}} Forecasting: {{What}} You Need to Know},
  shorttitle = {Local vs {{Global}} Forecasting},
  author = {Burba, Davide},
  year = {2023},
  month = aug,
  journal = {Medium},
  url = {https://towardsdatascience.com/local-vs-global-forecasting-what-you-need-to-know-1cc29e66cae0},
  urldate = {2023-09-28},
  abstract = {A comparison of Local and Global approaches to time series forecasting, with a Python demonstration using LightGBM and the Australian{\dots}},
  langid = {english},
  keywords = {forecast,multitaskLrn,normalization,noted},
  note = {Burba23localVsGlobalLGBMfrcst
\par
A kind of multitask learning (``global'') where a single model predicts several regions of AUS. ~Results not bad, if a little sketchty
\par
\begin{enumerate}

\item min/max normalize inputs \& outputs individually for each region (model is LGBM, so is input scaling necessary?)
\item add a region variable to each region's data, so model knows what region the data comes from (just like adding horizon inputs)
\item concatenate (in time) feature vectors and targets for each time. ~This is similar light multi-task quantile feature engineering
\item Train on this data

\end{enumerate}

\par
Results are pretty good
\par
\begin{itemize}

\item 43\% \emph{lower} MAE across regions than ``local,'' one-model-per-region approach
\item 59\% \emph{worse} on 1 of 7 regions

\end{itemize}

\par
But this could have been a bit of a straw man, as none of these models used hyperparm selection: just the default parameters.
\par
NOTE: multitask lightgbm papers I had in bibtex say that boosted tree models are bad for multitask, but this may not to be the case here.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Burba23localVsGlobalLGBMfrcst.html}
}

@phdthesis{Han22elecMkt3riskEssaysPhD,
  title = {Three Essays on Risk Management in Electricity Markets},
  author = {Han, Lin},
  year = {2022},
  url = {https://figshare.mq.edu.au/articles/thesis/Three_essays_on_risk_management_in_electricity_markets/21638057},
  urldate = {2023-09-28},
  school = {Macquarie University},
  keywords = {AEMO,intro,priceFrcst},
  note = {Han22elecMkt3riskEssaysPhD
\par
Factors affecting AEMO price forecast errors, price dependence across regions (suggesting multi-output forecasting). ~Good to at least skim for AEMO price forecast.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Han22elecMkt3riskEssaysPhD.pdf}
}

@article{Thakare23elecMktFrcstDimsRvw,
  title = {Forecasting Different Dimensions of Liquidity in the Intraday Electricity Markets: {{A}} Review},
  shorttitle = {Forecasting Different Dimensions of Liquidity in the Intraday Electricity Markets},
  author = {Thakare, Sameer and Bokde, Neeraj Dhanraj and {Feij{\'o}o-Lorenzo}, Andr{\'e}s E. and Thakare, Sameer and Bokde, Neeraj Dhanraj and {Feij{\'o}o-Lorenzo}, Andr{\'e}s E.},
  year = {2023},
  journal = {AIMS Energy},
  volume = {11},
  number = {energy-11-05-044},
  pages = {918--959},
  issn = {2333-8334},
  doi = {10.3934/energy.2023044},
  url = {http://www.aimspress.com/rticle/doi/10.3934/energy.2023044},
  urldate = {2023-09-28},
  abstract = {{$<$}abstract{$><$}p{$>$}Energy consumption increases daily across the world. Electricity is the best means that humankind has found for transmitting energy. This can be said regardless of its origin. Energy transmission is crucial for ensuring the efficient and reliable distribution of electricity from power generation sources to end-users. It forms the backbone of modern societies, supporting various sectors such as residential, commercial, and industrial activities. Energy transmission is a fundamental enabler of well-functioning and competitive electricity markets, supporting reliable supply, market integration, price stability, and the integration of renewable energy sources. Electric energy sourced from various regions worldwide is routinely traded within these electricity markets on a daily basis. This paper presents a review of forecasting techniques for intraday electricity markets prices, volumes, and price volatility. Electricity markets operate in a sequential manner, encompassing distinct components such as the day-ahead, intraday, and balancing markets. The intraday market is closely linked to the timely delivery of electricity, as it facilitates the trading and adjustment of electricity supply and demand on the same day of delivery to ensure a balanced and reliable power grid. Accurate forecasts are essential for traders to maximize profits within intraday markets, making forecasting a critical concern in electricity market management. In this review, statistical and econometric approaches, involving various machine learning and ensemble/hybrid techniques, are presented. Overall, the literature highlights the superiority of machine learning and ensemble/hybrid models over statistical models.{$<$}/p{$><$}/abstract{$>$}},
  copyright = {2023 The Author(s)},
  langid = {english},
  annotation = {Cc\_license\_type: cc\_by\\
Primary\_atype: AIMS Energy\\
Subject\_term: Review\\
Subject\_term\_id: Review},
  note = {Thakare23elecMktFrcstDimsRvw
\par
Seems to be about different forecasting targets. References my and Henry's paper. ~},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Thakare23elecMktFrcstDimsRvw.pdf}
}

@techreport{Twaice23battStoreFactSheet,
  title = {More {{Value}} and {{Less}}  {{Risk With Energy}}  {{Storage Systems}} ({{Fact Sheet}})},
  author = {Twaice},
  year = {2023},
  institution = {Twaice},
  url = {https://www.twaice.com/products/energy-solutions},
  abstract = {TWAICE's energy solutions enable energy storage system owners to operate storage  systems flexibly to achieve best returns whilst protecting warranty coverage and battery  health with predictive battery analytics software},
  keywords = {battSOH,batttery,intro},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Twaice23battStoreFactSheet.pdf}
}

@misc{Manokhin23worksTSfrcstNixtla,
  title = {What {{Truly Works}} in {{Time Series Forecasting}} --- {{The Results}} from {{Nixtla}}'s {{Mega Study}}},
  author = {Manokhin, Valeriy},
  year = {2023},
  month = sep,
  journal = {Medium},
  url = {https://valeman.medium.com/what-truly-works-in-time-series-forecasting-the-results-from-nixtlas-mega-study-78eda5133622},
  urldate = {2023-10-03},
  abstract = {Time series is a captivating domain where the quest for a crystal ball never ceases.},
  langid = {english},
  keywords = {forecast,hasCode,intro,noted,obsLitNote},
  note = {Manokhin23worksTSfrcstNixtla
\par
LightGBM is great for hourly time series (why?) but NHITS better for Monthly and otherwise competitive. ~DeepAR is bad, while TimeGPT is not bad. ~Simple Theta and SeasonalNaive seem good benchmarks
\par
Proprietary non-reproducible study on TS forecasting ofMonash Time Series Forecasting Repositiory at various time resolutions says that ML techniques now dominate traditional techniques like exponential smoothing.
\par
\section{Monthly}

\begin{enumerate}

\item \href{https://nixtla.github.io/neuralforecast/models.nhits.html}{NHITS}: Neural Hierarchical Interpolation for Time Series. ~Was best in comparison
\item \href{https://github.com/Nixtla/nixtla}{TimeGPT}: generative pre-trained transformer model dedicated to prediction tasks. ~Next best
\item \href{https://towardsdatascience.com/theta-model-for-time-series-forecasting-642ad1d00358}{Theta}: A decomposition approach to forecasting. ~``strong'' ~in statsmodels
\item DOTheta: (?) ``strong''
\item \href{https://arxiv.org/abs/1704.04110}{DeepAR}: Probabilistic Forecasting with Autoregressive Recurrent Networks: barely bested\href{https://openforecast.org/adam/simpleForecastingMethods.html}{ Seasonal Naive} benchmark. ~Seasonal Naive is ``previous period'': previous day, previous year, {\dots}

\end{enumerate}

\section{Weekly}

\begin{enumerate}

\item \href{https://arxiv.org/abs/1905.10437}{NBEATS}: Neural basis expansion analysis for interpretable time series forecasting
\item TBATS: ARIMA with with multiple seasonality
\item DeepAR: barely beat seasonal

\end{enumerate}

\section{Daily}

\begin{enumerate}

\item TBATS is tops
\item NHITS {\textasciitilde} TimeGPTS (weekly?)
\item DeepAR: worse than Theta / DOTheta

\end{enumerate}

\section{Hourly}

\begin{enumerate}

\item LGBM significantly better than all models, including TimeGPT
\item but NHITSwas ``strong''

\end{enumerate}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Manokhin23worksTSfrcstNixtla.html}
}

@techreport{Scott23ShapedOpEnvAEMO,
  title = {Shaped {{Operating Envelopes}}: {{Technical Design}} and {{Implementation}}},
  author = {Scott, Paul and Iria, Jose and Gordon, Dan and Fraser, Andrew and Weise, Benjamin and Attarha, Ahmad and Noori, S. Mahdi},
  year = {2023},
  month = feb,
  institution = {The Australian National University},
  url = {https://arena.gov.au/knowledge-bank/?technology=distributed-energy-resources},
  abstract = {This technical report presents an overview of the shaped operating envelope concept, its implementation in the project, and simulations to illustrate the potential benefits over a more conventional approach to operating envelopes.},
  keywords = {AEMO,dispatch,forecast,optimization,priceFrcst},
  note = {Scott23ShapedOpEnvAEMO
\par
A new way for AEMO to optimize dispatch. ~``Something'' will be opensource. ~Seems like it will come on line soon, maybe 2023?
\par
Is the ``envelope'' something like Pinson's polygon prob. forecasts?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Scott23ShapedOpEnvAEMO.pdf}
}

@inproceedings{Li22LearnBidDeep,
  title = {Learn to {{Bid}}: {{Deep Reinforcement Learning}} with {{Transformer}} for {{Energy Storage Bidding}} in {{Energy}} and {{Contingency Reserve Markets}}},
  shorttitle = {Learn to {{Bid}}},
  booktitle = {Tackling {{Climate Change}} with {{Machine Learning}} 2022},
  author = {Li, Jinhao and Wang, Changlong and Zhang, Yanru and Wang, Hao},
  year = {2022},
  pages = {1--8},
  publisher = {Neural Information Processing Systems (NIPS)},
  url = {https://www.climatechange.ai/papers/neurips2022/62/paper.pdf},
  urldate = {2023-10-03},
  keywords = {AEMO,forecast,optimization,priceFrcst,reinfLrn},
  note = {Li22LearnBidDeep
\par
A way to learn price forecasting features for a price forecast?
\par
Transformer learns price features from AEMO data, which is fed into a Reinforcement Learning approach which does the bidding. ~},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Li22LearnBidDeep.pdf}
}

@article{Karimi-Arpanahi23predictQntAEMO,
  title = {Quantifying the Predictability of Renewable Energy Data for Improving Power Systems Decision-Making},
  author = {{Karimi-Arpanahi}, Sahand and Pourmousavi, S. Ali and Mahdavi, Nariman},
  year = {2023},
  journal = {Patterns},
  volume = {4},
  number = {4},
  publisher = {Elsevier},
  url = {https://www.cell.com/patterns/pdf/S2666-3899(23)00045-4.pdf},
  urldate = {2023-10-03},
  abstract = {Decision-making in the power systems domain often relies on predictions of renewable generation. While sophisticated forecasting methods have been developed to improve the accuracy of such predictions, their accuracy is limited by the inherent predictability of the data used. However, the predictability of time series data cannot be measured by existing prediction techniques. This important measure has been overlooked by researchers and practitioners in the power systems domain. In this paper, we systematically assess the suitability of various predictability measures for renewable generation time series data, revealing the best method and providing instructions for tuning it. Using real-world examples, we then illustrate how predictability could save end users and investors millions of dollars in the electricity sector.},
  keywords = {AEMO,priceFrcst},
  note = {Karimi-Arpanahi23predictQntAEMO
\par
Possibly an AEMO probabilistic price forecasting feature. ~Develops predictability features which are correlated with AEMO prices (regulation only?)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Karimi-Arpanahi23predictQntAEMO.pdf}
}

@misc{Weiss23dataBESScommiss,
  type = {Accure {{Webinar}}},
  title = {Transforming {{BESS Commissioning}} with {{Data}}},
  shorttitle = {Event - {{Webinar}}},
  author = {Weiss, Alison and Peterson, Jay and Kaires, Kai-Philipp},
  year = {2023},
  month = oct,
  url = {https://www.accure.net/events/webinar-transforming-bess-commissioning-through-data-driven-intelligence-best-practices-and-insights},
  urldate = {2023-10-04},
  abstract = {We will explore big data's role in redefining BESS commissioning while unveiling best practices and insights that fuel optimal performance and seamless integration.},
  langid = {english},
  note = {Weiss23dataBESScommiss
\par
Wood Mackenzie webinar with Accure and National Grid Renewables (an Accure customer). ~Accure emphasizes the importance of \textbf{cell-level data}, and claims that you can't rely on accelerated aging data because manufacturers cherrypick the cells. ~Say their cell SOH ML is better b/c it is looking at real operation and every cell: \textbf{no proof this works}. ~At least here, Accure's product is consulting during battery plant commissioning, detecting warranty problems, and then providing a battery data dashboard during operation. ~NGR mentions their dashboard, SOC index, and mo
\par
\begin{itemize}

\item Wiess: Mackenzie (the moderator)
\item 
\par
Peterson: National Grid (customer of Accure)
\par
\begin{itemize}

\item says Li-ion will dominate for grid flexibility for next 30 years (I think this means short-term storage)
\item 
\par
telemetry:
\par
\begin{itemize}

\item ``data points'' must be submitted 120 days before 1st operating day
\item 30 days before any change afterwards
\item realtime checks of some kind

\end{itemize}

\item 
\par
Integrator Testing
\par
\begin{itemize}

\item RTO required
\item {\dots} a long list of tests e.g. in ERCOT
\item be sure to measure stuff where you're supposed to

\end{itemize}

\item 
\par
Data \& Information
\par
\begin{itemize}

\item you must collect and own your data, so you can look back
\item define data collection methodology and timing ahead of time
\item get as soon as possible

\end{itemize}

\item 
\par
Early stage operations
\par
\begin{itemize}

\item drove it hard from the beginning
\item exposed problems, which they were able to reduce

\end{itemize}

\item 
\par
Current probs
\par
\begin{itemize}

\item lots of SOC mismatch
\item re-calibration failures
\item False fire alarms
\item voltage boundary problems
\item thermal problems

\end{itemize}

\item 
\par
Want to use Accure to get (the easy stuff)
\par
\begin{itemize}

\item accurate ``SOC index''
\item dashboard, ``ROC'' monitoring. ~What is ROC?

\end{itemize}

\end{itemize}

\item 
\par
Kairies: Accure
\par
\begin{itemize}

\item 0\% of batteries are coming from prod lines {$<$} 2 yrs old: so lots of probs
\item 50\% of batts projects done by crews w/ {$<$} 2 yrs experience
\item 
\par
what they've done during comissioning
\par
\begin{itemize}

\item ID non-compliance w/ regs
\item manage ``corrective action''
\item speed site acceptance, avoid penalties
\item manage supplier relationships
\item 
\par
5 examples of commisioning probs found in last 5 mo.
\par
\begin{itemize}

\item 
\par
25\% of batts below qual specs
\par
\begin{itemize}

\item ID'ed non-compliant cells
\item adjusted ``augmentaion schedule'' to account for limited lifetime perf.

\end{itemize}

\item 
\par
module level: wide cell capacity variance,
\par
\begin{itemize}

\item bottom level cap cells degrade whole module b/c they stop all charging
\item foudn several ``strings'' same as module? w/ this problem

\end{itemize}

\item 
\par
BMS sensor probs
\par
\begin{itemize}

\item ID'ed anomalies across sensors (offset).
\item reasons to believe this was defective sensor, not defective cell
\item they initiated a ``module exchange''

\end{itemize}

\item 
\par
BMS faiure
\par
\begin{itemize}

\item found during comissioning deep discharge
\item 
\par
noticed @ end that batt spent a lot of time @ below min voltages
\par
\begin{itemize}

\item causes copper in electrode to dissolve into electrolyte
\item get metal particles in battery, which can bridge, short, burn

\end{itemize}

\item faulty modules exchanged

\end{itemize}

\item 
\par
sub-contractor negligence
\par
\begin{itemize}

\item unexplained temperature profile change (anomaly?) flagged an inspection
\item cause: water got into batt container, somebody put a dehumidifier in there to get it out
\item the temp (60 C) was the humidifier
\item this was unnoticed until temp data examined later
\item voided the warranty, got a new one

\end{itemize}

\end{itemize}

\end{itemize}

\end{itemize}

\item 
\par
Q\&A
\par
\begin{itemize}

\item 
\par
do OEMs accept accure's analysis?
\par
\begin{itemize}

\item they can test themselves if you find the problems while their engineers are on site

\end{itemize}

\item 
\par
why test if have warranty?
\par
\begin{itemize}

\item not testing means too much commercial risk (lost revenue)
\item some warrantees only last until 1st day of operations

\end{itemize}

\item 
\par
owning data
\par
\begin{itemize}

\item national grid gets raw data (I think) \& accure calc outptus, but not accures algorithms

\end{itemize}

\item 
\par
how X improves battery life
\par
\begin{itemize}

\item limiting temperature
\item number of cycles: ~Accure somehow estimates cost of cycling so can decide how hard to hit thme when price of power is high?
\item overall revenues greatly increased by {\textasciitilde}30\% greater lifetime if you monitor
\item But I believe that NGR guy said they don't use it. ~Stick within manufacturer limits

\end{itemize}

\item 
\par
degradation models
\par
\begin{itemize}

\item where do you get the data from accellerated test
\item look at individual cell and model its individual behavior
\item says don't rely on lab tests, since they're cherrypicked by manufacturer to look good
\item better to look at data from real life behavior (doesn't say how this extrapolation is done)

\end{itemize}

\end{itemize}

\end{itemize}}
}

@techreport{Twaice22enrgyStrAnlytceWhitePpr,
  title = {Energy {{Storage Analytics}}:  {{The}} Game Changer for Energy Storage},
  year = {2022},
  month = jan,
  institution = {Twaice},
  abstract = {Battery storage systems are an essential component  of the energy sector. However, they are complex  systems that require special attention. The primary  goal of storage owners is to maximize the profit  possible from the storage system without taking on  additional risk. This is where battery analytics comes  into play.},
  keywords = {battery,intro},
  note = {Twaice22enrgyStrAnlytceWhitePpr
\par
German battery analytics company. ~Contacted me in Oct. in 2023.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Twaice22enrgyStrAnlytceWhitePpr.pdf}
}

@article{Tang21recoverBattAgeDatML,
  title = {Recovering Large-Scale Battery Aging Dataset with Machine Learning},
  author = {Tang, Xiaopeng and Liu, Kailong and Li, Kang and Widanage, Widanalage Dhammika and Kendrick, Emma and Gao, Furong},
  year = {2021},
  month = aug,
  journal = {Patterns},
  volume = {2},
  number = {8},
  pages = {100302},
  issn = {26663899},
  doi = {10.1016/j.patter.2021.100302},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2666389921001458},
  urldate = {2023-10-05},
  abstract = {Batteries are crucial for building a clean and sustainable society, and their performance is highly affected by aging status. Reliable battery health assessment, however, is currently restrained by limited access to sufficient aging data, resulting from not only complicated battery operations but also long aging experimental time (several months or even years). Refining industrial datasets (e.g., the field data from electric vehicle batteries) unlocks opportunities to acquire large-volume aging data with low experimental efforts. We introduce the potential of combining industrial data with accelerated aging tests to recover high-quality battery aging datasets, through a migration-based machine learning. A comprehensive dataset containing 8,947 aging cycles with 15 operational modes is collected for evaluation. While saving up to 90\% experimental time, aging data can be recovered with ultra-low error within 1\%. It provides an alternative solution to significantly improve data shortage issues for assessment of battery and energy storage aging.},
  langid = {english},
  keywords = {battery,battTstProtocol,noted},
  note = {Tang21recoverBattAgeDatML
\par
This is a way to generate a ton of near-lab-quality capacity trajectories from operational industrial batteries, but you still have to wait for the end of the operational battery lifespan. The generated data can be used to train accurate capacity models for similar industrial batteries (I think it says), as well as for other prediction targets, like SOH, although this is not tested. ~Capacity result is very accurate: {$<$}1\% RMSE with only 3 capacity tests throughout the battery life (which 3\% of the time needed for laboratory test).
\par
Steven likes this paper.
\par
\subsubsection{Tidbits}

\begin{itemize}

\item 
\par
lab tests are expensive
\par
\begin{itemize}

\item hundreds or thousands of equivalent cycles
\item several months to a year (but a year is shorter than waiting for the industrial batteries used for modeling to die, isn't it?)

\end{itemize}

\item \textbf{industrial data is a huge}, if compromised, source; Chinese example has 1M more points than the open MIT-Standford battery aging dataset (mentions CALC and NASA sets too)
\item \textbf{industrial data normally not suitable} for model training: doesn't cover the range of operation or full charge/discharge
\item important to evenly space the 3 capacity tests

\end{itemize}

\par
Aging testing overview: (Pakjoo et al., 2023)
\par
\subsubsection{Method}

\par
Use industrial batteries in operation to generate accurate aging (capacity degradation) models without frequent capacity measurements (only 3, spread through battery's lifespan) using large data sets of batteries in operation, not following any cycling experiments.
\par
\begin{enumerate}

\item Do a laboratory accelerated test on a test battery of the same type (I think), measuring the capacity trajectory and collecting, \textbf{x}, corresponding sets of voltages, currents, temp (I think).
\item Train a base model, a simple 3 layer NN to predict the capacity given \textbf{x}. ~I think they're predicting \emph{incremental capacity} ``IC'' (Tang et al., 2021, p. 2).
\item On the target battery (the with only industrial data), do 3 or so laboratory capacity tests, at 0\%, 50\% and 100\% the battery's lifespan (during industrial operation, I guess). ~
\item 
\par
Generate fake target battery capacity data between the lab tests
\par
\begin{enumerate}

\item predict capacity using the base model
\item correct by interpolating from the laboratory capacity tests. ~They call this piecewise linear, but \emph{I don't quite get it.}

\end{enumerate}

\item Use fake target battery capacities to train other models, I guess.\\
\textbf{ ~}

\end{enumerate}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Tang21recoverBattAgeDatML.pdf}
}

@article{Cabrera-Castillo16battStateSafety,
  title = {Calculation of the State of Safety ({{SOS}}) for Lithium Ion Batteries},
  author = {{Cabrera-Castillo}, Eliud and Niedermeier, Florian and Jossen, Andreas},
  year = {2016},
  month = aug,
  journal = {Journal of Power Sources},
  volume = {324},
  pages = {509--520},
  issn = {03787753},
  doi = {10.1016/j.jpowsour.2016.05.068},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0378775316306140},
  urldate = {2023-10-05},
  abstract = {As lithium ion batteries are adopted in electric vehicles and stationary storage applications, the higher number of cells and greater energy densities increases the risks of possible catastrophic events. This paper shows a definition and method to calculate the state of safety of an energy storage system based on the concept that safety is inversely proportional to the concept of abuse. As the latter increases, the former decreases to zero.},
  langid = {english},
  keywords = {battery,battFire,battSOC,battSOH,noted},
  note = {Cabrera-Castillo16battStateSafety
\par
Steven recommends this, but I don't see how it helps: the example it gave comes from test data collected in a bombproof room! ~Apparently from many tries, so this doesn't solve our lack of data problem.
\par
Also, it ooks like one giant heuristic that would be hard to trust: e.g. how are those probabilities estimated? ~Why is safety a product (maybe it's like like when you approximate random variables as independent, so you can say likelihoold is a product?). ~
\par
\section{Intro to other methods}

\begin{itemize}

\item 
\par
``hazard modes and risk'' (Cabrera-Castillo et al., 2016, p. 510):
\par
\begin{itemize}

\item 
\par
for each type of hazard, separately define
\par
\begin{enumerate}

\item 
\par
\textbf{hazard severity / leve}l: 8 level classification of severity:
\par
\begin{itemize}

\item In EUCAR standard (``Table 1'' (Cabrera-Castillo et al., 2016, p. 510))
\item 0 is no effect; 7 is explosion

\end{itemize}

\begin{itemize}

\item assume a severity level is always preceded by the ones earlier in the ranking
\item different authors rank these differently
\item level 4 and under is ``safe'' even though battery might be unusable

\end{itemize}

\item 
\par
Hazard Likelihood: 10 level pseudo log of rate of failure occurance
\par
\begin{itemize}

\item example is: ``Table 2'' (Cabrera-Castillo et al., 2016, p. 510)
\item ppms oor \%
\item converted to a level: 10 means 10\%; 1 means 0.001\%

\end{itemize}

\item 
\par
Hazard control: measurs that can reduce a hazard
\par
\begin{itemize}

\item ``Table 3'' (Cabrera-Castillo et al., 2016, p. 510)
\item between 1 (no reduction) and 0 (complete hazard removal)

\end{itemize}

\end{enumerate}

\item The product of these 3 is the ``hazard risk,'' Hr

\end{itemize}

\item 
\par
state of function (SOF)(Cabrera-Castillo et al., 2016, p. 510)
\par
\begin{itemize}

\item huh?: goes to zero near perfect power delivery if the power is below Pmax
\item anyway, you can make it a product of SOC and SOH (with SOH defined as a voltage ratiio??). ~Maybe that's where the product of this paper comes from?

\end{itemize}

\item 
\par
``Catastrophe theory'' (Cabrera-Castillo et al., 2016, p. 511)
\par
\begin{itemize}

\item equation derived from heat balance equation
\item used to estimate prob of thermal runaway, but this is not explained

\end{itemize}

\end{itemize}

\section{2. safety=1/abuse \& total safety is product of factor safeties}

\begin{itemize}

\item ``2. Formulation of the state of safety as the reciprocal of a probability function for abuse'' (Cabrera-Castillo et al., 2016, p. 511)
\item 
\par
for each safety factor
\par
\begin{itemize}

\item 
\par
specify a state of safety function (SOS)
\par
\begin{itemize}

\item must have form: SOS(x) = 1/(g(x)+1)
\item x: variables that affect this safety
\item g(x) is quadratic, contains another ``abuse function'' h(x)
\item solve for params at 100\% ``safety'' and 80\% safety (doesn't have to be 80\%)
\item 
\par
abuse function, g(x), form defined by kind of variable x is
\par
\begin{itemize}

\item either monotonically related to x
\item or tests whether x is within bounds

\end{itemize}

\end{itemize}

\end{itemize}

\item State of safety is a product of individual safeties (or combinations thereof)
\item 
\par
Properties describing state of safety, SOS
\par
\begin{enumerate}

\item Temp
\item Current
\item Voltage
\item SOC
\item SOH
\item Internal Impedance
\item mech. deformation
\item derivatives of stuff e.g. fast rate of charge
\item variable combinations of above

\end{enumerate}

\end{itemize}

\section{Safety Thresholds Chosen By Experimental Tests}

\par
``5. Establishment of safety limits by means of experimental tests'' (Cabrera-Castillo et al., 2016, p. 515)
\par
For each saftety factor, chose 100\% and z\% safety thresholds. ~Could do this by physical/chem principles or by ``abuse experiments'' but they do it by ``abuse experiements on LPF cells.
\par
\begin{itemize}

\item Done in friggin bomb proof ``saftey chamber''
\item push abuse factors until reach a safety limit defined e.g. by manufacturer spec. max discharge current, etc.
\item curents below limit considered 100\% safe
\item somehow ``consisder the appropriate probability function'' for current above. ~With many trials until failure?
\item Did this for safety properties mentioned above

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Cabrera-Castillo16battStateSafety.pdf}
}

@article{Zhou23battStateEstBMS,
  title = {State {{Estimation Models}} of {{Lithium-Ion Batteries}} for {{Battery Management System}}: {{Status}}, {{Challenges}}, and {{Future Trends}}},
  shorttitle = {State {{Estimation Models}} of {{Lithium-Ion Batteries}} for {{Battery Management System}}},
  author = {Zhou, Long and Lai, Xin and Li, Bin and Yao, Yi and Yuan, Ming and Weng, Jiahui and Zheng, Yuejiu},
  year = {2023},
  month = feb,
  journal = {Batteries},
  volume = {9},
  number = {2},
  pages = {131},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2313-0105},
  doi = {10.3390/batteries9020131},
  url = {https://www.mdpi.com/2313-0105/9/2/131},
  urldate = {2023-10-10},
  abstract = {The state estimation technology of lithium-ion batteries is one of the core functions elements of the battery management system (BMS), and it is an academic hotspot related to the functionality and safety of the battery for electric vehicles. This paper comprehensively reviews the research status, technical challenges, and development trends of state estimation of lithium-ion batteries. First, the key issues and technical challenges of battery state estimation are summarized from three aspects of characteristics, models, and algorithms, and the technical challenges in state estimation are deeply analyzed. Second, four typical battery states (state of health, state of charge, state of energy, and state of power) and their joint estimation methods are reviewed, and feasible estimation frameworks are proposed, respectively. Finally, the development trends of state estimation are prospected. Advanced technologies such as artificial intelligence and cloud networking have further reshaped battery state estimation, bringing new methods to estimate the state of the battery under complex and extreme operating conditions. The research results provide a valuable reference for battery state estimation in the next-generation battery management system.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {battery,battFire,battSOC,battSOH,reading},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhou23battStateEstBMS.pdf}
}

@techreport{Accure20battPredSafety,
  title = {Predictive Diagnostics to Improve Battery Safety},
  author = {Accure},
  institution = {Accure Battery Intelligience},
  abstract = {Lithium-ion batteries (LIB) are a key enabler of our clean  energy future. Today, LIB already power devices from  electric scooters to ships and grid-connected storage  systems. However, as with all energy sources, batteries  carry a certain risk of failure. For LIB this can result in  gassing and burning, potentially harming people and  property. The ongoing trend towards ever higher energy  densities literally adds fuel to the fire.  Figure 1 shows the trade-offs typically made in the design  of LIB. To achieve higher energy densities, manufacturers  use more reactive materials while minimizing safety  margins. With more energy stored in a single battery,  larger amounts of energy are released in case of a failure.   With the fast growth of the battery industry, the number  of battery incidents has also increased. One prominent  example is the 2019 fire at an APS site in McMicken,  Arizona. A battery failure led to a massive explosion in one  of the storage containers, causing millions in damages  and hospitalizing several firefighters. In the automotive  world, General Motors, Hyundai, and Kia had to recall over  200,000 electric vehicles between 2020 and 2021 after  more than 30 battery fires. All these incidents involved  world leading companies with decades of experience in  the battery sector},
  keywords = {battery,battFire,battSOC,battSOH,battSOP,intro},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Accure20battPredSafety2.pdf}
}

@techreport{Gatt22mktAnsSpecAEMO,
  title = {Market Ancillary Service  Specification},
  author = {Gatt, Michael},
  year = {22},
  month = oct,
  institution = {AEMO},
  url = {https://aemo.com.au/-/media/files/stakeholder_consultation/consultations/nem-consultations/2022/amendment-of-the-mass/final-determination/market-ancillary-services-specification---v80-effective-9-oct-2023.pdf?la=en},
  keywords = {AEMO},
  note = {Gatt22mktAnsSpecAEMO
\par
Table 3 shows the 1 second requirement for AEMO's new very fast raise and lower services. ~This went into effect on 10/9/23.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gatt22mktAnsSpecAEMO.pdf}
}

@misc{Dungey18StrategicBidAEMO,
  type = {{{SSRN Scholarly Paper}}},
  title = {Strategic {{Bidding}} of {{Electric Power Generating Companies}}: {{Evidence}} from the {{Australian National Energy Market}}},
  shorttitle = {Strategic {{Bidding}} of {{Electric Power Generating Companies}}},
  author = {Dungey, Mardi H. and Ghahremanlou, Ali and Van Long, Ngo},
  year = {2018},
  month = jan,
  number = {3126673},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.3126673},
  url = {https://papers.ssrn.com/abstract=3126673},
  urldate = {2023-10-13},
  abstract = {We extend existing theoretical frameworks describing electricity markets where each generator provides a Market Operator (MO) with a supply schedule in advance. The MO combines these with demand forecasts to produce equilibrium prices and instructs firms on their dispatch. We incorporate the possibility that generating firms may rebid (or revise) their supply schedule prior to dispatch - an important feature of markets in many countries which has not previously been included in theoretical models. We show that a dominant firm can gain substantially by manipulating its bids, and take advantage of the opportunity to submit rebids. In the Australian National Energy Market (NEM) where settlement prices are an average of six dispatch prices, it can, for example, withhold capacity at lower prices for the first bid in a period, creating a price hike, and then add capacity at lower prices to ensure dispatch. Using data from the Australian NEM we provide the first empirical evidence consistent with the hypothesized theoretical behaviour in the observed data.},
  langid = {english},
  keywords = {AEMO},
  note = {\textbf{Dungey18StrategicBidAEMO}
\par
The most relevant-to-now AEMO price forecasting data will be \textbf{after 10/1/21}.
\par
View mentioned that \href{obsidian://open?vault=Obsidian\%20Share\%20Vault&file=priceFrcstAEMO\%2FMarket\%20Rule\%20Changes\%20(from\%20View)}{AEMO removed1 the 30 minute price averaging interval in 2021} in order to eliminate power companies' practice of bid pumping. ~I think this paper explains how that worked. ~It also suggests (along with \href{obsidian://open?vault=Obsidian\%20Share\%20Vault&file=priceFrcstAEMO\%2FMarket\%20Rule\%20Changes\%20(from\%20View)}{View's table}) that data after 10/1/2021 will be significantly different than earlier.
\par
When price was averaged over a half hour's worth of 5 minute intervals, AEMO generators could ``withhold capacity at lower prices for the first bid in a period, creating a price hike, and then add capacity at lower prices to ensure dispatch.''
\par
\textbf{Also}
\par
\begin{itemize}

\item (Dyson, 2022) has a short explanation
\item (AEMC, 2018) AEMO's report

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Dungey18StrategicBidAEMO.pdf}
}

@article{Pakjoo23battTestAgeBESSrvw,
  title = {A {{Review}} on {{Testing}} of {{Electrochemical Cells}} for {{Aging Models}} in {{BESS}}},
  author = {Pakjoo, Mehrshad and Piegari, Luigi and Rancilio, Giuliano and Colnago, Silvia and Mengou, Joseph Epoupa and Bresciani, Federico and Gorni, Giacomo and Mandelli, Stefano and Merlo, Marco},
  year = {2023},
  month = jan,
  journal = {Energies},
  volume = {16},
  number = {19},
  pages = {6887},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1996-1073},
  doi = {10.3390/en16196887},
  url = {https://www.mdpi.com/1996-1073/16/19/6887},
  urldate = {2023-10-16},
  abstract = {The use of electrochemical cells is becoming more widespread, especially in the energy industry and battery energy storage systems (BESSs). As we continue to deploy BESSs, it becomes increasingly important for us to understand how these systems age and accurately predict their performance over time. This knowledge is essential for ensuring that the systems operate optimally and can be properly maintained. Since the structure of a BESS is different from a single electrochemical cell, the existing models at the cell level cannot predict and estimate the life of the BESS with suitable accuracy. Furthermore, the test protocols available at the cell level mostly cannot be executed at the BESS level for many reasons. Therefore, in this paper, a review of test protocols for building aging models for BESSs has been performed. After reviewing the protocols for a single electrochemical cell and addressing the differences between BESSs and cells, a review of the works performed on a larger scale has been carried out, and the possible ways for testing the BESS for aging models were investigated.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {battery,battTstProtocol,intro},
  note = {Pakjoo23battTestAgeBESSrvw
\par
Review of battery aging testing protocols, and how to make them shorter. ~How BESS is different than EV.
\par
Make them shorter by data synthesis: (Tang et al., 2021)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Pakjoo23ReviewTestingElectrochemical.pdf}
}

@misc{AEMO21frcstAccuracyRprt,
  title = {Forecast {{Accuracy Report}} 2021: {{Review}} of the 2020 {{Demand}}, {{Supply}} and {{Reliability Forecasts}}},
  shorttitle = {Forecast {{Accuracy Report}} 2021},
  author = {AEMO},
  year = {2021},
  month = nov,
  publisher = {AEMO},
  url = {https://aemo.com.au/-/media/files/electricity/nem/planning_and_forecasting/accuracy-report/forecast-accuracy-report-2021.pdf},
  abstract = {Each year, AEMO publishes an assessment of forecast accuracy to help inform its Forecast Improvement Plan  and build confidence in the forecasts produced. This 2021 Forecast Accuracy Report primarily assesses the  accuracy of AEMO's 2020 Electricity Statement of Opportunities (ESOO)1 for each region in the National  Electricity Market (NEM). The report assesses the accuracy of forecast drivers and models of demand and  supply that influenced the reliability assessments for the 2020-21 financial year, in particular the summer},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\AEMO21frcstAccuracyRprt.pdf}
}

@misc{AEMO22frcstAccuracyRprt,
  title = {Forecast {{Accuracy Report}} 2022: {{Review}} of the 2021 Demand, Supply  and Reliability Forecasts for the {{National Electricity Market}}},
  shorttitle = {Forecast {{Accuracy Report}} 2022},
  author = {AEMO},
  year = {2022},
  month = dec,
  publisher = {AEMO},
  url = {https://aemo.com.au/-/media/files/electricity/nem/planning_and_forecasting/accuracy-report/forecast-accuracy-report-2021.pdf},
  abstract = {Each year, AEMO publishes an assessment of forecast accuracy to help inform its Forecast Improvement Plan  and build confidence in the forecasts produced. This 2022 Forecast Accuracy Report primarily assesses the  accuracy of AEMO's 2021 Electricity Statement of Opportunities (ESOO)1,2 for each region in the National  Electricity Market (NEM). The report assesses the accuracy of forecast drivers and models of demand and supply  that influenced the reliability assessments for the 2021-22 financial year, in particular the summer},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\AEMO22frcstAccuracyRprt.pdf}
}

@book{Gorman18nemosisAEMO,
  title = {{{NEMOSIS}} -- {{NEM Open Source Information Service}}; Open-Source Access to {{Australian National Electricity Market Data}}},
  author = {Gorman, Nicholas and Haghdadi, Navid and Bruce, Anna and Macgill, Iain},
  year = {2018},
  month = dec,
  url = {https://www.researchgate.net/publication/329798805_NEMOSIS_-_NEM_Open_Source_Information_Service_open-source_access_to_Australian_National_Electricity_Market_Data},
  abstract = {The Australian National Electricity Market (NEM) is undergoing rapid change as utility-scale wind, solar and now battery storage technologies enter the market. The impact of these changes on the operation of the NEM is of interest to a potentially wide range of stakeholders including researchers, market participants, consultants and energy NGOs. Detailed operational data is essential in assisting these stakeholders to understand these impacts and possible future outcomes in the NEM. Open data and open-source software for energy are increasingly seen as key to improving transparency and reproducibility, and therefore productive discussion on energy policy and market reform amongst a wide range of stakeholders. There is an excellent opportunity to further these aims in the NEM, due to high levels of transparency around market data, with the Australian Energy Market Operator (AEMO) releasing nearly all the data used in the central dispatch process. However, a significant processing and interpretation overhead exists when analysing the large and often complex datasets made available to the public. This paper reports on a new open-source software tool, NEMOSIS NEM Open Source Information Service, NEMOSIS, which aims to lower this overhead and improve the accessibility of data sourced from AEMO. While a number of similar proprietary tools exist and are widely used by industry stakeholders, NEMOSIS offers additional benefits. It is completely open to modification and extension with the source code available on GitHub, while also being highly portable with the ability to package the tool as an executable file and operate it via a graphical user interface. It complements some other open source data tools for the NEM by providing access to a wide selection of data without the need for the user to have any coding knowledge. To demonstrate the use of NEMOSIS, this paper presents two examples of how it can be used to assess utility-scale renewable generator performance; firstly, with a simple example that outlines how to access 5 minute dispatch data for all scheduled generators in the NEM, and then, a second more complex example explains the implementation of a custom data table of plant performance statistics.},
  keywords = {AEMO,noted,software},
  note = {Gorman18nemosisAEMO
\par
Describes the NEM database, and walks through a compule data-getting examples using some GUI, not python. ~Explains some terms, best go go through the markups, I think.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gorman18nemosisAEMO.pdf}
}

@article{Parakash23nemseerAEMO_GitHub,
  title = {{{NEMSEER}}},
  author = {Prakash, Abhijith},
  year = {2023},
  doi = {10.5281/zenodo.8181172},
  url = {https://github.com/UNSW-CEEM/NEMSEER},
  urldate = {2023-10-20},
  abstract = {What's Changed [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/UNSW-CEEM/NEMSEER/pull/47 New Contributors @pre-commit-ci made their first contribution in https://github.com/UNSW-CEEM/NEMSEER/pull/47 Full Changelog: https://github.com/UNSW-CEEM/NEMSEER/compare/v1.0.5...v1.0.6},
  langid = {english},
  note = {Parakash23nemseerAEMO\_GitHub
\par
Opensource for downloading AEMO forecasts etc. ~There's a paer in review too 
\par
\begin{itemize}

\item 
\par
From the nice timing graph on the GitHub pate, it looks like I should look at the tables:
\par
\begin{itemize}

\item 
\par
PASA tables:\\
''\emph{can operational demand be met in the forecast horizon with a sufficient safety (reserve) margin?"})''\\
STPASA and PDPASA contain many of the same columns. ~Huh.\\
Some of the explanations below came from Google Gemni, so{\dots}
\par
\begin{itemize}

\item 
\par
\href{https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS\%20Data\%20Model\%20Report_files/MMS_349.htm\#1}{STPASA\_SOLUTION}:\\
longer-term perspective for market planning and security assessments.\\
next~\textbf{eight days}~with a~\textbf{30-minute resolution}.
\par
\textbf{market planning}~and~\textbf{security of supply}~assessments.
\par
broader view of potential system imbalances over a longer timeframe than PDPASA
\par
\textbf{p}ublished twice daily (at 9:30 AM and 4:30 PM AEST)
\par
\begin{itemize}

\item 
\par
\href{https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS\%20Data\%20Model\%20Report_files/MMS_350.htm\#1}{STPASA\_CASESOLUTION}
\par
STPASA\_CASESOLUTION holds one record containing results pertaining to each entire solution
\par
\href{https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS\%20Data\%20Model\%20Report_files/MMS_351.htm\#1}{STPASA\_CONSTRAINTSOLUTION}
\par
STPASA\_CONSTRAINTSOLUTION shows binding and violated constraint results from the capacity evaluation, including the RHS value.
\par
\href{https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS\%20Data\%20Model\%20Report_files/MMS_352.htm\#1}{STPASA\_INTERCONNECTORSOLN}
\par
STPASA\_INTERCONNECTORSOLN shows the results of the capacity evaluation for Interconnectors, including the calculated limits for the interval.
\par
\href{https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS\%20Data\%20Model\%20Report_files/MMS_353.htm\#1}{STPASA\_REGIONSOLUTION}
\par
STPASA\_REGIONSOLUTION shows the results of the regional capacity, maximum surplus reserve and maximum spare capacity evaluations for each period of the study.
\par
\end{itemize}

\item 
\par
\href{https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS\%20Data\%20Model\%20Report_files/MMS_481.htm\#1}{PDPASA}\\
real-time picture of short-term system adequacy for dispatch purposes.
\par
next~\textbf{40 hours}~with a~\textbf{30-minute resolution}.\\
\textbf{regional prices}~and~\textbf{dispatch targets}~for scheduled and semi-scheduled generating units~\\
adequacy of~\textbf{generation resources}~based on~\textbf{forecasted demand}~and~\textbf{unit availabilitie\\
}updated more often (30 mins for some tables) than STPASA
\par
\begin{itemize}

\item 
\par
\href{https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS\%20Data\%20Model\%20Report_files/MMS_482.htm\#1}{PDPASA\_CASESOLUTION}
\par
The top-level table identifying a PDPASA case, reporting options applied in the case and summary results
\par
\href{https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS\%20Data\%20Model\%20Report_files/MMS_483.htm\#1}{PDPASA\_CONSTRAINTSOLUTION}
\par
PDPASA\_CONSTRAINTSOLUTION shows binding and violated constraint results from the capacity evaluation, including the RHS value.
\par
\href{https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS\%20Data\%20Model\%20Report_files/MMS_484.htm\#1}{PDPASA\_INTERCONNECTORSOLN}
\par
PDPASA\_INTERCONNECTORSOLN shows the results of the capacity evaluation for Interconnectors, including the calculated limits for the interval.
\par
\href{https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS\%20Data\%20Model\%20Report_files/MMS_485.htm\#1}{PDPASA\_REGIONSOLUTION}
\par
The PDPASA region solution data
\par
\item 30 minute updates
\item \href{https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS\%20Data\%20Model\%20Report_files/MMS_485.htm\#1}{PDPASA\_REGIONSOLUTION} seems most interesting? ~Many fields that seem relevant

\end{itemize}

\item 
\par
\href{https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS\%20Data\%20Model\%20Report_files/MMS_272.htm\#1}{PREDISPATCH}
\par
\begin{itemize}

\item Tons of tables, but{\dots}
\item 
\par
\href{https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS\%20Data\%20Model\%20Report_files/MMS_285.htm\#1}{PREDISPATCHREGIONSUM} is at the regional level
\par
forecast demand (total demand) and Frequency Control Ancillary Services (FCAS) requirements (specifically, for the Raise Regulation and Lower Regulation Ancillary Services plus improvements to demand calculations)\\
updates each half-hour with the latest Pre-Dispatch details for the remaining period.
\par
Regional demand = Total Demand + Dispatchable Load
\par
\item 
\par
Good tables columns (?):
\par
TOTALDEMAND
\par
NUMBER(15,5)
\par
Total demand in MW for period (less normally on loads)
\par
AVAILABLEGENERATION
\par
NUMBER(15,5)
\par
Aggregate generation bid available in region
\par
AVAILABLELOAD
\par
NUMBER(15,5)
\par
Aggregate load bid available in region
\par
DEMANDFORECAST
\par
NUMBER(15,5)
\par
Delta MW value only
\par
DISPATCHABLEGENERATION
\par
NUMBER(15,5)
\par
Generation dispatched in period
\par
DISPATCHABLELOAD
\par
NUMBER(15,5)
\par
Load dispatched in period
\par
NETINTERCHANGE
\par
NUMBER(15,5)
\par
Net interconnector flow from the regional reference node
\par
EXCESSGENERATION
\par
NUMBER(15,5)
\par
Excess generation in period / Deficit generation if VOLL
\par
\end{itemize}

\end{itemize}

\end{itemize}

\item PD PASA and ST PASA to be merged by July 31, 2025

\end{itemize}

\begin{itemize}

\item GitHub: \href{https://github.com/UNSW-CEEM/NEMSEER}{https://github.com/UNSW-CEEM/NEMSEER}
\item 
\par
NEMSEER used by
\par
\begin{itemize}

\item (Yurdakul and Billimoria, 2023)
\item (Prakash, 2022)

\end{itemize}

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Parakash23nemseerAEMO_GitHub.pdf}
}

@article{Gorman22nempyDsptchMdlAEMO,
  title = {Nempy: {{A Python}} Package for Modelling the {{Australian National Electricity Market}} Dispatch Procedure},
  shorttitle = {Nempy},
  author = {Gorman, Nicholas and Bruce, Anna and MacGill, Iain},
  year = {2022},
  month = feb,
  journal = {Journal of Open Source Software},
  volume = {7},
  number = {70},
  pages = {3596},
  issn = {2475-9066},
  doi = {10.21105/joss.03596},
  url = {https://joss.theoj.org/papers/10.21105/joss.03596},
  urldate = {2023-10-21},
  keywords = {AEMO,obsLitNote,optimization,Price forecasting,software},
  note = {Gorman22nempyDsptchMdlAEMO
\par
Software modeling AEMO dispatch. ~Could use to estimate demand-supply curves for liquidity and volatility features for price forecast.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gorman22nempyDsptchMdlAEMO.pdf}
}

@inproceedings{Datta16ElectricityMarketPricespike,
  title = {Electricity Market Price-Spike Classification in the Smart Grid},
  booktitle = {2016 {{IEEE Power}} \& {{Energy Society Innovative Smart Grid Technologies Conference}} ({{ISGT}})},
  author = {Datta, Aditi R. and Datta, Sohom},
  year = {2016},
  month = sep,
  pages = {1--5},
  issn = {2472-8152},
  doi = {10.1109/ISGT.2016.7781161},
  url = {https://ieeexplore.ieee.org/abstract/document/7781161},
  urldate = {2023-10-21},
  abstract = {Accurate electricity market price forecasting is essential for the market participants in the present competitive deregulated electric power industry. In this research, the smart meter data is concomitantly used with the market data to improve price spike forecast. Modern data mining tools like Na\&ve-Bayesian classifier, Random Forest and Artificial Neural Network algorithms have been used for price-spike forecasting. The Australian Energy Market Operator (AEMO) data of Victorian region has been used to train and test the price-spike forecasting methodology. The results show that the usage of smart meter data can have a significant impact on the accuracy of the price-spike forecasts.},
  note = {Datta16priceSpikeClassifAEMO
\par
One-step (30 mins)-ahead binary spike forecast for AEMO Vic. ~Results seem good, and since one-step-ahead might be most imp. for batteries, relevant. ~So \emph{maybe}, the features are relevant. ~But study done over {\textasciitilde}2013 data, and there have been many market changes since then e.g. the latest is (Connor, 2021).
\par
\begin{itemize}

\item 
\par
``spike detection'':
\par
\begin{itemize}

\item 
\par
2 stdev, only pos prices
\par
\begin{itemize}

\item better to use use of time-varyiing stdev like in (Sebasti{\'a}n et al., 2023)
\item or sliding norm as in (Vu et al., 2021)

\end{itemize}

\item a binary classification target?

\end{itemize}

\item 
\par
``spike classification'':
\par
\begin{itemize}

\item ``not the quantification''
\item is a binary predicted input to quantification/forecasting''
\item from eq (2), seems to be feature for a one-step-ahead forecast (step seems to be 30 mins)

\end{itemize}

\item 
\par
``spike quantification'':
\par
\begin{itemize}

\item not done in this paper
\item Fig 1 explanation suggests this is ``forecasting''

\end{itemize}

\item covers MANY price ``classification'' algorithms
\item 
\par
index (normalized) spike feats for classification and ``forecasting'' (but not done here?). All simple to implement.
\par
\begin{itemize}

\item some are one-step-ahead (eq 2) so are these measurements?
\item some are zer step ahead (eq 3), so are these forecasted inputs?

\end{itemize}

\item 
\item 
\par
``meter'' means one of the five Vic region distrib networks
\par
\begin{itemize}

\item to indicate local congestion
\item is this relevant to NEM now?

\end{itemize}

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Datta16priceSpikeClassifAEMO.pdf}
}

@article{Mackenzie22ResistanceFutileImplementing,
  title = {Resistance Is Futile - Implementing Automated Renewable Trading in Competitive Energy Markets},
  author = {Mackenzie, H. J.},
  year = {2022},
  month = jan,
  pages = {526--533},
  publisher = {IET Digital Library},
  doi = {10.1049/icp.2022.2821},
  url = {https://digital-library.theiet.org/content/conferences/10.1049/icp.2022.2821},
  urldate = {2023-10-21},
  abstract = {The recent introduction of five-minute settlements into the Australian National Electricity Market (NEM) in October 2021 saw a massive increase in the number of offers submitted to the market at every five-minute Dispatch Interval. While many of these offers were from non-renewable generators, a significant proportion was now from Variable Renewable Energy (VRE) Generators that had rarely submitted offers to the market before the market change. VRE generators use various approaches to actively manage their market risks, including internal and external multi-generator operations centres, advanced Machine Learning optimisers and simple single generator revenue optimisers. This paper will discuss the advantages and risks of all three approaches, how each has operated in the market, and how strategies adapt to the dynamic competitive energy and ancillary markets. Surprisingly, the simple and usually lower-cost option of a simple revenue optimisation seems to provide a low-risk, reliable and robust alternative for VRE generators and can also be combined with other systems to meet VRE compliance and other operational priorities generators. More sophisticated systems are required for multi-modal and Battery Energy Storage Systems (BESS) as the forecast horizon needs to consider an entire trading day operations due to the physical constraints of the Lithium-Ion technology. However, the same revenue optimisation approach can work for other battery technologies, such as supercapacitors, that do not have the cycle constraints of Lithium-Ion and are especially suited to providing frequency and contingency ancillary services.},
  langid = {english}
}

@inproceedings{Kato23ExploringModelsElectricity,
  title = {Exploring {{Models}} of {{Electricity Price Forecasting}}: {{Case Study}} on {{A FCAS Market}}},
  shorttitle = {Exploring {{Models}} of {{Electricity Price Forecasting}}},
  booktitle = {Companion {{Proceedings}} of the 14th {{ACM International Conference}} on {{Future Energy Systems}}},
  author = {Kato, Kenshiro and Iwabuchi, Koki and Watari, Daichi and Zhao, Dafang and Nishikawa, Hiroki and Taniguchi, Ittetsu and Onoye, Takao},
  year = {2023},
  month = jun,
  series = {E-{{Energy}} '23 {{Companion}}},
  pages = {115--119},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3599733.3600258},
  url = {https://doi.org/10.1145/3599733.3600258},
  urldate = {2023-10-20},
  abstract = {VPPs (Virtual Power Plants) play an important role in balancing supply and demand. In order to make VPP revenue, it is necessary to forecast market prices and bidding energy for supply and demand adjustment markets, called FCAS (Frequency Control Ancillary Service) markets. However, price forecasting for FCAS markets is still challenging because they have multiple different response times and one price, directly and indirectly, influences each other. There is no study on electricity price forecasting in FCAS markets, and a novel forecasting model considering not only its price but also the other prices of the different response times is necessary. This work presents a market price forecasting model for a FCAS market by exploring the forecasting models derived from a wholesale market, and then it takes into account the markets with different response times as well as the target one from AEMO (Australian Energy Market Operator). Through the experiments, our forecasting model achieves 7.8\$/MWh of RMSE on the electricity price in AEMO's 6-Second-Raise market. The proposed forecasting model reduces RMSE by 80\% compared to the forecast price published by AEMO.},
  isbn = {9798400702273}
}

@article{Fattah20elecMktOvrvwDA_ID,
  title = {Overview of Day-Ahead and Intraday Electricity Market},
  author = {Fattah, Md Abu},
  year = {2020},
  url = {https://trepo.tuni.fi/handle/10024/124424},
  urldate = {2023-10-21},
  abstract = {Electricity is a commodity, which has many different attributes from other products because of its unique features. It needs a special infrastructure for production, transportation, and consumption.  In this thesis, the electricity system is discussed in two subsystems. The flow of electricity happens in the technical subsystem, and monetary value is discussed in the economic subsystem.     Here is an attempt to do a comprehensive review of three different marketplaces: AEMO in Australia, CAISO in California, and Nord Pool in Nordic countries based on literature review and available public information. In these marketplaces, there is continuous or upcoming reform of the energy system, which can be strengthened by integrating different perspectives from current markets. Characteristics for a successful electricity market are also proposed at the end of the thesis.     Currently, CAISO has a centralized wholesale energy market, while Nord Pool and AEMO have also decentralized properties. The critical issue with the centralized CAISO market is that they don't have intra-day pricing that can be updated continuously when the renewable generation changes. Whereas Nord Pool and AEMO with their decentralized intraday market, has the flexibility to adjust the price according to renewable energy production as close to real-time. This iterative intra-day trading can address the coordination problem related to wind and solar power generation. The downside is that there is a risk of network constrain, which can be improved by analysing the network in more detail.     The limitation of this study is using only three marketplaces where in the future it can be extended to more marketplaces},
  copyright = {This publication is copyrighted. You may download, display and print it for Your own personal use. Commercial use is prohibited.},
  langid = {english},
  keywords = {AEMO,intro,obsLitNote},
  annotation = {Accepted: 2020-12-21T06:47:06Z},
  note = {Fattah20elecMktOvrvwDA\_ID
\par
Compares AEMO, Nordpool, and CAISO. ~I'm involved with 2 of them, so maybe worth a read.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Fattah20elecMktOvrvwDA_ID.pdf}
}

@inproceedings{Alvarez21elecMktUltraShrtAEMO,
  title = {Ultra-Short Term Wholesale Electricity Price Forecasting through Deep Learning},
  booktitle = {2021 {{IEEE PES Innovative Smart Grid Technologies}} - {{Asia}} ({{ISGT Asia}})},
  author = {Alvarez, Ana and Luo, Wei and Fryer, Simon},
  year = {2021},
  month = dec,
  pages = {1--5},
  issn = {2378-8542},
  doi = {10.1109/ISGTAsia49270.2021.9715585},
  url = {https://ieeexplore.ieee.org/abstract/document/9715585},
  urldate = {2023-10-21},
  abstract = {The increasing mix of renewable generation in the Australian National Electricity Market (NEM) electricity market presents economic challenges because generators are subject to extreme price drops in the short term dispatch cycles. The traditional forecasting models are inadequate in predicting such extreme price events due to the complexity in how numerous fac-tors impacting the short-term price. This paper's aim is to answer the degree of how deep learning can contribute to forecasting prices thresholds in a volatile real-time electricity price market. Additional research in this area is required post NEM transition to a unified 5 minute dispatch and bidding cycle. Effective short term forecasting can enable the transition from manual to automatic bidding fully optimising the revenue potential for generators. The algorithms presented in this paper present an ultra-short horizon prediction forecasting of 15 minutes into the future. The Queensland wholesale electricity market price is used to present the research findings. Our results show deep learning models can achieve significant performance gains.},
  note = {Alvarez21elecMktUltraShrtAEMO
\par
Deep learning price spike forecaster for AEMO NEM, specifically looking for extreme prices.
\par
Highly relevant but I haven't found a pdf for it.}
}

@techreport{Sanders23spotMktTimetableAEMO,
  title = {Spot {{Market Operations Timetable}}},
  author = {Sanders, Michael},
  year = {2023},
  month = apr,
  institution = {AEMO},
  url = {https://aemo.com.au/-/media/files/stakeholder_consultation/consultations/nem-consultations/2023/reliability-forecasting-guidelines-and-methodology-consultation/final/spot-market-operations-timetable.pdf?la=en},
  urldate = {2023-10-23},
  abstract = {This is the spot market operations timetable (Timetable) made under clause 3.4.3 of the National Electricity Rules (NER). The Timetable specifies who must do what by when in the spot market. The Timetable applies to AEMO and all Registered Participants. This Timetable has effect for only the purposes set out in the NER. The NER},
  langid = {english},
  keywords = {AEMO},
  note = {Sanders23spotMktTimetableAEMO
\par
Where View got her timetable?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sanders23spotMktTimetableAEMO.pdf}
}

@techreport{Nelson21demandTermsDataMdlAEMO,
  title = {Demand {{Terms}} in {{EMMS Data Model}}},
  author = {Nelson, Brian},
  year = {2021},
  month = apr,
  institution = {AEMO},
  url = {https://aemo.com.au/-/media/files/electricity/nem/security_and_reliability/dispatch/policy_and_process/demand-terms-in-emms-data-model.pdf?la=en},
  urldate = {2023-10-23},
  abstract = {This paper describes the composition, use and publication of the different types of demands and associated terms used in AEMO's Electricity Market Management Systems (EMMS) Data Model for National Electricity Market (NEM) participants or other interested parties. The particular focus of this paper is ``as generated'' demand, although other demands are defined. Other organisations such as Network Service Providers or Jurisdictional Planning Bodies (JPBs) might have a different definition for the same terms or associated terminology discussed in this paper. This paper does not delve into the differences. For definitional purposes, all references to ``demand'' in this paper equally apply to ``consumption''1.},
  keywords = {AEMO,noted},
  note = {Nelson21demandTermsDataMdlAEMO
\par
Said to be different from terms used in NEM (this is EMMS). ~This is probably irrelevant to parsing data in the MMSDM that NEMSEER looks like. ~NEMSEER docs refer you to the MMSDM data model; there is \href{https://aemo.com.au/-/media/files/electricity/nem/5ms/systems-workstream/2020/mms-data-model-v50-release-notes.pdf?la=en}{another doc describing the EMMS data model}.
\par
\subsubsection{According to Bing Chat}

\par
The~\textbf{Australian Energy Market Operator (AEMO)}~is responsible for managing the~\textbf{National Electricity Market (NEM)}~in Australia.~\href{https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem}{The NEM is a wholesale electricity market and the physical power system that operates in New South Wales, the Australian Capital Territory, Queensland, South Australia, Victoria, and Tasmania~}\textsuperscript{\href{https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem}{1}}.
\par
The~\textbf{Electricity Market Management Systems (EMMS)}~is a data model used by NEM participants to manage their electricity supply and demand.~\href{https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem/market-operations/dispatch-information}{It is used to manage the dispatch of electricity from generators to consumers~}\textsuperscript{\href{https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem/market-operations/dispatch-information}{2}}.
\par
\href{https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem}{In summary, the NEM is a wholesale electricity market and the physical power system that operates in Australia, while }\href{https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem}{EMMS}\href{https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem}{ }\href{https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem}{is a data model used by NEM participants to manage their electricity supply and demand~}\textsuperscript{\href{https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem}{1}\href{https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem/market-operations/dispatch-information}{2}}.
\par
Learn more:
\par
\href{https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem}{1. aemo.com.au}\href{https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem/market-operations/dispatch-information}{2. aemo.com.au}\href{https://www.theguardian.com/australia-news/2023/apr/28/record-levels-of-renewable-energy-help-bring-down-australias-energy-prices-says-aemo}{3. theguardian.com}\href{https://aemo.com.au/-/media/files/electricity/nem/security_and_reliability/ancillary_services/2023/october-2023-industry-go-live-plan-final.pdf}{4. aemo.com.au}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nelson21demandTermsDataMdlAEMO.pdf}
}

@misc{AEMO21glossary,
  title = {Glossary: {{AEMO}} So\_op\_2000},
  author = {AEMO},
  year = {2021},
  month = oct,
  url = {https://aemo.com.au/-/media/files/electricity/nem/security_and_reliability/power_system_ops/procedures/so_op_2000-glossary.pdf?la=en},
  urldate = {2023-10-23},
  note = {AEMO21glossary
\par
Also compare to: \href{obsidian://open?vault=Obsidian\%20Share\%20Vault&file=work\%2FpriceFrcstAEMO\%2FNEMSEER\%20AEMO\%20Data\%20Downloader}{NEMSEER glossary}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\AEMO21glossary.pdf}
}

@techreport{Gatt23loadForecastingAEMO,
  title = {Load {{Forecasting}}},
  author = {Gatt, Michael},
  year = {2023},
  number = {SO\_OP\_3710},
  institution = {AEMO},
  url = {https://aemo.com.au/-/media/files/electricity/nem/security_and_reliability/power_system_ops/procedures/so_op_3710-load-forecasting.pdf?la=en},
  abstract = {This Load Forecasting procedure is designated as a power system operating procedure under  clause 4.10.1 of the National Electricity Rules (NER), and has effect as set out in the NER. The  NER and the National Electricity Law prevail over this document to the extent of any  inconsistency. The purpose of this procedure is to provide information about the pre-dispatch and short term  load forecasts, which AEMO is required to prepare for each region for the following timeframes: {$\bullet$} Each day for the day ahead -- Pre-dispatch forecast {$\bullet$} Each day for the period two to seven days ahead -- Short term forecast These forecasts are published in accordance with the timetable},
  langid = {english},
  file = {C:\Users\scott\Zotero\storage\JLLRTTXC\Gatt - 2023 - Executive General Manager Operations.pdf}
}

@techreport{Gatt23dispatchProcAEMO,
  title = {Dispatch {{Procedure}}},
  author = {Gatt, Michael},
  year = {23},
  month = aug,
  number = {SO\_OP\_3705},
  url = {https://aemo.com.au/-/media/files/electricity/nem/security_and_reliability/power_system_ops/procedures/so_op_3705-dispatch.pdf?la=en},
  urldate = {2023-10-23},
  abstract = {This Dispatch procedure is a power system operating procedure under clause 4.10 of the National  Electricity Rules (NER).  This procedure has effect only for the purposes set out in the NER. The NER and the National  Electricity Law prevail over this procedure to the extent of any inconsistency. The purpose of this procedure is to provide instructions and guidelines covering market operations  in relation to the operation of the power system.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gatt23dispatchProcAEMO.pdf}
}

@techreport{Gatt23preDispatchAEMO,
  title = {Pre-Dispatch Procedure},
  author = {Gatt, Michael},
  year = {2023},
  month = jun,
  number = {SO\_OP\_3704},
  url = {https://aemo.com.au/-/media/files/electricity/nem/security_and_reliability/power_system_ops/procedures/so_op_3704-predispatch.pdf?la=en},
  urldate = {2023-10-23},
  abstract = {TThe purpose of this Pre-dispatch Procedure is to provide an overview of the pre-dispatch process in terms of the inputs to the process and the outputs provided by the process. It does  not attempt to describe in detail the functional design of the pre-dispatch process nor does it  provide information on the 5-minute pre-dispatch process. This Pre-dispatch Procedure is a power system operating procedure under clause 4.10.1 of the  National Electricity Rules (NER). If there is any inconsistency between this Procedure and the  NER, the NER will prevail to the extent of that inconsistency. This Procedure applies to applies to AEMO and all Registered Participants},
  keywords = {obsLitNote},
  note = {Gatt23preDispatchAEMO
\par
Describes what's available from the predispatch procedure. ~Many things but after a few minutes of skimming, these seem useful:
\par
For every 30 minutes in predispatch period.
\par
\begin{itemize}

\item prices and price sensitivities
\item loads and demand response
\item FCAS info
\item Constraint marginal value, which is an upper bound on price impact per MW of congestion (AEMO, 2024)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gatt23preDispatchAEMO.pdf}
}

@article{Hoyer23xarray,
  title = {Xarray: {{N-D}} Labeled Arrays and Datasets in {{Python}}.},
  author = {Hoyer, Stephan and Fitzgerald, Clark and Hamman, Joe and {others}},
  year = {2023},
  month = oct,
  series = {Version V2023.10.1},
  doi = {10.5281/zenodo.10023467},
  url = {https://zenodo.org/records/10023467},
  urldate = {2023-10-23},
  abstract = {N-D labeled arrays and datasets in Python.},
  langid = {english},
  keywords = {AEMO,noted,software},
  note = {Hoyer23xarray
\par
Used by NEMSEER for AEMO forecast data
\par
\texttt{xarray}~is among other things, a high level interface to netCDF files, and other multi-axis data that have labeled dimensions (lat, lon, time,{\dots}) ~One of the cool things is that it's good at storing metadata. ~Underneath, the data is stored in np.arrays.
\par
\section{Compared to pandas}

\par
This is kind-of opposed pandas DataFrames, which ~are more for ``tabular'' data in CSVs or SQL: 2D, and have labeled ~rows and columns (but you can do them both MultiIndex, and there is ``tall format'', in fact xarrays are easily converted to MultiIndex pd.DataFrames, so{\dots}). ~And, until pandas 2.0, I think, pandas is also np.array underneath.
\par
\section{Notes from the tutorial: \href{https://tutorial.xarray.dev/overview/xarray-in-45-min.html}{Xarray in 45 minutes} (lib{\textbackslash}python{\textbackslash}sandbox{\textbackslash}xarray)}

\subsection{Data hierarchy}

\par
From bottom to top, it's:
\par
\begin{itemize}

\item \textbf{numpy.array:} n-dim, where n are the number of indices into the data (\emph{.data}). The contents are the values of a data variable e.g. `temperature'
\item \textbf{DataArray}: all the data for the data variable e.g. temperature, along with its indices (\emph{.coords }or \emph{.dims}, I don't understand the difference) and metainformation (\emph{.attrs})
\item \textbf{DataSet}: a dict-like thing containing all the DataArrays, with a key for dimension name (\emph{.dims}) ~e.g. `temperature'; the value is the DataArray. ~I think the indices are assumed the same for all of those DataArrays. ~True?

\end{itemize}

\par
This makes indexing easy
\par
\begin{quotation}

\par
\# plot the first timestep\\
lat = ds.air.lat.data ~\# numpy array\\
lon = ds.air.lon.data ~\# numpy array\\
temp = ds.air.data ~\# numpy array

plt.figure()\\
plt.pcolormesh(lon, lat, temp[0, :, :]);
\par
\end{quotation}

\subsection{Interesting bits}

\begin{itemize}

\item 
\par
2D coordinates e.g. lat and lon, are described by row vectors, one for each dimension, of length N and D
\par
\begin{itemize}

\item They're blow out to NxD cells as you might expect: as arguments to pandas-like 2D plot functions
\item broadcasted when computing cross product

\end{itemize}

\item 
\par
computational arguments with mismatching coordinates
\par
\begin{itemize}

\item easy to check for matches with .align()
\item if unaligned, you'll get an empty dataarray in the computation output

\end{itemize}

\item 
\par
can do pandas-like things
\par
\begin{itemize}

\item 
\par
groupby and then reduce
\par
\begin{itemize}

\item TODO: try doing this to compute forecast error vs. horizon. ~Would be nice to use \href{https://github.com/raybellwaves/xskillscore-tutorial/tree/master}{xskillscore}

\end{itemize}

\item resample (for time): up or down
\item rolling: mean and others, I think, like in pandas
\item coarsen (downsample)
\item weighted: weight before reducing

\end{itemize}

\item time axes have datetime index things like time.season (you get a new coordinate in the aggregation like ``season''
\item plots methods are set up to do 2D plots w/ just .plot(), and also grouped plots e.g. 2D temp w/ plot for each season is just: ~\emph{seasonal\_mean.air.plot(col="season", col\_wrap=2);}
\item 
\par
can save in many formats: netCDF, HDF5, Matlab, \href{https://cupy.dev/}{CuPy }for GPU, even \href{https://docs.xarray.dev/en/stable/user-guide/io.html}{Amazon S3, using Zarr}{\dots}
\par
\begin{itemize}

\item TODO: try/thinkabout how ot use xarray with S3, considering performance, as discussed in link above

\end{itemize}

\item simple conversion to multi-index dataframe
\item 
\par
\href{https://docs.xarray.dev/en/stable/ecosystem.html}{tons of Xarray related projects}, for ML, sklearn, geoscoence, chem, visualization
\par
\begin{itemize}

\item 
\par
easy to make interactive plots using \href{https://hvplot.holoviz.org/}{hvplot} library
\par
\begin{itemize}

\item TODO: try hvplot (works with pandas too). ~Maybe simpler than plotly?

\end{itemize}

\item cf\_array package lets you access all ``cf'' attributes, which is maybe longer names like ``longitude?''
\item CuPy for GPU and something else

\end{itemize}

\end{itemize}}
}

@article{Nemseer23pythonAEMOpaper,
  title = {{{NEMSEER}}: {{A Python}} Package for Downloading and Handling Historical {{National Electricity Market}} Forecast Data Produced by the {{Australian Energy Market Operator}}},
  shorttitle = {{{NEMSEER}}},
  author = {Prakash, Abhi and Bruce, Anna and MacGill, Iain},
  year = {2023},
  month = dec,
  journal = {Journal of Open Source Software},
  doi = {10.21105/joss.05883},
  url = {https://joss.theoj.org/papers/5ba29c4a5cfec8721c876587699ab719},
  urldate = {2023-10-24},
  abstract = {In electrical power systems and their associated market frameworks, actions close to or during real-time (i.e. the time of power delivery) may be critical to ensuring that: 1. Supply and demand are balanced, and that the power system is operated within its technical envelope. 2. Market participants can maximise revenues from their generating and/or demand-side resources.},
  langid = {english},
  keywords = {AEMO,electricity_markets,Price forecasting,software},
  note = {Nemseer23pythonAEMOpaper},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nemseer23pythonAEMOpaper.pdf}
}

@misc{Dyson22pricePumpSelfFrcst,
  title = {"{{Rise}} of the {{Machines}}" - the Increasing Role of Auto-Bidding and Self-Forecasting in the Modern-Day {{NEM}}},
  author = {Dyson, Jonathon},
  year = {2022},
  month = mar,
  journal = {WattClarity},
  url = {https://wattclarity.com.au/articles/2022/03/rise-of-the-machines-the-increasing-role-of-auto-bidding-and-self-forecasting-in-the-modern-day-nem/},
  urldate = {2023-10-31},
  abstract = {On Thursday 17th March, Jonathon Dyson presented at the CEC Wind Industry Forum in Melbourne about the increasing role of auto-bidding and self-forecasting in the modern-day NEM. In this article, he shares some of the key points from that presentation.},
  langid = {australian},
  keywords = {AEMO,noted,priceFrcst},
  note = {Dyson22pricePumpSelfFrcst
\par
Why AEMO started penalizing forecasters for errors, instead of rewarding them for ``errors that went in the right direction,'' like EPEX does with Reguleistung.
\par
Solar and Wind traders were able to predict times when over-frequency was likely, and so they over-forecasted their own generation. ~Then, they got paid to curtail their over-forecast to what it actually would have been without curtailing. ~This is was called price pumping.
\par
\textbf{Also}
\par
\begin{itemize}

\item (Dungey et al., 2018)

\end{itemize}},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Dyson22pricePumpSelfFrcst.pdf;C\:\\Users\\scott\\Zotero\\storage\\S2YAIY9E\\rise-of-the-machines-the-increasing-role-of-auto-bidding-and-self-forecasting-in-the-modern-day.html}
}

@misc{Latief23volatileTransAEMO,
  title = {Transmission Line Issues Volatilise {{Australia}} Power Market},
  author = {Latief, Yusuf},
  year = {2023},
  month = oct,
  journal = {POWERGRID International},
  url = {https://www.power-grid.com/td/transmission/transmission-line-issues-volatilize-australias-power-market/},
  urldate = {2023-10-31},
  abstract = {Rystad Energy says the power market in Australia is the world's most volatile, a result of transmission line issues from natural disasters.},
  langid = {american},
  keywords = {AEMO,coal,electricity_markets,energy_storage,hydro_power,interesting_companies,Norway,noted,power_transmission,priceFrcst,solar_power,wind_power},
  note = {Latief23volatileTransAEMO
\par
Transmission outages make AEMO NEM the most volatile electricity market in the world. ~The main causes have been transmission outages dues to increasing natural disasters (extreme weather: winds and bushfires), along with unexpected coal generation outages. ~
\par
Other factors are RES solar power pushing down daytime prices, which then rocket at night when expensive natural gas is used. ~HOWEVER, this would not seem to contribute to what this article says is the study's definition of ``volatility'': difference between lowest and highest price in a given hour. ~If daytime and nighttime prices were consistently 10X apart, this would not increase ``volatility.'' ~Maybe the actual definition was just the high/low difference within a given day?
\par
Anyway, Norway's Rystad Energy did the study. ~I could look it up.
\par
\textbf{Also}
\par
\begin{itemize}

\item \href{https://www.evernote.com/l/AA0H5q11lI5D2IA6WaIKl4i/}{AI software company raises \$2.5M for grid resiliency}: predicts weather resilience of power grid

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Latief23volatileTransAEMO.pdf}
}

@inproceedings{VanDerDonckt22PlotlyResamp,
  title = {Plotly-{{Resampler}}: {{Effective Visual Analytics}} for {{Large Time Series}}},
  shorttitle = {Plotly-{{Resampler}}},
  booktitle = {2022 {{IEEE Visualization}} and {{Visual Analytics}} ({{VIS}})},
  author = {Van Der Donckt, Jonas and {Van der Donckt}, Jeroen and Deprost, Emiel and Van Hoecke, Sofie},
  year = {2022},
  month = oct,
  pages = {21--25},
  issn = {2771-9553},
  doi = {10.1109/VIS54862.2022.00013},
  url = {https://ieeexplore.ieee.org/abstract/document/9973221},
  urldate = {2023-11-06},
  abstract = {Visual analytics is arguably the most important step in getting acquainted with your data. This is especially the case for time series, as this data type is hard to describe and cannot be fully understood when using for example summary statistics. To realize effective time series visualization, four requirements have to be met; a tool should be (1) interactive, (2) scalable to millions of data points, (3) integrable in conventional data science environments, and (4) highly configurable. We observe that open source Python visualization toolkits empower data scientists in most visual analytics tasks, but lack the combination of scalability and interactivity to realize effective time series visualization. As a means to facilitate these requirements, we created Plotly-Resampler, an open source Python library. Plotly-Resampler is an add-on for Plotly's Python bindings, enhancing line chart scalability on top of an interactive toolkit by aggregating the underlying data depending on the current graph view. Plotly-Resampler is built to be snappy, as the reactivity of a tool qualitatively affects how analysts visually explore and analyze data. A benchmark task highlights how our toolkit scales better than alternatives in terms of number of samples and time series. Additionally, Plotly-Resampler's flexible data aggregation functionality paves the path towards researching novel aggregation techniques. Plotly-Resampler's integrability, together with its configurability, convenience, and high scalability, allows to effectively analyze high-frequency data in your day-to-day Python environment.},
  keywords = {graphics,noted,software},
  note = {VanDerDonckt22PlotlyResamp
\par
Speeds up plotly by downsampling points when zoomed out. ~Seems like it should also not downsample when zoomed in, but looking at AEMO price forecasting data, I'm not sure I saw that.
\par
The downsampler uses something like LTTB (Steinarsson, 2013), probably MinMaxLTTB (Van Der Donckt et al., 2023), but whatever it is, is seems much faster than other visually palatable techniques, and much better visually than (fast) ``decimation by throwing out N-1 points,'' where N is is the decimation ratio.
\par
But what I don't understand is how N is chosen in either technique.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\VanDerDonckt22PlotlyResamp.pdf}
}

@phdthesis{Steinarsson13timeSerVizDwnSamp,
  title = {Downsampling Time Series for Visual Representation},
  author = {Steinarsson, Sveinn},
  year = {2013},
  url = {https://skemman.is/handle/1946/15343},
  urldate = {2023-11-08},
  keywords = {noted,software},
  note = {Steinarsson13timeSerVizDwnSamp
\par
The LTTB algorithm in this thesis is related to the one used in the plotly-resampler plotly speeder-upper (Van Der Donckt et al., 2022). ~Plotly-Resample may actually be using the MinMaxLTTB algorith (Van Der Donckt et al., 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Steinarsson13timeSerVizDwnSamp.pdf}
}

@misc{VanDerDonckt23MinMaxLTTB,
  title = {{{MinMaxLTTB}}: {{Leveraging MinMax-Preselection}} to {{Scale LTTB}}},
  shorttitle = {{{MinMaxLTTB}}},
  author = {Van Der Donckt, Jeroen and Van Der Donckt, Jonas and Rademaker, Michael and Van Hoecke, Sofie},
  year = {2023},
  month = apr,
  number = {arXiv:2305.00332},
  eprint = {2305.00332},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2305.00332},
  urldate = {2023-11-08},
  abstract = {Visualization plays an important role in analyzing and exploring time series data. To facilitate efficient visualization of large datasets, downsampling has emerged as a well-established approach. This work concentrates on LTTB (Largest-Triangle-Three-Buckets), a widely adopted downsampling algorithm for time series data point selection. Specifically, we propose MinMaxLTTB, a two-step algorithm that marks a significant enhancement in the scalability of LTTB. MinMaxLTTB entails the following two steps: (i) the MinMax algorithm preselects a certain ratio of minimum and maximum data points, followed by (ii) applying the LTTB algorithm on only these preselected data points, effectively reducing LTTB's time complexity. The low computational cost of the MinMax algorithm, along with its parallelization capabilities, facilitates efficient preselection of data points. Additionally, the competitive performance of MinMax in terms of visual representativeness also makes it an effective reduction method. Experiments show that MinMaxLTTB outperforms LTTB by more than an order of magnitude in terms of computation time. Furthermore, preselecting a small multiple of the desired output size already provides similar visual representativeness compared to LTTB. In summary, MinMaxLTTB leverages the computational efficiency of MinMax to scale LTTB, without compromising on LTTB's favored visualization properties. The accompanying code and experiments of this paper can be found at https://github.com/predict-idlab/MinMaxLTTB.},
  archiveprefix = {arXiv},
  keywords = {noted,software},
  note = {Comment: The first two authors contributed equally. Submitted to IEEE VIS 2023
\par
VanDerDonckt23MinMaxLTTB
\par
From the references in the \href{https://github.com/predict-idlab/plotly-resampler\#citation-and-papers}{GitHub page} of Plotly-Resampler (Van Der Donckt et al., 2022) uses this modifiation of the LTTB algorithm (Steinarsson, 2013).
\par
Not terriblly clear but it seems like \emph{rps} = \{4,6\} is good for low roughness data, and I'm not sure what you want for high roughness. In any case \emph{rps} in this range is similar to LLTB.
\par
\emph{nout} is the ``number of points selected'' but I don't understand this.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\VanDerDonckt23MinMaxLTTB.pdf}
}

@misc{LeBalanc23aemoVolatileEmailRystad,
  title = {{{AEMO}} Study? ({{Request}} for "{{AEMO}} Is the Most Volatile..." Report)},
  author = {LeBlanc, Ashley},
  abstract = {General press article cited Rystad report saying that AEMO is the ost volatile electricity market in the world; I emailed Rystad and asked if I could get it.  The reply came with the attached slide image showing that volatility.  See note for more.},
  note = {LeBalanc23aemoVolatileEmailRystad
\par
We chatted on the phone (or Teams or something) and she said she'd get back to me. ~I sent her a reminder on 2/1/2024.
\par
\textbf{From:} Ashley LeBlanc {$<$}Ashley.LeBlanc@rystadenergy.com{$>$} \\
\textbf{Sent:} Monday, November 20, 2023 10:07 AM\\
\textbf{To:} Scott Otterson {$<$}scott.otterson@qcells.com{$>$}\\
\textbf{Cc:} Felecia Williams {$<$}Felecia.Williams@rystadenergy.com{$>$}\\
\textbf{Subject:} RE: AEMO study?
\par
Hi Scott,
\par
Thank you so much for reaching out! The report that was quoted in this article has gained lots of attention. We aren't releasing the entire report publicly, but I can share a slide or two on your main areas of interest. I am assuming it's primarily Austra
\par
Would you be interested in scheduling an introductory call next week post-Thanksgiving so we can learn more about your key areas of interest and share some insights from our side as well?
\par
Let me know! Thanks so much, ASHLEY LEBLANC
\par
\textbf{From:} Scott Otterson {$<$}\href{mailto:scott.otterson@qcells.com}{scott.otterson@qcells.com}{$>$} \\
\textbf{Sent:} Friday, November 17, 2023 1:09 PM\\
\textbf{To:} RE\_info {$<$}\href{mailto:info@rystadenergy.com}{info@rystadenergy.com}{$>$}\\
\textbf{Subject:} AEMO study?
\par
Hi,
\par
The Rystad Energy report on AEMO market volatility, the one mentioned in \href{https://www.power-grid.com/td/transmission/transmission-line-issues-volatilize-australias-power-market/}{this article}, sounds interesting.~ Is it publicly available?
\par
Thanks,
\par
Scott
\par
--
\par
Scott Otterson
\par
Staff Data Scientist
\par
Geli QCELLS
\par
San Francisco, CA USA
\par
01 206.472.9115}
}

@article{Nivarthi23MultiTaskRepresentationLearning,
  title = {Multi-{{Task Representation Learning}} for {{Renewable-Power Forecasting}}: {{A Comparative Analysis}} of {{Unified Autoencoder Variants}} and {{Task-Embedding Dimensions}}},
  shorttitle = {Multi-{{Task Representation Learning}} for {{Renewable-Power Forecasting}}},
  author = {Nivarthi, Chandana Priya and Vogt, Stephan and Sick, Bernhard},
  year = {2023},
  month = sep,
  journal = {Machine Learning and Knowledge Extraction},
  volume = {5},
  number = {3},
  pages = {1214--1233},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2504-4990},
  doi = {10.3390/make5030062},
  url = {https://www.mdpi.com/2504-4990/5/3/62},
  urldate = {2023-11-21},
  abstract = {Typically, renewable-power-generation forecasting using machine learning involves creating separate models for each photovoltaic or wind park, known as single-task learning models. However, transfer learning has gained popularity in recent years, as it allows for the transfer of knowledge from source parks to target parks. Nevertheless, determining the most similar source park(s) for transfer learning can be challenging, particularly when the target park has limited or no historical data samples. To address this issue, we propose a multi-task learning architecture that employs a Unified Autoencoder (UAE) to initially learn a common representation of input weather features among tasks and then utilizes a Task-Embedding layer in a Neural Network (TENN) to learn task-specific information. This proposed UAE-TENN architecture can be easily extended to new parks with or without historical data. We evaluate the performance of our proposed architecture and compare it to single-task learning models on six photovoltaic and wind farm datasets consisting of a total of 529 parks. Our results show that the UAE-TENN architecture significantly improves power-forecasting performance by 10 to 19\% for photovoltaic parks and 5 to 15\% for wind parks compared to baseline models. We also demonstrate that UAE-TENN improves forecast accuracy for a new park by 19\% for photovoltaic parks, even in a zero-shot learning scenario where there is no historical data. Additionally, we propose variants of the Unified Autoencoder with convolutional and LSTM layers, compare their performance, and provide a comparison among architectures with different numbers of task-embedding dimensions. Finally, we demonstrate the utility of trained task embeddings for interpretation and visualization purposes.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {autoencoders,multi-task learning,obsLitNote,power forecast,transfer learning,zero-shot learning},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nivarthi23MultiTaskRESfrcst.pdf}
}

@inproceedings{Emami23MultiTaskGradientBoosting,
  title = {Multi-{{Task Gradient Boosting}}},
  booktitle = {Hybrid {{Artificial Intelligent Systems}}},
  author = {Emami, Seyedsaman and Ruiz Pastor, Carlos and {Mart{\'i}nez-Mu{\~n}oz}, Gonzalo},
  editor = {Garc{\'i}a Bringas, Pablo and P{\'e}rez Garc{\'i}a, Hilde and {Mart{\'i}nez de Pis{\'o}n}, Francisco Javier and Mart{\'i}nez {\'A}lvarez, Francisco and Troncoso Lora, Alicia and Herrero, {\'A}lvaro and Calvo Rolle, Jos{\'e} Luis and Quinti{\'a}n, H{\'e}ctor and Corchado, Emilio},
  year = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {97--107},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-40725-3_9},
  abstract = {Gradient Boosting Machines (GBMs) have revealed outstanding proficiency in various machine learning applications, such as classification and regression. Gradient boosting builds a set of regression models in an iterative process, in which at each iteration, a regressor model is trained to reduce a given loss on a given objective. This paper proposes an extension of gradient boosting that can handle multi-task problems, that is, problems in which the tasks share the attribute space but not necessarily the data distribution. The objective of the proposed algorithm is to split the GB process into two phases, one in which the base models learn the multiple interconnected tasks simultaneously, and a second one, in which different models are built to optimize the loss function on each task. The performance of proposed model shows a better overall performance with respect to models that learn the tasks independently and all tasks together in several multi-task regression and classification problems.},
  isbn = {978-3-031-40725-3},
  langid = {english},
  note = {Emami23MultiTaskGradientBoosting
\par
Another multitask GBT approach. ~
\par
\begin{itemize}

\item \href{https://github.com/GAA-UAM/MT_GB}{GitHub for this paper}. ~Last commit in 11/2023 was in 05/2023

\end{itemize}

\begin{itemize}

\item References Ying22multiTaskTreeMTGB (energy.bib)
\item (IBM, 2023)

\end{itemize}}
}

@misc{Joachimiak23MultiOutGBT,
  title = {Multi-Output Regression with Gradient Boosting Machines},
  author = {Joachimiak, Krzysztof},
  year = {2023},
  month = nov,
  journal = {Medium},
  url = {https://medium.com/@joachimiak.krzysztof/multi-output-regression-with-gradient-boosting-machines-39c925b5a1d4},
  urldate = {2023-11-27},
  abstract = {The first type of models that comes into my mind when thinking about the multi-output regression is definitely not the GBM. I'd rather{\dots}},
  langid = {english},
  keywords = {forecast,multi-task learning,noted,todo},
  note = {\section{Joachimiak23MultiOutGBT}

\par
A decent review of boosting algorithms, and how each of them handle multi-output cases e.g. for each input sample, an output at multiple forecast horizons or quantiles. ~Turns out that LightGBM doesn't have multioutput, but it can be hacked to make a joint model. ~What's not clear to me is how this is different than, for mulitple horizons, just adding horizon as input. ~Could add quantile probability too. ~Seems like just that would demonstrate a benefit, like in (Burba, 2023)
\par
\section{``Multi Output''}

\begin{itemize}

\item more than one output is predicted per sample, e.g.
\item multiple quantiles
\item building heating and cooling load (his example, but he doesn't demonstarate a benefit, unlike (Burba, 2023)

\end{itemize}

\section{Ways to do it}

\subsection{Two basic ideas}

\begin{itemize}

\item 
\par
single joint model for all outputs
\par
\begin{itemize}

\item joint optimization might be better
\item additional targets might help, even if don't use them (``auxilliary tasks'')

\end{itemize}

\item 
\par
N indpendent univariate models for each output
\par
\begin{itemize}

\item no interaction between outputs, so undesirable, in some ways
\item e.g. quantiles in time would have no temporal correlation (does this make sense to say??)

\end{itemize}

\end{itemize}

\subsection{scikit-learn}

\begin{itemize}

\item 
\par
\emph{MultiOutputRegressor~}
\par
\begin{itemize}

\item does N-independent models
\item said not suitable for quantiles

\end{itemize}

\item 
\par
\emph{StackingRegressor}
\par
\begin{itemize}

\item get submodel outputs by calling transform method
\item not recommended: designed to have prediction method call a metaestimator

\end{itemize}

\end{itemize}

\subsection{ML-Ensemble library}

\begin{itemize}

\item seems to do the same as MultiOutputRegressor if don't specifiy \emph{Superlearner}

\end{itemize}

\section{Time-series}

\begin{itemize}

\item for mult-step forecasting
\item doesn't cover this, really but has an example bit of code
\item 
\par
Tree-based models: careful: not good at extrapolation
\par
\begin{itemize}

\item recommendation is to forecast components separately: trend, seasonality, residuals)

\end{itemize}

\item skforecast, tspiral (only mentions)
\item darts: time series basedon darts lib

\end{itemize}

\section{GBM Libraries}

\par
\begin{itemize}

\item GBMs iterate, yielding multiple models, and then you have multiple outputs too
\item 
\par
options
\par
\begin{itemize}

\item single weak model per iteration
\item multiple independent weak models per iteration

\end{itemize}

\item 
\par
sckikit-learn DecisionTreeRegressor
\par
\begin{itemize}

\item can do multiple outputs
\item example of this with code at end, but no quantitative comparison

\end{itemize}

\item 
\par
LightGBM
\par
\begin{itemize}

\item doesn't directly support multi-output but can hack by pretending regression is classifier
\item have to hide multiOutput targets in either a global, or do something like I did in VDER and hid them in X
\item This hack makes sense for the case when multiple outputs are generated from the same input vector e.g. for quantiles, usually. ~But not for horizons, which have unique data.
\item 
\par
Could even have two indicator inputs: one being horizon; the other, quantile. ~
\par
\begin{itemize}

\item The quantile would make sense if had different inputs for each quantile, which I haven't personally seen, but seems possible.

\end{itemize}

\end{itemize}

\item 
\par
XGBOOST
\par
\begin{itemize}

\item Looks like one independent model per target, like LightGBM 
\item But does this contradict the table above?
\item So no dependency (like temporal dependence) but "reusing data."
\item 
\par
``reusing data'' I think this means that, instead of having something like a horizon training data input, along with a duplicate set of other training data for each horizon, you have multiple outputs for each horizon, and use the same data. ~
\par
\begin{itemize}

\item But maybe the kind of forecasting I'm doing can't do that anyway? ~
\item For each horizon, I usually have a separate input forecast for that horizon: NWP, a load forecast, or something like that.
\item \textbf{HOWEVER}: multiple quantiles is such a case: a vector of forecasted quantiles at each horizon (which would usually have unique data for each horizon).
\item 
\par
\textbf{Appropriate architecture for my forecasts:}
\par
\begin{itemize}

\item multiple \emph{outputs}:\emph{ }for quantiles
\item multiple \emph{inputs}: for each horizon
\item Sounds like XGBOOST won't be ideal for multi-output quantiles, b/c of independent models.
\item 
\par
The LightGBM trick in this article might work (replaced w/ quantile loss)
\par
\begin{itemize}

\item seems like XGBOOST would need the same trick as LightGBM to model quantile interdependencies

\end{itemize}

\item \textbf{TODO}: Move this to a topic page someday

\end{itemize}

\end{itemize}

\end{itemize}

\item 
\par
CatBoost
\par
\begin{itemize}

\item has single model for multiple targets, unlike for LightGBM and XGBOOST

\end{itemize}

\item 
\par
scikit-learn GradientBoostRegressor
\par
\begin{itemize}

\item doesn't support multitarget regression

\end{itemize}

\item Interesting, maybe not ready, worth exploring: NGBOOST, TensorFlow Decision Forest, Py-Forest
\item 
\par
Homemade multioutput GBM
\par
\begin{itemize}

\item code it yourself
\item wrap that in scikit's DecisionTreeRegressor, with all its weaknesses

\end{itemize}

\end{itemize}

\subsection{}

\par
\textbf{Also}
\par
\begin{itemize}

\item boosting, iteration, {\dots} defined here: (IBM, 2023)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Joachimiak23MultiOutGBT.html}
}

@misc{IBM23whatIsBoosting,
  title = {What Is {{Boosting}}?},
  shorttitle = {What Is {{Boosting}}?},
  author = {IBM},
  year = {2023},
  url = {https://www.ibm.com/topics/boosting},
  urldate = {2023-11-27},
  abstract = {Learn about boosting algorithms and how they can improve the predictive power of your data mining initiatives.},
  langid = {american},
  keywords = {obsLitNote},
  note = {IBM23whatIsBoosting
\par
Boosting iteratively trains simple weak learners (just a little bit more accurate than random) to correct errors of the preceeding weak learner. ~It can do this by increasing the sample weight of samples where are gotten wrong (AdaBoost) or by predicting the resudual of the previous learner (Graident Boosting, like LightGBM and XGBOOST). ~
\par
Advantages are:
\par
\begin{itemize}

\item especially good at reducing bias of weak learners
\item no feature preprocessing needed
\item does a kind of dimension reduction (picks predictive features)
\item has built-in missing feature handing (\href{https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html}{not really in LightGBM})

\end{itemize}

\par
But is said to be more vulnerable to overtraining (not so clear this is true).},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\IBM23whatIsBoosting.pdf}
}

@techreport{AEMC18gamingRebidGrattan,
  title = {Gaming in Rebidding ({{Grattan}} Response) {\textbar} {{AEMC}}},
  author = {AEMC},
  year = {2018},
  institution = {AEMC / AEMO},
  url = {https://www.aemc.gov.au/market-reviews-advice/gaming-rebidding-grattan-response},
  urldate = {2023-11-28},
  abstract = {On the 28th September 2018 the AEMC published an assessment of generator rebidding in the national electricity market.},
  langid = {english},
  note = {AEMC18gamingRebidGrattan
\par
AEMOs findings on the AEMO NEM price gaming problem discussed in (Dungey et al., 2018) and which caused \href{obsidian://open?vault=Obsidian\%20Share\%20Vault&file=work\%2FpriceFrcstAEMO\%2FMarket\%20Rule\%20Changes\%20(from\%20View)}{AEMO market changes}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\AEMC18gamingRebidGrattan.pdf}
}

@misc{AEMO22quarterlyEngergyDynamicsQ1,
  title = {Quarterly {{Energy Dynamics Q1}} 2022},
  author = {AEMO},
  year = {2022},
  month = apr,
  url = {https://aemo.com.au/-/media/files/major-publications/qed/2022/qed-q1-report.pdf?la=en#:~:text=Higher%20operational%20demand%20contributed%20to,the%20frequency%20of%20negative%20prices.&text=saw%20increased%20output%20from%20all,evening%2C%20including%20coal%20and%20gas.},
  urldate = {2023-11-28},
  note = {AEMO22quarterlyEngergyDynamicsQ1
\par
\begin{quotation}

\par
View \href{https://growingenergylabs.atlassian.net/wiki/spaces/~6246877945ece00069ca8ef0/pages/8205009022/AEMO+price+data+instruction\#1-hour-forward-forecast}{says} that this explains AEMO price volatility in 2022:\\
See volatility analysis chart on p.11. You can also look at the \href{https://aemo.com.au/energy-systems/major-publications/quarterly-energy-dynamics-qed}{past QED reports }(published quarterly) which may have a similar chart for other periods.
\par
\end{quotation}

\par
Also see \href{https://growingenergylabs.atlassian.net/wiki/spaces/~6246877945ece00069ca8ef0/pages/8186921126/AEMO+Q1+2022+energy+dynamic+review}{View's Notes}. ~Also \href{https://growingenergylabs.atlassian.net/wiki/spaces/~6246877945ece00069ca8ef0/pages/8261173578/QED+Q2+2022}{here}.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\AEMO22quarterlyEngergyDynamicsQ1.pdf}
}

@inproceedings{Romano19ConformalizedQR,
  title = {Conformalized {{Quantile Regression}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Romano, Yaniv and Patterson, Evan and Candes, Emmanuel},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/5103c3584b063c431bd1268e9b5e76fb-Abstract.html},
  urldate = {2024-01-06},
  abstract = {Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. Despite this appeal, existing conformal methods can be unnecessarily conservative because they form intervals of constant or weakly varying length across the input space. In this paper we propose a new method that is fully adaptive to heteroscedasticity. It combines conformal prediction with classical quantile regression, inheriting the advantages of both. We establish a theoretical guarantee of valid coverage, supplemented by extensive experiments on popular regression datasets. We compare the efficiency of conformalized quantile regression to other conformal methods, showing that our method tends to produce shorter intervals.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Romano19ConformalizedQR.pdf}
}

@article{Cornell24probFrcstAEMO,
  title = {A Probabilistic Forecast Methodology for Volatile Electricity Prices in the {{Australian National Electricity Market}}},
  author = {Cornell, Cameron and Dinh, Nam Trong and Pourmousavi, S. Ali},
  year = {2024},
  month = jan,
  journal = {International Journal of Forecasting},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2023.12.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0169207023001358},
  urldate = {2024-01-06},
  abstract = {The South Australia region of the Australian National Electricity Market (NEM) displays some of the highest levels of price volatility observed in modern electricity markets. This paper outlines an approach to probabilistic forecasting under these extreme conditions, including spike filtration and several post-processing steps. We propose using quantile regression as an ensemble tool for probabilistic forecasting, with our combined forecasts achieving superior results compared to all constituent models. Within our ensemble framework, we demonstrate that averaging models with varying training-length periods leads to a more adaptive model and increased prediction accuracy. The applicability of the final model is evaluated by comparing our median forecasts with the point forecasts available from the Australian NEM operator, with our model outperforming these NEM forecasts by a significant margin.},
  note = {Cornell24probFrcstAEMO
\par
is throwing away spikes something like in (Liu et al., 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Cornell24probFrcstAEMO.pdf}
}

@article{Sousa24featRelConformQR,
  title = {Improving Conformalized Quantile Regression through Cluster-Based Feature Relevance},
  author = {Sousa, Martim and Tom{\'e}, Ana Maria and Moreira, Jos{\'e}},
  year = {2024},
  month = mar,
  journal = {Expert Systems with Applications},
  volume = {238},
  pages = {122322},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2023.122322},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417423028245},
  urldate = {2024-01-06},
  abstract = {Conformalized quantile regression, a cutting-edge and model-agnostic algorithm, has emerged as a recent innovation to generate valid prediction intervals on finite samples while addressing heteroscedasticity. It starts by employing quantile regression to estimate conditional quantiles. Subsequently, these estimated conditional quantiles undergo a rectification process using conformal prediction. Under the assumption of exchangeability, a slightly weaker form of independent and identically distributed (i.i.d.) data, the resulting prediction intervals are valid in finite samples. However, a drawback of the proposed conformalization step is identified: it lacks the capacity to adapt to heteroscedasticity due to its independence from the input. To overcome this limitation, we propose an improvement that involves partitioning the covariates space into clusters, assigning higher weights to features with greater predictive power. Following that, within each cluster, a conformal step is applied, leveraging a rectification that is reliant on the input cluster-wise. To demonstrate the superiority of our improved version over the classic version of conformalized quantile regression, we conducted a comprehensive comparison of their respective prediction intervals using synthetic data.},
  keywords = {obsLitNote},
  note = {Comment: 11 pages, 10 figures},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sousa24featRelConformQR.pdf}
}

@misc{Angelopoulos23ConformalPIDControl,
  title = {Conformal {{PID Control}} for {{Time Series Prediction}}},
  author = {Angelopoulos, Anastasios N. and Candes, Emmanuel J. and Tibshirani, Ryan J.},
  year = {2023},
  month = jul,
  number = {arXiv:2307.16895},
  eprint = {2307.16895},
  primaryclass = {cs, eess, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2307.16895},
  urldate = {2024-01-06},
  abstract = {We study the problem of uncertainty quantification for time series prediction, with the goal of providing easy-to-use algorithms with formal guarantees. The algorithms we present build upon ideas from conformal prediction and control theory, are able to prospectively model conformal scores in an online setting, and adapt to the presence of systematic errors due to seasonality, trends, and general distribution shifts. Our theory both simplifies and strengthens existing analyses in online conformal prediction. Experiments on 4-week-ahead forecasting of statewide COVID-19 death counts in the U.S. show an improvement in coverage over the ensemble forecaster used in official CDC communications. We also run experiments on predicting electricity demand, market returns, and temperature using autoregressive, Theta, Prophet, and Transformer models. We provide an extendable codebase for testing our methods and for the integration of new algorithms, data sets, and forecasting rules.},
  archiveprefix = {arXiv},
  note = {Angelopoulos23conformTSpredPID
\par
I think this is the simple adaptation method Candes talked about in his YouTube videos (my notes are in Obsidian), but in this paper, they forecast electricity demand once day at a time using a transformer network, and then fix it up for drift, etc. with this conformal method. ~Similar benefit as in (Xu and Xie, 2023), but the approaches are pretty different.
\par
Comment: Code available at https://github.com/aangelopoulos/conformal-time-series},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Angelopoulos23conformTSpredPID.pdf}
}

@inproceedings{Althoff23conformWindFrcst,
  title = {Evaluation of Conformal-Based Probabilistic Forecasting Methods for Short-Term Wind Speed Forecasting},
  booktitle = {Proceedings of the {{Twelfth Symposium}} on {{Conformal}}  and {{Probabilistic Prediction}} with {{Applications}}},
  author = {Althoff, Simon and Szabadv'ary, Johan Hallberg and Anderson, Jonathan and Carlsson, Lars},
  year = {2023},
  month = aug,
  pages = {100--115},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v204/althoff23a.html},
  urldate = {2024-01-06},
  abstract = {We apply Conformal Predictive Distribution Systems  (CPDS) and a non-exchangeable version of the  traditional Conformal Prediction (NECP) method to  short-term wind speed forecasting to generate  probabilistic forecasts. These are compared to the  more traditional Quantile Regression Forest (QRF)  method. A short-term forecast is available from a  few hours before the forecasted time period and is  only extended a couple days into the future.  The  methods are supplied ensemble forecasts as input and  additionally the Conformal methods are supplied with  post-processed point forecasts for generating the  probability distributions. In the NECP case we  propose a method of producing probability  distributions by creating sequentially larger  prediction intervals. The methods are compared  through a teaching schedule, to mimic a real-world  setting. For each model update in the teaching  schedule a grid-search approach is applied to select  each method's optimal hyperparameters, respectively.  The methods are tested out of the box with tweaks to  few hyperparameters. We also introduce a normalized  nonconformity score and use it with the conformal  method that handles data that violates the  exchangeability assumption. The resulting  probability distributions are compared to actual  wind measurements through Continuous Ranked  Probability Scores (CRPS) as well as their validity  and efficiency of certain prediction intervals.  Our  results suggest that the conformal based methods,  with the pre-trained underlying model, produce  slightly more conservative but more efficient  probability distributions than QRF at a lower  computational cost. We further propose how the  conformal-based methods could be improved for the  application to real-world scenarios.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Althoff23conformWindFrcst
\par
Prob wind speed forecast that handles non-stationarity. ~Conformal method is more conservative than baseline but more ``efficient''. ~Evaluation by CRPS, and full distribution is apparently estimated from coverage intervals.
\par
I think. ~Must read.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Althoff23conformWindFrcst.pdf}
}

@book{Manokhin23conformPredGuideBk,
  title = {Practical {{Guide}} to {{Applied Conformal Prediction}} in {{Python}}: {{Learn}} and Apply the Best Uncertainty Frameworks to Your Industry Applications},
  shorttitle = {Practical {{Guide}} to {{Applied Conformal Prediction}} in {{Python}}},
  author = {Manokhin, Valery and Sudjianto, Agus},
  year = {2023},
  month = dec,
  publisher = {Packt Publishing},
  abstract = {Take your machine learning skills to the next level by mastering the best framework for uncertainty quantification - Conformal PredictionKey FeaturesMaster Conformal Prediction, a fast-growing ML framework, with Python applications.Explore cutting-edge methods to measure and manage uncertainty in industry applications.The book will explain how Conformal Prediction differs from traditional machine learning.Book DescriptionIn the rapidly evolving landscape of machine learning, the ability to accurately quantify uncertainty is pivotal. "Practical Guide to Applied Conformal Prediction in Python" addresses this need by offering an in-depth exploration of Conformal Prediction, a cutting-edge framework set to revolutionize uncertainty management in various ML applications.Embark on a comprehensive journey through Conformal Prediction, exploring its fundamentals and practical applications in binary classification, regression, time series forecasting, imbalanced data, computer vision, and NLP. Each chapter delves into specific aspects, offering hands-on insights and best practices for enhancing prediction reliability. The book concludes with a focus on multi-class classification nuances, providing expert-level proficiency to seamlessly integrate Conformal Prediction into diverse industries. Practical examples in Python using real-world datasets reinforce intuitive explanations, ensuring you acquire a robust understanding of this modern framework for uncertainty quantification.This guide is a beacon for mastering Conformal Prediction in Python, providing a blend of theory and practical application. It serves as a comprehensive toolkit to enhance machine learning skills, catering to professionals from data scientists to ML engineers.What you will learnThe fundamental concepts and principles of conformal predictionLearn how conformal prediction differs from traditional ML methodsApply real-world examples to your own industry applicationsExplore advanced topics - imbalanced data and multi-class CPDive into the details of the conformal prediction frameworkBoost your career as a data scientist, ML engineer, or researcherLearn to apply conformal prediction to forecasting and NLPWho this book is forIdeal for readers with a basic understanding of machine learning concepts and Python programming, this book caters to data scientists, ML engineers, academics, and anyone keen on advancing their skills in uncertainty quantification in ML.Table of ContentsIntroducing Conformal PredictionOverview of Conformal PredictionFundamentals of Conformal PredictionValidity and Efficiency of Conformal PredictionTypes of Conformal PredictorsConformal Prediction for ClassificationConformal Prediction for RegressionConformal Prediction for Time Series and ForecastingConformal Prediction for Computer VisionConformal Prediction for Natural Language ProcessingHandling Imbalanced DataMulti-Class Conformal Prediction},
  isbn = {978-1-80512-276-0},
  langid = {english},
  keywords = {obsLitNote},
  note = {Manokhin23conformPredGuideBk
\par
A book with GitHub code, notebooks, etc, which is fairly new and has a time series forecasting chapter. ~Might be worth buying, but watch \href{https://www.youtube.com/watch?v=Cla7b6UYees}{author's talk} first before deciding to buy.
\par
\begin{itemize}

\item \href{https://www.packtpub.com/product/practical-guide-to-applied-conformal-prediction-in-python/9781805122760}{Pakt publisher's page}
\item \href{https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction}{GitHub page}

\end{itemize}}
}

@techreport{Tibshirani23calibScoreFrcst,
  type = {Course {{Notes}}},
  title = {Forecast {{Scoring}} and {{Calibration}}},
  author = {Tibshirani, Ryan},
  year = {2023},
  institution = {University of California, Berkeley},
  url = {https://www.stat.berkeley.edu/~ryantibs/statlearn-s23/lectures/calibration.pdf},
  urldate = {2024-01-11},
  abstract = {In this lecture, we cover a topic that sits adjacent to conformal prediction in a sense, which is calibration. We will cover the perspective(s) on calibration from the forecasting literature, where it is arguably the most developed. Instead of focusing solely on calibration, we'll cover some of the broader theory of forecasting, pertaining to scoring rules, calibration, and ensembles. There is a rich literature on forecast scoring and calibration. This literature has roots in statistics, but over the years it has been expanded and driven by applied forecasting disciplines, primarily weather forecasting. It is still quite an active area in terms of development: new results that seem foundational in nature are still being discovered year to year. As per our usual comment, what we cover in this lecture is only a sample of what is known in the theory of forecasting. Unlike other topics, we are not aware of a book that gives a modern and comprehensive treatment of forecast scoring rules, calibration, and so on. (Perhaps this would be premature, as these topics are still in active development, and such a book will come later.) One of the most active and influential researchers in forecasting theory today is Tilmann Gneiting, and much of what we cover will be drawing from his work with collaborators, such as Gneiting and Raftery (2007); Gneiting et al. (2007); Ranjan and Gneiting (2010); Gneiting and Ranjan (2013).},
  keywords = {obsLitNote},
  note = {Tibshirani23calibScoreFrcst
\par
Some versions of the interval scores commonly used in conformal prediction papers are both are proper, and tightly related to CRPS and QS. ~But then it seems to me that the log score might be better than any of these for rare events (also see (Taillardat et al., 2023) for more). ~Also, details about Bregman intervals and a lot on calibration (TODO: read that part)
\par
\section{Scores}

\subsubsection{Relationship between CRSP, QS and interval scores}

\par
The interval score, IS, often used in conformal forecasting, does already both reward for sharpness and penalize miscoverage. ~A composite metric for a set of interval forecasts is the officially proper weighted interval score, WIS. ~Is WIS what I've always computed (``Evalcast Docs: weighted\_interval\_score'', 2024), but I just called it ``CRPS?''
\par
Anyway, CRPS can be used to define WIS/QS, but I don't quite get that either.
\par
Finally, this paper has a refined CRPS derivation that shows that it's the integrated QS -- I have the older, apparently not-quite-correct derivation in energy.bib somehwere. ~TODO someday: find that paper and link it.
\par
\subsubsection{Other scores}

\begin{itemize}

\item log score (proper)
\item quadratic score (proper)
\item log score aggressively penalizes rare event underconfidence
\item but CRPS is more ``modern,'' but in DE, I found that it was not so good at rare events.
\item what about the Energy Score? ~TODO an link to that, as Steven V found this was better for rare events, if I remember correctly.

\end{itemize}

\section{Bregman representation}

\par
TODO: read some day
\par
\section{Calibration}

\par
\section{Also}

\begin{itemize}

\item See (Bosse et al., 2023) about the need to log (or something) normalize predictions before calculating CRPS/WIS on exponentially changing epedimiological data.
\item See (Taillardat et al., 2023) for CRPS modifications which fix its weakness on assessing extreme event performance.

\end{itemize}

\par
TODO: read, since important for conformal prediction},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Tibshirani23calibScoreFrcst.pdf}
}

@inproceedings{Zaffran22adaptCnfrmPriceFrcst,
  title = {Adaptive Conformal Predictions for Time Series},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Zaffran, Margaux and F{\'e}ron, Olivier and Goude, Yannig and Josse, Julie and Dieuleveut, Aymeric},
  year = {2022},
  pages = {25834--25866},
  publisher = {PMLR},
  url = {https://proceedings.mlr.press/v162/zaffran22a.html},
  urldate = {2024-01-11},
  keywords = {obsLitNote},
  note = {Zaffran22adaptCnfrmPriceFrcst
\par
Compares a kind of conformal predicion to other mehods (not sure how modern they are) and does a test case on French elecricity market price forecasting. ~Has code and there's a youtube video on this paper too, that I watched.
\par
Has some causual way of splitting the time series, maybe like what I did for VDER, but I didn't spend eough time on it to understand.
\par
This method said to be better than (Xu and Xie, 2023) in some way.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zaffran22adaptCnfrmPriceFrcst.pdf}
}

@article{Xu23conformPredTS,
  title = {Conformal {{Prediction}} for {{Time Series}}},
  author = {Xu, Chen and Xie, Yao},
  year = {2023},
  month = oct,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {10},
  pages = {11575--11587},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2023.3272339},
  url = {https://ieeexplore.ieee.org/document/10121511},
  urldate = {2024-01-11},
  abstract = {We present a general framework for constructing distribution-free prediction intervals for time series. We establish explicit bounds on the conditional and marginal coverage gaps of estimated prediction intervals, which asymptotically converge to zero under additional assumptions. We also provide similar bounds on the size of set differences between oracle and estimated prediction intervals. To implement this framework, we introduce an efficient algorithm called EnbPI, which utilizes ensemble predictors and is closely related to conformal prediction (CP) but does not require data exchangeability. Unlike other methods, EnbPI avoids data-splitting and is computationally efficient by avoiding retraining, making it scalable for sequentially producing prediction intervals. Extensive simulation and real-data analyses demonstrate the effectiveness of EnbPI compared to existing methods.},
  keywords = {obsLitNote},
  note = {Xu23conformPredTS
\par
A computationally efficient, adaptive conformal time series prediction method that somehow avoids a separate calibration set. ~(Zaffran et al., 2022) thought her method was better, but I didn't read carefully. ~Similar benefits as found in (Angelopoulos et al., 2023) but a different approach.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Xu23conformPredTS.pdf}
}

@misc{PapersWithCode24conformalTS,
  title = {Papers with {{Code}} - {{Search}} for Conformal Time Series},
  year = {2024},
  month = jan,
  url = {https://paperswithcode.com/search?q_meta=&q_type=&q=conformal+time+series},
  urldate = {2024-01-11},
  abstract = {10 search results},
  langid = {english},
  keywords = {obsLitNote,PapersWithCode24conformalTS},
  note = {PapersWithCode24conformalTS
\par
Lots of conformal time series papers with code.}
}

@article{Sesia20cnfrmQRcmpr,
  title = {A Comparison of Some Conformal Quantile Regression Methods},
  author = {Sesia, Matteo and Cand{\`e}s, Emmanuel J.},
  year = {2020},
  month = jan,
  journal = {Stat},
  volume = {9},
  number = {1},
  pages = {e261},
  issn = {2049-1573, 2049-1573},
  doi = {10.1002/sta4.261},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/sta4.261},
  urldate = {2024-01-12},
  abstract = {We compare two recent methods that combine conformal inference with quantile regression to produce locally adaptive and marginally valid prediction intervals under sample exchangeability (Romano, Patterson, \& Cand{\`e}s, 2019, arXiv:1905.03222; Kivaranovic, Johnson, \& Leeb, 2019, arXiv:1905.10634). First, we prove that these two approaches are asymptotically efficient in large samples, under some additional assumptions. Then we compare them empirically on simulated and real data. Our results demonstrate that the method of Romano et al.~typically yields tighter prediction intervals in finite samples. Finally, we discuss how to tune these procedures by fixing the relative proportions of observations used for training and conformalization. Our empirical results suggest that using between 70\% and 90\% of the data for training often achieves a good balance between minimizing the average width of the predictions intervals and the variability in their practical coverage.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Sesia20cnfrmQRcmpr
\par
At least in terms of conformity scores, QR followed by conformal regression yields tighter intervals. ~Method is said to be easiliy extended to non-uniform intervals, but doesn't say how. ~Also, says to use 70-80\% of training data to train the QR; the rest for conformal calibration.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sesia20cnfrmQRcmpr.pdf}
}

@misc{Evalcast24weightedIntervScore,
  title = {Evalcast {{Docs}}: Weighted\_interval\_score},
  year = {2024},
  url = {https://cmu-delphi.github.io/covidcast/evalcastR/reference/weighted_interval_score.html},
  urldate = {2024-01-12},
  abstract = {Computes weighted interval score (WIS), a well-known quantile-based approximation of the commonly-used continuous ranked probability score (CRPS). WIS is a proper score, and can be thought of as a distributional generalization of absolute error. For example, see Bracher et al. (2020) for discussion in the context of COVID-19 forecasting.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Evalcast24weightedIntervScore
\par
Documentation for a forecast eval library says (below) that WIS is a quantile-based approx of CRPS. ~So, is WIS just what I've already been computing in Matlab or whatever, and calling it ``CRPS''?
\par
CRPS, WIS, etc. is discussed in (Tibshirani, 2023)
\par
Evalcast package: (Bien and others, 2024)
\par
\emph{Documentation exerpt:}
\par
\section{\textbf{Compute weighted interval score}}

\par
Source:~\texttt{R/error\_measures.R}
\par
Computes weighted interval score (WIS), a well-known quantile-based approximation of the commonly-used continuous ranked probability score (CRPS). WIS is a proper score, and can be thought of as a distributional generalization of absolute error. For example, see~\href{https://arxiv.org/abs/2005.12881}{Bracher et al. (2020)}~for discussion in the context of COVID-19 forecasting.
\par
weighted\_interval\_score(quantile, value, actual\_value)

\subsection{\textbf{Arguments}}

\par
quantile
\par
vector of forecasted quantiles
\par
value
\par
vector of forecasted values
\par
actual\_value
\par
Actual value.}
}

@article{Bosse23scoreEpidFrcst,
  title = {Scoring Epidemiological Forecasts on Transformed Scales},
  author = {Bosse, Nikos I. and Abbott, Sam and Cori, Anne and van Leeuwen, Edwin and Bracher, Johannes and Funk, Sebastian},
  year = {2023},
  month = aug,
  journal = {PLOS Computational Biology},
  volume = {19},
  number = {8},
  pages = {e1011393},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1011393},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011393},
  urldate = {2024-01-12},
  abstract = {Forecast evaluation is essential for the development of predictive epidemic models and can inform their use for public health decision-making. Common scores to evaluate epidemiological forecasts are the Continuous Ranked Probability Score (CRPS) and the Weighted Interval Score (WIS), which can be seen as measures of the absolute distance between the forecast distribution and the observation. However, applying these scores directly to predicted and observed incidence counts may not be the most appropriate due to the exponential nature of epidemic processes and the varying magnitudes of observed values across space and time. In this paper, we argue that transforming counts before applying scores such as the CRPS or WIS can effectively mitigate these difficulties and yield epidemiologically meaningful and easily interpretable results. Using the CRPS on log-transformed values as an example, we list three attractive properties: Firstly, it can be interpreted as a probabilistic version of a relative error. Secondly, it reflects how well models predicted the time-varying epidemic growth rate. And lastly, using arguments on variance-stabilizing transformations, it can be shown that under the assumption of a quadratic mean-variance relationship, the logarithmic transformation leads to expected CRPS values which are independent of the order of magnitude of the predicted quantity. Applying a transformation of log(x + 1) to data and forecasts from the European COVID-19 Forecast Hub, we find that it changes model rankings regardless of stratification by forecast date, location or target types. Situations in which models missed the beginning of upward swings are more strongly emphasised while failing to predict a downturn following a peak is less severely penalised when scoring transformed forecasts as opposed to untransformed ones. We conclude that appropriate transformations, of which the natural logarithm is only one particularly attractive option, should be considered when assessing the performance of different models in the context of infectious disease incidence.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Bosse23scoreEpidFrcst
\par
Exponential-in-nature epedidemiological forecasts should be somehow normalized before calculating CRPS/WIS. ~The reason is the scale changes rapidly, and gives more weight to forecasts at high magnitude stages of an epedimic. ~They use the log transform but suggest other transforms would be OK too.
\par
Such normalization changes model ranking based on CRPS/WIS more severely penalizing the missing of the start of an upswing, and less severly penalizing the missing of the start of the downswing.
\par
See (Tibshirani, 2023) for a discussion of CRPS and WIS.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bosse23scoreEpidFrcst.pdf}
}

@article{Taillardat23extrmEventCRPS,
  title = {Evaluating Probabilistic Forecasts of Extremes Using Continuous Ranked Probability Score Distributions},
  author = {Taillardat, Maxime and Foug{\`e}res, Anne-Laure and Naveau, Philippe and {de Fondeville}, Rapha{\"e}l},
  year = {2023},
  month = jul,
  journal = {International Journal of Forecasting},
  volume = {39},
  number = {3},
  eprint = {1905.04022},
  primaryclass = {math, stat},
  pages = {1448--1459},
  issn = {01692070},
  doi = {10.1016/j.ijforecast.2022.07.003},
  url = {http://arxiv.org/abs/1905.04022},
  urldate = {2024-01-12},
  abstract = {Verifying probabilistic forecasts for extreme events is a highly active research area because popular media and public opinions are naturally focused on extreme events, and biased conclusions are readily made. In this context, classical verification methods tailored for extreme events, such as thresholded and weighted scoring rules, have undesirable properties that cannot be mitigated, and the well-known continuous ranked probability score (CRPS) is no exception. In this paper, we define a formal framework for assessing the behavior of forecast evaluation procedures with respect to extreme events, which we use to demonstrate that assessment based on the expectation of a proper score is not suitable for extremes. Alternatively, we propose studying the properties of the CRPS as a random variable by using extreme value theory to address extreme event verification. An index is introduced to compare calibrated forecasts, which summarizes the ability of probabilistic forecasts for predicting extremes. The strengths and limitations of this method are discussed using both theoretical arguments and simulations.},
  archiveprefix = {arXiv},
  keywords = {obsLitNote},
  note = {Taillardat23extrmEventCRPS
\par
CRPS -- the expected value of a proper score -- isn't good for scoring a model's skill at predicting rare events. So a new rare-vents-appropriate index is proposed, which somehow comes by considering CRPS and the extreme event distribution.
\par
Seems definitely worth reading.
\par
\section{Also}

\begin{itemize}

\item (Tibshirani, 2023) discusses CRPS and other scores

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Taillardat23extrmEventCRPS.pdf}
}

@article{Kim19probDNIfromGHI,
  title = {Probabilistic Prediction of Direct Normal Irradiance Derived from Global Horizontal Irradiance over the {{Korean Peninsula}} by Using {{Monte-Carlo}} Simulation},
  author = {Kim, Chang Ki and Kim, Hyun-Goo and Kang, Yong-Heack and Yun, Chang-Yeol and Kim, Shin Young},
  year = {2019},
  month = mar,
  journal = {Solar Energy},
  volume = {180},
  pages = {63--74},
  issn = {0038-092X},
  doi = {10.1016/j.solener.2019.01.030},
  url = {https://www.sciencedirect.com/science/article/pii/S0038092X19300398},
  urldate = {2024-01-18},
  abstract = {Solar resource assessment is carried out in a feasibility study using reliable meteorological elements including solar irradiance. In concentrating solar power plants, the direct normal irradiance is the key variable in the system operation. However, direct normal irradiance is rarely measured as compared to global horizontal irradiance. There are several models that can be used to derive the direct normal irradiance from global horizontal irradiance. In this study, the Engerer model is used as a decomposition model, then evaluated against in situ observations at three ground stations: Seoul, Buan, and Jeju ground stations. The relative root mean square errors between the observed and direct normal irradiance estimated by the Engerer model are 15.0\%, 19.4\%, and 17.1\% at Seoul, Buan, and Jeju ground stations, respectively. The uncertainty of estimates is represented by the prediction interval from probabilistic prediction through Monte-Carlo simulation that employs the bias between estimation and ground truth for training datasets. The prediction interval for 90\% confidence level is 117.9\,W\,m-2 at the Seoul station, resulting from Monte-Carlo simulation. The prediction interval coverage probability is 92.8\%, implying that the probability that observed DNI is not included in the prediction interval is 7.2\%. The error metrics for probabilistic prediction indicates that Monte-Carlo simulation provides both valid and more informative estimations.},
  note = {\textbf{Contents}
\par
\begin{itemize}

\item \href{zotero://open-pdf/0_V56HUFUA/1}{Introduction}
\item \href{zotero://open-pdf/0_V56HUFUA/2}{Surface observations}
\item \href{zotero://open-pdf/0_V56HUFUA/3}{Methodology}

\begin{itemize}

\item \href{zotero://open-pdf/0_V56HUFUA/3}{Deterministic prediction model}
\item \href{zotero://open-pdf/0_V56HUFUA/3}{Monte-Carlo simulation}
\item \href{zotero://open-pdf/0_V56HUFUA/4}{Multi-model ensembles}
\item \href{zotero://open-pdf/0_V56HUFUA/4}{Bootstrap}

\end{itemize}

\item \href{zotero://open-pdf/0_V56HUFUA/4}{Results and discussions}

\begin{itemize}

\item \href{zotero://open-pdf/0_V56HUFUA/7}{Deterministic predictions}
\item \href{zotero://open-pdf/0_V56HUFUA/8}{Probabilistic predictions}
\item \href{zotero://open-pdf/0_V56HUFUA/9}{Cloud impacts on deterministic and probabilistic predictions}

\end{itemize}

\item \href{zotero://open-pdf/0_V56HUFUA/9}{Conclusions}
\item \href{zotero://open-pdf/0_V56HUFUA/9}{Acknowledgements}
\item \href{zotero://open-pdf/0_V56HUFUA/10}{mk:H1\_14}
\item \href{zotero://open-pdf/0_V56HUFUA/10}{References}

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kim19probDNIfromGHI.pdf}
}

@misc{St.John24waryNYCposed,
  title = {Long Wary of Batteries, {{New York}}'s Now Poised to Go Big on Energy{\dots}},
  year = {2024},
  month = jan,
  journal = {Canary Media},
  url = {https://www.canarymedia.com/articles/energy-storage/long-wary-of-batteries-new-yorks-now-poised-to-go-big-on-energy-storage},
  urldate = {2024-01-22},
  abstract = {Can community-scale battery installations hit the sweet spot for the country's densest city? NineDot Energy has raised \$225 million more to take a crack at it.},
  langid = {english},
  keywords = {battery,battFire,noted},
  note = {John24waryNYCposed
\par
NYC is the US ``gold standard'' for safely permitting urban batteries. ~See my notes \href{https://www.evernote.com/l/AA0VUCSkliJJ1o1LwIeMGot/}{here}}
}

@article{Xu23elecPriceFrcstRegionAEMO,
  title = {Regional Electricity Market Price Forecasting Based on an Adaptive Spatial--Temporal Convolutional Network},
  author = {Xu, Jian and Hu, Bo and Zhang, Pengfei and Zhou, Xiaoming and Xing, Zuoxia and Hu, Zhanshuo},
  year = {2023},
  journal = {Frontiers in Energy Research},
  volume = {11},
  issn = {2296-598X},
  url = {https://www.frontiersin.org/articles/10.3389/fenrg.2023.1168944},
  urldate = {2024-02-01},
  abstract = {The accurate prediction of electricity prices has great significance for the power system and the electricity market, regional electricity prices are difficult to predict due to congestion issues in regional transmission lines. A regional electricity price prediction framework is proposed based on an adaptive spatial--temporal convolutional network. The proposed framework is expected to better explore regional electricity prices' spatial--temporal dynamic characteristics in the electricity spot market and improve the predictive accuracy of regional electricity prices. First, different areas of the electricity market are regarded as nodes. Then, each area's historical electricity price data are used as the corresponding node's characteristic information and constructed into a graph. Finally, a graph containing the spatial--temporal information on electricity prices is input to the adaptive spatial--temporal prediction framework to predict the regional electricity price. Operational data from the Australian electricity market are adopted, and the prediction results from the proposed adaptive spatial--temporal prediction framework are compared with those of existing methods. The numerical example results show that the predictive accuracy of the proposed framework is better than the existing baseline and similar methods. In the twelve-step forecast example in this paper, considering the spatial dependence of the spot electricity price can improve the forecast accuracy by at least 10.3\% and up to 19.8\%.},
  note = {Xu23elecPriceFrcstRegionAEMO
\par
AEMO price forecasting considering regional dependencies with a graph neural network. ~Dependency ID might be useful for any predictive model.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Xu23elecPriceFrcstRegionAEMO.pdf}
}

@article{Li19elecMktPriceFluctPCAnardl,
  title = {Determinants of Price Fluctuations in the Electricity Market: A Study with {{PCA}} and {{NARDL}} Models},
  shorttitle = {Determinants of Price Fluctuations in the Electricity Market},
  author = {Li, Kun and Cursio, Joseph D. and Sun, Yunchuan and Zhu, Zizheng},
  year = {2019},
  month = jan,
  journal = {Economic Research-Ekonomska Istra{\v z}ivanja},
  volume = {32},
  number = {1},
  pages = {2404--2421},
  publisher = {Routledge},
  issn = {1331-677X},
  doi = {10.1080/1331677X.2019.1645712},
  url = {https://doi.org/10.1080/1331677X.2019.1645712},
  urldate = {2024-02-01},
  abstract = {In the modern electricity markets, negative prices and spike prices coexist as a pair of opposite economic phenomena. This study investigates how these extreme prices play as the determinants to drive price fluctuations in the electricity market. We construct a two-stage analysis including a principal component analysis (PCA) and a nonlinear autoregressive distributed lags model (NARDL). We apply this analytical method to the wholesale Pennsylvania, New Jersey and Maryland (PJM) electricity market. We find that according to PCA, in the individual transmission lines, spike prices are determinants with largest explanatory power to the variation of prices, while according to NARDL, from the standpoint of the overall market, negative prices have a larger potential effect on both the real-time market and the forward market. These results are valuable and contributive to managers and operators in the electricity markets for policy decision making.},
  note = {Li19elecMktPriceFluctPCAnardl
\par
PCA finds that price spikes determine volatility. Is that a surprise?? ~ This must have better points, and since it's PCA-based, and NARDL (basic lagged forecast model), I might understand them{\dots}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Li19elecMktPriceFluctPCAnardl.pdf}
}

@article{Cramer23electPriceFrcstNormFlow,
  title = {Multivariate Probabilistic Forecasting of Intraday Electricity Prices Using Normalizing Flows},
  author = {Cramer, Eike and Witthaut, Dirk and Mitsos, Alexander and Dahmen, Manuel},
  year = {2023},
  month = sep,
  journal = {Applied Energy},
  volume = {346},
  pages = {121370},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2023.121370},
  url = {https://www.sciencedirect.com/science/article/pii/S0306261923007341},
  urldate = {2024-02-01},
  abstract = {Electricity is traded on various markets with different time horizons and regulations. Short-term intraday trading becomes increasingly important due to the higher penetration of renewables. In Germany, the intraday electricity price typically fluctuates around the day-ahead price of the European Power EXchange (EPEX) spot markets in a distinct hourly pattern. This work proposes a probabilistic modeling approach that models the intraday price difference to the day-ahead contracts. The model captures the emerging hourly pattern by considering the four 15min intervals in each day-ahead price interval as a four-dimensional joint probability distribution. The resulting nontrivial, multivariate price difference distribution is learned using a normalizing flow, i.e., a deep generative model that combines conditional multivariate density estimation and probabilistic regression. Furthermore, this work discusses the influence of different external impact factors based on literature insights and impact analysis using explainable artificial intelligence (XAI). The normalizing flow is compared to an informed selection of historical data and probabilistic forecasts using a Gaussian copula and a Gaussian regression model. Among the different models, the normalizing flow identifies the trends with the highest accuracy and has the narrowest prediction intervals. Both the XAI analysis and the empirical experiments highlight that the immediate history of the price difference realization and the increments of the day-ahead price have the most substantial impact on the price difference.},
  note = {Cramer23electPriceFrcstNormFlow
\par
Normalizing flow forecast DA price (difference?) scenarios better than a normal copula, immediate history of price difference is the best features. ~Seems like there are anly 4 features? ~
\par
Also used SHAP and wind features.},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Cramer22priceFrcstElecMktNormFLow.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Cramer23electPriceFrcstNormFlow.pdf}
}

@inproceedings{Passalis20adaptInNormLoadFrcst,
  title = {Global {{Adaptive Input Normalization}} for {{Short-Term Electric Load Forecasting}}},
  booktitle = {2020 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  author = {Passalis, Nikolaos and Tefas, Anastasios},
  year = {2020},
  month = dec,
  pages = {1--8},
  doi = {10.1109/SSCI47803.2020.9308553},
  url = {https://ieeexplore.ieee.org/document/9308553},
  urldate = {2024-02-01},
  abstract = {Recent advances in Deep Learning (DL) provided immensely powerful tools for various time series forecasting tasks. However, DL models are often quite sensitive to the used input normalization method, requiring extensive experimentation and fine-tuning to identify and apply the most appropriate method for different tasks. Even though trainable adaptive normalization methods have been recently proposed to overcome this to a certain extend, these methods also tend to remove useful information from the data, ignoring the global statistics of the input time series. To overcome this limitation, in this paper we propose a trainable adaptive normalization approach that is capable of both maintaining important mode information, since global statistics are employed for the normalization, as well as taking into account the current behavior of the time series and adjusting the normalization scheme to this. The effectiveness of the proposed method over baseline and state-of the-art normalization methods is demonstrated using extensive experiments on two different electric load forecasting datasets.},
  keywords = {normalization},
  note = {Passalis20adaptInNormLoadFrcst
\par
Adaptively normalizing load foredcast inputs adaptively, but not too much -- somehow, ``global'' (long term?) features are maintained, so context isn't lost (?). ~Steps are built into NN layers, but they look pretty simple. ~Could use for other algs too, maybe.
\par
I wonder, if it would make sense normalize each window, but to augment the feature set with some pre-normalization summary stats: mean, variance, etc.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Passalis20adaptInNormLoadFrcst.pdf}
}

@inproceedings{Liu23adaptNormTSfrcstTslice,
  title = {Adaptive {{Normalization}} for {{Non-stationary Time Series Forecasting}}: {{A Temporal Slice Perspective}}},
  shorttitle = {Adaptive {{Normalization}} for {{Non-stationary Time Series Forecasting}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Liu, Zhiding and Cheng, Mingyue and Li, Zhi and Huang, Zhenya and Liu, Qi and Xie, Yanhu and Chen, Enhong},
  year = {2023},
  url = {https://openreview.net/forum?id=5BqDSw8r5j},
  urldate = {2024-02-01},
  abstract = {Deep learning models have progressively advanced time series forecasting due to their powerful capacity in capturing sequence dependence. Nevertheless, it is still challenging to make accurate predictions due to the existence of non-stationarity in real-world data, denoting the data distribution rapidly changes over time. To mitigate such a dilemma, several efforts have been conducted by reducing the non-stationarity with normalization operation. However, these methods typically overlook the distribution discrepancy between the input series and the horizon series, and assume that all time points within the same instance share the same statistical properties, which is too ideal and may lead to suboptimal relative improvements. To this end, we propose a novel slice-level adaptive normalization, referred to SAN, which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization. SAN includes two crucial designs. First, SAN tries to eliminate the non-stationarity of time series in units of a local temporal slice (i.e., sub-series) rather than a global instance. Second, SAN employs a slight network module to independently model the evolving trends of statistical properties of raw time series. Consequently, SAN could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data. We instantiate the proposed SAN on four widely used forecasting models and test their prediction results on benchmark datasets to evaluate its effectiveness. Also, we report some insightful findings to deeply analyze and understand our proposed SAN. We make our codes publicly available2.},
  keywords = {hasCode,normalization},
  note = {Liu23adaptNormTSfrcstTslice
\par
Data is mean/vars normalized within sliding windows, and time series predictions are run on this. ~In tandem, a basic NN is trained to predict the mean/var of a time window. ~This mean/var prediction is used to denormalize the time series predictions back to the original domain. ~Is algorithm agnostic.
\par
Has code: \href{https://github.com/icantnamemyself/SAN}{https://github.com/icantnamemyself/SAN}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Liu23adaptNormTSfrcstTslice.pdf}
}

@article{Lago21epftoolboxElecPriceFrcst,
  title = {{{EPFTOOLBOX}}: {{The}} First Open-Access {{PYTHON}} Library for Driving Research in Electricity Price Forecasting ({{EPF}})},
  shorttitle = {{{EPFTOOLBOX}}},
  author = {Lago, Jesus and Marcjasz, Grzegorz and Schutter, Bart De and Weron, Rafal},
  year = {2021},
  month = jul,
  journal = {WORMS Software (WORking papers in Management Science Software)},
  publisher = {{Department of Operations Research and Business Intelligence, Wroclaw University of Science and Technology}},
  url = {https://ideas.repec.org//c/ahh/wcodes/wormsc2101.html},
  urldate = {2024-02-01},
  abstract = {The library includes three distinct modules. (1) The DATA MANAGEMENT module provides functionality to manage, process, and obtain data for EPF. The module also provides access to data from five different day-ahead electricity markets: EPEX-BE, EPEX-FR, EPEX-DE, NordPool, and PJM. (2) The MODELS module grants access to state-of-the-art forecasting methods for day-ahead electricity prices - the Lasso-Estimated AutoRegressive (LEAR) model and the Deep Neural Network (DNN) model - that require no expert knowledge and can be automatically employed. (3) The EVALUATION module provides with an easy-to-use interface for evaluating forecasts in EPF. This module includes both scalar metrics like MAE or MASE as well as statistical tests to evaluate the statistical significance in forecasting performance. The EPFTOOLBOX library is thoroughly described in: J. Lago, G. Marcjasz, B. De Schutter, R. Weron (2021) "Forecasting day-ahead electricity prices: A review of state-of-the-art algorithms, best practices and an open-access benchmark", Applied Energy 293, 116983 (https://doi.org/10.1016/j.apenergy.2021.116983; open access).},
  langid = {english},
  keywords = {hasCode},
  note = {An electricty price forecasting toolbox.
\par
Used e.g. by (Sebasti{\'a}n et al., 2023)}
}

@misc{Sebastian23adaptNormElecPriceFrcst,
  title = {An Adaptive Standardisation Model for {{Day-Ahead}} Electricity Price Forecasting},
  author = {Sebasti{\'a}n, Carlos and {Gonz{\'a}lez-Guill{\'e}n}, Carlos E. and Juan, Jes{\'u}s},
  year = {2023},
  month = nov,
  number = {arXiv:2311.02610},
  eprint = {2311.02610},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.02610},
  url = {http://arxiv.org/abs/2311.02610},
  urldate = {2024-02-01},
  abstract = {The study of Day-Ahead prices in the electricity market is one of the most popular problems in time series forecasting. Previous research has focused on employing increasingly complex learning algorithms to capture the sophisticated dynamics of the market. However, there is a threshold where increased complexity fails to yield substantial improvements. In this work, we propose an alternative approach by introducing an adaptive standardisation to mitigate the effects of dataset shifts that commonly occur in the market. By doing so, learning algorithms can prioritize uncovering the true relationship between the target variable and the explanatory variables. We investigate four distinct markets, including two novel datasets, previously unexplored in the literature. These datasets provide a more realistic representation of the current market context, that conventional datasets do not show. The results demonstrate a significant improvement across all four markets, using learning algorithms that are less complex yet widely accepted in the literature. This significant advancement unveils opens up new lines of research in this field, highlighting the potential of adaptive transformations in enhancing the performance of forecasting models.},
  archiveprefix = {arXiv},
  keywords = {normalization},
  note = {Sebastian23adaptNormElecPriceFrcst
\par
Simple, hour-specific sliding window prediction of next day's mean/var. ~model Predicts the future price window's mean and variance, horizon-by-horizon, kind of like (Liu et al., 2023).
\par
This is for a Spanish DA mkt.
\par
Uses models in (Lago et al., 2021) to test. ~
\par
``terminology'' (?) same as in (Gamakumara et al., 2023)
\par
Better to clip for classification, as in (Vu et al., 2021)
\par
Also see: (Liu et al., 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sebastian23adaptNormElecPriceFrcst.pdf}
}

@article{Yang22elecPriceFrstGraphAttntn,
  title = {Short-term Electricity Price Forecasting Based on Graph Convolution Network and Attention Mechanism},
  author = {Yang, Yuyun and Tan, Zhenfei and Yang, Haitao and Ruan, Guangchun and Zhong, Haiwang and Liu, Fengkui},
  year = {2022},
  month = sep,
  journal = {IET Renewable Power Generation},
  volume = {16},
  number = {12},
  pages = {2481--2492},
  issn = {1752-1416, 1752-1424},
  doi = {10.1049/rpg2.12413},
  url = {https://onlinelibrary.wiley.com/doi/10.1049/rpg2.12413},
  urldate = {2024-02-01},
  langid = {english},
  note = {Yang22elecPriceFrstGraphAttntn
\par
Models dependency across LBMP nodes with some kind of graph and attention mechanism. ~Must be a kind of graph NN and a kind of transformer NN.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Yang22elecPriceFrstGraphAttntn.pdf}
}

@inproceedings{Ogasawara10adaptNormNonStatTseries,
  title = {Adaptive {{Normalization}}: {{A}} Novel Data Normalization Approach for Non-Stationary Time Series},
  shorttitle = {Adaptive {{Normalization}}},
  booktitle = {The 2010 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Ogasawara, Eduardo and Martinez, Leonardo C. and {de Oliveira}, Daniel and Zimbr{\~a}o, Geraldo and Pappa, Gisele L. and Mattoso, Marta},
  year = {2010},
  month = jul,
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2010.5596746},
  url = {https://ieeexplore.ieee.org/document/5596746},
  urldate = {2024-02-01},
  abstract = {Data normalization is a fundamental preprocessing step for mining and learning from data. However, finding an appropriated method to deal with time series normalization is not a simple task. This is because most of the traditional normalization methods make assumptions that do not hold for most time series. The first assumption is that all time series are stationary, i.e., their statistical properties, such as mean and standard deviation, do not change over time. The second assumption is that the volatility of the time series is considered uniform. None of the methods currently available in the literature address these issues. This paper proposes a new method for normalizing non-stationary heteroscedastic (with non-uniform volatility) time series. The method, named Adaptive Normalization (AN), was tested together with an Artificial Neural Network (ANN) in three forecast problems. The results were compared to other four traditional normalization methods, and showed AN improves ANN accuracy in both short- and long-term predictions.},
  keywords = {normalization},
  note = {Ogasawara10adaptNormNonStatTseries
\par
Sliding min/max normalization but the min/max values used in normalization come from a lookup table, which matches some kind of summary statistics measured sometime recently, I think, e.g. mean/std. ~Also, min/max are quantiles, not literal min/max.
\par
Seems like you couldn't look too far back e.g. inflation would have shifted prices in the future and those old min/max values would be wrong. ~So, wouldn't you need to normalize those too?
\par
Good review of other simple adaptive normalization approaches.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ogasawara10adaptNormNonStatTseries.pdf}
}

@misc{Jakobs24treeAdaptModelSelTSfrcst,
  title = {Explainable {{Adaptive Tree-based Model Selection}} for {{Time Series Forecasting}}},
  author = {Jakobs, Matthias and Saadallah, Amal},
  year = {2024},
  month = jan,
  number = {arXiv:2401.01124},
  eprint = {2401.01124},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.01124},
  url = {http://arxiv.org/abs/2401.01124},
  urldate = {2024-02-01},
  abstract = {Tree-based models have been successfully applied to a wide variety of tasks, including time series forecasting. They are increasingly in demand and widely accepted because of their comparatively high level of interpretability. However, many of them suffer from the overfitting problem, which limits their application in real-world decision-making. This problem becomes even more severe in online-forecasting settings where time series observations are incrementally acquired, and the distributions from which they are drawn may keep changing over time. In this context, we propose a novel method for the online selection of tree-based models using the TreeSHAP explainability method in the task of time series forecasting. We start with an arbitrary set of different tree-based models. Then, we outline a performance-based ranking with a coherent design to make TreeSHAP able to specialize the tree-based forecasters across different regions in the input time series. In this framework, adequate model selection is performed online, adaptively following drift detection in the time series. In addition, explainability is supported on three levels, namely online input importance, model selection, and model output explanation. An extensive empirical study on various real-world datasets demonstrates that our method achieves excellent or on-par results in comparison to the state-of-the-art approaches as well as several baselines.},
  archiveprefix = {arXiv},
  keywords = {normalization},
  note = {Jakobs24treeAdaptModelSelTSfrcst
\par
Instead of adapting forecasting model inputs, use TreeSHAP to select from the most appropriate model, maybe spawn new models too.
\par
Could this also be an adaptive feature selection, or adaptive inference-time feature weigthing?
\par
Accepted and presented at ICDM 2023},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Jakobs24treeAdaptModelSelTSfrcst.pdf}
}

@misc{Gruver23LargeLanguageModels,
  title = {Large {{Language Models Are Zero-Shot Time Series Forecasters}}},
  author = {Gruver, Nate and Finzi, Marc and Qiu, Shikai and Wilson, Andrew Gordon},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07820},
  eprint = {2310.07820},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.07820},
  url = {http://arxiv.org/abs/2310.07820},
  urldate = {2024-02-05},
  abstract = {By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.},
  archiveprefix = {arXiv},
  keywords = {hasCode},
  note = {Gruver23llmOneShotTSfrcstr
\par
Friendly blog post: (Sen and Zhou, 2024)
\par
NeurIPS 2023. Code available at: https://github.com/ngruver/llmtime},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gruver23llmOneShotTSfrcstr.pdf}
}

@misc{Sen24decOnlyFoundTSfrcst,
  title = {A Decoder-Only Foundation Model for Time-Series Forecasting},
  author = {Sen, Rajat and Zhou, Yichen},
  year = {2024},
  month = feb,
  journal = {Google Research Blog},
  url = {https://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html?m=1},
  urldate = {2024-02-05},
  abstract = {Time-series forecasting is ubiquitous in various domains, such as retail, finance, manufacturing, healthcare and natural sciences. In retail use cases, for example, it has been observed that improving demand forecasting accuracy can meaningfully reduce inventory costs and increase revenue. Deep learning (DL) models have emerged as a popular approach for forecasting rich, multivariate, time-series data because they have proven to perform well in a variety of settings (e.g., DL models dominated the M5 competition leaderboard).},
  langid = {english},
  keywords = {obsLitNote},
  note = {Sen24decOnlyFoundTSfrcst
\par
A ~probabilistic TS forecaster similar to LLMs. ~Paper: (Gruver et al., 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sen24decOnlyFoundTSfrcst.pdf}
}

@article{Ciarreta23ForecastingElectricityPrices,
  title = {Forecasting Electricity Prices Using Bid Data},
  author = {Ciarreta, Aitor and Martinez, Blanca and Nasirov, Shahriyar},
  year = {2023},
  month = jul,
  journal = {International Journal of Forecasting},
  volume = {39},
  number = {3},
  pages = {1253--1271},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2022.05.011},
  url = {https://www.sciencedirect.com/science/article/pii/S0169207022000711},
  urldate = {2024-02-08},
  abstract = {Market liberalization and the expansion of variable renewable energy sources in power systems have made the dynamics of electricity prices more uncertain, leading them to show high volatility with sudden, unexpected price spikes. Thus, developing more accurate price modeling and forecasting techniques is a challenge for all market participants and regulatory authorities. This paper proposes a forecasting approach based on using auction data to fit supply and demand electricity curves. More specifically, we fit linear (LinX-Model) and logistic (LogX-Model) curves to historical sale and purchase bidding data from the Iberian electricity market to estimate structural parameters from 2015 to 2019. Then we use time series models on structural parameters to predict day-ahead prices. Our results provide a solid framework for forecasting electricity prices by capturing the structural characteristics of markets.},
  keywords = {Electricity markets,Linear functions,Logistic functions,obsLitNote,Price forecasting,Time series models},
  note = {Ciarreta23elecPriceFrcstBid
\par
Curve modeling followed by time series price models.
\par
\begin{itemize}

\item 
\par
reducing spike effects
\par
\begin{itemize}

\item neg. prices =={$>$} can't do log()
\item Insead do Normal Probability Integral Transformation (NPIT): inverse ecdf (over all time or periodically?)

\end{itemize}

\item Transmisson congestion dummy feature indicates market splitting, like happends in AEMO

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ciarreta23elecPriceFrcstBid.pdf}
}

@techreport{Harper23semiSchedFrcstAEMO,
  title = {Semi-{{Scheduled}}  {{Generation Dispatch}}  {{Self-Forecast}} --  {{Assessment Procedure}}},
  author = {Harper, Ken},
  year = {2023},
  month = aug,
  url = {https://aemo.com.au/-/media/Files/Electricity/NEM/Security_and_Reliability/Dispatch/Policy_and_Process/Semi-Scheduled-Generation-Dispatch-Self-Forecast---Assessment-Procedure.pdf},
  keywords = {AEMO,forecast,solar_power,wind_power},
  note = {Harper23semiSchedFrcstAEMO
\par
\begin{itemize}

\item ``UIGF'' (Harper, 2023, p. 3) are 5 minute ahead forecasts for wind and solar. ~Not clear if they go beyond that.
\item ``This procedure is only concerned with the UIGF for 5 minutes ahea'' (Harper, 2023, p. 4)
\item They're self forecasts: ``5-minute ahead dispatch self-forecast (SF'' (Harper, 2023, p. 4)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Harper23semiSchedFrcstAEMO.pdf}
}

@misc{Prakash22nemStorageUncertAEMOgithub,
  title = {{{NEMStorageUnderUncertainty}}. {{In GitHub}} Repository.},
  author = {Prakash, Abhi},
  year = {2022},
  abstract = {This repository contains the following: Analysis of historical prices and of (30-minute) pre-dispatch and 5-minute pre-dispatch price "forecast"1 errors from the Australian National Electricity Market (NEM). Source code and results from simulations that investigate how imperfect foresight affects storage arbitrage operation and revenues. This modelling uses historical actual price data and price forecasts from pre-dispatch \& 5-minute pre-dispatch. The results from this study have been submitted for publication. You can use this repository to access the model source code, documentation and results in the form of charts.},
  keywords = {AEMO,hasCode,noted,optimization,priceFrcst},
  note = {Prakash22nemStorageUncertAEMOgithub
\par
Open source Julia code to run battery optimization over perfect and forecasted AEMO prices.
\par
\begin{itemize}

\item 
\par
Two kinds of ``perfect''
\par
\begin{itemize}

\item Value of Perfect Information: perfect forecasts out to AEMO predispatch forecast horizons
\item Value of Perfect Foresight: perfect prices known ahead of time for the whole year.

\end{itemize}

\item Some results reported in the graphs, and it's said that this will be published.
\item uses NEMSEER: (Prakash, 2023)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Prakash22nemStorageUncertAEMOgithub.pdf}
}

@inproceedings{Yurdakul23RiskAvrsBattDecentMktsAEMO,
  title = {Risk-{{Averse Self-Scheduling}} of {{Storage}} in {{Decentralized Markets}}},
  booktitle = {2023 {{IEEE Power}} \& {{Energy Society General Meeting}} ({{PESGM}})},
  author = {Yurdakul, Ogun and Billimoria, Farhad},
  year = {2023},
  pages = {1--5},
  publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/10253330/},
  urldate = {2024-02-13},
  keywords = {AEMO,hasCode,normalization,optimization,priceFrcst},
  note = {Yurdakul23RiskAvrsBattDecentMktsAEMO
\par
\begin{itemize}

\item says that results show that batteries operate at very short term and, I think, miss a lot of spikes.
\item Uses NEMSEER: (Prakash, 2023)
\item Says price forecast errors are non-normal (on the Github page, at least, maybe not in the paper).

\end{itemize}

\par
GitHub: \href{https://github.com/oyurdakul/pesgm23}{https://github.com/oyurdakul/pesgm23}},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Yurdakul23RiskAvrsBattDecentMktsAEMO_GitHub.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Yurdakul23RiskAvrsBattDecentMktsAEMO.pdf}
}

@article{Mcardle23lowPctVREaemo,
  title = {Case {{Study}} (Part 1) of Low \% {{VRE NEM-wide}} on 3rd and 4th --- {{July}} 2023},
  author = {Mcardle, Paul},
  year = {2023},
  month = nov,
  journal = {WattClarity},
  abstract = {I've noted before recently (e.g on 2nd November) that we're* busy at work in crunching a wide range of numbers in order to produce our Genlnsights Quarterly Updates (for 2023 Q3) O By continuing, you accept the privacy * as with prior iterations of this report, these are a collaborative effort between us at Global-Roam Pty Ltd and the team at policy Greenview Strategic Consulting. That's what we mean here by `we'. Subscribe ... are we are looking forward to providing this (with the Executive Briefing) to our subscribing clients when it's released later in November. This is intended to augment some of the more traditional metrics included in the other two Quarterly Reports we see part of a triptych of Quarterly Reports that can sum up a quarter: 1) The AER's WholesaleMarket Quarterly publication for 2023 Q3 ... published 19th Oct 2023. LAST PUBLICATION 2) The AEMO's QuarterlyEnergyDynamics for 2023 Q3 was published, as published 23rd Oct 2023. Now for 2023 Q3 (for the first time in these reports), we've added in the analysis of an additional metric --- Percentage [A ------ supply from VRE across the NE},
  keywords = {AEMO,Electricity markets,Price forecasting},
  note = {Mcardle23lowPctVREaemo
\par
Maybe price spikes are predicted by \%VRE. ~This is instantaneous (large \& rooftop solar + wind) / all else (including hydro)
\par
Guy argues that ``NEM challenges'' and ``decision difficulty'' are best indicated by times of high \%VRE, not high \%RES. ~Also, that's system-wide, not regional.
\par
\section{\% VRE and \%RES}

\begin{itemize}

\item 
\par
\textbf{\% VRE} better for decision difficulty assessment
\par
\begin{itemize}

\item large \& rooftop solar + wind over all else (including hydro)
\item hydro is on the bottom because it's controllable

\end{itemize}

\item \textbf{system-wide} instead of regional is better for "physical realities"
\item Somewhat more thorough definitions at: (WattClarity, 2023)

\end{itemize}

\section{TODO}

\begin{itemize}

\item did the price spike on the days mentioned here: ~3rd and 4th July 2023

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Mcardle23lowPctVREaemo.pdf}
}

@article{WattClarity23pctVREpctRESwidget,
  title = {\% {{VRE}} and \% {{Renewables}} in the `{{Live Supply}} \& {{Demand}}' Widget},
  author = {WattClarity},
  year = {2023},
  journal = {WattClarity},
  url = {https://wattclarity.com.au/other-resources/widgets/reneweconomy-widget/percent-vre-and-renewables/},
  abstract = {We (at Global-Roam Pty Ltd) added in these four percentages in November 2022 to assist users of the widget (at RenewEconomy and elsewhere) keep track of the current status of energy transition. Background to the Upgrade There are two sets of two percentages -- one set for the NEM (which is where our focus is), and one set for the WEM (which operates in the south-west corner of Western Australia).},
  note = {WattClarity23pctVREpctRESwidget
\par
\%VRE as these guys define it, is not a forecast but a realtime number. ~This is a bit more detail about what data goes into the \%VRE advocated for in (Mcardle, 2023), which gives examples for a couple of days. ~
\par
\begin{quotation}

\par
\%VRE - (VRE / total supply) * 100
\par
\end{quotation}

\par
Is total suppy something like ``dispatchable'' generation?
\par
Different definitions for NEM and WEM (SW AUS).
\par
\textbf{For NEM it's} (see pdf for more details)
\par
\begin{itemize}

\item wind: actual (``metered,'' I think) 5 minutes
\item large solar: actual, 5 minutes
\item wind and large solar are labeled by AEMO as ``semi-scheduled,'' so look for that.
\item 
\par
small sloar: estimated by NEM
\par
\begin{itemize}

\item 30 minute periods
\item somewhate lagged

\end{itemize}

\item total supply must be controllable gen (which would include hydro)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\WattClarity23pctVREpctRESwidget.pdf}
}

@misc{TASK0245312MMSPredispatch,
  title = {{{TASK0245312}} - {{MMS Predispatch}} Documentation for {{PeriodID}} - Scotto@sharpleaf.Org - {{Scotto Mail}}},
  url = {https://mail.google.com/mail/u/0/#drafts/FMfcgzGxRndgXpGDxPpDPbPBkcblJgSB},
  urldate = {2024-02-22},
  file = {C:\Users\scott\Zotero\storage\EZI6AB6Q\0.html}
}

@misc{AEMO24constraintFAQ,
  title = {{{AEMO}} {\textbar} {{Constraint FAQ}}},
  author = {AEMO},
  year = {2024},
  journal = {Constraint Frequently Asked Questions},
  url = {https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem/system-operations/congestion-information-resource/constraint-faq},
  urldate = {2024-02-23},
  langid = {english},
  note = {AEMO24constraintFAQ
\par
Describes what AEMO considers a ``contstraint,'' and how it affects AEMO prices.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\AEMO24constraintFAQ.pdf}
}

@misc{Susanto23nemdeConstraintEqsAEMO,
  type = {{{LinkedIn}}},
  title = {How to Decipher {{NEMDE}} Constraint Equation Formulations},
  author = {Susanto, Julius},
  year = {2023},
  month = apr,
  url = {https://www.linkedin.com/pulse/how-decipher-nemde-constraint-equation-formulations-julius-susanto/},
  urldate = {2024-02-23},
  abstract = {The way constraint equation data that is used in the NEM Dispatch Engine (NEMDE) is organised and publicly disseminated is not very intuitive. This article is an attempt to demystify and make sense of how NEMDE constraint equation formulations are structured and published.},
  keywords = {AEMO,hasCode,Price forecasting},
  note = {Susanto23nemdeConstraintEqsAEMO
\par
Explains how to understand the AEMO constraint equations. ~Has worked examples, and a Jupyter notebook in a GitHub repo.},
  file = {C:\Users\scott\Zotero\storage\KLZDLPY2\Susanto23nemdeConstraintEqsAEMO.html}
}

@techreport{AEMO24mnthCnstraintRepDec23.,
  title = {Monthly {{Constraint Report December}} 2023},
  author = {AEMO},
  year = {2024},
  note = {AEMO24mnthCnstraintRepDec23
\par
How constraints affect price, how to they can be converted to an upper bound on the value per MW of congestion. ~This requires computing the ``binding impact,'' which is the marginal impact / 12. ~Marginal impact is reported in predispatch (Gatt, 2023)
\par
Lists top 10 constraints for the month etc. ~Rank seems to be by binding impact, but I'm not sure.
\par
\begin{itemize}

\item is binding impact reported in AEMO tool? ~This one: (AEMO, 2023)
\item another AEMO viz tool: (Global-Roam, 2022)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\AEMO24mnthCnstraintRepDec23..pdf}
}

@misc{AEMO23aboutConstraintsTool,
  title = {About {{Constraints}}},
  author = {AEMO},
  year = {2023},
  month = oct,
  url = {https://visualisations.aemo.com.au/aemo/web-help/Content/EMMScommon/AboutConstraints.html?TocPath=Energy%20Market%20Management%20System%20(EMMS)%7CMarketInfo%7CAbout%20Constraints%7C_____0},
  urldate = {2024-02-23},
  abstract = {The Constraints interface is a tool allowing participants to view constraint equation IDs in a human readable format. IDs used on the left and right-hand sides of the constraint equation are converted to a plain English description, for example: "Q\_DIRLK\_H31MDNR\_TRFMR" becomes "MW flow on Molendinar 275/110kV transformer". Participants can make use of this tool to obtain plain English descriptions of constraint equations associated with constraint sets.},
  keywords = {AEMO,hasCode,Price forecasting},
  note = {AEMO23aboutConstraintsTool
\par
AEMO has a tool available for market participants that displays constraints in human readable form. ~But is the data available in their tables already? ~Also, does this tool report binding impact?
\par
\begin{itemize}

\item binding impact defined here: (AEMO, 2024)
\item  Another AEMO viz tool: (Global-Roam, 2022)

\end{itemize}},
  file = {C:\Users\scott\Zotero\storage\VVHN4NG9\AEMO23aboutConstraintsTool.html}
}

@misc{Global-Roam22Ez2viewNavigateSoftware,
  title = {Ez2view {\textbar} {{Navigate}} the Complexity of the {{NEM}}},
  author = {{Global-Roam}},
  year = {2022},
  url = {https://www.ez2viewaustralia.info/},
  urldate = {2024-02-23},
  keywords = {AEMO,hasCode,Price forecasting},
  note = {Global-Roam22Ez2viewNavigateSoftware
\par
Software for visualizating AEMO constraints, bid stacks, etc. ~Does it know binding impact, etc?
\par
\begin{itemize}

\item binding impact: (AEMO, 2024)
\item mentions this tool: (Susanto, 2023)
\item AEMO constraint tool: (AEMO, 2023)

\end{itemize}}
}

@article{Katona23priceMechSurveyAEMO,
  title = {A {{Price Mechanism Survey}} of the {{Australian National Electricity Market}}},
  author = {Katona, Krisztina and Sklibosios Nikitopoulos, Christina and Schl{\"o}gl, Erik},
  year = {2023},
  journal = {Available at SSRN 4428450},
  url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4428450},
  urldate = {2024-02-23},
  abstract = {The Australian National Electricity Market (NEM) is an energy-only zonal instance of the integrated pool model without a day-ahead market where a security-constrained economic- dispatch (SCED) engine controls dispatch and sets the price every 5 minutes. After a brief overview of pool markets across the world, this paper provides a detailed description of the NEM market design and price mechanism, drawn from a multitude of industry publications and the available auction data. Then to elucidate the NEM Dispatch Engine's process of wholesale price determination, a bid stack--type modelling framework is constructed from first principles with its suitability and limitations discussed against real-world examples from the NEM's SCED mechanism.},
  keywords = {obsLitNote},
  note = {Katona23priceMechSurveyAEMO
\par
View said that this wasn't the current AEMO procedure, maybe it was a proposal for one. ~Also, she couldn't make sense of the dual fuel generator explanation either.
\par
New and current paper explaining how prices are set, was referenced in \href{https://nemseer.readthedocs.io/en/latest/glossary.html\#id23}{NEMSEER docs}. READ.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Katona23priceMechSurveyAEMO.pdf}
}

@misc{McArdle18nemPriceSetBeginners,
  title = {Beginner's {{Guide}} to How Dispatch Works in the {{NEM}}, and Hence How Prices Are Set},
  author = {McArdle, Paul},
  year = {2018},
  month = aug,
  journal = {WattClarity},
  url = {https://wattclarity.com.au/articles/2018/08/beginners-guide-to-how-dispatch-works-in-the-nem-and-hence-how-prices-are-set/},
  urldate = {2024-02-23},
  abstract = {Walking through 5 (much simplified) "Dispatch Intervals" to illustrate some starting principles of marginal price based dispatch arrangements, such as used in the National Electricity Market},
  langid = {australian},
  keywords = {AEMO,Price forecasting},
  note = {McArdle18nemPriceSetBeginners
\par
2018 tutorial on AEMO price setting w/ video. ~How relevant now? ~It's a long time before the 2021 (?) change in the market.},
  file = {C:\Users\scott\Zotero\storage\H299FY57\McArdle18nemPriceSetBeginners.html}
}

@article{Naik24riskHardEstGW,
  title = {Risk {{Models Behind World}}'s {{Best Hedge Fund Strategy Are Getting}} a {{Lot Harder}} to {{Crack}}},
  author = {{Gautam Naik {and} Sheryl Tian Tong Lee}},
  year = {2024},
  month = feb,
  journal = {Bloomberg.com},
  url = {https://www.bloomberg.com/news/articles/2024-02-25/catastrophe-bonds-behind-record-hedge-fund-returns-face-new-era-of-risk},
  urldate = {2024-02-26},
  abstract = {As the best hedge fund strategy of 2023 becomes a magnet for mainstream investors, the risk models it relies on are getting a lot tougher to crack.},
  langid = {english},
  note = {Naik24riskHardEstGW
\par
Global warming is increasing mid-severity, mid-probability weather at a rate less than less probable 1-100 events. ~It's mostly thunderstorms, messing up insurance markets. ~For some reason, estimating the aggregate risks is harder. ~
\par
They mention a lack of data, but wouldn't there be an even larger lack of data for 1-100 events? ~Is this b/c thunderstorms haven't been recorded until recently? ~Or because GW has made recent changes to thunderstorms but not to hurricanes (seems unlikely b/c I keep reading about increased hurricane severity and frequency)
\par
See my \href{https://www.evernote.com/shard/s13/nl/1523219/f84cea84-7ef9-4f57-83ef-856c909c2d92?title=Catastrophe\%20Bonds\%20Behind\%20Record\%20Hedge\%20Fund\%20Returns\%20Face\%20New\%20Era\%20of\%20Risk}{evernote}}
}

@article{McArdle24OpinionFemalePopes,
  title = {Opinion {\textbar} {{Female}} Popes? {{Google}}'s Amusing {{AI}} Bias Underscores a Serious Problem.},
  shorttitle = {Opinion {\textbar} {{Female}} Popes?},
  author = {McArdle, Megan},
  year = {2024},
  month = feb,
  journal = {Washington Post},
  issn = {0190-8286},
  url = {https://www.washingtonpost.com/opinions/2024/02/27/google-gemini-bias-race-politics/},
  urldate = {2024-02-27},
  abstract = {Gemini's prejudices demand a rewrite of the implicit rules by which large swaths of tech, education and media appear to have been governed.},
  langid = {american}
}

@article{Bernard23explainMLphysAge,
  title = {Explainable Machine Learning Framework to Predict Personalized Physiological Aging},
  author = {Bernard, David and Doumard, Emmanuel and Ader, Isabelle and Kemoun, Philippe and Pag{\`e}s, Jean-Christophe and Galinier, Anne and Cussat-Blanc, Sylvain and Furger, Felix and Ferrucci, Luigi and Aligon, Julien and Delpierre, Cyrille and P{\'e}nicaud, Luc and Monsarrat, Paul and Casteilla, Louis},
  year = {2023},
  month = jun,
  journal = {Aging Cell},
  volume = {22},
  number = {8},
  pages = {e13872},
  issn = {1474-9718},
  doi = {10.1111/acel.13872},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10410015/},
  urldate = {2024-03-01},
  abstract = {Attaining personalized healthy aging requires accurate monitoring of physiological changes and identifying subclinical markers that predict accelerated or delayed aging. Classic biostatistical methods most rely on supervised variables to estimate physiological aging and do not capture the full complexity of inter-parameter interactions. Machine learning (ML) is promising, but its black box nature eludes direct understanding, substantially limiting physician confidence and clinical usage. Using a broad population dataset from the National Health and Nutrition Examination Survey (NHANES) study including routine biological variables and after selection of XGBoost as the most appropriate algorithm, we created an innovative explainable ML framework to determine a Personalized physiological age (PPA). PPA predicted both chronic disease and mortality independently of chronological age. Twenty-six variables were sufficient to predict PPA. Using SHapley Additive exPlanations (SHAP), we implemented a precise quantitative associated metric for each variable explaining physiological (i.e., accelerated or delayed) deviations from age-specific normative data. Among the variables, glycated hemoglobin (HbA1c) displays a major relative weight in the estimation of PPA. Finally, clustering profiles of identical contextualized explanations reveal different aging trajectories opening opportunities to specific clinical follow-up. These data show that PPA is a robust, quantitative and explainable ML-based metric that monitors personalized health status. Our approach also provides a complete framework applicable to different datasets or variables, allowing precision physiological age estimation., Personalized physiological age (PPA) is a robust, quantitative and explainable machine learning-based metric that monitors health status. Our approach provides a complete framework applicable to different datasets or variables, allowing precision physiological age estimation.},
  pmcid = {PMC10410015},
  pmid = {37300327},
  keywords = {featsel,hasCode},
  note = {Bernard23explainMLphysAge
\par
Does featsel using GrootCV from the \href{https://www.evernote.com/l/AA0n6_GrboFHip8sh-ynF0N/}{ARFS} python \href{https://arfs.readthedocs.io/en/latest/Methods\%20overview.html}{package} to pick physiological variables (GrootCV uses lightgbm SHAP-derived feature importance, is \href{https://arfs.readthedocs.io/en/latest/Methods\%20overview.html\#grootcv-a-new-method}{said} to smooth ``noise''. I believe the author thinks its better than Boruta).
\par
Also uses \emph{optuna }to tune XGBOOST. ~Maybe that's better?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bernard23explainMLphysAge.pdf}
}

@misc{Pyomo24optimPkg,
  title = {About {{Pyomo}}},
  journal = {Pyomo},
  url = {http://www.pyomo.org/about},
  urldate = {2024-03-02},
  langid = {american},
  keywords = {hasCode,optimization},
  note = {Pyomo24optimPkg
\par
Python optimization package. ~Has a ton of extensions, like DOE (design of experiments), etc.}
}

@misc{Hao24waterDesertAI,
  title = {{{AI Is Taking Water From}} the {{Desert}}},
  author = {Hao, Karen},
  year = {2024},
  month = mar,
  journal = {The Atlantic},
  url = {https://www.theatlantic.com/technology/archive/2024/03/ai-water-climate-microsoft/677602/},
  urldate = {2024-03-03},
  abstract = {New data centers are springing up every week. Can the Earth sustain them?},
  chapter = {Technology},
  langid = {english},
  keywords = {genAI},
  note = {Hao24waterDesertAI
\par
Bits
\par
\begin{itemize}

\item rapid buildout ``biggest ever'' ~Bigger than RES?
\item water
\item MS pledges
\item etc.

\end{itemize}},
  file = {C:\Users\scott\Zotero\storage\JWUM9XYV\Hao24waterDesertAI.html}
}

@misc{Simkute24genAIironyProductivty,
  title = {Ironies of {{Generative AI}}: {{Understanding}} and Mitigating Productivity Loss in Human-{{AI}} Interactions},
  shorttitle = {Ironies of {{Generative AI}}},
  author = {Simkute, Auste and Tankelevitch, Lev and Kewenig, Viktor and Scott, Ava Elizabeth and Sellen, Abigail and Rintel, Sean},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2402.11364v1},
  urldate = {2024-03-03},
  abstract = {Generative AI (GenAI) systems offer opportunities to increase user productivity in many tasks, such as programming and writing. However, while they boost productivity in some studies, many others show that users are working ineffectively with GenAI systems and losing productivity. Despite the apparent novelty of these usability challenges, these 'ironies of automation' have been observed for over three decades in Human Factors research on the introduction of automation in domains such as aviation, automated driving, and intelligence. We draw on this extensive research alongside recent GenAI user studies to outline four key reasons for productivity loss with GenAI systems: a shift in users' roles from production to evaluation, unhelpful restructuring of workflows, interruptions, and a tendency for automation to make easy tasks easier and hard tasks harder. We then suggest how Human Factors research can also inform GenAI system design to mitigate productivity loss by using approaches such as continuous feedback, system personalization, ecological interface design, task stabilization, and clear task allocation. Thus, we ground developments in GenAI system usability in decades of Human Factors research, ensuring that the design of human-AI interactions in this rapidly moving field learns from history instead of repeating it.},
  langid = {english},
  keywords = {genAI},
  note = {Simkute24genAIironyProductivty},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Simkute24genAIironyProductivty.pdf}
}

@techreport{ITRex24genAIbizLeaders,
  title = {Generative {{AI}} for {{Business Leaders}}: {{Act Now}}, {{Lead Tomorrow}}},
  author = {{ITRex}},
  year = {2024},
  institution = {ITRex.  University of Washington},
  url = {https://itrexgroup.com/wp-content/uploads/2023/09/generative-ai-for-business-leaders.pdf},
  urldate = {2024-03-03},
  abstract = {This revolution has begun. The next big disruption is here, ready to shift the economy and change the way we live and work forever {\texthorizontalbar} much as personal  computers, the internet, and smartphones did. It's known as generative AI. After the 30 November 2022 release of OpenAI's GPT-3.5, a landmark example of generative AI, this new type of intelligence in machines has taken the world by  storm. Unlike traditional AI known since the 1950s, generative AI doesn't just process  existing data. It learns from it and uses what it has learned to generate new  creations that didn't exist before, including text, images, videos, and code.~ Tools such as OpenAI's ChatGPT, DALL-E, and Stable Diffusion, Google's Bard,  and IBM Watson are swiftly democratizing generative AI for business and private  applications. As this technology develops, experts predict tectonic changes.~ Skills will evolve. Smaller players will gain creative and technical powers once  exclusive to big organizations. Education will become more personalized, and  healthcare breakthroughs should be expected too. As this technology develops, experts predict tectonic changes.~ Skills will evolve. Smaller players will gain creative and technical powers once exclusive to big organizations. Education will become more personalized, and  healthcare breakthroughs should be expected too. The transformation will take time, but it won't be long. The speed at which this technology is being adopted for innovation is breathtaking. So, a key question  business leaders face today is: what's in this for their business right now? What steps can they take to ride this wave of disruption?  We've prepared this ebook to help you find the answer.},
  langid = {american},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\ITRex24genAIbizLeaders.pdf}
}

@article{Rowlett24GenerativeAIAccuracy,
  title = {Generative {{AI}} and Accuracy in the History of Mathematics},
  author = {Rowlett, Peter},
  year = {2024},
  journal = {British Journal for the History of Mathematics},
  volume = {0},
  number = {0},
  pages = {1--6},
  publisher = {Taylor \& Francis},
  issn = {2637-5451},
  doi = {10.1080/26375451.2024.2312789},
  url = {https://doi.org/10.1080/26375451.2024.2312789},
  urldate = {2024-03-03},
  abstract = {Generative AI systems designed to produce text do so by drawing on inferences made from training data, which may mean they reproduce factual errors or biases contained in that data. This process is illustrated by querying ChatGPT with questions from a history of mathematics quiz designed to highlight the common occurrence of mathematical results being misattributed. ChatGPT's performance on a set of decades-old common misconceptions is mixed, illustrating the potential for these systems to reproduce and reinforce historical inaccuracies and misconceptions.},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Metz24chatGPTphysWorld.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Rowlett24GenerativeAIAccuracy.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Z2TESTyDonadee15operBattUncertPhDSlides.pptx;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Z2TESTytextfile.txt;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\ZTEST_Babic23TailCoRNewSimplea.docx;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Ng24BrobotsAgentsRisksPolitics.html}
}

@misc{Shipman24CanAIThat,
  title = {Can {{AI Do That}}? {{The Challenges}}, {{Limitations}}, and {{Opportunities}} of {{Generative AI}}},
  shorttitle = {Can {{AI Do That}}?},
  author = {Shipman, Matt},
  year = {2024},
  month = feb,
  journal = {Medium},
  url = {https://medium.com/@shiplives/can-ai-do-that-the-challenges-limitations-and-opportunities-of-generative-ai-a1e3c0e0bc00},
  urldate = {2024-03-03},
  abstract = {There is a lot of discussion in the public sphere about how tools that use artificial intelligence (AI) to generate images and text will{\dots}},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Shipman24CanAIThat.pdf}
}

@misc{Fraser24genAIhammerNail,
  title = {Generative {{AI}} Is a Hammer and No One Knows What Is and Isn't a Nail},
  author = {Fraser, Colin},
  year = {2024},
  month = feb,
  journal = {Medium},
  url = {https://medium.com/@colin.fraser/generative-ai-is-a-hammer-and-no-one-knows-what-is-and-isnt-a-nail-4c7f3f0911aa},
  urldate = {2024-03-03},
  abstract = {This analogy is going to seem a bit tortured but bear with me. Imagine a world without hammers. You're driving nails into the wall with{\dots}},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Fraser24genAIhammerNail2.pdf}
}

@misc{Memon24hallucLMsearchEngine,
  title = {Search Engines Post-{{ChatGPT}}: {{How}} Generative Artificial Intelligence Could Make Search Less Reliable},
  shorttitle = {Search Engines Post-{{ChatGPT}}},
  author = {Memon, Shahan Ali and West, Jevin D.},
  year = {2024},
  month = feb,
  journal = {Center for an Informed Public},
  url = {https://www.cip.uw.edu/2024/02/18/search-engines-chatgpt-generative-artificial-intelligence-less-reliable/},
  urldate = {2024-03-03},
  abstract = {What happens when inherently hallucinating language models are employed within search engines without proper guardrails in place?},
  langid = {american},
  note = {Memon24hallucLMsearchEngine
\par
\begin{itemize}

\item google trying to filter AI generated stuff: (Will Shanklin, 2024)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Memon24hallucLMsearchEngine.html}
}

@misc{McCOy24tenMostCommoncGPTlims,
  title = {10 {{Most Common ChatGPT Limitations}}},
  author = {McCoy, Juia},
  year = {2024},
  month = feb,
  journal = {Content @ Scale},
  url = {https://contentatscale.ai/blog/chatgpt-limitations/},
  urldate = {2024-03-03},
  abstract = {Discover how to navigate chatgpt limitations effectively for better AI interactions, addressing biases, technical challenges, and more in our latest guide.},
  chapter = {Artificial Intelligence},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\McCOy24tenMostCommoncGPTlims2.pdf}
}

@article{Prillaman24chatGPTscientistHyperProd,
  title = {Is {{ChatGPT}} Making Scientists Hyper-Productive? {{The}} Highs and Lows of Using {{AI}}},
  shorttitle = {Is {{ChatGPT}} Making Scientists Hyper-Productive?},
  author = {Prillaman, McKenzie},
  year = {2024},
  month = feb,
  journal = {Nature},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-024-00592-w},
  url = {https://www.nature.com/articles/d41586-024-00592-w},
  urldate = {2024-03-03},
  abstract = {Large language models are transforming scientific writing and publishing. But the productivity boost that these tools bring could come with a downside.},
  copyright = {2024 Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: News Explainer\\
Subject\_term: Machine learning, Scientific community, Publishing, Computer science},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Prillaman24chatGPTscientistHyperProd2.pdf}
}

@misc{Clancy23WhenChatGPTwrong,
  title = {When {{ChatGPT}} Gets It Wrong},
  author = {Clancy, Lisa},
  year = {2023},
  month = jun,
  journal = {International Science Editing},
  url = {https://www.internationalscienceediting.com/when-chatgpt-gets-it-wrong/},
  urldate = {2024-03-03},
  abstract = {In this blog, we present the mistakes we found when we tested ChatGPT against our human editors.},
  langid = {american},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Clancy23WhenChatGPTwrong2.pdf}
}

@misc{Gutierrez24GenerativeAIReport3124,
  title = {Generative {{AI Report}} -- 3/1/2024},
  author = {Gutierrez, Daniel},
  year = {2024},
  month = mar,
  journal = {insideBIGDATA},
  url = {https://insidebigdata.com/2024/03/01/generative-ai-report-3-1-2024/},
  urldate = {2024-03-03},
  abstract = {Welcome to the Generative AI Report round-up feature here on insideBIGDATA with a special focus on all the new applications and integrations tied to generative AI technologies. We've been receiving so many cool news items relating to applications and deployments centered on large language models (LLMs), we thought it would be a timely service for readers to start a new channel along these lines. The combination of a LLM, fine tuned on proprietary data equals an AI application, and this is what these innovative companies are creating. The field of AI is accelerating at such fast rate, we want to help our loyal global audience keep pace.},
  langid = {american},
  keywords = {obsLitNote},
  note = {Gutierrez24GenerativeAIReport3124
\par
Also
\par
\begin{itemize}

\item (Maslej et al., 2023)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gutierrez24GenerativeAIReport32.pdf}
}

@article{Chesterman24GoodBorrowGreatSteal,
  title = {Good Models Borrow, Great Models Steal: Intellectual Property Rights and Generative {{AI}}},
  shorttitle = {Good Models Borrow, Great Models Steal},
  author = {Chesterman, Simon},
  year = {2024},
  month = feb,
  journal = {Policy and Society},
  pages = {puae006},
  issn = {1449-4035},
  doi = {10.1093/polsoc/puae006},
  url = {https://doi.org/10.1093/polsoc/puae006},
  urldate = {2024-03-03},
  abstract = {Two critical policy questions will determine the impact of generative artificial intelligence (AI) on the knowledge economy and the creative sector. The first concerns how we think about the training of such models---in particular, whether the creators or owners of the data that are ``scraped'' (lawfully or unlawfully, with or without permission) should be compensated for that use. The second question revolves around the ownership of the output generated by AI, which is continually improving in quality and scale. These topics fall in the realm of intellectual property, a legal framework designed to incentivize and reward only human creativity and innovation. For some years, however, Britain has maintained a distinct category for ``computer-generated'' outputs; on the input issue, the EU and Singapore have recently introduced exceptions allowing for text and data mining or computational data analysis of existing works. This article explores the broader implications of these policy choices, weighing the advantages of reducing the cost of content creation and the value of expertise against the potential risk to various careers and sectors of the economy, which might be rendered unsustainable. Lessons may be found in the music industry, which also went through a period of unrestrained piracy in the early digital era, epitomized by the rise and fall of the file-sharing service Napster. Similar litigation and legislation may help navigate the present uncertainty, along with an emerging market for ``legitimate'' models that respect the copyright of humans and are clear about the provenance of their own creations.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chesterman24GoodBorrowGreatSteal.pdf}
}

@misc{Casus24accountAIcomplement,
  title = {The {{Golden Age}} of ``{{AI Complementarity}}'' for the {{Accounting Profession}}{\dots}},
  author = {Casas, Rafael},
  year = {2024},
  month = feb,
  journal = {PrimeGlobal: Association of Advisory Accounting Firms},
  url = {https://www.primeglobal.net/news/golden-age-ai-complementarity-quickfee},
  urldate = {2024-03-03},
  abstract = {This is a thought leadership article from PrimeGlobal Alliance Partner QuickFee on how Generative AI will impact the accounting profession, and the importance of intrapreneurship within companies to ensure its successful adoption.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Casus24accountAIcomplement2.pdf}
}

@misc{Pratik24genAIuseCaseLegal,
  title = {Generative {{AI}} in {{Legal}}: 5 {{Most Effective Use Cases}}},
  shorttitle = {Generative {{AI}} in {{Legal}}},
  author = {Pratik, R},
  year = {2024},
  month = feb,
  url = {https://www.intuz.com/blog/generative-ai-in-legal-industry},
  urldate = {2024-03-03},
  abstract = {Discover 5 ways Generative AI is transforming legal practices, helping lawyers with legal research, contract analysis, draft documents, search for patents, and more.},
  langid = {english},
  file = {C:\Users\scott\Zotero\storage\6ATW4QVV\Pratik24genAIuseCaseLegal.html}
}

@misc{zhu24genAIsecurityCounterMeas,
  title = {Generative {{AI Security}}: {{Challenges}} and {{Countermeasures}}},
  shorttitle = {Generative {{AI Security}}},
  author = {Zhu, Banghua and Mu, Norman and Jiao, Jiantao and Wagner, David},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2402.12617v1},
  urldate = {2024-03-03},
  abstract = {Generative AI's expanding footprint across numerous industries has led to both excitement and increased scrutiny. This paper delves into the unique security challenges posed by Generative AI, and outlines potential research directions for managing these risks.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\zhu24genAIsecurityCounterMeas2.pdf}
}

@misc{Sisson24aiDataCtrPow,
  title = {{{AI}} Frenzy Complicates Efforts to Keep Power-Hungry Data Sites Green},
  author = {Sisson, Patrick},
  year = {2024},
  month = mar,
  journal = {The Seattle Times},
  url = {https://www.seattletimes.com/business/ai-frenzy-complicates-efforts-to-keep-power-hungry-data-sites-green/},
  urldate = {2024-03-04},
  abstract = {The AI booming growth is radically reshaping an already red-hot data center market, raising questions about whether these sites can be operated sustainably.},
  langid = {american},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sisson24aiDataCtrPow2.pdf}
}

@article{Fowler24aiTaxAdviceAwful,
  title = {Review {\textbar} {{TurboTax}} and {{H}}\&{{R Block}} Now Use {{AI}} for Tax Advice. {{It}}'s Awful.},
  author = {Fowler, Geoffrey A.},
  year = {2024},
  month = mar,
  journal = {Washington Post},
  issn = {0190-8286},
  url = {https://www.washingtonpost.com/technology/2024/03/04/ai-taxes-turbotax-hrblock-chatbot/},
  urldate = {2024-03-04},
  abstract = {In our tech columnist's tests, new chatbots in popular tax services were unhelpful or wrong as much as half of the time.},
  langid = {american},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Fowler24aiTaxAdviceAwful2.pdf}
}

@article{Ghosh24aiCashFlowJPMorgan,
  title = {{{JPMorgan}}'s {{AI-Aided Cashflow Model Can Cut Manual Work}} by 90\%},
  author = {Ghosh, Suvashree},
  year = {2024},
  month = mar,
  journal = {Bloomberg.com},
  url = {https://www.bloomberg.com/news/articles/2024-03-04/jpmorgan-s-ai-aided-cashflow-model-can-cut-manual-work-by-90},
  urldate = {2024-03-04},
  abstract = {JPMorgan Chase \&amp; Co. helped some of its corporate customers slash manual work by almost 90\% with its cashflow management tool that runs on artificial intelligence, bringing the largest US bank one step closer to charging for this service.},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ghosh24aiCashFlowJPMorgan2.pdf}
}

@article{Knight24genAIscaleSoFar,
  title = {Google's {{AI Boss Says Scale Only Gets You So Far}}},
  author = {Knight, Will},
  year = {2024},
  month = feb,
  journal = {Wired},
  issn = {1059-1028},
  url = {https://www.wired.com/story/deepmind-ceo-demis-hassabis-interview-artificial-intelligence-scale/},
  urldate = {2024-03-04},
  abstract = {In an interview with WIRED, DeepMind CEO Demis Hassabis says the biggest breakthroughs in AI are yet to come---and will take more than just chips.},
  chapter = {tags},
  langid = {american},
  keywords = {alphabet,artificial intelligence,chatbots,deepmind,google,openai},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Knight24genAIscaleSoFar.pdf;C\:\\Users\\scott\\Zotero\\storage\\YAHPP2AV\\deepmind-ceo-demis-hassabis-interview-artificial-intelligence-scale.html}
}

@misc{Kilcher24mlNewsYouTube240402,
  title = {[{{ML News}}] {{Groq}}, {{Gemma}}, {{Sora}}, {{Gemini}}, and {{Air Canada}}'s Chatbot Troubles},
  author = {Kilcher, Yannic},
  year = {2024},
  month = mar,
  url = {https://www.youtube.com/watch?v=3nF8Z6HgSLQ},
  urldate = {2024-03-05},
  abstract = {Your dose of ML News! OUTLINE: 0:00 - Intro 0:20 - Gemma \& Gemini 3:40 - Groq 6:30 - Nvidia EOS Supercomputer 7:15 - Gpulist.ai 8:20 - Demis Hassabis on scale 10:10 - Hardware wars 12:05 - Sora 15:10 - Gemini 1.5 Pro \& Long Context 18:45 - Air Canada must pay for chatbot mistake 23:30 - Giant Rat Balls 26:25 - Various News References: https://blog.google/technology/develo... https://twitter.com/altryne/status/17... https://twitter.com/paulg/status/1760... https://groq.com/ https://twitter.com/mattshumer\_/statu... ~~/~1759483896322781584~~ https://wow.groq.com/news\_press/groq-... https://twitter.com/tianle\_cai/status... ~~/~1759728119005712837~~ ~~/~1759720197055791188~~ ~~/~1759704303810519271~~ ~~/~1759709223276228825~~ https://www.techpowerup.com/319172/nv... https://andromeda.ai/ https://gpulist.ai/ https://archive.ph/G6POi https://www.tomshardware.com/tech-ind... https://futurism.com/the-byte/ai-dest... https://twitter.com/\_akhaliq/status/1... https://twitter.com/\_Borriss\_/status/... ~~/~1758650919430848991~~ https://twitter.com/tsarnick/status/1... https://twitter.com/MartinNebelong/st... https://twitter.com/OriolVinyalsML/st... ~~/~1759804492919275555~~ https://twitter.com/mattshumer\_/statu... https://twitter.com/haoliuhl/status/1... https://github.com/lucidrains/ring-at... https://bc.ctvnews.ca/air-canada-s-ch... https://arstechnica.com/tech-policy/2... https://www.cbc.ca/news/canada/britis... https://twitter.com/kareem\_carr/statu... https://arstechnica.com/science/2024/... https://www.vice.com/en/article/4a389... https://twitter.com/karpathy/status/1... https://twitter.com/karpathy/status/1... https://www.nature.com/articles/d4158... ~~/~1757359611399532921~~ https://cohere.com/research/aya https://twitter.com/OfirPress/status/... https://github.com/mut-ex/gligen-gui https://www.projectaria.com/datasets/... https://twitter.com/StabilityAI/statu... https://twitter.com/gordic\_aleksa/sta... https://huggingface.co/gordicaleksa/Y... https://huggingface.co/datasets/nvidi... https://interestingengineering.com/sc... https://archive.ph/caW1Y\#selection-49... https://newatlas.com/robotics/seeing-... https://os-copilot.github.io/ https://www.businessinsider.com/apple... https://techcrunch.com/2024/02/16/ant... https://archive.ph/Gbcgb Links: Homepage: https://ykilcher.com Merch: https://ykilcher.com/merch YouTube: ~~~/~yannickilcher~~ Twitter: ~~/~ykilcher~~ Discord: https://ykilcher.com/discord LinkedIn: ~~/~ykilcher~~ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https://www.subscribestar.com/yannick... Patreon: ~~/~yannickilcher~~ Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n},
  keywords = {externalNote,noted},
  note = {\textbf{Kilcher24mlNewsYouTube240402}
\par
See my Obsidian page named \textbf{@Kilcher24mlNewsYouTube240402}}
}

@misc{Guo23CuriousDeclineLinguistic,
  title = {The {{Curious Decline}} of {{Linguistic Diversity}}: {{Training Language Models}} on {{Synthetic Text}}},
  shorttitle = {The {{Curious Decline}} of {{Linguistic Diversity}}},
  author = {Guo, Yanzhu and Shang, Guokan and Vazirgiannis, Michalis and Clavel, Chlo{\'e}},
  year = {2023},
  month = nov,
  number = {arXiv:2311.09807},
  eprint = {2311.09807},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.09807},
  url = {http://arxiv.org/abs/2311.09807},
  urldate = {2024-03-05},
  abstract = {This study investigates the consequences of training large language models (LLMs) on synthetic data generated by their predecessors, an increasingly prevalent practice aimed at addressing the limited supply of human-generated training data. Diverging from the usual emphasis on performance metrics, we focus on the impact of this training methodology on linguistic diversity, especially when conducted recursively over time. To assess this, we developed a set of novel metrics targeting lexical, syntactic, and semantic diversity, applying them in recursive fine-tuning experiments across various natural language generation tasks. Our findings reveal a marked decrease in the diversity of the models' outputs through successive iterations. This trend underscores the potential risks of training LLMs on predecessor-generated text, particularly concerning the preservation of linguistic richness. Our study highlights the need for careful consideration of the long-term effects of such training approaches on the linguistic capabilities of LLMs.},
  archiveprefix = {arXiv},
  keywords = {genAI},
  note = {Guo23CuriousDeclineLLM
\par
AI performance declines when AI fed AI output, as it would when the internet is clogged w/ AI generated content. ~Good table.
\par
Same thing for stable diffusion images: (Hataya et al., 2023)
\par
\begin{itemize}

\item google trying to filter AI generated stuff: (Shanklin, 2024)

\end{itemize}

\par
Comment: Work in progress},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Guo23CuriousDeclineLLM.pdf}
}

@misc{Hataya23WillLargescaleGenerative,
  title = {Will {{Large-scale Generative Models Corrupt Future Datasets}}?},
  author = {Hataya, Ryuichiro and Bao, Han and Arai, Hiromi},
  year = {2023},
  month = aug,
  number = {arXiv:2211.08095},
  eprint = {2211.08095},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.08095},
  url = {http://arxiv.org/abs/2211.08095},
  urldate = {2024-03-05},
  abstract = {Recently proposed large-scale text-to-image generative models such as DALL\${\textbackslash}cdot\$E 2, Midjourney, and StableDiffusion can generate high-quality and realistic images from users' prompts. Not limited to the research community, ordinary Internet users enjoy these generative models, and consequently, a tremendous amount of generated images have been shared on the Internet. Meanwhile, today's success of deep learning in the computer vision field owes a lot to images collected from the Internet. These trends lead us to a research question: "{\textbackslash}textbf\{will such generated images impact the quality of future datasets and the performance of computer vision models positively or negatively?\}" This paper empirically answers this question by simulating contamination. Namely, we generate ImageNet-scale and COCO-scale datasets using a state-of-the-art generative model and evaluate models trained with "contaminated" datasets on various tasks, including image classification and image generation. Throughout experiments, we conclude that generated images negatively affect downstream performance, while the significance depends on tasks and the amount of generated images. The generated datasets and the codes for experiments will be publicly released for future research. Generated datasets and source codes are available from {\textbackslash}url\{https://github.com/moskomule/dataset-contamination\}.},
  archiveprefix = {arXiv},
  note = {Hataya23genAIcorruptData
\par
Images fed AI generated images look more and more alike
\par
Same thing for LLMs:
\par
\begin{itemize}

\item google trying to filter AI generated stuff: (Will Shanklin, 2024)

\end{itemize}

\par
Comment: ICCV 2023},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hataya23genAIcorruptData.pdf}
}

@misc{Robinson24contingCondPowInterruptAEMO,
  title = {Evaluating {{Major Contingencies}} \& {{Conditions}} with the {{Potential}} to {{Cause Power System Disruptions}}},
  author = {{Luke Robinson} and {Daniel Fracalossi}},
  year = {2024},
  month = mar,
  address = {AEMO Australia},
  url = {https://www.youtube.com/watch?v=A1ir9W4f56I},
  urldate = {2024-03-05},
  abstract = {Featured Speakers: Luke Robinson, Group Manager - Modelling \& Engineering, AEMO \& Daniel Fracalossi, Senior Engineer - Grid Performance \& Integration, AEMO Luke Robinson About the Webinar: AEMO undertakes the general power system risk review (GPSRR) annually for the National Electricity Market (NEM) in consultation with network service providers (NSPs), in accordance with the National Electricity Rules (NER). The purpose of the GPSRR is to review a prioritized set of power system risks, comprising events or conditions that, alone or in combination, would likely lead to cascading outages or major supply disruptions. For each priority risk, the GPSRR assesses the adequacy of current risk management arrangements and (where appropriate) options for future management. This webinar will provide an overview of the scope of the GPSRR including AEMO's approach to: Identifying priority risks based on actual power system incidents as well as extensive internal and external consultation. Conducting detailed power system analysis of non-credible contingency events using RMS and EMT simulation tools. Evaluating solution options to mitigate risks and improve power system resilience.},
  keywords = {AEMO,Electricity markets,externalNote,noted,Price forecasting},
  note = {Robinson24contingCondPowInterruptAEMO
\par
Maybe use for price forecasting features?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Robinson24contingCondPowInterruptAEMO.pdf}
}

@misc{Ananthan23genAIandCFDdesign,
  title = {Conceptual Design Using Generative {{AI}} and {{CFD}} Simulations on {{AWS}}},
  author = {{Vidyasagar Ananthan} and {Satheesh Maheswaran} and {Neil Ashton,} and {Srinivas Tadepalli}},
  year = {2023},
  month = oct,
  journal = {AWS HPC Blog},
  url = {https://aws.amazon.com/blogs/hpc/conceptual-design-using-generative-ai-and-cfd-simulations-on-aws/},
  urldate = {2024-03-06},
  abstract = {In this post we'll demonstrate how generative AI techniques can be combined with conventional physics-based computational fluid dynamics (CFD) simulations to create a rapid conceptual design process that can be used to explore new design concepts in the automotive, motorsport, and aerospace sectors from just a single image. Thanks to AWS services like AWS Batch, and the open-source TwinGraph, this can all be combined into an event-driven workflow on AWS that could scale to explore millions of possible scenarios. TwinGraph is the model orchestration module within the open-source TwinFlow framework that enables deploying predictive modeling, simulation, and Level 4 Digital Twin workflows at scale. The generative capability of machine learning (ML) algorithms holds significant promise across diverse industries. Generative AI techniques are powered by large machine learning models, pre-trained on very large amounts of data. We're seeing the impact of these models in several areas, including transformer models for natural language processing, text-to-image models like Stable Diffusion for image manipulation, and generative adversarial networks for zero-shot classifiers.},
  chapter = {AWS Batch},
  langid = {american},
  keywords = {genAI},
  note = {Ananthan23genAIandCFDdesign
\par
gen AI used kind of like genetic algorithm in optimization (CFD of cars, for drag).
\par
\begin{itemize}

\item Somebody else wrote detailed AWS instructions \href{https://ai.plainenglish.io/sagemaker-generative-ai-cfd-a-winning-formula-for-design-optimization-e699b60e0cf8}{here}.
\item also is a small LLM?
\item TODO: read carefully
\item detailed AWS in (Ugbaja, 2023)
\item more general LLM optimizer: (Yang et al., 2023)
\item could have used aesthic criteria to improve look too: (Kirstain et al., 2023) (if they already didn't)

\end{itemize}},
  file = {C:\Users\scott\Zotero\storage\KLIC7S3B\Ananthan23genAIandCFDdesign.html}
}

@misc{Ugbaja23aiPoweredDesignOpt,
  title = {Future-{{Proof Your Designs}}: {{Master AI}} and {{CFD Optimization}}},
  shorttitle = {Future-{{Proof Your Designs}}},
  author = {Ugbaja, Timothy},
  year = {2023},
  month = dec,
  journal = {Medium},
  url = {https://ai.plainenglish.io/sagemaker-generative-ai-cfd-a-winning-formula-for-design-optimization-e699b60e0cf8},
  urldate = {2024-03-06},
  abstract = {From Sketch to Simulation: A Step-by-Step Guide to AI-Powered Design Optimization},
  langid = {english},
  note = {Ugbaja23aiPoweredDesignOpt
\par
Detailed instructions for how to build (Vidyasagar Ananthan et al., 2023)},
  file = {C:\Users\scott\Zotero\storage\WHGNKEQ8\Ugbaja23aiPoweredDesignOpt.html}
}

@misc{Ghojogh21GenAdversNetsTutorial,
  title = {Generative {{Adversarial Networks}} and {{Adversarial Autoencoders}}: {{Tutorial}} and {{Survey}}},
  shorttitle = {Generative {{Adversarial Networks}} and {{Adversarial Autoencoders}}},
  author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
  year = {2021},
  month = nov,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2111.13282v1},
  urldate = {2024-03-06},
  abstract = {This is a tutorial and survey paper on Generative Adversarial Network (GAN), adversarial autoencoders, and their variants. We start with explaining adversarial learning and the vanilla GAN. Then, we explain the conditional GAN and DCGAN. The mode collapse problem is introduced and various methods, including minibatch GAN, unrolled GAN, BourGAN, mixture GAN, D2GAN, and Wasserstein GAN, are introduced for resolving this problem. Then, maximum likelihood estimation in GAN are explained along with f-GAN, adversarial variational Bayes, and Bayesian GAN. Then, we cover feature matching in GAN, InfoGAN, GRAN, LSGAN, energy-based GAN, CatGAN, MMD GAN, LapGAN, progressive GAN, triple GAN, LAG, GMAN, AdaGAN, CoGAN, inverse GAN, BiGAN, ALI, SAGAN, Few-shot GAN, SinGAN, and interpolation and evaluation of GAN. Then, we introduce some applications of GAN such as image-to-image translation (including PatchGAN, CycleGAN, DeepFaceDrawing, simulated GAN, interactive GAN), text-to-image translation (including StackGAN), and mixing image characteristics (including FineGAN and MixNMatch). Finally, we explain the autoencoders based on adversarial learning including adversarial autoencoder, PixelGAN, and implicit autoencoder.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ghojogh21GenAdversNetsTutorial.pdf}
}

@misc{Brownlee19introGAN,
  title = {A {{Gentle Introduction}} to {{Generative Adversarial Networks}} ({{GANs}})},
  author = {Brownlee, Jason},
  year = {2019},
  month = jun,
  journal = {MachineLearningMastery.com},
  url = {https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/},
  urldate = {2024-03-06},
  abstract = {Generative Adversarial Networks, or GANs for short, are an approach to generative modeling using deep learning methods, such as convolutional neural networks. Generative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used [{\dots}]},
  langid = {american},
  note = {Brownlee19introGAN
\par
A good place to start w/ GANs?
\par
Brownlee19introGAN test note 2
\par
Just to see if its copied to obsidian.},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Brownlee19introGAN.pdf;C\:\\Users\\scott\\Zotero\\storage\\ENU32RZY\\Brownlee19introGAN.html}
}

@misc{Biswal23whatAreGANs,
  title = {What Are {{Generative Adversarial Networks}} ({{GANs}})},
  author = {{Avijeet Biswal}},
  year = {2023},
  month = feb,
  journal = {Simplilearn.com},
  url = {https://www.simplilearn.com/tutorials/deep-learning-tutorial/generative-adversarial-networks-gans},
  urldate = {2024-03-06},
  abstract = {Understand what are \ding{52}Generative Adversarial Networks (GANs), Generator, and Discriminator, the\ding{52}types \ding{52}applications \& how \ding{52}GAN works with Math equations.},
  langid = {american},
  file = {C:\Users\scott\Zotero\storage\9Q922QXG\Biswal23whatAreGANs.html}
}

@misc{Shanklin24googleWeedAIspam,
  title = {Google Is Changing Its Search Results to Weed out {{SEO}} Spam},
  author = {Shanklin, Will},
  year = {2024},
  month = mar,
  journal = {Engadget},
  url = {https://www.engadget.com/google-is-changing-its-search-results-to-weed-out-seo-spam-195259063.html},
  urldate = {2024-03-06},
  abstract = {Amid complaints about its search results declining in quality, Google is tweaking its algorithms to do a better job of weeding out spammy or automated content. The company says the ranking updates, arriving in May, will ``keep the lowest-quality content out of search.''},
  langid = {american},
  file = {C:\Users\scott\Zotero\storage\DVDCEZKJ\Shanklin24googleWeedAIspam.html}
}

@misc{Rasul24LagLlamaFoundationModels,
  title = {Lag-{{Llama}}: {{Towards Foundation Models}} for {{Probabilistic Time Series Forecasting}}},
  shorttitle = {Lag-{{Llama}}},
  author = {Rasul, Kashif and Ashok, Arjun and Williams, Andrew Robert and Ghonia, Hena and Bhagwatkar, Rishika and Khorasani, Arian and Bayazi, Mohammad Javad Darvishi and Adamopoulos, George and Riachi, Roland and Hassen, Nadhir and Bilo{\v s}, Marin and Garg, Sahil and Schneider, Anderson and Chapados, Nicolas and Drouin, Alexandre and Zantedeschi, Valentina and Nevmyvaka, Yuriy and Rish, Irina},
  year = {2024},
  month = feb,
  number = {arXiv:2310.08278},
  eprint = {2310.08278},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.08278},
  url = {http://arxiv.org/abs/2310.08278},
  urldate = {2024-03-06},
  abstract = {Over the past years, foundation models have caused a paradigm shift in machine learning due to their unprecedented capabilities for zero-shot and few-shot generalization. However, despite the success of foundation models in modalities such as natural language processing and computer vision, the development of foundation models for time series forecasting has lagged behind. We present Lag-Llama, a general-purpose foundation model for univariate probabilistic time series forecasting based on a decoder-only transformer architecture that uses lags as covariates. Lag-Llama is pretrained on a large corpus of diverse time series data from several domains, and demonstrates strong zero-shot generalization capabilities compared to a wide range of forecasting models on downstream datasets across domains. Moreover, when fine-tuned on relatively small fractions of such previously unseen datasets, Lag-Llama achieves state-of-the-art performance, outperforming prior deep learning approaches, emerging as the best general-purpose model on average. Lag-Llama serves as a strong contender to the current state-of-art in time series forecasting and paves the way for future advancements in foundation models tailored to time series data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,obsLitNote},
  note = {Rasul24LagLlamaFoundationModels
\par
LLM forecast model. ~Lots of tutorials on it, plus a GitHub
\par
Comment: First two authors contributed equally. All data, models and code used are open-source. GitHub: https://github.com/time-series-foundation-models/lag-llama},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Rasul24LagLlamaFoundationTSfrcst.pdf}
}

@misc{Kilcher24claudeNotSentient,
  title = {No, {{Anthropic}}'s {{Claude}} 3 Is {{NOT}} Sentient},
  author = {Kilcher, Yannic},
  year = {2024},
  month = mar,
  url = {https://www.youtube.com/watch?v=GBOE9fVVVSM},
  urldate = {2024-03-06},
  abstract = {No, Anthropic's Claude 3 is not conscious or sentient or self-aware. References: https://www.anthropic.com/news/claude... https://twitter.com/\_akhaliq/status/1... https://twitter.com/idavidrein/status... https://twitter.com/TolgaBilge\_/statu... https://twitter.com/karinanguyen\_/sta... ~~/~1764722513014329620~~ https://www.lesswrong.com/posts/pc8uP... Links: Homepage: https://ykilcher.com Merch: https://ykilcher.com/merch YouTube: ~~~/~yannickilcher~~ Twitter: ~~/~ykilcher~~ Discord: https://ykilcher.com/discord LinkedIn: ~~/~ykilcher~~ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https://www.subscribestar.com/yannick... Patreon: ~~/~yannickilcher~~ Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n},
  keywords = {genAI},
  note = {Kilcher24claudeNotSentient
\par
Good at least for the screenshot: ``it's just statistics''
\par
Claude 3 performance: (Anthropic, 2024)}
}

@misc{Tyrangiel24aiRemakeUSgovt,
  title = {Let {{AI}} Remake the Whole {{U}}.{{S}}. Government (Oh, and Save the Country)},
  author = {Tyrangiel, Josh},
  year = {2024},
  month = mar,
  journal = {Washington Post},
  url = {https://www.washingtonpost.com/opinions/2024/03/06/artificial-intelligence-state-of-the-union/},
  urldate = {2024-03-06},
  abstract = {Thanks to AI, Operation Warp Speed was a rare triumph for our federal bureaucracy. Now, it can help us blaze a new path to the shining city on a hill.},
  chapter = {Opinion Columns},
  langid = {english},
  keywords = {obsLitNote},
  note = {Tyrangiel24aiRemakeUSgovt
\par
Mostly about gov't inefficiency, and failure to adopt software lifecycle, but a few AI use cases:
\par
\begin{itemize}

\item in 2023 the IRS answered only 29\% of the calls it received. ~From annual report by the Office of the National Taxpayer Advocate. (Rappeport, 2024)
\item Human-based eligibility decisions for the Supplemental Nutrition Assistance Program, have a 44 percent error rate. ~Says LLMs could do better but no cite for that.
\item Could have better service in: ~veterans benefits,~student loans,~unemployment,~social security~and~Medicare.
\item Chase has 1000's rules per business: cut redundant, contradictory by 60\%: (Tyrangiel, 2023)
\item ~\href{https://www.mentalhealth.va.gov/docs/data-sheets/2023/2023-National-Veteran-Suicide-Prevention-Annual-Report-FINAL-508.pdf}{6,392 veterans died by}~suicide in 2021,~the most recent year numbers are available. A~\href{https://www.propublica.org/article/how-veterans-affairs-fails-mental-health-patients}{ProPublica review}~of inspector general reports found VA employees regularly ``botched screenings meant to assess veterans' risk of suicide or violence; sometimes they didn't perform the screenings at all.''

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Tyrangiel24aiRemakeUSgovt.html}
}

@article{Rappenport24challengesIRS,
  title = {Effort to {{Revamp I}}.{{R}}.{{S}}. {{Faces Challenges Despite Funding Infusion}}},
  author = {Rappeport, Alan},
  year = {2024},
  month = jan,
  journal = {The New York Times},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2024/01/10/us/politics/irs-tax-report-congress.html},
  urldate = {2024-03-06},
  abstract = {The Office of the National Taxpayer Advocate lamented that the tax collection agency, which now faces budget cuts in Congress, is still struggling to answer the telephones.},
  chapter = {U.S.},
  langid = {american},
  note = {Rappenport24challengesIRS
\par
Source for claim in (Josh Tyrangiel, 2024):
\par
\begin{itemize}

\item in 2023 the IRS answered only 29\% of the calls it received. ~From annual report by the Office of the National Taxpayer Advocate.

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Rappenport24challengesIRS.html}
}

@article{Tyrangiel23smartBanksAI,
  title = {{{AI}} Is Making the World's Biggest Banks Much Smarter},
  author = {Tyrangiel, Josh},
  year = {2023},
  month = nov,
  journal = {Washington Post},
  issn = {0190-8286},
  url = {https://www.washingtonpost.com/opinions/2023/11/17/ai-big-banks-risks/},
  urldate = {2024-03-06},
  abstract = {Artificial intelligence could revolutionize banking by eliminating much of the risk.},
  langid = {american},
  keywords = {obsLitNote},
  note = {Tyrangiel23smartBanksAI
\par
\begin{itemize}

\item Source of (Josh Tyrangiel, 2024)'s claim that banks use AI to remove redundant, contradictory rules. ~JPMorganThousands of rules for one business. ~AI reduced them by 60\%
\item JPMorgan Chase invested hundreds of millions in AI, got \$1.5B in profits (2022, I think)
\item surprising claim that young people trust algorithm decisions to be unbiased more than people

\end{itemize}},
  file = {C:\Users\scott\Zotero\storage\4SNDTVIF\Tyrangiel23smartBanksAI.html}
}

@misc{Simon21llmNewMooresLaw,
  title = {Large {{Language Models}}: {{A New Moore}}'s {{Law}}?},
  shorttitle = {Large {{Language Models}}},
  author = {Simon, Julien},
  year = {2021},
  month = oct,
  url = {https://huggingface.co/blog/large-language-models},
  urldate = {2024-03-06},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  keywords = {obsLitNote},
  note = {Simon21llmNewMooresLaw
\par
A good source for the graph showing increase in params over time.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Simon21llmNewMooresLaw.pdf}
}

@misc{Alammar18illustratedTransformer,
  title = {The {{Illustrated Transformer}}},
  author = {Alammar, Jay},
  year = {2018},
  month = jun,
  url = {https://jalammar.github.io/illustrated-transformer/},
  urldate = {2024-03-06},
  abstract = {Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese Watch: MIT's Deep Learning State of the Art lecture referencing this post Featured in courses at Stanford, Harvard, MIT, Princeton, CMU and others In the previous post, we looked at Attention -- a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer -- a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud's recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let's try to break the model apart and look at how it functions. The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard's NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter. 2020 Update: I've created a ``Narrated Transformer'' video which is a gentler approach to the topic: A High-Level Look Let's begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.},
  keywords = {obsLitNote},
  note = {Alammar18illustratedTransformer
\par
Shows how outputs are generated, and how the model doesn't need retraining on each iteration, even though context kinda increases. ~Lots of figures ot borrow from. ~Watch before (Vaswani et al., 2017)?
\par
Also has a video: (``The Narrated Transformer Language Model - YouTube'', 2020). ~Maybe watch this 1st?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Alammar18illustratedTransformer.pdf}
}

@misc{Alammar20NarratedTransformerLLM,
  title = {The {{Narrated Transformer Language Model}}},
  author = {{Jay Alammar}},
  year = {2020},
  month = oct,
  address = {YouTube},
  url = {https://www.youtube.com/watch?v=-QH8fRhqFHM},
  urldate = {2024-03-06},
  abstract = {AI/ML has been witnessing a rapid acceleration in model improvement in the last few years. The majority of the state-of-the-art models in the field are based on the Transformer architecture. Examples include models like BERT (which when applied to Google Search, resulted in what Google calls "one of the biggest leaps forward in the history of Search") and OpenAI's GPT2 and GPT3 (which are able to generate coherent text and essays). This video by the author of the popular "Illustrated Transformer" guide will introduce the Transformer architecture and its various applications. This is a visual presentation accessible to people with various levels of ML experience.},
  keywords = {obsLitNote},
  note = {Alammar20NarratedTransformerLLM
\par
Video accompanying: (Alammar, 2018)}
}

@misc{Alammar23getAIwhatBigDeal,
  title = {What's the Big Deal with {{Generative AI}}? {{Is}} It the Future or the Present?},
  shorttitle = {What's the Big Deal with {{Generative AI}}?},
  author = {Alammar, Jay},
  year = {2023},
  month = feb,
  journal = {Context by Cohere},
  url = {https://txt.cohere.com/generative-ai-future-or-present/},
  urldate = {2024-03-06},
  abstract = {Part 1 of "Generative AI is.. Not Enough?" It is almost impossible to ignore the astounding progress in artificial intelligence these days. From the new generation of generative chatbots, to the models that can generate (almost) any picture (or very soon, video), the pace of development in the AI field},
  langid = {english},
  note = {Alammar23getAIwhatBigDeal
\par
\begin{itemize}

\item Use cases
\item 
\par
expectation vs. reality
\par
\begin{itemize}

\item (sketch graphic of that). ~
\item Also, funny old self driving predictions (we're way behind)

\end{itemize}

\item Part 2: (Alammar, 2023)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Alammar23getAIwhatBigDeal.pdf}
}

@misc{Reimers23neuralSearch,
  title = {What Is {{Neural Search}}? {{Nils Reimers}} - {{Sentence Transformers}} and {{Embedding Evaluation}} - {{YouTube}}},
  author = {Reimers, Nils},
  year = {2023},
  url = {https://www.youtube.com/watch?v=Z_4rohX4Ki8},
  urldate = {2024-03-06},
  abstract = {Nils is the creator of Sentence-BERT and has authored several well-known research papers, including Sentence-BERT and the popular Sentence Transformers library. He's also worked as a Research Scientist at HuggingFace, (co-)founded several web companies, and worked as an AI consultant in the area of investment banking, media, and IoT. === In our conversation, Nils gives us an introduction to the Sentence-BERT package and the large language models provided in it. He also shares some lessons from his experience in open-source development of such a popular package. Finally, Nils touches on his research collaborations on how to evaluate embeddings through works like MTEB: Massive Text Embedding Benchmark and BEIR. To go deeper into these tools, and other concepts around embeddings, watch the video and join the conversation on Discord. Stay tuned for more episodes in our Talking Language AI series!},
  keywords = {obsLitNote}
}

@misc{Alammar23aiEatingWorld,
  title = {{{AI}} Is {{Eating The World}}},
  author = {Alammar, Jay},
  year = {2023},
  month = mar,
  journal = {Context by Cohere},
  url = {https://txt.cohere.com/ai-is-eating-the-world/},
  urldate = {2024-03-06},
  abstract = {Part 2 of ``Generative AI is {\dots} Not Enough?'' Over a decade ago, the phrase ``software is eating the world'' described how software was rapidly becoming the center of many industries beyond the technology sector. The leading book retailers, video services providers, music companies, entertainment companies, and even movie production companies},
  langid = {english},
  note = {Alammar23aiEatingWorld
\par
Part 2 of: (Alammar, 2023)
\par
Video:(``AI is Eating The World - This is Where YOU Can Use it to Compete (AI Product Moats)'', 2023)}
}

@misc{Alammar23aiEatingWorldVideo,
  title = {{{AI}} Is {{Eating The World}} - {{This}} Is {{Where YOU Can Use}} It to {{Compete}} ({{AI Product Moats}})},
  author = {Alammar, Jay},
  year = {2023},
  month = may,
  url = {https://www.youtube.com/watch?v=oTqG2DbXl2Y},
  urldate = {2024-03-06},
  abstract = {Over a decade ago, the phrase ``software is eating the world'' described how software was rapidly becoming the center of many industries beyond the technology sector. The leading book retailers, video services providers, music companies, entertainment companies, and even movie production companies were essentially software companies. That trend is still going strong. In this video, Jay shares observations on the value in the AI technology stack and focuses on where some of the technical moats might be. The previous video in this series (~~~{$\bullet~$}What~is~Generative~AI?~4~Important~Th...~~) discussed 4 major points about useful perspectives on generative AI. Here we continue the series with points 5-8. Blog post: https://txt.cohere.com/ai-is-eating-t... --- Twitter: ~~/~jayalammar~~ Blog: https://jalammar.github.io/ Mailing List: https://jayalammar.substack.com/ -- 0:00 Introduction 3:44 5) Maps and Landscapes of AI Technology and Value Stacks 7:16 6) Enterprises: Plan Not for One, but Thousands of AI Touchpoints in Your Systems 8:46 7) Account for the Many Descendants and Iterations of a Foundation Model 16:01 8) Model Usage Datasets Allow Collective Exploration of a Model's Generative Space},
  note = {Alammar23aiEatingWorldVideo
\par
Goes with (Alammar, 2023)}
}

@misc{JayAlammar23WhatGenerativeAI,
  title = {What Is {{Generative AI}}? 4 {{Important Things}} to {{Know}} (about {{ChatGPT}}, {{MidJourney}}, {{Cohere}} \& Future {{AIs}})},
  shorttitle = {What Is {{Generative AI}}?},
  author = {Alammar, Jay},
  year = {2023},
  month = apr,
  url = {https://www.youtube.com/watch?v=AeW9r3lopp0},
  urldate = {2024-03-06},
  abstract = {What's the big deal with Generative AI? Is it the future or the present? In this video, Jay goes over four key reflections on how best to think of the current state of AI products and features, and avoid pitfalls people tend to make with new tech. Blog post: https://txt.cohere.ai/generative-ai-f... What is Neural Search? Nils Reimers - Sentence Transformers and Embedding Evaluation ~~~{$\bullet~$}What~is~Neural~Search?~Nils~Reimers~-...~~ --- Twitter: ~~/~jayalammar~~ Blog: https://jalammar.github.io/ Mailing List: https://jayalammar.substack.com/ -- 0:00 Introduction 1:04 1- Recent AI developments are awe-inspiring and promise to change the world. But when? 5:47 2- Make a distinction between impressive  cherry-picked demos, and reliable use cases that are ready for the marketplace 6:35 3- Think of models as components of intelligent systems, not minds 7:56 4- Generative AI alone is only the tip of the iceberg}
}

@article{Vaswani17AttentionAllYou,
  title = {Attention Is All You Need},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30},
  url = {https://proceedings.neurips.cc/paper/7181-attention-is-all},
  urldate = {2024-03-06},
  keywords = {obsLitNote},
  note = {Vaswani17AttentionAllYou
\par
The classic transformer paper recommended by Zach Shafran. ~(Alammar, 2018) might be a friendlier start.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Vaswani17AttentionAllYou.pdf}
}

@misc{Nakajima23GenAIResearch,
  title = {Gen {{AI}} for {{Research Market Map}} -- {{Yohei Nakajima}}},
  author = {Nakajima, Yohei},
  year = {2023},
  month = oct,
  url = {https://yoheinakajima.com/gen-ai-for-research-market-map/},
  urldate = {2024-03-06},
  abstract = {Research is a domain where AI can bring about a transformative increase in efficiency. Here, we delve into the expansive realm of ``General AI for Research,'' segmenting it into various markets: Investment Research Legal Research Academic/Scientific Research Real Estate Investment Research Consulting Research Healthcare Research Customer Research Wealth Advisory Research Let's dive into the tools and platforms that are paving the way in each of these sectors:},
  langid = {english},
  note = {Nakajima23GenAIResearch
\par
Many links to gen AI applications in some kind of ``research''},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nakajima23GenAIResearch.html}
}

@misc{Nakajima23taskDrivenAutonomAgentGPT4,
  title = {Task-Driven {{Autonomous Agent Utilizing GPT-4}}, {{Pinecone}}, and {{LangChain}} for {{Diverse Applications}}},
  author = {Nakajima, Yohei},
  year = {2023},
  month = mar,
  url = {https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/},
  urldate = {2024-03-06},
  abstract = {n this research, we propose a novel task-driven autonomous agent that leverages OpenAI's GPT-4 language model, Pinecone vector search, and the LangChain framework to perform a wide range of tasks across diverse domains. Our system is capable of completing tasks, generating new tasks based on completed results, and prioritizing tasks in real-time. We discuss potential future improvements, including the integration of a security/safety agent, expanding functionality, generating interim milestones, and incorporating real-time priority updates. The significance of this research lies in demonstrating the potential of AI-powered language models to autonomously perform tasks within various constraints and contexts.},
  langid = {english},
  note = {Nakajima23TaskdrivenAutonomAgentGPT4
\par
\begin{itemize}

\item GPT4's hooked together to make an agent (something not yet doable? ~(``[ML News] Groq, Gemma, Sora, Gemini, and Air Canada's chatbot troubles'', 2024))
\item Agent vaguely explained in (Rodriguez, 2024)
\item Also a bunch of AI risks and catastrophes this could be vulnerable to

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nakajima23taskDrivenAutonomAgentGPT4.pdf}
}

@misc{Rodriguez24nakajimaBabyAGIagentsInvest,
  type = {Substack Newsletter},
  title = {The {{Sequence Chat}}: {{Yohei Nakajima}} on {{Creating BabyAGI}}, {{Autonomous Agents}} and {{Investing}} in {{Generative AI}}},
  shorttitle = {The {{Sequence Chat}}},
  author = {Rodriguez, Jesus},
  year = {2024},
  month = mar,
  journal = {TheSequence},
  url = {https://thesequence.substack.com/p/the-sequence-chat-yohei-nakajima?publication_id=54309&utm_campaign=email-post-title&r=haqky&utm_medium=email},
  urldate = {2024-03-06},
  abstract = {The creator of one of the most popular open source generative AI projects shares his views about AI tech, investing and the future.},
  note = {Rodriguez24nakajimaBabyAGIagentsInvest
\par
Vaguely explains Agent in (Nakajima, 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Rodriguez24nakajimaBabyAGIagentsInvest.html}
}

@article{Genkina24aiPromptEngDead,
  title = {{{AI Prompt Engineering Is Dead}}},
  author = {{Dina Genkina}},
  year = {2024},
  month = mar,
  journal = {IEEE Spectrum},
  url = {https://spectrum.ieee.org/prompt-engineering-is-dead},
  urldate = {2024-03-07},
  abstract = {Long live AI prompt engineering},
  langid = {english},
  note = {Genkina24aiPromptEngDead
\par
\begin{itemize}

\item more use genAI cases
\item why prompt engineering seemed reasonable \& funny: (Battle and Gollapudi, 2024)
\item 
\par
autotuned prompts better than humans for both text and pictures
\par
\begin{itemize}

\item boy on horse pictures might be a good slide (didn't survive in pdf, or saved html; revisit website)
\item 
\par
links to many autoprompt papers
\par
\begin{itemize}

\item the tuner: (Rosenman et al., 2023)
\item PickaPick paper (Kirstain et al., 2023) (called ``PickScore,'' I think in this IEEE article

\end{itemize}

\end{itemize}

\item 
\par
But ``prompt'' engineering transform
\par
\begin{itemize}

\item testing genAI not like traditional software test. ~Although some say you can still use them (Hayes, 2024)
\item new job LLMOPS (MLOPS people best qualified?)
\item more about commercializing this random thing

\end{itemize}

\item Or is it not dead? ~(Brubaker, 2024)

\end{itemize}},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Genkina24aiPromptEngDead.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Genkina24aiPromptEngDead.html}
}

@misc{Roth24copilotDisturbImages,
  title = {Microsoft {{AI}} Engineer Warns {{FTC}} about {{Copilot Designer}} Safety Concerns},
  author = {Roth, Emma},
  year = {2024},
  month = mar,
  journal = {The Verge},
  url = {https://www.theverge.com/2024/3/6/24092191/microsoft-ai-engineer-copilot-designer-ftc-safety-concerns},
  urldate = {2024-03-07},
  abstract = {The engineer is urging Microsoft to apply more safety guardrails.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Roth24copilotDisturbImages.pdf}
}

@misc{Li24PersonalLLMAgents,
  title = {Personal {{LLM Agents}}: {{Insights}} and {{Survey}} about the {{Capability}}, {{Efficiency}} and {{Security}}},
  shorttitle = {Personal {{LLM Agents}}},
  author = {Li, Yuanchun and Wen, Hao and Wang, Weijun and Li, Xiangyu and Yuan, Yizhen and Liu, Guohong and Liu, Jiacheng and Xu, Wenxing and Wang, Xiang and Sun, Yi and Kong, Rui and Wang, Yile and Geng, Hanfei and Luan, Jian and Jin, Xuefeng and Ye, Zilong and Xiong, Guanjing and Zhang, Fan and Li, Xiang and Xu, Mengwei and Li, Zhijun and Li, Peng and Liu, Yang and Zhang, Ya-Qin and Liu, Yunxin},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2401.05459v1},
  urldate = {2024-03-07},
  abstract = {Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences. With the development of smartphones and IoT, computing and sensing devices have become ubiquitous, greatly expanding the boundaries of IPAs. However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability. Recently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs. With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously. In this paper, we focus on Personal LLM Agents, which are LLM-based agents that are deeply integrated with personal data and personal devices and used for personal assistance. We envision that Personal LLM Agents will become a major software paradigm for end-users in the upcoming era. To realize this vision, we take the first step to discuss several important questions about Personal LLM Agents, including their architecture, capability, efficiency and security. We start by summarizing the key components and design choices in the architecture of Personal LLM Agents, followed by an in-depth analysis of the opinions collected from domain experts. Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges.},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Li24PersonalLLMAgents.pdf}
}

@misc{IBM24autBoringCallCtrClass,
  title = {Automate the {{Boring Stuff With Large Language Models}}},
  author = {IBMSkillsNetwork},
  year = {2023},
  month = sep,
  url = {https://cognitiveclass.ai/courses/course-v1:IBMSkillsNetwork+GPXX0C2NEN+v1},
  urldate = {2024-03-07},
  abstract = {Learn how to use Large Language Models (LLMs) to get more done in less time. Increase efficiency by automating boring, redundant tasks! In this guided project, you will learn how to use LLMs to cover more ground with just a bit of Python code. As an industry use case example, we will take on the role of a manager at a call center who wants to review call transcripts and produce feedback for agents that have fallen below expectations. While this is one specific use case, the same idea may be extended to countless applications!},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\IBM24autBoringCallCtrClass.pdf}
}

@misc{Parnin23buildingOwnCopilot,
  title = {Building {{Your Own Product Copilot}}: {{Challenges}}, {{Opportunities}}, and {{Needs}}},
  shorttitle = {Building {{Your Own Product Copilot}}},
  author = {Parnin, Chris and Soares, Gustavo and Pandita, Rahul and Gulwani, Sumit and Rich, Jessica and Henley, Austin Z.},
  year = {2023},
  month = dec,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2312.14231v1},
  urldate = {2024-03-07},
  abstract = {A race is underway to embed advanced AI capabilities into products. These product copilots enable users to ask questions in natural language and receive relevant responses that are specific to the user's context. In fact, virtually every large technology company is looking to add these capabilities to their software products. However, for most software engineers, this is often their first encounter with integrating AI-powered technology. Furthermore, software engineering processes and tools have not caught up with the challenges and scale involved with building AI-powered applications. In this work, we present the findings of an interview study with 26 professional software engineers responsible for building product copilots at various companies. From our interviews, we found pain points at every step of the engineering process and the challenges that strained existing development practices. We then conducted group brainstorming sessions to collaborative on opportunities and tool designs for the broader software engineering community.},
  langid = {english},
  note = {Parnin23buildingOwnCopilot
\par
Interviews w/ people trying to build copilots.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Parnin23BuildingYourOwn.pdf}
}

@misc{Yang23llmOptimizers,
  title = {Large {{Language Models}} as {{Optimizers}}},
  author = {Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V. and Zhou, Denny and Chen, Xinyun},
  year = {2023},
  month = sep,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2309.03409v2},
  urldate = {2024-03-07},
  abstract = {Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8\% on GSM8K, and by up to 50\% on Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.},
  langid = {english},
  note = {Yang23llmOptimizers
\par
Actually optimizes stuff like linear regression and travelling salesman by prompt tuning (I think)
\par
Kinda related to car cfm optimizer? (Vidyasagar Ananthan et al., 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Yang23LargeLanguageModels.pdf}
}

@misc{Khattab23DSPyLLMselfImprove,
  title = {{{DSPy}}: {{Compiling Declarative Language Model Calls}} into {{Self-Improving Pipelines}}},
  shorttitle = {{{DSPy}}},
  author = {Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03714},
  eprint = {2310.03714},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.03714},
  url = {http://arxiv.org/abs/2310.03714},
  urldate = {2024-03-07},
  abstract = {The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded "prompt templates", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot prompting (generally by over 25\% and 65\%, respectively) and pipelines with expert-created demonstrations (by up to 5-46\% and 16-40\%, respectively). On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at https://github.com/stanfordnlp/dspy},
  archiveprefix = {arXiv},
  note = {Khattab23DSPyLLMselfImprove
\par
A self-tuning LLM prompt system. ~Very long paper, though.
\par
A test: (Vivek, 2024) of a link reference. ~How does it export to Obsidian? ~Is it clickable, and if so, where does it send you?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Khattab23DSPyLLMselfImprove.pdf}
}

@misc{Spirin23masterLLMOps,
  title = {Mastering {{LLM Techniques}}: {{LLMOps}}},
  shorttitle = {Mastering {{LLM Techniques}}},
  author = {Spirin, Nik and Balint, Michael},
  year = {2023},
  month = nov,
  journal = {NVIDIA Technical Blog},
  url = {https://developer.nvidia.com/blog/mastering-llm-techniques-llmops/},
  urldate = {2024-03-07},
  abstract = {Businesses rely more than ever on data and AI to innovate, offer value to customers, and stay competitive. The adoption of machine learning (ML), created a need for tools, processes{\dots}},
  langid = {american},
  note = {Spirin23masterLLMOps
\par
The thing after prompt engineering. ~Also another Venn diagram situating ML and getAI, etc.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Spirin23masterLLMOps.html}
}

@misc{Kirstain23PickaPicOpenDataset,
  title = {Pick-a-{{Pic}}: {{An Open Dataset}} of {{User Preferences}} for {{Text-to-Image Generation}}},
  shorttitle = {Pick-a-{{Pic}}},
  author = {Kirstain, Yuval and Polyak, Adam and Singer, Uriel and Matiana, Shahbuland and Penna, Joe and Levy, Omer},
  year = {2023},
  month = nov,
  number = {arXiv:2305.01569},
  eprint = {2305.01569},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.01569},
  url = {http://arxiv.org/abs/2305.01569},
  urldate = {2024-03-07},
  abstract = {The ability to collect a large dataset of human preferences from text-to-image users is usually limited to companies, making such datasets inaccessible to the public. To address this issue, we create a web app that enables text-to-image users to generate images and specify their preferences. Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image prompts and real users' preferences over generated images. We leverage this dataset to train a CLIP-based scoring function, PickScore, which exhibits superhuman performance on the task of predicting human preferences. Then, we test PickScore's ability to perform model evaluation and observe that it correlates better with human rankings than other automatic evaluation metrics. Therefore, we recommend using PickScore for evaluating future text-to-image generation models, and using Pick-a-Pic prompts as a more relevant dataset than MS-COCO. Finally, we demonstrate how PickScore can enhance existing text-to-image models via ranking.},
  archiveprefix = {arXiv},
  note = {Kirstain23PickaPicOpenDataset
\par
Is said to be the image quality metric used to autotune image promps (Dina Genkina, 2024)
\par
Car CFD metric instead: (Vidyasagar Ananthan et al., 2023)
\par
I think this was used to tune image prompts here: (Rosenman et al., 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kirstain23PickaPicOpenDataset.pdf}
}

@misc{Rosenman23NeuroPromptTuneTextImage,
  title = {{{NeuroPrompts}}: {{An Adaptive Framework}} to {{Optimize Prompts}} for {{Text-to-Image Generation}}},
  shorttitle = {{{NeuroPrompts}}},
  author = {Rosenman, Shachar and Lal, Vasudev and Howard, Phillip},
  year = {2023},
  month = nov,
  number = {arXiv:2311.12229},
  eprint = {2311.12229},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.12229},
  url = {http://arxiv.org/abs/2311.12229},
  urldate = {2024-03-07},
  abstract = {Despite impressive recent advances in text-to-image diffusion models, obtaining high-quality images often requires prompt engineering by humans who have developed expertise in using them. In this work, we present NeuroPrompts, an adaptive framework that automatically enhances a user's prompt to improve the quality of generations produced by text-to-image models. Our framework utilizes constrained text decoding with a pre-trained language model that has been adapted to generate prompts similar to those produced by human prompt engineers. This approach enables higher-quality text-to-image generations and provides user control over stylistic features via constraint set specification. We demonstrate the utility of our framework by creating an interactive application for prompt enhancement and image generation using Stable Diffusion. Additionally, we conduct experiments utilizing a large dataset of human-engineered prompts for text-to-image generation and show that our approach automatically produces enhanced prompts that result in superior image quality. We make our code, a screencast video demo and a live demo instance of NeuroPrompts publicly available.},
  archiveprefix = {arXiv},
  note = {Rosenman23NeuroPromptTuneTextImage
\par
\begin{itemize}

\item the image prompt tuner mentioned in (Dina Genkina, 2024)
\item from above, I think they used this (Kirstain et al., 2023) as the tuning metric

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Rosenman23NeuroPromptTuneTextImage.pdf}
}

@misc{Winn24genAIsoftwareTst,
  title = {Using Generative {{AI}} to Improve Software Testing},
  year = {2024},
  month = mar,
  journal = {MIT News {\textbar} Massachusetts Institute of Technology},
  url = {https://news.mit.edu/2024/using-generative-ai-improve-software-testing-datacebo-0305},
  urldate = {2024-03-07},
  abstract = {MIT spinout DataCebo helps companies bolster their datasets by creating synthetic data that mimic the real thing.},
  langid = {english},
  note = {Winn24genAIsoftwareTst
\par
Seems mainly about synthetic test data (read to be sure). ~
\par
This might be useful for e.g. VDER. ~They use synthetic weather, I think they're saying.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Winn24genAIsoftwareTst.pdf}
}

@misc{Battle24eccentricPrompts,
  title = {The {{Unreasonable Effectiveness}} of {{Eccentric Automatic Prompts}}},
  author = {Battle, Rick and Gollapudi, Teja},
  year = {2024},
  month = feb,
  number = {arXiv:2402.10949},
  eprint = {2402.10949},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.10949},
  url = {http://arxiv.org/abs/2402.10949},
  urldate = {2024-03-07},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable problem-solving and basic mathematics abilities. However, their efficacy is highly contingent on the formulation of the prompt. This study endeavors to quantify the influence of incorporating "positive thinking" into the system message of the prompt, then compare that to systematic prompt optimization. We assess the performance of 60 combinations of system message snippets, tested with and without Chain of Thought prompting, across three models with parameters ranging from 7 to 70 billion on the GSM8K dataset. Our findings reveal that results do not universally generalize across models. In most instances, the inclusion of "positive thinking" prompts positively affected model performance. Notably, however, Llama2-70B exhibited an exception when not utilizing Chain of Thought, as the optimal system message was found to be none at all. Given the combinatorial complexity, and thus computation time, of experimenting with hand-tuning prompts for large black-box models, we then compared the performance of the best "positive thinking" prompt against the output of systematic prompt optimization. We show that employing an automated prompt optimizer emerges as the most effective method for enhancing performance, even when working with smaller open-source models. Additionally, our findings reveal that the highest-scoring, automatically-optimized prompt exhibits a degree of peculiarity far beyond expectations.},
  archiveprefix = {arXiv},
  note = {Battle24eccentricPrompts
\par
The paper that suggested that crazy prompts work -- funny, so shows why prompt engineering looked reasonable -- but which may now be superseded by prompt autotuning (Dina Genkina, 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Battle24eccentricPrompts.pdf}
}

@misc{Anthropic24introNextGenClaude3,
  title = {Introducing the next Generation of {{Claude}}},
  author = {{Anthropic}},
  year = {2024},
  month = mar,
  url = {https://www.anthropic.com/news/claude-3-family},
  urldate = {2024-03-07},
  abstract = {Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus.},
  langid = {english},
  note = {Anthropic24introNextGenClaude3
\par
Comparison of sentient (?) Claude 3 with other models. ~Good for talk into, in addition to earliest such test comparisons, which I haven't found yet.(``No, Anthropic's Claude 3 is NOT sentient'', 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Anthropic24introNextGenClaude3.pdf}
}

@article{Bethea24aiPhoneScamLovedOne,
  title = {The {{Terrifying A}}.{{I}}. {{Scam That Uses Your Loved One}}'s {{Voice}}},
  author = {Bethea, Charles},
  year = {2024},
  month = mar,
  journal = {The New Yorker},
  issn = {0028-792X},
  url = {https://www.newyorker.com/science/annals-of-artificial-intelligence/the-terrifying-ai-scam-that-uses-your-loved-ones-voice},
  urldate = {2024-03-07},
  abstract = {A Brooklyn couple got a call from relatives who were being held ransom. Their voices---like many others these days---had been cloned.},
  chapter = {annals of artificial intelligence},
  langid = {american},
  note = {Bethea24aiPhoneScamLovedOne
\par
Cloned voice claims to be a ransomed lovedd one.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bethea24aiPhoneScamLovedOne.pdf}
}

@misc{Halper24americaRunOutOfPower,
  title = {Amid Explosive Demand, {{America}} Is Running out of Power},
  author = {Halper, Evan},
  year = {2024},
  month = mar,
  journal = {Washington Post},
  url = {https://www.washingtonpost.com/business/2024/03/07/ai-data-centers-power/},
  urldate = {2024-03-07},
  abstract = {Artificial intelligence, data centers and the boom in clean-tech manufacturing are pushing America's aging power grid to the brink. Utilities can't keep up.},
  chapter = {Business},
  langid = {english},
  note = {Halper24americaRunOutOfPower
\par
Utility forecasts for new US electricity demand have now doubled by 2030, and utilities point to AI that require ``exponentially more power than traditional data centers,'' as one major cause, threatening all the electrification needed for the Clean Power Transition, delaying coal plant shutdown, and encouraging small nuclear.
\par
Data center usage:
\par
\begin{itemize}

\item 2020: 4\%
\item 2026: 6\%

\end{itemize}

\par
Helpful AI use cases
\par
\begin{itemize}

\item speeding up plant approval pricess (Microsoft)
\item helping grid operate efficiently
\item maybe microgrids, since these are getting forced by increased demand?

\end{itemize}

\par
\textbf{Images missed in html and pdf capture:}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Halper24americaRunOutOfPower.html}
}

@article{Weber24OpenSourceEnergyArbitrage,
  title = {An {{Open-Source Energy Arbitrage Model Involving Price Bands}} for {{Risk Hedging}} with {{Imperfect Price Signals}}},
  author = {Weber, Timothy and Lu, Bin},
  year = {2024},
  month = jan,
  journal = {Energies},
  volume = {17},
  number = {1},
  pages = {13},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1996-1073},
  doi = {10.3390/en17010013},
  url = {https://www.mdpi.com/1996-1073/17/1/13},
  urldate = {2024-03-08},
  abstract = {The increased uptake of variable renewable energy sources has increased electricity price volatility in many energy pool markets, providing an opportunity for storage systems to profit through energy arbitrage. Comparison between the cost or value of storage systems engaging in energy arbitrage should be performed on a levelised basis due to differences in system lifetime. Existing energy arbitrage models with bid/offer curves and imperfect forecasting are typically computationally expensive and are impractical for calculating lifetime levelised cost metrics. In this work, an open-source modular energy arbitrage model with bid and offer curve inputs was developed for a lithium-ion battery energy storage system (BESS) and pumped hydro system (PHS) to analyse lifetime levelised cost and revenue. The mixed integer linear program scheduling module included a new piece-wise linearised description of PHS charging behaviour for rapid optimisation. A one-at-a-time sensitivity analysis indicated that levelised cost and revenue were highly sensitive to discharging efficiency. In a case study based on Australia's National Electricity Market, imperfect forecasting with no risk hedging was found to increase levelised costs by up to 24\% and decrease levelised revenue by up to 50\% relative to perfect price forecasting, despite 95\% of prices being forecast to be within \$35/MWh of the actual trading price. BESS levelised costs were more significantly correlated with consistent low risk bids (Kendall Tau-b of 0.75), since the undiscounted capital costs contribute to a larger proportion of the overall costs than in the PHS systems.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  note = {Weber24OpenSourceEnergyArbitrage
\par
Explains optimal bidding mainly for storage, I think.
\par
\href{https://github.com/TimWeberRE100/Energy_Arbitrage_Model}{Code is on Github}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Weber24OpenSourceEnergyArbitrage.pdf}
}

@misc{Connor21nemBidStacks5minSettle,
  title = {The {{NEM}}, Bid Stacks and Five-Minute Settlement},
  author = {Connor, Madeleine},
  year = {2021},
  month = oct,
  journal = {Flow Power},
  url = {https://flowpower.com.au/the-nem-bid-stacks-and-five-minute-settlement/},
  urldate = {2024-03-09},
  abstract = {This article explains wholesale electricity prices, generator bids, the bid stack and the significant change to the calculation of the NEM spot price.},
  langid = {australian},
  note = {Connor21nemBidStacks5minSettle
\par
The 2021 move to 5 minute trading intervals probably made prices less temporally independent within 30 minute intervals, and removed a lot of very short oscillating price spikes.
\par
\textbf{Before Oct. 1 2021}
\par
The dispatch was @ 5 mins and trading interval was @ 30 min (averge of 6 dispatch intervals). ~The averaging made it possible to know the minimum trading price until the interval's end; if one dispatch period was very high, then even assuming the the rest of the interval was at the market minimu allowed, the trading price would still be gauranteed high, so tons of traders jumped in and the dispatch price shot down. ~The result was an often rapidly oscillating price, sometimes from near the market cap, to negative. ~Still for physical incidents (floods and fires) dispatch prices were more stable (remain high) at high prices.
\par
\textbf{Expected result after Oct 1, 2021 (this article from Oct. 10, 2021)}
\par
\begin{itemize}

\item Dispatch price and market price cover the same interval (les temporal dependence)
\item it's not so easy to predict the min next (5 minute) trading interval price,
\item so less ``aggressive'' bidding within half hour intervals
\item but pricing should be more ``efficient.''
\item batteries and fast acting demand response are more rewarded, especially for short intervals.

\end{itemize}},
  file = {C:\Users\scott\Zotero\storage\AS2ZMLFM\Connor21nemBidStacks5minSettle.html}
}

@article{Stathakis21elecMktPriceSpikeFrcst,
  title = {Forecasting {{Price Spikes}} in {{Electricity Markets}}},
  author = {Stathakis, Efthymios and Papadimitriou, Theophilos and Gogas, Periklis},
  year = {2021},
  month = mar,
  journal = {Review of Economic Analysis},
  volume = {13},
  number = {1},
  pages = {65--87},
  issn = {1973-3909},
  doi = {10.15353/rea.v13i1.1822},
  url = {https://openjournals.uwaterloo.ca/index.php/rofea/article/view/1822},
  urldate = {2024-03-09},
  abstract = {Electricity markets are considered to be the most volatile amongst commodity markets. The non-storability of electricity and the need for instantaneous balancing of demand and supply can often cause extreme short-lived fluctuations in electricity prices. These fluctuations are termed price spikes. In this paper, we employ a multiclass Support Vector Machine (SVM) model to forecast the occurrence of price spikes in the German intraday electricity market. As price spikes, we define the prices that lie above the 95th quantile estimated by fitting a Generalized Pareto distribution in the innovation distribution of an AR-EGARCH model. The generalization ability of the model is tested in an out-of-the-sample dataset consisting of 4080 hours. Furthermore, we compare the performance of our best SVM model against Neural Networks (NNs) and Gradient Boosted Machines (GBMs).},
  copyright = {Copyright (c) 2021 Efthymios Stathakis, Theophilos Papadimitriou, Periklis Gogas},
  langid = {english},
  note = {Stathakis21elecMktPriceSpikeFrcst
\par
\begin{itemize}

\item 
\par
``spike'': price above 95\% quantile
\par
\begin{itemize}

\item (Vu et al., 2021) spike def better?

\end{itemize}

\item AR-EGARCH w/ Gen Pareto

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Stathakis21elecMktPriceSpikeFrcst.pdf}
}

@article{Vu21detPriceSpikeFrcstAEMO,
  title = {A {{Multi-Feature Based Approach Incorporating Variable Thresholds}} for {{Detecting Price Spikes}} in the {{National Electricity Market}} of {{Australia}}},
  author = {Vu, Dao H. and Muttaqi, Kashem M. and Agalgaonkar, Ashish P. and Bouzerdoum, Abdesselam},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {13960--13969},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3051313},
  url = {https://ieeexplore.ieee.org/abstract/document/9321354},
  urldate = {2024-03-09},
  abstract = {Detecting electricity price spikes is crucial as it helps market participants to gain confidence and formulate appropriate strategy to maximize their benefits. In this paper, a multi-feature based approach with the incorporation of variable thresholds is developed to detect electricity price spikes in the national electricity market of Australia. The variable thresholds, which are determined using a weighted sliding window average and an adjusted standard deviation, help to segregate spikes from normal price variations. Also, significant features are extracted from the market after thoroughly analyzing the underlying causes resulting into the price spikes. These features are employed as inputs to a support vector machine to classify electricity prices as spikes or non-spikes. A case study is conducted using a dataset acquired from the state of New South Wales, Australia. The results show that the proposed method can successfully detect the price spikes with high accuracy and confidence.},
  note = {Vu21detPriceSpikeFrcstAEMO
\par
a time varying AEMO price spike definition is ~the spike forecast target. ~Seems like a more useful forecast definition than static e.g (Stathakis et al., 2021; Datta and Datta, 2016)
\par
\begin{enumerate}

\item clip prices to (\$0, \$300)/MWh [99.95 of prices unclipped]
\item mean/std of clipped prices
\item More stuff, could be interesting

\end{enumerate}

\par
Is there a place covariate-based mean/var norm, as in (Sebasti{\'a}n et al., 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Vu21detPriceSpikeFrcstAEMO.pdf}
}

@article{Bajai22elecMktLiquidSpikeHU,
  title = {Electricity {{Market Liquidity}} and {{Price Spikes}}: {{Evidence}} from {{Hungary}}},
  shorttitle = {Electricity {{Market Liquidity}} and {{Price Spikes}}},
  author = {Bajai, M{\'a}ty{\'a}s and V{\'i}g, Attila A. and Hortay, Oliv{\'e}r},
  year = {2022},
  month = jan,
  journal = {Periodica Polytechnica Social and Management Sciences},
  volume = {30},
  number = {1},
  pages = {49--56},
  issn = {1587-3803},
  doi = {10.3311/PPso.16857},
  url = {https://www.pp.bme.hu/so/article/view/16857},
  urldate = {2024-03-09},
  abstract = {This article examines how electricity market liquidity, renewable production and cross-border activity together in combination explain price spikes in the Hungarian Power Exchange day-ahead auctions. In the applied logit model, the dependent variable representing the price spike is binary, and the key explanatory variable is a modified bid-ask spread depicting liquidity. Weather-dependent renewable production and the difference between exports and imports appear as control variables in the model. The empirical analysis was based on data from 2017 and 2018. The results show that the control variables have no effect on the bid-ask spread and that the model explains 96 per cent of the spikes well, with an AUC-ROC of 0.75 and a Gini coefficient of 0.5. Based on the results, it may be worthwhile for traders to incorporate their data from sales and purchase curves into their forecasts, as this will improve their chances of successfully predicting extreme prices.},
  copyright = {Copyright (c) 2021 Periodica Polytechnica Social and Management Sciences},
  langid = {english},
  note = {Bajai22elecMktLiquidSpikeHU
\par
bid-ask spread in logit explains 96\% of HU price spikes},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bajai22elecMktLiquidSpikeHU.pdf}
}

@article{ManfreJaimes23elecPriceSpikeHybrid,
  title = {A {{Hybrid Model}} for {{Multi-Day-Ahead Electricity Price Forecasting}} Considering {{Price Spikes}}},
  author = {Manfre Jaimes, Daniel and Zamudio L{\'o}pez, Manuel and Zareipour, Hamidreza and Quashie, Mike},
  year = {2023},
  month = sep,
  journal = {Forecasting},
  volume = {5},
  number = {3},
  pages = {499--521},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2571-9394},
  doi = {10.3390/forecast5030028},
  url = {https://www.mdpi.com/2571-9394/5/3/28},
  urldate = {2024-03-09},
  abstract = {This paper proposes a new hybrid model to forecast electricity market prices up to four days ahead. The components of the proposed model are combined in two dimensions. First, on the ``vertical'' dimension, long short-term memory (LSTM) neural networks and extreme gradient boosting (XGBoost) models are stacked up to produce supplementary price forecasts. The final forecasts are then picked depending on how the predictions compare to a price spike threshold. On the ``horizontal'' dimension, five models are designed to extend the forecasting horizon to four days. This is an important requirement to make forecasts useful for market participants who trade energy and ancillary services multiple days ahead. The horizontally cascaded models take advantage of the availability of specific public data for each forecasting horizon. To enhance the forecasting capability of the model in dealing with price spikes, we deploy a previously unexplored input in the proposed methodology. That is, to use the recent variations in the output power of thermal units as an indicator of unplanned outages or shift in the supply stack. The proposed method is tested using data from Alberta's electricity market, which is known for its volatility and price spikes. An economic application of the developed forecasting model is also carried out to demonstrate how several market players in the Alberta electricity market can benefit from the proposed multi-day ahead price forecasting model. The numerical results demonstrate that the proposed methodology is effective in enhancing forecasting accuracy and price spike detection.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  note = {ManfreJaimes23elecPriceSpikeHybrid
\par
DAM price forecast switch model, thermal gen. feat.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\ManfreJaimes23HybridModelMultiDayAhead.pdf}
}

@misc{Gamakumara23condNormTS,
  title = {Conditional Normalization in Time Series Analysis},
  author = {Gamakumara, Puwasala and {Santos-Fernandez}, Edgar and Talagala, Priyanga Dilini and Hyndman, Rob J. and Mengersen, Kerrie and Leigh, Catherine},
  year = {2023},
  month = may,
  number = {arXiv:2305.12651},
  eprint = {2305.12651},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.12651},
  url = {http://arxiv.org/abs/2305.12651},
  urldate = {2024-03-09},
  abstract = {Time series often reflect variation associated with other related variables. Controlling for the effect of these variables is useful when modeling or analysing the time series. We introduce a novel approach to normalize time series data conditional on a set of covariates. We do this by modeling the conditional mean and the conditional variance of the time series with generalized additive models using a set of covariates. The conditional mean and variance are then used to normalize the time series. We illustrate the use of conditionally normalized series using two applications involving river network data. First, we show how these normalized time series can be used to impute missing values in the data. Second, we show how the normalized series can be used to estimate the conditional autocorrelation function and conditional cross-correlation functions via additive models. Finally we use the conditional cross-correlations to estimate the time it takes water to flow between two locations in a river network.},
  archiveprefix = {arXiv},
  note = {Gamakumara23condNormTS
\par
``terminology'' in this paper used in (Sebasti{\'a}n et al., 2023) for mean/var normaliztion. Besides the mean/var estimates, this paper uses them to impute missing values; compute conditional auto \& cross correlation functions; and river flow lag times.
\par
Mean/var estimations
\par
\begin{enumerate}

\item a deterministic GAMs model estimates the expected value (mean) of a variable, y, given some covariates 
\item Compute a variance prediction target by logging the logging the squared difference between y and its esimated mean above
\item Estimate this with a gamma parameterization on the same covariates
\item Get the estimated variance from the estimated gamma function output 

\end{enumerate}

\par
What I don't understand is 
\par
\begin{itemize}

\item why not just use the actual mean over the window? ~
\item Is this because don't want the smearing due to the window?
\item are thoise covarites some forecastable parameter? 
\item For missing data, this all makes sense (example of missing stream water temp, but have air temp some other way, I guess). ~I didn't bother figuring out the other example uses of this technique.

\end{itemize}

\par
Comment: 36 pages, 26 Figures, Journal Article},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gamakumara23condNormTS.pdf}
}

@misc{Hayes24evalLLMapps,
  title = {Evaluating {{LLM Applications}}},
  author = {Hayes, Peter},
  year = {2024},
  month = feb,
  journal = {Humanloop: Collaboration and evaluation for LLM applications},
  url = {https://humanloop.com/blog/evaluating-llm-apps?utm_source=newsletter&utm_medium=sequence&utm_campaign=},
  urldate = {2024-03-11},
  abstract = {An overview of evaluating LLM applications. The emerging evaluation framework, parallels to traditional software testing and some guidance on best practices.},
  langid = {english},
  note = {Hayes24evalLLMapps
\par
How to test LLMs. ~Mentions traditional software tests, while (Dina Genkina, 2024) says LLM can't be tested the same way. ~Mabye this paper will explain how to test them.
\par
There's a \href{https://thesequence.substack.com/p/guest-post-evaluating-llm-applications?publication_id=54309&post_id=142484844&isFreemail=false&r=haqky&triedRedirect=true}{shorter version} of this article on substack.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hayes24evalLLMapps2.pdf}
}

@article{Metz24chatGPTphysWorld,
  title = {How the {{A}}.{{I}}. {{That Drives ChatGPT Will Move Into}} the {{Physical World}}},
  author = {Metz, Cade and Gardi, Balazs},
  year = {2024},
  month = mar,
  journal = {The New York Times},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2024/03/11/technology/ai-robots-technology.html},
  urldate = {2024-03-11},
  abstract = {Covariant, a robotics start-up, is designing technology that lets robots learn skills much like chatbots do.},
  chapter = {Technology},
  langid = {american},
  note = {Metz24chatGPTphysWorld
\par
I haven't read this but{\dots}
\par
Aparently, text and images from chatGPT like models (stable diffusion) give robots knowledge about phys world. ~Can pick up a banana even though its never seen one.
\par
This republished version has different details, some of which I wanted for my gen AI talk (Metz, 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Metz24chatGPTphysWorld2.pdf}
}

@misc{Dean23genAIlangVisionGoogleBlog,
  title = {Google {{Research}}, 2022 \& beyond: {{Language}}, Vision and Generative Models},
  shorttitle = {Google {{Research}}, 2022 \& Beyond},
  author = {{Jeff Dean}},
  year = {2023},
  month = jan,
  journal = {Google Research Blog},
  url = {https://blog.research.google/2023/01/google-research-2022-beyond-language.html},
  urldate = {2024-03-11},
  abstract = {I've always been interested in computers because of their ability to help people better understand the world around them. Over the last decade, much of the research done at Google has been in pursuit of a similar vision --- to help people better understand the world around them and get things done. We want to build more capable machines that partner with people to accomplish a huge variety of tasks. All kinds of tasks. Complex, information-seeking tasks. Creative tasks, like creating music, drawing new pictures, or creating videos. Analysis and synthesis tasks, like crafting new documents or emails from a few sentences of guidance, or partnering with people to jointly write software together. We want to solve complex mathematical or scientific problems. Transform modalities, or translate the world's information into any language. Diagnose complex diseases, or understand the physical world. Accomplish complex, multi-step actions in both the virtual software world and the physical world of robotics. We've demonstrated early versions of some of these capabilities in research artifacts, and we've partnered with many teams across Google to ship some of these capabilities in Google products that touch the lives of billions of users. But the most exciting aspects of this journey still lie ahead! With this post, I am kicking off a series in which researchers across Google will highlight some exciting progress we've made in 2022 and present our vision for 2023 and beyond. I will begin with a discussion of language, computer vision, multi-modal models, and generative machine learning models. Over the next several weeks, we will discuss novel developments in research topics ranging from responsible AI to algorithms and computer systems to science, health and robotics. Let's get started!},
  langid = {english},
  note = {Dean23genAIlangVisionGoogleBlog
\par
Good intro to genAI language, audio and visual models. ~Covers Google research on gen AI. ~language models, GANs, and how they work in image processing. ~
\par
These algorithms seem to have gone into the ~Pixel 8 phone: (Dina Berrada, 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Dean23genAIlangVisionGoogleBlog3.pdf}
}

@misc{Berrada23googlePhotos4newFeatsPix8,
  title = {4 New {{Google Photos}} Features on {{Pixel}} 8 and {{Pixel}} 8 {{Pro}}},
  author = {{Dina Berrada}},
  year = {2023},
  month = oct,
  journal = {The Keyword (Google Newsletter)},
  url = {https://blog.google/products/photos/google-photos-features-pixel-8-pro/},
  urldate = {2024-03-11},
  abstract = {New AI-powered editing tools in Google Photos are coming to Pixel 8 and Pixel 8 Pro, making complex video and photo edits simple and intuitive.},
  langid = {american},
  note = {Berrada23googlePhotos4newFeatsPix8
\par
Google Pixel 8 has 4 new gen AI features in
\par
\begin{itemize}

\item Magic Editor
\item Audio Magic Eraser
\item Zoom Enhance

\end{itemize}

\par
Probably not genAI, it seems to me
\par
\begin{itemize}

\item Best Take ? ~Hard to say

\end{itemize}

\par
Google discusses its Gen AI (Jeff Dean, 2023) . ~I think Dean article was linked to by an article this Berrada article linked to. ~Somehow, I found it by following links, anyway.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Berrada23googlePhotos4newFeatsPix8.pdf}
}

@article{Zimmer24learn2BaliveAI,
  title = {A.{{I}}. {{Is Learning What It Means}} to {{Be Alive}}},
  author = {Zimmer, Carl},
  year = {2024},
  month = mar,
  journal = {The New York Times},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2024/03/10/science/ai-learning-biology.html},
  urldate = {2024-03-12},
  abstract = {Given troves of data about genes and cells, A.I. models have made some surprising discoveries. What could they teach us someday?},
  chapter = {Science},
  langid = {american},
  keywords = {obsLitNote},
  note = {Zimmer24learn2BaliveAI
\par
FINISH READING
\par
biodmed research: heart arrythmia (?) treatment, developmental bio, cellID (and more). ~Also examples of some important genAI concepts.
\par
\begin{itemize}

\item transfer learning:
\item ``universal cell embedding''
\item Universal Cell Embedding is a "foundation model,"or is it an ``embedding''? ~I think?
\item after Universal Cell Embedding, could ID cells from an animal it had never seen before (trained on?) it was able to ID its cells, given the animal's "genetic provifile"

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zimmer24learn2BaliveAI.pdf}
}

@article{Woody24aiArchitClimDreamHm,
  title = {This {{AI Architect Will Design Your Climate-Friendly Dream Home}}},
  author = {Woody, Todd},
  year = {2024},
  month = mar,
  journal = {Bloomberg.com},
  url = {https://www.bloomberg.com/news/features/2024-03-12/this-ai-architect-will-design-your-climate-friendly-dream-home},
  urldate = {2024-03-13},
  abstract = {A suite of new technologies from Austin-based startup Icon also includes low-carbon concrete and a robot that can 3D print two-story homes.},
  langid = {english},
  keywords = {Architecture,Artificial Intelligence,Austin,California,Construction,Design,green,Phoenix,Sam Altman,Startups,Tom Allison},
  note = {Woody24aiArchitClimDreamHm
\par
chatbot converses with homeowner, asking questions and showing several renders of physics and building-code-aware home designs. ~Eventually contruction docs, permia apps, budges, bills of material, building schedule
\par
\begin{itemize}

\item ~``no reason to build ugly-ass spec homes anymore,'' no reason to build spec homes anymore
\item 18 mos. to develop AI system
\item zero hallucinations meant cookie cutter houses
\item can't trust this yet to estimate construction costs
\item can try for free
\item 
\par
benefits
\par
\begin{itemize}

\item full automation could shave \$100K from building cost (including the Phoenix printed concrete robot?)
\item use less materials and building time
\item architects relieved of onerous construction document duties
\item architects get 1\% payment for a building using this company's AI

\end{itemize}

\item 
\par
dialog example:
\par
\begin{itemize}

\item ``Sounds like you're envisioning a contemporary one-story home with an open feel,''
\item ``Before we proceed with the design, can you tell me more about your lifestyle and how you envision using the space in your home?''
\item ``Considering your location in Bolinas and desire to integrate with nature, would you like the design to include specific outdoor features such as a deck patio or garden areas?'' 

\end{itemize}

\item Sam Altman said company should be starting AI design ``right now,'' maybe a couple of years ago. ~So they did
\item Using standard concrete, their printed concrete would have ``substantially higher'' GHG impact, but 2-6\% below wood-framed using low-carbon concrete formula
\item  ``An AI system isn't limited in its capacity for memory and intelligence and creativity.''

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Woody24aiArchitClimDreamHm.html}
}

@techreport{Maslej23aiIndexReport,
  title = {Artificial {{Intelligence Index Report}} 2023},
  author = {Maslej, Nestor and Fattorini, Loredana and Brynjolfsson, Erik and Etchemendy, John and Ligett, Katrina and Lyons, Terah and Manyika, James and Ngo, Helen and Niebles, Juan Carlos and Parli, Vanessa and Shoham, Yoav and Wald, Russell and Clark, Jack and Perrault, Raymond},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03715},
  eprint = {2310.03715},
  primaryclass = {cs},
  institution = {Stanford University},
  url = {https://aiindex.stanford.edu/report/},
  urldate = {2024-03-13},
  abstract = {The AI Index is an independent initiative at the Stanford Institute for Human-Centered Artificial Intelligence (HAI), led by the AI Index Steering Committee, an interdisciplinary group of experts from across academia and industry. The annual report tracks, collates, distills, and visualizes data relating to artificial intelligence, enabling decision-makers to take meaningful action to advance AI responsibly and ethically with humans in mind. The AI Index collaborates with many different organizations to track progress in artificial intelligence. These organizations include: the Center for Security and Emerging Technology at Georgetown University, LinkedIn, NetBase Quid, Lightcast, and McKinsey. The 2023 report also features more self-collected data and original analysis than ever before. This year's report included new analysis on foundation models, including their geopolitics and training costs, the environmental impact of AI systems, K-12 AI education, and public opinion trends in AI. The AI Index also broadened its tracking of global AI legislation from 25 countries in 2022 to 127 in 2023.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,obsLitNote},
  note = {Maslej23aiIndexReport
\par
\begin{itemize}

\item 
\par
Size
\par
\begin{itemize}

\item GPT-2, released in 2019, 1.5 billion parameters and cost an estimated \$50,000 USD to train (``first LLM'')
\item 
\par
PaLM, one of the flagship large language models launched in 2022,had 540 billion parameters and cost an estimated \$8 million USD
\par
\begin{itemize}

\item 360 times larger than GPT-2 and cost 160 times more
\item 

\end{itemize}

\end{itemize}

\end{itemize}

\par
Wikipedia uses this: (Wikipedia, 2024)
\par
Also
\par
\begin{itemize}

\item (Gutierrez, 2024)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Maslej23aiIndexReport.pdf}
}

@article{wikipedia24FoundationModel,
  title = {Foundation Model},
  author = {{Wikipedia}},
  year = {2024},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Foundation_model&oldid=1212150730},
  urldate = {2024-03-13},
  abstract = {A foundation model is a machine learning model that is trained on broad data such that it can be applied across a wide range of use cases. Foundation models have transformed artificial intelligence (AI), powering prominent generative AI applications like ChatGPT. The Stanford Institute for Human-Centered Artificial Intelligence's (HAI) Center for Research on Foundation Models (CRFM) created and popularized the term.Foundation models are general-purpose technologies that can support a diverse range of use cases. Building foundation models is often highly resource-intensive, with the most expensive models costing hundreds of millions of dollars to pay for the underlying data and compute required. In contrast, adapting an existing foundation model for a specific use case or using it directly is much less expensive.  Early examples of foundation models were language models (LMs) like Google's BERT and OpenAI's "GPT-n" series. Beyond text, foundation models have been developed across a range of modalities---including DALL-E and Flamingo for images, MusicGen for music, and RT-2 for robotic control. Foundation models constitute a broad shift in AI development: foundation models are being built for astronomy, radiology, genomics, music, coding, and mathematics.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  keywords = {obsLitNote},
  annotation = {Page Version ID: 1212150730},
  note = {wikipedia24FoundationModel
\par
Good review of foundation models?
\par
I associated some of the stuff in this article with the entry for one of its sources: (Maslej et al., 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\wikipedia24FoundationModel.html}
}

@misc{ArtOfTheProblem23ChatGPT30Year,
  title = {{{ChatGPT}}: 30 {{Year History}}: {{How AI Learned}} to {{Talk}}},
  shorttitle = {{{ChatGPT}}},
  author = {Cruise, Brit},
  year = {2023},
  month = nov,
  url = {https://www.youtube.com/watch?v=OFS90-FX6pg},
  urldate = {2024-03-13},
  abstract = {This video explores the journey of AI language models, from their modest beginnings through the development of OpenAI's GPT models. Our journey takes us through the key moments in generative neural network research involved in next word prediction. We delve into the early experiments with tiny language models in the 1980s, highlighting significant contributions by researchers like Jordan, who introduced Recurrent Neural Networks, and Elman, whose work on learning word boundaries revolutionized our understanding of language processing. It leaves us with a question: what is thought? Is simulated thought, thought? Featuring Noam Chomsky Douglas Hofstadter Michael I. Jordan Jeffrey Elman Geoffrey Hinton Ilya Sutskever Andrej Karpathy Yann LeCun and more. (Sam altman) My script, references \& visualizations here: https://docs.google.com/document/d/1s... consider joining my channel as a YouTube member: ~~~/~@artoftheproblem~~ This is the last video in the series "The Pattern Machine" you can watch it all here: ~~~{$\bullet~$}Episode~5:~Deep~Learning~(The~Pattern...~~ 00:00 - Introduction 00:32 - hofstader's thoughts on chatGPT 01:00 - recap of supervised learning 01:55 - first paper on sequential learning 02:55 - first use of state units (RNN) 04:33 - first observation of word boundary detection 05:30 - first observation of word clustering 07:16 - first "large" language model Hinton/Sutskever  10:10 - sentiment neuron (Ilya {\textbar} OpenAI) 12:30 - transformer explaination 15:50 - GPT-1 17:00 - GPT-2 17:55 - GPT-3 18:20 - In-context learning 19:40 - ChatGPT 21:10 - tool use 23:25 - philosophical question: what is thought?},
  keywords = {obsLitNote}
}

@article{Bommasani21OpportunRisksFoundationMdls,
  title = {On the Opportunities and Risks of Foundation Models},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and {von Arx}, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma},
  year = {2021},
  journal = {arXiv preprint arXiv:2108.07258},
  eprint = {2108.07258},
  url = {https://arxiv.org/abs/2108.07258},
  urldate = {2024-03-14},
  archiveprefix = {arXiv},
  note = {Bommasani21OpportunRisksFoundationMdls
\par
Good graphics. ~Could be a good tutorial.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bommasani21OpportunRisksFoundationMdls.pdf}
}

@misc{Martineau23whatIsRAGblog,
  title = {What Is Retrieval-Augmented Generation?},
  author = {Martineau, Kim},
  year = {2023},
  month = aug,
  journal = {IBM Research Blog},
  url = {https://research.ibm.com/blog/retrieval-augmented-generation-RAG},
  urldate = {2024-03-14},
  abstract = {RAG is an AI framework for retrieving facts to ground LLMs on the most accurate information and to give users insight into AI's decisionmaking process.},
  copyright = {{\copyright} Copyright IBM Corp. 2021},
  langid = {american},
  note = {Martineau23whatIsRAGblog
\par
5 min explainer of retrieval augmented ~generation (RAG). ~Linked-to video is six mins long:(``What is Retrieval-Augmented Generation (RAG)?'', 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Martineau23whatIsRAG.html}
}

@misc{Ng24AnthropicUpsAnte,
  title = {Anthropic {{Ups}} the {{Ante}}, {{India Warns Developers}}, and More},
  author = {Ng, Andrew},
  year = {2024},
  month = mar,
  journal = {Anthropic Ups the Ante, India Warns Developers, and more},
  url = {https://www.deeplearning.ai/the-batch/issue-240/},
  urldate = {2024-03-14},
  abstract = {The Batch AI News and Insights: I've noticed a trend in how generative AI applications are built that might affect both big companies and developers...},
  langid = {english},
  note = {Ng24AnthropicUpsAnte
\par
Slide idea: show performance tables of the three Claude 3 models, and their costs per token.
\par
This costs are in this article, and the performance tables (pasted below). ~Maybe there are less fuzzy images in an Anthropic report
\par
\begin{itemize}

\item System prompt: inserted before the user's input is explained
\item ``sentience'' the ``needle-in-the-haystack' response to the pizza question
\item Auto generated news stories
\item news fact checking

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ng24AnthropicUpsAnte.html}
}

@misc{Rodriguez24googleFoundationTSfrcst,
  type = {Substack Newsletter},
  title = {Edge 378: {{Meet TimesFM}}: {{Google}}'s {{New Foundation Model}} for {{Time-Series Forecasting}}},
  shorttitle = {Edge 378},
  author = {Rodriguez, Jesus},
  year = {2024},
  month = mar,
  journal = {TheSequence},
  url = {https://thesequence.substack.com/p/edge-378-meet-timesfm-googles-new?publication_id=54309&utm_campaign=email-post-title&r=haqky&utm_medium=email},
  urldate = {2024-03-14},
  abstract = {The model is about 200M parameters and has been trained in over 100 billion data points.},
  note = {Rodriguez24googleFoundationTSfrcst
\par
Intro article on Google's time series forecasting foundation model, TimesFM. ~The paper is: (Das et al., 2024)}
}

@misc{Das24DecoderonlyFoundationModel,
  title = {A Decoder-Only Foundation Model for Time-Series Forecasting},
  author = {Das, Abhimanyu and Kong, Weihao and Sen, Rajat and Zhou, Yichen},
  year = {2024},
  month = feb,
  number = {arXiv:2310.10688},
  eprint = {2310.10688},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.10688},
  urldate = {2024-03-14},
  abstract = {Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.},
  archiveprefix = {arXiv},
  note = {Das24decoderFoundatnTSfrcst
\par
Google's time series forecasting foundation model, TimesFM. ~
\par
Summary in: (Rodriguez, 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Das24decoderFoundatnTSfrcst.pdf}
}

@misc{Bastian23DALLESystemPrompt,
  title = {{{DALL-E}} 3's System Prompt Reveals {{OpenAI}}'s Rules for {{AI}} Image Generation},
  author = {Bastian, Matthias},
  year = {2023},
  month = oct,
  journal = {THE DECODER},
  url = {https://the-decoder.com/dall-e-3s-system-prompt-reveals-openais-rules-for-generative-image-ai/},
  urldate = {2024-03-14},
  abstract = {OpenAI has equipped DALL-E 3 with a complex set of rules to prevent the generation of discriminatory or potential illegal images.},
  langid = {american},
  note = {Bastian23DALLESystemPrompt
\par
Explains what a system prompt is, and gives the example of DALLES's very long prompt. ~There's a collection of system prompts here:},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bastian23DALLESystemPrompt.html}
}

@misc{Miller24chatGPTAutoExpertSystemPrompts,
  title = {{{ChatGPT-AutoExpert}}/{{System Prompts}}.Md at Main {$\cdot$} Spdustin/{{ChatGPT-AutoExpert}}},
  author = {Miller, Dustin},
  year = {2024},
  month = mar,
  journal = {GitHub},
  url = {https://github.com/spdustin/ChatGPT-AutoExpert/blob/main/System%20Prompts.md},
  urldate = {2024-03-14},
  abstract = { Supercharged Custom Instructions for ChatGPT (non-coding) and ChatGPT Advanced Data Analysis (coding).  - spdustin/ChatGPT-AutoExpert},
  langid = {english},
  note = {Miller24chatGPTAutoExpertSystemPrompts
\par
A collection of gathered (I think) system prompts. ~More explanation here (Bastian, 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Miller24chatGPTAutoExpertSystemPrompts.pdf}
}

@misc{Thormundsson24powUseLLM,
  title = {Power Use in Training {{LLMs}} 2022},
  author = {Thormudsson, Bergur},
  year = {2024},
  month = feb,
  journal = {Statista},
  url = {https://www.statista.com/statistics/1384401/energy-use-when-training-llm-models/},
  urldate = {2024-03-14},
  abstract = {Energy consumption of artificial intelligence (AI) models in training is considerable, with both GPT-3, the original release of the current iteration of OpenAI's popular ChatGPT, and Gopher consuming well over a thousand-megawatt hours of energy simply for training.},
  langid = {english},
  note = {Thormundsson24powUseLLM
\par
LLMs use a ton of power to train and operate, but this could be small compared to any energy efficiency they improve.
\par
In 2022 numbers
\par
\begin{itemize}

\item GPT-3, ChatGPT, Gopher used ``well over'' a GWh to train
\item 
\par
usage ``significantly higher'' than that
\par
\begin{itemize}

\item But still GPT-3 used only as much as 200 Germans in 2022

\end{itemize}

\item mobile phone operators expext to save 10-15\% on power consumption, so author says LLMs could be a considerable energy saver.

\end{itemize}

\par
In 2024 numbers, chatGPT: (Sarah McQuate, 2023)
\par
-},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Thormundsson24powUseLLM.html}
}

@misc{McQuate23uwRsrchrEnergyChatGPT,
  title = {{{UW}} Researcher Discusses Just How Much Energy {{ChatGPT}} Uses},
  shorttitle = {Q\&{{A}}},
  author = {McQuate, Sarah},
  year = {2023},
  month = jul,
  journal = {UW News},
  url = {https://www.washington.edu/news/2023/07/27/how-much-energy-does-chatgpt-use/},
  urldate = {2024-03-14},
  abstract = {Training a large language model, such as ChatGPT, uses on average roughly equivalent to the yearly electricity consumption of over 1,000 U.S. households, according to Sajjad Moazeni, UW assistant...},
  langid = {english},
  note = {McQuate23uwRsrchrEnergyChatGPT
\par
ChatGPT-like large LLMs:
\par
\begin{itemize}

\item 
\par
supporting daily use:
\par
\begin{itemize}

\item 1000's of procs @ 400W each, 2X when consider cooling, etc.
\item can only be done in a datacenter
\item 
\par
v.s. convetional cloud computing: databases, video streaming
\par
\begin{itemize}

\item 
\par
``far less computationally intensive''
\par
\begin{itemize}

\item ``exponentionally'' less power: (Halper, 2024)''

\end{itemize}

\item ``orders of magnitude'' less memory

\end{itemize}

\item ChatGPT queries: 1GWh/day == 33K US houseolds

\end{itemize}

\item hundreds of such data centers in world (and increasing: see projected increase graphs in (Halper, 2024)
\item 
\par
Training
\par
\begin{itemize}

\item chatGPT-3: 10 GWh for one model (1000 US households)

\end{itemize}

\item This guy's solution: replacing electrical comms w/ optical

\end{itemize}

\par
Seems like ChatGPT-3 is much power hungrier than GPT-3: (Bergur Thormudsson, 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\McQuate23uwRsrchrEnergyChatGPT2.pdf}
}

@misc{Chowdhury24evalAIpowerUse,
  title = {Power-Hungry {{AI}}: {{Researchers}} Evaluate Energy Consumption across Models},
  shorttitle = {Power-Hungry {{AI}}},
  author = {Chowdhury, Mosharaf},
  journal = {U. of Mich. Computer Science and Engineering},
  url = {https://cse.engin.umich.edu/stories/power-hungry-ai-researchers-evaluate-energy-consumption-across-models/},
  urldate = {2024-03-14},
  abstract = {A new tool designed by researchers at the University of Michigan allows users to compare the energy efficiency of AI-powered language models.},
  langid = {american},
  keywords = {obsLitNote},
  note = {Chowdhury24evalAIpowerUse
\par
AI power consumption vs. quality \href{https://ml.energy/}{ML.ENERGY}~Leaderboard compares genAI params and energy consumption. ~Also lets you do A/B prompt response quality vs. energy consumption.
\par
\begin{itemize}

\item 
\par
data centers use 2\% of US electricity
\par
\begin{itemize}

\item data center: 50X average commercial building

\end{itemize}

\item LLMs could emit equivalent of ``5 B'' US cross-country flights/year. ~That's almost certainly a typo, and s/b ``5 M''. ~See (Chien, 2023)

\end{itemize}

\par
Part of (``The ML.ENERGY Initiative'', 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chowdhury24evalAIpowerUse.html}
}

@misc{MLENERGYInitiative24,
  title = {The {{ML}}.{{ENERGY Initiative}}},
  year = {2024},
  url = {https://ml.energy},
  urldate = {2024-03-15},
  abstract = {Making modern ML energy efficient},
  langid = {english},
  note = {MLENERGYInitiative24
\par
project to eval ML energy consumption, help trade w/ quality. ~See more at: (Research News)}
}

@article{Chien23GenAIGigaTeraWattHours,
  title = {{{GenAI}}: {{Giga}}\$\$\$, {{TeraWatt-Hours}}, and {{GigaTons}} of {{CO}}},
  shorttitle = {{{GenAI}}},
  author = {Chien, Andrew A.},
  year = {2023},
  month = aug,
  journal = {Communications of the ACM},
  volume = {66},
  number = {8},
  pages = {5--5},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3606254},
  url = {https://dl.acm.org/doi/10.1145/3606254},
  urldate = {2024-03-15},
  langid = {english},
  note = {Chien23GenAIGigaTeraWattHours
\par
\begin{itemize}

\item Source in for 5 B flights worth of genAI CO2/year in (Research News). ~But this article has a type: ~``billions'' should have been ``millions,'' at least when I cross-check it, I get 6.3 M flights/year (see pdf).

\end{itemize}

\par
``Several hyperscalers have stopped disclosing total power consumption due to growing public outcry.7'' (Chien, 2023, p. 1)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chien23GenAIGigaTeraWattHours.pdf}
}

@misc{Lambert23devPathsLLM,
  title = {Different Development Paths of {{LLMs}}},
  author = {Lambert, Nathan},
  year = {2023},
  month = nov,
  url = {https://www.interconnects.ai/p/llm-development-paths},
  urldate = {2024-03-15},
  abstract = {In industry, open-source, and academia, each of these giant pools of talent are driven by different incentives and will create very different language models.},
  langid = {english},
  note = {Lambert23devPathsLLM
\par
Prospects for open source LLMs, a very nice graph showing the evolutionary trees of LLMs, for example, which are transformer based (almost all of them)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lambert23devPathsLLM.html}
}

@inproceedings{Samsi23wordsToWattsLLM,
  title = {From Words to Watts: {{Benchmarking}} the Energy Costs of Large Language Model Inference},
  shorttitle = {From Words to Watts},
  booktitle = {2023 {{IEEE High Performance Extreme Computing Conference}} ({{HPEC}})},
  author = {Samsi, Siddharth and Zhao, Dan and McDonald, Joseph and Li, Baolin and Michaleas, Adam and Jones, Michael and Bergeron, William and Kepner, Jeremy and Tiwari, Devesh and Gadepally, Vijay},
  year = {2023},
  pages = {1--9},
  publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/10363447/},
  urldate = {2024-03-15},
  note = {Samsi23wordsToWattsLLM
\par
\begin{itemize}

\item for small LLM (?): each response token costs at most 7 Joules (Fig 7)
\item response (1024 len?) costs a little less than 2k Joules (Fig. 9)

\end{itemize}

\par
Tree graphic came from (Lambert, 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Samsi23wordsToWattsLLM.pdf}
}

@misc{Aster24langChainSQLagent,
  title = {{{LangChain SQL Agent}} for {{Massive Documents Interaction}}},
  author = {Aster, Ruben},
  year = {2024},
  month = mar,
  journal = {Medium},
  url = {https://pub.towardsai.net/langchain-sql-agent-for-massive-documents-interaction-510fc4bc65a4},
  urldate = {2024-03-15},
  abstract = {Leverage LangChain SQL Agent and GPT for Document Analysis and Interaction},
  langid = {english},
  keywords = {obsLitNote},
  note = {Aster24langChainSQLagent
\par
Turning a pile of PDF into a chatGPT-style queryable database. ~Has python.
\par
Uses of GenAI
\par
\begin{itemize}

\item turning extracted pdf text into JSON (function thing is interesting)
\item embedding this (embedding is kind of generative, isn't it?)
\item answering Q's, like ``what's a smallest box dimension that will fit all of our products?''

\end{itemize}

\par
Also, I think, a good example of a system prompt -- what they call: ``system message,'' I think.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Aster24langChainSQLagent.html}
}

@misc{Ma24Era1bitLLMs,
  title = {The {{Era}} of 1-Bit {{LLMs}}: {{All Large Language Models}} Are in 1.58 {{Bits}}},
  shorttitle = {The {{Era}} of 1-Bit {{LLMs}}},
  author = {Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei and Wang, Wenhui and Huang, Shaohan and Dong, Li and Wang, Ruiping and Xue, Jilong and Wei, Furu},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2402.17764v1},
  urldate = {2024-03-15},
  abstract = {Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary \{-1, 0, 1\}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Ma24Era1bitLLMs
\par
A 1.58 bit LLM matches a 16 bit floating point LLM (transformer, I think) in terms of compelexity. ~Doesn't multiply, but manipulates signs =={$>$} something needed other than a GPU.
\par
Google uses 8 or fewer bits for its TPUs: (Sato and Young, 2017)
\par
Good slides images, combine with (Sato and Young, 2017)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ma24Era1bitLLMs.pdf}
}

@misc{Sato17googleFirstTPU,
  title = {An In-Depth Look at {{Google}}'s First {{Tensor Processing Unit}} ({{TPU}})},
  author = {Sato, Kaz and Young, Cliff},
  year = {2017},
  month = may,
  journal = {Google Cloud Blog},
  url = {https://cloud.google.com/blog/products/ai-machine-learning/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu},
  urldate = {2024-03-15},
  langid = {english},
  keywords = {obsLitNote},
  note = {Sato17googleFirstTPU
\par
Google's first TPU used 8 bit math. Is 83X more power-efficient than a CPU; 29X than a GPU.
\par
Good graph bar chart comparison of CPU/GPU/TPU power performance 
\par
1 bit LLMs are probably more efficient: (Ma et al., 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sato17googleFirstTPU.html}
}

@article{Joosten24ComparingIdeationQuality,
  title = {Comparing the {{Ideation Quality}} of {{Humans With Generative Artificial Intelligence}}},
  author = {Joosten, J. and Bilgram, V. and Hahn, A. and Totzek, D.},
  year = {2024},
  journal = {IEEE Engineering Management Review},
  pages = {1--10},
  issn = {1937-4178},
  doi = {10.1109/EMR.2024.3353338},
  url = {https://ieeexplore.ieee.org/abstract/document/10398283},
  urldate = {2024-03-16},
  abstract = {Traditionally, ideating new product innovations is primarily the responsibility of marketers, engineers, and designers. However, a rapidly growing interest lies in leveraging generative artificial intelligence (AI) to brainstorm new product and service ideas. This study conducts a comparative analysis of ideas generated by human professionals and an AI system. The results of a blind expert evaluation show that AI-generated ideas score significantly higher in novelty and customer benefit, while their feasibility scores are similar to those of human ideas. Overall, AI-generated ideas comprise the majority of the top-performing ideas, while human-generated ideas scored lower than expected. The executive's emotional and cognitive reactions were measured during the evaluation to check for potential biases and showed no differences between the idea groups. These findings suggest that, under certain circumstances, companies can benefit from integrating generative AI into their traditional idea-generation processes.},
  keywords = {AI-augmented innovation,artificial intelligence,Artificial intelligence,Chatbots,ChatGPT,Companies,creativity,Creativity,generative AI,Generative AI,idea generation,innovation,large language models,Task analysis,Technological innovation},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Joosten24ComparingIdeationQuality.pdf;C\:\\Users\\scott\\Zotero\\storage\\ZLJ4P67P\\10398283.html}
}

@misc{Jia24CorporateResponsesGenerative,
  type = {{{SSRN Scholarly Paper}}},
  title = {Corporate {{Responses}} to {{Generative AI}}: {{Early Evidence}} from {{Conference Calls}}},
  shorttitle = {Corporate {{Responses}} to {{Generative AI}}},
  author = {Jia, Ning and Li, Ningzhong and Ma, Guang and Xu, Da},
  year = {2024},
  month = feb,
  number = {4736295},
  address = {Rochester, NY},
  url = {https://papers.ssrn.com/abstract=4736295},
  urldate = {2024-03-16},
  abstract = {We provide early evidence of generative artificial intelligence (GAI)'s potential impact on corporations through managerial discussions of GAI in conference calls after the release of ChatGPT in November 2022. Following the release, managerial discussions of GAI in conference calls increase substantially, and the increase is more pronounced for firms with greater innovation intensity, cybersecurity threats, product differentiation, labor exposure to AI, and customer operations, suggesting that these firms are more likely to be affected by GAI. Managers tend to believe that GAI is more beneficial to firms with greater innovation intensity and cybersecurity threats but is more detrimental to firms with greater product differentiation. While they hold mixed views on GAI's impact on firms with greater labor exposure to AI and customer operations, their views are more likely to be positive than negative. Managers of firms with greater innovation intensity, cybersecurity threats and customer operations (product differentiation) increase initiative-related (non-initiative-related) discussions more. While managers of firms with greater labor exposure to AI increase discussions of both types more, they are more likely to increase initiative-related discussions. Overall, our study sheds light on the heterogeneous corporate perceptions of GAI's impacts and responses.},
  langid = {english},
  keywords = {ChatGPT,conference call,customer operations,cybersecurity,generative AI,innovation,labor exposure to AI,product differentiation},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Jia24CorporateResponsesGenerative.pdf}
}

@misc{Reinhard24GenerativeAICustomer,
  type = {{{SSRN Scholarly Paper}}},
  title = {Generative {{AI}} in {{Customer Support Services}}: {{A Framework}} for {{Augmenting}} the {{Routines}} of {{Frontline Service Employees}}},
  shorttitle = {Generative {{AI}} in {{Customer Support Services}}},
  author = {Reinhard, Philipp and Li, Mahei Manhai and Peters, Christoph and Leimeister, J. M.},
  year = {2024},
  month = jan,
  number = {4612768},
  address = {Rochester, NY},
  url = {https://papers.ssrn.com/abstract=4612768},
  urldate = {2024-03-16},
  abstract = {Customer support service employees are facing an increased workload, while artificial intelligence (AI) appears to possess the potential to change the way we work. With the advent of modern types of generative AI, new opportunities to augment frontline service employees have emerged. However, little is known about how to integrate generative AI in customer support service organizations and purposefully change service employee work routines. Following multi-method qualitative research, we performed a literature review, conducted workshops, and interviewed IT support agents, managers, and AI experts. Thereby, we examine AI augmentation for frontline service employees in the context of IT support to carve out where and how GenAI can be leveraged to develop more efficient and higher-quality customer support. Our resulting framework reveals that especially adapting solutions and retaining knowledge is subject to a high degree of AI augmentation.},
  langid = {english},
  keywords = {artificial intelligence,customer service,Generative AI augmentation,large language models},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Reinhard24GenerativeAICustomer.pdf}
}

@article{Urban23ChatGPTImprovesCreative,
  title = {{{ChatGPT Improves Creative Problem-Solving Performance}} in {{University Students}}: {{An Experimental Study}}},
  shorttitle = {{{ChatGPT Improves Creative Problem-Solving Performance}} in {{University Students}}},
  author = {Urban, Marek and Dechterenko, Filip and Lukavsky, Jiri and Hrabalov{\'a}, Veronika and Svacha, Filip and Brom, Cyril and Urban, Kamila},
  year = {2023},
  publisher = {PsyArXiv},
  url = {https://osf.io/preprints/psyarxiv/9z2tc/},
  urldate = {2024-03-16},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Urban23ChatGPTImprovesCreative.pdf}
}

@article{Potts23LargeLanguageModels,
  title = {Large Language Models Reduce Agency Costs},
  author = {Potts, Jason and Allen, Darcy WE and Berg, Chris and Ilyushina, Nataliya},
  year = {2023},
  journal = {Available at SSRN},
  url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4437679},
  urldate = {2024-03-16},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Potts23LargeLanguageModels.pdf}
}

@article{Kreitmeir24HeterogeneousProductivityEffects,
  title = {The {{Heterogeneous Productivity Effects}} of {{Generative AI}}},
  author = {Kreitmeir, David and Raschky, Paul A.},
  year = {2024},
  journal = {arXiv preprint arXiv:2403.01964},
  eprint = {2403.01964},
  url = {https://arxiv.org/abs/2403.01964},
  urldate = {2024-03-16},
  archiveprefix = {arXiv},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kreitmeir24HeterogeneousProductivityEffects.pdf}
}

@article{Ju23ExperimentalEvidenceNegative,
  title = {Experimental {{Evidence}} on {{Negative Impact}} of {{Generative AI}} on {{Scientific Learning Outcomes}}},
  author = {Ju, Qirui},
  year = {2023},
  journal = {arXiv preprint arXiv:2311.05629},
  eprint = {2311.05629},
  url = {https://arxiv.org/abs/2311.05629},
  urldate = {2024-03-16},
  archiveprefix = {arXiv},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ju23ExperimentalEvidenceNegative.pdf}
}

@techreport{Agarwal23humanExpertAIradiology,
  title = {Combining Human Expertise with Artificial Intelligence: {{Experimental}} Evidence from Radiology},
  shorttitle = {Combining Human Expertise with Artificial Intelligence},
  author = {Agarwal, Nikhil and Moehring, Alex and Rajpurkar, Pranav and Salz, Tobias},
  year = {2023},
  institution = {National Bureau of Economic Research},
  url = {https://www.nber.org/papers/w31422},
  urldate = {2024-03-16},
  note = {Agarwal23humanExpertAIradiology
\par
More human expertise combining:
\par
\begin{itemize}

\item (Van Leeuwen, 2024)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Agarwal23humanExpertAIradiology.pdf}
}

@article{LIU23ExperimentalEvidenceProductivity,
  title = {Experimental {{Evidence}} on the {{Productivity Effects}} of {{Generative Arti-Cial Intelligence}}},
  author = {LIU, {\relax SANNYUYA} and LIU, {\relax SHIQI} and Liu, Z. and SUN, {\relax JIANWEN} and LI, {\relax QING} and YANG, {\relax ZONGKAI}},
  year = {2023},
  journal = {Science},
  volume = {381},
  number = {6654},
  pages = {187--192},
  url = {https://www.researchgate.net/profile/Shiqi-Liu-27/publication/374371101_AI_is_also_reshaping_education_in_both_practice_and_science_eLetters/links/651cfba5b0df2f20a20e88fd/AI-is-also-reshaping-education-in-both-practice-and-science-eLetters.pdf},
  urldate = {2024-03-16},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\LIU23ExperimentalEvidenceProductivity.pdf}
}

@article{Dohmke23SeaChangeSoftware,
  title = {Sea Change in Software Development: {{Economic}} and Productivity Analysis of the Ai-Powered Developer Lifecycle},
  shorttitle = {Sea Change in Software Development},
  author = {Dohmke, Thomas and Iansiti, Marco and Richards, Greg},
  year = {2023},
  journal = {arXiv preprint arXiv:2306.15033},
  eprint = {2306.15033},
  url = {https://arxiv.org/abs/2306.15033},
  urldate = {2024-03-16},
  archiveprefix = {arXiv},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Dohmke23SeaChangeSoftware.pdf}
}

@article{Haslberger23NoGreatEqualizer,
  title = {No Great Equalizer: Experimental Evidence on {{AI}} in the {{UK}} Labor Market},
  shorttitle = {No Great Equalizer},
  author = {Haslberger, Matthias and Gingrich, Jane and Bhatia, Jasmine},
  year = {2023},
  journal = {Available at SSRN},
  url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4594466},
  urldate = {2024-03-16},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Haslberger23NoGreatEqualizer.pdf}
}

@article{DellAcqua23NavigatingJaggedTechnological,
  title = {Navigating the Jagged Technological Frontier: {{Field}} Experimental Evidence of the Effects of {{AI}} on Knowledge Worker Productivity and Quality},
  shorttitle = {Navigating the Jagged Technological Frontier},
  author = {Dell'Acqua, Fabrizio and McFowland, Edward and Mollick, Ethan R. and {Lifshitz-Assaf}, Hila and Kellogg, Katherine and Rajendran, Saran and Krayer, Lisa and Candelon, Fran{\c c}ois and Lakhani, Karim R.},
  year = {2023},
  journal = {Harvard Business School Technology \& Operations Mgt. Unit Working Paper},
  number = {24-013},
  url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321},
  urldate = {2024-03-16},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\DellAcqua23NavigatingJaggedTechnological.pdf}
}

@article{Zhu23RoleGenerativeAI,
  title = {The {{Role}} of {{Generative AI}} in {{Human Creative Processes}}: {{Experimental Evidence}}},
  shorttitle = {The {{Role}} of {{Generative AI}} in {{Human Creative Processes}}},
  author = {Zhu, Feng and Zou, Wenbo},
  year = {2023},
  journal = {Available at SSRN 4676053},
  url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4676053},
  urldate = {2024-03-16}
}

@article{Nakavachara24ExperimentingGenerativeAI,
  title = {Experimenting with {{Generative AI}}: {{Does ChatGPT Really Increase Everyone}}'s {{Productivity}}?},
  shorttitle = {Experimenting with {{Generative AI}}},
  author = {Nakavachara, Voraprapa and Potipiti, Tanapong and Chaiwat, Thanee},
  year = {2024},
  journal = {arXiv preprint arXiv:2403.01770},
  eprint = {2403.01770},
  url = {https://arxiv.org/abs/2403.01770},
  urldate = {2024-03-16},
  archiveprefix = {arXiv},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nakavachara24ExperimentingGenerativeAI.pdf}
}

@article{AlNaqbi24EnhancingWorkProductivitya,
  title = {Enhancing {{Work Productivity}} through {{Generative Artificial Intelligence}}: {{A Comprehensive Literature Review}}},
  shorttitle = {Enhancing {{Work Productivity}} through {{Generative Artificial Intelligence}}},
  author = {Al Naqbi, Humaid and Bahroun, Zied and Ahmed, Vian},
  year = {2024},
  journal = {Sustainability},
  volume = {16},
  number = {3},
  pages = {1166},
  publisher = {MDPI},
  url = {https://www.mdpi.com/2071-1050/16/3/1166},
  urldate = {2024-03-16},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\AlNaqbi24EnhancingWorkProductivity.pdf}
}

@article{Noy23ExperimentalEvidenceProductivity,
  title = {Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence},
  author = {Noy, Shakked and Zhang, Whitney},
  year = {2023},
  month = jul,
  journal = {Science},
  volume = {381},
  number = {6654},
  pages = {187--192},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.adh2586},
  url = {https://www.science.org/doi/10.1126/science.adh2586},
  urldate = {2024-03-16},
  abstract = {We examined the productivity effects of a generative artificial intelligence (AI) technology, the assistive chatbot ChatGPT, in the context of midlevel professional writing tasks. In a preregistered online experiment, we assigned occupation-specific, incentivized writing tasks to 453 college-educated professionals and randomly exposed half of them to ChatGPT. Our results show that ChatGPT substantially raised productivity: The average time taken decreased by 40\% and output quality rose by 18\%. Inequality between workers decreased, and concern and excitement about AI temporarily rose. Workers exposed to ChatGPT during the experiment were 2 times as likely to report using it in their real job 2 weeks after the experiment and 1.6 times as likely 2 months after the experiment.           ,              Editor's summary             Automation has historically displaced human workers in factories (e.g., automotive manufacturing) or in performing routine computational tasks. Will generative artificial intelligence (AI) tools such as ChatGPT disrupt the labor market by making educated professionals obsolete, or will these tools complement their skills and enhance productivity? Noy and Zhang examined this issue in an experiment that recruited college-educated professionals to complete incentivized writing tasks. Participants assigned to use ChatGPT were more productive, efficient, and enjoyed the tasks more. Participants with weaker skills benefited the most from ChatGPT, which carries policy implications for efforts to reduce productivity inequality through AI. ---EEU           ,              The assistive chatbot ChatGPT raises productivity in professional writing tasks and reduces productivity inequality.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Noy23ExperimentalEvidenceProductivity.pdf}
}

@misc{Woo24UnifiedTrainingUniversal,
  title = {Unified {{Training}} of {{Universal Time Series Forecasting Transformers}}},
  author = {Woo, Gerald and Liu, Chenghao and Kumar, Akshat and Xiong, Caiming and Savarese, Silvio and Sahoo, Doyen},
  year = {2024},
  month = feb,
  number = {arXiv:2402.02592},
  eprint = {2402.02592},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.02592},
  urldate = {2024-03-18},
  abstract = {Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, model weights, and data will be released.},
  archiveprefix = {arXiv},
  note = {Woo24trainTSfrcstTransfrmrs
\par
Tech Note: (Woo et al., 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Woo24trainTSfrcstTransfrmrs.pdf}
}

@misc{Monserrate24ecoImpactsCloudComp,
  title = {The {{Staggering Ecological Impacts}} of {{Computation}} and the {{Cloud}}},
  author = {{Steven Gonzalez Monserrate}},
  year = {2022},
  month = feb,
  journal = {The MIT Press Reader},
  url = {https://thereader.mitpress.mit.edu/the-staggering-ecological-impacts-of-computation-and-the-cloud/},
  urldate = {2024-03-18},
  abstract = {Anthropologist Steven Gonzalez Monserrate draws on five years of research and ethnographic fieldwork in server farms to illustrate some of the diverse environmental impacts of data storage.},
  langid = {english},
  note = {Monserrate24ecoImpactsCloudComp}
}

@misc{Metz24chatGPTphysWorldST,
  title = {How the {{AI}} That Drives {{ChatGPT}} Will Move into the Physical World},
  author = {Metz, Cade},
  year = {2024},
  month = mar,
  journal = {The Seattle Times},
  url = {https://www.seattletimes.com/business/how-the-ai-that-drives-chatgpt-will-move-into-the-physical-world/},
  urldate = {2024-03-18},
  abstract = {Covariant, a robotics start-up, is designing technology that lets robots learn skills like chatbots.},
  langid = {american},
  note = {Metz24chatGPTphysWorldST
\par
This is the Seattle Times version of (Metz and Gardi, 2024). It has some details not in the NYT version. Maybe the NYT version has details not in this article?
\par
More details on this robot LLM: (Ng, 2024)
\par
\section{Points}

\begin{itemize}

\item trained from words, sounds and images
\item text-bot like text, images a videos gave it world knowledge
\item it can respond to human language
\item Appliccation-specific data needs (a lot of it): ~spent years gathering data --- from cameras and other sensors --- that showshow theserobots operate
\item 
\par
\textbf{generative part}: it predicts images of what would happen if it takes some action. ~Allows it to plan. ~This works on real-world object that weren't in its application specific data
\par
\begin{itemize}

\item Question from me: is this like the prediction in reinforcement learning?

\end{itemize}

\item 
\par
old school:
\par
\begin{itemize}

\item robots trained for precise repeatable motions,
\item repeatable
\item but fail on uexpected situations

\end{itemize}

\item 
\par
new school
\par
\begin{itemize}

\item robots also trained on general world data
\item can sometimes handle unexpected situations
\item 
\par
sometimes drops things
\par
\begin{itemize}

\item OK in wharehouses where failure is an option

\end{itemize}

\end{itemize}

\item 
\par
Foundation model
\par
\begin{itemize}

\item RFM: robot foundation model

\end{itemize}

\item Similar robot idea but in video, human forces help: (Chu, 2024)

\end{itemize}

\section{Chunks}

\par
``learn from different kinds of data at the same time'' (Metz, 2024, p. 1)
\par
``recognize words, sounds and images'' (Metz, 2024, p. 1)
\par
``spent years gatheringdata --- from cameras and other sensors --- that showshow theserobots operate.'' (Metz, 2024, p. 1)
\par
``text used to train chatbot'' (Metz, 2024, p. 1)
\par
``If it can predict the next framesina video, it can pinpoint the right strategy to follow'' (Metz, 2024, p. 1)
\par
``RFM'' (Metz, 2024, p. 2)
\par
``robotics foundational model,'' (Metz, 2024, p. 2)
\par
``useful inwarehouses and other situations wheremistakes are acceptable'' (Metz, 2024, p. 2)
\par
``rogrammedrobots to perform the sameprecisemotion'' (Metz, 2024, p. 2)
\par
``couldnot deal with unexpected or random situations.'' (Metz, 2024, p. 2)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Metz24chatGPTphysWorldST.pdf}
}

@misc{VanLeeuwen24predMaintGenAIwaylay,
  title = {Predictive {{Maintenance}} and {{Generative AI}}},
  author = {Van Leeuwen, Tom},
  year = {2024},
  month = feb,
  journal = {Waylay Blog},
  url = {https://www.waylay.io/articles/predictive-maintenance-and-generative-ai},
  urldate = {2024-03-18},
  abstract = {Blog by Tom Van Leeuwen.},
  langid = {english},
  note = {VanLeeuwen24predMaintGenAIwaylay
\par
A chatbot quizzes a maintenance guy (I think) and builds a causal failure of graphs for use in interpreting and responding to (the usual, I suppose) predictive maintenance alarms -- example is a battery health alarm from a bus.
\par
A Service Engineer and Field Service Agent foundation model for predictive maintenance. ~The Digital Twin Rule Explainer automatically generates an explanation for some kind of alarm. This is said to help with decision making of the Service Agent -- support ticket, word order, escalate (really? I'm not sure).. The text is pasted into a case description, said to improve understanding, urgency and trust (really?). ~The description is generated from a causual rule graph, generated by an LLM which interrogates a company rule maker.
\par
\begin{itemize}

\item Is based on a strict set of causal rules, that I think the customer must generate, in response to LLM prompts, which are heavily prompt engineered (a system prompt, you could say, goals are listed)
\item rules are ``stateful causal graphs''
\item 
\par
rule explainer LLM trained on ``~trained on hundreds of anonymized asset monitoring rules from different markets~in which~Waylay has been active: industrial assets, building management, automotive, agritech, telecoms and finance.''
\par
\begin{itemize}

\item An example of collecting domain knowledge mentioned in other articles

\end{itemize}

\item says LLM can understand parameters, root causes, and meanings, but the exampel they give looks like boring cut and paste to me.

\end{itemize}

\begin{itemize}

\item one advantage is that it's multilingual
\item its ~example of a ``foundational LLM'' is gpt-3.5-turbo but they also say the trained a foundational LLM for remote Service engineers and field service agents

\end{itemize}

\par
(``GE's Predictive Maintenance Portfolio'', 2022) explains causal AI
\par
More human/AI combos
\par
\begin{itemize}

\item (Agarwal et al., 2023)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\VanLeeuwen24predMaintGenAIwaylay.html}
}

@misc{Lawton23genAIvspredAI,
  title = {Generative {{AI}} vs. Predictive {{AI}}: {{Understanding}} the Differences},
  shorttitle = {Generative {{AI}} vs. Predictive {{AI}}},
  author = {{George Lawton}},
  year = {2023},
  month = sep,
  journal = {TechTarget},
  url = {https://www.techtarget.com/searchenterpriseai/tip/Generative-AI-vs-predictive-AI-Understanding-the-differences},
  urldate = {2024-03-19},
  abstract = {Discover the benefits, limitations and business use cases for generative AI vs. predictive AI.},
  langid = {english},
  note = {Lawton23genAIvspredAI
\par
Good for classifying AI types. ~Woth reading},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lawton23genAIvspredAI.html}
}

@misc{Farmer24genAIpredAInoReplace,
  title = {Generative {{AI}} Can Improve -- Not Replace -- Predictive Analytics},
  author = {Farmer, Donald},
  year = {2024},
  month = feb,
  journal = {TechTarget},
  url = {https://www.techtarget.com/searchbusinessanalytics/tip/Generative-AI-can-improve-not-replace-predictive-analytics},
  urldate = {2024-03-19},
  abstract = {Generative AI brings new capabilities for predictive analytics models to simulate and capture a wider range of outcomes through synthetic data generation.},
  langid = {english},
  note = {Farmer24genAIpredAInoReplace
\par
Good for classifiying AI types. ~Worth reading.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Farmer24genAIpredAInoReplace.html}
}

@misc{Bigelow24genAivsML,
  title = {Generative {{AI}} vs. Machine Learning: {{How}} Are They Different?},
  shorttitle = {Generative {{AI}} vs. Machine Learning},
  author = {Bigelow,, Stephen J.},
  year = {2024},
  month = jan,
  journal = {TechTarget},
  url = {https://www.techtarget.com/searchenterpriseai/tip/Generative-AI-vs-machine-learning-How-are-they-different},
  urldate = {2024-03-19},
  abstract = {Compare generative AI vs. machine learning, including the similarities and differences between the two technologies and examples of real-world use cases.},
  langid = {english},
  note = {Bigelow24genAivsML
\par
Good for AI classifying slides.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bigelow24genAivsML.html}
}

@misc{Lawton23transformerNNshakeAI,
  title = {Transformer Neural Networks Are Shaking up {{AI}}},
  author = {Lawton, George},
  year = {2023},
  month = dec,
  journal = {TechTarget},
  url = {https://www.techtarget.com/searchenterpriseai/feature/Transformer-neural-networks-are-shaking-up-AI},
  urldate = {2024-03-19},
  abstract = {Transformer neutral networks were a key advance in natural language processing. Learn what transformers are, how they work and their role in generative AI.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Lawton23transformerNNshakeAI
\par
Perhaps good for AI classification, but generally, this series of posts was written, so generally worth reading.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lawton23transformerNNshakeAI.html}
}

@misc{Lawton23whatisGenAI,
  title = {What Is {{Generative AI}}? {{Everything You Need}} to {{Know}}},
  shorttitle = {What Is {{Generative AI}}?},
  author = {Lawton, George},
  year = {2023},
  month = may,
  journal = {TechTarget},
  url = {https://www.techtarget.com/searchenterpriseai/definition/generative-AI},
  urldate = {2024-03-19},
  abstract = {Generative AI is a type of artificial intelligence technology that can produce various types of content. Find out how it works and why it's a hot commodity.},
  langid = {english},
  note = {Lawton23whatisGenAI
\par
For AI classification sequence, start reading here},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lawton23whatisGenAI.pdf}
}

@misc{Mishra22aiMLpredMaintGE,
  title = {{{GE}}'s {{Predictive Maintenance Portfolio}}},
  author = {Mishra, Madhav},
  year = {2022},
  month = aug,
  url = {https://www.youtube.com/watch?v=Z9SN2fQ2dow},
  urldate = {2024-03-19},
  abstract = {Using AI/ML in Improving Industrial Reliability through PM Predictive Maintenance (PM) is becoming ubiquitous for improving availability and reliability along with reducing O\&M costs in industrial systems. Despite significant research and development investment in the last decade most deployed solutions still tend to be piecemeal (component or failure mode specific) point solutions and generally lack trust with respect to automated decision making. Full end-to-end deployment with system-wide coverage and autonomy still remains an elusive goal in industrial setting. This is primarily due to high cost and limited scalability of conventional modeling approaches for underlying complex systems and processes in large fleets. Specifically, capabilities to safe-guard against unknown-unknowns, lack of explain-ability and trust tend to be key bottlenecks. Given these systems are heavily instrumented generating large volumes of high-speed data and compute costs continue to go down, recent advancements in data-driven methods using machine learning (ML) and artificial intelligence (AI) have shown promise in a number of areas that previously led to valley of death between PM technology and commercialization. GE's Digital Twin technology for Predictive Maintenance is leveraging AI to bridge a number of such critical gaps that were otherwise very challenging to tackle through conventional methods. This session will enumerate key challenges in enabling system-wide predictive maintenance and how AI is being used to overcome these. Specifically, a causal deep learning-based approach will be described that provides a causal graph of inter-variable relationships allowing validation of deep learning model with domain experts. Further, by providing causal factors for identified anomalies root cause analysis can be facilitated for alert disposition in efficient manner at the fleet level. We will also describe our approach towards competency awareness of AI models, which aims to solve uncertainty management and trust for industrial applications of AI. Various applications and use-cases will be shared to show effectiveness of AI and ML using both structured and unstructured data in the context of intelligent PM. Speaker: Dr. Abhinav Saxena is a Principal Scientist in AI \& Learning Systems at GE Research. Abhinav has been developing ML/AI-based PHM solutions for various industrial systems (aviation, nuclear, power, and healthcare) at GE and has been driving integration of AI-based PHM analytics in GE's industrial systems. He is the PI for ARPA-E GEMINA program on AI-Enabled Predictive Maintenance Digital twins for Advanced Nuclear Reactors.},
  note = {Has example of causal AI, maybe good to understand (Van Leeuwen, 2024)}
}

@misc{causuaLens23CausalGenAItogeth,
  title = {Causal {{AI}} \& {{Gen AI Synergies}} {\textbar} {{causaLens}}},
  author = {{causuaLens}},
  year = {2023},
  month = oct,
  url = {https://causalens.com/causal-ai-gen-ai-synergies/},
  urldate = {2024-03-19},
  langid = {american},
  keywords = {todo},
  note = {causuaLens23CausalGenAItogeth
\par
Casual AI said to make genAI trustable, I think
\par
Also (Dynatrace, 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\causuaLens23CausalGenAItogeth.pdf}
}

@misc{Dynatrace24omboPredCausGenAI,
  title = {How Combining Predictive, Causal, and Generative {{AI}} Can Deliver Amplified {{AI}} Value},
  author = {{Dynatrace}},
  year = {2024},
  month = jan,
  journal = {B2B Digital Now},
  url = {https://b2bdigitalnow.com/dynatrace/how-combining-predictive-causal-and-generative-ai-can-deliver-amplified-ai-value/},
  urldate = {2024-03-19},
  abstract = {How combining predictive, causal, and generative AI can deliver amplified AI value},
  langid = {english},
  note = {Dynatrace24omboPredCausGenAI
\par
Good for AI types section?
\par
Also GenAI by itself can't be trusted (causuaLens, 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Dynatrace24omboPredCausGenAI.pdf}
}

@misc{Vivek24liveUpToHypeDSPy,
  title = {{{DSPy}} --- {{Does It Live Up To The Hype}}?},
  author = {Vivek, Skanda},
  year = {2024},
  month = mar,
  journal = {EMAlpha},
  url = {https://medium.com/emalpha/dspy-does-it-live-up-to-the-hype-6e56c2c6e7a0},
  urldate = {2024-03-19},
  abstract = {The DSPy framework promises to replace manual  prompt engineering with a programming framework for auto-tunded prompts. Let's see whether{\dots}},
  langid = {english},
  note = {Vivek24liveUpToHypeDSPy
\par
Can DSPY eliminate prompt engineering:?
\par
DSPy: (Khattab et al., 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Vivek24liveUpToHypeDSPy.html}
}

@misc{Hozumi22CCPCorrelatedClustering,
  title = {{{CCP}}: {{Correlated Clustering}} and {{Projection}} for {{Dimensionality Reduction}}},
  shorttitle = {{{CCP}}},
  author = {Hozumi, Yuta and Wang, Rui and Wei, Guo-Wei},
  year = {2022},
  month = jun,
  number = {arXiv:2206.04189},
  eprint = {2206.04189},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.04189},
  url = {http://arxiv.org/abs/2206.04189},
  urldate = {2024-03-19},
  abstract = {Most dimensionality reduction methods employ frequency domain representations obtained from matrix diagonalization and may not be efficient for large datasets with relatively high intrinsic dimensions. To address this challenge, Correlated Clustering and Projection (CCP) offers a novel data domain strategy that does not need to solve any matrix. CCP partitions high-dimensional features into correlated clusters and then projects correlated features in each cluster into a one-dimensional representation based on sample correlations. Residue-Similarity (R-S) scores and indexes, the shape of data in Riemannian manifolds, and algebraic topology-based persistent Laplacian are introduced for visualization and analysis. Proposed methods are validated with benchmark datasets associated with various machine learning algorithms.},
  archiveprefix = {arXiv},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hozumi22CCPCorrelatedClustering.pdf}
}

@techreport{AEMO21schedLoadGuide,
  title = {Guide to {{Scheduled Loads}}},
  author = {{AEMO}},
  year = {2021},
  month = jul,
  number = {Version 2.0},
  address = {Australia},
  institution = {Australian Energy Market Operator Limited.},
  url = {https://aemo.com.au/-/media/files/stakeholder_consultation/consultations/nem-consultations/2019/5ms-dispatch-non-rules-procedures/guide-to-scheduled-loads.pdf?la=en&hash=E68AF0298AE4D88E78A582992AF6D12F},
  abstract = {Under the Rules, scheduled loads are net consumers of electricity that register to participate in the central dispatch and pricing processes operated by AEMO. For the purposes of economic scheduling of electricity to meet demand, scheduled loads are essentially  treated on equal terms with scheduled generating units with no dispatch models designed to favour  scheduling from one type of unit over another. In this document the models described for scheduled loads equally apply for scheduled generating units unless otherwise specified. Furthermore, the same participant interfaces are available for the bidding, dispatch and market reporting of scheduled generating units and scheduled loads. However, market participants with registered scheduled loads should specifically be aware of: - The expected structure of their dispatch bids and how the central dispatch process interprets these  bids;  - Market ancillary services offer data and how the central dispatch process interprets this data;  - The other data required by the central dispatch process;  - Modelling in the dispatch algorithm that is specific to scheduled loads;  - The interpretation of load dispatch instructions.},
  keywords = {obsLitNote},
  note = {AEMO21schedLoadGuide
\par
View says this describes AEMO price formation.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\AEMO21schedLoadGuide.pdf}
}

@misc{Brunton24physInformedMLseries,
  title = {Physics {{Informed Machine Learning}}: {{High Level Overview}} of {{AI}} and {{ML}} in {{Science}} and {{Engineering}}},
  shorttitle = {Physics {{Informed Machine Learning}}},
  author = {Brunton, Steve},
  year = {2024},
  month = feb,
  url = {https://www.youtube.com/watch?v=JoFW2uSd3Uo},
  urldate = {2024-03-19},
  abstract = {This video describes how to incorporate physics into the machine learning process.  The process of machine learning is broken down into five stages: (1) formulating a problem to model, (2) collecting and curating training data to inform the model, (3) choosing an architecture with which to represent the model, (4) designing a loss function to assess the performance of the model, and (5) selecting and implementing an optimization algorithm to train the model. At each stage, we discuss how prior physical knowledge may be embedding into the process.   Physics informed machine learning is critical for many engineering applications, since many engineering systems are governed by physics and involve safety critical components.  It also makes it possible to learn more from sparse and noisy data sets.  \%\%\% CHAPTERS \%\%\% 00:00 Intro 03:53 What is Physics Informed Machine Learning? 06:41 Case Study: Encoding Pendulum Movement 09:19 The Five Stages of Machine Learning 16:09 A Principled Approach to Machine Learning 20:00 Physics Informed Problem Modeling 21:48 Physics Informed Data Curation 25:34 Physics Informed Architecture Design 28:59 Physics Informed Loss Functions 30:55 Physics Informed Optimization Algorithms 34:56 What This Course Will Cover 46:48 Outro},
  keywords = {obsLitNote},
  note = {Brunton24physInformedMLseries
\par
UW lecture series on physical models in ML. ~This is the 1st one.}
}

@misc{Danilevsky23whatIsRAGyoutube,
  title = {What Is {{Retrieval-Augmented Generation}} ({{RAG}})?},
  author = {Danilevsky, Marina},
  year = {2023},
  month = aug,
  address = {IBM Technology},
  url = {https://www.youtube.com/watch?v=T-D1OfcDW1M},
  urldate = {2024-03-20},
  abstract = {Try RAG with watsonx {$\rightarrow$} https://ibm.biz/BdMsRT Learn more about RAG{$\rightarrow$} https://ibm.biz/BdMsRt Large language models usually give great answers, but because they're limited to the training data used to create the model, over time they can become incomplete--or worse, generate answers that are just plain wrong. One way of improving the LLM results is called "retrieval-augmented generation" or RAG. In this video, IBM Senior Research Scientist Marina Danilevsky explains the LLM/RAG framework and how this combination delivers two big advantages, namely: the model gets the most up-to-date and trustworthy facts, and you can see where the model got its info, lending more credibility to what it generates. Get started for free on IBM Cloud {$\rightarrow$} https://ibm.biz/sign-up-now Subscribe to see more videos like this in the future {$\rightarrow~$}http://ibm.biz/subscribe-now},
  note = {Danilevsky23whatIsRAGyoutube
\par
\begin{itemize}

\item An LLM trained with old (or maybe partly irrelevant (?)) information can give correct answers by retrieving new, relevant information.
\item ``retrieve relevant information'' is in the ``system prompt'' (I think you would call it). ~
\item also provides a reference
\item also helps it say ``I don't know''

\end{itemize}

\par
Blog post: (Martineau, 2023)}
}

@misc{TutorMaster24ChronosZeroShotTSfrcst,
  title = {Chronos: {{Another Zero-Shot Time Series Forecaster LLM}}},
  shorttitle = {Chronos},
  author = {TutorMaster, A. I.},
  year = {2024},
  month = mar,
  journal = {Medium},
  url = {https://levelup.gitconnected.com/chronos-another-zero-shot-time-series-forecaster-llm-0e80753a7ad0},
  urldate = {2024-03-20},
  abstract = {Pretrained language models revolutionize the way we predict trends and patterns in data.},
  langid = {english},
  keywords = {obsLitNote},
  note = {TutorMaster24ChronosZeroShotTSfrcst
\par
Quick tutorial for: (Ansari et al., 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\TutorMaster24ChronosZeroShotTSfrcst.html}
}

@misc{Ansari24ChronosLearningLanguage,
  title = {Chronos: {{Learning}} the {{Language}} of {{Time Series}}},
  shorttitle = {Chronos},
  author = {Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Sundar and Arango, Sebastian Pineda and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Wilson, Andrew Gordon and {Bohlke-Schneider}, Michael and Wang, Yuyang},
  year = {2024},
  month = mar,
  number = {arXiv:2403.07815},
  eprint = {2403.07815},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2403.07815},
  urldate = {2024-03-20},
  abstract = {We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained specifically on them. Our results demonstrate that Chronos models can leverage time series data from diverse domains to improve zero-shot accuracy on unseen forecasting tasks, positioning pretrained models as a viable tool to greatly simplify forecasting pipelines.},
  archiveprefix = {arXiv},
  note = {Ansari24chronosTSlangLearn
\par
One-short TS forecast, somehow learning teries langauge. ~I think this could be called a foundation model. ~Has code.
\par
\begin{itemize}

\item tutorial: (Ansari et al., 2024)
\item Inference code and model checkpoints available at https://github.com/amazon-science/chronos-forecasting

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ansari24chronosTSlangLearn.pdf}
}

@misc{Bury24collinearityHarmulARFS,
  title = {{{ARFS}} - {{Is}} Collinearity Harmul? {\textbar} {{ARFS Documentation}}},
  shorttitle = {{{ARFS}} - {{Is}} Collinearity Harmul?},
  author = {Bury, Thomas},
  year = {2024},
  month = mar,
  url = {https://arfs.readthedocs.io/en/latest/notebooks/issue_collinearity.html},
  urldate = {2024-03-20},
  abstract = {As I'll show, collinearity is harmful. All relevant feature selection methods are not 100\% robust when the data have a strong correlation structure. Many (most) feature selection schemes suffer from collinearity.},
  langid = {english},
  note = {Bury24collinearityHarmulARFS
\par
Collinearity messes up all methods tested in the ARFS package. ~That includes Boruta, SHAP-based techniques, and GrootCV, all of which I'm interested in.}
}

@article{Schaeffer23emergentLLMmirage,
  title = {Are {{Emergent Abilities}} of {{Large Language Models}} a {{Mirage}}?},
  author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {55565--55581},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/adc98a266f45005c403b8311ca7e8bd7-Abstract-Conference.html},
  urldate = {2024-03-21},
  langid = {english},
  note = {Schaeffer23emergentLLMmirage
\par
Arguments that LLM skills aren't emergent, but a function of chunky metrics (I think).
\par
Need to read.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Schaeffer23emergentLLMmirage.pdf}
}

@article{Davalos23AIDoubleEdgedSword,
  title = {{{AI}} Is a {{Double-Edged Sword}} for {{Climate Change}}},
  author = {Davalos, Jackie and Lanxon, Nate},
  year = {2023},
  month = dec,
  journal = {Bloomberg.com},
  url = {https://www.bloomberg.com/news/articles/2023-12-14/ai-is-a-double-edged-sword-for-climate-change},
  urldate = {2024-03-21},
  abstract = {AI companies say the technology can be used to help climate change, but it is also contributing to the problem},
  langid = {english},
  note = {Davalos23AIDoubleEdgedSword
\par
AI will consume as much electricity as the Netherlands by 2027.
\par
Google's making a tool, ``lever,'' that helps researchers reduce emissions, but what they mention is just switching to greener sources, not reducing the power consumption.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Davalos23AIDoubleEdgedSword.pdf}
}

@misc{Ropek24altmanAIneedsNuclear,
  title = {Sam {{Altman}} at {{Davos}}: {{AI Needs}} to {{Go Nuclear}}},
  shorttitle = {Sam {{Altman}} at {{Davos}}},
  author = {Ropek, Lucas},
  year = {2024},
  month = jan,
  journal = {Gizmodo},
  url = {https://gizmodo.com/sam-altman-openai-davos-wef-ai-needs-to-go-nuclear-1851173201},
  urldate = {2024-03-21},
  abstract = {At this year's World Economic Forum, the AI guru told listeners that his industry needs more power, both figuratively and literally.},
  langid = {english},
  note = {Ropek24altmanAIneedsNuclear
\par
For the title! ~},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ropek24altmanAIneedsNuclear.pdf}
}

@misc{Bentsen23transformersTimeSeriesData,
  title = {Transformers for {{Time-Series Data}}},
  author = {Bentsen, Lars {\O}degaard},
  year = {2023},
  month = dec,
  journal = {BearingPoint Data, Analytics \& AI},
  url = {https://medium.com/bearingpoint-data-analytics-ai/transformers-for-time-series-data-3fadff9f07d8},
  urldate = {2024-03-21},
  abstract = {In this article, we aim to provide a brief overview of some interesting work that adapts the Transformer architecture to facilitate time series data (and in particular forecasting) or that aims to reduce the model complexity (},
  langid = {english},
  keywords = {obsLitNote},
  note = {Bentsen23transformersTimeSeriesData
\par
A collection of transformer structures for time series, generally different than for language and image processing.
\par
Transformers can learn longer sequences than LSTM, but at much higher computational cost: O(L{\textasciicircum}2) vs. O(L). ~
\par
These architectures are about reducing that complexity. ~Among others, one technique is to use traditional forecasting techniques -- ``huge improvements'' with trend, periodic, and frequency domain techniques.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bentsen23transformersTimeSeriesData.html}
}

@article{Herrman24chatGPTGeminiDoomed,
  title = {{{ChatGPT}} and {{Google Gemini Are Both Doomed}}},
  author = {Herrman, John},
  year = {2024},
  month = mar,
  journal = {Intelligencer},
  url = {https://nymag.com/intelligencer/article/chatgpt-and-google-gemini-are-both-doomed.html},
  urldate = {2024-03-21},
  abstract = {All-purpose chatbots have an impossible job.},
  langid = {english},
  note = {Herrman24chatGPTGeminiDoomed
\par
Limiting problem scope limits AI craziness.
\par
Some qood quotes
\par
\begin{quotation}

\par
One solution to this problem is~to deploy AI to the public in the form of more specialized applications about which~\_most\_~people basically agree ---~to ``scope'' it, in other words. A good customer-service chatbot is polite, perhaps a bit stubborn, and refuses to talk about anything but the matter at hand.~It's clear when they're doing something they're not supposed to do:
\par
\end{quotation}

\begin{quotation}

\par
This is how most large-language-model--based products are actually being developed and used in 2024: by start-ups with a specific task and type of customer in mind; by companies like Google and Microsoft in the form of purpose-built products (meeting transcribers, translation tools, coding assistants, image generators used instead of background stock images); in the form of better-defined personae, as in the case of OpenAI's tailor-made GPTs, through which users can basically assign characters themselves. Specialized AI represents real products and an aggregate situation in which questions about AI bias, training data, and ideology at least feel less salient to customers and users. The ``characters'' performed by scoped, purpose-built AI are performing joblike roles with employeelike personae. They don't need to have an opinion on Hitler or Elon Musk because the customers aren't looking for one, and the bosses won't let it have one, and that makes perfect sense to everyone in the contexts in which they're being deployed. They're expected to be careful about what they say and to avoid subjects that aren't germane to the task for which they've been ``hired.''
\par
In contrast, general-purpose public chatbots like ChatGPT and Gemini are practically begging to be asked about Hitler
\par
\end{quotation}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Herrman24chatGPTGeminiDoomed.html}
}

@misc{Roque24TimeGPTVsTiDEZeroShot,
  title = {{{TimeGPT}} vs {{TiDE}}: {{Is Zero-Shot Inference}} the {{Future}} of {{Forecasting}} or {{Just Hype}}?},
  shorttitle = {{{TimeGPT}} vs {{TiDE}}},
  author = {Roque, Lu{\'i}s},
  year = {2024},
  month = mar,
  journal = {Medium},
  url = {https://towardsdatascience.com/timegpt-vs-tide-is-zero-shot-inference-the-future-of-forecasting-or-just-hype-9063bdbe0b76},
  urldate = {2024-03-21},
  abstract = {Foundational models: A comprehensive comparison of TimeGPT and TiDE in time series forecasting},
  langid = {english},
  keywords = {obsLitNote},
  note = {Roque24TimeGPTVsTiDEZeroShot
\par
More simpler autoencoder forecaster, TiDE, outperforms TimeGPT.
\par
Need to read.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Roque24TimeGPTVsTiDEZeroShot.pdf}
}

@misc{Keen23openSourceLLMs,
  title = {Should {{You Use Open Source Large Language Models}}?},
  author = {Keen, Martin},
  year = {2023},
  month = nov,
  url = {https://www.youtube.com/watch?v=y9k-U9AuDeM},
  urldate = {2024-03-21},
  abstract = {Want to experiment with foundation models? Explore our interactive demo for watsonx.ai  {$\rightarrow$} https://ibm.biz/Bdvu3f To dive deeper get the guide to choosing the right AI foundation model {$\rightarrow$} https://ibm.biz/Bdvu3H Large Language Models (LLMs) can be proprietary to a given company, or open source and free for anyone to access and modify. While proprietary LLMs are often larger, the benefits of transparency, fine-tuning, and community contributions make open source an attractive alternative. Both proprietary and open source LLMs share risks, including inaccuracies, bias, and security concerns. In this video, Master Inventor Martin Keen covers the tradeoffs so you can make an informed decision of which option is best for you. AI news moves fast. Sign up for a monthly newsletter for AI updates from IBM. {$\rightarrow$} https://ibm.biz/Bdvu3M},
  keywords = {obsLitNote},
  note = {Keen23openSourceLLMs
\par
I've TEMPORARILY put my literature notes for this in Obsidian AI talk folder, \href{obsidian://advanced-uri?vault=Obsidian\%20Share\%20Vault&filepath=work\%252FGenerative\%2520AI\%2520talk\%252F\%2540IBM23openSourceLLMs.md}{here}
\par
Remove this note when I've settled on how ot merge zotero and obsidian.
\par
Hugging face security risks in (Ng, 2024)}
}

@misc{Dugam22CollinearityFeatures,
  title = {The {{Collinearity}} of {{Features}}},
  author = {Dugam, Akash},
  year = {2022},
  month = aug,
  journal = {Medium},
  url = {https://dugamakash.medium.com/the-collinearity-of-features-9bc34a3e3efd},
  urldate = {2024-03-22},
  abstract = {It's wise to understand the stuff related to `collinearity' or `multicollinearity' to excel in the field of data science. Though both of{\dots}},
  langid = {english},
  note = {Dugam22CollinearityFeatures
\par
\begin{itemize}

\item collinearity causes problems
\item can assess w/ corr matrix, but this is complicated
\item 
\par
simpler way to ID colinearity:
\par
\begin{itemize}

\item Variance Inflation Factor: VIF
\item each var gets VIF = 1-R{\textasciicircum}2 of prediction by other variables

\end{itemize}

\end{itemize}

\par
What to do about it
\par
\begin{itemize}

\item 
\par
iteratively eliminate variables w/ high VIF
\par
\begin{itemize}

\item problem: which is the best set is combinatoric

\end{itemize}

\item combine variables e.g. sum ones where sum makes sense
\item run PCA: they they're guaranteed to be have VIF=1 (min possible value)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Dugam22CollinearityFeatures.html}
}

@misc{Ng24BrobotsAgentsRisksPolitics,
  type = {Newsletter},
  title = {Robots {{Talk Back}}, {{AI Security Risks}}, {{Political Deepfakes}}, and More},
  author = {Ng, Andrew},
  year = {2024},
  month = mar,
  journal = {The Batch {\textbar} DeepLearning.AI {\textbar} AI News \& Insights},
  url = {https://www.deeplearning.ai/the-batch/issue-241/},
  urldate = {2024-03-22},
  abstract = {I think AI agent workflows will drive massive AI progress this year --- perhaps even more than the next generation of foundation models. This is an important...},
  langid = {english},
  note = {Ng24BrobotsAgentsRisksPolitics
\par
\section{Zero shot v. Agents}

\begin{itemize}

\item LLMs mostly used in oneshot: like printing 1st draft w/ never hitting delete key or proof reading
\item Agents can iterate on documents or code
\item GPT-3.5 agent better than GPT-4 zero-shot

\end{itemize}

\par
\begin{itemize}

\item Design patterns for building agents (see list in article)

\end{itemize}

\section{Robot Foundation Model (RFM-1)}

\begin{itemize}

\item 8 B params
\item robot responds coversationally: asking and answering questions
\item given verbal instruction, generates the (motion?) tokens to pick up e.g. apples
\item if the robot's goal is to emtpy a bin, then if the tokens generates predict ~an image of an empty bin, then it uses those tokens to empty the bin (as I understand it).
\item I stored the \href{obsidian://advanced-uri?vault=Obsidian\%20Share\%20Vault&filepath=work\%252FGenerative\%2520AI\%2520talk\%252FGraphics\%2520to\%2520Borrow.md&block=la5kij}{GIF of robot getting stumped, asking questions and responding to instructions} in obsidian
\item More details on RFM-1 in (Metz, 2024)
\item Similar robot idea but in video, human forces help: (Chu, 2024)

\end{itemize}

\section{LLM security risks}

\begin{itemize}

\item 
\par
found 100 ``worrisome'' models in Hugging Face (``Should You Use Open Source Large Language Models?'', 2023)
\par
\begin{itemize}

\item 50\% could hijack device objects
\item 20\% could open a reverse shell
\item one used pickle to insert code inton PyTorch (95\% LLms based on PyTorch)
\item one used ``safer pickle'', Safetensors, to send pull requests, excute arbitrary code, view stuff

\end{itemize}

\item Hugging face: download at your own risk
\item found 6000 API tokens from the biggies: Google, MS, Meta
\item Hackers add risk to Hugging Face: (Claburn, 2024)

\end{itemize}

\section{Politics}

\begin{itemize}

\item headline: ``Deepfakes become politics as usual''
\item ~``Manipulating voters by AI is not being considered a sin by any party,'' an anonymous Indian political consultant told~\emph{Al Jazeera}. ``It is just a part of the campaign~strategy.''

\end{itemize}

\section{Multiple Cheap pretrained models cheaper than good ones}

\begin{itemize}

\item 12 commercial LLMs of varying quality
\item one 3000X cheaper per token
\item tuned something to chooses which of 3 LLMs was ``good enough'' for a given level accuracy.
\item ``FrugalGPT'' saved 98.3, 73.3 and 59.2\% of cost compared to most expensive model

\end{itemize}

\section{LLM course}

\begin{itemize}

\item new short course ``Efficiently Serving LLMs,''
\item many things, including LoRA model tuning

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ng24BrobotsAgentsRisksPolitics.html}
}

@misc{Brubaker24ChainofThoughtLLM,
  title = {How {{Chain-of-Thought Reasoning Helps Neural Networks Compute}}},
  author = {Brubaker, Ben},
  year = {2024},
  month = mar,
  journal = {Quanta Magazine},
  url = {https://www.quantamagazine.org/how-chain-of-thought-reasoning-helps-neural-networks-compute-20240321/},
  urldate = {2024-03-25},
  abstract = {Large language models do better at solving problems when they show their work. Researchers are beginning to understand why.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Brubaker24ChainofThoughtLLM
\par
Maybe this prompt works?
\par
(Dina Genkina, 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Brubaker24ChainofThoughtLLM.pdf}
}

@misc{Se24canMambaBeatTransformer,
  title = {What Is {{Mamba}} and Can It Beat {{Transformers}}?},
  author = {Se, Ksenia},
  year = {2024},
  month = mar,
  journal = {The Turing Post},
  url = {https://www.turingpost.com/p/fod46},
  urldate = {2024-03-26},
  abstract = {+ new research extending and improving the Mamba architecture and the best curated list of the freshest ML news and papers},
  langid = {english},
  note = {Se24canMambaBeatTransformer},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Se24canMambaBeatTransformer.pdf}
}

@misc{Lazy24mambaTransformerAlternative,
  title = {Mamba ({{Transformer Alternative}}): {{The Future}} of {{LLMs}} and {{ChatGPT}}?},
  shorttitle = {Mamba ({{Transformer Alternative}})},
  author = {{Lazy}},
  year = {2024},
  month = jan,
  journal = {Lazy Programmer},
  url = {https://lazyprogrammer.me/mamba-transformer-alternative-the-future-of-llms-and-chatgpt/},
  urldate = {2024-03-26},
  abstract = {The article discusses the emergence of a non-attention architecture for language modeling, in particular Mamba, which has shown promising results in experimental tests. Mamba is an example of a state-space model (SSM). But what is a state-space model? State-Space Models~(SSMs) State-space models (SSMs) are a class of mathematical models used to describe the evolution of [{\dots}]},
  note = {Lazy24mambaTransformerAlternative
\par
Good images, some might be animated in live page.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lazy24mambaTransformerAlternative.pdf}
}

@misc{Grootendorst24VisualGuideMamba,
  type = {Substack Newsletter},
  title = {A {{Visual Guide}} to {{Mamba}} and {{State Space Models}}},
  author = {Grootendorst, Maarten},
  year = {2024},
  month = feb,
  journal = {Exploring Language Models},
  url = {https://maartengrootendorst.substack.com/p/a-visual-guide-to-mamba-and-state},
  urldate = {2024-03-26},
  abstract = {Exploring Mamba, a State Space Model, as an alternative to Transformers for Language Modeling. With more than 50 illustrations, expect an intuitive guide.},
  note = {Grootendorst24VisualGuideMamba
\par
Good pictures},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Grootendorst24VisualGuideMamba.pdf}
}

@misc{Albanie23MambaReplaceTransformers,
  title = {Mamba - a Replacement for {{Transformers}}?},
  author = {Albanie, Samuel},
  year = {2023},
  month = dec,
  url = {https://www.youtube.com/watch?app=desktop&v=ouF-H35atOY},
  urldate = {2024-03-26},
  abstract = {Mamba is a new neural network architecture proposed by Albert Gu and Tri Dao. Timestamps: 00:00 - Mamba - a replacement for Transformers? 00:19 - The Long Range Arena benchmark 01:20 - Legendre Memory Units 02:07 - HiPPO: Recurrent Memory with Optimal Polynomial Projections 02:38 - Combining Recurrent, Convolutional and Continuous-time Models with Linear State-Space Layers 03:28 - Efficiently Modeling Long Sequences with Structured State Spaces (S4) 05:46 - The Annotated S4 06:13 - Mamba: Linear-Time Sequence Modeling with Selective State Spaces 07:42 - Motivation: Why selection is needed 09:59 - S5 12:00 - Empirical evaluation The paper can be found here: https://arxiv.org/abs/2312.00752 Topics: \#mamba \#foundation References for papers mentioned in the video can be found at https://samuelalbanie.com/digests/202... For related content: - Twitter: ~~/~samuelalbanie~~ - personal webpage: https://samuelalbanie.com/ - YouTube: ~~~/~@samuelalbanie1},
  note = {Albanie23MambaReplaceTransformers
\par
From right after Mamba was released, but only 16 mins long, references orginal paper: (Gu and Dao, 2023)}
}

@misc{Gu23MambaLinearTimeSequence,
  title = {Mamba: {{Linear-Time Sequence Modeling}} with {{Selective State Spaces}}},
  shorttitle = {Mamba},
  author = {Gu, Albert and Dao, Tri},
  year = {2023},
  month = dec,
  number = {arXiv:2312.00752},
  eprint = {2312.00752},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.00752},
  url = {http://arxiv.org/abs/2312.00752},
  urldate = {2024-03-26},
  abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  archiveprefix = {arXiv},
  note = {Gu23MambaLinearTimeSequence
\par
original paper
\par
brief tutorial: (``Mamba - a replacement for Transformers?'', 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gu23MambaLinearTimeSequence.pdf}
}

@misc{Instantinopaul24MambaVsTransformers,
  type = {Reddit {{Post}}},
  title = {[{{D}}] {{So}}, {{Mamba}} vs. {{Transformers}}... Is the Hype Real?},
  author = {Instantinopaul},
  year = {2024},
  month = jan,
  journal = {r/MachineLearning},
  url = {www.reddit.com/r/MachineLearning/comments/190q1vb/d_so_mamba_vs_transformers_is_the_hype_real/},
  urldate = {2024-03-26},
  note = {Instantinopaul24MambaVsTransformers
\par
This might be net folklore but it's said that:
\par
\begin{itemize}

\item Mamba is nonlinear in features but linear in time
\item linear in time w/ orthogonal basis function (Legendre polynomials), said to be familiar to electrical and control engineers!

\end{itemize}

\par
Neither copilot and gemini accept that, say I should read the paper: (Gu and Dao, 2023)},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Instantinopaul24MambaVsTransformers.pdf;C\:\\Users\\scott\\Zotero\\storage\\WBRCXY8P\\d_so_mamba_vs_transformers_is_the_hype_real.html}
}

@misc{AIDIGITALNEWS23MambaSeqModelTransf,
  title = {Mamba: {{Redefining Sequence Modeling}} and {{Outforming Transformers Architecture}} {\textbar} {{AI}} Digitalnews},
  shorttitle = {Mamba},
  author = {{AIDIGITALNEWS}},
  year = {2023},
  month = dec,
  journal = {AIDIGITALNEWS},
  url = {https://aidigitalnews.com/ai/mamba-redefining-sequence-modeling-and-outforming-transformers-architecture/},
  urldate = {2024-03-26},
  langid = {american},
  note = {AIDIGITALNEWS23MambaSeqModelTransf
\par
A simpler Mamba tutorial?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\AIDIGITALNEWS23MambaSeqModelTransf.pdf}
}

@misc{Jelassi24TransformBetterSSMcopy,
  title = {Repeat {{After Me}}: {{Transformers}} Are {{Better}} than {{State Space Models}} at {{Copying}}},
  shorttitle = {Repeat {{After Me}}},
  author = {Jelassi, Samy and Brandfonbrener, David and Kakade, Sham M. and Malach, Eran},
  year = {2024},
  month = feb,
  number = {arXiv:2402.01032},
  eprint = {2402.01032},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.01032},
  url = {http://arxiv.org/abs/2402.01032},
  urldate = {2024-03-26},
  abstract = {Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. Taken together, these results suggest a fundamental gap between transformers and GSSMs on tasks of practical interest.},
  archiveprefix = {arXiv},
  note = {Jelassi24TransformBetterSSMcopy
\par
\begin{itemize}

\item Autoritative? comparison of SSM (mamba, (Gu and Dao, 2023)) strengths and weaknesses?

\end{itemize}

\begin{itemize}

\item Website article w/ animations also attached. ~From: \href{https://www.harvard.edu/kempner-institute/2024/02/02/repeat-after-me-transformers-are-better-than-state-space-models-at-copying-2/}{Harvard Kempner Institute blog}

\end{itemize}},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Jelassi24TransformBetterSSMcopy.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Jelassi24TransformBetterSSMcopy.html}
}

@misc{Heaven23genAIjustPhase,
  title = {{{DeepMind}}'s Cofounder: {{Generative AI}} Is Just a Phase. {{What}}'s next Is Interactive {{AI}}.},
  shorttitle = {{{DeepMind}}'s Cofounder},
  author = {Heaven, Will Douglas},
  year = {2023},
  month = sep,
  journal = {MIT Technology Review},
  url = {https://www.technologyreview.com/2023/09/15/1079624/deepmind-inflection-generative-ai-whats-next-mustafa-suleyman/},
  urldate = {2024-03-26},
  abstract = {``This is a profound moment in the history of technology,'' says Mustafa Suleyman.},
  langid = {english},
  note = {Heaven23genAIjustPhase
\par
Great headline for gen AI talk
\par
Big AI says interactive AI is next thing
\par
\begin{itemize}

\item mabye this describes what he means: (Zia, 2023)
\item maybe this robot is an example: (Ng, 2024)
\item he just started a job at Microsoft:(Weise and Metz, 2024)

\end{itemize}

\par
He also wants to ditch genAI: (Wodecki, 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Heaven23genAIjustPhase.pdf}
}

@article{Weise24MicrosoftHiresDeepMind,
  title = {Microsoft {{Hires DeepMind Co-Founder}} to {{Run Consumer A}}.{{I}}.},
  author = {Weise, Karen and Metz, Cade},
  year = {2024},
  month = mar,
  journal = {The New York Times},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2024/03/19/technology/mustafa-suleyman-google-gemini.html},
  urldate = {2024-03-26},
  abstract = {Mustafa Suleyman is leaving a start-up called Inflection to take the senior position with Microsoft.},
  chapter = {Technology},
  langid = {american},
  note = {Weise24MicrosoftHiresDeepMind
\par
guy who says gen AI is a phase: (Heaven, 2023)}
}

@misc{Wodecki24yannLeCunDitchGenAI,
  title = {Meta's {{Yann LeCun Wants}} to {{Ditch Generative AI}}},
  author = {Wodecki, Ben},
  year = {2024},
  month = feb,
  url = {https://aibusiness.com/nlp/meta-s-yann-lecun-wants-to-ditch-generative-ai},
  urldate = {2024-03-26},
  abstract = {Meta's chief AI scientist proposes an alternative view of AGI - advanced machine intelligence.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Wodecki24yannLeCunDitchGenAI
\par
So does this guy: (Heaven, 2023)
\par
\begin{itemize}

\item doesn't like LLMs
\item doesn't like autoregressive models forecasting future behavior (gulp)
\item thinks JEPA is the way forward, which do more than text (``predicting text is simple)'' but it's decades away.
\item machine IQ {$>$} human IQ ``not in the next 10 years or longer''

\end{itemize}

\par
Some choice quotes:
\par
\begin{itemize}

\item ``The future of AI, I tell you, is non-generative. It works for text, doesn't work for anything else.''
\item Predicting text is simple,'' 
\item No AI system, no intelligent system is general including humans. 
\item Any 17-year-old can learn to drive a car with 20 hours of practice. We still don't have Level 5 autonomous cars
\item 
\par
``Most of what we know {\dots} doesn't come
\par
from language. ``
\par
\item 
\par
``a child has seen 50 times more data than the LLMs that are trained on the totality of
\par
all text that is perfectly available,''
\par
\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wodecki24yannLeCunDitchGenAI.pdf}
}

@misc{Zia23InteractiveAInextPhase,
  title = {How {{Interactive AI}} Is the {{Next Phase}} of {{Generative AI}}},
  author = {Zia, Dr Tehseen},
  year = {2023},
  month = oct,
  journal = {Techopedia},
  url = {https://www.techopedia.com/how-interactive-ai-is-the-next-phase-of-generative-ai},
  urldate = {2024-03-26},
  abstract = {Join us as we explore the transition from Generative AI to Interactive AI, coined last month by Mustafa Suleyman, co-founder of DeepMind.},
  langid = {american},
  note = {Zia23InteractiveAInextPhase
\par
Maybe this is what is meant by this guy: (Heaven, 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zia23InteractiveAInextPhase.pdf}
}

@misc{Tufano24AutoDevAutomatedAIDriven,
  title = {{{AutoDev}}: {{Automated AI-Driven Development}}},
  shorttitle = {{{AutoDev}}},
  author = {Tufano, Michele and Agarwal, Anisha and Jang, Jinu and Moghaddam, Roshanak Zilouchian and Sundaresan, Neel},
  year = {2024},
  month = mar,
  number = {arXiv:2403.08299},
  eprint = {2403.08299},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.08299},
  url = {http://arxiv.org/abs/2403.08299},
  urldate = {2024-03-26},
  abstract = {The landscape of software development has witnessed a paradigm shift with the advent of AI-powered assistants, exemplified by GitHub Copilot. However, existing solutions are not leveraging all the potential capabilities available in an IDE such as building, testing, executing code, git operations, etc. Therefore, they are constrained by their limited capabilities, primarily focusing on suggesting code snippets and file manipulation within a chat-based interface. To fill this gap, we present AutoDev, a fully automated AI-driven software development framework, designed for autonomous planning and execution of intricate software engineering tasks. AutoDev enables users to define complex software engineering objectives, which are assigned to AutoDev's autonomous AI Agents to achieve. These AI agents can perform diverse operations on a codebase, including file editing, retrieval, build processes, execution, testing, and git operations. They also have access to files, compiler output, build and testing logs, static analysis tools, and more. This enables the AI Agents to execute tasks in a fully automated manner with a comprehensive understanding of the contextual information required. Furthermore, AutoDev establishes a secure development environment by confining all operations within Docker containers. This framework incorporates guardrails to ensure user privacy and file security, allowing users to define specific permitted or restricted commands and operations within AutoDev. In our evaluation, we tested AutoDev on the HumanEval dataset, obtaining promising results with 91.5\% and 87.8\% of Pass@1 for code generation and test generation respectively, demonstrating its effectiveness in automating software engineering tasks while maintaining a secure and user-controlled development environment.},
  archiveprefix = {arXiv},
  note = {Tufano24AutoDevAutomatedAIDriven
\par
\href{https://www.youtube.com/watch?v=goVq3gRvTA4}{short video}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Tufano24AutoDevAutomatedAIDriven.pdf}
}

@misc{Chu24householdRobotCommonSense,
  title = {Engineering Household Robots to Have a Little Common Sense},
  author = {Chu, Jennifer},
  year = {2024},
  month = mar,
  journal = {MIT News {\textbar} Massachusetts Institute of Technology},
  url = {https://news.mit.edu/2024/engineering-household-robots-have-little-common-sense-0325},
  urldate = {2024-03-26},
  abstract = {MIT engineers aim to give robots a bit of common sense when faced with situations that push them off their trained path, so they can self-correct after missteps and carry on with their chores. The team's method connects robot motion data with the common sense knowledge of large language models, or LLMs.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Chu24householdRobotCommonSense
\par
Another example of robots merging LLMs, physical coordinate systems and human assistance, although that's not with voice, as in (Ng, 2024)
\par
Paper linked to by this article (Wang et al., 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chu24householdRobotCommonSense.pdf}
}

@inproceedings{Wang23GroundingLangPlanCtrFact,
  title = {Grounding {{Language Plans}} in {{Demonstrations Through Counter-Factual Perturbations}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Wang, Yanwei and Wang, Tsun-Hsuan and Mao, Jiayuan and Hagenow, Michael and Shah, Julie},
  year = {2023},
  url = {https://openreview.net/forum?id=qoHeuRAcSl},
  urldate = {2024-03-27},
  keywords = {obsLitNote},
  note = {Wang23GroundingLangPlanCtrFact
\par
Linked to by (Chu, 2024), which covers a robot learing from human teaching. ~Maybe this explains more how a LLM can do spatial stuff, and plan.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wang23GroundingLangPlanCtrFact.pdf}
}

@misc{Morrison24ClaudeTopGPT4,
  title = {Claude Takes the Top Spot in {{AI}} Chatbot Ranking --- Finally Knocking {{GPT-4}} down to Second Place},
  author = {Morrison, Ryan},
  year = {2024},
  month = mar,
  journal = {Tom's Guide},
  url = {https://www.tomsguide.com/ai/claude-takes-the-top-spot-in-ai-chatbot-ranking-finally-knocking-gpt-4-down-to-second-place},
  urldate = {2024-03-27},
  abstract = {All three Claude 3 versions are in the top ten},
  langid = {english},
  keywords = {obsLitNote},
  note = {Morrison24ClaudeTopGPT4
\par
Describes another leaderboard based on human voting. ~Chinese and opensource models are inching up in the rankings.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Morrison24ClaudeTopGPT4.pdf}
}

@misc{Fried24AIChatbotLetdown,
  title = {{{AI}} Chatbot Letdown: {{Hype}} Hits Rocky Reality},
  shorttitle = {{{AI}} Chatbot Letdown},
  author = {Fried, Ina},
  year = {2024},
  month = mar,
  journal = {Axios},
  url = {https://www.axios.com/2024/03/27/ai-chatbot-letdown-hype-reality},
  urldate = {2024-03-27},
  abstract = {The generative AI iindustry hits a "trough of disillusionment" as early awe dims and tough problems mount.},
  langid = {english},
  note = {Fried24AIChatbotLetdown
\par
All the genAI problems. ~I'm saving this one for the headline and the GIF.
\par
\subsubsection{AI helping coding might be considered a success:}

\par
Says ``outside of a few areas such as \textbf{coding}, companies have found generative AI isn't the panacea they once imagined.''},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Fried24AIChatbotLetdown.html}
}

@misc{TechWithTim24pythonRAGtutorial,
  title = {{{ADVANCED Python AI Agent Tutorial}} - {{Using RAG}}},
  author = {{Tech With Tim}},
  year = {2024},
  month = feb,
  url = {https://www.youtube.com/watch?v=ul0QsodYct4},
  urldate = {2024-03-29},
  abstract = {In this video, I will be showing you how to create an artificial intelligence agent that will be able to use all of the tools that we provide it with. That's right, we will create and AI agent \& give it various tools for it to use selectively based on the task at hand. Note: This video refers to llamaindex version 0.9. On Feb 12th, 2024 llamaindex version 0.10 was released which is mostly the same but has some added functionality :)  Video Resources  Check out Llama Index: https://www.llamaindex.ai/ Llama Index Docs: https://docs.llamaindex.ai/en/stable/ Population Dataset: https://www.kaggle.com/datasets/joebe... Llama Hub: https://llamahub.ai/ Code in this video: https://github.com/techwithtim/Python...  Timestamps  00:00 {\textbar} Overview 00:27 {\textbar} Project Demos 03:54 {\textbar} Understanding Agents \& RAG 06:29 {\textbar} Installation/Setup 09:00 {\textbar} Data Sources 13:18 {\textbar} Querying Pandas Data 19:17 {\textbar} Building Agent Capabilities 29:18 {\textbar} Querying Unstructured Data 39:38 {\textbar} Next Steps Hashtags \#PythonAI \#TechWithTim \#AICoding},
  keywords = {obsLitNote},
  note = {TechWithTim24pythonRAGtutorial
\par
Actually builds a RAG project. ~I should try it before the talk. It's not langchain but probably something that could be, given the number of langchain projects out there.
\par
Watch this first? ~(``What is LangChain?'', 2024)}
}

@misc{Hernandez24linRelatTransfrmrLLM,
  title = {Linearity of {{Relation Decoding}} in {{Transformer Language Models}}},
  author = {Hernandez, Evan and Sharma, Arnab Sen and Haklay, Tal and Meng, Kevin and Wattenberg, Martin and Andreas, Jacob and Belinkov, Yonatan and Bau, David},
  year = {2024},
  month = feb,
  number = {arXiv:2308.09124},
  eprint = {2308.09124},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2308.09124},
  urldate = {2024-03-30},
  abstract = {Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in transformer LMs.},
  archiveprefix = {arXiv},
  note = {Hernandez24linRelatTransfrmrLLM
\par
LLMs based on transformers encode some relationship (x is the capitol of y) w/ a ``linear'' function sometimes.
\par
News article on this paper: (Hernandez et al., 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hernandez24LinearityRelationDecoding.pdf}
}

@misc{Zewe24linRelatTransfrmrLLM,
  title = {Large Language Models Use a Surprisingly Simple Mechanism to Retrieve Some Stored Knowledge},
  author = {Zewe, Adam},
  year = {2024},
  month = mar,
  journal = {MIT News {\textbar} Massachusetts Institute of Technology},
  url = {https://news.mit.edu/2024/large-language-models-use-surprisingly-simple-mechanism-retrieve-stored-knowledge-0325},
  urldate = {2024-03-30},
  abstract = {Researchers find large language models use a simple mechanism to retrieve stored knowledge when they respond to a user prompt. These mechanisms can be leveraged to see what the model knows about different subjects and possibly to correct false information it has stored.},
  langid = {english},
  note = {Zewe 24linRelatTransfrmrLLM
\par
LLMs based on transformers encode some relationship (x is the capitol of y) w/ a ``linear'' function sometimes.
\par
Original paper: (Hernandez et al., 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hernandez24linRelatTransfrmrLLM.pdf}
}

@misc{Leswing24GenerativeAIFOMO,
  title = {Generative {{AI}} '{{FOMO}}' Is Driving Tech Heavyweights to Invest Billions of Dollars in Startups},
  author = {Leswing, Kif, Hayden Field},
  year = {2024},
  month = mar,
  journal = {CNBC},
  url = {https://www.cnbc.com/2024/03/30/fomo-drives-tech-heavyweights-to-invest-billions-in-generative-ai-.html},
  urldate = {2024-03-31},
  abstract = {Tech giants are backing artificial intelligence startups, often investing huge sums in exchange for access to the technology.},
  langid = {english},
  note = {Leswing24GenerativeAIFOMO
\par
About how much \$ is being invested in Gen AI},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Leswing24GenerativeAIFOMO.pdf}
}

@misc{Claburn24AIBotsHallucinateMalware,
  title = {{{AI}} Bots Hallucinate Software Packages and Devs Download Them},
  author = {Claburn, Thomas},
  year = {2024},
  month = mar,
  journal = {The Register},
  url = {https://www.theregister.com/2024/03/28/ai_bots_hallucinate_software_packages/},
  urldate = {2024-03-31},
  abstract = {Simply look out for libraries imagined by ML and make them real, with actual malicious code. No wait, don't do that},
  langid = {english},
  note = {Claburn24AIBotsHallucinateMalware
\par
Good headline.
\par
More on hugging face: (Ng, 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Claburn24AIBotsHallucinateMalware.pdf}
}

@misc{ZTEST_Ng24RobotsTalkBack,
  type = {Newsletter},
  title = {{{ZTEST Robots Back}}, {{AI Security Risks}}, {{Political Deepfakes}}, and More},
  author = {Ng, Andrew},
  year = {2024},
  month = mar,
  journal = {The Batch {\textbar} DeepLearning.AI {\textbar} AI News \& Insights},
  url = {https://www.deeplearning.ai/the-batch/issue-241/},
  urldate = {2024-03-22},
  abstract = {I think AI agent workflows will drive massive AI progress this year --- perhaps even more than the next generation of foundation models. This is an important...},
  langid = {english},
  keywords = {genA,Startups},
  note = {Zotero Note: Z2TEST\_Ng24RobotsTalkBack
\par
\section{Here's a new heading of this note}

\par
some text inside this new heading
\par
\section{Zero shot v. Agents}

\begin{itemize}

\item LLMs mostly used in oneshot: like printing 1st draft w/ never hitting delete key or proof reading
\item Agents can iterate on documents or code
\item GPT-3.5 agent better than GPT-4 zero-shot

\end{itemize}

\par
\begin{itemize}

\item Design patterns for building agents (see list in article)

\end{itemize}

\section{Robot Foundation Model (RFM-1)}

\begin{itemize}

\item 8 B params
\item robot responds coversationally: asking and answering questions
\item given verbal instruction, generates the (motion?) tokens to pick up e.g. apples
\item if the robot's goal is to emtpy a bin, then if the tokens generates predict ~an image of an empty bin, then it uses those tokens to empty the bin (as I understand it).
\item I stored the \href{obsidian://advanced-uri?vault=Obsidian\%20Share\%20Vault&filepath=work\%252FGenerative\%2520AI\%2520talk\%252FGraphics\%2520to\%2520Borrow.md&block=la5kij}{GIF of robot getting stumped, asking questions and responding to instructions} in obsidian
\item More details on RFM-1 in (Metz, 2024)
\item Similar robot idea but in video, human forces help: (Chu, 2024)

\end{itemize}

\section{LLM security risks}

\begin{itemize}

\item 
\par
found 100 ``worrisome'' models in Hugging Face (``Should You Use Open Source Large Language Models?'', 2023)
\par
\begin{itemize}

\item 50\% could hijack device objects
\item 20\% could open a reverse shell
\item one used pickle to insert code inton PyTorch (95\% LLms based on PyTorch)
\item one used ``safer pickle'', Safetensors, to send pull requests, excute arbitrary code, view stuff

\end{itemize}

\item Hugging face: download at your own risk
\item found 6000 API tokens from the biggies: Google, MS, Meta
\item Hackers add risk to Hugging Face: (Claburn, 2024)

\end{itemize}

\section{Politics}

\begin{itemize}

\item headline: ``Deepfakes become politics as usual''
\item ~``Manipulating voters by AI is not being considered a sin by any party,'' an anonymous Indian political consultant told~\emph{Al Jazeera}. ``It is just a part of the campaign~strategy.''

\end{itemize}

\section{Multiple Cheap pretrained models cheaper than good ones}

\begin{itemize}

\item 12 commercial LLMs of varying quality
\item one 3000X cheaper per token
\item tuned something to chooses which of 3 LLMs was ``good enough'' for a given level accuracy.
\item ``FrugalGPT'' saved 98.3, 73.3 and 59.2\% of cost compared to most expensive model

\end{itemize}

\section{LLM course}

\begin{itemize}

\item new short course ``Efficiently Serving LLMs,''
\item many things, including LoRA model tuning

\end{itemize}

\par
ZTEST\_Ng24RobotsTalkBack
\par
\section{Zero shot v. Agents}

\begin{itemize}

\item LLMs mostly used in oneshot: like printing 1st draft w/ never hitting delete key or proof reading
\item Agents can iterate on documents or code
\item GPT-3.5 agent better than GPT-4 zero-shot

\end{itemize}

\par
\begin{itemize}

\item Design patterns for building agents (see list in article)

\end{itemize}

\section{Robot Foundation Model (RFM-1)}

\begin{itemize}

\item 8 B params
\item robot responds coversationally: asking and answering questions
\item given verbal instruction, generates the (motion?) tokens to pick up e.g. apples
\item if the robot's goal is to emtpy a bin, then if the tokens generates predict ~an image of an empty bin, then it uses those tokens to empty the bin (as I understand it).
\item I stored the \href{obsidian://advanced-uri?vault=Obsidian\%20Share\%20Vault&filepath=work\%252FGenerative\%2520AI\%2520talk\%252FGraphics\%2520to\%2520Borrow.md&block=la5kij}{GIF of robot getting stumped, asking questions and responding to instructions} in obsidian
\item More details on RFM-1 in (Metz, 2024)
\item Similar robot idea but in video, human forces help: (Chu, 2024)

\end{itemize}

\section{LLM security risks}

\begin{itemize}

\item 
\par
found 100 ``worrisome'' models in Hugging Face (``Should You Use Open Source Large Language Models?'', 2023)
\par
\begin{itemize}

\item 50\% could hijack device objects
\item 20\% could open a reverse shell
\item one used pickle to insert code inton PyTorch (95\% LLms based on PyTorch)
\item one used ``safer pickle'', Safetensors, to send pull requests, excute arbitrary code, view stuff

\end{itemize}

\item Hugging face: download at your own risk
\item found 6000 API tokens from the biggies: Google, MS, Meta
\item Hackers add risk to Hugging Face: (Claburn, 2024)

\end{itemize}

\section{Politics}

\begin{itemize}

\item headline: ``Deepfakes become politics as usual''
\item ~``Manipulating voters by AI is not being considered a sin by any party,'' an anonymous Indian political consultant told~\emph{Al Jazeera}. ``It is just a part of the campaign~strategy.''

\end{itemize}

\section{Multiple Cheap pretrained models cheaper than good ones}

\begin{itemize}

\item 12 commercial LLMs of varying quality
\item one 3000X cheaper per token
\item tuned something to chooses which of 3 LLMs was ``good enough'' for a given level accuracy.
\item ``FrugalGPT'' saved 98.3, 73.3 and 59.2\% of cost compared to most expensive model

\end{itemize}

\section{LLM course}

\begin{itemize}

\item new short course ``Efficiently Serving LLMs,''
\item many things, including LoRA model tuning

\end{itemize}},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Metz24chatGPTphysWorldST.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\ZTEST_Babic23TailCoRNewSimplea.docx;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Ng24BrobotsAgentsRisksPolitics.html}
}

@misc{Lacy24genAIwhyHallucinate,
  title = {Hallucinations: {{Why AI Makes Stuff Up}}, and {{What}}'s {{Being Done About It}}},
  shorttitle = {Hallucinations},
  author = {Lacy, Lisa},
  year = {2024},
  month = apr,
  journal = {CNET},
  url = {https://www.cnet.com/tech/hallucinations-why-ai-makes-stuff-up-and-whats-being-done-about-it/},
  urldate = {2024-04-01},
  abstract = {There's an important distinction between using AI to generate content and to answer questions.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Lacy24genAIwhyHallucinate
\par
As the title says{\dots}
\par
\begin{itemize}

\item 
\par
Hallucination def
\par
\begin{itemize}

\item 
\par
"synthetically generated data," or "fake data that is statistically indistinguishable from actual factually correct
\par
data."
\par
\item 
\par
generated data is "statistically indistinguishable" from the training data, or that has the same type of generic
\par
characteristics. There's no requirement for it to be "true," Soatto said.
\par
\end{itemize}

\item 
\par
How Gen AI works, and how this makes hallucinations
\par
\begin{itemize}

\item 
\par
adjective example
\par
\begin{itemize}

\item has never seen ``crimson'' in some kind of sentence
\item has seen enough to know that crimson and red are used in similar contexts
\item could eventually say something is crimson

\end{itemize}

\item that's a kind of invention, or ``creativity''. ~ Can actually make that sentence more interesting, novel
\item 
\par
but gen AI doesn't know the difference between that and ``true''

"It generalizes or makes an inference based on what it knows about language,
\par
what it knows about the occurrence of words in di erent contexts," said
\par
Swabha Swayamdipta, assistant professor of computer science at the USC
\par
Viterbi School of Engineering and leader of the Data, Interpretability,
\par
Language and Learning(DILL) lab. "This is why these language models produce
\par
facts which kind of seem plausible but are not quite true because they're not
\par
trained to just produce exactly what they have seen before."

\par
\item can also be result of improper and/or biased training data

\end{itemize}

\par
\item 
\par
"It's just saying, 'Based on this word, I think that the right probability is this next word.' That's what it is.
\par
Just math in the basic sense."
\par
\item 
\par
There's a Hallucination leaderboard: (Hughes and Bae, 2023)
\par
\begin{itemize}

\item by this metric: hallucinations are between 3-27\%
\item my own use is much worse than that

\end{itemize}

\item 
\par
Solutions to hallucinations
\par
\begin{itemize}

\item 
\par
This is none: ``hallucination is their job'' I say.
\par
\begin{itemize}

\item 
\par
``"Hallucination as a property of an AI model is unavoidable, but as a property of
\par
the system that uses the model, it is not only unavoidable, it is very avoidable
\par
and manageable," ~Soatto said.
\par
\end{itemize}

\item don't gen I for factual info
\item 
\par
use RAG
\par
\begin{itemize}

\item you can check yourself
\item systems can check the internet for you (GPT-4, copilot, and gemini (if you ask) do this)

\end{itemize}

\item 
\par
MS guy says solutions are
\par
\begin{itemize}

\item grounding
\item fine-tuning
\item steering
\item \textbf{TODO}: look these up

\end{itemize}

\end{itemize}

\item 
\par
Tip on how to detect
\par
\begin{itemize}

\item ask question slightly different way, see if the reponse changes rapidly

\end{itemize}

\item IBM VP gives the EU's AI Act (Yamimova and Ojamo, 2024) good marks for AI safety
\item 

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lacy24genAIwhyHallucinate.pdf}
}

@techreport{Yaminova24ArtificialIntelligenceAct,
  type = {Press Release},
  title = {Artificial {{Intelligence Act}}: {{MEPs}} Adopt Landmark Law {\textbar} {{News}} {\textbar} {{European Parliament}}},
  shorttitle = {Artificial {{Intelligence Act}}},
  author = {Yamimova, Yasmina and Ojamo, Janne},
  year = {2024},
  month = mar,
  number = {20240308IPR19015},
  url = {https://www.europarl.europa.eu/news/en/press-room/20240308IPR19015/artificial-intelligence-act-meps-adopt-landmark-law},
  urldate = {2024-04-01},
  abstract = {On Wednesday, Parliament approved the Artificial Intelligence Act that ensures safety and compliance with fundamental rights, while boosting innovation.},
  langid = {english},
  note = {Yaminova24ArtificialIntelligenceAct
\par
The EU's new AI safety law. ~
\par
The EU's own bullet on it are:
\par
\begin{quotation}

\begin{itemize}

\item Safeguards on general purpose artificial intelligence

\end{itemize}

\begin{itemize}

\item Limits on the use of biometric identification systems by law enforcement
\item Bans on social scoring and AI used to manipulate or exploit user vulnerabilities
\item Right of consumers to launch complaints and receive meaningful explanations

\end{itemize}

\end{quotation}

\par
But there's more in the press release{\dots}
\par
Tarun Chopra, vice president of product management at IBM Data \& AI, says it:
\par
\begin{quotation}

\par
"provides a much tidier framework for ensuring transparency, accountability and human oversight" in developing and deploying AI. "Not every country is going to do the same thing, but the basic principles {\dots} are super, super critical," he added.
\par
\end{quotation}

\par
(Lacy, 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Yaminova24ArtificialIntelligenceAct.pdf}
}

@misc{Hughes23VectaraHallucLdrBrd,
  title = {Vectara {{Hallucination Leaderboard}}},
  author = {Hughes, Simon and Bae, Minseok},
  year = {2023},
  month = nov,
  url = {https://github.com/vectara/hallucination-leaderboard},
  urldate = {2024-04-01},
  abstract = {Leaderboard Comparing LLM Performance at Producing Hallucinations when Summarizing Short Documents},
  copyright = {Apache-2.0},
  keywords = {obsLitNote},
  note = {Hughes23VectaraHallucLdrBrd
\par
The hallucination leaderboard mentioned in (Lacy, 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hughes23VectaraHallucLdrBrd.pdf}
}

@misc{Day24AmazonBets150BonAI,
  title = {Amazon Bets \$150 Billion on Data Centers Required for {{AI}} Boom},
  author = {Day, Matt},
  year = {2024},
  month = apr,
  journal = {The Seattle Times},
  url = {https://www.seattletimes.com/business/amazon-bets-150-billion-on-data-centers-required-for-ai-boom/},
  urldate = {2024-04-01},
  abstract = {The spending spree is a show of force as the company looks to maintain its grip on the cloud services market.},
  langid = {american},
  note = {Day24AmazonBets150BonAI
\par
\begin{itemize}

\item \$150B for data centers in next 15 years, largely driven by expected AI boom.
\item Expect ``tens of billions of dollars~in AI-related revenue.''
\item 
\par
Energy
\par
\begin{itemize}

\item 
\par
Oregon
\par
\begin{itemize}

\item originally built there for the cheap hydro
\item now exceeds local utility's hydro share, forcing it to buy NG generated electricity

\end{itemize}

\item 
\par
Georgia
\par
\begin{itemize}

\item AWS spent \$650M to buy nuclear connected data center

\end{itemize}

\item 
\par
Mississippi
\par
\begin{itemize}

\item AMZ will pay for new solar to operate datacenter
\item will also be powered by a new NG plant

\end{itemize}

\item 
\par
Caveat: 
\par
\begin{itemize}

\item AMZ world's larges corporate RES buyer
\item Plans 10\% RES by 2025
\item 
\par
But RES far from datacenters, causing supply/demand mismatch, grid probs
\par
\begin{itemize}

\item and NG peakers?, not counting much

\end{itemize}

\end{itemize}

\item looking into battery and nuclear

\end{itemize}

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Day24AmazonBets150BonAI.pdf}
}

@misc{Keen24WhatLangChain,
  title = {What Is {{LangChain}}?},
  author = {Keen, Martin},
  year = {2024},
  month = mar,
  url = {https://www.youtube.com/watch?v=1bUy-1hGZpI},
  urldate = {2024-04-02},
  abstract = {Learn about IBM watsonx{$\rightarrow$} https://ibm.biz/BdvkK8 LangChain became immensely popular when it was launched in 2022, but how can it impact your development and application of AI models, Large Language Models (LLM) in particular. In this video Martin Keen shares an overview of the features and uses of LangChain. Get started for free on IBM Cloud {$\rightarrow$} https://ibm.biz/sign-up-now Subscribe to see more videos like this in the future {$\rightarrow$} http://ibm.biz/subscribe-now},
  note = {IBMTechnology24WhatLangChain
\par
Good one to watch b/f building lang chain in (``ADVANCED Python AI Agent Tutorial - Using RAG'', 2024)
\par
Another tutorial that builds something:}
}

@misc{AIJason23HuggingFaceLangchain,
  title = {Hugging {{Face}} + {{Langchain}} in 5 Mins},
  author = {{AI Jason}},
  year = {2023},
  month = jun,
  url = {https://www.youtube.com/watch?v=_j7JEDWuqLE},
  urldate = {2024-04-02},
  abstract = {Learn how to use Hugging Face, and get access to 200k+ AI models while building in Langchain for FREE.  Links - Hugging Face tutorials: https://hf.co/tasks - Follow me on twitter: ~~/~jasonzhou1993~~ - My discord: ~~/~discord~~ - Join my AI email list: https://www.ai-jason.com/  Timestamps 0:00 Intro 0:34 What is Hugging Face? 2:41 Demo: Turn image into audio story 3:08 Tutorial overview 3:27 Step 1: Image to Text model 5:12 Step 2: LLM 5:43 Step 3: Text to speech model 6:58 Step 4: Build UI with streamlit 8:46 More Hugging Face tutorials 9:10 No code alternative  About Me My name is Jason Zhou, a product designer who share interesting AI experiments \& products. Email me if you need help building AI apps! ask@ai-jason.com \#huggingface \#langchain \#autogpt \#ai \#nocode \#tutorial \#stepbystep \#langflow \#flowise \#gpt \#falcon},
  note = {AIJason23HuggingFaceLangchain
\par
``Access 200k+ FREE AI models for your AI apps''}
}

@misc{Keen24pickAIFoundModel,
  title = {How to {{Pick}} the {{Right AI Foundation Model}}},
  author = {Keen, Martin},
  year = {2024},
  month = feb,
  url = {https://www.youtube.com/watch?v=pePAAGfh-IU},
  urldate = {2024-04-02},
  abstract = {Test foundation models on watsonx {$\rightarrow$} https://ibm.biz/BdvGb6 There are so many foundation models available for AI Development, but how do you pick the right one? Picking the wrong one might cost you money, time, accuracy, and reliability. Martin Keen, Master Inventor, walks through his six step approach to picking the right model for your next project. Get started for free on IBM Cloud {$\rightarrow$} https://ibm.biz/sign-up-now Subscribe to see more videos like this in the future {$\rightarrow~$}http://ibm.biz/subscribe-now},
  note = {Keen24pickAIFoundModel
\par
Like it says{\dots}}
}

@misc{Imsys24LMSysChatbotArena,
  title = {{{LMSys Chatbot Arena Leaderboard}} - a {{Hugging Face Space}} by Lmsys},
  author = {Lmsys},
  year = {2024},
  month = apr,
  journal = {Hugging Face},
  url = {https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard},
  urldate = {2024-04-02},
  abstract = {Discover amazing ML apps made by the community},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Imsys24LMSysChatbotArena.pdf}
}

@misc{Google23geminiNanoNow,
  title = {Gemini {{Nano}} Now Running on {{Pixel}} 8 {{Pro}} --- the First Smartphone with {{AI}} Built In},
  author = {Google},
  year = {2023},
  month = dec,
  journal = {Google Store},
  url = {https://store.google.com/intl/en/ideas/articles/pixel-feature-drop-december-2023/},
  urldate = {2024-04-02},
  abstract = {Gemini is here, the most capable and flexible AI model we've ever built. Plus more AI updates coming to the Pixel portfolio.},
  langid = {american},
  keywords = {obsLitNote}
}

@misc{Pixegami23RAGLangchainPython,
  title = {{{RAG}} + {{Langchain Python Project}}: {{Easy AI}}/{{Chat For Your Docs}}},
  shorttitle = {{{RAG}} + {{Langchain Python Project}}},
  author = {{pixegami}},
  year = {2023},
  month = nov,
  publisher = {YouTube},
  url = {https://www.youtube.com/watch?v=tcqEUSNCn8I},
  urldate = {2024-04-03},
  abstract = {Learn how to build a "retrieval augmented generation" (RAG) app with Langchain and OpenAI in Python. You can use this to create chat-bots for your documents, books or files. You can also use it to build rich, interactive AI applications that use your data as a source.  Links  Code: https://github.com/pixegami/langchain...  (Sample Data) AWS Docs: https://github.com/awsdocs/aws-lambda...  (Sample Data) Alice in Wonderland: https://www.gutenberg.org/ebooks/11  Chapters 00:00 What is RAG? 01:36 Preparing the Data 05:05 Creating Chroma Database 06:36 What are Vector Embeddings? 09:38 Querying for Relevant Data 12:47 Crafting a Great Response 16:18 Wrapping Up \#pixegami \#python},
  keywords = {obsLitNote}
}

@misc{Schreiner23GPT4ArchitectureDatasets,
  title = {{{GPT-4}} Architecture, Datasets, Costs and More Leaked},
  author = {Schreiner, Maximilian},
  year = {2023},
  month = jul,
  journal = {THE DECODER},
  url = {https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/},
  urldate = {2024-04-03},
  abstract = {A new report reveals the architecture, training datasets, cost, and more of OpenAI's GPT-4.},
  langid = {american},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Schreiner23GPT4ArchitectureDatasets.pdf}
}

@misc{Chen24ChatGPTOneyearAnniversary,
  title = {{{ChatGPT}}'s {{One-year Anniversary}}: {{Are Open-Source Large Language Models Catching}} Up?},
  shorttitle = {{{ChatGPT}}'s {{One-year Anniversary}}},
  author = {Chen, Hailin and Jiao, Fangkai and Li, Xingxuan and Qin, Chengwei and Ravaut, Mathieu and Zhao, Ruochen and Xiong, Caiming and Joty, Shafiq},
  year = {2024},
  month = jan,
  number = {arXiv:2311.16989},
  eprint = {2311.16989},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.16989},
  url = {http://arxiv.org/abs/2311.16989},
  urldate = {2024-04-03},
  abstract = {Upon its release in late 2022, ChatGPT has brought a seismic shift in the entire landscape of AI, both in research and commerce. Through instruction-tuning a large language model (LLM) with supervised fine-tuning and reinforcement learning from human feedback, it showed that a model could answer human questions and follow instructions on a broad panel of tasks. Following this success, interests in LLMs have intensified, with new LLMs flourishing at frequent interval across academia and industry, including many start-ups focused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's Claude) generally outperform their open-source counterparts, the progress on the latter has been rapid with claims of achieving parity or even better on certain tasks. This has crucial implications not only on research but also on business. In this work, on the first anniversary of ChatGPT, we provide an exhaustive overview of this success, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT.},
  archiveprefix = {arXiv},
  note = {Comment: version v4, included latest top-performing open-sourced LLMs},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\Chen24areOpenSrcLLMcatchUp.pdf}
}

@misc{Hou24SystematicEvaluationLarge,
  title = {A Systematic Evaluation of Large Language Models for Generating Programming Code},
  author = {Hou, Wenpin and Ji, Zhicheng},
  year = {2024},
  month = mar,
  number = {arXiv:2403.00894},
  eprint = {2403.00894},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.00894},
  url = {http://arxiv.org/abs/2403.00894},
  urldate = {2024-04-03},
  abstract = {We systematically evaluated the performance of seven large language models in generating programming code using various prompt strategies, programming languages, and task difficulties. GPT-4 substantially outperforms other large language models, including Gemini Ultra and Claude 2. The coding performance of GPT-4 varies considerably with different prompt strategies. In most LeetCode and GeeksforGeeks coding contests evaluated in this study, GPT-4 employing the optimal prompt strategy outperforms 85 percent of human participants. Additionally, GPT-4 demonstrates strong capabilities in translating code between different programming languages and in learning from past errors. The computational efficiency of the code generated by GPT-4 is comparable to that of human programmers. These results suggest that GPT-4 has the potential to serve as a reliable assistant in programming code generation and software development.},
  archiveprefix = {arXiv},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hou24vevalLLMprogCode.pdf}
}

@article{Guimaraes24whatPretrainedLLMknow,
  title = {Pre-Trained Language Models: {{What}} Do They Know?},
  shorttitle = {Pre-Trained Language Models},
  author = {Guimar{\~a}es, Nuno and Campos, Ricardo and Jorge, Al{\'i}pio},
  year = {2024},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {14},
  number = {1},
  pages = {e1518},
  issn = {1942-4795},
  doi = {10.1002/widm.1518},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1518},
  urldate = {2024-04-03},
  abstract = {Large language models (LLMs) have substantially pushed artificial intelligence (AI) research and applications in the last few years. They are currently able to achieve high effectiveness in different natural language processing (NLP) tasks, such as machine translation, named entity recognition, text classification, question answering, or text summarization. Recently, significant attention has been drawn to OpenAI's GPT models' capabilities and extremely accessible interface. LLMs are nowadays routinely used and studied for downstream tasks and specific applications with great success, pushing forward the state of the art in almost all of them. However, they also exhibit impressive inference capabilities when used off the shelf without further training. In this paper, we aim to study the behavior of pre-trained language models (PLMs) in some inference tasks they were not initially trained for. Therefore, we focus our attention on very recent research works related to the inference capabilities of PLMs in some selected tasks such as factual probing and common-sense reasoning. We highlight relevant achievements made by these models, as well as some of their current limitations that open opportunities for further research. This article is categorized under: Fundamental Concepts of Data and Knowledge {$>$} Key Design Issues in Data Mining Technologies {$>$} Artificial Intelligence},
  copyright = {{\copyright} 2023 Wiley Periodicals LLC.},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Guimaraes24whatPretrainedLLMknow.pdf}
}

@misc{OpenAI24synthVoiceChllngOppty,
  title = {Navigating the {{Challenges}} and {{Opportunities}} of {{Synthetic Voices}}},
  author = {{OpenAI}},
  year = {2024},
  month = mar,
  url = {https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices},
  urldate = {2024-04-03},
  abstract = {We're sharing lessons from a small scale preview of Voice Engine, a model for creating custom voices.},
  langid = {american},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\OpenAI24synthVoiceChllngOppty.pdf}
}

@misc{David24OpenAIVoiceCloning,
  title = {{{OpenAI}}'s Voice Cloning {{AI}} Model Only Needs a 15-Second Sample to Work},
  author = {David, Emilia},
  year = {2024},
  month = mar,
  journal = {The Verge},
  url = {https://www.theverge.com/2024/3/29/24115701/openai-voice-generation-ai-model},
  urldate = {2024-04-03},
  abstract = {About ten developers have access to Voice Generation.},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\David24OpenAIVoiceCloning.pdf}
}

@misc{Liu24AreLLMsCapable,
  title = {Are {{LLMs Capable}} of {{Data-based Statistical}} and {{Causal Reasoning}}? {{Benchmarking Advanced Quantitative Reasoning}} with {{Data}}},
  shorttitle = {Are {{LLMs Capable}} of {{Data-based Statistical}} and {{Causal Reasoning}}?},
  author = {Liu, Xiao and Wu, Zirui and Wu, Xueqing and Lu, Pan and Chang, Kai-Wei and Feng, Yansong},
  year = {2024},
  month = feb,
  number = {arXiv:2402.17644},
  eprint = {2402.17644},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.17644},
  url = {http://arxiv.org/abs/2402.17644},
  urldate = {2024-04-03},
  abstract = {Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58\%, which has a large room for improvement. Among open-source models, Deepseek-coder-instruct, a code LLM pretrained on 2T tokens, gets the highest accuracy of 37\%. Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously. Code and data are in https://github.com/xxxiaol/QRData.},
  archiveprefix = {arXiv},
  keywords = {hasCode},
  note = {Comment: Project website: https://xxxiaol.github.io/QRData/},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\Liu24canLLMstatCausalRsn.pdf}
}

@article{Lu24AnalysisPredictionSCR,
  title = {Analysis and Prediction in {{SCR}} Experiments Using {{GPT-4}} with an Effective Chain-of-Thought Prompting Strategy},
  author = {Lu, Muyu and Gao, Fengyu and Tang, Xiaolong and Chen, Linjiang},
  year = {2024},
  month = mar,
  journal = {iScience},
  volume = {27},
  number = {4},
  pages = {109451},
  issn = {2589-0042},
  doi = {10.1016/j.isci.2024.109451},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10960113/},
  urldate = {2024-04-03},
  abstract = {This study explores the use of large language models (LLMs) in interpreting and predicting experimental outcomes based on given experimental variables, leveraging the human-like reasoning and inference capabilities of LLMs, using selective catalytic reduction of NOx with NH3 as a case study. We implement the chain of thought (CoT) concept to formulate logical steps for uncovering connections within the data,~introducing an ``Ordered-and-Structured'' CoT (OSCoT) prompting strategy. We compare the OSCoT strategy with the more conventional ``One-Pot'' CoT (OPCoT) approach and with human experts. We~demonstrate that GPT-4, equipped with this new OSCoT prompting strategy, outperforms the other two settings and accurately predicts experimental outcomes and provides intuitive reasoning for its predictions., {$\bullet$}Application of a large language model (LLM) in chemistry tasks{$\bullet$}A new chain-of-thought prompting strategy focusing on formulating logical steps{$\bullet$}An LLM-powered assistant that interprets, predicts, and rationalizes experimental data, Natural sciences; Chemistry; Computer science},
  pmcid = {PMC10960113},
  pmid = {38523781},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\Lu24AnalysisPredictionSCR.pdf}
}

@misc{Barousse24dataAnlystGPT4Outperf,
  title = {Study: {{GPT-4}} Outperforms {{Data Analysts}}},
  shorttitle = {Study},
  author = {Barousse, Luke},
  year = {2024},
  month = apr,
  publisher = {YouTube},
  url = {https://www.youtube.com/watch?v=qdhMfuGi0tA},
  urldate = {2024-04-03},
  abstract = { My SQL Course  ~~~{$\bullet~$}SQL~for~Data~Analytics~-~Learn~SQL~in...~~ Courses for Data Nerds ==================================  Google Data Analytics Certificate   https://lukeb.co/GoogleCert  Python for Everybody   https://lukeb.co/PythonForEverybody  SQL for Data Science  https://lukeb.co/SQLdataScience  Excel Skills for Business ~ https://lukeb.co/ExcelBusinessAnalyst  PowerBI for Data Viz  https://lukeb.co/PowerBI  Tableau for Data Viz ~https://lukeb.co/Tableau\_UCDavis {$\skull$} Data Science: Foundations using R  https://lukeb.co/RforDataScienceJH  Coursera Plus Subscription (7-day free trial)  https://lukeb.co/CourseraPlus  All courses  https://kit.co/lukebarousse/data-anal... Books for Data Nerds ==================================  Books I've read   https://kit.co/lukebarousse/book-reco...  Data Analyst Must Read  https://geni.us/StorytellingWithData Tech for Data Nerds ==================================  Tech I use  https://kit.co/lukebarousse/computer-... Windows Virtual Machine for Mac (Parallels)  https://lukeb.co/ParallelsFreeTrial Social Media / Contact Me ====================== Newsletter: https://www.lukebarousse.com/  Linkedin: ~~/~luke-b~~  X/Twitter: ~~/~lukebarousse~~  Instagram: ~~/~lukebarousse~~  TikTok: ~~/~lukebarousse~~ As an Amazon, Coursera, and Parallels Affiliate Programs member, I earn a commission from qualifying purchases on the links above.  It costs you nothing but helps me with content creation. \#datanerd \#dataanalyst \#datascience},
  keywords = {obsLitNote}
}

@misc{Novis23ObsidianChatGPT,
  title = {Obsidian plus {{ChatGPT Webinar}}},
  author = {Novis, Scott},
  year = {2023},
  month = sep,
  url = {https://www.youtube.com/watch?v=c7iU-QCbrqg},
  urldate = {2024-04-03},
  abstract = {This walkthrough is an overview of why I use Obsidian, and how I integrate ChatGPT into my workflow. Useful links: https://obsidian.md/ - download the software https://obsidian.md/sync - sync it everywhere https://obsidian.md/publish - publish your notes. openai.com playground:  https://platform.openai.com/playground},
  keywords = {obsLitNote}
}

@misc{Forte24googleNotebookLM,
  title = {How to {{Use NotebookLM}} ({{Google}}'s {{New AI Tool}})},
  author = {Forte, Tiago},
  year = {2024},
  month = feb,
  url = {https://www.youtube.com/watch?v=iWPjBwXy_Io},
  urldate = {2024-04-04},
  abstract = {Google's NotebookLM is way more than notetaking, writing, or organizational tool. It's an AI collaborator, grounded in your data with your unique view of the world.  In this tutorial, I'll give you a tour of the main functionalities, demonstrate concrete use cases such as making sense of meeting notes or writing an article, and cover the current limitations.  Visit https://notebooklm.google.com to sign up for a free NotebookLM account using your Google login (You must be at least 18 years of age, based in the U.S., and Workspace users must have ``Experimental apps enabled'' by an administrator)  Get 60 days of Readwise for free: https://readwise.io/basb/  Watch my full Readwise tutorial: ~~~{$\bullet~$}How~I~remember~everything~I~read~with...~~  TECHNICAL NOTES [08:37]: NotebookLM isn't actually ``trained'' on your data. The model is pre-trained, and the software just shuttles your inputs into its context window temporarily so it can answer factually based on that information. Once you end your session, the information you entered is wiped from the model's memory so your data is secure. [13:30]: NotebookLM can answer specific questions, but holistic questions (such as "Give me an outline of the entire book, in order") are harder, because the model can't take in the entire document at once; it can only see the most relevant passages for a given query.  CHAPTERS 00:00 - 01:20 Intro 01:21 - 09:04 Getting started 09:06 - 14:52 Use cases for understanding  14:53 - 21:12 Use cases for writing 21:13 - 23:12 Using Readwise with NotebookLM  23:13 - 24:23 Limitations 24:24 - 25:23 Conclusion 25:24 - 25:47 Blooper ------------------------------- Building a Second Brain is the proven method to organize your digital life and unlock your creative potential.  We're on a mission to help you~increase your productivity, and \hspace{0pt}\hspace{0pt}lead a more fulfilling life with more ease and less stress.    TEST YOUR PRODUCTIVITY POTENTIAL If you had a Second Brain, how much more organized, effective, calm, and creative could you be?~ Take our quiz \& find out: https://fortelabs.com/productivity-po...   HOW YOU CAN BUILD YOUR SECOND BRAIN Order the book: https://www.buildingasecondbrain.com/... Take the course: https://www.buildingasecondbrain.com/...   JOIN THE FORTE LABS NEWSLETTER Level up your productivity and life with new essays, videos, event invites, and other resources every Tuesday. Join 125k subscribers exploring the frontier of modern work, experimenting with new ways of doing more with less, and discovering what it means to fulfill our human potential. Subscribe here: https://fortelabs.com/subscribe   FOLLOW US ON SOCIALS  Always get the latest updates and insights around Building a Second Brain. Twitter: ~~/~fortelabs~~ LinkedIn: ~~/~tiagoforte~~ Facebook: ~~/~fortelabs~~ Instagram: ~~/~fortelabsco},
  keywords = {obsLitNote}
}

@phdthesis{Donadee15operBattUncertPhD,
  type = {Thesis},
  title = {Operation and {{Valuation}} of {{Multi-Function Battery Energy Storage}} under {{Uncertainty}}},
  author = {Donadee, Jonathan},
  year = {2015},
  month = jun,
  doi = {10.1184/R1/7206779.v1},
  url = {https://kilthub.cmu.edu/articles/thesis/Operation_and_Valuation_of_Multi-Function_Battery_Energy_Storage_under_Uncertainty/7206779/1},
  urldate = {2024-04-04},
  abstract = {Electrical energy storage resources (ESRs) o er a promising solution to many of the issues facing the electric grid. In order for this promise to be fully realized,new intelligent decision-making technologies are required. This dissertation studies the operation and valuation of ESRs in an uncertain electric grid environment.ESRs can include both stationary battery energy storage systems (BESSs) and distributed deferrable loads such as plug-in electric vehicles (EVs). An ESR can be operated to provide multiple services simultaneously, maximizingits value. An EV can provide transportation services as well as participate in electric grid frequency regulation. A BESS can also provide frequency regulation while providing load peak shifting. In this thesis, we propose new andinnovative solutions that enable optimal operation and accurate valuation of multi-function ESRs under uncertainty. New Markov decision problems (MDPs) for smart charging of EVs are developed for cases of price, ancillary services, anddriver behavior uncertainty. In order to compare the proposed MDP approaches with deterministic optimization approaches, a Dynamic Monitoring and DecisionSystems (DYMONDS) energy market simulation is developed. We also propose an in nite horizon MDP approach to estimating the net present value of a BESSthat degrades over time. In order to optimize the economic scheduling of an ESR that provides frequency regulation service, one needs a predictive model of theautomatic generation control (AGC) signal. We investigate timeseries and other statistical models for the prediction of an AGC signal and its cumulative eff ect on the state of charge of an ESR.},
  langid = {english},
  school = {Carnegie Mellon University},
  keywords = {obsLitNote},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Donadee15operBattUncertPhD.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Donadee15operBattUncertPhDSlides.pptx}
}

@misc{AEMO24vulnerableTransmissionList,
  title = {List of {{Vulnerable Transmission Lines}}},
  author = {{AEMO}},
  year = {2024},
  month = jun,
  url = {https://aemo.com.au/energy-systems/electricity/national-electricity-market-nem/system-operations/power-system-operation/power-system-operating-procedures/list-of-vulnerable-transmission-lines},
  urldate = {2024-04-05},
  langid = {english},
  keywords = {obsLitNote}
}

@misc{NotebookLM,
  title = {{{NotebookLM}}},
  author = {{Google}},
  year = {2024},
  month = apr,
  url = {https://notebooklm.google/},
  urldate = {2024-04-05},
  abstract = {NotebookLM gives you a personalized AI, grounded in the information you trust. NotebookLM is only available in the U.S. for users 18 and up.},
  langid = {english},
  keywords = {obsLitNote}
}

@article{Birch24genAIwasteTimeMoney,
  title = {Generative Artificial Intelligence Is Simply a Waste of Our Time and Money},
  shorttitle = {Opinion},
  author = {Birch, Kean},
  year = {2024},
  month = apr,
  journal = {The Globe and Mail},
  url = {https://www.theglobeandmail.com/business/commentary/article-generative-artificial-intelligence-is-simply-a-waste-of-our-time-and/},
  urldate = {2024-04-05},
  abstract = {Generative artificial intelligence have the potential to create enormous social costs without any significant social benefits},
  langid = {canadian},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Birch24genAIwasteTimeMoney.pdf}
}

@misc{Sadowski23habsburgAI,
  type = {Tweet},
  title = {Habsburg {{AI}}},
  author = {{Jathan Sadowski}},
  year = {2023},
  month = feb,
  journal = {Twitter},
  url = {https://twitter.com/jathansadowski/status/1625245803211272194},
  urldate = {2024-04-05},
  abstract = {I coined a term on @machinekillspod that I feel like needs its own essay: Habsburg AI -- a system that is so heavily trained on the outputs of other generative AI's that it becomes an inbred mutant, likely with exaggerated, grotesque features. It joins the lineage of Potemkin AI.},
  langid = {english},
  keywords = {obsLitNote}
}

@misc{Bogost24aiLostMagic,
  title = {{{AI Has Lost Its Magic}}},
  author = {Bogost, Ian},
  year = {2024},
  month = apr,
  journal = {The Atlantic},
  url = {https://www.theatlantic.com/technology/archive/2024/04/ai-magic-taking-over/677968/},
  urldate = {2024-04-05},
  abstract = {That's how you know it's taking over.},
  chapter = {Technology},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bogost24aiLostMagic.pdf}
}

@misc{Downing23startChatGPTAdvDatAnlys,
  title = {Getting {{Started}} with {{ChatGPT}}'s {{Advanced Data Analysis Feature}}},
  author = {Downing, Chuck},
  year = {2023},
  month = sep,
  url = {https://www.youtube.com/watch?v=SzTjGAO6fKo},
  urldate = {2024-04-05},
  abstract = {Join Chuck Downing, a PhD student at MIT Sloan, as he dives into the capabilities of ChatGPT's Advanced Data Analysis features. This tutorial showcases how to enable and use Advanced Data Analysis, from reading and describing datasets to generating advanced data visualization and regression analyses. Downing demonstrates the process using the World Bank's carbon emissions dataset, highlighting the seamless integration of data upload, code generation, and result interpretation. Whether you're a researcher or just curious about this AI tool, this video provides a helpful introduction to harnessing the power of ChatGPT for your data tasks. Update: ChatGPT Plus subscribers can now access Advanced Data Analysis in a standard chat window by default (without specifically enabling the tool). However, the example use cases for Advanced Data Analysis that you'll see in this video have not changed. To view or download the dataset used in the video, go to the World Bank website: https://data.worldbank.org/indicator/.... For more MIT Sloan resources on teaching with generative AI, visit our Resource Hub: https://mitsloanedtech.mit.edu/ai/.},
  keywords = {obsLitNote}
}

@misc{Bogost23endRecommendLttrs,
  title = {The {{End}} of {{Recommendation Letters}}},
  author = {Bogost, Ian},
  year = {2023},
  month = apr,
  journal = {The Atlantic},
  url = {https://www.theatlantic.com/technology/archive/2023/04/chatgpt-ai-college-professors/673796/},
  urldate = {2024-04-05},
  abstract = {Professors, like their students, use ChatGPT to get out of doing their assignments.},
  chapter = {Technology},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bogost23endRecommendLttrs.pdf}
}

@article{Lim21TemporalFusionTransfr,
  title = {Temporal Fusion Transformers for Interpretable Multi-Horizon Time Series Forecasting},
  author = {Lim, Bryan and Ar{\i}k, Sercan {\"O} and Loeff, Nicolas and Pfister, Tomas},
  year = {2021},
  journal = {International Journal of Forecasting},
  volume = {37},
  number = {4},
  pages = {1748--1764},
  publisher = {Elsevier},
  url = {https://www.sciencedirect.com/science/article/pii/S0169207021000637},
  urldate = {2024-04-06},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lim21TemporalFusionTransfr.pdf}
}

@misc{Challu22NHiTSNeuralHierarchical,
  title = {N-{{HiTS}}: {{Neural Hierarchical Interpolation}} for {{Time Series Forecasting}}},
  shorttitle = {N-{{HiTS}}},
  author = {Challu, Cristian and Olivares, Kin G. and Oreshkin, Boris N. and Garza, Federico and {Mergenthaler-Canseco}, Max and Dubrawski, Artur},
  year = {2022},
  month = jan,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2201.12886v6},
  urldate = {2024-04-06},
  abstract = {Recent progress in neural forecasting accelerated improvements in the performance of large-scale forecasting systems. Yet, long-horizon forecasting remains a very difficult task. Two common challenges afflicting the task are the volatility of the predictions and their computational complexity. We introduce N-HiTS, a model which addresses both challenges by incorporating novel hierarchical interpolation and multi-rate data sampling techniques. These techniques enable the proposed method to assemble its predictions sequentially, emphasizing components with different frequencies and scales while decomposing the input signal and synthesizing the forecast. We prove that the hierarchical interpolation technique can efficiently approximate arbitrarily long horizons in the presence of smoothness. Additionally, we conduct extensive large-scale dataset experiments from the long-horizon forecasting literature, demonstrating the advantages of our method over the state-of-the-art methods, where N-HiTS provides an average accuracy improvement of almost 20\% over the latest Transformer architectures while reducing the computation time by an order of magnitude (50 times). Our code is available at bit.ly/3VA5DoT},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\zotero\Challu22NHiTSNeuralHierarchical.pdf}
}

@misc{Nixtla24openSrcTimeSeriesFrcst,
  title = {Open {{Source Time Series Ecosystem}} - {{Nixtla}}},
  author = {{Nixtla}},
  year = {2024},
  url = {https://www.nixtla.io/open-source},
  urldate = {2024-04-06},
  abstract = {Your friendly assistant. Fast, capable, and truly conversational.},
  langid = {english},
  keywords = {obsLitNote}
}

@misc{HuggingFace24benchmarkMTEB,
  type = {Blog},
  title = {{{MTEB}}: {{Massive Text Embedding Benchmark}}},
  author = {{HuggingFace}},
  year = {2024},
  journal = {GitHub},
  url = {https://github.com/huggingface/blog/blob/main/mteb.md},
  urldate = {2024-04-07},
  abstract = {Public repo for HF blog posts. Contribute to huggingface/blog development by creating an account on GitHub.},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\HuggingFace24benchmarkMTEB.pdf}
}

@misc{HuggingFace24leaderboardMTEB,
  type = {Leaderboard},
  title = {{{MTEB Leaderboard}} - a {{Hugging Face Space}} by Mteb},
  author = {{HuggingFace}},
  year = {2024},
  url = {https://huggingface.co/spaces/mteb/leaderboard},
  urldate = {2024-04-07},
  abstract = {Discover amazing ML apps made by the community},
  keywords = {obsLitNote}
}

@misc{3Blue1Brown24visAttenTransfrmr,
  title = {Visualizing {{Attention}}, a {{Transformer}}'s {{Heart}} {\textbar} {{Chapter}} 6, {{Deep Learning}}},
  author = {{3Blue1Brown}},
  year = {2024},
  month = apr,
  url = {https://www.youtube.com/watch?v=eMlx5fFNoYc},
  urldate = {2024-04-07},
  abstract = {Demystifying attention, the key mechanism inside transformers and LLMs. Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support Special thanks to these supporters: https://www.3blue1brown.com/lessons/a... An equally valuable form of support is to simply share the videos. Demystifying self-attention, multiple heads, and cross-attention. Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support The first pass for the translated subtitles here is machine-generated, and therefore notably imperfect. To contribute edits or fixes, visit https://translate.3blue1brown.com/ ------------------ Here are a few other relevant resources Build a GPT from scratch, by Andrej Karpathy ~~~{$\bullet~$}Let's~build~GPT:~from~scratch,~in~cod...~~ If you want a conceptual understanding of language models from the ground up, @vcubingx just started a short series of videos on the topic: ~~~{$\bullet~$}What~does~it~mean~for~computers~to~un...~~ If you're interested in the herculean task of interpreting what these large networks might actually be doing, the Transformer Circuits posts by Anthropic are great. In particular, it was only after reading one of these that I started thinking of the combination of the value and output matrices as being a combined low-rank map from the embedding space to itself, which, at least in my mind, made things much clearer than other sources. https://transformer-circuits.pub/2021... Site with exercises related to ML programming and GPTs https://www.gptandchill.ai/codingprob... History of language models by Brit Cruise, ~@ArtOfTheProblem~  ~~~{$\bullet~$}ChatGPT:~30~Year~History~{\textbar}~How~AI~Lea...~~ An early paper on how directions in embedding spaces have meaning: https://arxiv.org/pdf/1301.3781.pdf ------------------ Timestamps: 0:00 - Recap on embeddings 1:39 - Motivating examples 4:29 - The attention pattern 11:08 - Masking 12:42 - Context size 13:10 - Values 15:44 - Counting parameters 18:21 - Cross-attention 19:19 - Multiple heads 22:16 - The output matrix 23:19 - Going deeper 24:54 - Ending ------------------ These animations are largely made using a custom Python library, manim.  See the FAQ comments here: https://3b1b.co/faq\#manim https://github.com/3b1b/manim https://github.com/ManimCommunity/manim/ All code for specific videos is visible here: https://github.com/3b1b/videos/ The music is by Vincent Rubinetti. https://www.vincentrubinetti.com https://vincerubinetti.bandcamp.com/a... https://open.spotify.com/album/1dVyjw... ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. If you're reading the bottom of a video description, I'm guessing you're more interested than the average viewer in lessons here. It would mean a lot to me if you chose to stay up to date on new ones, either by subscribing here on YouTube or otherwise following on whichever platform below you check most regularly. Mailing list: https://3blue1brown.substack.com Twitter: ~~/~3blue1brown~~ Instagram: ~~/~3blue1brown~~ Reddit: ~~/~3blue1brown~~ Facebook: ~~/~3blue1brown~~ Patreon: ~~/~3blue1brown~~ Website: https://www.3blue1brown.com},
  keywords = {obsLitNote}
}

@misc{3Blue1Brown24WvizIntroGPTtransfrmr,
  title = {But What Is a {{GPT}}?  {{Visual}} Intro to Transformers {\textbar} {{Chapter}} 5, {{Deep Learning}}},
  shorttitle = {But What Is a {{GPT}}?},
  author = {{3Blue1Brown}},
  year = {2024},
  month = apr,
  url = {https://www.youtube.com/watch?v=wjZofJX0v4M},
  urldate = {2024-04-07},
  abstract = {Unpacking how large language models work under the hood Early view of the next chapter for patrons: https://3b1b.co/early-attention Special thanks to these supporters: https://3b1b.co/lessons/gpt\#thanks To contribute edits to the subtitles, visit https://translate.3blue1brown.com/ Other recommended resources on the topic. Richard Turner's introduction is one of the best starting places: https://arxiv.org/pdf/2304.10557.pdf Coding a GPT with Andrej Karpathy ~~~{$\bullet~$}Let's~build~GPT:~from~scratch,~in~cod...~~ Introduction to self-attention by John Hewitt https://web.stanford.edu/class/cs224n... History of language models by Brit Cruise: ~~~{$\bullet~$}ChatGPT:~30~Year~History~{\textbar}~How~AI~Lea...~~ Paper about examples like the ``woman - man'' one presented here: https://arxiv.org/pdf/1301.3781.pdf ------------------ Timestamps 0:00 - Predict, sample, repeat 3:03 - Inside a transformer 6:36 - Chapter layout 7:20 - The premise of Deep Learning 12:27 - Word embeddings 18:25 - Embeddings beyond words 20:22 - Unembedding 22:22 - Softmax with temperature 26:03 - Up next ------------------ These animations are largely made using a custom Python library, manim.  See the FAQ comments here: https://3b1b.co/faq\#manim https://github.com/3b1b/manim https://github.com/ManimCommunity/manim/ All code for specific videos is visible here: https://github.com/3b1b/videos/ The music is by Vincent Rubinetti. https://www.vincentrubinetti.com https://vincerubinetti.bandcamp.com/a... https://open.spotify.com/album/1dVyjw... ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. If you're reading the bottom of a video description, I'm guessing you're more interested than the average viewer in lessons here. It would mean a lot to me if you chose to stay up to date on new ones, either by subscribing here on YouTube or otherwise following on whichever platform below you check most regularly. Mailing list: https://3blue1brown.substack.com Twitter: ~~/~3blue1brown~~ Instagram: ~~/~3blue1brown~~ Reddit: ~~/~3blue1brown~~ Facebook: ~~/~3blue1brown~~ Patreon: ~~/~3blue1brown~~ Website: https://www.3blue1brown.com},
  keywords = {obsLitNote}
}

@misc{Barnard23whatIsEmbeddingIBM,
  title = {What Is {{Embedding}}?},
  shorttitle = {What Is {{Embedding}}?},
  author = {Barnard, Joel},
  year = {2023},
  month = dec,
  journal = {IBM Think},
  url = {https://www.ibm.com/topics/embedding},
  urldate = {2024-04-08},
  abstract = {Embedding is a means of representing text and other objects as points in a continuous vector space that are semantically meaningful to machine learning algorithms.},
  langid = {american},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Barnard23whatIsEmbeddingIBM.pdf}
}

@misc{SwimmTeam24embedInML,
  title = {Embeddings in {{Machine Learning}}: {{Types}}, {{Models}} \& {{Best Practices}}},
  shorttitle = {Embeddings in {{Machine Learning}}},
  author = {{SwimmTeam}},
  year = {2024},
  journal = {Swimm},
  url = {https://swimm.io/learn/large-language-models/embeddings-in-machine-learning-types-models-and-best-practices},
  urldate = {2024-04-08},
  abstract = {Embeddings are a type of feature learning technique where high-dimensional data is converted into low-dimensional vectors while preserving the relevant information.},
  langid = {american},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\SwimmTeam24embedInML.html}
}

@misc{SwimmTeam24TransfrmrBasics7Models,
  title = {Transformer {{Model}}: {{The Basics}} and 7 {{Models You Should Know}}},
  shorttitle = {Transformer {{Model}}},
  author = {{SwimmTeam}},
  year = {2024},
  month = apr,
  journal = {Swimm},
  url = {https://swimm.io/learn/large-language-models/transformer-model-the-basics-and-7-models-you-should-know},
  urldate = {2024-04-09},
  abstract = {The Transformer model is a type of model used in machine learning, particularly in the area of natural language processing (NLP).},
  langid = {american},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\SwimmTeam24TransfrmrBasics7Models.pdf}
}

@misc{SwimmTeam24Tword2VecHowWork,
  title = {What {{Is Word2Vec}} and {{How Does It Work}}?},
  author = {{SwimmTeam}},
  year = {2024},
  month = apr,
  journal = {Swimm},
  url = {https://swimm.io/learn/large-language-models/what-is-word2vec-and-how-does-it-work},
  urldate = {2024-04-09},
  abstract = {Word2Vec is a group of machine learning architectures that can find words with similar contexts and group them together.},
  langid = {american},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\SwimmTeam24Tword2VecHowWork.pdf}
}

@misc{SwimmTeam24twordEmbed5TypNLPapp,
  title = {5 {{Types}} of {{Word Embeddings}} and {{Example NLP Applications}}},
  author = {{Swimm}},
  url = {https://swimm.io/learn/large-language-models/5-types-of-word-embeddings-and-example-nlp-applications},
  urldate = {2024-04-09},
  abstract = {Word embeddings are a key concept in natural language processing (NLP), a field within machine learning.},
  langid = {american},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\SwimmTeam24twordEmbed5TypNLPapp.pdf}
}

@techreport{Swimm24devKnowldgShare,
  title = {The State  of Developer Knowledge Sharing},
  author = {{Swimm}},
  year = {2024},
  month = apr,
  institution = {Swimm},
  abstract = {With almost all developers (92\%) using or experimenting with AI coding tools,  AI continues to transform nearly every aspect of the software development lifecycle. Despite rapid technological advancements, the fundamental requirement to understand code remains unchanged, for developers and managers alike. Whether code is manually written by a developer or generated by AI, eventually, someone else will need to understand and interpret that code.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Swimm24devKnowldgShare.pdf}
}

@misc{Muennighoff23MTEBMassiveText,
  title = {{{MTEB}}: {{Massive Text Embedding Benchmark}}},
  shorttitle = {{{MTEB}}},
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"i}c and Reimers, Nils},
  year = {2023},
  month = mar,
  number = {arXiv:2210.07316},
  eprint = {2210.07316},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.07316},
  url = {http://arxiv.org/abs/2210.07316},
  urldate = {2024-04-09},
  abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.},
  archiveprefix = {arXiv},
  note = {Comment: 24 pages, 14 tables, 6 figures},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Muennighoff23MTEBMassiveText.pdf}
}

@misc{CognitiveCreator23word2VecNLPgtwy,
  title = {{{Word2Vec}}: {{NLP}}'s {{Gateway}} to {{Word Embeddings}}},
  shorttitle = {{{Word2Vec}}},
  author = {{Cognitive Creator}},
  year = {2023},
  month = nov,
  journal = {Medium},
  url = {https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe},
  urldate = {2024-04-09},
  abstract = {Explore the world of Word2Vec and its significance in NLP},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\CognitiveCreator23word2VecNLPgtwy.pdf}
}

@misc{Caballar24codingAIautopilot,
  title = {{{AI Coding Is Going From Copilot}} to {{Autopilot}} {$>$} {{But}} so Far {{Devin AI}} and Others Still Really Need Humans},
  shorttitle = {{{AI Coding Is Going From Copilot}} to {{Autopilot}}},
  author = {{Rina Diane Caballar}},
  year = {24},
  month = apr,
  journal = {IEEE Spectrum},
  url = {https://spectrum.ieee.org/ai-code-generator},
  urldate = {2024-04-10},
  langid = {english},
  keywords = {obsLitNote}
}

@misc{Monigatti23vectorDBprotoToProd,
  type = {Blog},
  title = {From Prototype to Production: {{Vector}} Databases in Generative {{AI}} Applications},
  shorttitle = {From Prototype to Production},
  author = {Monigatti, Leonie},
  year = {2023},
  month = oct,
  journal = {Stackoverflow},
  url = {https://stackoverflow.blog/2023/10/09/from-prototype-to-production-vector-databases-in-generative-ai-applications/},
  urldate = {2024-04-10},
  abstract = {Since the rise of ChatGPT, the general public has realized that generative artificial intelligence (GenAI) could potentially transform our lives. The availability of large language models (LLMs) has also changed how developers build AI-powered applications and has led to the emergence of various new developer tools. Although vector databases have been around long before ChatGPT, they have become an integral part of the GenAI technology stack, as vector databases can address some of LLMs' key limitations, such as hallucinations and lack of long-term memory This article first introduces vector databases and their use cases. Next, you will learn more about how vector databases are designed to help developers get started with building GenAI applications quickly. As a developer advocate at Weaviate, an open-source vector database, I will use Weaviate to demonstrate relevant concepts as we go along. In the final discussion, you will learn how they can address the challenges enterprises face when moving these prototypes to production.},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Monigatti23vectorDBprotoToProd.pdf}
}

@article{Taipalus24vectorDBfundamental,
  title = {Vector Database Management Systems: {{Fundamental}} Concepts, Use-Cases, and Current Challenges},
  shorttitle = {Vector Database Management Systems},
  author = {Taipalus, Toni},
  year = {2024},
  month = jun,
  journal = {Cognitive Systems Research},
  volume = {85},
  eprint = {2309.11322},
  primaryclass = {cs},
  pages = {101216},
  issn = {13890417},
  doi = {10.1016/j.cogsys.2024.101216},
  url = {http://arxiv.org/abs/2309.11322},
  urldate = {2024-04-10},
  abstract = {Vector database management systems have emerged as an important component in modern data management, driven by the growing importance for the need to computationally describe rich data such as texts, images and video in various domains such as recommender systems, similarity search, and chatbots. These data descriptions are captured as numerical vectors that are computationally inexpensive to store and compare. However, the unique characteristics of vectorized data, including high dimensionality and sparsity, demand specialized solutions for efficient storage, retrieval, and processing. This narrative literature review provides an accessible introduction to the fundamental concepts, use-cases, and current challenges associated with vector database management systems, offering an overview for researchers and practitioners seeking to facilitate effective vector data management.},
  archiveprefix = {arXiv},
  keywords = {obsLitNote},
  note = {Comment: 13 pages, 5 figures},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Taipalus24VectorDatabaseManagement.pdf}
}

@misc{Junco24fineTuningAIpower,
  title = {The {{Power Of Fine-Tuning In Generative AI}}},
  shorttitle = {Council {{Post}}},
  author = {Junco, Pablo},
  year = {2023},
  month = oct,
  journal = {Forbes Council Post},
  url = {https://www.forbes.com/sites/forbestechcouncil/2023/10/10/the-power-of-fine-tuning-in-generative-ai/},
  urldate = {2024-04-10},
  abstract = {In conclusion, fine-tuning within the realm of Generative AI stands as a transformative force for both businesses and developers.},
  chapter = {Innovation},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Junco24fineTuningAIpower.pdf}
}

@misc{OpenAI24pricing,
  title = {Pricing},
  author = {{OpenAI}},
  year = {2024},
  month = apr,
  journal = {OpenAI},
  url = {https://openai.com/pricing},
  urldate = {2024-04-10},
  abstract = {Simple and flexible. Only pay for what you use.},
  langid = {american},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\OpenAI24pricing.html}
}

@misc{OpenAI24tokensWhatHowCount,
  title = {What Are Tokens and How to Count Them?},
  shorttitle = {What Are Tokens and How to Count Them?},
  author = {{OpenAI}},
  year = {2024},
  month = apr,
  journal = {OpenAI Help Center},
  url = {https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them},
  urldate = {2024-04-10},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\OpenAI24tokensWhatHowCount.pdf}
}

@misc{OpenAI24tokenizer,
  title = {Tokenizer},
  author = {{OpenAI}},
  year = {2024},
  month = apr,
  journal = {OpenAI},
  url = {https://platform.openai.com},
  urldate = {2024-04-10},
  abstract = {Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.},
  langid = {english},
  keywords = {obsLitNote},
  note = {OpenAI24tokenizer
\par
Fun website showing how OpenAI tokenizes. Shows how the words are split, how words w/ same spelling get different tokens (bear and bear, verb/noun, so POS parsing), prefixes and punctuation gets coded (codes are displayed, so I could try to look them up, and make a slide showing the difference.

For example:
\par
\begin{quotation}

\par
``She wanted to read `I can't' as a single word, but the tokenizer insisted on splitting it into three tokens: `I,' `can,' and `t'.'' 
\par
\end{quotation}

\par
See that GPT4, GPT3.5 has fewer tokens (37) than GPT3 (54) for this sentence:
\par
Here's another:
\par
\begin{quotation}

\par
"While hiking, we spotted a large brown bear lumbering through the trees. We had to bear right at the next fork to avoid its path."
\par
\end{quotation}

\par
There, bear and bear get different tokens. ~Interesting how ``While is grouped.
\par
I could make some slides with OpenAI's free tokenizer package, \href{https://github.com/openai/tiktoken?tab=readme-ov-file}{tiktoken}. ~There's a \href{zotero://select/library/items/H24M82K9}{python notebook} for this in OpenAI's \href{https://github.com/openai/openai-cookbook/tree/main}{openai-cookbook} GitHub repo.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\OpenAI24tokenizer.pdf}
}

@misc{James23ChatGPT4howWorks,
  title = {Chat-{{GPT4}}: {{How}} It {{Works}} -- {{Explained}} for {{Experts}}},
  shorttitle = {Chat-{{GPT4}}},
  author = {{James}},
  year = {2023},
  month = mar,
  journal = {London Data Consulting},
  url = {https://london-data-consulting.com/chat-gpt4-how-it-works-explained-for-experts/},
  urldate = {2024-04-11},
  abstract = {The recent advancements in artificial intelligence and natural language processing have led to the development of powerful and sophisticated language models.},
  langid = {american},
  keywords = {obsLitNote},
  note = {James23ChatGPT4howWorks
\par
A little about how it works, but mostly about what it can do.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\James23ChatGPT4howWorks.pdf}
}

@misc{OpenAI24ragAndSemSrchGPT,
  title = {Retrieval {{Augmented Generation}} ({{RAG}}) and {{Semantic Search}} for {{GPTs}}},
  author = {{OpenAI}},
  year = {2024},
  month = apr,
  journal = {OpenAI Help Center},
  url = {https://help.openai.com/en/articles/8868588-retrieval-augmented-generation-rag-and-semantic-search-for-gpts},
  urldate = {2024-04-11},
  abstract = {Learn about RAG and how it is useful to GPT builders},
  langid = {english},
  keywords = {obsLitNote},
  note = {OpenAI24ragAndSemSrchGPT
\par
Link to both RAG and semantic search. ~Very simple, maybe nice graphic?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\OpenAI24ragAndSemSrchGPT.pdf}
}

@misc{OpenAI24gptsVsAssistants,
  title = {{{GPTs}} vs {{Assistants}}},
  author = {{OpenAI}},
  journal = {OpenAI Help Center},
  url = {https://help.openai.com/en/articles/8673914-gpts-vs-assistants},
  urldate = {2024-04-11},
  abstract = {Comparison of GPTs in ChatGPT to Assistants in the OpenAI API.},
  langid = {english},
  keywords = {obsLitNote},
  note = {OpenAI24gptsVsAssistants
\par
A catelog distinction for OpenAI. ~Assistant vs. chatGPT. ~
\par
Try an OpenAI assistant, which can do math, and which has a simple playground interface. ~See the project part of ``Gen AI Talk ideas.md'' for the URLs in this sentence.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\OpenAI24gptsVsAssistants.pdf}
}

@misc{Konen24StyleVectorsSteering,
  title = {Style {{Vectors}} for {{Steering Generative Large Language Model}}},
  author = {Konen, Kai and Jentzsch, Sophie and Diallo, Diaoul{\'e} and Sch{\"u}tt, Peer and Bensch, Oliver and Baff, Roxanne El and Opitz, Dominik and Hecking, Tobias},
  year = {2024},
  month = feb,
  number = {arXiv:2402.01618},
  eprint = {2402.01618},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.01618},
  url = {http://arxiv.org/abs/2402.01618},
  urldate = {2024-04-11},
  abstract = {This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.},
  archiveprefix = {arXiv},
  keywords = {obsLitNote},
  note = {Konen24StyleVectorsSteering
\par
An apparently automaticish way of steering, something other than prompt engineering?
\par
Comment: Will be published as findings paper at EACL2024 - 18th Conference of the European Chapter of the Association for Computational Linguistics},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Konen24StyleVectorsSteering.pdf}
}

@misc{Se23creatorsTimeGPT,
  title = {Revolutionizing {{Time Series Forecasting}}: {{Interview}} with {{TimeGPT}}'s Creators},
  shorttitle = {Revolutionizing {{Time Series Forecasting}}},
  author = {Se, Ksenia and Spektor, Ian},
  year = {2023},
  month = sep,
  journal = {Turing Post},
  url = {https://www.turingpost.com/p/timegpt},
  urldate = {2024-04-11},
  abstract = {It's not an LLM! Azul Garza and Max Mergenthaler talk innovations, open-source, diversity, and pain of wrangling 100 billion data points},
  langid = {english},
  keywords = {obsLitNote},
  note = {Se23creatorsTimeGPT
\par
Creators of TimeGPT interviews. They're from Nixtla (see other lit note), where I don't think TimeGPT compared that well (in 2024, I think)
\par
But they'll explain its advantages, and tell about Nixtla.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Se23creatorsTimeGPT.pdf}
}

@misc{OpenAI24tiktoken,
  title = {Tiktoken},
  author = {{OpenAI}},
  year = {2024},
  month = apr,
  url = {https://github.com/openai/tiktoken},
  urldate = {2024-04-11},
  abstract = {tiktoken is a fast BPE tokeniser for use with OpenAI's models.},
  copyright = {MIT},
  howpublished = {OpenAI},
  keywords = {obsLitNote}
}

@misc{Karpathy24buildGPTtokenizer,
  title = {Let's Build the {{GPT Tokenizer}}},
  author = {{Andrej Karpathy}},
  year = {2024},
  month = feb,
  url = {https://www.youtube.com/watch?v=zduSFxRajkE},
  urldate = {2024-04-11},
  abstract = {The Tokenizer is a necessary and pervasive component of Large Language Models (LLMs), where it translates between strings and tokens (text chunks). Tokenizers are a completely separate stage of the LLM pipeline: they have their own training sets, training algorithms (Byte Pair Encoding), and after training implement two fundamental functions: encode() from strings to tokens, and decode() back from tokens to strings. In this lecture we build from scratch the Tokenizer used in the GPT series from OpenAI. In the process, we will see that a lot of weird behaviors and problems of LLMs actually trace back to tokenization. We'll go through a number of these issues, discuss why tokenization is at fault, and why someone out there ideally finds a way to delete this stage entirely. Chapters: 00:00:00 intro: Tokenization, GPT-2 paper, tokenization-related issues 00:05:50 tokenization by example in a Web UI (tiktokenizer) 00:14:56 strings in Python, Unicode code points 00:18:15 Unicode byte encodings, ASCII, UTF-8, UTF-16, UTF-32 00:22:47 daydreaming: deleting tokenization 00:23:50 Byte Pair Encoding (BPE) algorithm walkthrough 00:27:02 starting the implementation 00:28:35 counting consecutive pairs, finding most common pair 00:30:36 merging the most common pair 00:34:58 training the tokenizer: adding the while loop, compression ratio 00:39:20 tokenizer/LLM diagram: it is a completely separate stage 00:42:47 decoding tokens to strings 00:48:21 encoding strings to tokens 00:57:36 regex patterns to force splits across categories 01:11:38 tiktoken library intro, differences between GPT-2/GPT-4 regex 01:14:59 GPT-2 encoder.py released by OpenAI walkthrough 01:18:26 special tokens, tiktoken handling of, GPT-2/GPT-4 differences 01:25:28 minbpe exercise time! write your own GPT-4 tokenizer 01:28:42 sentencepiece library intro, used to train Llama 2 vocabulary 01:43:27 how to set vocabulary set? revisiting gpt.py transformer 01:48:11 training new tokens, example of prompt compression 01:49:58 multimodal [image, video, audio] tokenization with vector quantization 01:51:41 revisiting and explaining the quirks of LLM tokenization 02:10:20 final recommendations 02:12:50 ??? :) Exercises: - Advised flow: reference this document and try to implement the steps before I give away the partial solutions in the video. The full solutions if you're getting stuck are in the minbpe code https://github.com/karpathy/minbpe/bl... Links: - Google colab for the video: https://colab.research.google.com/dri... - GitHub repo for the video: minBPE https://github.com/karpathy/minbpe - Playlist of the whole Zero to Hero series so far: ~~~{$\bullet~$}The~spelled-out~intro~to~neural~netwo...~~ - our Discord channel: ~~/~discord~~ - my Twitter: ~~/~karpathy~~ Supplementary links: - tiktokenizer https://tiktokenizer.vercel.app - tiktoken from OpenAI: https://github.com/openai/tiktoken - sentencepiece from Google https://github.com/google/sentencepiece},
  keywords = {obsLitNote}
}

@misc{Obeng24textToSQLpinterest,
  title = {How We Built {{Text-to-SQL}} at {{Pinterest}}},
  author = {Obeng, Adam},
  year = {2024},
  month = apr,
  journal = {Pinterest Engineering Blog},
  url = {https://medium.com/pinterest-engineering/how-we-built-text-to-sql-at-pinterest-30bad30dabff},
  urldate = {2024-04-11},
  abstract = {Writing queries to solve analytical problems is the core task for Pinterest's data users. However, finding the right data and translating an analytical problem into correct and efficient SQL code can be challenging tasks in a fast-paced environment with significant amounts of data spread across different domains. We took the rise in availability of Large Language Models (LLMs) as an opportunity to explore whether w e could assist our data users with this task by developing a Text-to-SQL feature which transforms these analytical questions directly into code.},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Obeng24textToSQLpinterest.pdf}
}

@misc{Liang24MonitoringAIModifiedContent,
  title = {Monitoring {{AI-Modified Content}} at {{Scale}}: {{A Case Study}} on the {{Impact}} of {{ChatGPT}} on {{AI Conference Peer Reviews}}},
  shorttitle = {Monitoring {{AI-Modified Content}} at {{Scale}}},
  author = {Liang, Weixin and Izzo, Zachary and Zhang, Yaohui and Lepp, Haley and Cao, Hancheng and Zhao, Xuandong and Chen, Lingjiao and Ye, Haotian and Liu, Sheng and Huang, Zhi and McFarland, Daniel A. and Zou, James Y.},
  year = {2024},
  month = mar,
  number = {arXiv:2403.07183},
  eprint = {2403.07183},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.07183},
  url = {http://arxiv.org/abs/2403.07183},
  urldate = {2024-04-11},
  abstract = {We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5\% and 16.9\% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices.},
  archiveprefix = {arXiv},
  note = {Liang24chatGPTpeerReview
\par
One source for (Singh Chawla, 2024), which has a snappy headline. Also, figure 1 shows a shift in adjective use just as LLMs hit the scene. ~A perfect follow next slide to (Drum, 2024)
\par
From the abstract
\par
\begin{itemize}

\item Between 6.5\% and 16.9\% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs
\item estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals.

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Liang24chatGPTpeerReview.pdf}
}

@article{SinghChawla24chatGPTpeerReviewNature,
  title = {Is {{ChatGPT}} Corrupting Peer Review? {{Telltale}} Words Hint at {{AI}} Use},
  shorttitle = {Is {{ChatGPT}} Corrupting Peer Review?},
  author = {Singh Chawla, Dalmeet},
  year = {2024},
  month = apr,
  journal = {Nature},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-024-01051-2},
  url = {https://www.nature.com/articles/d41586-024-01051-2},
  urldate = {2024-04-11},
  abstract = {A study of review reports identifies dozens of adjectives that could indicate text written with the help of chatbots.},
  copyright = {2024 Springer Nature Limited},
  langid = {english},
  keywords = {obsLitNote},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: News\\
Subject\_term: Machine learning, Publishing, Peer review},
  note = {SinghChawla24chatGPTpeerReviewNature
\par
Detection of genAI written peer reviews. ~Source article is (Liang et al., 2024), but headline and screenshot of this article is better.
\par
In talk, this image and/or details from (Liang et al., 2024) go after (Drum, 2024)}
}

@misc{Hofferber24noteTakingAEobsidian,
  title = {The {{AI Note Taking Powerhouse}} - {{Obsidian}}},
  author = {Hofferber, Ben},
  year = {2024},
  month = mar,
  url = {https://www.youtube.com/watch?v=2Yr0H8nudgE},
  urldate = {2024-04-11},
  abstract = {Today we dive into how to enhance our note taking experience using modern LLMs like ChatGPT, Gemini, and Claude. Obsidian is uniquely positioned to enable you to enhance your notes with very little overhead and at no cost (outside what AI tools you pay for). Check out how to get things setup and get started empowering yourself with a unique note taking setup! Show Links: Obsidian: https://obsidian.md/ File over App by Steph Ango: https://stephango.com/file-over-app Matter: https://hq.getmatter.com/ Follow us on Stable Discussion: https://blog.stablediscussion.com/},
  keywords = {obsLitNote}
}

@misc{Reid24GeminiUnlockingMultimodal,
  title = {Gemini 1.5: {{Unlocking}} Multimodal Understanding across Millions of Tokens of Context},
  shorttitle = {Gemini 1.5},
  author = {Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and Antonoglou, Ioannis and Anil, Rohan and Borgeaud, Sebastian and Dai, Andrew and Millican, Katie and Dyer, Ethan and Glaese, Mia and Sottiaux, Thibault and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Molloy, James and Chen, Jilin and Isard, Michael and Barham, Paul and Hennigan, Tom and McIlroy, Ross and Johnson, Melvin and Schalkwyk, Johan and Collins, Eli and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Meyer, Clemens and Thornton, Gregory and Yang, Zhen and Michalewski, Henryk and Abbas, Zaheer and Schucher, Nathan and Anand, Ankesh and Ives, Richard and Keeling, James and Lenc, Karel and Haykal, Salem and Shakeri, Siamak and Shyam, Pranav and Chowdhery, Aakanksha and Ring, Roman and Spencer, Stephen and Sezener, Eren and Vilnis, Luke and Chang, Oscar and Morioka, Nobuyuki and Tucker, George and Zheng, Ce and Woodman, Oliver and Attaluri, Nithya and Kocisky, Tomas and Eltyshev, Evgenii and Chen, Xi and Chung, Timothy and Selo, Vittorio and Brahma, Siddhartha and Georgiev, Petko and Slone, Ambrose and Zhu, Zhenkai and Lottes, James and Qiao, Siyuan and Caine, Ben and Riedel, Sebastian and Tomala, Alex and Chadwick, Martin and Love, Juliette and Choy, Peter and Mittal, Sid and Houlsby, Neil and Tang, Yunhao and Lamm, Matthew and Bai, Libin and Zhang, Qiao and He, Luheng and Cheng, Yong and Humphreys, Peter and Li, Yujia and Brin, Sergey and Cassirer, Albin and Miao, Yingjie and Zilka, Lukas and Tobin, Taylor and Xu, Kelvin and Proleev, Lev and Sohn, Daniel and Magni, Alberto and Hendricks, Lisa Anne and Gao, Isabel and Onta{\~n}{\'o}n, Santiago and Bunyan, Oskar and Byrd, Nathan and Sharma, Abhanshu and Zhang, Biao and Pinto, Mario and Sinha, Rishika and Mehta, Harsh and Jia, Dawei and Caelles, Sergi and Webson, Albert and Morris, Alex and Roelofs, Becca and Ding, Yifan and Strudel, Robin and Xiong, Xuehan and Ritter, Marvin and Dehghani, Mostafa and Chaabouni, Rahma and Karmarkar, Abhijit and Lai, Guangda and Mentzer, Fabian and Xu, Bibo and Li, YaGuang and Zhang, Yujing and Paine, Tom Le and Goldin, Alex and Neyshabur, Behnam and Baumli, Kate and Levskaya, Anselm and Laskin, Michael and Jia, Wenhao and Rae, Jack W. and Xiao, Kefan and He, Antoine and Giordano, Skye and Yagati, Lakshman and Lespiau, Jean-Baptiste and Natsev, Paul and Ganapathy, Sanjay and Liu, Fangyu and Martins, Danilo and Chen, Nanxin and Xu, Yunhan and Barnes, Megan and May, Rhys and Vezer, Arpi and Oh, Junhyuk and Franko, Ken and Bridgers, Sophie and Zhao, Ruizhe and Wu, Boxi and Mustafa, Basil and Sechrist, Sean and Parisotto, Emilio and Pillai, Thanumalayan Sankaranarayana and Larkin, Chris and Gu, Chenjie and Sorokin, Christina and Krikun, Maxim and Guseynov, Alexey and Landon, Jessica and Datta, Romina and Pritzel, Alexander and Thacker, Phoebe and Yang, Fan and Hui, Kevin and Hauth, Anja and Yeh, Chih-Kuan and Barker, David and {Mao-Jones}, Justin and Austin, Sophia and Sheahan, Hannah and Schuh, Parker and Svensson, James and Jain, Rohan and Ramasesh, Vinay and Briukhov, Anton and Chung, Da-Woon and {von Glehn}, Tamara and Butterfield, Christina and Jhakra, Priya and Wiethoff, Matthew and Frye, Justin and Grimstad, Jordan and Changpinyo, Beer and Lan, Charline Le and Bortsova, Anna and Wu, Yonghui and Voigtlaender, Paul and Sainath, Tara and Smith, Charlotte and Hawkins, Will and Cao, Kris and Besley, James and Srinivasan, Srivatsan and Omernick, Mark and Gaffney, Colin and Surita, Gabriela and Burnell, Ryan and Damoc, Bogdan and Ahn, Junwhan and Brock, Andrew and Pajarskas, Mantas and Petrushkina, Anastasia and Noury, Seb and Blanco, Lorenzo and Swersky, Kevin and Ahuja, Arun and Avrahami, Thi and Misra, Vedant and {de Liedekerke}, Raoul and Iinuma, Mariko and Polozov, Alex and York, Sarah and van den Driessche, George and Michel, Paul and Chiu, Justin and Blevins, Rory and Gleicher, Zach and Recasens, Adri{\`a} and Rrustemi, Alban and Gribovskaya, Elena and Roy, Aurko and Gworek, Wiktor and Arnold, S{\'e}b and Lee, Lisa and {Lee-Thorp}, James and Maggioni, Marcello and Piqueras, Enrique and Badola, Kartikeya and Vikram, Sharad and Gonzalez, Lucas and Baddepudi, Anirudh and Senter, Evan and Devlin, Jacob and Qin, James and Azzam, Michael and Trebacz, Maja and Polacek, Martin and Krishnakumar, Kashyap and Chang, Shuo-yiin and Tung, Matthew and Penchev, Ivo and Joshi, Rishabh and Olszewska, Kate and Muir, Carrie and Wirth, Mateo and Hartman, Ale Jakse and Newlan, Josh and Kashem, Sheleem and Bolina, Vijay and Dabir, Elahe and {van Amersfoort}, Joost and Ahmed, Zafarali and {Cobon-Kerr}, James and Kamath, Aishwarya and Hrafnkelsson, Arnar Mar and Hou, Le and Mackinnon, Ian and Frechette, Alexandre and Noland, Eric and Si, Xiance and Taropa, Emanuel and Li, Dong and Crone, Phil and Gulati, Anmol and Cevey, S{\'e}bastien and Adler, Jonas and Ma, Ada and Silver, David and Tokumine, Simon and Powell, Richard and Lee, Stephan and Chang, Michael and Hassan, Samer and Mincu, Diana and Yang, Antoine and Levine, Nir and Brennan, Jenny and Wang, Mingqiu and Hodkinson, Sarah and Zhao, Jeffrey and Lipschultz, Josh and Pope, Aedan and Chang, Michael B. and Li, Cheng and Shafey, Laurent El and Paganini, Michela and Douglas, Sholto and Bohnet, Bernd and Pardo, Fabio and Odoom, Seth and Rosca, Mihaela and dos Santos, Cicero Nogueira and Soparkar, Kedar and Guez, Arthur and Hudson, Tom and Hansen, Steven and Asawaroengchai, Chulayuth and Addanki, Ravi and Yu, Tianhe and Stokowiec, Wojciech and Khan, Mina and Gilmer, Justin and Lee, Jaehoon and Bostock, Carrie Grimes and Rong, Keran and Caton, Jonathan and Pejman, Pedram and Pavetic, Filip and Brown, Geoff and Sharma, Vivek and Lu{\v c}i{\'c}, Mario and Samuel, Rajkumar and Djolonga, Josip and Mandhane, Amol and Sj{\"o}sund, Lars Lowe and Buchatskaya, Elena and White, Elspeth and Clay, Natalie and Jiang, Jiepu and Lim, Hyeontaek and Hemsley, Ross and Labanowski, Jane and De Cao, Nicola and Steiner, David and Hashemi, Sayed Hadi and Austin, Jacob and Gergely, Anita and Blyth, Tim and Stanton, Joe and Shivakumar, Kaushik and Siddhant, Aditya and Andreassen, Anders and Araya, Carlos and Sethi, Nikhil and Shivanna, Rakesh and Hand, Steven and Bapna, Ankur and Khodaei, Ali and Miech, Antoine and Tanzer, Garrett and Swing, Andy and Thakoor, Shantanu and Pan, Zhufeng and Nado, Zachary and Winkler, Stephanie and Yu, Dian and Saleh, Mohammad and Maggiore, Loren and Barr, Iain and Giang, Minh and Kagohara, Thais and Danihelka, Ivo and Marathe, Amit and Feinberg, Vladimir and Elhawaty, Mohamed and Ghelani, Nimesh and Horgan, Dan and Miller, Helen and Walker, Lexi and Tanburn, Richard and Tariq, Mukarram and Shrivastava, Disha and Xia, Fei and Chiu, Chung-Cheng and Ashwood, Zoe and Baatarsukh, Khuslen and Samangooei, Sina and Alcober, Fred and Stjerngren, Axel and Komarek, Paul and Tsihlas, Katerina and Boral, Anudhyan and Comanescu, Ramona and Chen, Jeremy and Liu, Ruibo and Bloxwich, Dawn and Chen, Charlie and Sun, Yanhua and Feng, Fangxiaoyu and Mauger, Matthew and Dotiwalla, Xerxes and Hellendoorn, Vincent and Sharman, Michael and Zheng, Ivy and Haridasan, Krishna and {Barth-Maron}, Gabe and Swanson, Craig and Rogozi{\'n}ska, Dominika and Andreev, Alek and Rubenstein, Paul Kishan and Sang, Ruoxin and Hurt, Dan and Elsayed, Gamaleldin and Wang, Renshen and Lacey, Dave and Ili{\'c}, Anastasija and Zhao, Yao and Aroyo, Lora and Iwuanyanwu, Chimezie and Nikolaev, Vitaly and Lakshminarayanan, Balaji and Jazayeri, Sadegh and Kaufman, Rapha{\"e}l Lopez and Varadarajan, Mani and Tekur, Chetan and Fritz, Doug and Khalman, Misha and Reitter, David and Dasgupta, Kingshuk and Sarcar, Shourya and Ornduff, Tina and Snaider, Javier and Huot, Fantine and Jia, Johnson and Kemp, Rupert and Trdin, Nejc and Vijayakumar, Anitha and Kim, Lucy and Angermueller, Christof and Lao, Li and Liu, Tianqi and Zhang, Haibin and Engel, David and Greene, Somer and White, Ana{\"i}s and Austin, Jessica and Taylor, Lilly and Ashraf, Shereen and Liu, Dangyi and Georgaki, Maria and Cai, Irene and Kulizhskaya, Yana and Goenka, Sonam and Saeta, Brennan and Vodrahalli, Kiran and Frank, Christian and {de Cesare}, Dario and Robenek, Brona and Richardson, Harry and Alnahlawi, Mahmoud and Yew, Christopher and Ponnapalli, Priya and Tagliasacchi, Marco and Korchemniy, Alex and Kim, Yelin and Li, Dinghua and Rosgen, Bill and Ashwood, Zoe and Levin, Kyle and Wiesner, Jeremy and Banzal, Praseem and Srinivasan, Praveen and Yu, Hongkun and {\"U}nl{\"u}, {\c C}a{\u g}lar and Reid, David and Tung, Zora and Finchelstein, Daniel and Kumar, Ravin and Elisseeff, Andre and Huang, Jin and Zhang, Ming and Zhu, Rui and Aguilar, Ricardo and Gim{\'e}nez, Mai and Xia, Jiawei and Dousse, Olivier and Gierke, Willi and Yeganeh, Soheil Hassas and Yates, Damion and Jalan, Komal and Li, Lu and {Latorre-Chimoto}, Eri and Nguyen, Duc Dung and Durden, Ken and Kallakuri, Praveen and Liu, Yaxin and Johnson, Matthew and Tsai, Tomy and Talbert, Alice and Liu, Jasmine and Neitz, Alexander and Elkind, Chen and Selvi, Marco and Jasarevic, Mimi and Soares, Livio Baldini and Cui, Albert and Wang, Pidong and Wang, Alek Wenjiao and Ye, Xinyu and Kallarackal, Krystal and Loher, Lucia and Lam, Hoi and Broder, Josef and {Holtmann-Rice}, Dan and Martin, Nina and Ramadhana, Bramandia and Toyama, Daniel and Shukla, Mrinal and Basu, Sujoy and Mohan, Abhi and Fernando, Nick and Fiedel, Noah and Paterson, Kim and Li, Hui and Garg, Ankush and Park, Jane and Choi, DongHyun and Wu, Diane and Singh, Sankalp and Zhang, Zhishuai and Globerson, Amir and Yu, Lily and Carpenter, John and Quitry, F{\'e}lix de Chaumont and Radebaugh, Carey and Lin, Chu-Cheng and Tudor, Alex and Shroff, Prakash and Garmon, Drew and Du, Dayou and Vats, Neera and Lu, Han and Iqbal, Shariq and Yakubovich, Alex and Tripuraneni, Nilesh and Manyika, James and Qureshi, Haroon and Hua, Nan and Ngani, Christel and Raad, Maria Abi and Forbes, Hannah and Bulanova, Anna and Stanway, Jeff and Sundararajan, Mukund and Ungureanu, Victor and Bishop, Colton and Li, Yunjie and Venkatraman, Balaji and Li, Bo and Thornton, Chloe and Scellato, Salvatore and Gupta, Nishesh and Wang, Yicheng and Tenney, Ian and Wu, Xihui and Shenoy, Ashish and Carvajal, Gabriel and Wright, Diana Gage and Bariach, Ben and Xiao, Zhuyun and Hawkins, Peter and Dalmia, Sid and Farabet, Clement and Valenzuela, Pedro and Yuan, Quan and Welty, Chris and Agarwal, Ananth and Chen, Mia and Kim, Wooyeol and Hulse, Brice and Dukkipati, Nandita and Paszke, Adam and Bolt, Andrew and Davoodi, Elnaz and Choo, Kiam and Beattie, Jennifer and Prendki, Jennifer and Vashisht, Harsha and {Santamaria-Fernandez}, Rebeca and Cobo, Luis C. and Wilkiewicz, Jarek and Madras, David and Elqursh, Ali and Uy, Grant and Ramirez, Kevin and Harvey, Matt and Liechty, Tyler and Zen, Heiga and Seibert, Jeff and Hu, Clara Huiyi and Elhawaty, Mohamed and Khorlin, Andrey and Le, Maigo and Aharoni, Asaf and Li, Megan and Wang, Lily and Kumar, Sandeep and Lince, Alejandro and Casagrande, Norman and Hoover, Jay and Badawy, Dalia El and Soergel, David and Vnukov, Denis and Miecnikowski, Matt and Simsa, Jiri and Koop, Anna and Kumar, Praveen and Sellam, Thibault and Vlasic, Daniel and Daruki, Samira and Shabat, Nir and Zhang, John and Su, Guolong and Zhang, Jiageng and Liu, Jeremiah and Sun, Yi and Palmer, Evan and Ghaffarkhah, Alireza and Xiong, Xi and Cotruta, Victor and Fink, Michael and Dixon, Lucas and Sreevatsa, Ashwin and Goedeckemeyer, Adrian and Dimitriev, Alek and Jafari, Mohsen and Crocker, Remi and FitzGerald, Nicholas and Kumar, Aviral and Ghemawat, Sanjay and Philips, Ivan and Liu, Frederick and Liang, Yannie and Sterneck, Rachel and Repina, Alena and Wu, Marcus and Knight, Laura and Georgiev, Marin and Lee, Hyo and Askham, Harry and Chakladar, Abhishek and Louis, Annie and Crous, Carl and Cate, Hardie and Petrova, Dessie and Quinn, Michael and {Owusu-Afriyie}, Denese and Singhal, Achintya and Wei, Nan and Kim, Solomon and Vincent, Damien and Nasr, Milad and {Choquette-Choo}, Christopher A. and Tojo, Reiko and Lu, Shawn and Casas, Diego de Las and Cheng, Yuchung and Bolukbasi, Tolga and Lee, Katherine and Fatehi, Saaber and Ananthanarayanan, Rajagopal and Patel, Miteyan and Kaed, Charbel and Li, Jing and Sygnowski, Jakub and Belle, Shreyas Rammohan and Chen, Zhe and Konzelmann, Jaclyn and P{\~o}der, Siim and Garg, Roopal and Koverkathu, Vinod and Brown, Adam and Dyer, Chris and Liu, Rosanne and Nova, Azade and Xu, Jun and Petrov, Slav and Hassabis, Demis and Kavukcuoglu, Koray and Dean, Jeffrey and Vinyals, Oriol},
  year = {2024},
  month = mar,
  number = {arXiv:2403.05530},
  eprint = {2403.05530},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.05530},
  url = {http://arxiv.org/abs/2403.05530},
  urldate = {2024-04-12},
  abstract = {In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval ({$>$}99\%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.},
  archiveprefix = {arXiv},
  note = {IReid24GeminiMultimodalMlnsContext
\par
Is this why gemini has such a hug context, speculates \href{https://youtu.be/QASOCG5QLUM?t=600}{this guy}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Reid24GeminiUnlockingMultimodal.pdf}
}

@misc{Rodriguez24Edge386YiChnLLM,
  type = {Substack Newsletter},
  title = {Edge 386: {{Inside Yi}}, 01's {{Model Leading}} the {{Chinese LLM Movement}}},
  shorttitle = {Edge 386},
  author = {Rodriguez, Jesus},
  year = {2024},
  month = apr,
  journal = {TheSequence},
  url = {https://thesequence.substack.com/p/edge-386-inside-yi-01s-model-leading?publication_id=54309&utm_campaign=email-post-title&r=haqky&utm_medium=email},
  urldate = {2024-04-12},
  abstract = {Yi has achieved remarkable performance across language and image tasks.},
  keywords = {obsLitNote},
  note = {Rodriguez24Edge386YiChnLLM
\par
Leader of open source models is Chinese, it's not small but gigantic model. ~Guy who worked at google and MS is a founder.Y}
}

@misc{Rein23GPQAGraduateLevelGoogleProof,
  title = {{{GPQA}}: {{A Graduate-Level Google-Proof Q}}\&{{A Benchmark}}},
  shorttitle = {{{GPQA}}},
  author = {Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R.},
  year = {2023},
  month = nov,
  number = {arXiv:2311.12022},
  eprint = {2311.12022},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.12022},
  url = {http://arxiv.org/abs/2311.12022},
  urldate = {2024-04-12},
  abstract = {We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65\% accuracy (74\% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34\% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are "Google-proof"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39\% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.},
  archiveprefix = {arXiv},
  note = {Rein23GPQAbenchmrkGradLevGglePrf
\par
An AI benchmark, so tough that PhD expet types get only 65\% right; high-skilled experts w/ 30 mins on the web get 34\%; GPT4 got 39\% in 2023.
\par
\textbf{TODO}: One of the leaderboards in this group of web pages I just stored in zotero (evening 4/11/14) uses this one, I think. ~Lood for the one that has a pdf of the actual leader board scores for toda.
\par
Would be interesting to look at these questions myself.
\par
Comment: 28 pages, 5 figures, 7 tables},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Rein23GPQAGraduateLevelGoogleProof.pdf}
}

@misc{Jain24LiveCodeBenchHolisticContamination,
  title = {{{LiveCodeBench}}: {{Holistic}} and {{Contamination Free Evaluation}} of {{Large Language Models}} for {{Code}}},
  shorttitle = {{{LiveCodeBench}}},
  author = {Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and {Solar-Lezama}, Armando and Sen, Koushik and Stoica, Ion},
  year = {2024},
  month = mar,
  number = {arXiv:2403.07974},
  eprint = {2403.07974},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.07974},
  url = {http://arxiv.org/abs/2403.07974},
  urldate = {2024-04-12},
  abstract = {Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and February 2024. We have evaluated 9 base LLMs and 20 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model},
  archiveprefix = {arXiv},
  note = {Jain24LiveCodeBenchHolisticNoCntam
\par
The paper describing the benchmark. ~Corresponding leaderboard and easy-to-read summary at (Jain, 2024)
\par
Comment: Website - https://livecodebench.github.io/},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Jain24LiveCodeBenchHolisticNoCntam.pdf}
}

@misc{Jain24liveCodeBenchLeaderboard,
  title = {{{LiveCodeBench Leaderboard}}},
  author = {Jain, Namen},
  year = {2024},
  url = {https://livecodebench.github.io/leaderboard.html},
  urldate = {2024-04-12},
  keywords = {obsLitNote},
  note = {Jain24liveCodeBenchLeaderboard
\par
The leaderboard for (Jain et al., 2024)},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Jain24liveCodeBenchLeaderboard.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Jain24LiveCodeBenchLeaderboardScrnShot.pdf}
}

@misc{Epoch23MachineLearningTrends,
  title = {Key {{Trends}} and {{Figures}} in {{Machine Learning}}},
  author = {Epoch},
  year = {2023},
  month = apr,
  journal = {Epoch},
  url = {https://epochai.org/trends},
  urldate = {2024-04-12},
  abstract = {Explore Epoch's trends section of key numbers and data visualizations in Artificial Intelligence and machine learning, showcasing the change and growth in AI over time.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Epoch23MachineLearningTrends
\par
Great trend graphs, and numbers, but I can't tell if it's still up to date in 4/2024.
\par
Lots of articles, and the whole website should be looked into.
\par
\section{Most notable}

\begin{itemize}

\item year most public high-quality human data will be used in some training run: 2024!
\item 
\par
will we run out of training data? ~``Yes?'' ~I need to read the linked-to paper
\par
\begin{itemize}

\item there's a lot of contradictory stuff about this on this page. ~Maye the paper will make sense of it?
\item maybe this page was generated by AI?
\item largest training set known (DBRX): 9 trillion words
\item internet data: 100T words
\item etc.

\end{itemize}

\item For optimal results, need 20 tokens/param (page says it's chinchilla scaling laws, and to see Hoffmann 2022, which doesn't give this \# in its abstract).
\item Gemini Ultra cost \textbf{\$630M (}all costs) to develop. ~It likely used the most train compute too: (Epoch, 2024)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Epoch23MachineLearningTrends.html}
}

@misc{Epoch24trainComputeVsTime,
  title = {Training {{Compute}} of {{Notable}} Machine Learning {{Systems Over Time}}},
  author = {Epoch},
  year = {2024},
  journal = {Epoch},
  url = {https://epochai.org/data/epochdb},
  urldate = {2024-04-12},
  abstract = {Explore Epoch's database of notable machine learning systems and dive into our interactive visualization to inform your research, work and education. Learn about the training compute, parameters, and training dataset sizes of frontier AI systems.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Epoch24trainComputeVsTime
\par
Exponential growth in train compute time, with sharp knee in 2010 at the start of deep learning. ~Pre, it was {\textasciitilde}1.4X/yr; not it's about 4.1X/yr.
\par
Note that Geminit Ultra is the most expensive in dollars too: (Epoch, 2023)
\par
Can downoad an \textbf{svg of this graph} at the Downloads button.
\par
\textbf{TODO}: put this in the graphics to borrow pae},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Epoch24trainComputeVsTime.pdf}
}

@misc{Davidson23AICapabilitiesCan,
  title = {{{AI}} Capabilities Can Be Significantly Improved without Expensive Retraining},
  author = {Davidson, Tom and Denain, Jean-Stanislas and Villalobos, Pablo and Bas, Guillem},
  year = {2023},
  month = dec,
  number = {arXiv:2312.07413},
  eprint = {2312.07413},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.07413},
  url = {http://arxiv.org/abs/2312.07413},
  urldate = {2024-04-12},
  abstract = {State-of-the-art AI systems can be significantly improved without expensive retraining via "post-training enhancements"-techniques applied after initial training like fine-tuning the system to use a web browser. We review recent post-training enhancements, categorizing them into five types: tool-use, prompting methods, scaffolding, solution selection, and data generation. Different enhancements improve performance on different tasks, making it hard to compare their significance. So we translate improvements from different enhancements into a common currency, the compute-equivalent gain: how much additional training compute would be needed to improve performance by the same amount as the enhancement. Our non-experimental work shows that post-training enhancements have significant benefits: most surveyed enhancements improve benchmark performance by more than a 5x increase in training compute, some by more than 20x. Post-training enhancements are relatively cheap to develop: fine-tuning costs are typically {$<$}1\% of the original training cost. Governing the development of capable post-training enhancements may be challenging because frontier models could be enhanced by a wide range of actors.},
  archiveprefix = {arXiv},
  note = {Davidson23improveAInoExpensTrain
\par
Fine-Tuning typically take {$<$} 1\% of original training cost, and the resulting performance gains are far cheaper than what it would have taken to invest in more compute in the original training run: 5-10X better for the same investment.
\par
Conclusion is that non-state actors with small budgets will be able to generate very sophisticated algorithms (Didn't the EU AI Act (Yamimova and Ojamo, 2024), limit compute power somehow to prevent AI terrorism? ~I read/heard this on a video or article somewhere but didin't follow up on it.
\par
TODO: What is different about the fine-tuning that couldn't have been done during the initial train? ~From the table discussed below, it seems to be that the 1st trainigs are general purpose, and the fine-tunings are special purpose.
\par
\subsubsection{attached webpage capture from \href{https://epochai.org/blog/ai-capabilities-can-be-significantly-improved-without-expensive-retraining}{here}, which references his paper has }

\begin{itemize}

\item a nice table showing fine-tuning algorithm options in Dec. 2023 (I htink)
\item some confusing graphs

\end{itemize}

\par
Comment: 30 pages, 24 figures},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Davidson23improveAInoExpensTrain.pdf}
}

@misc{Owen23howPredictableLLMperf,
  title = {How {{Predictable Is Language Model Benchmark Performance}}?},
  author = {Owen, David},
  year = {2023},
  month = jun,
  journal = {Epoch},
  url = {https://epochai.org/blog/how-predictable-is-language-model-benchmark-performance},
  urldate = {2024-04-12},
  abstract = {We investigate large language model performance across five orders of magnitude of compute scaling, finding that compute-focused extrapolations are a promising way to forecast AI capabilities.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Owen23howPredictableLLMperf
\par
Could be a good graph showing how you must vastly increase compute power to incease performanc. ~
\par
Y axis on top graph (``performance') is a little hard to explain (can I explain to to myself?). Graphs below are expressed directly in some kind of error, but I'm not sure what a zeor on the y axis means on them.
\par
The do show a couple tasks where there is a sharp improvement in performance with (exponential) increase in compute.
\par
There might be better graphs for a presentation, but these are interesting.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Owen23howPredictableLLMperf.pdf}
}

@misc{Ho24algoProgressLLM,
  title = {Algorithmic {{Progress}} in {{Language Models}}},
  author = {Ho, Anson},
  year = {2024},
  month = mar,
  journal = {Epoch},
  url = {https://epochai.org/blog/algorithmic-progress-in-language-models},
  urldate = {2024-04-12},
  abstract = {We study how algorithmic improvements and increases in computational power have improved the performance of language models from 2014 to 2024. We find that the progress from new algorithms surpasses what we'd expect from merely increasing our computing resources, occurring at a pace equivalent to doubling computational power every 5 to 14 months.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Ho24algoProgressLLM
\par
I can't quite make sense of this page/paper. ~It seems to say that both algorithms are important to the progress of LLM performance, and that they are insignificant recently, compared to the gains due to increased compute (in spite of Fig. 3, which shows the opposite, AFIK).
\par
\emph{Anyway}{\dots} 
\par
\begin{itemize}

\item Transformers are estimated to have advanced progress by 2 years
\item The Chinchilla scaling laws have improved it by 8-16 months. ~AFIK these seem to be about optimal profits if you are paying for both inference and training. ~You get more profit if you have the right amount of training data?
\item TODO: figure this out.

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ho24algoProgressLLM.html}
}

@misc{Villalobos22WillWeRun,
  title = {Will We Run out of Data? {{An}} Analysis of the Limits of Scaling Datasets in {{Machine Learning}}},
  shorttitle = {Will We Run out of Data?},
  author = {Villalobos, Pablo and Sevilla, Jaime and Heim, Lennart and Besiroglu, Tamay and Hobbhahn, Marius and Ho, Anson},
  year = {2022},
  month = oct,
  number = {arXiv:2211.04325},
  eprint = {2211.04325},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2211.04325},
  urldate = {2024-04-12},
  abstract = {We analyze the growth of dataset sizes used in machine learning for natural language processing and computer vision, and extrapolate these using two methods; using the historical growth rate and estimating the compute-optimal dataset size for future predicted compute budgets. We investigate the growth in data usage by estimating the total stock of unlabeled data available on the internet over the coming decades. Our analysis indicates that the stock of high-quality language data will be exhausted soon; likely before 2026. By contrast, the stock of low-quality language data and image data will be exhausted only much later; between 2030 and 2050 (for low-quality language) and between 2030 and 2060 (for images). Our work suggests that the current trend of ever-growing ML models that rely on enormous datasets might slow down if data efficiency is not drastically improved or new sources of data become available.},
  archiveprefix = {arXiv},
  note = {Villalobos22outOfDataLLM
\par
Yes, of high quality text data, by 2026.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Villalobos22WillWeRun.pdf}
}

@misc{Erdil24optimAllocInfrncTrn,
  title = {Optimally {{Allocating Compute Between Inference}} and {{Training}}},
  author = {Erdil, Ege},
  year = {2024},
  month = mar,
  journal = {Epoch},
  url = {https://epochai.org/blog/optimally-allocating-compute-between-inference-and-training},
  urldate = {2024-04-12},
  abstract = {If it is feasible to trade off inference and training compute, we find that it is optimal for AI labs to spend similar amounts on training and inference.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Erdil24optimAllocInfrncTrn
\par
This may explain why Chinchilla scaling law is an advancement in AI performance. ~Also, as an often asked tidbit, that training tokens are about 3X more expensive than generated inference tokens (guestimated from a Sam Altman statement about tokens/year). ~So you can compare the cost of yearly train and yearly inference operation.
\par
Also, a table of ways to cut inference cost to improve performance of the given trained model. ~This may explain why people have declared that GPT3 or whatever was undertrained.
\par
I should probably read this.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Erdil24optimAllocInfrncTrn.html}
}

@misc{Lottick19EnergyUsageReports,
  title = {Energy {{Usage Reports}}: {{Environmental}} Awareness as Part of Algorithmic Accountability},
  shorttitle = {Energy {{Usage Reports}}},
  author = {Lottick, Kadan and Susai, Silvia and Friedler, Sorelle A. and Wilson, Jonathan P.},
  year = {2019},
  month = dec,
  number = {arXiv:1911.08354},
  eprint = {1911.08354},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.08354},
  url = {http://arxiv.org/abs/1911.08354},
  urldate = {2024-04-12},
  abstract = {The carbon footprint of algorithms must be measured and transparently reported so computer scientists can take an honest and active role in environmental sustainability. In this paper, we take analyses usually applied at the industrial level and make them accessible for individual computer science researchers with an easy-to-use Python package. Localizing to the energy mixture of the electrical power grid, we make the conversion from energy usage to CO2 emissions, in addition to contextualizing these results with more human-understandable benchmarks such as automobile miles driven. We also include comparisons with energy mixtures employed in electrical grids around the world. We propose including these automatically-generated Energy Usage Reports as part of standard algorithmic accountability practices, and demonstrate the use of these reports as part of model-choice in a machine learning context.},
  archiveprefix = {arXiv},
  keywords = {obsLitNote},
  note = {Comment: Workshop on Tackling Climate Change with Machine Learning at NeurIPS 2019},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lottick19EnergyUsageReports.pdf}
}

@misc{Henderson22SystematicReportingEnergy,
  title = {Towards the {{Systematic Reporting}} of the {{Energy}} and {{Carbon Footprints}} of {{Machine Learning}}},
  author = {Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle},
  year = {2022},
  month = nov,
  number = {arXiv:2002.05651},
  eprint = {2002.05651},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.05651},
  url = {http://arxiv.org/abs/2002.05651},
  urldate = {2024-04-12},
  abstract = {Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.},
  archiveprefix = {arXiv},
  note = {Henderson22energyCarbonFtPrntML
\par
Comment: Published in JMLR: https://jmlr.org/papers/v21/20-312.html},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Henderson22SystematicReportingEnergy.pdf}
}

@misc{Pandey24beyondChinchillaOptScalel,
  title = {{{MosaicML Announces Beyond Chinchilla-Optimal}} for {{LLM Scaling Laws}} in {{Inference}}},
  author = {Pandey, Mohit},
  year = {2024},
  month = jan,
  journal = {Analytics India Magazine},
  url = {https://analyticsindiamag.com/mosaicml-announces-beyond-chinchilla-optimal-for-llm-scaling-laws-in-inference/},
  urldate = {2024-04-12},
  abstract = {MosaicML has unveiled their latest research, titled "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws.},
  langid = {american},
  keywords = {obsLitNote},
  note = {Pandey24beyondChinchillaOptScalel
\par
Oh, Chinchilla is only a trade between training tokens and training tokens (I think). ~The proposal here is to also include inference costs.
\par
Note that Ambitious student(?) plants to train LLamma2 (1.1T params) on 3T tokens, about the ratio estimated from a Sam Altman statement
\par
\textbf{Link all this up! and correct my earlier chinchilla misunderstanding.}
\par
The paper this article talks about is: (Sardana and Frankle, 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Pandey24beyondChinchillaOptScalel.pdf}
}

@misc{Sardana23ChinchillaOptimalAccountingInference,
  title = {Beyond {{Chinchilla-Optimal}}: {{Accounting}} for {{Inference}} in {{Language Model Scaling Laws}}},
  shorttitle = {Beyond {{Chinchilla-Optimal}}},
  author = {Sardana, Nikhil and Frankle, Jonathan},
  year = {2023},
  month = dec,
  number = {arXiv:2401.00448},
  eprint = {2401.00448},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.00448},
  url = {http://arxiv.org/abs/2401.00448},
  urldate = {2024-04-12},
  abstract = {Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular DeepMind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand ({\textasciitilde}1B requests) should train models smaller and longer than Chinchilla-optimal.},
  archiveprefix = {arXiv},
  note = {Sardana23beyondChinchillaOptInference
\par
The paper for (Pandey, 2024)
\par
Comment: 8 pages, 2 figures, To appear in the 3rd NeurIPS Workshop on Efficient Natural Language and Speech Processing (ENLSP), 2023},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sardana23ChinchillaOptimalAccountingInference.pdf}
}

@misc{Rahman24TrackComputeIntensiveAI,
  title = {Tracking {{Compute-Intensive AI Models}}},
  author = {Rahman, Robi},
  year = {2024},
  month = apr,
  journal = {Epoch},
  url = {https://epochai.org/blog/tracking-compute-intensive-ai-models},
  urldate = {2024-04-12},
  abstract = {We present a dataset of 81 compute-intensive models, from AlphaGo to Gemini, developed across 18 countries, at the leading edge of scale and capabilities.},
  langid = {english},
  note = {Rahman24TrackComputeIntensiveAI
\par
Very interesting graphs showing how compute has increases and the number of big models have decreased. ~Most compute intensive models are in the US, but China is ``well represented'' or something like that.
\par
Could make for some good presentation slides.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Rahman24TrackComputeIntensiveAI.pdf}
}

@misc{Garza23TimeGPT1,
  title = {{{TimeGPT-1}}},
  author = {Garza, Azul and {Mergenthaler-Canseco}, Max},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03589},
  eprint = {2310.03589},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.03589},
  url = {http://arxiv.org/abs/2310.03589},
  urldate = {2024-04-12},
  abstract = {In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Garza23TimeGPT1
\par
A GPT TS forecaster that's not an LLM (Se and Spektor, 2023)
\par
Compared with the alternatives:
\par
\begin{itemize}

\item (Manokhin, 2023)
\item (Roque, 2024)

\end{itemize}

\par
Transformers for TS frcst in general
\par
\begin{itemize}

\item (Bentsen, 2023)
\item (Simple, 2023)
\item 

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Garza23TimeGPT1.pdf}
}

@article{Copeland24endWorstWallStAI,
  title = {The {{Worst Part}} of a {{Wall Street Career May Be Coming}} to an {{End}}},
  author = {Copeland, Rob},
  year = {2024},
  month = apr,
  journal = {The New York Times},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2024/04/10/business/investment-banking-jobs-artificial-intelligence.html},
  urldate = {2024-04-12},
  abstract = {Artificial intelligence tools can replace much of Wall Street's entry-level white-collar work, raising tough questions about the future of finance.},
  chapter = {Business},
  langid = {american},
  keywords = {obsLitNote},
  note = {Copeland24endWorstWallStAI
\par
More on investement bank automation e.g (\textbf{link to article about Chase AI efforts}). ~
\par
Lots of additional details like
\par
\begin{itemize}

\item  ``1000'' developers incorporating AI
\item job that took 10 minutes takes 10 seconds

\end{itemize}

\par
\textbf{TODO}: collect from pdf highlights and link.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Copeland24endWorstWallStAI.pdf}
}

@misc{Hofferber23openAIapiBetterChatGPT,
  title = {The {{OpenAI API}} Is Better than {{ChatGPT}}},
  author = {{Stable Discussion}},
  year = {2023},
  month = nov,
  url = {https://www.youtube.com/watch?v=SCeqWFjBGjE},
  urldate = {2024-04-13},
  abstract = {You don't get much control with ChatGPT and can do a lot more if you learn how to use the OpenAI API for GPT. While the Playground provides a similar experience, knowing how to use the API is an essential first step toward app creation. In this video we go over how to hit the API and why it's so much more powerful! OpenAI API Documentation: https://platform.openai.com/docs/api-... Insomnia API Tool: https://insomnia.rest Follow us on Stable Discussion: https://blog.stablediscussion.com},
  keywords = {obsLitNote},
  note = {Hofferber23openAIapiBetterChatGPT
\par
Using API allows you more control of context, which model you use, system prompt, other controls. ~Good graphic for this.}
}

@misc{LMStudio24LMStudioDiscover,
  title = { {{LM Studio}} - {{Discover}} and Run Local {{LLMs}}},
  author = {LM Studio},
  year = {2024},
  url = {https://lmstudio.ai},
  urldate = {2024-04-13},
  abstract = {Find, download, and experiment with local LLMs},
  langid = {english},
  keywords = {obsLitNote},
  note = {LMStudio24LMStudioDiscover
\par
A tool to compare LLMs on your own computer, an acessory to leaderboards?
\par
Recommended by Hofferber of Stable Discussion, \href{https://youtu.be/IRPhIpsrisg?t=509}{here}.}
}

@misc{Gill23typesGenAItrainEval,
  title = {Types of {{Generative AI Models}} and {{LLM Model Training}} and {{Evaluation}}},
  author = {Gill, Jagreet Kaur},
  year = {2023},
  month = nov,
  journal = {Xenon Stack: A Stack Innovator},
  url = {https://www.xenonstack.com/blog/generative-ai-models},
  urldate = {2024-04-13},
  abstract = {Types of Generative AI Models and Guide for Model Training, evaluation and Inference for different applications},
  langid = {english},
  keywords = {obsLitNote},
  note = {Gill23typesGenAItrainEval
\par
Good for talk intro, make point that there are a lot of types.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gill23typesGenAItrainEval.pdf}
}

@misc{Krishnamoorthy23roleVecDBgenAI,
  title = {The Role of Vector Datastores in Generative {{AI}} Applications {\textbar} {{AWS Database Blog}}},
  author = {Krishnamoorthy, G2 and Pathak, Rahul and Vlasceanu, Vlad},
  year = {2023},
  month = jul,
  journal = {AWS Database Blog},
  url = {https://aws.amazon.com/blogs/database/the-role-of-vector-datastores-in-generative-ai-applications/},
  urldate = {2024-04-13},
  abstract = {Generative AI has captured our imagination and is transforming industries with its ability to answer questions, write stories, create art, and even generate code. AWS customers are increasingly asking us how they can best take advantage of generative AI in their own businesses. Most have accumulated a wealth of domain-specific data (financial records, health records, genomic data, supply chain, and so on), which provides them with a unique and valuable perspective into their business and broader industry. This proprietary data can be an advantage and differentiator for your generative AI strategy. At the same time, many customers have also noticed the rise in popularity of vector datastores, or vector databases, used in generative AI applications, and are wondering how these solutions fit in their overall data strategy around generative AI applications. In this post, we describe the role of vector databases in generative AI applications, and how AWS solutions can help you harness the power of generative AI.},
  chapter = {Amazon Aurora},
  langid = {american},
  keywords = {obsLitNote},
  note = {Krishnamoorthy23roleVecDBgenAI
\par
Good description of vec DBs in RAG, and decent graphics. ~Good comparison of vecDBs and traditional.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Krishnamoorthy23roleVecDBgenAI.pdf}
}

@misc{Exxact24vecDBllmGenAIdpLrn,
  type = {Blog},
  title = {Vector {{Database}} Used in {{AI}} {\textbar} {{Exxact Blog}}},
  author = {{Exxact}},
  year = {2024},
  month = feb,
  journal = {Exxact Blog},
  url = {https://www.exxactcorp.com/blog/deep-learning/vector-database-for-llms-generative-ai-and-deep-learning},
  urldate = {2024-04-13},
  abstract = {Vector Database's ability to store, manage, and rapidly retrieve complex data structures has not only enhanced the performance of these systems but also opened new possibilities in AI applications.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Exxact24vecDBllmGenAIdpLrn
\par
The \textbf{best} vector DB graphics, and also coverge of vecDBs for something other than LLMs.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Exxact24vecDBllmGenAIdpLrn.html}
}

@article{Naughton24boomBurstAI,
  title = {From Boom to Burst, the {{AI}} Bubble Is Only Heading in One Direction},
  author = {Naughton, John},
  year = {2024},
  month = apr,
  journal = {The Observer},
  issn = {0029-7712},
  url = {https://www.theguardian.com/commentisfree/2024/apr/13/from-boom-to-burst-the-ai-bubble-is-only-heading-in-one-direction},
  urldate = {2024-04-13},
  abstract = {No one should be surprised that artificial intelligence is following a well-worn and entirely predictable financial arc},
  chapter = {Opinion},
  langid = {british},
  keywords = {obsLitNote},
  note = {Naughton24boomBurstAI
\par
Just for the headline and picture! ~Adjust the browser screen width to get just the right headline and picture placement for a slide.
\par
Otherwise, it's just saying that AI is like other boom/busts in the past, like when Netscape came out, and suddenly everybody understood what the internet was.
\par
I think AI's different than that, and is more like \textbf{another good headline:} (Bogost, 2024): 
\par
\begin{quotation}

\par
\textbf{AI Has Lost Its Magic} \\
That's how you know it's taking over
\par
\end{quotation}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Naughton24boomBurstAI.pdf}
}

@misc{OpenAI24improvFinetuningAssist,
  title = {Introducing Improvements to the Fine-Tuning {{API}} and Expanding Our Custom Models Program},
  author = {{OpenAI}},
  year = {2024},
  month = apr,
  journal = {OpenAI Announcements},
  url = {https://openai.com/blog/introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program?utm_source=substack&utm_medium=email},
  urldate = {2024-04-13},
  abstract = {We're adding new features to help developers have more control over fine-tuning and announcing new ways to build custom models with OpenAI.},
  langid = {american},
  keywords = {obsLitNote},
  note = {OpenAI24improvFinetuningAssist
\par
OpenAI's Fine Tuning options. ~I
\par
Intereresting that they offer an \textbf{Assisted Fine-Tuning }service, where AI engineers meet with customers to help find tune for their applications.
\par
Example was SK telecom got big improvements like this and much greater customer satisfaction: 4.5/5. ~
\par
\textbf{In talk, this could be a way to alternate applications with theory}
\par
Could have applications of genAI, ending with customer service.
\par
Note that the initial results were bad, but OpenAI personally helped them tune it (no fee mentioned).
\par
Then could seque into what fine tuning is.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\OpenAI24improvFinetuningAssist.pdf}
}

@misc{AssemblyAI24multilangASRuniversal1,
  title = {Introducing {{Universal-1}}},
  author = {{AssemblyAI}},
  year = {2024},
  month = apr,
  journal = {News, Tutorials, AI Research},
  url = {https://www.assemblyai.com/blog/announcing-universal-1-speech-recognition-model/},
  urldate = {2024-04-13},
  abstract = {Universal-1 is our most powerful speech recognition model. Trained on over 12.5 million hours of multilingual audio data, Universal-1 achieves best-in-class speech-to-text accuracy across four major languages: English, Spanish, French, and German.},
  langid = {english},
  keywords = {obsLitNote},
  note = {AssemblyAI24multilangASRuniversal1
\par
Good graph: ``This is a chart of the Average Word Error Rate of AssemblyAI's Universal-1{\dots}''
\par
Interesting points about new ASR, put in the ``multilingual, translation'' part, maybe next to OpenAI's multilingual voice generation (David, 2024). ~Possibly before the part where I say that LLMs are multilingual, and have made the point about transformers, and call centers? ~Was that why the S Korean phone company got so much better results (``Introducing improvements to the fine-tuning API and expanding our custom models program'', 2024), or was that a text-only job?
\par
\begin{itemize}

\item Really good numbers
\item multilingual
\item English no longer best (``I used to be an ASR researcher, and English used to be the best, but now Spanish is best, while German and French are comparable. ~Would love to see other languages{\dots})

\end{itemize}}
}

@misc{Wu24ReFTRepresentationFinetuning,
  title = {{{ReFT}}: {{Representation Finetuning}} for {{Language Models}}},
  shorttitle = {{{ReFT}}},
  author = {Wu, Zhengxuan and Arora, Aryaman and Wang, Zheng and Geiger, Atticus and Jurafsky, Dan and Manning, Christopher D. and Potts, Christopher},
  year = {2024},
  month = apr,
  number = {arXiv:2404.03592},
  eprint = {2404.03592},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.03592},
  url = {http://arxiv.org/abs/2404.03592},
  urldate = {2024-04-13},
  abstract = {Parameter-efficient fine-tuning (PEFT) methods seek to adapt large models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. Here, we pursue this hypothesis by developing a family of \${\textbackslash}textbf\{Representation Finetuning (ReFT)\}\$ methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT). LoReFT is a drop-in replacement for existing PEFTs and learns interventions that are 10x-50x more parameter-efficient than prior state-of-the-art PEFTs. We showcase LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks, Alpaca-Eval v1.0, and GLUE. In all these evaluations, LoReFT delivers the best balance of efficiency and performance, and almost always outperforms state-of-the-art PEFTs. We release a generic ReFT training library publicly at https://github.com/stanfordnlp/pyreft.},
  archiveprefix = {arXiv},
  keywords = {hasCode},
  note = {Wu24fineLLMtuneReFTR
\par
Introduction links to papers on causal explainability, and algorithmic steering during inference (this paper itself is about steering). ~Fig 4 might be a good graph showing how fine futning can improve results on different applications.
\par
\begin{itemize}

\item TODO: look these up, b/c I'm weak on how causality works in LLMs (causuaLens, 2023; Liu et al., 2024; Dynatrace, 2024; causuaLens, 2023), and also on steering
\item Has a python lib

\end{itemize}

\par
Comment: 40 pages, preprint},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wu24fineLLMtuneReFTR.pdf}
}

@misc{Raposo24mixDepthCmputTrnsfrmrLLM,
  title = {Mixture-of-{{Depths}}: {{Dynamically}} Allocating Compute in Transformer-Based Language Models},
  shorttitle = {Mixture-of-{{Depths}}},
  author = {Raposo, David and Ritter, Sam and Richards, Blake and Lillicrap, Timothy and Humphreys, Peter Conway and Santoro, Adam},
  year = {2024},
  month = apr,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2404.02258v1},
  urldate = {2024-04-14},
  abstract = {Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens (\$k\$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-\$k\$ routing mechanism. Since \$k\$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the \$k\$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50{\textbackslash}\% faster to step during post-training sampling.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Raposo24mixDepthCmputTrnsfrmrLLM
\par
A way of constraining compute cost by dynamically selecting only certain tokens from participating in forward pass. ~Other ideas like Mixture of Experts pick blocks or MLPs to run (I think).
\par
Not sure if this reduces comp during training, inference, or both.
\par
In talk, could be used as a technique for reducing model power consumption (I think), in addition to TPU's etc.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Raposo24mixDepthCmputTrnsfrmrLLM.pdf}
}

@misc{Anthropic24manyShotJailbreak,
  title = {Many-Shot Jailbreaking},
  author = {{Anthropic}},
  year = {2024},
  month = apr,
  journal = {Anthrop{\textbackslash}c},
  url = {https://www.anthropic.com/research/many-shot-jailbreaking?utm_source=substack&utm_medium=email},
  urldate = {2024-04-14},
  abstract = {Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Anthropic24manyShotJailbreak
\par
You can override an LLMs safety protections by asking for ``bad'' information over and over. ~The reason has something to do with this is how harmless dialogs progress, with answer accuracy improving with the number of Q's (verify my understanding of this). ~It's said tha this makes models with increasingly large context windows ({$>$} 1M) more vulnerable. ~Interesting that Anthropic itself published this, and showed how it broke their own model.
\par
Good graph. ~For talk, file under
\par
\begin{itemize}

\item context window
\item one shot (``shot'' not quite used in the same sense, though)
\item AI danger
\item model complexity consequences

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Anthropic24manyShotJailbreak.pdf}
}

@misc{Adib23encodeDecodeGenAI,
  title = {Encoders, {{Decoders}} and {{Their Model Relationship Within Generative AI}}},
  author = {Adib, Khalil},
  year = {2023},
  month = sep,
  journal = {Firemind},
  url = {https://www.firemind.com/insights/encoders-decoders-and-their-model-relationship-within-generative-ai/},
  urldate = {2024-04-14},
  abstract = {In today's post, written by resident Data Scientist, Khalil Adib, we explore Generative AI and it's Encoder-Only/Decoder-Only Models.},
  langid = {british},
  keywords = {obsLitNote},
  note = {Adib23encodeDecodeGenAI
\par
Says something about Decoder-only Encoder-only models and what their uses are. ~Dunno if this is the best article.
\par
Mainly, is says that, in the genAI world, \textbf{``autoregressive'' means ``decoder-only''}, which clears up some confusion I had when reading (Wodecki, 2024)
\par
\begin{itemize}

\item (Gueddari, 2023) has a short and seemingly clearer explanation of decoder-only.

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Adib23encodeDecodeGenAI.html}
}

@misc{Gueddari23decOnlyGenAI,
  title = {The {{Dual Worlds}} of {{Decoder-only Transformers}}: {{Training}} vs. {{Inference}}},
  shorttitle = {The {{Dual Worlds}} of {{Decoder-only Transformers}}},
  author = {Gueddari, Nouamane El},
  year = {2023},
  month = nov,
  journal = {Medium},
  url = {https://medium.com/@ElGueddari/the-dual-worlds-of-decoder-only-transformers-training-vs-inference-1ea1c2aabaaa},
  urldate = {2024-04-14},
  abstract = {In the swiftly evolving landscape of artificial intelligence, the launch of ChatGPT last year marked a pivotal moment, redefining the{\dots}},
  langid = {english},
  keywords = {obsLitNote},
  note = {Gueddari23decOnlyGenAI
\par
Short and sweet inputs and outputs descrition of a decoder-only model. ~Other options are (well?) explained in (Adib, 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gueddari23decOnlyGenAI.pdf}
}

@misc{YannicKilcher24MLNewsJamba,
  title = {[{{ML News}}] {{Jamba}}, {{CMD-R}}+, and Other New Models (Yes, {{I}} Know This Is like a Week behind )},
  author = {{Yannic Kilcher}},
  year = {2024},
  month = apr,
  url = {https://www.youtube.com/watch?v=Kk8YhCpo1b8},
  urldate = {2024-04-14},
  abstract = {A flurry of new models continues to appear. Links: Homepage: https://ykilcher.com Merch: https://ykilcher.com/merch YouTube: ~~~/~yannickilcher~~ Twitter: ~~/~ykilcher~~ Discord: https://ykilcher.com/discord LinkedIn: ~~/~ykilcher~~ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https://www.subscribestar.com/yannick... Patreon: ~~/~yannickilcher~~ Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n},
  keywords = {obsLitNote},
  note = {many interesting things (left in my browser tabs, for now), but, more of a comment
\par
Just as Zak volunteered/confessed after interrogation: 
\par
\begin{itemize}

\item a new model (1 bit?) trains on general data 1st
\item then \href{https://youtu.be/Kk8YhCpo1b8?t=1291}{specific data at a lower learning rate}
\item learning rate in combination w/ sequence of data ``\href{https://youtu.be/Kk8YhCpo1b8?t=1307}{seems to be quite an important piece in training these language models}.''
\item another leaderboard: \href{https://youtu.be/Kk8YhCpo1b8?t=1478}{berkeley function calling leaderboard. } Important for agent models. ~Claude3 is on top

\end{itemize}}
}

@misc{Robinson24unstructDatPreprocLLM,
  title = {Preprocessing {{Unstructured Data}} for {{LLM Applications}}},
  author = {Robinson, Matt},
  year = {2024},
  month = apr,
  journal = {DLAI - Learning Platform},
  url = {https://learn.deeplearning.ai/courses/preprocessing-unstructured-data-for-llm-applications/lesson/1/introduction},
  urldate = {2024-04-14},
  abstract = {Introduction {$\cdot$} Overview of LLM Data Preprocessing {$\cdot$} Normalizing the Content {$\cdot$} Metadata Extraction and Chunking {$\cdot$} Preprocessing PDFs and Images {$\cdot$} Extracting Tables {$\cdot$} Build Your Own RAG Bot {$\cdot$} Conclusion {$\cdot$} Appendix - Tips and Help},
  keywords = {obsLitNote}
}

@inproceedings{Nassar22tableFormerStructTransfrmr,
  title = {Tableformer: {{Table}} Structure Understanding with Transformers},
  shorttitle = {Tableformer},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Nassar, Ahmed and Livathinos, Nikolaos and Lysak, Maksym and Staar, Peter},
  year = {2022},
  pages = {4614--4623},
  url = {http://openaccess.thecvf.com/content/CVPR2022/html/Nassar_TableFormer_Table_Structure_Understanding_With_Transformers_CVPR_2022_paper.html},
  urldate = {2024-04-14},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nassar22tableFormerStructTransfrmr.pdf}
}

@misc{Peixeiro24iTransfrmrTSfrcst,
  title = {{{iTransformer}}: {{The Latest Breakthrough}} in {{Time Series Forecasting}}},
  shorttitle = {{{iTransformer}}},
  author = {Peixeiro, Marco},
  year = {2024},
  month = apr,
  journal = {Medium},
  url = {https://towardsdatascience.com/itransformer-the-latest-breakthrough-in-time-series-forecasting-d538ddc6c5d1},
  urldate = {2024-04-14},
  abstract = {Discover the architecture of iTransformer and apply the model in a small experiment using Python.},
  langid = {english},
  keywords = {todo},
  note = {Peixeiro24iTransfrmrTSfrcst
\par
I've only skimmed, not read, this. ~Good pictures, maybe.
\par
Thus far, transformer architectures not great compared other more standard forecast algoriths (cites nxtla study). ~Guy says it's b/c they don't consider time continuity in features, just analyze each full set of features in isolation in transformer (why not just induce lags as features?). ~
\par
\href{https://medium.com/towards-data-science/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc}{PatchTST} tries to fix this by chunking in time (kinda like document chunking?) and gets SOA long-horizon perf that way.
\par
iTransformer, is said to do this better: ~all features are embedded jointly in a single layer (?), capture cross-feature relations, while the MLPs hand time correlation?
\par
Anyway, READ THIS, and the corresponding paper: (Liu et al., 2024)
\par
Also, it's test on an electricity transformer dataset.}
}

@misc{Liu24ITransformerInvertedTransformers,
  title = {{{iTransformer}}: {{Inverted Transformers Are Effective}} for {{Time Series Forecasting}}},
  shorttitle = {{{iTransformer}}},
  author = {Liu, Yong and Hu, Tengge and Zhang, Haoran and Wu, Haixu and Wang, Shiyu and Ma, Lintao and Long, Mingsheng},
  year = {2024},
  month = mar,
  number = {arXiv:2310.06625},
  eprint = {2310.06625},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.06625},
  url = {http://arxiv.org/abs/2310.06625},
  urldate = {2024-04-14},
  abstract = {The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-forward network on the inverted dimensions. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting. Code is available at this repository: https://github.com/thuml/iTransformer.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Liu24iTrnsfrmsAreEffectiveTSfrst
\par
Pretty picture intro: (Peixeiro, 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Liu24iTrnsfrmsAreEffectiveTSfrst.pdf}
}

@misc{Wiggers24genAIhealthCrNoThrill,
  title = {Generative {{AI}} Is Coming for Healthcare, and Not Everyone's Thrilled},
  author = {Wiggers, Kyle},
  year = {2024},
  month = apr,
  journal = {TechCrunch},
  url = {https://techcrunch.com/2024/04/14/generative-ai-is-coming-for-healthcare-and-not-everyones-thrilled/},
  urldate = {2024-04-14},
  abstract = {The broad enthusiasm for generative AI is reflected in the investments in GenAI efforts targeting healthcare. But is healthcare-focused generative AI ready for prime time?},
  langid = {american},
  keywords = {todo},
  note = {Wiggers24genAIhealthCrNoThrill
\par
Lots of investment in clinical AI, but it's still making serious errors in recent clinical tests. ~If legal and regulatory issues are resolved, it could be used for routine letter writing (if physicians don't get lulled into never checking them), and for specific, narrow diagnostic problems where it may be able to accurately say ``I don't know.'' and hand it off to a human. ~
\par
\begin{itemize}

\item Maybe \textbf{``saying no'' i}s where genAI \textbf{predictive maintenance} should go too.
\item Value of \textbf{prompt engineering} in a recent study, so not dead yet. Better than \textbf{fine tuning,} somehow? ~But it seem to be more than just chain-of-thought prompting. ~

\end{itemize}

\par
\section{Medical errors:}

\begin{itemize}

\item JAMA Pediatrics journal artilce (link): chatGPT rrors 83\% of the time for pediactric diseases: (Bettelheim, 2024)
\item GPT-4 dianostis assisstant: top answer wrong nearly 2 of 3 times (\href{https://www.statnews.com/2023/07/20/chatgpt-gpt4-health-care-medical-education/}{here}, but no paywall, article is July 20, 2023)
\item Summarizing health records, searching across notes: 35\% of the time: (Fleming et al., 2023)
\item racial stereotypes: wrong answers on kidney function, lung capacity, skin thickness perpetuated long-held but still-believed inaccrate answers about black people. ~Wrong answers know to give wrong diagnoses.

\end{itemize}

\section{Useful}

\begin{itemize}

\item 
\par
MS study: got 90.2\% accuracy on four challenging clinical benchmark, but that was on GPT-4 after \textbf{prompt} engineering (16.2\% boost, to vanilla GPT-4 must have been 74\%). ~
\par
\begin{itemize}

\item Seems to have beaten a fine-tuned model --
\item TODO: read the original: (Eric Horvitz, 2023)

\end{itemize}

\item 
\par
specific, mundane
\par
\begin{itemize}

\item discharge letters, but human physicians must read and be responsible (must be short\&mid-term too)
\item 
\par
short ~\& midterm:
\par
\begin{itemize}

\item text correction
\item 
\par
automatic documentation of notes and letters, improved search
\par
\begin{itemize}

\item \textbf{BUT} didn't it just say above that summarization of records and not search was bad?

\end{itemize}

\end{itemize}

\end{itemize}

\item Chinese Panda study: highly accurae in pancreatic lesion desection w/ X-rays (often missed until too late)
\item 
\par
CoDOC: complimetarity driven deferral clinical workflow: (Dvijotham et al., 2023)
\par
\begin{itemize}

\item figure out when to turned AI for diagnosis vs. humans
\item better than humans at this
\item reduced workflows by 66\% w/o increasing risk of missing serious disease (TB, breast cancer)
\item Same problem as chatGPT saying ``I don't know,'' I think.
\item Is this how preventative maintenance could go? ~

\end{itemize}

\end{itemize}

\section{Needed}

\begin{itemize}

\item randomized, controlled trials to demonstrate clinical benefit b/f patient-facing genAI
\item regulatorry, confidentiality, legal framework not ready

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wiggers24genAIhealthCrNoThrill.pdf}
}

@misc{Fleming23MedAlignClinicianGeneratedDataset,
  title = {{{MedAlign}}: {{A Clinician-Generated Dataset}} for {{Instruction Following}} with {{Electronic Medical Records}}},
  shorttitle = {{{MedAlign}}},
  author = {Fleming, Scott L. and Lozano, Alejandro and Haberkorn, William J. and Jindal, Jenelle A. and Reis, Eduardo P. and Thapa, Rahul and Blankemeier, Louis and Genkins, Julian Z. and Steinberg, Ethan and Nayak, Ashwin and Patel, Birju S. and Chiang, Chia-Chun and Callahan, Alison and Huo, Zepeng and Gatidis, Sergios and Adams, Scott J. and Fayanju, Oluseyi and Shah, Shreya J. and Savage, Thomas and Goh, Ethan and Chaudhari, Akshay S. and Aghaeepour, Nima and Sharp, Christopher and Pfeffer, Michael A. and Liang, Percy and Chen, Jonathan H. and Morse, Keith E. and Brunskill, Emma P. and Fries, Jason A. and Shah, Nigam H.},
  year = {2023},
  month = dec,
  number = {arXiv:2308.14089},
  eprint = {2308.14089},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.14089},
  url = {http://arxiv.org/abs/2308.14089},
  urldate = {2024-04-14},
  abstract = {The ability of large language models (LLMs) to follow natural language instructions with human-level fluency suggests many opportunities in healthcare to reduce administrative burden and improve quality of care. However, evaluating LLMs on realistic text generation tasks for healthcare remains challenging. Existing question answering datasets for electronic health record (EHR) data fail to capture the complexity of information needs and documentation burdens experienced by clinicians. To address these challenges, we introduce MedAlign, a benchmark dataset of 983 natural language instructions for EHR data. MedAlign is curated by 15 clinicians (7 specialities), includes clinician-written reference responses for 303 instructions, and provides 276 longitudinal EHRs for grounding instruction-response pairs. We used MedAlign to evaluate 6 general domain LLMs, having clinicians rank the accuracy and quality of each LLM response. We found high error rates, ranging from 35\% (GPT-4) to 68\% (MPT-7B-Instruct), and an 8.3\% drop in accuracy moving from 32k to 2k context lengths for GPT-4. Finally, we report correlations between clinician rankings and automated natural language generation metrics as a way to rank LLMs without human review. We make MedAlign available under a research data use agreement to enable LLM evaluations on tasks aligned with clinician needs and preferences.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Fleming23MedAlignClinDataInstructFollow
\par
(Wiggers, 2024) says this article says that: 
\par
\begin{quotation}

\par
On the MedAlign benchmark to evaluate how well generative AI canperform things like summarizing patient health records and searching across notes, GPT-4 failed in 35\% of cases.
\par
\end{quotation}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Fleming23MedAlignClinicianGeneratedDataset.pdf}
}

@misc{Bettelheim24ChatGPThighErrPediatric,
  title = {{{ChatGPT}} Had a High Error Rate for Pediatric Cases},
  author = {Bettelheim, Adriel},
  year = {2024},
  month = jan,
  journal = {Axios},
  url = {https://www.axios.com/2024/01/03/ai-fails-diagnosing-childrens-cases},
  urldate = {2024-04-14},
  abstract = {A new study raises questions about some bots' ability to help doctors diagnose complex conditions.},
  langid = {english},
  keywords = {todo},
  note = {Bettelheim24ChatGPThighErrPediatric
\par
Referred to in summary: (Wiggers, 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bettelheim24ChatGPThighErrPediatric.pdf}
}

@article{Dvijotham23aiDeferralToClinicians,
  title = {Enhancing the Reliability and Accuracy of {{AI-enabled}} Diagnosis via Complementarity-Driven Deferral to Clinicians},
  author = {Dvijotham, Krishnamurthy and Winkens, Jim and Barsbey, Melih and Ghaisas, Sumedh and Stanforth, Robert and Pawlowski, Nick and Strachan, Patricia and Ahmed, Zahra and Azizi, Shekoofeh and Bachrach, Yoram},
  year = {2023},
  journal = {Nature Medicine},
  volume = {29},
  number = {7},
  pages = {1814--1820},
  publisher = {Nature Publishing Group US New York},
  url = {https://www.nature.com/articles/s41591-023-02437-x},
  urldate = {2024-04-14},
  abstract = {Predictive artificial intelligence (AI) systems based on deep learning have been shown to achieve expert-level identification of diseases in multiple medical imaging settings, but can make errors in cases accurately diagnosed by clinicians and vice versa. We developed Complementarity-Driven Deferral to Clinical Workflow (CoDoC), a system that can learn to decide between the opinion of a predictive AI model and a clinical workflow. CoDoC enhances accuracy relative to clinician-only or AI-only baselines in clinical workflows that screen for breast cancer or tuberculosis (TB). For breast cancer screening, compared to double reading with arbitration in a screening program in the UK, CoDoC reduced false positives by 25\% at the same false-negative rate, while achieving a 66\% reduction in clinician workload. For TB triaging, compared to standalone AI and clinical workflows, CoDoC achieved a 5--15\% reduction in false positives at the same false-negative rate for three of five commercially available predictive AI systems. To facilitate the deployment of CoDoC in novel futuristic clinical settings, we present results showing that CoDoC's performance gains are sustained across several axes of variation (imaging modality, clinical setting and predictive AI system) and discuss the limitations of our evaluation and where further validation would be needed. We provide an open-source implementation to encourage further research and application.},
  keywords = {todo},
  note = {Dvijotham23aiDeferralToClinicians
\par
AI diagnosis apparently not perfect, but is OK at knowing when it can't be trusted. ~Maye a clue to predictive maintenance?
\par
System was OK at deciding when do a diagnosis w/ AI vs traditional human. ~
\par
At same false negative rate (missed disease),
\par
\begin{itemize}

\item Breast cancer screen: reduced false pos by 25\% , 66\% reduction in workload
\item TB triage: 5-15\% reduction in false pos
\item 

\end{itemize}

\par
No PDF, but from abstract:~
\par
\begin{quotation}

\par
For breast cancer screening, compared to double reading with arbitration in a screening program in the UK, CoDoC reduced false positives by 25\% at the same false-negative rate, while achieving a 66\% reduction in clinician workload. For TB triaging, compared to standalone AI and clinical workflows, CoDoC achieved a 5--15\% reduction in false positives at the same false-negative rate for three of five commercially available predictive AI systems.~
\par
\end{quotation}

\par
Referenced by (Wiggers, 2024)}
}

@misc{Horvitz23powOfPrompt,
  title = {The Power of Prompting},
  author = {{Eric Horvitz}},
  year = {2023},
  month = nov,
  journal = {Microsoft Research},
  url = {https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/},
  urldate = {2024-04-15},
  abstract = {Microsoft Chief Scientific Officer Eric Horvitz explains how new prompting strategies can enable generalist large language models like GPT-4 to achieve exceptional expertise in specific domains, such as medicine, and outperform fine-tuned specialist models.},
  langid = {american},
  keywords = {todo},
  note = {Horvitz23powOfPrompt
\par
The case mentioned in (Wiggers, 2024), where general model prompting made a big improvement in medical benchmarks. ~Is it a bigger gain that a specifically fine-tuned model?
\par
A good example of effect of various combos of tricks to improve
\par
\begin{itemize}

\item zeroshot (starting point)
\item Random few-shot
\item Random few-shot, Chain of thought
\item kNN, few shot, chain-of-thoght
\item Ensemble w/ choice shuffle

\end{itemize}

\par
More than just chain-of-thought prompting, but anyway, prompt engineering doesn't seem to be dead: (Dina Genkina, 2024), unless Horvitz paper is really old, in spite of the date. ~So is Zak wrong?
\par
\textbf{I haven't read this.}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Horvitz23powOfPrompt.pdf}
}

@misc{Hendy23HowGoodAre,
  title = {How {{Good Are GPT Models}} at {{Machine Translation}}? {{A Comprehensive Evaluation}}},
  shorttitle = {How {{Good Are GPT Models}} at {{Machine Translation}}?},
  author = {Hendy, Amr and Abdelrehim, Mohamed and Sharaf, Amr and Raunak, Vikas and Gabr, Mohamed and Matsushita, Hitokazu and Kim, Young Jin and Afify, Mohamed and Awadalla, Hany Hassan},
  year = {2023},
  month = feb,
  number = {arXiv:2302.09210},
  eprint = {2302.09210},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2302.09210},
  urldate = {2024-04-15},
  abstract = {Generative Pre-trained Transformer (GPT) models have shown remarkable capabilities for natural language generation, but their performance for machine translation has not been thoroughly investigated. In this paper, we present a comprehensive evaluation of GPT models for machine translation, covering various aspects such as quality of different GPT models in comparison with state-of-the-art research and commercial systems, effect of prompting strategies, robustness towards domain shifts and document-level translation. We experiment with eighteen different translation directions involving high and low resource languages, as well as non English-centric translations, and evaluate the performance of three GPT models: ChatGPT, GPT3.5 (text-davinci-003), and text-davinci-002. Our results show that GPT models achieve very competitive translation quality for high resource languages, while having limited capabilities for low resource languages. We also show that hybrid approaches, which combine GPT models with other translation systems, can further enhance the translation quality. We perform comprehensive analysis and human evaluation to further understand the characteristics of GPT translations. We hope that our paper provides valuable insights for researchers and practitioners in the field and helps to better understand the potential and limitations of GPT models for translation.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Hendy23howGoodGPTmchnTrans
\par
I didn't read. ~It's mostly so I can borrow Figure 1 for my presentation. ~It shows the accuracy of 12 language pairs. ~It shows that:
\par
\begin{enumerate}

\item some languages are simply hard to translate e.g. English-Chinese and English-Japanese: no surprise there
\item one direction can be harder than the other: En-JP vs. JP-EN.
\item I'm a little surprised that English and Czech are the easiest to machine transalate. ~Gemini says this could be because of data availablity, or some subtle similarities in gramatic structures. ~But it had no evidence of this.

\end{enumerate}

\par
If I wanted to talk more than 30 seconds, I should read a bit about MS-translate, which I think is the real topic of this paper.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hendy23howGoodGPTmchnTrans.pdf}
}

@misc{Ananthaswamy24HowMachinesGrok,
  title = {How {{Do Machines}} `{{Grok}}' {{Data}}?},
  author = {Ananthaswamy, Anil},
  year = {2024},
  month = apr,
  journal = {Quanta Magazine},
  url = {https://www.quantamagazine.org/how-do-machines-grok-data-20240412/},
  urldate = {2024-04-15},
  abstract = {By apparently overtraining them, researchers have seen neural networks discover novel solutions to problems.},
  langid = {english},
  keywords = {todo},
  note = {Ananthaswamy24HowMachinesGrok
\par
A small transformer, trained to do modulo 97 arithmetic at first memorized the training data and then test performance caved. ~But when accidentally allowed to keep training, it eventually found an exact solution to the general problem, brining the test error to zero too. ~And it learned it for the general case, for any M modulo N (I think it says).
\par
In one case, it had learned to do modulor arithmetic with a Fourier transform, and another time, it learned a ``pizza slice`` algorithm, that involved bisecting angles -- both methods apppropriate for a brain that only does linear algebra.
\par
But researchers could only figure out the discovered algorithm 40\% (I think it was) of the time; the rest remain a mystery.
\par
A general understanding of the method seems to be that, under regualation, the initial training uses all its resources to memorize the training data, outcompeting the other goal of the alogorithm (somehow) to find a general solution. ~Once it's done memoriztion, the generalization part of the net win out, and get adjusted.
\par
This works even when given noisy incorrect data. ~At first, it memorizes the noisy points and then it unmemorizes that and coverges on the correct generalization, ignoring the noisy points.
\par
Researchers is not optimistic about figure out how much larger transformers figure things out. ~But this might be a clue.
\par
\textbf{Note}: A while ago, I put papers related to this in energy.bib. ~The author talked about how memorization in deep nets was desirable, and had some theory about why that I can't remember.
\par
\textbf{TODO}: link this paper with those papers.
\par
\begin{itemize}

\item Related: ~(Kaplan et al., 2020)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ananthaswamy24HowMachinesGrok.pdf}
}

@misc{Liang24FoundationModelsTime,
  title = {Foundation {{Models}} for {{Time Series Analysis}}: {{A Tutorial}} and {{Survey}}},
  shorttitle = {Foundation {{Models}} for {{Time Series Analysis}}},
  author = {Liang, Yuxuan and Wen, Haomin and Nie, Yuqi and Jiang, Yushan and Jin, Ming and Song, Dongjin and Pan, Shirui and Wen, Qingsong},
  year = {2024},
  month = apr,
  number = {arXiv:2403.14735},
  eprint = {2403.14735},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2403.14735},
  urldate = {2024-04-15},
  abstract = {Time series analysis stands as a focal point within the data mining community, serving as a cornerstone for extracting valuable insights crucial to a myriad of real-world applications. Recent advancements in Foundation Models (FMs) have fundamentally reshaped the paradigm of model design for time series analysis, boosting various downstream tasks in practice. These innovative approaches often leverage pre-trained or fine-tuned FMs to harness generalized knowledge tailored specifically for time series analysis. In this survey, we aim to furnish a comprehensive and up-to-date overview of FMs for time series analysis. While prior surveys have predominantly focused on either the application or the pipeline aspects of FMs in time series analysis, they have often lacked an in-depth understanding of the underlying mechanisms that elucidate why and how FMs benefit time series analysis. To address this gap, our survey adopts a model-centric classification, delineating various pivotal elements of time-series FMs, including model architectures, pre-training techniques, adaptation methods, and data modalities. Overall, this survey serves to consolidate the latest advancements in FMs pertinent to time series analysis, accentuating their theoretical underpinnings, recent strides in development, and avenues for future research exploration.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Liang24foundationMdlsTimeSer
\par
Figure 3 has a huge tree of time series foundation models. ~I had no idea!},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Liang24FoundationModelsTime.pdf}
}

@misc{Woo24MoiraiTSunivFrcst,
  title = {Moirai: {{A Time Series Foundation Model}} for {{Universal Forecasting}}},
  shorttitle = {Moirai},
  author = {Woo, Gerald and Liu, Chenghao and Sahoo, Doyen and Xiong, Caiming},
  year = {2024},
  month = mar,
  journal = {Salesforce AI},
  url = {https://blog.salesforceairesearch.com/moirai/},
  urldate = {2024-04-15},
  abstract = {TL;DR: Moirai is a cutting-edge time series foundation model, offering universal forecasting capabilities. It stands out as a versatile time series forecasting model capable of addressing diverse forecasting tasks across multiple domains, frequencies, and variables in a zero-shot manner.~ To achieve this, Moirai tackles four major challenges: (i) construction},
  langid = {english},
  keywords = {todo},
  note = {Woo24MoiraiTSunivFrcst
\par
Tech note for (Woo et al., 2024). ~
\par
Some intesting capabilities
\par
\begin{itemize}

\item can handle variable dimenions
\item probabilities are mixtures of parametric distributions
\item exogenous variables allowed

\end{itemize}}
}

@techreport{HAI24AIIndexReport,
  title = {Artificial {{Intelligence Index Report}} 2024},
  author = {{HAI}},
  year = {2024},
  institution = {Stanford University},
  url = {https://aiindex.stanford.edu/report/},
  urldate = {2024-04-15},
  abstract = {Welcome to the seventh edition of the AI Index report. The 2024 Index is our most comprehensive to date and  arrives at an important moment when AI's influence on society has never been more pronounced. This year,  we have broadened our scope to more extensively cover essential trends such as technical advancements  in AI, public perceptions of the technology, and the geopolitical dynamics surrounding its development.  Featuring more original data than ever before, this edition introduces new estimates on AI training costs,  detailed analyses of the responsible AI landscape, and an entirely new chapter dedicated to AI's impact on  science and medicine.  The AI Index report tracks, collates, distills, and visualizes data related to artificial intelligence (AI). Our  mission is to provide unbiased, rigorously vetted, broadly sourced data in order for policymakers, researchers,  executives, journalists, and the general public to develop a more thorough and nuanced understanding of the  complex field of AI. The AI Index is recognized globally as one of the most credible and authoritative sources for data and insights  on artificial intelligence. Previous editions have been cited in major newspapers, including the The New York  Times, Bloomberg, and The Guardian, have amassed hundreds of academic citations, and been referenced  by high-level policymakers in the United States, the United Kingdom, and the European Union, among  other places. This year's edition surpasses all previous ones in size, scale, and scope, reflecting the growing  significance that AI is coming to hold in all of our lives.},
  keywords = {obsLitNote},
  note = {HAI24AIIndexReport
\par
\textbf{TODO}:
\par
\begin{enumerate}

\item weave points below into genAI talk ideas
\item 
\par
flesh out
\par
\begin{enumerate}

\item where AI beats/loses to humans
\item worker productivity
\item scientific discovery

\end{enumerate}

\end{enumerate}

\par
From the ``key takeaways'' part at the front:
\par
\begin{itemize}

\item 
\par
AI vs. humans
\par
\begin{itemize}

\item AI beats humans on image classification, visual reasoning, English understanding. ~
\item Trails on competition-level math, visual commonsense resoning, planning

\end{itemize}

\item 
\par
Training costs
\par
\begin{itemize}

\item GPT-4: \$78M
\item Gemini Ultra: \$191M

\end{itemize}

\item 
\par
US lead by a longshot
\par
\begin{itemize}

\item 2023: 61 of 97 ``notable'' models from US; EU was 21; China 15

\end{itemize}

\item Lack of standardized quality reporting makes it hard to compare risks and limitations
\item 
\par
AI imporves human workers, if used properly
\par
\begin{itemize}

\item several 2023 studies
\item improve task speed, quality
\item bridge gap between low and high skill workers i.e., I suppose, that it de-skills both of their jobs

\end{itemize}

\item 
\par
Scientific Discovery
\par
\begin{itemize}

\item AlphaDev: efficient algorithmic sorting: 1st update in LLVM stdC++ in 10 years, designed with reinforcement learning
\item GNoME: helps materials study

\end{itemize}

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\HAI24AIIndexReport.pdf}
}

@misc{Eadline24genAIFuture,
  title = {The {{Generative AI Future Is Now}}, {{Nvidia}}'s {{Huang Says}}},
  author = {Eadline, Doug},
  year = {2024},
  month = mar,
  journal = {HPCwire},
  url = {https://www.hpcwire.com/2024/03/19/the-generative-ai-future-is-now-nvidias-huang-says/},
  urldate = {2024-04-15},
  abstract = {We are in the early days of a transformative shift in how business gets done thanks to the advent of generative AI, according to Nvidia CEO and cofounder Jensen Huang, [{\dots}]},
  langid = {american},
  keywords = {todo},
  note = {Eadline24genAIFuture
\par
More to read here, but the only thing I read was the compute power thing:
\par
``The latest state-of-theart AI model, GPT-4, has about 1.8 trillion parameters, which required several trillion tokens to go train,'' (Eadline, 2024, p. 3)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Eadline24genAIFuture.pdf}
}

@misc{Wolf24littleGuideBuildLLM,
  title = {A Little Guide to Building {{Large Language Models}} in 2024},
  author = {Wolf, Thom},
  year = {2024},
  month = mar,
  address = {YouTube},
  url = {https://www.youtube.com/watch?v=2-SPH9hIKT8},
  urldate = {2024-04-15},
  abstract = {A little guide through all you need to know to train a good performance large language model in 2024. This is an introduction talk with link to references for further reading. This is the first video of a 2 part series: - Video 1 (this video): covering all the concepts to train a good performance LLM in 2024 - Video 2 (next video): hands-on applying all these concepts with code example This video is adapted from a talk I gave in 2024 at a AI/ML winter school for graduate student. When I shared the slides online people kept asking for a recording of the unrecorded class so I decided to spend a morning recording it to share it more widely along the slides. Link to the slides: https://docs.google.com/presentation/... Chapters: 00:00:00 Intro 00:00:59 Workflow for LLMs Part 1: Training: data 00:01:17 Data preparation - intro and good recent ressources on data preparation 00:05:28 A web scale pretraining corpus - goals and challenges 00:11:29 Web scale data sources -- Focus on recent datasets 00:18:01 Language, and quality filtering 00:24:34 Diving in data deduplication 00:27:40 Final data preparation for training 00:31:31 How to evaluate data quality at scale 00:36:29 The datatrove and lighteval libraries Part 2: Training: modeling 00:38:18 Introduction in modeling technics for LLM training 00:39:09 When the model is too big: parallelism  00:40:00 Data parallelism 00:41:18 Tensor parallelism 00:44:38 Pipeline parallelism 00:47:00 Sequence parallelism and references on 4D parallelism 00:47:52 Synchronisation: GPU-CPU and GPU-GPU challenges 00:52:14 Flash attention v1 and v2 00:56:23 Stable training recipes 00:59:12 New architectures: Mixture-of-experts 01:03:13 New architectures: Mamba 01:04:49 The nanotron library Part 3: Fine-tuning: RLHF and alignement 01:06:15 RLHF in 2024 01:08:23 PPO, DPO and REINFORCE Part 4: Fast inference techniques 01:11:23 Quantization, speculative decoding and compilation: overview and ressources End 01:14:36 Sharing your model, datasets and demo -- final words},
  keywords = {todo},
  note = {Wolf24littleGuideBuildLLM
\par
Recommended by Yannic Kilcher. ~Has slides for video.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wolf24littleGuideBuildLLM.pdf}
}

@misc{Sevilla22ComputeTrendsThree,
  title = {Compute {{Trends Across Three Eras}} of {{Machine Learning}}},
  author = {Sevilla, Jaime},
  year = {2022},
  month = feb,
  journal = {Epoch},
  url = {https://epochai.org/blog/compute-trends},
  urldate = {2024-04-16},
  abstract = {We've compiled a dataset of the training compute for over 120 machine learning models, highlighting novel trends and insights into the development of AI since 1952, and what to expect going forward.},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sevilla22ComputeTrendsThree.pdf}
}

@misc{Thissen23Mixtral8x7BalternGPT4,
  title = {Mixtral {{8x7B}} on {{Your Local Computer}} {\textbar} {{Free GPT-4 Alternative}}},
  author = {Thissen, Martin},
  year = {2023},
  month = dec,
  journal = {Medium},
  url = {https://medium.com/@martin-thissen/mixtral-8x7b-on-your-local-computer-free-gpt-4-alternative-e3cb301984e3},
  urldate = {2024-04-16},
  abstract = {In this article I will point out the key features of the Mixtral 8x7B model and show you how you can run the Mixtral 8x7B model on your{\dots}},
  langid = {english},
  keywords = {todo},
  note = {Thissen23Mixtral8x7BalternGPT4
\par
Quick blurb showing to set up a Sparse Mixture LLM (switching transformer?) on your desktop, with some theory explained. ~Mixtral is made by same company that makes Mistral foundation model.
\par
The Mixtral8x7B ia a mixture of experts LLM that has far fewer params and faster decode time than other dense models, with comparable performance and that you can run on your desktop -- if you have a ridiculous about of VRAM, and probably a GPU. ~(they don't actually compare w/ GPT-4, only GPT3.5turbo).
\par
This efficiency means that, with a fixed computational budget, you can train a complex model for more iterations and/or using more data. ~The smallest (4bit) variant needs 23 GB ``VRAM'' (video RAM). It seems possible to run on a CPU, but this guy sets it up on a beefy GPU.
\par
The idea of Mixture of Experts transformers is explained a bit. ~The idea is, in a transformer, to replace each MLP forward layer block in a transformer with committee of smaller MLPs which specialize in converting categories of tokens. ~Only two run for each token (according to this explanation) and the results are averaged. Diagram shows for MLPs in the block.
\par
I'm not certain this explanation is exactly correct. ~Another explanation, ~(Sanseviero et al., 2023), seemd to suggest that this isn't really a mixture of experts, but a switching transformer or something like that. ~And some other details{\dots}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Thissen23Mixtral8x7BalternGPT4.pdf}
}

@misc{Gaievski24multiModalSearch,
  type = {Blog},
  title = {Multimodal Search: {{Searching}} with Semantic and Visual Understanding},
  shorttitle = {Multimodal Search},
  author = {Gaievski, Martin and Yang, Hao and {Dylan Tong}},
  year = {2024},
  month = apr,
  journal = {OpenSearch},
  url = {https://opensearch.org/blog/multimodal-semantic-search/},
  urldate = {2024-04-16},
  abstract = {Learn about the enhancements to OpenSearch 2.11's neural query search by image capabilities, and dive deeper by exploring how multimodal models generate embeddings.},
  langid = {english},
  keywords = {todo},
  note = {Gaievski24multiModalSearch
\par
Muliti-embedding search, can be done by OpenSearch, which \href{https://growingenergylabs.slack.com/archives/C03EHN0JSQ1/p1713294808407369}{QCELLS is starting to use}.
\par
It can do:
\par
\begin{enumerate}

\item Text-to-image search, which retrieves images based on text queries.
\item Image-to-text search, which retrieves textual content based on visual queries.
\item Image-to-image search, which retrieves images that are most similar to a provided image.

\end{enumerate}}
}

@misc{Sanseviero23mixOfExpertsLLM,
  title = {Mixture of Experts Explained},
  author = {Sanseviero, Omar and Tunstall, Lewis and Schmid, Philipp and Mangrulkar, Sourab and Belkada, Younes and {Pedro Cuenca}},
  year = {2023},
  publisher = {Hugging Face Blog},
  url = {https://huggingface.co/blog/moe},
  keywords = {todo},
  note = {Sanseviero23mixOfExpertsLLM
\par
An updated verson of a medium post, explaining mixtures of experts, and their importation into transformers as (I think) switching transformers. ~I skimmed this, didn't read too much.
\par
One LLM that uses this is in (Thissen, 2023)}
}

@inproceedings{Li20trainBigThenCompress,
  title = {Train Big, Then Compress: {{Rethinking}} Model Size for Efficient Training and Inference of Transformers},
  shorttitle = {Train Big, Then Compress},
  booktitle = {International {{Conference}} on Machine Learning},
  author = {Li, Zhuohan and Wallace, Eric and Shen, Sheng and Lin, Kevin and Keutzer, Kurt and Klein, Dan and Gonzalez, Joey},
  year = {2020},
  pages = {5958--5968},
  publisher = {PMLR},
  url = {http://proceedings.mlr.press/v119/li20m.html},
  urldate = {2024-04-16},
  keywords = {todo},
  note = {Li20trainBigThenCompress
\par
I think this says that my phone's small LLM, is a compressed large LLM. ~
\par
Figure 1 or Figure 2 could be presentation slides. ~Note how different this is that what I've usually thought. ~
\par
But \textbf{INSIGHT}: this is tens of million param trainsformers, trained on GIGANTIC data (could look up how big those corpora are). 
\par
\textbf{TODO}: this is on the same theme as (Pandey, 2024) and its Chinchilla ancestors. ~Link and relate them.
\par
\section{Summary}

\par
The most compute-efficient training strategy is to:
\par
\begin{itemize}

\item train huge models and stop after a small number of iterations.
\item then, since large models are more robust to compression that small ones, compress this big one
\item ``compression,'' includes quantization and pruning.

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Li20trainBigThenCompress.pdf}
}

@misc{Raieli24CosineSimEmbed,
  title = {Cosine {{Similarity}} and {{Embeddings Are Still}} in {{Love}}?},
  author = {Raieli, Salvatore},
  year = {2024},
  month = mar,
  journal = {Medium},
  url = {https://levelup.gitconnected.com/cosine-similarity-and-embeddings-are-still-in-love-f9aec98396a4},
  urldate = {2024-04-16},
  abstract = {Cosine similarity is the most used method, but it is really the best?},
  langid = {english},
  note = {Raieli24CosineSimEmbed
\par
Reasons to believe that measuring similarity between embeddings with the cosine similarity is not the best. ~Still, it's the most popular.
\par
Also (Steck et al., 2024) might be more concise and limited than this article.}
}

@misc{Steck24isCosSimEmbedSim,
  title = {Is {{Cosine-Similarity}} of {{Embeddings Really About Similarity}}?},
  author = {Steck, Harald and Ekanadham, Chaitanya and Kallus, Nathan},
  year = {2024},
  month = mar,
  eprint = {2403.05440},
  primaryclass = {cs},
  doi = {10.1145/3589335.3651526},
  url = {http://arxiv.org/abs/2403.05440},
  urldate = {2024-04-16},
  abstract = {Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless `similarities.' For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models; these have implicit and unintended effects when taking cosine-similarities of the resulting embeddings, rendering results opaque and possibly arbitrary. Based on these insights, we caution against blindly using cosine-similarity and outline alternatives.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Steck24isCosSimEmbedSim
\par
Shows how cosine sim of embeddings can lose meaning for complicated models w/ regularization, offers some alternatives that I haven't looked at.
\par
Also (Raieli, 2024), for an expansion of this idea, but just a medium article citing related articles.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Steck24isCosSimEmbedSim.pdf}
}

@article{Mikolov13distribRepWordsSkipGram,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
  year = {2013},
  journal = {Advances in neural information processing systems},
  volume = {26},
  url = {https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
  urldate = {2024-04-16},
  note = {Mikolov13distribRepWordsSkipGram
\par
Original SkipGram embedding paper. ~
\par
\begin{itemize}

\item Fig 3 could be a slide. ~
\item 
\par
Could ask
\par
\begin{itemize}

\item China looks closer to Russia than Bejing, even though Bejing is \emph{inside} China. ~Huh?
\item 
\par
What kind of similarity are you looking for?'' ~Vertical is space, horizontal is ``capitalness.'' ~Cos similarity doesn't distinguish
\par
\begin{itemize}

\item So alternative cos sims: (Steck et al., 2024)

\end{itemize}

\end{itemize}

\item Still it works, and cosine similarity is the most popular metric: (Raieli, 2024)
\item \textbf{TODO}: if use this for a slide, squish it so it's square, not rectangular. 

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Mikolov13distribRepWordsSkipGram.pdf}
}

@misc{Zhou23benchmarkCheatLLM,
  title = {Don't {{Make Your LLM}} an {{Evaluation Benchmark Cheater}}},
  author = {Zhou, Kun and Zhu, Yutao and Chen, Zhipeng and Chen, Wentong and Zhao, Wayne Xin and Chen, Xu and Lin, Yankai and Wen, Ji-Rong and Han, Jiawei},
  year = {2023},
  month = nov,
  number = {arXiv:2311.01964},
  eprint = {2311.01964},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.01964},
  url = {http://arxiv.org/abs/2311.01964},
  urldate = {2024-04-16},
  abstract = {Large language models{\textasciitilde}(LLMs) have greatly advanced the frontiers of artificial intelligence, attaining remarkable improvement in model capacity. To assess the model performance, a typical approach is to construct evaluation benchmarks for measuring the ability level of LLMs in different aspects. Despite that a number of high-quality benchmarks have been released, the concerns about the appropriate use of these benchmarks and the fair comparison of different models are increasingly growing. Considering these concerns, in this paper, we discuss the potential risk and impact of inappropriately using evaluation benchmarks and misleadingly interpreting the evaluation results. Specially, we focus on a special issue that would lead to inappropriate evaluation, {\textbackslash}ie {\textbackslash}emph\{benchmark leakage\}, referring that the data related to evaluation sets is occasionally used for model training. This phenomenon now becomes more common since pre-training data is often prepared ahead of model test. We conduct extensive experiments to study the effect of benchmark leverage, and find that it can dramatically boost the evaluation results, which would finally lead to an unreliable assessment of model performance. To improve the use of existing evaluation benchmarks, we finally present several guidelines for both LLM developers and benchmark maintainers. We hope this work can draw attention to appropriate training and evaluation of LLMs.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Zhou23benchmarkCheatLLM
\par
LLM benchmarks can be easily or accidentally cheated upon b/c many are based on Foundation models trained on the same data as in the benchmark test set.
\par
\begin{itemize}

\item probably pointed to in (HAI, 2024)
\item 
\par
almost unavoidable once run out of data: 
\par
\begin{itemize}

\item in 2024: \emph{somebody} I can't refind
\item in 2026: (Villalobos et al., 2022) 

\end{itemize}

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhou23benchmarkCheatLLM.pdf}
}

@misc{AICodeKing24OpenWebUIOllama,
  title = {{{OpenWebUI}} + {{Ollama}} : {{Stop}} Paying for {{ChatGPT}} with This {{New}}, {{Free}} \& {{Easy Alternative}}.},
  shorttitle = {{{OpenWebUI}} + {{Ollama}}},
  author = {{AICodeKing}},
  year = {2024},
  month = apr,
  url = {https://www.youtube.com/watch?v=s26jB92xsco},
  urldate = {2024-04-17},
  abstract = {In this video, We'll be talking about how to self-host your own ChatGPT Clone that is better \& faster than ChatGPT / GPT-4 Itself. It is a fully free option for those who don't want to pay for ChatGPT and are looking for a free and better alternativ to it. You can use it with Mistral AI's Mixtral 8x22b, LLaMa, Gemini and OpenAI API. [Key Takeaways]  Embracing open-source tools for AI tasks, stepping back from subscription services.  Challenges with outdated AI services like ChatGPT led to exploration of alternatives.  Discovered Ollama as a self-hosted solution for serving open-source models.  OpenWebUI identified as a compatible frontend for Ollama, providing a familiar interface.  Installation process: Ollama for backend, Mistral model, and OpenWebUI for frontend.  Seamlessly chat with Mistral model through OpenWebUI, offering flexibility for various inputs.  Personal preference for Mistral due to chat-focused usage, but options available for image/document inputs.  Recommending Ollama and OpenWebUI for AI tasks, inviting engagement from viewers. [Additional Tags \& Keywords] chatgpt local chatgpt chatgpt local install local ai chatbot openai chatgpt free chatgpt local chatgpt install gpt4all free chatgpt run chatgpt locally chatgpt demo run ai chatbot locally train chatgpt on pdf free chatgpt alternatives chatgpt local download chatgpt ai chatbot chat-gpt local local gpt chatgpt4 host chatgpt chatgpt free mixtral 8x22b mixtral mistral ai mistral mixtral 8x7b test mixtral 8x22b mistral 8x22b uncensored mixtral mixtral installation how to install mixtral mistral 7b 8x22b uncensored mistral ai m3 max mixtral 8x7b mistral 8x7b how to install uncensored mistral mixtral of experts m3 max 128gb mixtral mistral next m3 max mixtral mission mistral ai\_,mistral 7bx8 mistral ai 7b mistral small mixture of experts mistral mixture of experts local llm llm local ai open source llm local llama llm tutorial llm locally local llm python run llm locally install local llm local ai chatbot local llm run on android llama 2 local llama local llms llm local deployment localai run llama locally how to run llm models locally locally local models install llm mistral llm how to create local chatbots how to run llms locally run ai locally local text ai local agents local chatgpt},
  keywords = {todo},
  note = {AICodeKing24OpenWebUIOllama
\par
Local install LLM, with Docker, runs in your browser. ~Many kinds of LLMs available, including ones that work with images and documents (see (ollama, 2024))
\par
\begin{itemize}

\item Based on ollama local LLM handler (ollama, 2024)
\item May need to do the unstructured document processing (Robinson, 2024) yourself, with this package
\item but for local RAG, see RAGflow, also based on ollama

\end{itemize}

\par
Guy installs text-based Mistral.}
}

@misc{InfiniFlow24RAGFlowDocStruct,
  title = {{{RAGFlow}}: {{Customizable}}, {{Credible}}, {{Explainable RAG}} Engine Based on Document Structure Recognition{\dots}},
  shorttitle = {{{RAGFlow}}},
  author = {InfiniFlow},
  year = {2024},
  month = apr,
  journal = {Medium},
  url = {https://medium.com/@infiniflowai/ragflow-customizable-credible-explainable-rag-engine-based-on-document-structure-recognition-6a2a2369bd2a},
  urldate = {2024-04-17},
  abstract = {Following the official open-sourcing of the AI-native database Infinity at the end of 2023, our end-to-end RAG solution, RAGFlow, was also{\dots}},
  langid = {english},
  keywords = {todo},
  note = {InfiniFlow24RAGFlowDocStruct
\par
Open Source RAG that can process lots of doc types, including pdfs, and can point to the place in a doc where it gets its info.
\par
\begin{itemize}

\item Based on (ollama, 2024),
\item so go there to see what local models can be installed
\item I'm \textbf{assuming} that RAGflow can do local{\dots}
\item Document structure parsing (Robinson, 2024) seems to be OCR (InfiniFlow, 2024)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\InfiniFlow24RAGFlowDocStruct.html}
}

@misc{ollama24ollamaGitHub,
  title = {Ollama {{GitHub}}},
  author = {{ollama}},
  year = {2024},
  month = apr,
  url = {https://github.com/ollama/ollama},
  urldate = {2024-04-17},
  abstract = {Get up and running with Llama 2, Mistral, Gemma, and other large language models.},
  copyright = {MIT},
  howpublished = {Ollama},
  keywords = {todo},
  note = {ollama24ollamaGitHub
\par
LLM server for local models
\par
GitHub says, on 4/17/24, that:
\par
\begin{itemize}

\item it works on MacOS, and has a Windows preview.
\item has 17 LLM options
\item Need {$>$}=8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.
\item 
\par
Is integrated with \href{https://github.com/ollama/ollama?tab=readme-ov-file\#web--desktop}{many 3rd party apps} e.g.
\par
\begin{itemize}

\item (``OpenWebUI + Ollama : Stop paying for ChatGPT with this New, Free \& Easy Alternative.'', 2024)
\item But more interesting is open source RAGflow (InfiniFlow, 2024) ?

\end{itemize}

\end{itemize}}
}

@misc{InfiniFlow24ndleHaystckLLMdeathRAG,
  title = {{{LLM}}'s Ability to Find Needles in a Haystack Signifies the Death of {{RAG}}?},
  author = {InfiniFlow},
  year = {2024},
  month = apr,
  journal = {Medium},
  url = {https://medium.com/@infiniflowai/llms-ability-to-find-needles-in-a-haystack-signifies-the-death-of-rag-0414d8818463},
  urldate = {2024-04-17},
  abstract = {Since February 24th, there have been significant news in the field of AI. We were a bit behind in addressing these pieces of AI news, as we{\dots}},
  langid = {english},
  keywords = {todo},
  note = {InfiniFlow24ndleHaystckLLMdeathRAG},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\InfiniFlow24ndleHaystckLLMdeathRAG.pdf}
}

@misc{Zhang23genDiffusion3DShape2Vec,
  title = {{{3DShape2VecSet}}: {{A 3D Shape Representation}} for {{Neural Fields}} and {{Generative Diffusion Models}}},
  shorttitle = {{{3DShape2VecSet}}},
  author = {Zhang, Biao and Tang, Jiapeng and Niessner, Matthias and Wonka, Peter},
  year = {2023},
  month = may,
  number = {arXiv:2301.11445},
  eprint = {2301.11445},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.11445},
  url = {http://arxiv.org/abs/2301.11445},
  urldate = {2024-04-17},
  abstract = {We introduce 3DShape2VecSet, a novel shape representation for neural fields designed for generative diffusion models. Our shape representation can encode 3D shapes given as surface models or point clouds, and represents them as neural fields. The concept of neural fields has previously been combined with a global latent vector, a regular grid of latent vectors, or an irregular grid of latent vectors. Our new representation encodes neural fields on top of a set of vectors. We draw from multiple concepts, such as the radial basis function representation and the cross attention and self-attention function, to design a learnable representation that is especially suitable for processing with transformers. Our results show improved performance in 3D shape encoding and 3D shape generative modeling tasks. We demonstrate a wide variety of generative applications: unconditioned generation, category-conditioned generation, text-conditioned generation, point-cloud completion, and image-conditioned generation.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Zhang23genDiffusion3DShape2Vec
\par
A 3D shape embedder that makes shapes queriable. ~From fig 3, measured points on a chair are like an LLM ``sentence.'' ~Get converted to positional embeddings, seems like the goal is to compress further and regenerate original shape (Transformer based). ~Seems similar, but way fancier than the way word2vec learns a small word representation that can reproduce the word after decoding (Mikolov et al., 2013).
\par
Text prompts generate new shapes, seems better than other deep learning techniques.
\par
Good pictures. ~Use in talk?
\par
Comment: Accepted by SIGGRAPH 2023 (Journal Track), Project website: https://1zb.github.io/3DShape2VecSet/, Project demo: https://youtu.be/KKQsQccpBFk},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhang23genDiffusion3DShape2Vec.pdf}
}

@article{Regenwetter22deepGenMdlsEngrRvw,
  title = {Deep {{Generative Models}} in {{Engineering Design}}: {{A Review}}},
  shorttitle = {Deep {{Generative Models}} in {{Engineering Design}}},
  author = {Regenwetter, Lyle and Nobari, Amin Heyrani and Ahmed, Faez},
  year = {2022},
  month = mar,
  journal = {Journal of Mechanical Design},
  volume = {144},
  number = {071704},
  issn = {1050-0472},
  doi = {10.1115/1.4053859},
  url = {https://doi.org/10.1115/1.4053859},
  urldate = {2024-04-17},
  abstract = {Automated design synthesis has the potential to revolutionize the modern engineering design process and improve access to highly optimized and customized products across countless industries. Successfully adapting generative machine learning to design engineering may enable such automated design synthesis and is a research subject of great importance. We present a review and analysis of deep generative machine learning models in engineering design. Deep generative models (DGMs) typically leverage deep networks to learn from an input dataset and synthesize new designs. Recently, DGMs such as feedforward neural networks (NNs), generative adversarial networks (GANs), variational autoencoders (VAEs), and certain deep reinforcement learning (DRL) frameworks have shown promising results in design applications like structural optimization, materials design, and shape synthesis. The prevalence of DGMs in engineering design has skyrocketed since 2016. Anticipating the continued growth, we conduct a review of recent advances to benefit researchers interested in DGMs for design. We structure our review as an exposition of the algorithms, datasets, representation methods, and applications commonly used in the current literature. In particular, we discuss key works that have introduced new techniques and methods in DGMs, successfully applied DGMs to a design-related domain, or directly supported the development of DGMs through datasets or auxiliary methods. We further identify key challenges and limitations currently seen in DGMs across design fields, such as design creativity, handling constraints and objectives, and modeling both form and functional performance simultaneously. In our discussion, we identify possible solution pathways as key areas on which to target the future work.},
  keywords = {todo},
  note = {Regenwetter22deepGenMdlsEngrRvw
\par
Maybe a good survey paper to start with on generative engineering. ~Seems mostly on mech eng and materials, but it's pretty short, and may give hints.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Regenwetter22deepGenMdlsEngrRvw.pdf}
}

@misc{Gerkin22digitizSmellMdlMap,
  title = {Digitizing {{Smell}}: {{Using Molecular Maps}} to {{Understand Odor}}},
  shorttitle = {Digitizing {{Smell}}},
  author = {Gerkin, Richard C. and Wiltschko, Alexander B.},
  year = {2022},
  month = sep,
  url = {http://research.google/blog/digitizing-smell-using-molecular-maps-to-understand-odor/},
  urldate = {2024-04-17},
  abstract = {How can we measure a smell? Smells are produced by molecules that waft through the air, enter our noses, and bind to sensory receptors. Potentially billions of molecules can produce a smell, so figuring out which ones produce which smells is difficult to catalog or predict. Sensory maps can help us solve this problem. Color vision has the most familiar examples of these maps, from the color wheel we each learn in primary school to more sophisticated variants used to perform color correction in video production. While these maps have existed for centuries, useful maps for smell have been missing, because smell is a harder problem to crack: molecules vary in many more ways than photons do; data collection requires physical proximity between the smeller and smell (we don't have good smell ``cameras'' and smell ``monitors''); and the human eye only has three sensory receptors for color while the human nose has {$>$} 300 for odor. As a result, previous efforts to produce odor maps have failed to gain traction. In 2019, we developed a graph neural network (GNN) model that began to explore thousands of examples of distinct molecules paired with the smell labels that they evoke, e.g., ``beefy'', ``floral'', or ``minty'', to learn the relationship between a molecule's structure and the probability that such a molecule would have each smell label. The embedding space of this model contains a representation of each molecule as a fixed-length vector describing that molecule in terms of its odor, much as the RGB value of a visual stimulus describes its color.},
  langid = {english},
  note = {Gerkin22digitizSmellMdlMap
\par
Funny, interesting embedding, the ``Principal Odor Map'' (POM),'' (Gerkin and Wiltschko, 2022, pp. -)
\par
First graph would be funny. ~``Principal Odor Graph'' is funny. ~Pointing out cabbage is funny.
\par
Mapping from words to chemicals was done with a graph neural net. ~I didn't find a generative method that used this, but it's still an embedding.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gerkin22digitizSmellMdlMap.pdf}
}

@misc{News23AICracksCode,
  title = {{{AI Cracks}} the {{Code}} on {{Odor Perception}}},
  author = {Kreeger, Karen},
  year = {2023},
  month = sep,
  journal = {Neuroscience News},
  url = {https://neurosciencenews.com/odor-perception-ai-23858/},
  urldate = {2024-04-17},
  abstract = {The study validates a first-of-its-kind data-driven map of human olfaction, which correlates chemical structure to odor perception.},
  langid = {american}
}

@misc{YannicKilcher24HugFaceHacked,
  title = {Hugging {{Face}} Got Hacked},
  author = {{Yannic Kilcher}},
  year = {2024},
  month = apr,
  url = {https://www.youtube.com/watch?v=ZcoOW8nqVP8},
  urldate = {2024-04-17},
  abstract = {Links: Homepage: https://ykilcher.com Merch: https://ykilcher.com/merch YouTube: ~~~/~yannickilcher~~ Twitter: ~~/~ykilcher~~ Discord: https://ykilcher.com/discord LinkedIn: ~~/~ykilcher~~ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https://www.subscribestar.com/yannick... Patreon: ~~/~yannickilcher~~ Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n},
  keywords = {todo},
  note = {YannicKilcher24HugFaceHacked
\par
All of the hugging face got hacked by the Wiz security firm, sometime in 4/2024, in spite of precautions. ~A buried pickle file hidden in \emph{secureish file format} model did it, and hugging fact itself was tricked into running the malicious model.
\par
Here's the Wiz blog post about their hack: (Tamari and Sagi Tzadik, 2024)}
}

@misc{Tamari24HuggingFaceSecurity,
  title = {Hugging {{Face}} Works with {{Wiz}} to Strengthen {{AI}} Cloud Security},
  author = {Tamari, Shir and {Sagi Tzadik}},
  year = {2024},
  month = apr,
  journal = {Wiz Blog},
  url = {https://www.wiz.io/blog/wiz-and-hugging-face-address-risks-to-ai-infrastructure},
  urldate = {2024-04-17},
  abstract = {Wiz researchers find architecture risks that may compromise AI-as-a-Service providers and risk customer data; works with Hugging Face on mitigations.},
  langid = {american},
  keywords = {todo},
  note = {Tamari24HuggingFaceSecurity
\par
Wiz security firm proved that it could hack Hugging face. ~See the video explanation here: (``Hugging Face got hacked'', 2024)}
}

@misc{Martinez23reEvalGPT4Bar,
  type = {{{SSRN Scholarly Paper}}},
  title = {Re-{{Evaluating GPT-4}}'s {{Bar Exam Performance}}},
  author = {Mart{\'i}nez, Eric},
  year = {2023},
  month = may,
  number = {4441311},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.4441311},
  url = {https://papers.ssrn.com/abstract=4441311},
  urldate = {2024-04-17},
  abstract = {Perhaps the most widely touted of GPT-4's at-launch, zero-shot capabilities has been its reported 90th-percentile performance on the Uniform Bar Exam. This paper begins by investigating the methodological challenges in documenting and verifying the 90th-percentile claim, presenting four sets of findings that indicate that OpenAI's estimates of GPT-4's UBE percentile are overinflated.First, although GPT-4's UBE score nears the 90th percentile when examining approximate conversions from February administrations of the Illinois Bar Exam, these estimates are heavily skewed towards repeat test-takers who failed the July administration and score significantly lower than the general test-taking population. Second, data from a recent July administration of the same exam suggests GPT-4's overall UBE percentile was below the 69th percentile, and \${\textbackslash}sim\$48th percentile on essays. Third, examining official NCBE data and using several conservative statistical assumptions, GPT-4's performance against first-time test takers is estimated to be \${\textbackslash}sim\$62nd percentile, including \${\textbackslash}sim\$42nd percentile on essays. Fourth, when examining only those who passed the exam (i.e. licensed or license-pending attorneys), GPT-4's performance is estimated to drop to \${\textbackslash}sim\$48th percentile overall, and \${\textbackslash}sim\$15th percentile on essays.In addition to investigating the validity of the percentile claim, the paper also investigates the validity of GPT-4's reported scaled UBE score of 298. The paper successfully replicates the MBE score, but highlights several methodological issues in the grading of the MPT + MEE components of the exam, which call into question the validity of the reported essay score.Finally, the paper investigates the effect of different hyperparameter combinations on GPT-4's MBE performance, finding no significant effect of adjusting temperature settings, and a significant effect of few-shot chain-of-thought prompting over basic zero-shot prompting. Taken together, these findings carry timely insights for the desirability and feasibility of outsourcing legally relevant tasks to AI models, as well as for the importance for AI developers to implement rigorous and transparent capabilities evaluations to help secure safe and trustworthy AI.},
  langid = {english},
  note = {Martinez23reEvalGPT4Bar
\par
Nah, it wasn't scoring in the 90\% percentile, as was announced somewhere when GPT4 come out. ~In fact, it got 15\%. or 45\%, depending upon what you look at. ~See Yannic Kilcher's comments on this \href{https://youtu.be/ZcoOW8nqVP8?t=215}{here}.
\par
But I barely looked at this paper, but I should. ~Also find the 90\% claim, and put that in my ``AI disappointments'' section, hopefully with the 90\% claim appearing a slide or two before it.
\par
I wonder if test/train pollution could also be a factor: (Zhou et al., 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Martinez23reEvalGPT4Bar.pdf}
}

@misc{Drum24LetDelveMedical,
  title = {Let's Delve into Medical Studies},
  author = {Drum, Kevin},
  year = {2024},
  month = mar,
  journal = {Kevin Drum},
  url = {https://jabberwocking.com/lets-delve-into-medical-studies/},
  urldate = {2024-04-17},
  abstract = {Some food for thought: Are medical studies being written with ChatGPT? Well, we all know ChatGPT overuses the word},
  langid = {american},
  keywords = {todo},
  note = {Drum24LetDelveMedical
\par
Something funny for the talk under ``creativity'' or ``cheating''
\par
\href{https://aiphrasefinder.com/why-does-chatgpt-use-delve-a-lot/\#more-1387}{AI phrase finder} says ``delve'' is one of chatGPT's \href{https://aiphrasefinder.com/common-chatgpt-words/}{10 most common} (must mean overused?) words. Yannic Kilcher \href{https://youtu.be/ZcoOW8nqVP8?t=383}{says} it means more used than humans.
\par
The use of Delve in in medical publications skyrocketed in 2023. ~GPT-4 was introduced \href{https://en.wikipedia.org/wiki/GPT-4}{on March 4 2023}. ~Given the word freq. diagram on March 30, 2024, it looks like delve will be an even bigger hit in 2024.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Drum24LetDelveMedical.pdf}
}

@misc{Cahyawijaya24HighDimensionHumanValue,
  title = {High-{{Dimension Human Value Representation}} in {{Large Language Models}}},
  author = {Cahyawijaya, Samuel and Chen, Delong and Bang, Yejin and Khalatbari, Leila and Wilie, Bryan and Ji, Ziwei and Ishii, Etsuko and Fung, Pascale},
  year = {2024},
  month = apr,
  number = {arXiv:2404.07900},
  eprint = {2404.07900},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.07900},
  url = {http://arxiv.org/abs/2404.07900},
  urldate = {2024-04-18},
  abstract = {The widespread application of Large Language Models (LLMs) across various tasks and fields has necessitated the alignment of these models with human values and preferences. Given various approaches of human value alignment, ranging from Reinforcement Learning with Human Feedback (RLHF), to constitutional learning, etc. there is an urgent need to understand the scope and nature of human values injected into these models before their release. There is also a need for model alignment without a costly large scale human annotation effort. We propose UniVaR, a high-dimensional representation of human value distributions in LLMs, orthogonal to model architecture and training data. Trained from the value-relevant output of eight multilingual LLMs and tested on the output from four multilingual LLMs, namely LlaMA2, ChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare the distribution of human values embedded in different LLMs with different langauge sources. Through UniVaR, we explore how different LLMs prioritize various values in different languages and cultures, shedding light on the complex interplay between human values and language modeling.},
  archiveprefix = {arXiv},
  note = {Cahyawijaya24humanValuesVecLLM
\par
Somehow high dimensional vectors are extracted which represent human values (what is good; what is bad, I suppose). ~
\par
I have no idea{\dots}
\par
But this is supposed to make it cheap to align LLMs without expensive reannotation of good and bad responses, as in reinforcement learning, I think it says.
\par
Again, I have no idea{\dots}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Cahyawijaya24humanValuesVecLLM.pdf}
}

@misc{Geiger24FindingAlignmentsInterpretable,
  title = {Finding {{Alignments Between Interpretable Causal Variables}} and {{Distributed Neural Representations}}},
  author = {Geiger, Atticus and Wu, Zhengxuan and Potts, Christopher and Icard, Thomas and Goodman, Noah D.},
  year = {2024},
  month = feb,
  number = {arXiv:2303.02536},
  eprint = {2303.02536},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.02536},
  url = {http://arxiv.org/abs/2303.02536},
  urldate = {2024-04-18},
  abstract = {Causal abstraction is a promising theoretical framework for explainable artificial intelligence that defines when an interpretable high-level causal model is a faithful simplification of a low-level deep learning system. However, existing causal abstraction methods have two major limitations: they require a brute-force search over alignments between the high-level model and the low-level one, and they presuppose that variables in the high-level model will align with disjoint sets of neurons in the low-level one. In this paper, we present distributed alignment search (DAS), which overcomes these limitations. In DAS, we find the alignment between high-level and low-level models using gradient descent rather than conducting a brute-force search, and we allow individual neurons to play multiple distinct roles by analyzing representations in non-standard bases-distributed representations. Our experiments show that DAS can discover internal structure that prior approaches miss. Overall, DAS removes previous obstacles to conducting causal abstraction analyses and allows us to find conceptual structure in trained neural nets.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Geiger24alignCausalNeuralRep
\par
I belive that somehow, a causal model is extracted from a trained LLM. (Wu et al., 2024) says that it does this by intervening on the representations (vectors that start out as embedded tokens).},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Geiger24FindingAlignmentsInterpretable.pdf}
}

@misc{Naveed24overviewLLM,
  title = {A {{Comprehensive Overview}} of {{Large Language Models}}},
  author = {Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  year = {2024},
  month = apr,
  number = {arXiv:2307.06435},
  eprint = {2307.06435},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.06435},
  url = {http://arxiv.org/abs/2307.06435},
  urldate = {2024-04-18},
  abstract = {Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.},
  archiveprefix = {arXiv},
  note = {Naveed24overviewLLM
\par
Kind of a huge glossary, with big tables of papers and tests. ~Likely boring to read, but could be a good target for a RAG system, if I ever get a decent one.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Naveed24overviewLLM.pdf}
}

@misc{Yan24berkeleyFuncCallLeaderBrd,
  title = {Berkeley Function Calling Leaderboard},
  author = {Yan, Fanjia and Mao, Huanzhi and Ji, Charlie Cheng-Jie and Zhang, Tianjun and Patil, Shishir G. and Stoica, Ion and Gonzalez, Joseph E.},
  year = {2024},
  publisher = {Berkeley},
  url = {https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html},
  keywords = {todo},
  note = {Yan24berkeleyFuncCallLeaderBrd
\par
A leaderboard that measures LLM ability to recognize that it should call a funciton in response to a prompt. ~Apparently, functions are defined in advance, and the LLM gets a gold star when your prompt is answered by generated text and the result of the appropriate call -- but the board reports many metrics e.g. relevance detection, some kind of AST generation,{\dots} and answering accuracy.
\par
I haven't read this document, only viewed the page at the leaderboard URL. ~I \emph{shouldn't say much about it in public until I've at least skimmed the doc.}
\par
\textbf{At the \href{https://gorilla.cs.berkeley.edu/leaderboard.html}{leaderboard URL}:} 
\par
Interesting ``squares within squares plots of error totals and types'' ~Model with the tiniest error box is actually the best one.
\par
\textbf{Good Graphic} at the end of this page are ~few examples, showing the prompt, the function it should have called, and the generated calls. ~Could use this in the presentation slides.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Yan24berkeleyFuncCallLeaderBrd.html}
}

@misc{Stenberg24theIinLLMintelligence,
  title = {The {{I}} in {{LLM}} Stands for Intelligence {\textbar} Daniel.Haxx.Se},
  author = {Stenberg, Daniel},
  year = {2024},
  month = jan,
  url = {https://daniel.haxx.se/blog/2024/01/02/the-i-in-llm-stands-for-intelligence/},
  urldate = {2024-04-20},
  abstract = {I have held back on writing anything about AI or how we (not) use AI for development in the curl factory. Now I can't hold back anymore. Let me show you the most significant effect of AI on curl as of today -- with examples.a},
  langid = {american},
  keywords = {todo},
  note = {Stenberg24theIinLLMintelligence
\par
A screenshot for the AI disappointment, or the starting quote for the errors section.
\par
If screenshot, then the full web page also has a pciture of the author.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Stenberg24theIinLLMintelligence.pdf}
}

@misc{Mukherjee24PolarisSafetyfocusedLLM,
  title = {Polaris: {{A Safety-focused LLM Constellation Architecture}} for {{Healthcare}}},
  shorttitle = {Polaris},
  author = {Mukherjee, Subhabrata and Gamble, Paul and Ausin, Markel Sanz and Kant, Neel and Aggarwal, Kriti and Manjunath, Neha and Datta, Debajyoti and Liu, Zhengliang and Ding, Jiayuan and Busacca, Sophia and Bianco, Cezanne and Sharma, Swapnil and Lasko, Rae and Voisard, Michelle and Harneja, Sanchay and Filippova, Darya and Meixiong, Gerry and Cha, Kevin and Youssefi, Amir and Buvanesh, Meyhaa and Weingram, Howard and {Bierman-Lytle}, Sebastian and Mangat, Harpreet Singh and Parikh, Kim and Godil, Saad and Miller, Alex},
  year = {2024},
  month = mar,
  number = {arXiv:2403.13313},
  eprint = {2403.13313},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.13313},
  url = {http://arxiv.org/abs/2403.13313},
  urldate = {2024-04-20},
  abstract = {We develop Polaris, the first safety-focused LLM constellation for real-time patient-AI healthcare conversations. Unlike prior LLM works in healthcare focusing on tasks like question answering, our work specifically focuses on long multi-turn voice conversations. Our one-trillion parameter constellation system is composed of several multibillion parameter LLMs as co-operative agents: a stateful primary agent that focuses on driving an engaging conversation and several specialist support agents focused on healthcare tasks performed by nurses to increase safety and reduce hallucinations. We develop a sophisticated training protocol for iterative co-training of the agents that optimize for diverse objectives. We train our models on proprietary data, clinical care plans, healthcare regulatory documents, medical manuals, and other medical reasoning documents. We align our models to speak like medical professionals, using organic healthcare conversations and simulated ones between patient actors and experienced nurses. This allows our system to express unique capabilities such as rapport building, trust building, empathy and bedside manner. Finally, we present the first comprehensive clinician evaluation of an LLM system for healthcare. We recruited over 1100 U.S. licensed nurses and over 130 U.S. licensed physicians to perform end-to-end conversational evaluations of our system by posing as patients and rating the system on several measures. We demonstrate Polaris performs on par with human nurses on aggregate across dimensions such as medical safety, clinical readiness, conversational quality, and bedside manner. Additionally, we conduct a challenging task-based evaluation of the individual specialist support agents, where we demonstrate our LLM agents significantly outperform a much larger general-purpose LLM (GPT-4) as well as from its own medium-size class (LLaMA-2 70B).},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Mukherjee24polarisHealthcareLLM
\par
Instruction-tuned foundation model trains itself as a nurse agent. ~ Does nurse calls reporting test results. ~Human nurses give it better grades than they give other human nurses.
\par
\begin{itemize}

\item Fox News says it will work for \href{https://www.foxbusiness.com/technology/nvidia-announces-ai-powered-health-care-agents-outperform-nurses-cost-9-hour}{\$9/hr}.
\item \href{https://www.hippocraticai.com/video}{Video demo call}
\item \href{https://youtu.be/pal-dMJFU6Q?t=336}{AI Explained video} chunk on it, which then goes back to the fake video AI he was talking about
\item \textbf{TODO}: make link to paper where some kind of instruction fine tuning came out on top.
\item Saved Hippocratic AI web page has good diagrams, good for talk?

\end{itemize}},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Mukherjee24polarisHealthcareLLM.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Mukherjee24polarisHealthcareLLM_web.html}
}

@article{Yang23diffusionModelSurvey,
  title = {Diffusion {{Models}}: {{A Comprehensive Survey}} of {{Methods}} and {{Applications}}},
  shorttitle = {Diffusion {{Models}}},
  author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  year = {2023},
  month = nov,
  journal = {ACM Computing Surveys},
  volume = {56},
  number = {4},
  pages = {105:1--105:39},
  issn = {0360-0300},
  doi = {10.1145/3626235},
  url = {https://doi.org/10.1145/3626235},
  urldate = {2024-04-21},
  abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy},
  keywords = {todo},
  note = {Yang23diffusionModelSurvey
\par
Mindmap in Fig 1 might be a good slide. ~Anyway, I should look at it and maybe skim this paper.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Yang23diffusionModelSurvey.pdf}
}

@misc{Elsevier24smallMoleculeDrugAI,
  title = {{{AI}} in Small Molecule Drug Discovery},
  author = {{Elsevier}},
  year = {2024},
  journal = {www.elsevier.com},
  url = {https://www.elsevier.com/industry/ai-in-small-molecule-drug-discovery},
  urldate = {2024-04-21},
  abstract = {Learn about opportunities to apply AI in small molecule drug discovery to increase speed, lower cost, improve success rates and boost innovation.},
  langid = {american},
  keywords = {todo},
  note = {Elsevier24smallMoleculeDrugAI
\par
Page says generative AI was used in a drug discover. ~Seems like many of them. 
\par
Good quote: 
\par
\begin{quotation}

\par
AI can't replace medicinal and synthetic chemists' intuition, nuance and creativity, but chemists who are AI-enabled will ultimately supersede chemists who are not. 
\par
\end{quotation}

\par
There's a web page tab called ``Upskilling of chemists''.
\par
This is the opposite of the Harvard study of (I think) marketing people, where AI deskilled the job, making unskilled workers more equal to skilled, experienced ones.
\par
\textbf{TODO}: link the Harvard paper here.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Eelsevier24smallMoleculeDrugAI.pdf}
}

@misc{Bolgar24cancerPersonalizedTrtmntAI,
  title = {How {{AI}} Can Help Cancer Patients Receive Personalized and Precise Treatment Faster},
  author = {Bolgar, Catherine},
  year = {24},
  month = apr,
  journal = {Source},
  url = {https://news.microsoft.com/source/features/digital-transformation/how-ai-can-help-cancer-patients-receive-personalized-and-precise-treatment-faster/},
  urldate = {2024-04-21},
  abstract = {Providence and Microsoft are developing research prototype AI tools to sort through growing mountains of patient data.},
  langid = {american},
  keywords = {todo},
  note = {Bolgar24cancerPersonalizedTrtmntAI
\par
Something about analyzing huge set of data coming from patients for personalized medicine. ~Says it's gen AI.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bolgar24cancerPersonalizedTrtmntAI.pdf}
}

@misc{Hui23shortTermEmplymntAI,
  title = {Artificial Intelligence and Its Short-Term Effects on Employment},
  author = {Hui, Xiang and Reshef, Oren and Zhou, Luofeng},
  year = {2023},
  month = dec,
  journal = {CEPR},
  url = {https://cepr.org/voxeu/columns/artificial-intelligence-and-its-short-term-effects-employment},
  urldate = {2024-04-21},
  abstract = {Much attention has been paid recently to the potential for generative AI to disrupt labour markets in the long term, rendering certain professions redundant whilst enhancing the productivity of human labour in others. However, relatively little attention has been paid to the impact AI has already had on the labour market. This column investigates the effect of generative AI models on affected professions, using data from an online labour platform showing that AI has diminished the potential earnings of exposed writers and image-related professionals across skills distribution.},
  langid = {english},
  keywords = {todo},
  note = {Hui23shortTermEmplymntAI
\par
Figure 2 would be a good slide. ~Generative AI definitely hurt job market for writers and graphics designers, and it didn't matter if they were high or low quality workers; it affected them equally. ~The authors say that employers, rather than employees are poised to benefit from AI, and that the effect might be more pronounced in the long run.
\par
Researchers watched for immediate ~employment effects of chatGPT, DALL-E2 and Midjourney on workers on the online job market, upWork, one of the largest in the world. ~It was expected to be fast acting because it's a spot market. ~It definitely hurt writers and graphics designers on that market. 
\par
chatGPT reduced the number of jobs and earlings of writers in the Upwork online labor market: -2\% of job numbers; -5.2\% earnings. ~Graphics designers (I think, although they don't use that or any other descriptive word): -2.1\% jobs and -5.2\% earnings.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hui23shortTermEmplymntAI.pdf}
}

@misc{MayaAkim24MergeLLMsBest,
  title = {Merge {{LLMs}} to {{Make Best Performing AI Model}}},
  author = {{Maya Akim}},
  year = {2024},
  month = mar,
  url = {https://www.youtube.com/watch?v=byf-y0P4hMg},
  urldate = {2024-04-21},
  abstract = {This video is about mergekit, how to choose and blend models. It's non technical but links to technical papers are included. You need to know how to navigate the terminal but no programming is required.   Join my Discord community: ~~/~discord~~  My tutorials on Medium: ~~/~mayaakim~~   My twitter profile: ~~/~maya\_akim~~   To rent a GPU from Massed Compute (mergekit preinstalled) follow the link  https://bit.ly/maya-akim Code for 50\% discount: MayaAkim All links: mergekit: https://github.com/arcee-ai/mergekit Open LLM Leaderboard https://huggingface.co/spaces/Hugging...  my huggingface profile (with model configs you can copy): https://huggingface.co/mayacinka git installation:  https://gitforwindows.org/ lfs installation:  https://docs.github.com/en/repositori... supported architecture for mergekit:  https://github.com/arcee-ai/mergekit/... best blog about mergekit: ~~/~merge-large-language-models-with-mergekit~~ other really good blog about mergekit: ~~/~merge-large-language-models~~ Charles Goddard's blog: (author of mergekit) https://goddard.blog/about/ Mona lisa with Mohawk https://www.designboom.com/technology... What is YAML: https://www.techtarget.com/searchitop... What is Data Contamination: https://bdtechtalks.com/2023/07/17/ll... Goodharts law https://www.cna.org/reports/2022/09/g... LazyMergekit: https://colab.research.google.com/dri... Auto evaluation: (requires runpod profile) https://colab.research.google.com/dri... configuration with 14 models merged: https://huggingface.co/EmbeddedLLM/Mi... MoE instructions: https://github.com/arcee-ai/mergekit/... higher density - better results https://github.com/arcee-ai/mergekit/... Model family tree: (visualization) https://colab.research.google.com/dri... https://huggingface.co/spaces/mlabonn... cost of training mistral:  https://www.ft.com/content/387eeeab-1... Leaderboard is disgusting: ~~/~open\_llm\_leaderboard\_is\_disgusting~~ Merging models with different architectures:  https://arxiv.org/pdf/2401.10491.pdf merging models different arch:  https://github.com/18907305772/FuseLLM Blending is all you need: https://arxiv.org/pdf/2401.02994.pdf Model soups https://arxiv.org/pdf/2203.05482.pdf Ties-merging research paper: https://arxiv.org/pdf/2306.01708.pdf Dare merge research paper: https://arxiv.org/pdf/2311.03099.pdf Task arithemtic: https://arxiv.org/pdf/2212.04089.pdf Benchmarks Arc benchmarks https://deepgram.com/learn/arc-llm-be... https://arxiv.org/pdf/1803.05457.pdf HellaSwag https://arxiv.org/pdf/1905.07830.pdf MMLU https://arxiv.org/pdf/2009.03300.pdf TrithfulQA https://arxiv.org/abs/2109.07958 WinoGrande https://arxiv.org/pdf/1907.10641.pdf GSM8K  https://arxiv.org/pdf/2110.14168.pdf overfitting problem Ann Lotz:  https://arstechnica.com/tech-policy/2... Benchmarks are a problem screenshots: https://analyticsindiamag.com/the-pro... ~~/~llm\_benchmarks\_are\_broken\_what\_can\_we\_do\_t...~~ ~~/~llm\_benchmarks\_are\_bullshit~~ Attributions: [https://commons.wikimedia.org/wiki/Fi...](https://commons.wikimedia.org/wiki/Fi...) Timecodes: 0:00 - 1:47 - blending intro 1:48 - 3:36 - promise of blending 3:37 - 4:22 - blending steps and requirements 4:23 - 5:05 - all you need is hardware  5:06 - 5:30 - mergekit installation 5:31 - 9:23 - merge methods 10:48 - 13:31 - configurations and yaml  13:32 - 14:38 - how to run merge  14:39 - 14:42 - upload merged model 14:43 - 16:27 - best merge method 16:28 - 20:16 benchmark problems, overfitting and contamination  \#mergekit \#llm \#localmodels},
  keywords = {todo},
  note = {MayaAkim24MergeLLMsBest
\par
Quick tutorial on model merging a.k.a. model blending. ~Shows you what to type. ~She can do this on her 16 GB Macbook (examples discussing chunking param allowing CPU and in RAM running). ~I fastforwarded though it. ~A remember a couple of points.
\par
\begin{itemize}

\item a way of gettng benefits of different models fine-tuned on different skills into a single model (in \href{https://www.youtube.com/watch?v=6IWPG1M_IoE}{another video}, author said that fine tuning was actually not that easy, but model merging is easy.
\item 
\par
several kinds of merging,
\par
\begin{itemize}

\item mainly based on vector adds of weights, results in same size model
\item but can also concatenate layers to get a bigger model (frankenmodel)

\end{itemize}

\item 
\par
for now, must be different fine-tuned models of the same core model. ~Same number of layers.
\par
\begin{itemize}

\item in that \href{https://www.youtube.com/watch?v=6IWPG1M_IoE}{another video} above, Mark McQuade says that goal is different model types
\item This \href{https://colab.research.google.com/drive/1s2eQlolcI1VGgDhqWIANfkfKvcKrMyNr}{notebook} computes the family tree of models, so can figure out which are mergeable, although I didn't understand it immediately, and still don't.

\end{itemize}

\item In several examples, you youself define the weighting between each model, but in that ~\href{https://www.youtube.com/watch?v=6IWPG1M_IoE}{other video}, somebody say some evolutionary algorithm works now.
\item \href{https://youtu.be/byf-y0P4hMg?t=1139}{overfitting} occurs when merged models are tuned on the same datasets. ~It seems like it might also contain data from the leaderboard test set. ~The author may be mixing overfitting with training set data leakage. ~She references (keisukegoda3804, 2024) which talks about ``overfitting on the validation set,'' so that's probably what this is.
\item Seems like she's not sure that she, in fact, has created some merged models with data leakage.

\end{itemize}}
}

@misc{Keisukegoda380424_24_disgustLdrBrd,
  type = {Reddit {{Post}}},
  title = {Open {{LLM}} Leaderboard Is Disgusting},
  author = {{keisukegoda3804}},
  year = {2024},
  month = jan,
  journal = {r/LocalLLaMA},
  url = {www.reddit.com/r/LocalLLaMA/comments/18xbevs/open_llm_leaderboard_is_disgusting/},
  urldate = {2024-04-21},
  abstract = {t's hilarious how overfit the top models are to the evals. Take jeonsworld/CarbonVillain-en-10.7B-v4, which currently tops the leaderboard. It's a merge of:},
  keywords = {todo},
  note = {Keisukegoda380424\_24\_disgustLdrBrd
\par
Reddit post on how merged models are ``overfitting to the validation set.'' ~In otherwords each model was tuned on the same validation set, merged, and then tested on the same validation set? ~This seems more like test/train leakage.
\par
A screenshot of this post could go into the section on LLM metric leakage.},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Keisukegoda380424_24_disgustLdrBrd.pdf;C\:\\Users\\scott\\Zotero\\storage\\ZLFQGLZ5\\open_llm_leaderboard_is_disgusting.html}
}

@misc{Ng24nextAgenticWorkflow,
  title = {What's next for {{AI}} Agentic Workflows},
  author = {Ng, Andrew},
  year = {2024},
  month = mar,
  url = {https://www.youtube.com/watch?v=sal78ACtGTc},
  urldate = {2024-04-21},
  abstract = {Andrew Ng, founder of DeepLearning.AI and AI Fund, speaks at Sequoia Capital's AI Ascent about what's next for AI agentic workflows and their potential to significantly propel AI advancements---perhaps even surpassing the impact of the forthcoming generation of foundational models.  \#AI \#AIAscent \#Sequoia \#Startup \#Founder \#entrepreneur},
  keywords = {todo},
  note = {Ng24nextAgenticWorkflow
\par
A lot of people reference this video. ~I think it will way that GPT3.5 exceeds performance of GPT4 if use agents.}
}

@misc{Llorente24TransformerApproachElectricity,
  title = {A {{Transformer}} Approach for {{Electricity Price Forecasting}}},
  author = {Llorente, Oscar and Portela, Jose},
  year = {2024},
  month = mar,
  number = {arXiv:2403.16108},
  eprint = {2403.16108},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.16108},
  url = {http://arxiv.org/abs/2403.16108},
  urldate = {2024-04-22},
  abstract = {This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model. As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism. Hence, showing that the attention layer is enough for capturing the temporal patterns. The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research. The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Llorente24elecPriceFrcstTransfrmr
\par
Is this probabilistic? ~Can it be made convex like in the convex neural thing @ UW?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Llorente24elecPriceFrcstTransfrmr.pdf}
}

@misc{Miret24AreLLMsReady,
  title = {Are {{LLMs Ready}} for {{Real-World Materials Discovery}}?},
  author = {Miret, Santiago and Krishnan, N. M. Anoop},
  year = {2024},
  month = feb,
  number = {arXiv:2402.05200},
  eprint = {2402.05200},
  primaryclass = {cond-mat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.05200},
  url = {http://arxiv.org/abs/2402.05200},
  urldate = {2024-04-22},
  abstract = {Large Language Models (LLMs) create exciting possibilities for powerful language processing tools to accelerate research in materials science. While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge. Given those shortcomings, we outline a framework for developing Materials Science LLMs (MatSci-LLMs) that are grounded in materials science knowledge and hypothesis generation followed by hypothesis testing. The path to attaining performant MatSci-LLMs rests in large part on building high-quality, multi-modal datasets sourced from scientific literature where various information extraction challenges persist. As such, we describe key materials science information extraction challenges which need to be overcome in order to build large-scale, multi-modal datasets that capture valuable materials science knowledge. Finally, we outline a roadmap for applying future MatSci-LLMs for real-world materials discovery via: 1. Automated Knowledge Base Generation; 2. Automated In-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials Laboratories.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Miret24readyMaterialDiscovLLM
\par
Says LLMs aren't ready for materials discovery. ~Don't I have papers saying they were helpful?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Miret24AreLLMsReady.pdf}
}

@misc{Dahl24HhallucLawLLM,
  title = {Hallucinating {{Law}}: {{Legal Mistakes}} with {{Large Language Models}} Are {{Pervasive}}},
  shorttitle = {Hallucinating {{Law}}},
  author = {Dahl, Matthew and Magesh, Varun and Suzgun,, Mirac and Ho, Daniel E.},
  year = {2024},
  month = jan,
  journal = {HAI Stanford},
  url = {https://hai.stanford.edu/news/hallucinating-law-legal-mistakes-large-language-models-are-pervasive},
  urldate = {2024-04-22},
  abstract = {A new study finds disturbing and pervasive errors among three popular models on a wide range of legal tasks.},
  langid = {english},
  keywords = {todo},
  note = {Dahl24HhallucLawLLM
\par
Be sure to link to
\par
\begin{itemize}

\item hallucination
\item instruction following fine tuning (verify it's the same one as the one that got a top score in that paper)
\item training data needed: see that it fails where models have fewer cases (lower courts, etc.)
\item creativity: unable to see 

\end{itemize}

\par
A summary article of this paper: (Dahl et al., 2024)
\par
LLM answers to 200K legal questions were 69-88\% hallucinations
\par
\section{Hallucination correlates}

\begin{enumerate}

\item \textbf{complex interpretations:} no better than random about legal precedent; court's ruling was 75\% hallucination
\item lower courts had high hallucinations. ~My guess is that these aren't cited as much, as higher, so fewer training examples
\item better with prominent cases, influential courts. ~Training data again.
\item worst for oldest and newest cases. ~Training data, again, I think. ~Detects a several year lag with current law
\item believe some justices have more cases than they did e.g. Justics Joseph Sotry.

\end{enumerate}

\par
Done in 2023 (best moldel is mostly GPT 3.5), but it seems like problems will remain
\par
\section{Counterfactual bias}

\par
Tend to belive that the premise in a question is true e.g. Ginsberg example. ~This is probably due to \textbf{instruction-following} training. ~It also seems to be overconfident exactly in cases where it tends to hallucinate.
\par
\section{Legal Monoculture}

\par
LLMs limited to ``narrow judicial perspective,'' may erase contributions of some members, may miss old but still relevant law (I put that in this category, not the authors).},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Dahl24HhallucLawLLM.pdf}
}

@misc{Dahl24LargeLegalFictions,
  title = {Large {{Legal Fictions}}: {{Profiling Legal Hallucinations}} in {{Large Language Models}}},
  shorttitle = {Large {{Legal Fictions}}},
  author = {Dahl, Matthew and Magesh, Varun and Suzgun, Mirac and Ho, Daniel E.},
  year = {2024},
  month = jan,
  number = {arXiv:2401.01301},
  eprint = {2401.01301},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.01301},
  url = {http://arxiv.org/abs/2401.01301},
  urldate = {2024-04-22},
  abstract = {Large language models (LLMs) have the potential to transform the practice of law, but this potential is threatened by the presence of legal hallucinations -- responses from these models that are not consistent with legal facts. We investigate the extent of these hallucinations using an original suite of legal queries, comparing LLMs' responses to structured legal metadata and examining their consistency. Our work makes four key contributions: (1) We develop a typology of legal hallucinations, providing a conceptual framework for future research in this area. (2) We find that legal hallucinations are alarmingly prevalent, occurring between 69\% of the time with ChatGPT 3.5 and 88\% with Llama 2, when these models are asked specific, verifiable questions about random federal court cases. (3) We illustrate that LLMs often fail to correct a user's incorrect legal assumptions in a contra-factual question setup. (4) We provide evidence that LLMs cannot always predict, or do not always know, when they are producing legal hallucinations. Taken together, these findings caution against the rapid and unsupervised integration of popular LLMs into legal tasks. Even experienced lawyers must remain wary of legal hallucinations, and the risks are highest for those who stand to benefit from LLMs the most -- pro se litigants or those without access to traditional legal resources.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Dahl24LargeLegalFictions
\par
Lots of legal mistakes. ~A short summary is in (Dahl et al., 2024). ~See my notes there.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Dahl24LargeLegalFictions.pdf}
}

@misc{Kilcher24MixtralExpertsPaperExpln,
  title = {Mixtral of {{Experts}} ({{Paper Explained}})},
  author = {Kilcher, Yannic},
  year = {2024},
  month = jan,
  publisher = {YouTube},
  url = {https://www.youtube.com/watch?v=mwO6v4BlgZQ},
  urldate = {2024-04-22},
  abstract = {\#mixtral \#mistral \#chatgpt  OUTLINE: 0:00 - Introduction 3:00 - Mixture of Experts 6:00 - Classic Transformer Blocks 11:15 - Expert Routing 17:00 - Sparse Expert Routing 22:00 - Expert Parallelism 25:00 - Experimental Results 31:30 - Routing Analysis 33:20 - Conclusion Paper: https://arxiv.org/abs/2401.04088 Abstract: We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license. Authors: Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L{\'e}lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th{\'e}ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth{\'e}e Lacroix, William El Sayed Links: Homepage: https://ykilcher.com Merch: https://ykilcher.com/merch YouTube: ~~~/~yannickilcher~~ Twitter: ~~/~ykilcher~~ Discord: https://ykilcher.com/discord LinkedIn: ~~/~ykilcher~~ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https://www.subscribestar.com/yannick... Patreon: ~~/~yannickilcher~~ Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n},
  note = {Interesting bits
\par
\begin{itemize}

\item the FFN expert is the same for all tokens (I thought it was unique per token)
\item 
\par
Expert tasks
\par
\begin{itemize}

\item they don't \href{https://youtu.be/mwO6v4BlgZQ?t=1897}{find any obvious specialization in what the experts do} based on topic
\item adjacent tokens often go to same expert
\item python: leading tokens e.g. def in python to to same tokens
\item doesn't mean there's not a pattern, Yannic says, could be that we just don't understand it.

\end{itemize}

\item Says he covers how router works in another video; this must be \href{https://youtu.be/ccBMRryxGog?t=478}{here}.
\item \href{https://www.youtube.com/watch?v=U8J32Z3qV8s}{This Stanford video} might be a little more rigorous

\end{itemize}}
}

@misc{Chen19optCntrlConvexNN,
  title = {Optimal {{Control Via Neural Networks}}: {{A Convex Approach}}},
  shorttitle = {Optimal {{Control Via Neural Networks}}},
  author = {Chen, Yize and Shi, Yuanyuan and Zhang, Baosen},
  year = {2019},
  month = feb,
  number = {arXiv:1805.11835},
  eprint = {1805.11835},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.11835},
  url = {http://arxiv.org/abs/1805.11835},
  urldate = {2024-04-22},
  abstract = {Control of complex systems involves both system identification and controller design. Deep neural networks have proven to be successful in many identification tasks, however, from model-based control perspective, these networks are difficult to work with because they are typically nonlinear and nonconvex. Therefore many systems are still identified and controlled based on simple linear models despite their poor representation capability. In this paper we bridge the gap between model accuracy and control tractability faced by neural networks, by explicitly constructing networks that are convex with respect to their inputs. We show that these input convex networks can be trained to obtain accurate models of complex physical systems. In particular, we design input convex recurrent neural networks to capture temporal behavior of dynamical systems. Then optimal controllers can be achieved via solving a convex model predictive control problem. Experiment results demonstrate the good potential of the proposed input convex neural network based approach in a variety of control applications. In particular we show that in the MuJoCo locomotion tasks, we could achieve over 10\% higher performance using 5* less time compared with state-of-the-art model-based reinforcement learning method; and in the building HVAC control example, our method achieved up to 20\% energy reduction compared with classic linear models.},
  archiveprefix = {arXiv},
  note = {Chen19optCntrlConvexNN
\par
Recurrent NNs that are convex-in-input used in optimal control. ~From UW's clean energy intstitute:\\
\href{https://www.cei.washington.edu/research/energy-systems/}{https://www.cei.washington.edu/research/energy-systems/}
\par
Comment: Published as a conference paper at ICLR 2019: https://openreview.net/forum?id=H1MW72AcK7},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chen19optCntrlConvexNN.pdf}
}

@article{VanHentenryck24AI4OPTAIInstitute,
  title = {{{AI4OPT}}: {{AI Institute}} for {{Advances}} in {{Optimization}}},
  shorttitle = {{{AI4OPT}}},
  author = {Van Hentenryck, Pascal and Dalmeijer, Kevin},
  year = {2024},
  month = mar,
  journal = {AI Magazine},
  volume = {45},
  number = {1},
  pages = {42--47},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {0738-4602},
  doi = {10.1002/aaai.12146},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/aaai.12146},
  urldate = {2024-04-22},
  abstract = {Abstract This article is a short introduction to AI4OPT, the NSF AI Institute for Advances in Optimization. AI4OPT fuses AI and optimization, inspired by societal challenges in supply chains, energy systems, chip design and manufacturing, and sustainable food systems. By combining machine learning and mathematical optimization, AI4OPT strives to develop AI-assisted optimization systems that bring orders of magnitude improvements in efficiency, perform accurate uncertainty quantification, and address challenges in resiliency and sustainability. AI4OPT also applies its ?teaching the teachers? philosophy to provide longitudinal educational pathways in AI for engineering.},
  keywords = {obsLitNote},
  note = {VanHentenryck24AI4OPTAIInstitute
\par
Official statement about what AI4OPT does.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\VanHentenryck24AI4OPTAIInstitute.pdf}
}

@inproceedings{Chatzivasileiadis22powSysMLseminar,
  title = {Machine {{Learning}} for {{Power Systems}}:  {{Is}} It Time to Trust It?},
  booktitle = {{{AI4OPT Seminar Series}},  {{Fall}} 2022},
  author = {Chatzivasileiadis, Spyros},
  year = {2022},
  month = oct,
  address = {Georgia Tech},
  keywords = {obsLitNote},
  note = {Chatzivasileiadis22powSysMLseminar
\par
DTU ML mixed with optimization for power systems, presented to AI4OPT.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chatzivasileiadis22powSysMLseminar.pdf}
}

@misc{Anderton-Yang24AIReEducationHere,
  title = {"{{AI Re-Education}} Is {{Here}}"},
  author = {{Anderton-Yang}, David},
  year = {2024},
  month = apr,
  publisher = {YouTube},
  url = {https://www.youtube.com/watch?v=qDsH10bKtGg},
  urldate = {2024-04-23},
  keywords = {todo},
  note = {Anderton-Yang24AIReEducationHere
\par
\begin{itemize}

\item \href{https://youtu.be/qDsH10bKtGg?t=261}{700K people} have finished some form of AI training: online learning
\item Cisco survey\href{https://youtu.be/qDsH10bKtGg?t=272}{ 97\% say urgency to deploy} AI has increased; but 50\% say employee has no idea how to do it
\item Executive education courses and at universities
\item the new computer literacy: word processors, Excel, etc.
\item Harvard has free courses online for AI re-education

\end{itemize}}
}

@misc{Bhargava24controlTheoryLLM,
  title = {{{LLM Control Theory}}: {{What}}'s the Magic Word?  {{A Control Theory}} of {{LLM Prompting}}},
  author = {Bhargava, Aman and Looi, Shi-Zhuo},
  year = {2024},
  month = apr,
  address = {Caltech},
  url = {https://www.youtube.com/watch?v=9QtS9sVBFM0},
  urldate = {2024-04-23},
  abstract = {Stay tuned for our new results in our preprint, "What's the Magic Word? A Control Theory of LLM Prompting": https://arxiv.org/abs/2310.04444 Thank you to the Information, Geometry, and Physics Seminar run by Juan Pablo Vigneaux for the opportunity to present our work. For a full reference list, see our preprint. 0:00 Introduction 1:51 1: MOTIVATIONS 1:55 Zero-shot learning miracle (Motivations) 3:28 Background on LLMs? (Motivations)   4:22 Transformer Information Flow for Generative Inference (Motivations)  6:03 LLMs are increasingly used as systems (Motivations) 6:42 We do not understand LLMs as systems (Motivations)  9:23 Control theory is great for understanding systems (Motivations)  11:36 Prompt engineering = system control problem (Motivations)  14:12 LLM systems get complicated fast (Motivations)  15:16 Plan for LLM Control Theory (Motivations)  16:58 2: FRAMEWORK 17:01 LLM systems are unusual (Framework)  18:44 LLM system formalization (Framework) 21:43 Reachability (Framework) 23:43 Reachable Sets (Framework)  25:11 k-epsilon Controllability (Framework)  27:29 3: SELF-ATTENTION CONTROLLABILITY THEOREM 27:40 Background on self attention (Theorem) 31:23 Controllability for self-attention (Theorem)  32:20 Theorem (Theorem) 40:23 Connecting self-attention theorem with empirical results (Theorem)  41:13 EXPERIMENTAL RESULTS ON LLM CONTROLLABILITY 41:24 Measuring k-epsilon controllability via prompt optimization (Experiments) 42:58 Experimental design for k-epsilon controllability measurement (Experiments) 45:15 Results on k-epsilon controllability (Experiments) 48:36 OPEN QUESTIONS IN LLM CONTROL THEORY 49:57 Typical sequences, AEP  50:58 More next steps  52:21 Acknowledgements},
  keywords = {obsLitNote},
  note = {Bhargava24controlTheoryLLM
\par
Prompts analyzed a inputs to a control system trying to get a certain results. ~Also \textbf{good slides} for a breezy intro to LLMs
\par
\textbf{TODO}: watch, get screenshots of good ideas. ~Also learn something!
\par
\section{Seminar announcment}

\par
\textbf{Toward a Control Theory of LLMs}
\par
\href{https://aman-bhargava.com/}{Aman Bhargava}, Computation and Neural Systems, Caltech and~\href{https://scholar.google.com/citations?user=gzG4qoIAAAAJ&hl=en}{Shi-Zhuo Looi}, Department of Mathematics, Caltech
\par
Prompt engineering is crucial for deploying LLMs but is poorly understood mathematically. We formalize LLM systems as a class of discrete stochastic dynamical systems to explore prompt engineering through the lens of control theory. We investigate the reachable set of output token sequences \${\textbackslash}mathcal R\_y({\textbackslash}mathbf x\_0)\$ for which there exists a control input sequence \${\textbackslash}mathbf u\$ for each \${\textbackslash}mathbf y {\textbackslash}in {\textbackslash}mathcal R\_y({\textbackslash}mathbf x\_0)\$ that steers the LLM to output \${\textbackslash}mathbf y\$ from initial state sequence \${\textbackslash}mathbf x\_0\$. We offer analytic analysis on the limitations on the controllability of self-attention in terms of reachable set, where we prove an upper bound on the reachable set of outputs \${\textbackslash}mathcal R\_y({\textbackslash}mathbf x\_0)\$ as a function of the singular values of the parameter matrices. We present complementary empirical analysis on the controllability of a panel of LLMs, including Falcon-7b, Llama-7b, and Falcon-40b. Our results demonstrate a lower bound on the reachable set of outputs \${\textbackslash}mathcal R\_y({\textbackslash}mathbf x\_0)\$ w.r.t. initial state sequences \${\textbackslash}mathbf x\_0\$ sampled from the Wikitext dataset. We find that the correct next Wikitext token following sequence \${\textbackslash}mathbf x\_0\$ is reachable over 97{\textbackslash}\% of the time with prompts of \$k{\textbackslash}leq 10\$ tokens. We also establish that the top 75 most likely next tokens, as estimated by the LLM itself, are reachable at least 85{\textbackslash}\% of the time with prompts of \$k{\textbackslash}leq 10\$ tokens. Intriguingly, short prompt sequences can dramatically alter the likelihood of specific outputs, even making the least likely tokens become the most likely ones. This control-centric analysis of LLMs demonstrates the significant and poorly understood role of input sequences in steering output probabilities, offering a foundational perspective for enhancing language model system capabilities.
\par
\section{Youtube video Notes}

\par
Stay tuned for our new results in our preprint, "What's the Magic Word? A Control Theory of LLM Prompting": https://arxiv.org/abs/2310.04444
\par
Thank you to the Information, Geometry, and Physics Seminar run by Juan Pablo Vigneaux for the opportunity to present our work. For a full reference list, see our preprint.
\par
0:00 Introduction
\par
1:51 1: MOTIVATIONS
\par
1:55 Zero-shot learning miracle (Motivations)
\par
3:28 Background on LLMs? (Motivations) ~
\par
4:22 Transformer Information Flow for Generative Inference (Motivations) 
\par
6:03 LLMs are increasingly used as systems (Motivations)
\par
6:42 We do not understand LLMs as systems (Motivations) 
\par
9:23 Control theory is great for understanding systems (Motivations) 
\par
11:36 Prompt engineering = system control problem (Motivations) 
\par
14:12 LLM systems get complicated fast (Motivations) 
\par
15:16 Plan for LLM Control Theory (Motivations) 
\par
16:58 2: FRAMEWORK
\par
17:01 LLM systems are unusual (Framework) 
\par
18:44 LLM system formalization (Framework)
\par
21:43 Reachability (Framework)
\par
23:43 Reachable Sets (Framework) 
\par
25:11 k-epsilon Controllability (Framework) 
\par
27:29 3: SELF-ATTENTION CONTROLLABILITY THEOREM
\par
27:40 Background on self attention (Theorem)
\par
31:23 Controllability for self-attention (Theorem) 
\par
32:20 Theorem (Theorem)
\par
40:23 Connecting self-attention theorem with empirical results (Theorem) 
\par
41:13 EXPERIMENTAL RESULTS ON LLM CONTROLLABILITY
\par
41:24 Measuring k-epsilon controllability via prompt optimization (Experiments)
\par
42:58 Experimental design for k-epsilon controllability measurement (Experiments)
\par
45:15 Results on k-epsilon controllability (Experiments)
\par
48:36 OPEN QUESTIONS IN LLM CONTROL THEORY
\par
49:57 Typical sequences, AEP 
\par
50:58 More next steps 
\par
52:21 Acknowledgements}
}

@book{Bishop24deepLearningBk,
  title = {Deep Learning: {{Foundations}} and Concepts},
  author = {Bishop, Christopher M. and Bishop, Hugh},
  year = {2024},
  edition = {1},
  publisher = {Springer},
  address = {Cham},
  doi = {10.1007/978-3-031-45468-4},
  url = {https://www.bishopbook.com/},
  abstract = {This book offers a comprehensive introduction to the central ideas that underpin deep learning. It is intended both for newcomers to machine learning and for those already experienced in the field. Covering key concepts relating to contemporary architectures and techniques, this essential book equips readers with a robust foundation for potential future specialization. The field of deep learning is undergoing rapid evolution, and therefore this book focusses on ideas that are likely to endure the test of time. The book is organized into numerous bite-sized chapters, each exploring a distinct topic, and the narrative follows a linear progression, with each chapter building upon content from its predecessors. This structure is well-suited to teaching a two-semester undergraduate or postgraduate machine learning course, while remaining equally relevant to those engaged in active research or in self-study. A full understanding of machine learning requires some mathematical background and so the book includes a self-contained introduction to probability theory. However, the focus of the book is on conveying a clear understanding of ideas, with emphasis on the real-world practical value of techniques rather than on abstract theory. Complex concepts are therefore presented from multiple complementary perspectives including textual descriptions, diagrams, mathematical formulae, and pseudo-code.},
  isbn = {978-3-031-45467-7},
  keywords = {obsLitNote},
  note = {Bishop24deepLearningBk
\par
\section{Access}

\begin{itemize}

\item Viewable online \href{https://www.bishopbook.com/}{here}
\item All figures are downloadable from \href{https://www.bishopbook.com/Bishop_DL_Figs.zip}{here}.
\item (Pinson et al., 2007)

\end{itemize}}
}

@misc{Ghoshal24dominanceLlama3,
  title = {Meta Eyes {{LLM}} Dominance with New {{Llama}} 3 Models},
  author = {Ghoshal, Anirban},
  year = {2024},
  month = apr,
  journal = {InfoWorld},
  url = {https://www.infoworld.com/article/3715265/meta-eyes-llm-dominance-with-new-llama-3-models.html},
  urldate = {2024-04-23},
  abstract = {The new generation of models is expected to take on all large language models, including GPT-3.5, Claude Sonnet, Mistal models, and Grok.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Ghoshal24dominanceLlama3
\par
On 4/19/24 (date of this article) the best LLM score on tough human expertise GPAQ (Rein et al., 2023) ~was from GeminiPro 1.5: \textbf{41.5}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ghoshal24dominanceLlama3.pdf}
}

@misc{Tyl23chinchillaDeath,
  type = {Blog},
  title = {Chinchilla's {{Death}}},
  author = {Tyl, Thadd{\'e}e},
  year = {2023},
  month = jul,
  journal = {espadrine blog},
  url = {https://espadrine.github.io/blog/posts/chinchilla-s-death.html},
  urldate = {2024-04-23},
  keywords = {todo},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Tyl23chinchillaDeath.pdf}
}

@misc{Serwani24primisAIgenAIhardwareDsgn,
  title = {Introducing {{PrimisAI}}'s - {{A Generative AI Tool}} to {{Accelerate Hardware Design}}},
  author = {Sherwani, Naveed and Bouwmeester, Hans},
  year = {2024},
  month = apr,
  url = {https://www.youtube.com/watch?v=7h8M7thqW94},
  urldate = {2024-04-23},
  abstract = {Watch our Efabless webinar featuring PrimisAI! Explore the future of AI-driven Electronic Design Automation (EDA) with RapidGPT, an innovative tool that's transforming hardware engineering.  Learn how its generative AI and natural language interface can streamline the design process from concept to RTL, and revolutionize traditional methods to enhance productivity and speed up time-to-market.},
  keywords = {todo},
  note = {Serwani24primisAIgenAIhardwareDsgn
\par
Gen AI does hardware (chip, I think) design. ~I didn't watch, but it can go on the list.}
}

@misc{Kaplan20ScalingLawsNeural,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  number = {arXiv:2001.08361},
  eprint = {2001.08361},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.08361},
  url = {http://arxiv.org/abs/2001.08361},
  urldate = {2024-04-23},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Kaplan20neuralLMscaleLaws
\par
Seems to say that once an LLM gets big enough, it becomes more data efficient, which is counterintuitive, like Belkin18modernMachLrnBiasVar (energy.bib)
\par
Good graph: fig 1 shows how L(?)LM error drops w/ compute, dataset size, parameters
\par
Referred to in \href{https://youtu.be/U8J32Z3qV8s?t=47}{this MOE video}, which points out that the X axis on the rightmost plot in Fig 1 means Parameters ``with increasing FLOPS per example,'' whatever that means.
\par
Add to part about Chinchilla laws, dead or not.
\par
Comment: 19 pages, 15 figures
\par
Related to? (Ananthaswamy, 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kaplan20neuralLMscaleLaws.pdf}
}

@article{Bousquette24ModernaOpenAIGPTs,
  title = {At {{Moderna}}, {{OpenAI}}'s {{GPTs Are Changing Almost Everything}}},
  author = {Bousquette, Isabelle},
  year = {2024},
  month = apr,
  journal = {Wall Street Journal},
  issn = {0099-9660},
  url = {https://www.wsj.com/articles/at-moderna-openais-gpts-are-changing-almost-everything-6ff4c4a5},
  urldate = {2024-04-24},
  abstract = {``People literally talk about how AI is going to cure diseases someday, and I think this is a very meaningful first step,'' said OpenAI CEO Sam Altman.},
  chapter = {C Suite},
  langid = {american},
  keywords = {todo},
  note = {Bousquette24ModernaOpenAIGPTs
\par
Moderna has a deep partnership w/ OpenAI, in a big internal push to use omnipresent OpenAI products to roll out 15 new products in 5 years. ~Idea is to ``transform every business process,'' while concrete plans metined are in regulatory processes and scientific development.
\par
\section{Organization}

\begin{itemize}

\item 3000 will get ChatGPT Enterprise
\item CEOs goal is that employees ChatGPT 20 times a day
\item weekly strategy meetings between Moderna and OpenAI

\end{itemize}

\section{Applications}

\begin{itemize}

\item employees have created {$>$} 750 versons of ChatGPT, taylored to specific tasks
\item 
\par
draft answers to regulator's questions after digesting reasearch docs: 
\par
\begin{itemize}

\item weekslong process into minutes 
\item a bureeaucracy wrangler like JPMorgan's rule digester (Ghosh, 2024)

\end{itemize}

\item 
\par
predict optimal drug dose for clinical trials: 
\par
\begin{itemize}

\item consume years of previous research and ``medical knowledge''

\end{itemize}

\item 
\par
predict new enzyme structures wthat will increase yield and reduce waste
\par
\begin{itemize}

\item 
\par
links in article to AI drug discovery doubters -- paywalled, so I couldn't follow
\par
\end{itemize}

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bousquette24ModernaOpenAIGPTs.pdf}
}

@misc{Toloka23optTechniqueLLM,
  title = {{{LLMs Optimization Techniques}}: {{Prompt Tuning}} and {{Prompt Engineering}}},
  shorttitle = {{{LLMs Optimization Techniques}}},
  author = {{Toloka}},
  year = {2023},
  month = nov,
  journal = {LLMs Optimization Techniques: Prompt Tuning and Prompt Engineering},
  url = {https://toloka.ai/blog/llms-optimization-techniques/},
  urldate = {2024-04-24},
  abstract = {In this article, we will delve into the concept of such techniques and how they can empower data scientists to tackle the full potential of AI models for various applications.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Toloka23optTechniqueLLM
\par
Adpation already-trained (I think, I haven't read this yet). ~Explains prompting and soft prompts.
\par
And much, much more!
\par
Note that this 2023 article was updated on March 26, 2024 (bottom)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Toloka23optTechniqueLLM.pdf}
}

@misc{Scarfe24bishopNewDpLrnBk,
  title = {Prof. {{Chris Bishop}}'s {{NEW Deep Learning Textbook}}!},
  author = {Scarfe, Tim},
  year = {2024},
  month = apr,
  url = {https://www.youtube.com/watch?v=kuvFoXzTK3E},
  urldate = {2024-04-25},
  abstract = {Professor Chris Bishop is a Technical Fellow and Director at Microsoft Research AI4Science, in Cambridge. He is also Honorary Professor of Computer Science at the University of Edinburgh, and a Fellow of Darwin College, Cambridge. In 2004, he was elected Fellow of the Royal Academy of Engineering, in 2007 he was elected Fellow of the Royal Society of Edinburgh, and in 2017 he was elected Fellow of the Royal Society. Chris was a founding member of the UK AI Council, and in 2019 he was appointed to the Prime Minister's Council for Science and Technology. At Microsoft Research, Chris oversees a global portfolio of industrial research and development, with a strong focus on machine learning and the natural sciences. Chris obtained a BA in Physics from Oxford, and a PhD in Theoretical Physics from the University of Edinburgh, with a thesis on quantum field theory.  Chris's contributions to the field of machine learning have been truly remarkable. He has authored (what is arguably) the original textbook in the field - 'Pattern Recognition and Machine Learning' (PRML) which has served as an essential reference for countless students and researchers around the world, and that was his second textbook after his highly acclaimed first textbook Neural Networks for Pattern Recognition.  Recently, Chris has co-authored a new book with his son, Hugh, titled 'Deep Learning: Foundations and Concepts.' This book aims to provide a comprehensive understanding of the key ideas and techniques underpinning the rapidly evolving field of deep learning. It covers both the foundational concepts and the latest advances, making it an invaluable resource for newcomers and experienced practitioners alike. Buy Chris' textbook here: https://amzn.to/3vvLcCh More about Prof. Chris Bishop: https://en.wikipedia.org/wiki/Christo... https://www.microsoft.com/en-us/resea... Support MLST: Please support us on Patreon. We are entirely funded from Patreon donations right now. Patreon supports get private discord access, biweekly calls, early-access + exclusive content and lots more.   ~~/~mlst~~ Donate: https://www.paypal.com/donate/?hosted... If you would like to sponsor us, so we can tell your story - reach out on mlstreettalk at gmail TOC: 00:00:00 - Intro to Chris 00:06:54 - Changing Landscape of AI 00:08:16 - Symbolism 00:09:32 - PRML 00:11:02 - Bayesian Approach 00:14:49 - Are NNs One Model or Many, Special vs General 00:20:04 - Can Language Models Be Creative 00:22:35 - Sparks of AGI 00:25:52 - Creativity Gap in LLMs 00:35:40 - New Deep Learning Book 00:39:01 - Favourite Chapters 00:44:11 - Probability Theory 00:45:42 - AI4Science 00:48:31 - Inductive Priors 00:58:52 - Drug Discovery 01:05:19 - Foundational Bias Models 01:07:46 - How Fundamental Is Our Physics Knowledge? 01:12:05 - Transformers 01:12:59 - Why Does Deep Learning Work? 01:16:59 - Inscrutability of NNs 01:18:01 - Example of Simulator 01:21:09 - Control},
  keywords = {obsLitNote}
}

@misc{Gade24LeveragingP90Requirement,
  title = {Leveraging {{P90 Requirement}}: {{Flexible Resources Bidding}} in {{Nordic Ancillary Service Markets}}},
  shorttitle = {Leveraging {{P90 Requirement}}},
  author = {Gade, Peter A. V. and Bindner, Henrik W. and Kazempour, Jalal},
  year = {2024},
  month = apr,
  number = {arXiv:2404.12807},
  eprint = {2404.12807},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.12807},
  url = {http://arxiv.org/abs/2404.12807},
  urldate = {2024-04-25},
  abstract = {The P90 requirement of the Danish transmission system operator, Energinet, incentivizes flexible resources with stochastic power consumption/production baseline to bid in Nordic ancillary service markets with the minimum reliability of 90\%, i.e., letting them cause reserve shortfall with the probability of up to 10\%. Leveraging this requirement, we develop a distributionally robust joint chance-constrained optimization model for aggregators of flexible resources to optimize their volume of reserve capacity to be offered. Having an aggregator of electric vehicles as a case study, we show how distributional robustness is key for the aggregator when making bidding decisions in a non-stationary uncertain environment. We also develop a heuristic based on a grid search for the system operator to adjust the P90 requirement and the level of conservativeness, aiming to procure the maximum reserve capacity from stochastic resources with least expected shortfall.},
  archiveprefix = {arXiv},
  keywords = {obsLitNote},
  note = {Gade24LeveragingP90Requirement
\par
TODO: compare with AI4OPT ideas
\par
Has code \href{https://github.com/PeterAVG/p90_drjcc}{here}
\par
Comment: Submitted to SmartGridComm 2024},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gade24LeveragingP90Requirement.pdf}
}

@misc{Sawers24WhyVectorDatabases,
  title = {Why Vector Databases Are Having a Moment as the {{AI}} Hype Cycle Peaks},
  author = {Sawers, Paul},
  year = {2024},
  month = apr,
  journal = {TechCrunch},
  url = {https://techcrunch.com/2024/04/20/why-vector-databases-are-having-a-moment-as-the-ai-hype-cycle-peaks/},
  urldate = {2024-04-26},
  abstract = {The proliferation of large language models and generative AI has created fertile ground for vector database technologies to flourish.},
  langid = {american},
  keywords = {todo},
  note = {Sawers24WhyVectorDatabases
\par
I saved this mainly for this quote, which belongs in on a slide somewhere:
\par
\begin{quotation}

\par
Vector search can also help reduce ``hallucinations'' in LLM applications, through providing additional information that might not have been available in the original training dataset.
\par
\end{quotation}

\par
I imagine this could mean the multi-search presented by the \href{https://onedrive.live.com/view.aspx?resid=4BBD96B3698748F8\%21737227&id=documents&wd=target\%28LLMs.one\%7CD5EB236C-D9E9-42D2-898B-A0BE0DB032C2\%2FSeminar\%3A\%20LLMs\%20Beyond\%20the\%20Lab\%3A\%20Productionizing\%20LLM-Powered\%20Applications\%7C74F76F26-F830-41C5-844F-20751EB33181\%2F\%29 onenote:https://d.docs.live.net/4bbd96b3698748f8/Documents/Geli/LLMs.one\#Seminar\%20LLMs\%20Beyond\%20the\%20Lab\%20Productionizing\%20LLM-Powered\%20Applications&section-id=D5EB236C-D9E9-42D2-898B-A0BE0DB032C2&page-id=74F76F26-F830-41C5-844F-20751EB33181&end}{MongoDB guy at the LLMs Beyond the Lab: Productionizing LLM-Powered Applications} seminar I went to in Seattle on 4/25/24 (OneNote notes in the link)
\par
It mentions something else the MongoDB guy said about JSON based ``document databases,'' which becme a standard at some time. ~Says that vector DB's will becom a standard like that, too. ~And indeed, the MongoDB guy showed a JSON document chunk with an LLM-style vector in it, in fact, it could hold multiple vectors from different embedding types.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sawers24WhyVectorDatabases.pdf}
}

@misc{Eric24softmaxAInotes,
  title = {Softmax},
  journal = {Eric's AI Notes},
  url = {https://ericwiener.github.io/ai-notes/AI-Notes/Activation/Softmax},
  urldate = {2024-04-27},
  abstract = {Softmax is often used to normalize the logits from a model (outputs from the last layer) so that they all lie between [0,1] and sum to 1. A visualization can be seen below: Source When to use softmax vs.},
  keywords = {todo},
  note = {Eric24softmaxAInotes
\par
Example of a good softmax/temp graph. ~I could make one with pytorch.},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Alammar18illustratedTransformer.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Eric24softmaxAInotes.html}
}

@misc{Gou23tempLLMperformance,
  title = {The {{Impact}} of {{Temperature}} on the {{Performance}} of {{Large Language Model Systems}} and {{Business Applications}}},
  author = {Gou, Michael},
  year = {2023},
  month = sep,
  journal = {TICKR},
  url = {https://tickr.com},
  urldate = {2024-04-27},
  abstract = {In today's data-driven world, businesses are increasingly turning to advanced technologies to gain a competitive edge. Large language models ("LLMs") have emerged as a game-changer, enabling businesses to have intelligent conversations with data and extract valuable insights. In this study we explore the effects of LLM temperature, a concept borrowed from statistical physics and thermodynamics, on the impact of LLM business applications.},
  langid = {english},
  keywords = {tags},
  note = {Gou23tempLLMperformance
\par
Graph of accuracy vs. temp, also example responses at different temperatures.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gou23tempLLMperformance.html}
}

@misc{Duwal24llmTempTopKtopP,
  title = {Mastering {{LLM Parameters}}: {{A Deep Dive}} into {{Temperature}}, {{Top-K}}, and {{Top-P}}},
  shorttitle = {Mastering {{LLM Parameters}}},
  author = {Duwal, Amit},
  year = {2024},
  month = feb,
  journal = {Medium},
  url = {https://aws.plainenglish.io/mastering-llm-parameters-a-deep-dive-into-temperature-top-k-and-top-p-623b6aa2e6e5},
  urldate = {2024-04-27},
  abstract = {As someone who has been quite fond of LLMs and has been developing LLM applications, one thing I am always playing with is the LLM{\dots}},
  langid = {english},
  keywords = {todo},
  note = {Duwal24llmTempTopKtopP
\par
Explains 3 LLM tuning params found in e.g. Claude. ~ID's which one is called ``creativity,'' I think.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Duwal24llmTempTopKtopP.pdf}
}

@article{Paulin21EventsEarlyNervous,
  title = {Events in {{Early Nervous System Evolution}}},
  author = {Paulin, Michael G. and {Cahill-Lane}, Joseph},
  year = {2021},
  month = jan,
  journal = {Topics in Cognitive Science},
  volume = {13},
  number = {1},
  pages = {25--44},
  issn = {1756-8765},
  doi = {10.1111/tops.12461},
  abstract = {We propose that neurons and nervous systems evolved among thin, motile, microbe-eating animals during the Ediacaran period (635-543~million years ago). Spiking neurons evolved from epithelial cells around the margins of Ediacaran microbial mat grazers that initially specialized to detect weak bioelectric fields of nearby animals and to trigger rapid withdrawal movements. According to this scenario, nervous systems are a consequence of two preceding animal innovations, external digestion and motility, which have co-evolved in concert with nervous systems ever since. We suggest that fundamental characteristics of modern nervous systems can be explained by studying how nervous systems originated during the Ediacaran period, as natural computers for predictive statistical inference given event-based sense data.},
  langid = {english},
  pmid = {31564066},
  keywords = {obsLitNote},
  note = {Paulin21EventsEarlyNervous
\par
From the abstract: We propose that neurons and nervous systems evolved among thin, motile, microbe-eating animals during the Ediacaran period (\textbf{635-543 million years ago)}.
\par
Compare to first artificial NN for talks.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Paulin21EventsEarlyNervous.pdf}
}

@misc{Hardesty17explainedNNs,
  title = {Explained: {{Neural}} Networks},
  shorttitle = {Explained},
  author = {Hardesty, Larry},
  year = {2017},
  month = apr,
  journal = {MIT News: Massachusetts Institute of Technology},
  url = {https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414},
  urldate = {2024-04-27},
  abstract = {``Deep learning,'' the machine-learning technique behind the best-performing artificial-intelligence systems of the past decade, is really a revival of the 70-year-old concept of neural networks.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Hardesty17explainedNNs
\par
First ANN proposed in 1944 and has gone in and out of fashion a few times since. ~But doesn't note when the 1st NN that passed the Minsky test was, or 1st paper that knew about backprop.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hardesty17explainedNNs.pdf}
}

@misc{Kurenkov20briefHistoryNN,
  title = {A {{Brief History}} of {{Neural Nets}} and {{Deep Learning}}},
  author = {Kurenkov, Andrey},
  year = {2020},
  month = sep,
  journal = {Skynet Today},
  url = {https://www.skynettoday.com/overviews/neural-net-history},
  urldate = {2024-04-27},
  abstract = {The story of how neural nets evolved from the earliest days of AI to now.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Kurenkov20briefHistoryNN
\par
This web page is cited by many uinversity class notes (*), and also shows up in a couple chatGPT-like things, so it seems authoritative.
\par
Says \textbf{first fully modern NN like in books today} was (Rumelhart et al., 1986). ~It was first proposed in 1944 (Hardesty, 2017), but the Rumelhart paper was concise, and its explanations are what we see in books and classes to day. ~Was also followed on by more papers that addressed Minsky's NN objections, and further explained representation learning.
\par
(*) citing examples:
\par
\begin{itemize}

\item (Fradkov, 2020)
\item Stanford : CS 486/686 Lecture 21 A brief history of deep learning
\item U. of Waterloo: seem to have copied Stanford's notes
\item DePaul U ~CSC 578
\item UW CSE 490H

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kurenkov20briefHistoryNN.pdf}
}

@article{Rumelhart86learnRepBackprop,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  url = {https://www.nature.com/articles/323533a0},
  urldate = {2024-04-27},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  copyright = {1986 Springer Nature Limited},
  langid = {english},
  keywords = {obsLitNote},
  note = {Rumelhart86learnRepBackprop
\par
Said to be the paper that ``started'' the NN concepts still used today. ~(Kurenkov, 2020)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Rumelhart86LearningRepresentationsBackpropagating.pdf}
}

@article{Hornik89mlpUnivApprox,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {DOI: 10.1016/0893-6080(89)90020-8},
  url = {http://www.sciencedirect.com/science/article/B6T08-485RHTR-5R/2/5d13881c0b9d8128fde7950a6f55849d},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  owner = {scot},
  keywords = {todo},
  note = {Hornik89mlpUnivApprox
\par
Neural networks can approximate almost any function.
\par
\textbf{Note} that I copied this over from energy.bib w/o deleting it there. ~Watch out for dups!},
  timestamp = {2011.05.10},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hornik89mlpUnivApprox.pdf}
}

@misc{Liu24physSysLrnByThemslvs,
  title = {Physical Systems That Can Learn by Themselves},
  author = {Liu, Andrea},
  year = {2024},
  month = apr,
  address = {UC Berkeley},
  url = {https://www.youtube.com/watch?v=7hz4cs-hGew},
  urldate = {2024-04-27},
  abstract = {Brains learn and perform an enormous variety of tasks on their own, using relatively little energy. Brains are able to accomplish this without an external computer because their analog constituent parts (neurons) update their connections without knowing what all the other neurons are doing using local rules. We have developed an approach to learning that shares the property that analog constituent parts update their properties via a local rule, but does not otherwise emulate the brain. Instead, we exploit physics to learn in a far simpler way. Our collaborators have implemented this approach in the lab, developing physical systems that learn and perform machine learning tasks on their own with little energy cost. These systems should open up the opportunity to study how many more is different within a new paradigm for scalable learning.},
  keywords = {obsLitNote},
  note = {Liu24physSysLrnByThemslvs
\par
Intersting lecture on self organizing soft matter, but I saved this because the energy cost comparison w/ chatGPT and human brains.}
}

@article{Fradkov20earlyHistoryML,
  title = {Early {{History}} of {{Machine Learning}}},
  author = {Fradkov, Alexander L.},
  year = {2020},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {21st {{IFAC World Congress}}},
  volume = {53},
  number = {2},
  pages = {1385--1390},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2020.12.1888},
  url = {https://www.sciencedirect.com/science/article/pii/S2405896320325027},
  urldate = {2024-04-27},
  abstract = {Machine learning belongs to the crossroad of cybernetics (control science) and computer science. It is attracting recently an overwhelming interest, both of professionals and of the general public. In the talk a brief overview of the historical development of the machine learning field with a focus on the development of mathematical apparatus in its first decades is provided. A number of little-known facts published in hard to reach sources are presented.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Fradkov20earlyHistoryML.pdf}
}

@incollection{Rumelhart88LearningInternalRepresentations,
  title = {Learning {{Internal Representations}} by {{Error Propagation}}},
  booktitle = {Readings in {{Cognitive Science}}},
  author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
  editor = {Collins, Allan and Smith, Edward E.},
  year = {1988},
  month = jan,
  pages = {399--421},
  publisher = {Morgan Kaufmann},
  doi = {10.1016/B978-1-4832-1446-7.50035-2},
  url = {https://www.sciencedirect.com/science/article/pii/B9781483214467500352},
  urldate = {2024-04-27},
  isbn = {978-1-4832-1446-7},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Rumelhart88LearningInternalRepresentations.pdf}
}

@misc{Rabbitmetrics23LangChainIn13mins,
  title = {{{LangChain Explained}} in 13 {{Minutes}} {\textbar} {{QuickStart Tutorial}} for {{Beginners}}},
  author = {{Rabbitmetrics}},
  year = {2023},
  month = apr,
  publisher = {YouTube},
  url = {https://www.youtube.com/watch?v=aywZrzNaKjs},
  urldate = {2024-04-28},
  abstract = {In this video, we're going to explore the core concepts of LangChain and understand how the framework can be used to build your own large language model applications. Code for the video is available here: https://github.com/rabbitmetrics/lang...  V I D E O  C H A P T E R S  \&  T I M E S T A M P S   0:00 Introduction and overview 0:38 Why Langchain? 3:40 The value proposition of Langchain 4:50 Unpacking Langchain 5:42 LLM Wrappers 6:58 Prompts and Prompt Templates 7:45 Chains 9:00 Embeddings and VectorStores 11:40 An example of a Langchain Agent},
  keywords = {hasCode,todo},
  note = {\href{https://github.com/rabbitmetrics/langchain-agents-explained/tree/main}{Jupyter notebook demo} of langchain: chat, chained language models (not clear why), and agents. ~\href{https://youtu.be/aywZrzNaKjs?t=700}{Agent example} used OpenAI chatGPT code interpreter (python), which apparently generated python based on natural language query, wrote the python, ran it, and then gave back the answer. ~
\par
What's the difference between an agent like this and function caller, as in (Aster, 2024). ~Is the agent just doing function calls inside of the agent?}
}

@misc{Ufberg24headedForAIwinter,
  title = {Why We May Be Headed for a Generative {{AI}} Winter},
  author = {Ufberg, Max},
  year = {2024},
  month = apr,
  journal = {Fast Company},
  url = {https://www.fastcompany.com/91112588/we-may-be-headed-for-a-generative-ai-winter},
  urldate = {2024-04-29},
  abstract = {As the generative AI buzz fades, its positive effects seem spotty. Meanwhile, execs may wonder if all these tools are really helping.},
  langid = {english},
  keywords = {todo},
  note = {Ufberg24headedForAIwinter
\par
Mainly for the headline for the genAI talk. ~If use as ~slide, go original web page and slide the text up and down, to get the FastCompany logo next to the title.
\par
Anyway, it notes that people haven't seen huge productivity gains, except for ~few examples, and there is worry about funding drying up, I think.
\par
Says, Perplexity is kind of hot, though -- I subscribed to it just yesterday, and I like it. ~The startup is in downtown SF, a Hanwha neighbor.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ufberg24headedForAIwinter.pdf}
}

@misc{Murgatroyd24teacherAIvoiceImpers,
  title = {Teacher Arrested after Using {{AI}} to Impersonate Voice of Principal Making Racist Comments},
  author = {Murgatroyd, Laura},
  year = {2024},
  month = apr,
  journal = {The Telegraph},
  url = {https://www.telegraph.co.uk/us/news/2024/04/26/teacher-arrested-ai-recording-principal-racist-comments/},
  urldate = {2024-04-29},
  abstract = {Dazhon Darien accused of falsifying voice of Eric Eiswert to make offensive slurs about students and staff of Maryland school},
  langid = {english},
  keywords = {todo},
  note = {Murgatroyd24teacherAIvoiceImpers
\par
Use for talk? ~Interleave with voice synthesis part?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Murgatroyd24teacherAIvoiceImpers.pdf}
}

@misc{Jin24IstartupShwrCashNoBiz,
  title = {Investors {{Are Showering AI Startups With Cash}}. {{One Problem}}: {{They Don}}'t {{Have Much}} of a {{Business}}},
  shorttitle = {Investors {{Are Showering AI Startups With Cash}}. {{One Problem}}},
  author = {Jin, Berber},
  year = {2024},
  month = apr,
  journal = {WSJ},
  url = {https://www.wsj.com/tech/ai/investors-are-showering-ai-startups-with-cash-one-problem-they-dont-have-much-of-a-business-94534fc9},
  urldate = {2024-04-29},
  abstract = {Some startups are raising hundreds of millions of dollars before they even have a product or any revenue.},
  chapter = {Business},
  langid = {american},
  keywords = {obsLitNote},
  note = {Jin24IstartupShwrCashNoBiz
\par
For the headline{\dots}
\par
A point of caution, don't invest in startups?}
}

@article{Jedrzejewski22ElectricityPriceForecasting,
  title = {Electricity {{Price Forecasting}}: {{The Dawn}} of {{Machine Learning}}},
  shorttitle = {Electricity {{Price Forecasting}}},
  author = {J{\k e}drzejewski, Arkadiusz and Lago, Jesus and Marcjasz, Grzegorz and Weron, Rafa{\l}},
  year = {2022},
  month = may,
  journal = {IEEE Power and Energy Magazine},
  volume = {20},
  number = {3},
  pages = {24--31},
  issn = {1558-4216},
  doi = {10.1109/MPE.2022.3150809},
  url = {https://ieeexplore.ieee.org/document/9761111},
  urldate = {2024-04-30},
  abstract = {Electricity Price Forecasting (EPF) IS A branch of forecasting at the interface of electrical engineering, statistics, computer science, and finance that focuses on predicting prices in wholesale electricity markets for a whole spectrum of horizons. These range from a few minutes (real-time/intraday auctions and continuous trading); through days (day-ahead [DA] auctions); to weeks, months, or even years (exchange and over-the-counter traded futures and forward contracts). DA markets are the workhorse of power trading, particularly in Europe, and a commonly used proxy for ``the electricity price.'' The vast majority---up to 90\%---of the EPF literature has focused on predicting DA prices. These are typically determined around noon during 24 uniform-price auctions, one for each hour of the next day. This has direct implications for the way EPF models are built.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Jedrzejewski22ElectricityPriceForecasting.pdf}
}

@misc{24WhyAutoThefts,
  title = {Why Auto Thefts, Still High in {{WA}}, Aren't `Just a Property Crime'},
  year = {2024},
  month = may,
  journal = {The Seattle Times},
  url = {https://www.seattletimes.com/seattle-news/law-justice/why-auto-thefts-still-high-in-wa-arent-just-a-property-crime/},
  urldate = {2024-05-01},
  abstract = {The surge of stolen vehicles is attributed to a mix of pandemic-era social disruptions, viral social-media challenges targeting Hyundais and Kias, and legislation restricting police pursuits.},
  langid = {american}
}

@article{Tanzi24AIreduceStaffTX,
  title = {{{AI}} to {{Reduce}}, {{Alter Staffing}} at 1 of 4 {{Texas Firms}}, {{Survey Shows}}},
  author = {Tanzi, Alexandre},
  year = {2024},
  month = apr,
  journal = {Bloomberg.com},
  url = {https://www.bloomberg.com/news/articles/2024-04-30/ai-to-reduce-alter-staffing-at-1-of-4-texas-firms-survey-shows},
  urldate = {2024-05-01},
  abstract = {About one in four Texas businesses say artificial intelligence will either decrease staffing requirements or alter the composition of their workforce.},
  langid = {english},
  note = {Tanzi24AIreduceStaffTX
\par
Good headline, graph, for gen AI talk.}
}

@misc{Pauwels24aiInActionBizOptROI,
  title = {{{AI}} in {{Action Exploring Tools}} \& {{Technology}} for {{Business Optimization}} and {{ROI}}},
  author = {Pauwels, Koen},
  year = {2024},
  month = apr,
  publisher = {YouTube},
  url = {https://www.youtube.com/watch?v=JnXevyoeUD8},
  urldate = {2024-05-01},
  keywords = {obsLitNote}
}

@misc{Belanger24airCanadaChatbot,
  title = {Air {{Canada}} Must Honor Refund Policy Invented by Airline's Chatbot},
  author = {Belanger, Ashley},
  year = {2024},
  month = feb,
  journal = {Ars Technica},
  url = {https://arstechnica.com/tech-policy/2024/02/air-canada-must-honor-refund-policy-invented-by-airlines-chatbot/},
  urldate = {2024-05-01},
  abstract = {Air Canada appears to have quietly killed its costly chatbot support.},
  langid = {american},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Belanger24airCanadaChatbot.pdf}
}

@misc{Lucek24test5LLMagents,
  title = {Breaking {{Down}} \& {{Testing FIVE LLM Agent Architectures}} - ({{Reflexion}}, {{LATs}}, {{P}}\&{{E}}, {{ReWOO}}, {{LLMCompiler}})},
  author = {Lucek, Adam},
  year = {2024},
  month = apr,
  publisher = {YouTube},
  url = {https://www.youtube.com/watch?v=ZJlfF1ESXVw},
  urldate = {2024-05-02},
  abstract = {Large Language Model Agents have taken over LLM and Artificial Intelligence application design by storm, so this time we check out and simplify six main concepts and five popular papers documenting ways to set up language model based agents, as well as directly testing examples.  Resources -  @LangChain  Agent Tutorials \& Code - ~~~{$\bullet~$}LangGraph~(Python)~~ Overview on LLM Agents - ~~~{$\bullet~$}AI~Agents!~Giving~Reasoning~and~Tools...~~ Papers: Reflexion Paper: https://arxiv.org/abs/2303.11366 LATs Paper: https://arxiv.org/abs/2310.04406 Plan-And-Execute Paper: https://arxiv.org/abs/2305.04091 ReWOO Paper: https://arxiv.org/abs/2305.18323 LLMCompiler Paper: https://arxiv.org/abs/2312.04511 LangSmith Traces: Basic Reflection: https://smith.langchain.com/public/60... Reflexion: https://smith.langchain.com/public/83... LATs: https://smith.langchain.com/public/d4... Plan-And-Execute: https://smith.langchain.com/public/c1... ReWOO: https://smith.langchain.com/public/ee... LLMCompiler: https://smith.langchain.com/public/28... Chapters: 00:00 - Intro 01:08 - Basic Reflection 02:44 - Basic Reflection Testing 06:32 - Reflexion Actor 09:57 - Reflexion Action Testing 12:25 - Language Agent Tree Search (LATs) 17:04 - LATs Testing 20:54 - Plan And Execute 23:38 - Plan And Execute Testing 26:28 - Reasoning Without Observation (ReWOO) 29:26 - ReWOO Testing 31:11 - LLMCompiler 35:19 - LLMCompiler Testing 36:05 - Outro},
  keywords = {obsLitNote},
  note = {Lucek24test5LLMagents
\par
Runs through langchain implmentations of 5 LLM agent architectures that are found in fairly recent papers. ~At this level, these seem really simple -- hacky heuristics that will be replaced with something for formal.
\par
But it's probably worth checking the last and best agent structure, to see if some of that has already happened.
\par
This is apparently open source, and I think runs on chatGPT. ~The details are in the youtube comments.}
}

@misc{Kim24LLMCompilerParallel,
  title = {An {{LLM Compiler}} for {{Parallel Function Calling}}},
  author = {Kim, Sehoon and Moon, Suhong and Tabrizi, Ryan and Lee, Nicholas and Mahoney, Michael W. and Keutzer, Kurt and Gholami, Amir},
  year = {2024},
  month = feb,
  number = {arXiv:2312.04511},
  eprint = {2312.04511},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.04511},
  url = {http://arxiv.org/abs/2312.04511},
  urldate = {2024-05-02},
  abstract = {Recent language models have shown remarkable results on various complex reasoning benchmarks. The reasoning capabilities of LLMs enable them to execute external function calls to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has allowed LLMs to select and coordinate multiple functions based on the context to tackle more complex problems. However, current methods for multiple function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler, which executes functions in parallel to efficiently orchestrate multiple function calling. Drawing from the principles of classical compilers, LLMCompiler streamlines parallel function calling with three components: (i) an LLM Planner, formulating execution plans; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor, executing these tasks in parallel. LLMCompiler automatically generates an optimized orchestration for the function calls and can be used with both open-source and closed-source models. We have benchmarked LLMCompiler on a range of tasks with different patterns of function calling. We observe consistent latency speedup of up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to {\textasciitilde}9\% compared to ReAct.},
  archiveprefix = {arXiv},
  note = {Kim24LLMCompilerParallelFunc
\par
The best(?) of the agent models in (``Breaking Down \& Testing FIVE LLM Agent Architectures - (Reflexion, LATs, P\&E, ReWOO, LLMCompiler)'', 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kim24LLMCompilerParallelFunc.pdf}
}

@misc{AIExplained24NewOpenAIModel,
  title = {New {{OpenAI Model}} '{{Imminent}}' and {{AI Stakes Get Raised}} (plus {{Med Gemini}}, {{GPT}} 2 {{Chatbot}} and {{Scale AI}})},
  author = {{AI Explained}},
  year = {2024},
  month = may,
  url = {https://www.youtube.com/watch?v=77IqNP6rNL8},
  urldate = {2024-05-02},
  abstract = {Altman `knows the release date', Politico calls it `imminent' according to Insiders, and then the mystery GPT-2 chatbot causes mass confusion and hysteria. I break it all down and cover two papers -- MedGemini and Scale AI Contamination -- released in the last 24 hours. I've read them in full and they might be more important than all the rest. Let's hope life wins over death in the deployment of AI.  AI Insiders: ~~/~aiexplained~~ Politico Article: https://www.politico.eu/article/rishi... Sam Altman Talk: ~~~{$\bullet~$}The~Possibilities~of~AI~[Entire~Talk]...~~ MIT Interview: https://www.technologyreview.com/2024... Logan Kilpatrick Tweet: ~~/~1785834464804794820~~ Bubeck Response: ~~/~1785888787484291440~~ GPT2: ~~/~1785107943664566556~~ Where it used to be hosted: https://arena.lmsys.org/ Unicorns?; ~~/~1784969111430103494~~ No Unicorns: ~~/~1785159370512421201~~ GPT2 chatbot logic fail: ~~/~1785367736157175859~~ And language fails: ~~/~1785101624475537813~~ James Betker Blog: https://nonint.com/2023/06/10/the-it-... Scale AI Benchmark Paper: https://arxiv.org/pdf/2405.00332 Dwarkesh Zuckerberg Interview: ~~~{$\bullet~$}Mark~Zuckerberg~-~Llama~3,~\$10B~Model...~~ Lavander Misuse: https://www.972mag.com/lavender-ai-is... Autonomous Tank: https://www.techspot.com/news/102769-... Claude 3 GPQA: https://www.anthropic.com/news/claude... Med Gemini: https://arxiv.org/pdf/2404.18416 Medical Mistakes: https://www.cnbc.com/2018/02/22/medic... MedPrompt Microsoft: https://www.microsoft.com/en-us/resea... My Benchmark Flaws Tweet: ~~/~1782716249639670000~~ My Stargate Video: ~~~{$\bullet~$}Why~Does~OpenAI~Need~a~'Stargate'~Sup...~~ My GPT-5 Video: ~~~{$\bullet~$}GPT-5:~Everything~You~Need~to~Know~So...~~ Non-hype Newsletter: https://signaltonoise.beehiiv.com/ AI Insiders: ~~/~aiexplained},
  keywords = {obsLitNote}
}

@misc{Sutton19bitterLesson,
  type = {Incomplete {{Ideas}} (Blog)},
  title = {The {{Bitter Lesson}}},
  author = {{Rich Sutton}},
  year = {2019},
  month = mar,
  url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
  urldate = {2024-05-02},
  abstract = {The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin. The ultimate reason for this is Moore's law, or rather its generalization of continued exponentially falling cost per unit of computation. Most AI research has been conducted as if the computation available to the agent were constant (in which case leveraging human knowledge would be one of the only ways to improve performance) but, over a slightly longer time than a typical research project, massively more computation inevitably becomes available. Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation. These two need not run counter to each other, but in practice they tend to. Time spent on one is time not spent on the other. There are psychological commitments to investment in one approach or the other. And the human-knowledge approach tends to complicate methods in ways that make them less suited to taking advantage of general methods leveraging computation.  There were many examples of AI researchers' belated learning of this bitter lesson, and it is instructive to review some of the most prominent.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sutton19BitterLesson.pdf}
}

@misc{Wiggers24hallucRAGwontSolve,
  title = {Why {{RAG}} Won't Solve Generative {{AI}}'s Hallucination Problem},
  author = {Wiggers, Kyle},
  year = {2024},
  month = may,
  journal = {TechCrunch},
  url = {https://techcrunch.com/2024/05/04/why-rag-wont-solve-generative-ais-hallucination-problem/},
  urldate = {2024-05-04},
  abstract = {RAG is being pitched as a solution of sorts to generative AI hallucinations. But there's limits to what the technique can do.},
  langid = {american},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wiggers24hallucRAGwontSolve.pdf}
}

@misc{Caruso23mapBrainConnections,
  title = {A {{New Field}} of {{Neuroscience Aims}} to {{Map Connections}} in the {{Brain}}},
  author = {Caruso, Catharine},
  year = {2023},
  month = jan,
  journal = {Harvard Medical School News \& Research},
  url = {https://hms.harvard.edu/news/new-field-neuroscience-aims-map-connections-brain},
  urldate = {2024-05-04},
  abstract = {Many of us have seen microscopic images of neurons in the brain --- each neuron appearing as a glowing cell in a vast sea of blackness. This image is misleading: Neurons don't exist in isolation. In the human brain, some 86 billion neurons form 100 trillion connections to each other --- numbers that, ironically, are far too large for the human brain to fathom.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Caruso23mapBrainConnections
\par
A source for the human brain having 100 T connections, 86B neurons.
\par
\begin{itemize}

\item Yann Le Cunn likes to say 100T connections in (Wodecki, 2024) and elsewhere.

\end{itemize}

\par
This article is about how researchers are trying to find how every neuron is connected to every other neuron, creating a connectome. ~Then they layer cell type and activity pattern on top. (I wonder if/where stuff like dopamine fit in)
\par
They hope to model higher order connection activity between clusters of neurons.
\par
Idea that ``you are your connectome,'' argument being that when you sleep, your consciousness, thoughts and feelings are interrupted, and brain activity changes dramatically. ~But your connectome stays ``largely intact,'' and you wake up still being yourself.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Caruso23mapBrainConnections.pdf}
}

@misc{Cao24leCunCatSmarterThanAI,
  title = {Meta's {{A}}.{{I}}. {{Chief Yann LeCun Explains Why}} a {{House Cat Is Smarter Than The Best A}}.{{I}}.},
  author = {Cao, Sissi},
  year = {2024},
  month = feb,
  journal = {Observer},
  url = {https://observer.com/2024/02/metas-a-i-chief-yann-lecun-explains-why-a-house-cat-is-smarter-than-the-best-a-i/},
  urldate = {2024-05-04},
  abstract = {``We are missing something conceptually big to get machines to be as intelligent as animals and humans.''},
  langid = {american},
  keywords = {obsLitNote},
  note = {Cao24leCunCatSmarterThanAI
\par
LLMs are not as smart as a cat, even though LLM parameters (``connections'') are comparable to a cat's. ~Far from human intelligence, but could be as smart as a human in 10-20 years. ~Currenlty not better than a search engine (compare to (Wiggers, 2024) re. RAG keyword search).
\par
Add this to the AI vs. humans part of genAI talk. Also see (Wodecki, 2024).
\par
\begin{itemize}

\item 
\par
gen AI vs. humans
\par
\begin{itemize}

\item ``we are really far from human intelligence''
\item can't really invent anythingv
\item not smart enough to give you more useful info than a search engine
\item super-human AI {\textasciitilde} 10 years, maybe 20 (but says every AI researcher has been overly optimistic)

\end{itemize}

\item 
\par
\textbf{house cat} 800M neurons X 2000 for num synapses (connections).
\par
\begin{itemize}

\item GPT4 is {\textasciitilde} num params and cat's connections, but{\dots}
\item 
\par
things cat can do that human can't
\par
\begin{itemize}

\item understands phys world
\item can plan
\item some level of reasoning

\end{itemize}

\end{itemize}

\item \textbf{dog} 2B neurons
\item \textbf{human} 100B neurons
\item 
\par
GPTs
\par
\begin{itemize}

\item 175 B params (GPT-3.5)
\item 220B params (GPT-4)

\end{itemize}

\end{itemize}

\section{LeCun's creds}

\begin{itemize}

\item NYU prof (part-time)
\item Meta chief AI scientist
\item Turning Award Winner (Wodecki, 2024)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Cao24leCunCatSmarterThanAI.pdf}
}

@misc{Nielsen21reflectBitterLesson,
  title = {Reflections on '{{The Bitter Lesson}}'},
  author = {Nielsen, Michael},
  year = {2021},
  month = jul,
  url = {https://cognitivemedium.com/bitter-lesson},
  urldate = {2024-05-05},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nielsen21reflectBitterLesson.pdf}
}

@misc{LeCun24animalSmarterLessDat,
  type = {Blog},
  title = {Animals and Humans Get Very Smart Very Quickly...},
  author = {LeCun, Yann},
  year = {2024},
  month = jan,
  journal = {LinkedIn},
  url = {https://www.linkedin.com/posts/yann-lecun_animals-and-humans-get-very-smart-very-quickly-activity-7133567569684238336-szrF/},
  urldate = {2024-05-05},
  keywords = {obsLitNote},
  note = {LeCun24animalSmarterLessDat
\par
Animals get much smarter with much less data than LLMs. ~Current LLMs trained on data that would take 20,000 years to read but still haven't learned transitive logic: if A=B, then B=A.
\par
Says that both a new more data efficient architecture is needed, and that knowledge can be gained from sensory data: Calculates that 2 year old human has 30X more training data than Current LLM.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\LeCun24animalSmarterLessDat.pdf}
}

@misc{LeCun24child50xMoreDat,
  type = {Blog},
  title = {I've Made That Point Before...},
  author = {LeCun, Yann},
  year = {2024},
  month = feb,
  journal = {LinkedIn},
  url = {https://www.linkedin.com/posts/yann-lecun_ive-made-that-point-before-llm-1e13-activity-7156484065603280896-QH63/},
  urldate = {2024-05-05},
  keywords = {obsLitNote},
  note = {LeCun24child50xMoreDat
\par
4 year old child has 50X more training data than current LLMs all available data would take 170 k years to read -- in (LeCun, 2024), he says that current LLMs trained-on text data that would take 20,000 years to read.
\par
Says video is more redundant than text but that's good for self-supervized learning, and that youtube accumulated 32khours of video per hour.
\par
\begin{itemize}

\item Do his 170k and 20k reading times agree with used and available high quality text in (Villalobos et al., 2022)?
\item Similar to (LeCun, 2024) and (Wodecki, 2024)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\LeCun24child50xMoreDat.pdf}
}

@misc{Patel23WillScalingWork,
  type = {Blog},
  title = {Will Scaling Work?},
  author = {Patel, Dwarkesh},
  year = {2023},
  month = dec,
  journal = {Dwarkesh Podcast},
  url = {https://www.dwarkeshpatel.com/p/will-scaling-work},
  urldate = {2024-05-06},
  abstract = {Data bottlenecks, generalization benchmarks, primate evolution, intelligence as compression, world modelers, and other considerations},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Patel23WillScalingWork.pdf}
}

@misc{Schaeffer23PretrainingTestSet,
  title = {Pretraining on the {{Test Set Is All You Need}}},
  author = {Schaeffer, Rylan},
  year = {2023},
  month = sep,
  number = {arXiv:2309.08632},
  eprint = {2309.08632},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.08632},
  url = {http://arxiv.org/abs/2309.08632},
  urldate = {2024-05-06},
  abstract = {Inspired by recent work demonstrating the promise of smaller Transformer-based language models pretrained on carefully curated data, we supercharge such approaches by investing heavily in curating a novel, high quality, non-synthetic data mixture based solely on evaluation benchmarks. Using our novel dataset mixture consisting of less than 100 thousand tokens, we pretrain a 1 million parameter transformer-based LLM {\textbackslash}textbf\{phi-CTNL\} (pronounced ``fictional") that achieves perfect results across diverse academic benchmarks, strictly outperforming all known foundation models. {\textbackslash}textbf\{phi-CTNL\} also beats power-law scaling and exhibits a never-before-seen grokking-like ability to accurately predict downstream evaluation benchmarks' canaries.},
  archiveprefix = {arXiv},
  note = {Schaeffer23trainOnTestAllNeed
\par
Tiny LLM beats big ones by training on test. ~Point is (I think) that we don't know if that's what OpenAI, etc. is doing too, even if accidentally.
\par
Comment: 3 pages, satire},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Schaeffer23PretrainingTestSet.pdf}
}

@misc{Xue23RepeatNotRepeat,
  title = {To {{Repeat}} or {{Not To Repeat}}: {{Insights}} from {{Scaling LLM}} under {{Token-Crisis}}},
  shorttitle = {To {{Repeat}} or {{Not To Repeat}}},
  author = {Xue, Fuzhao and Fu, Yao and Zhou, Wangchunshu and Zheng, Zangwei and You, Yang},
  year = {2023},
  month = oct,
  number = {arXiv:2305.13230},
  eprint = {2305.13230},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.13230},
  url = {http://arxiv.org/abs/2305.13230},
  urldate = {2024-05-07},
  abstract = {Recent research has highlighted the importance of dataset size in scaling language models. However, large language models (LLMs) are notoriously token-hungry during pre-training, and high-quality text data on the web is approaching its scaling limit for LLMs. To further enhance LLMs, a straightforward approach is to repeat the pre-training data for additional epochs. In this study, we empirically investigate three key aspects under this approach. First, we explore the consequences of repeating pre-training data, revealing that the model is susceptible to overfitting, leading to multi-epoch degradation. Second, we examine the key factors contributing to multi-epoch degradation, finding that significant factors include dataset size, model parameters, and training objectives, while less influential factors consist of dataset quality and model FLOPs. Finally, we explore whether widely used regularization can alleviate multi-epoch degradation. Most regularization techniques do not yield significant improvements, except for dropout, which demonstrates remarkable effectiveness but requires careful tuning when scaling up the model size. Additionally, we discover that leveraging mixture-of-experts (MoE) enables cost-effective and efficient hyper-parameter tuning for computationally intensive dense LLMs with comparable trainable parameters, potentially impacting efficient LLM development on a broader scale.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Xue23RepeatNotRepeat
\par
Dropout regulation lets you repeat training data during training, at least for LLMs. ~Also, mixture of experts help with turning.
\par
Repeating data for multiple epochs of LLM pretraining causes overfitting and degraded performance, but dropout regularization does have ``remarkable effectiveness'' if you're careful.
\par
\begin{itemize}

\item wasn't overfitting a good thing for huge LLMs?
\item I think this was referenced by (Patel, 2023)

\end{itemize}

\par
Comment: Accepted at NeurIPS 2023},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Xue23RepeatNotRepeat.pdf}
}

@misc{Bai22ConstitutionalAIHarmlessness,
  title = {Constitutional {{AI}}: {{Harmlessness}} from {{AI Feedback}}},
  shorttitle = {Constitutional {{AI}}},
  author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and {Tran-Johnson}, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and {Telleen-Lawton}, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and {Hatfield-Dodds}, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
  year = {2022},
  month = dec,
  number = {arXiv:2212.08073},
  eprint = {2212.08073},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.08073},
  url = {http://arxiv.org/abs/2212.08073},
  urldate = {2024-05-07},
  abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
  archiveprefix = {arXiv},
  note = {Bai22ConstitutionalAIHarmless
\par
Tuning out harmful responses without human labels, other than a list of rules to follow -- the labels come from models themselves. In the 1st step, the LLM is fine tuned on some kindof cost function produced by ``self-critques and revisions''. ~In the 2nd phase, a couple realizations (I think) are generated from the fine-tuned LLM, and another model (not described in abstract) says which is better; this is the feedback for reinforcement learning, and is also callsed RLAIF: reinforecement from AI feedback. ~Some kind of chain-of-thought reasoning also helps.
\par
I think this was cited by (Patel, 2023)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bai22ConstitutionalAIHarmless.pdf}
}

@misc{Thewhitebox24llmBackboneAI,
  title = {{LLMs, The Backbones of Frontier AI {\textbar} TheWhiteBox}},
  author = {{TheWhiteBox}},
  year = {2024},
  month = apr,
  journal = {The White Box},
  url = {https://thewhitebox.ai/llms-the-backbones-of-frontier-ai/},
  urldate = {2024-05-07},
  abstract = {LLMs are almost synonymous with state-of-the-art these days, with examples such as ChatGPT. But how are these systems actually built?},
  langid = {spanish},
  keywords = {obsLitNote},
  note = {Big picture view of LLMs their training. ~Recommeded by (Patel, 2023), I think},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Thewhitebox24llmBackboneAI.pdf}
}

@misc{Zewe22ToolPredictingFuture,
  title = {A Tool for Predicting the Future},
  author = {Zewe, Adam},
  year = {2022},
  month = mar,
  journal = {MIT News:  Massachusetts Institute of Technology},
  url = {https://news.mit.edu/2022/tensor-predicting-future-0328},
  urldate = {2024-05-08},
  abstract = {By adapting a powerful algorithm, MIT researchers created a user-friendly tool that enables a nonexpert to make predictions with high accuracy using time-series data with just a few keystrokes and in a matter of seconds.},
  langid = {english},
  keywords = {obsLitNote},
  note = {Zewe22ToolPredictingFuture
\par
\section{Related links from the article:}

\par
\textbf{\href{https://arxiv.org/pdf/2006.13448.pdf}{Paper: "On Multivariate Singular Spectrum Analysis and its Variants"}}
\par
\subsection{\textbf{RELATED LINKS}}

\begin{itemize}

\item \href{https://tspdb.mit.edu/}{tspDB project website}
\item \href{https://devavrat.mit.edu/}{Devavrat Shah}
\item \href{https://abdullaho.me/}{Abdullah Alomar}
\item \href{https://lids.mit.edu/}{Laboratory for Information and Decision Systems}
\item \href{https://idss.mit.edu/}{Institute for Data, Systems, and Society}
\item \href{https://www.eecs.mit.edu/}{Department of Electrical Engineering and Computer Science}
\item \href{https://engineering.mit.edu/}{School of Engineering}
\item \href{https://computing.mit.edu/}{MIT Schwarzman College of Computing}

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zewe22ToolPredictingFuture.pdf}
}

@misc{Agarwal22MultivariateSingularSpectrum,
  title = {On {{Multivariate Singular Spectrum Analysis}} and Its {{Variants}}},
  author = {Agarwal, Anish and Alomar, Abdullah and Shah, Devavrat},
  year = {2022},
  month = jun,
  number = {arXiv:2006.13448},
  eprint = {2006.13448},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.13448},
  url = {http://arxiv.org/abs/2006.13448},
  urldate = {2024-05-08},
  abstract = {We introduce and analyze a variant of multivariate singular spectrum analysis (mSSA), a popular time series method to impute and forecast a multivariate time series. Under a spatio-temporal factor model we introduce, given \$N\$ time series and \$T\$ observations per time series, we establish prediction mean-squared-error for both imputation and out-of-sample forecasting effectively scale as \$1 / {\textbackslash}sqrt\{{\textbackslash}min(N, T )T\}\$. This is an improvement over: (i) \$1 /{\textbackslash}sqrt\{T\}\$ error scaling of SSA, the restriction of mSSA to a univariate time series; (ii) \$1/{\textbackslash}min(N, T)\$ error scaling for matrix estimation methods which do not exploit temporal structure in the data. The spatio-temporal model we introduce includes any finite sum and products of: harmonics, polynomials, differentiable periodic functions, and Holder continuous functions. Our out-of-sample forecasting result could be of independent interest for online learning under a spatio-temporal factor model. Empirically, on benchmark datasets, our variant of mSSA performs competitively with state-of-the-art neural-network time series methods (e.g. DeepAR, LSTM) and significantly outperforms classical methods such as vector autoregression (VAR). Finally, we propose extensions of mSSA: (i) a variant to estimate time-varying variance of a time series; (ii) a tensor variant which has better sample complexity for certain regimes of \$N\$ and \$T\$.},
  archiveprefix = {arXiv},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Agarwal22multivarSinglSpect.pdf}
}

@misc{Agarwal21TspDBTimeSeries,
  title = {{{tspDB}}: {{Time Series Predict DB}}},
  shorttitle = {{{tspDB}}},
  author = {Agarwal, Anish and Alomar, Abdullah and Shah, Devavrat},
  year = {2021},
  month = feb,
  number = {arXiv:1903.07097},
  eprint = {1903.07097},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1903.07097},
  url = {http://arxiv.org/abs/1903.07097},
  urldate = {2024-05-08},
  abstract = {A major bottleneck of the current Machine Learning (ML) workflow is the time consuming, error prone engineering required to get data from a datastore or a database (DB) to the point an ML algorithm can be applied to it. Hence, we explore the feasibility of directly integrating prediction functionality on top of a data store or DB. Such a system ideally: (i) provides an intuitive prediction query interface which alleviates the unwieldy data engineering; (ii) provides state-of-the-art statistical accuracy while ensuring incremental model update, low model training time and low latency for making predictions. As the main contribution we explicitly instantiate a proof-of-concept, tspDB, which directly integrates with PostgreSQL. We rigorously test tspDB's statistical and computational performance against the state-of-the-art time series algorithms, including a Long-Short-Term-Memory (LSTM) neural network and DeepAR (industry standard deep learning library by Amazon). Statistically, on standard time series benchmarks, tspDB outperforms LSTM and DeepAR with 1.1-1.3x higher relative accuracy. Computationally, tspDB is 59-62x and 94-95x faster compared to LSTM and DeepAR in terms of median ML model training time and prediction query latency, respectively. Further, compared to PostgreSQL's bulk insert time and its SELECT query latency, tspDB is slower only by 1.3x and 2.6x respectively. That is, tspDB is a real-time prediction system in that its model training / prediction query time is similar to just inserting / reading data from a DB. As an algorithmic contribution, we introduce an incremental multivariate matrix factorization based time series method, which tspDB is built off. We show this method also allows one to produce reliable prediction intervals by accurately estimating the time-varying variance of a time series, thereby addressing an important problem in time series analysis.},
  archiveprefix = {arXiv},
  keywords = {obsLitNote},
  note = {Agarwal21TspDBTimeSeries
\par
The postgres-based imputing auto-forecaster meant for amateurs in (Zewe, 2022). I found this during top N forecasting universities survey.
\par
\section{Related}

\begin{itemize}

\item (Agarwal et al., 2022)
\item (Zewe, 2022)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Agarwal21TspDBTimeSeries.pdf}
}

@article{Sadana24contextualOptSurvey,
  title = {A Survey of Contextual Optimization Methods for Decision-Making under Uncertainty},
  author = {Sadana, Utsav and Chenreddy, Abhilash and Delage, Erick and Forel, Alexandre and Frejinger, Emma and Vidal, Thibaut},
  year = {2024},
  month = mar,
  journal = {European Journal of Operational Research},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2024.03.020},
  url = {https://www.sciencedirect.com/science/article/pii/S0377221724002200},
  urldate = {2024-05-08},
  abstract = {Recently there has been a surge of interest in operations research (OR) and the machine learning (ML) community in combining prediction algorithms and optimization techniques to solve decision-making problems in the face of uncertainty. This gave rise to the field of contextual optimization, under which data-driven procedures are developed to prescribe actions to the decision-maker that make the best use of the most recently updated information. A large variety of models and methods have been presented in both OR and ML literature under a variety of names, including data-driven optimization, prescriptive optimization, predictive stochastic programming, policy optimization, (smart) predict/estimate-then-optimize, decision-focused learning, (task-based) end-to-end learning/forecasting/optimization, etc. This survey article unifies these models under the lens of contextual stochastic optimization, thus providing a general presentation of a large variety of problems. We identify three main frameworks for learning policies from data and present the existing models and methods under a uniform notation and terminology. Our objective with this survey is to both strengthen the general understanding of this active field of research and stimulate further theoretical and algorithmic advancements in integrating ML and stochastic programming.},
  keywords = {obsLitNote,optimization},
  note = {Sadana24contextualOptSurvey
\par
Survey of techniques where learning alogrithms are integrated with optimization.
\par
Cites Donte's work, and maybe others?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sadana24contextualOptSurvey.pdf}
}

@misc{Donti21EnforcingRobustControl,
  title = {Enforcing Robust Control Guarantees within Neural Network Policies},
  author = {Donti, Priya L. and Roderick, Melrose and Fazlyab, Mahyar and Kolter, J. Zico},
  year = {2021},
  month = jan,
  number = {arXiv:2011.08105},
  eprint = {2011.08105},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.08105},
  url = {http://arxiv.org/abs/2011.08105},
  urldate = {2024-05-08},
  abstract = {When designing controllers for safety-critical systems, practitioners often face a challenging tradeoff between robustness and performance. While robust control methods provide rigorous guarantees on system stability under certain worst-case disturbances, they often yield simple controllers that perform poorly in the average (non-worst) case. In contrast, nonlinear control methods trained using deep learning have achieved state-of-the-art performance on many control tasks, but often lack robustness guarantees. In this paper, we propose a technique that combines the strengths of these two approaches: constructing a generic nonlinear control policy class, parameterized by neural networks, that nonetheless enforces the same provable robustness criteria as robust control. Specifically, our approach entails integrating custom convex-optimization-based projection layers into a neural network-based policy. We demonstrate the power of this approach on several domains, improving in average-case performance over existing robust control methods and in worst-case stability over (non-robust) deep RL methods.},
  archiveprefix = {arXiv},
  note = {Donti21robustCtrlNeural
\par
convex-optimization-based projection layers built into a neural network-based policy for robust gaurantees. ~Could provide same gaurantees but better average performance than robust.
\par
Comment: Code available online: https://github.com/locuslab/robust-nn-control},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Donti21robustCtrlNeural.pdf}
}

@misc{Donti23optInLoopTalk,
  title = {Optimization-in-the-Loop {{ML}} for Energy and Climate},
  shorttitle = {Priya {{Donti}}},
  author = {Donti, Priya},
  year = {2023},
  month = nov,
  url = {https://www.youtube.com/watch?v=yren7Dq92yc},
  urldate = {2024-05-08},
  abstract = {Title: Optimization-in-the-loop ML for energy and climate Speaker: Priya Donti, MIT and Climate Change AI Abstract: Addressing climate change will require concerted action across society, including the development of innovative technologies. While methods from machine learning (ML) have the potential to play an important role, these methods often struggle to contend with the physics, hard constraints, and complex decision-making processes that are inherent to many climate and energy problems. To address these limitations, I present the framework of ``optimization-in-the-loop ML,'' and show how it can enable the design of ML models that explicitly capture relevant constraints and decision-making processes. For instance, this framework can be used to design learning-based controllers that provably enforce the stability criteria or operational constraints associated with the systems in which they operate. It can also enable the design of task-based learning procedures that are cognizant of the downstream decision-making processes for which a model's outputs will be used. By significantly improving performance and preventing critical failures, such techniques can unlock the potential of ML for operating low-carbon power grids, improving energy efficiency in buildings, and addressing other high-impact problems of relevance to climate action. About the speaker: Priya Donti is an Assistant Professor at MIT EECS and LIDS, whose research focuses on machine learning for forecasting, optimization, and control in high-renewables power grids. Specifically, her work explores methods to incorporate the physics and hard constraints associated with electric power systems into deep learning workflows. Priya is also the co-founder of Climate Change AI, a global non-profit initiative to catalyze impactful work at the intersection of climate change and machine learning. Priya received her Ph.D. in Computer Science and Public Policy from Carnegie Mellon University, and is a recipient of the MIT Technology Review's 2021 ``35 Innovators Under 35'' award, the ACM SIGEnergy Doctoral Dissertation Award, the Siebel Scholarship, the U.S. Department of Energy Computational Science Graduate Fellowship, and best paper awards at ICML (honorable mention), ACM e-Energy (runner-up), PECI, the Duke Energy Data Analytics Symposium, and the NeurIPS workshop on AI for Social Good. Location: This is an online seminar. Connect using Zoom. Date: 2023-11-02 15:00 Upcoming seminars:     2023-11-09, 15:00: Nata{\v s}a Sladoje, Uppsala University     2023-11-16, 15:00: Jonas Hellgren, RISE     2023-11-30, 16:00: Ben Weinstein, University of Florida     2023-12-07, 15:00: Stefan Bauer, TU Munich     All times are in CET. More information and coming seminars: https://ri.se/lm-sem -- The Learning Machines Team},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Donti23optInLoopTalk.pdf}
}

@inproceedings{Donti17taskEndToEndLrnStochOpt,
  title = {Task-Based {{End-to-end Model Learning}} in {{Stochastic Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Donti, Priya and Amos, Brandon and Kolter, J. Zico},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2017/hash/3fc2c60b5782f641f76bcefc39fb2392-Abstract.html},
  urldate = {2024-05-08},
  abstract = {With the increasing popularity of machine learning techniques, it has become common to see prediction algorithms operating within some larger process. However, the criteria by which we train these algorithms often differ from the ultimate criteria on which we evaluate them. This paper proposes an end-to-end approach for learning probabilistic machine learning models in a manner that directly captures the ultimate task-based objective for which they will be used, within the context of stochastic programming. We present three experimental evaluations of the proposed approach: a classical inventory stock problem, a real-world electrical grid scheduling task, and a real-world energy storage arbitrage task. We show that the proposed approach can outperform both traditional modeling and purely black-box policy optimization approaches in these applications.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Donti17taskEndToEndLrnStochOpt.pdf}
}

@inproceedings{Chen21enforcePolicyFeasibilityOpt,
  title = {Enforcing {{Policy Feasibility Constraints}} through {{Differentiable Projection}} for {{Energy Optimization}}},
  booktitle = {Proceedings of the {{Twelfth ACM International Conference}} on {{Future Energy Systems}}},
  author = {Chen, Bingqing and Donti, Priya L. and Baker, Kyri and Kolter, J. Zico and Berg{\'e}s, Mario},
  year = {2021},
  month = jun,
  series = {E-{{Energy}} '21},
  pages = {199--210},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3447555.3464874},
  url = {https://dl.acm.org/doi/10.1145/3447555.3464874},
  urldate = {2024-05-08},
  abstract = {While reinforcement learning (RL) is gaining popularity in energy systems control, its real-world applications are limited due to the fact that the actions from learned policies may not satisfy functional requirements or be feasible for the underlying physical system. In this work, we propose PROjected Feasibility (PROF), a method to enforce convex operational constraints within neural policies. Specifically, we incorporate a differentiable projection layer within a neural network-based policy to enforce that all learned actions are feasible. We then update the policy end-to-end by propagating gradients through this differentiable projection layer, making the policy cognizant of the operational constraints. We demonstrate our method on two applications: energy-efficient building operation and inverter control. In the building operation setting, we show that PROF maintains thermal comfort requirements while improving energy efficiency by 4\% over state-of-the-art methods. In the inverter control setting, PROF perfectly satisfies voltage constraints on the IEEE 37-bus feeder system, as it learns to curtail as little renewable energy as possible within its safety set.},
  isbn = {978-1-4503-8333-2},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chen21enforcePolicyFeasibilityOpt.pdf}
}

@phdthesis{Donti22deeplLrnPowSysPhD,
  title = {Bridging Deep Learning and Electric Power Systems},
  author = {Donti, Priya},
  year = {2022},
  school = {Carnegie Mellon University},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Donti22deeplLrnPowSysPhD.pdf}
}

@article{Agarwal22adversRobustStochOptPowFlow,
  title = {Employing Adversarial Robustness Techniques for Large-Scale Stochastic Optimal Power Flow},
  author = {Agarwal, Aayushya and Donti, Priya L. and Kolter, J. Zico and Pileggi, Larry},
  year = {2022},
  month = nov,
  journal = {Electric Power Systems Research},
  volume = {212},
  pages = {108497},
  issn = {0378-7796},
  doi = {10.1016/j.epsr.2022.108497},
  url = {https://www.sciencedirect.com/science/article/pii/S0378779622006101},
  urldate = {2024-05-08},
  abstract = {High penetrations of renewables and extreme weather phenomena are driving the need for large-scale stochastic optimization for power grid operations and planning. However, large-scale stochastic optimization remains computationally challenging due to the wide range of potential probabilistic outcomes and the non-convexity of the AC network constraints. Using a minimax formulation rooted in robust optimization, we define a generic methodology to obtain a feasible dispatch that accommodates the probabilistic nature of loads and renewable generation sources. We introduce a method to efficiently solve this minimax formulation, adopting techniques from the literature on adversarial robustness in machine learning to improve scalability and convergence. We demonstrate that our method maintains AC feasibility over a wide range of probabilistic scenarios, and demonstrate the scalability of our method by determining a dispatch for a synthetic 11,000 bus system.},
  keywords = {obsLitNote}
}

@misc{Donti21DC3LearningMethod,
  title = {{{DC3}}: {{A}} Learning Method for Optimization with Hard Constraints},
  shorttitle = {{{DC3}}},
  author = {Donti, Priya L. and Rolnick, David and Kolter, J. Zico},
  year = {2021},
  month = apr,
  number = {arXiv:2104.12225},
  eprint = {2104.12225},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.12225},
  url = {http://arxiv.org/abs/2104.12225},
  urldate = {2024-05-08},
  abstract = {Large optimization problems with hard constraints arise in many settings, yet classical solvers are often prohibitively slow, motivating the use of deep networks as cheap "approximate solvers." Unfortunately, naive deep learning approaches typically cannot enforce the hard constraints of such problems, leading to infeasible solutions. In this work, we present Deep Constraint Completion and Correction (DC3), an algorithm to address this challenge. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. We demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid. In both cases, DC3 achieves near-optimal objective values while preserving feasibility.},
  archiveprefix = {arXiv},
  note = {Comment: In ICLR 2021. Code available at https://github.com/locuslab/DC3},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Donti21learnMethHardCnstrntsDC3.pdf}
}

@misc{Udandarao24NoZeroShotExponential,
  title = {No "{{Zero-Shot}}" {{Without Exponential Data}}: {{Pretraining Concept Frequency Determines Multimodal Model Performance}}},
  shorttitle = {No "{{Zero-Shot}}" {{Without Exponential Data}}},
  author = {Udandarao, Vishaal and Prabhu, Ameya and Ghosh, Adhiraj and Sharma, Yash and Torr, Philip H. S. and Bibi, Adel and Albanie, Samuel and Bethge, Matthias},
  year = {2024},
  month = apr,
  number = {arXiv:2404.04125},
  eprint = {2404.04125},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.04125},
  url = {http://arxiv.org/abs/2404.04125},
  urldate = {2024-05-09},
  abstract = {Web-crawled pretraining datasets underlie the impressive "zero-shot" evaluation performance of multimodal models, such as CLIP for classification/retrieval and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of "zero-shot" generalization is for such multimodal models, as it is not known to what extent their pretraining datasets encompass the downstream concepts targeted for during "zero-shot" evaluation. In this work, we ask: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets? We comprehensively investigate this question across 34 models and five standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. We consistently find that, far from exhibiting "zero-shot" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream "zero-shot" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions. Furthermore, upon benchmarking models on long-tailed data sampled based on our analysis, we demonstrate that multimodal models across the board perform poorly. We contribute this long-tail test set as the "Let it Wag!" benchmark to further research in this direction. Taken together, our study reveals an exponential need for training data which implies that the key to "zero-shot" generalization capabilities under large-scale training paradigms remains to be found.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Udandarao24NoZeroShotExponential
\par
Evidence that there's not enough data to reach AGI with current methods -- performance gains from more data will plateau too early.
\par
\section{Also}

\begin{itemize}

\item \href{https://www.youtube.com/watch?v=dDUC-LqVrPU}{Video summary} of this paper
\item (Villalobos et al., 2022)
\item (Patel, 2023)
\item Extended version of the short paper accepted at DPFM, ICLR'24

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Udandarao24NoZeroShotExponential.pdf}
}

@misc{Bubeck23SparksArtificialGeneral,
  title = {Sparks of {{Artificial General Intelligence}}: {{Early}} Experiments with {{GPT-4}}},
  shorttitle = {Sparks of {{Artificial General Intelligence}}},
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  year = {2023},
  month = apr,
  number = {arXiv:2303.12712},
  eprint = {2303.12712},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2303.12712},
  urldate = {2024-05-12},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Bubeck23SparksAGIgpt4
\par
Sparks of AGI an GPT4. ~Cited by (Bishop and Bishop, 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bubeck23SparksArtificialGenerala.pdf}
}

@misc{Jin24TimeLLMTimeSeries,
  title = {Time-{{LLM}}: {{Time Series Forecasting}} by {{Reprogramming Large Language Models}}},
  shorttitle = {Time-{{LLM}}},
  author = {Jin, Ming and Wang, Shiyu and Ma, Lintao and Chu, Zhixuan and Zhang, James Y. and Shi, Xiaoming and Chen, Pin-Yu and Liang, Yuxuan and Li, Yuan-Fang and Pan, Shirui and Wen, Qingsong},
  year = {2024},
  month = jan,
  number = {arXiv:2310.01728},
  eprint = {2310.01728},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.01728},
  url = {http://arxiv.org/abs/2310.01728},
  urldate = {2024-05-14},
  abstract = {Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Jin24frcastTStimeLLM
\par
\begin{itemize}

\item Medium \href{https://towardsdatascience.com/time-llm-reprogram-an-llm-for-time-series-forecasting-e2558087b8ac}{article on Time-LLM}
\item \href{https://paperswithcode.com/paper/time-llm-time-series-forecasting-by}{has code}

\end{itemize}

\par
Comment: Accepted by the 12th International Conference on Learning Representations (ICLR 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Jin24TimeLLMTimeSeries.pdf}
}

@misc{Wolf24simpleKAN,
  title = {Kolmogorov-{{Arnold Networks}}: The Latest Advance in {{Neural Networks}}, Simply Explained},
  shorttitle = {Kolmogorov-{{Arnold Networks}}},
  author = {Wolf, Theo},
  year = {2024},
  month = may,
  journal = {Medium},
  url = {https://towardsdatascience.com/kolmogorov-arnold-networks-the-latest-advance-in-neural-networks-simply-explained-f083cf994a85},
  urldate = {2024-05-15},
  abstract = {The new type of network that is making waves in the ML world.},
  langid = {english},
  keywords = {hasCode,obsLitNote},
  note = {Wolf24simpleKAN
\par
This is a new ``neural'' network that is said to be less likely to overfit than MLPs, more explainable, and quicker to train. ~At this stage of development, training is still flakey -- \emph{very} sensitive to small hyperparam differences -- but the author says that it's likely that big AI labs are already trying it out in their LLMs. ~Maybe a research innovation will result in a deep learning level of hyperparameter insensitivity. The author provides \href{https://github.com/TheodoreWolf/KAN}{a GitHub of his experiments}, and says he's excited about the possibilities in ML for science.
\par
MLPs, a sequence of linear regressions (trainable weights) separated by nonlinearities (fixed-form nodes) can approximate anything, kind of like a Taylors series. ~There is also the Kolmogorov-Arnold Representation Theorem, in which a similarly simple structure can represent almost anything: ~any multivariate function can be represented by the sum of (trainable) 1-dimensional non-linear functions; there are no weights. The network nodes only sum. ~The Kolmogorov-Arnold network (KAN) can be trained with standard backprop.
\par
In the paper being reviewed here (which also has a GitHub repository), the user specifies network hyperparameters:
\par
\begin{itemize}

\item num layers and num nodes in each
\item A bspline grid which serves as a starting point for defining the network nonlinearity functions. ~I'm guessing this is number of knots, not sure if they are evenly spaced.
\item bspline order
\item random seed

\end{itemize}

\par
Training goes like this:
\par
\begin{itemize}

\item train the spline coeffs with backprop (seems like solver is configurable, example uses LBFGS)
\item prune: remove edges and nodes ``below a threshold of relevance.'' ~I guess I'd need to read the paper to know what that means
\item retrain
\item 
\par
replace each spline with a symbolic function chosen from the list you supply. ~
\par
\begin{itemize}

\item I think the replacements must have affine params
\item Either you do the replacent manually, or it does it automatically (how not explained)

\end{itemize}

\item retrain (the new affine parameters)

\end{itemize}

\par
The result is a symbolic equation. ~With current tuning procedure, this can vary wildly with hyperparam choice, but the python PySR library (Cranmer, 2023) is even flakier. ~},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wolf24simpleKAN.pdf}
}

@misc{Cranmer23InterpretableMachineLearning,
  title = {Interpretable {{Machine Learning}} for {{Science}} with {{PySR}} and {{SymbolicRegression}}.Jl},
  author = {Cranmer, Miles},
  year = {2023},
  month = may,
  number = {arXiv:2305.01582},
  eprint = {2305.01582},
  primaryclass = {astro-ph, physics:physics},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.01582},
  url = {http://arxiv.org/abs/2305.01582},
  urldate = {2024-05-15},
  abstract = {PySR is an open-source library for practical symbolic regression, a type of machine learning which aims to discover human-interpretable symbolic models. PySR was developed to democratize and popularize symbolic regression for the sciences, and is built on a high-performance distributed back-end, a flexible search algorithm, and interfaces with several deep learning packages. PySR's internal search algorithm is a multi-population evolutionary algorithm, which consists of a unique evolve-simplify-optimize loop, designed for optimization of unknown scalar constants in newly-discovered empirical expressions. PySR's backend is the extremely optimized Julia library SymbolicRegression.jl, which can be used directly from Julia. It is capable of fusing user-defined operators into SIMD kernels at runtime, performing automatic differentiation, and distributing populations of expressions to thousands of cores across a cluster. In describing this software, we also introduce a new benchmark, "EmpiricalBench," to quantify the applicability of symbolic regression algorithms in science. This benchmark measures recovery of historical empirical equations from original and synthetic datasets.},
  archiveprefix = {arXiv},
  keywords = {hasCode},
  note = {Cranmer23interpMLrgrssnPySR
\par
Python lib that fits a symbolic expression to your data. ~Use for physical solar model? ~Use to make an interpretable model of a deep NN (as suggested by author)? Is said to be flakier than KAN nets on some small problems: (Wolf, 2024)
\par
Comment: 24 pages, 5 figures, 3 tables. Feedback welcome. Paper source found at https://github.com/MilesCranmer/pysr\_paper ; PySR at https://github.com/MilesCranmer/PySR ; SymbolicRegression.jl at https://github.com/MilesCranmer/SymbolicRegression.jl},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Cranmer23interpMLrgrssnPySR.pdf}
}

@misc{Liu24KANKolmogorovArnoldNetworks,
  title = {{{KAN}}: {{Kolmogorov-Arnold Networks}}},
  shorttitle = {{{KAN}}},
  author = {Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Solja{\v c}i{\'c}, Marin and Hou, Thomas Y. and Tegmark, Max},
  year = {2024},
  month = may,
  number = {arXiv:2404.19756},
  eprint = {2404.19756},
  primaryclass = {cond-mat, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.19756},
  url = {http://arxiv.org/abs/2404.19756},
  urldate = {2024-05-15},
  abstract = {Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons"), KANs have learnable activation functions on edges ("weights"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.},
  archiveprefix = {arXiv},
  keywords = {hasCode},
  note = {Liu24kolmogArnoldKAN
\par
Comment: 48 pages, 20 figures. Codes are available at https://github.com/KindXiaoming/pykan},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Liu24kolmogArnoldKAN.pdf}
}

@article{Shi19optBattCtrlAging,
  title = {Optimal {{Battery Control Under Cycle Aging Mechanisms}} in {{Pay}} for {{Performance Settings}}},
  author = {Shi, Yuanyuan and Xu, Bolun and Tan, Yushi and Kirschen, Daniel and Zhang, Baosen},
  year = {2019},
  month = jun,
  journal = {IEEE Transactions on Automatic Control},
  volume = {64},
  number = {6},
  pages = {2324--2339},
  issn = {1558-2523},
  doi = {10.1109/TAC.2018.2867507},
  url = {https://ieeexplore.ieee.org/abstract/document/8449100},
  urldate = {2024-05-15},
  abstract = {We study the optimal control of battery energy storage under a general ``pay-for-performance'' setup such as providing frequency regulation and renewable integration. In these settings, batteries need to carefully balance the tradeoff between following the instruction signals and their degradation costs in real-time. Existing battery control strategies either do not consider the uncertainty of future signals, or cannot accurately account for battery cycle aging mechanism during operation. In this paper, we take a different approach to the optimal battery control problem. Instead of attacking the complexity of a battery degradation function or the lack of future information one at a time, we address these two challenges together in a joint fashion. In particular, we present an electrochemically accurate and trackable battery degradation model called the rainflow cycle-based model. We prove that the degradation cost is convex. Then, we propose an online control policy with a simple threshold structure and show that it achieves near-optimal performance with respect to an offline controller that has complete future information. We explicitly characterize the optimality gap and show that it is independent to the duration of operation. Simulation results with both synthetic and real regulation traces are conducted to illustrate the theoretical results.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Shi19optBattCtrlAging.pdf}
}

@misc{Peixeiro24tsFrcstKAN,
  title = {Kolmogorov-{{Arnold Networks}} ({{KANs}}) for {{Time Series Forecasting}}},
  author = {Peixeiro, Marco},
  year = {2024},
  month = may,
  journal = {Medium},
  url = {https://towardsdatascience.com/kolmogorov-arnold-networks-kans-for-time-series-forecasting-9d49318c3172},
  urldate = {2024-05-17},
  abstract = {Discover the Kolmogorov-Arnold Networks (KANs) and apply them for time series forecasting using Python},
  langid = {english},
  keywords = {hasCode,obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Peixeiro24tsFrcstKAN.pdf}
}

@misc{Liu24kolmogArnoldKANytube,
  title = {{{KAN}}: {{Kolmogorov-Arnold Networks}}},
  shorttitle = {{{KAN}}},
  author = {Liu, Ziming},
  year = {2024},
  month = may,
  publisher = {YouTube},
  address = {Valence Labs},
  url = {https://www.youtube.com/watch?v=AUDHb-tnlB0},
  urldate = {2024-05-18},
  abstract = {Portal is the home of the AI for drug discovery community. Join for more details on this talk and to connect with the speakers: https://portal.valencelabs.com/logg Abstract: Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons"), KANs have learnable activation functions on edges ("weights"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs. Speakers: Ziming Liu Twitter Hannes: ~~/~hannesstaerk~~ Twitter Dominique: ~~/~dom\_beaini~~ {\textasciitilde} Chapters 00:00 - Intro + Background 05:06 - From KART to KAN 07:56 - MLP vs KAN 16:05 - Accuracy: Scaling of KANs 26:35 - Interpretability: KAN for Science 38:04 - Q+A Break 57:15 - Strengths and Weaknesses 59:28 - Philosophy 1:08:45 - Anecdotes Behind the Scenes 1:11:49 - Final Thoughts 1:14:58 - Q+A},
  keywords = {obsLitNote}
}

@misc{Vaca-Rubio24KolmogorovArnoldNetworksKANs,
  title = {Kolmogorov-{{Arnold Networks}} ({{KANs}}) for {{Time Series Analysis}}},
  author = {{Vaca-Rubio}, Cristian J. and Blanco, Luis and Pereira, Roberto and Caus, M{\`a}rius},
  year = {2024},
  month = may,
  number = {arXiv:2405.08790},
  eprint = {2405.08790},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.08790},
  url = {http://arxiv.org/abs/2405.08790},
  urldate = {2024-05-18},
  abstract = {This paper introduces a novel application of Kolmogorov-Arnold Networks (KANs) to time series forecasting, leveraging their adaptive activation functions for enhanced predictive modeling. Inspired by the Kolmogorov-Arnold representation theorem, KANs replace traditional linear weights with spline-parametrized univariate functions, allowing them to learn activation patterns dynamically. We demonstrate that KANs outperforms conventional Multi-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting task, providing more accurate results with considerably fewer number of learnable parameters. We also provide an ablation study of KAN-specific parameters impact on performance. The proposed approach opens new avenues for adaptive forecasting models, emphasizing the potential of KANs as a powerful tool in predictive analytics.},
  archiveprefix = {arXiv},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\VacaRubio24timeSeriesKAN.pdf}
}

@article{Uddin23waveletPhysBasedNNdiffyQ,
  title = {Wavelets Based Physics Informed Neural Networks to Solve Non-Linear Differential Equations},
  author = {Uddin, Ziya and Ganga, Sai and Asthana, Rishi and Ibrahim, Wubshet},
  year = {2023},
  month = feb,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {2882},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-29806-3},
  url = {https://www.nature.com/articles/s41598-023-29806-3},
  urldate = {2024-05-18},
  abstract = {In this study, the applicability of physics informed neural networks using wavelets as an activation function is discussed to solve non-linear differential equations. One of the prominent equations arising in fluid dynamics namely Blasius viscous flow problem is solved. A linear coupled differential equation, a non-linear coupled differential equation, and partial differential equations are also solved in order to demonstrate the method's versatility. As the neural network's optimum design is important and is problem-specific, the influence of some of the key factors on the model's accuracy is also investigated. To confirm the approach's efficacy, the outcomes of the suggested method were compared with those of the existing approaches. The suggested method was observed to be both efficient and accurate.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Uddin23waveletPhysBasedNNdiffyQ.pdf}
}

@misc{Oreshkin20NBEATSNeuralBasis,
  title = {N-{{BEATS}}: {{Neural}} Basis Expansion Analysis for Interpretable Time Series Forecasting},
  shorttitle = {N-{{BEATS}}},
  author = {Oreshkin, Boris N. and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
  year = {2020},
  month = feb,
  number = {arXiv:1905.10437},
  eprint = {1905.10437},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.10437},
  url = {http://arxiv.org/abs/1905.10437},
  urldate = {2024-05-18},
  abstract = {We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11\% over a statistical benchmark and by 3\% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.},
  archiveprefix = {arXiv},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Oreshkin20neuraBasisFrcstNBEATS.pdf}
}

@misc{Cranmer24greatTheoryInNN,
  title = {The {{Next Great Scientific Theory}} Is {{Hiding Inside}} a {{Neural Network}}},
  author = {Cranmer, Miles},
  year = {2024},
  month = apr,
  address = {Simons Foundation},
  url = {https://www.youtube.com/watch?v=fk2r8y5TfNY},
  urldate = {2024-05-18},
  abstract = {Machine learning methods such as neural networks are quickly finding uses in everything from text generation to construction cranes. Excitingly, those same tools also promise a new paradigm for scientific discovery. In this Presidential Lecture, Miles Cranmer will outline an innovative approach that leverages neural networks in the scientific process. Rather than directly modeling data, the approach interprets neural networks trained using the data. Through training, the neural networks can capture the physics underlying the system being studied. By extracting what the neural networks have learned, scientists can improve their theories. He will also discuss the Polymathic AI initiative, a collaboration between researchers at the Flatiron Institute and scientists around the world. Polymathic AI is designed to spur scientific discovery using similar technology to that powering ChatGPT. Using Polymathic AI, scientists will be able to model a broad range of physical systems across different scales. More details: https://www.simonsfoundation.org/even...},
  keywords = {obsLitNote}
}

@misc{Nixtla24elecDemandPeakFrcst,
  title = {Detect {{Demand Peaks}}},
  author = {{Nixtla}},
  year = {2024},
  journal = {Nixtla},
  url = {https://nixtlaverse.nixtla.io/mlforecast/docs/tutorials/electricity_peak_forecasting.html},
  urldate = {2024-05-18},
  abstract = {In this example we will show how to perform electricity load forecasting on the ERCOT (Texas) market for detecting daily peaks.},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nixtla24elecDemandPeakFrcst.pdf}
}

@misc{Genet24TKANTemporalKolmogorovArnold,
  title = {{{TKAN}}: {{Temporal Kolmogorov-Arnold Networks}}},
  shorttitle = {{{TKAN}}},
  author = {Genet, Remi and Inzirillo, Hugo},
  year = {2024},
  month = may,
  number = {arXiv:2405.07344},
  eprint = {2405.07344},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.07344},
  url = {http://arxiv.org/abs/2405.07344},
  urldate = {2024-05-18},
  abstract = {Recurrent Neural Networks (RNNs) have revolutionized many areas of machine learning, particularly in natural language and data sequence processing. Long Short-Term Memory (LSTM) has demonstrated its ability to capture long-term dependencies in sequential data. Inspired by the Kolmogorov-Arnold Networks (KANs) a promising alternatives to Multi-Layer Perceptrons (MLPs), we proposed a new neural networks architecture inspired by KAN and the LSTM, the Temporal Kolomogorov-Arnold Networks (TKANs). TKANs combined the strenght of both networks, it is composed of Recurring Kolmogorov-Arnold Networks (RKANs) Layers embedding memory management. This innovation enables us to perform multi-step time series forecasting with enhanced accuracy and efficiency. By addressing the limitations of traditional models in handling complex sequential patterns, the TKAN architecture offers significant potential for advancements in fields requiring more than one step ahead forecasting.},
  archiveprefix = {arXiv},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Genet24kolmogArnoldFrcstTKAN.pdf}
}

@misc{Bordt23ShapleyValuesGeneralized,
  title = {From {{Shapley Values}} to {{Generalized Additive Models}} and Back},
  author = {Bordt, Sebastian and {von Luxburg}, Ulrike},
  year = {2023},
  month = feb,
  number = {arXiv:2209.04012},
  eprint = {2209.04012},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.04012},
  url = {http://arxiv.org/abs/2209.04012},
  urldate = {2024-05-18},
  abstract = {In explainable machine learning, local post-hoc explanation algorithms and inherently interpretable models are often seen as competing approaches. This work offers a partial reconciliation between the two by establishing a correspondence between Shapley Values and Generalized Additive Models (GAMs). We introduce \$n\$-Shapley Values, a parametric family of local post-hoc explanation algorithms that explain individual predictions with interaction terms up to order \$n\$. By varying the parameter \$n\$, we obtain a sequence of explanations that covers the entire range from Shapley Values up to a uniquely determined decomposition of the function we want to explain. The relationship between \$n\$-Shapley Values and this decomposition offers a functionally-grounded characterization of Shapley Values, which highlights their limitations. We then show that \$n\$-Shapley Values, as well as the Shapley Taylor- and Faith-Shap interaction indices, recover GAMs with interaction terms up to order \$n\$. This implies that the original Shapely Values recover GAMs without variable interactions. Taken together, our results provide a precise characterization of Shapley Values as they are being used in explainable machine learning. They also offer a principled interpretation of partial dependence plots of Shapley Values in terms of the underlying functional decomposition. A package for the estimation of different interaction indices is available at {\textbackslash}url\{https://github.com/tml-tuebingen/nshap\}.},
  archiveprefix = {arXiv},
  note = {Comment: AISTATS 2023},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bordt23nShapleyValsGnrlzd.pdf}
}

@misc{Kafritsas24tsFrcstFndtnMOMENT,
  title = {{{MOMENT}}: {{A Foundation Model}} for {{Time Series Forecasting}}, {{Classification}}, {{Anomaly Detection}}},
  shorttitle = {{{MOMENT}}},
  author = {Kafritsas, Nikos},
  year = {2024},
  month = apr,
  journal = {Medium},
  url = {https://towardsdatascience.com/moment-a-foundation-model-for-time-series-forecasting-classification-anomaly-detection-1e35f5b6ca76},
  urldate = {2024-05-19},
  abstract = {A unified model that covers multiple time-series tasks},
  langid = {english},
  keywords = {todo},
  note = {Kafritsas24tsFrcstFndtnMOMENT
\par
A transformer-based forecasting foundation model that can also do TS classification, anomaly detection, and imputation. ~Shortest horizon reported is monthly. Lots of open source forecasting data is mentioned, including the data used to train this one, as well as the code, and soon, the weights.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kafritsas24tsFrcstFndtnMOMENT.pdf}
}

@article{Basciftci21DistributionallyRobustFacility,
  title = {Distributionally Robust Facility Location Problem under Decision-Dependent Stochastic Demand},
  author = {Basciftci, Beste and Ahmed, Shabbir and Shen, Siqian},
  year = {2021},
  month = jul,
  journal = {European Journal of Operational Research},
  volume = {292},
  number = {2},
  pages = {548--561},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2020.11.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0377221720309449},
  urldate = {2024-06-02},
  abstract = {While the traditional facility location problem considers exogenous demand, in some applications, locations of facilities could affect the willingness of customers to use certain types of services, e.g., carsharing, and therefore they also affect realizations of random demand. Moreover, a decision maker may not know the exact distribution of such endogenous demand and how it is affected by location choices. In this paper, we consider a distributionally robust facility location problem, in which we interpret the moments of stochastic demand as functions of facility-location decisions. We reformulate a two-stage decision-dependent distributionally robust optimization model as a monolithic formulation, and then derive exact mixed-integer linear programming reformulation as well as valid inequalities when the means and variances of demand are piecewise linear functions of location solutions. We conduct extensive computational studies, in which we compare our model with a decision-dependent deterministic model, as well as stochastic programming and distributionally robust models without the decision-dependent assumption. The results show superior performance of our approach with remarkable improvement in profit and quality of service under various settings, in addition to computational speed-ups given by formulation enhancements. These results draw attention to the need of considering the impact of location decisions on customer demand within this strategic-level planning problem.},
  keywords = {todo},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Basciftci21DistributionallyRobustFacility.pdf}
}

@article{Blondel22EfficientModularImplicit,
  title = {Efficient and {{Modular Implicit Differentiation}}},
  author = {Blondel, Mathieu and Berthet, Quentin and Cuturi, Marco and Frostig, Roy and Hoyer, Stephan and {Llinares-Lopez}, Felipe and Pedregosa, Fabian and Vert, Jean-Philippe},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {5230--5242},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/228b9279ecf9bbafe582406850c57115-Abstract-Conference.html},
  urldate = {2024-06-02},
  langid = {english},
  keywords = {todo},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Blondel22EfficientModularImplicit.pdf}
}

@misc{Kolter20deepImplicitLayersNeuralODE,
  title = {Deep {{Implicit Layers}} - {{Neural ODEs}}, {{Deep Equilibirum Models}}, and {{Beyond}}},
  author = {Kolter, Zico and Duvenaud, David and Johnson., Matt},
  year = {2020},
  journal = {Neural Information Processing Systems Tutorial},
  url = {https://implicit-layers-tutorial.org/},
  urldate = {2024-06-02},
  abstract = {This web page is the companion website to our NeurIPS 2020 tutorial, created by Zico Kolter, David Duvenaud, and Matt Johnson. The page constain notes to accompany our tutorial (all created via Colab notebooks, which you can experiment with as you like), as well as links to our video presentation as slides. This web page will be under development until the official scheduled time of the tutorial (December 7, 1:30pm PT), and may undergo additional changes after that time.},
  keywords = {todo},
  note = {Kolter20deepImplicitLayersNeuralODE
\par
Tutorial video, slides, etc.}
}

@article{Gupta24DebiasingInSamplePolicy,
  title = {Debiasing {{In-Sample Policy Performance}} for {{Small-Data}}, {{Large-Scale Optimization}}},
  author = {Gupta, Vishal and Huang, Michael and Rusmevichientong, Paat},
  year = {2024},
  month = mar,
  journal = {Operations Research},
  volume = {72},
  number = {2},
  pages = {848--870},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.2022.2377},
  url = {https://pubsonline.informs.org/doi/10.1287/opre.2022.2377},
  urldate = {2024-06-02},
  abstract = {In many modern large-scale decision-making problems, data can be scarce. As a result, traditional methods such as cross-validation perform poorly in evaluating the performance of decision-making policies. In ``Debiasing In-Sample Policy Performance for Small-Data, Large-Scale Optimization,'' Gupta, Huang, and Rusmevichientong propose a novel estimator of the out-of-sample performance for a policy in data-driven optimization. Unlike cross-validation, their approach avoids sacrificing training data for evaluation. As a result, they theoretically show the estimator is asymptotically unbiased as the problem size grows. Furthermore, they show that the estimator is asymptotically optimal when applied to more specialized ``weakly coupled'' optimization problems. Finally, using a case study on dispatching emergency medical response services, they demonstrate their proposed method provides more accurate estimates of out-of-sample performance and selects better policies.           ,              Motivated by the poor performance of cross-validation in settings where data are scarce, we propose a novel estimator of the out-of-sample performance of a policy in data-driven optimization. Our approach exploits the optimization problem's sensitivity analysis to estimate the gradient of the optimal objective value with respect to the amount of noise in the data and uses the estimated gradient to debias the policy's in-sample performance. Unlike cross-validation techniques, our approach avoids sacrificing data for a test set and uses all data when training and hence is well suited to settings where data are scarce. We prove bounds on the bias and variance of our estimator for optimization problems with uncertain linear objectives but known, potentially nonconvex, feasible regions. For more specialized optimization problems where the feasible region is ``weakly coupled'' in a certain sense, we prove stronger results. Specifically, we provide explicit high-probability bounds on the error of our estimator that hold uniformly over a policy class and depends on the problem's dimension and policy class's complexity. Our bounds show that under mild conditions, the error of our estimator vanishes as the dimension of the optimization problem grows, even if the amount of available data remains small and constant. Said differently, we prove our estimator performs well in the small-data, large-scale regime. Finally, we numerically compare our proposed method to state-of-the-art approaches through a case-study on dispatching emergency medical response services using real data. Our method provides more accurate estimates of out-of-sample performance and learns better-performing policies.             Funding: This work was partially supported by the National Science Foundation, Division of Civil, Mechanical and Manufacturing Innovation [Grant 1661732].             Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.2377 .},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Gupta24DebiasingInSamplePolicy.pdf}
}

@article{Kallus23StochasticOptimizationForests,
  title = {Stochastic {{Optimization Forests}}},
  author = {Kallus, Nathan and Mao, Xiaojie},
  year = {2023},
  month = apr,
  journal = {Management Science},
  volume = {69},
  number = {4},
  pages = {1975--1994},
  issn = {0025-1909, 1526-5501},
  doi = {10.1287/mnsc.2022.4458},
  url = {https://pubsonline.informs.org/doi/10.1287/mnsc.2022.4458},
  urldate = {2024-06-02},
  abstract = {We study contextual stochastic optimization problems, where we leverage rich auxiliary observations (e.g., product characteristics) to improve decision making with uncertain variables (e.g., demand). We show how to train forest decision policies for this problem by growing trees that choose splits to directly optimize the downstream decision quality rather than split to improve prediction accuracy as in the standard random forest algorithm. We realize this seemingly computationally intractable problem by developing approximate splitting criteria that use optimization perturbation analysis to eschew burdensome reoptimization for every candidate split, so that our method scales to large-scale problems. We prove that our splitting criteria consistently approximate the true risk and that our method achieves asymptotic optimality. We extensively validate our method empirically, demonstrating the value of optimization-aware construction of forests and the success of our efficient approximations. We show that our approximate splitting criteria can reduce running time hundredfold while achieving performance close to forest algorithms that exactly reoptimize for every candidate split.             This paper was accepted by Hamid Nazerzadeh, data science.             Funding: This work was supported by the National Science Foundation [Grant 1846210].             Supplemental Material: The data files and online appendices are available at https://doi.org/10.1287/mnsc.2022.4458 .},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kallus23StochasticOptimizationForests.pdf}
}

@article{Liu22CoupledLearningEnabled,
  title = {Coupled {{Learning Enabled Stochastic Programming}} with {{Endogenous Uncertainty}}},
  author = {Liu, Junyi and Li, Guangyu and Sen, Suvrajeet},
  year = {2022},
  month = may,
  journal = {Mathematics of Operations Research},
  volume = {47},
  number = {2},
  pages = {1681--1705},
  issn = {0364-765X, 1526-5471},
  doi = {10.1287/moor.2021.1185},
  url = {https://pubsonline.informs.org/doi/10.1287/moor.2021.1185},
  urldate = {2024-06-02},
  abstract = {Predictive analytics, empowered by machine learning, is usually followed by decision-making problems in prescriptive analytics. We extend the previous sequential prediction-optimization paradigm to a coupled scheme such that the prediction model can guide the decision problem to produce coordinated decisions yielding higher levels of performance. Specifically, for stochastic programming (SP) models with latently decision-dependent uncertainty, without any parametric assumption of the latent dependency, we develop a coupled learning enabled optimization (CLEO) algorithm in which the learning step of predicting the local dependency and the optimization step of computing a candidate decision are conducted interactively. The CLEO algorithm automatically balances the exploration and exploitation via the trust region method with active sampling. Under certain assumptions, we show that the sequence of solutions provided by CLEO converges to a directional stationary point of the original nonconvex and nonsmooth SP problem with probability 1. In addition, we present preliminary experimental results which demonstrate the computational potential of this data-driven approach.},
  langid = {english},
  keywords = {todo},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Liu22CoupledLearningEnabled.pdf}
}

@article{Liu23EndtoendLearningUser,
  title = {End-to-End Learning of User Equilibrium with Implicit Neural Networks},
  author = {Liu, Zhichen and Yin, Yafeng and Bai, Fan and Grimm, Donald K.},
  year = {2023},
  journal = {Transportation Research Part C: Emerging Technologies},
  volume = {150},
  pages = {104085},
  publisher = {Elsevier},
  url = {https://www.sciencedirect.com/science/article/pii/S0968090X23000748},
  urldate = {2024-06-02},
  keywords = {todo},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Liu23EndtoendLearningUser.pdf}
}

@article{Sang22elecPriceFrcstDcsnFocus,
  title = {Electricity Price Prediction for Energy Storage System Arbitrage: {{A}} Decision-Focused Approach},
  shorttitle = {Electricity Price Prediction for Energy Storage System Arbitrage},
  author = {Sang, Linwei and Xu, Yinliang and Long, Huan and Hu, Qinran and Sun, Hongbin},
  year = {2022},
  journal = {IEEE Transactions on Smart Grid},
  volume = {13},
  number = {4},
  pages = {2822--2832},
  publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/9755891/},
  urldate = {2024-06-02},
  keywords = {obsLitNote},
  note = {Sang22elecPriceFrcstDcsnFocus
\par
Price forecasts that forecast stuff related to the decisions made by the user. ~Does log-normal distribution target, but this won't work for neg. prices like in AEMO and ERCOT.
\par
Sadana says this is an ILO based forecast/optimization algorithm with a hybrid loss for both prediction and decision errors.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sang22elecPriceFrcstDcsnFocus.pdf}
}

@article{Stratigakos22PrescriptiveTreesIntegrated,
  title = {Prescriptive Trees for Integrated Forecasting and Optimization Applied in Trading of Renewable Energy},
  author = {Stratigakos, Akylas and Camal, Simon and Michiorri, Andrea and Kariniotakis, Georges},
  year = {2022},
  journal = {IEEE Transactions on Power Systems},
  volume = {37},
  number = {6},
  pages = {4696--4708},
  publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/9716858/},
  urldate = {2024-06-02},
  keywords = {todo},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Stratigakos22PrescriptiveTreesIntegrated.pdf}
}

@inproceedings{Sun23MaximumOptimalityMargin,
  title = {Maximum Optimality Margin: {{A}} Unified Approach for Contextual Linear Programming and Inverse Linear Programming},
  shorttitle = {Maximum Optimality Margin},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Sun, Chunlin and Liu, Shang and Li, Xiaocheng},
  year = {2023},
  pages = {32886--32912},
  publisher = {PMLR},
  url = {https://proceedings.mlr.press/v202/sun23e.html},
  urldate = {2024-06-02},
  keywords = {todo},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sun23MaximumOptimalityMargin.pdf}
}

@inproceedings{Vohra23EndtoendLearningMultiple,
  title = {End-to-End Learning with Multiple Modalities for System-Optimised Renewables Nowcasting},
  booktitle = {2023 {{IEEE Belgrade PowerTech}}},
  author = {Vohra, Rushil and Rajaei, Ali and Cremer, Jochen L.},
  year = {2023},
  pages = {1--8},
  publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/10202904/},
  urldate = {2024-06-02},
  keywords = {todo},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Vohra23EndtoendLearningMultiple.pdf}
}

@article{Wahdany23endToEndWindOptSys,
  title = {More than Accuracy: End-to-End Wind Power Forecasting That Optimises the Energy System},
  shorttitle = {More than Accuracy},
  author = {Wahdany, Dariush and Schmitt, Carlo and Cremer, Jochen L.},
  year = {2023},
  journal = {Electric Power Systems Research},
  volume = {221},
  pages = {109384},
  publisher = {Elsevier},
  url = {https://www.sciencedirect.com/science/article/pii/S0378779623002730},
  urldate = {2024-06-02},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wahdany23endToEndWindOptSys.pdf}
}

@article{Li18HyperbandNovelBanditbased,
  title = {Hyperband: {{A}} Novel Bandit-Based Approach to Hyperparameter Optimization},
  shorttitle = {Hyperband},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {185},
  pages = {1--52},
  url = {https://www.jmlr.org/papers/v18/16-558.html},
  urldate = {2024-06-11},
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems. Keywords: hyperparameter optimization, model selection, infinite-armed bandits, online optimization, deep learning},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Li18HyperbandNovelBanditbased.pdf}
}

@misc{Zygmunt17fastHyperband,
  type = {Blog},
  title = {Tuning Hyperparams Fast with {{Hyperband}}},
  author = {Zygmunt, Z},
  year = {2017},
  month = feb,
  journal = {FastML},
  url = {https://fastml.com/tuning-hyperparams-fast-with-hyperband/},
  urldate = {2024-06-13},
  abstract = {Hyperband is a relatively new method for tuning iterative algorithms. It performs random sampling and attempts to gain an edge by using time spent optimizing in the best way. We explain a few things that were not clear to us right away, and try the algorithm in practice.},
  keywords = {obsLitNote},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Zygmunt17fastHyperband.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Zygmunt17fastHyperband.html}
}

@inproceedings{Falkner18hyperparamBOHB,
  title = {{{BOHB}}: {{Robust}} and Efficient Hyperparameter Optimization at Scale},
  shorttitle = {{{BOHB}}},
  booktitle = {International Conference on Machine Learning},
  author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
  year = {2018},
  pages = {1437--1446},
  publisher = {PMLR},
  url = {https://proceedings.mlr.press/v80/falkner18a.html},
  urldate = {2024-06-14},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Falkner18hyperparamBOHB.pdf}
}

@misc{Ohta20hyperbandOptunaImplmt,
  title = {How {{We Implement Hyperband}} in {{Optuna}}},
  author = {Ohta, Takeru},
  year = {2020},
  month = feb,
  journal = {Preferred Networks Research \& Development},
  url = {https://tech.preferred.jp/en/blog/how-we-implement-hyperband-in-optuna/},
  urldate = {2024-06-14},
  abstract = {This is a guest blog from a part time engineer, Masaki Kozuki (@crcrpar). This post requires some familiarity with Optuna and targets those who are rather interested in software for Machine Learning and Deep Learning than ML and DL technologies and/or algorithms themselves. Also, I hope some of you feel like giving Optuna a try, or contributing to Optuna after you read this.},
  langid = {american},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ohta20hyperbandOptunaImplmt.pdf}
}

@inproceedings{Akiba19OptunaHyperparamOpt,
  title = {Optuna: {{A Next-generation Hyperparameter Optimization Framework}}},
  shorttitle = {Optuna},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = {2019},
  month = jul,
  pages = {2623--2631},
  publisher = {ACM},
  address = {Anchorage AK USA},
  doi = {10.1145/3292500.3330701},
  url = {https://dl.acm.org/doi/10.1145/3292500.3330701},
  urldate = {2024-06-14},
  isbn = {978-1-4503-6201-6},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Akiba19OptunaHyperparamOpt.pdf}
}

@misc{Watanabe23TreeStructuredParzenEstimator,
  title = {Tree-{{Structured Parzen Estimator}}: {{Understanding Its Algorithm Components}} and {{Their Roles}} for {{Better Empirical Performance}}},
  shorttitle = {Tree-{{Structured Parzen Estimator}}},
  author = {Watanabe, Shuhei},
  year = {2023},
  month = may,
  number = {arXiv:2304.11127},
  eprint = {2304.11127},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.11127},
  url = {http://arxiv.org/abs/2304.11127},
  urldate = {2024-06-14},
  abstract = {Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.},
  archiveprefix = {arXiv},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Watanabe23undestandTPE.pdf}
}

@article{Bergstra11hyperparmOptTPE,
  title = {Algorithms for Hyper-Parameter Optimization},
  author = {Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  year = {2011},
  journal = {Advances in neural information processing systems},
  volume = {24},
  url = {https://proceedings.neurips.cc/paper/4443-algorithms-for-hyper-parameter-optimization},
  urldate = {2024-06-14},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bergstra11hyperparmOptTPE.pdf}
}

@misc{Falkner18libHpBandSter,
  title = {{{HpBandSter}}: A Distributed Hyperband Implementation on Steroids},
  author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
  year = {2018},
  url = {https://github.com/automl/HpBandSter},
  keywords = {obsLitNote},
  note = {GitHub repository}
}

@article{Lindauer22libSMAC3,
  title = {{{SMAC3}}: A Versatile Bayesian Optimization Package for Hyperparameter Optimization},
  author = {Lindauer, Marius and Eggensperger, Katharina and Feurer, Matthias and Biedenkapp, Andr{\'e} and Deng, Difan and Benjamins, Carolin and Ruhkopf, Tim and Sass, Ren{\'e} and Hutter, Frank},
  year = {2022},
  journal = {Journal of Machine Learning Research},
  volume = {23},
  number = {54},
  pages = {1--9},
  url = {http://jmlr.org/papers/v23/21-0888.html},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lindauer22libSMAC3.pdf}
}

@misc{Eggensperger22HPOBenchCollectionReproducible,
  title = {{{HPOBench}}: {{A Collection}} of {{Reproducible Multi-Fidelity Benchmark Problems}} for {{HPO}}},
  shorttitle = {{{HPOBench}}},
  author = {Eggensperger, Katharina and M{\"u}ller, Philipp and Mallik, Neeratyoy and Feurer, Matthias and Sass, Ren{\'e} and Klein, Aaron and Awad, Noor and Lindauer, Marius and Hutter, Frank},
  year = {2022},
  month = oct,
  number = {arXiv:2109.06716},
  eprint = {2109.06716},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2109.06716},
  urldate = {2024-06-14},
  abstract = {To achieve peak predictive performance, hyperparameter optimization (HPO) is a crucial component of machine learning and its applications. Over the last years, the number of efficient algorithms and tools for HPO grew substantially. At the same time, the community is still lacking realistic, diverse, computationally cheap, and standardized benchmarks. This is especially the case for multi-fidelity HPO methods. To close this gap, we propose HPOBench, which includes 7 existing and 5 new benchmark families, with a total of more than 100 multi-fidelity benchmark problems. HPOBench allows to run this extendable set of multi-fidelity HPO benchmarks in a reproducible way by isolating and packaging the individual benchmarks in containers. It also provides surrogate and tabular benchmarks for computationally affordable yet statistically sound evaluations. To demonstrate HPOBench's broad compatibility with various optimization tools, as well as its usefulness, we conduct an exemplary large-scale study evaluating 13 optimizers from 6 optimization tools. We provide HPOBench here: https://github.com/automl/HPOBench.},
  archiveprefix = {arXiv},
  note = {Comment: Published at NeurIPS Datasets and Benchmarks Track 2021. Updated version},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Eggensperger22hyperparamOptHPOBench.pdf}
}

@misc{Tang23PyEPOPyTorchbasedEndtoEnd,
  title = {{{PyEPO}}: {{A PyTorch-based End-to-End Predict-then-Optimize Library}} for {{Linear}} and {{Integer Programming}}},
  shorttitle = {{{PyEPO}}},
  author = {Tang, Bo and Khalil, Elias B.},
  year = {2023},
  month = apr,
  number = {arXiv:2206.14234},
  eprint = {2206.14234},
  primaryclass = {cs, math},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2206.14234},
  urldate = {2024-06-15},
  abstract = {In deterministic optimization, it is typically assumed that all problem parameters are fixed and known. In practice, however, some parameters may be a priori unknown but can be estimated from historical data. A typical predict-then-optimize approach separates predictions and optimization into two stages. Recently, end-to-end predict-then-optimize has become an attractive alternative. In this work, we present the PyEPO package, a PyTorchbased end-to-end predict-then-optimize library in Python. To the best of our knowledge, PyEPO (pronounced like pineapple with a silent "n") is the first such generic tool for linear and integer programming with predicted objective function coefficients. It provides four base algorithms: a convex surrogate loss function from the seminal work of Elmachtoub and Grigas [16], a differentiable black-box solver approach of Pogancic et al. [35], and two differentiable perturbation-based methods from Berthet et al. [6]. PyEPO provides a simple interface for the definition of new optimization problems, the implementation of state-of-the-art predict-then-optimize training algorithms, the use of custom neural network architectures, and the comparison of end-to-end approaches with the two-stage approach. PyEPO enables us to conduct a comprehensive set of experiments comparing a number of end-to-end and two-stage approaches along axes such as prediction accuracy, decision quality, and running time on problems such as Shortest Path, Multiple Knapsack, and the Traveling Salesperson Problem. We discuss some empirical insights from these experiments, which could guide future research. PyEPO and its documentation are available at https://github.com/khalil-research/PyEPO.},
  archiveprefix = {arXiv},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Tang23end2endPredOptPyEPO.pdf}
}

@book{Robinson13eatingWildSide,
  title = {Eating on the Wild Side: {{The}} Missing Link to Optimum Health},
  author = {Robinson, Jo},
  year = {2013},
  publisher = {{Little, Brown and Company}},
  address = {New York, NY, USA},
  isbn = {978-0-316-22794-0},
  keywords = {obsLitNote}
}

@article{Zimmer24thinkNeedLanguage,
  title = {Do {{We Need Language}} to {{Think}}?},
  author = {Zimmer, Carl},
  year = {2024},
  month = jun,
  journal = {The New York Times},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2024/06/19/science/brain-language-thought.html},
  urldate = {2024-06-19},
  abstract = {A group of neuroscientists argue that our words are primarily for communicating, not for reasoning.},
  chapter = {Science},
  langid = {american},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zimmer24thinkNeedLanguage.pdf}
}

@misc{Khalil23predictThenOptimize,
  title = {Predict-Then-Optimize: {{A}} Tour of the State-of-the-Art Using {{PyEPO}}},
  author = {Khalil, Elias},
  year = {2023},
  month = aug,
  publisher = {YouTube},
  address = {ACP Summer School 2023},
  url = {https://www.youtube.com/watch?v=pZqm-i57gxk},
  urldate = {2024-06-21},
  abstract = {In deterministic optimization, it is typically assumed that all parameters of the problem are fixed and known. In practice, however, some parameters may be a priori unknown but can be estimated from historical data. A typical predict-then-optimize approach separates predictions and optimization into two stages. Recently, end-to-end predict-then-optimize has become an attractive alternative. In this talk, I will provide an overview of the predict-then-optimize problem as well as a suite of end-to-end learning methods that have been developed independently in the operations research and machine learning communities. To ground the discussion, I will use the PyEPO package (https://github.com/khalil-research/PyEPO/), a PyTorch-based end-to-end predict-then-optimize library in Python. PyEPO (pronounced like 'pineapple' with a silent 'n') is the first generic tool for linear and integer programming with predicted objective function coefficients. It provides four base algorithms: a convex surrogate loss function from the seminal work of Elmachtoub \& Grigas (2021), the differentiable black-box solver approach of Vlastelica et al. (2019), and two differentiable perturbation-based methods from Berthet et al. (2020). PyEPO provides a simple interface for the definition of new optimization problems, the implementation of state-of-the-art predict-then-optimize training algorithms, the use of custom neural network architectures, and the comparison of end-to-end approaches with the two-stage approach. PyEPO enables the first comprehensive comparison of these methods on problems such as Shortest Path, Multiple Knapsack, and the Traveling Salesperson Problem, including on an image dataset. The empirical insights therein could guide future research. Joint work with Ph.D. student Bo Tang at the University of Toronto},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Khalil23predictThenOptimize.pdf}
}

@incollection{Hu23branchLrnUnkParamCnstrnts,
  title = {Branch \& {{Learn}} with {{Post-hoc Correction}} for {{Predict}}+{{Optimize}} with {{Unknown Parameters}} in {{Constraints}}},
  booktitle = {Integration of {{Constraint Programming}}, {{Artificial Intelligence}}, and {{Operations Research}}},
  author = {Hu, Xinyi and Lee, Jasper C. H. and Lee, Jimmy H. M.},
  editor = {Cire, Andre A.},
  year = {2023},
  volume = {13884},
  pages = {264--280},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-33271-5_18},
  url = {https://link.springer.com/10.1007/978-3-031-33271-5_18},
  urldate = {2024-06-25},
  isbn = {978-3-031-33270-8 978-3-031-33271-5},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Hu23branchLrnUnkParamCnstrnts.pdf}
}

@inproceedings{Tang24fastPredictConeBinOptCaVE,
  title = {{{CaVE}}: {{A Cone-Aligned Approach}} for~{{Fast Predict-then-optimize}} with~{{Binary Linear Programs}}},
  shorttitle = {{{CaVE}}},
  booktitle = {Integration of {{Constraint Programming}}, {{Artificial Intelligence}}, and {{Operations Research}}},
  author = {Tang, Bo and Khalil, Elias B.},
  editor = {Dilkina, Bistra},
  year = {2024},
  pages = {193--210},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-60599-4_12},
  abstract = {The end-to-end predict-then-optimize framework, also known as decision-focused learning, has gained popularity for its ability to integrate optimization into the training procedure of machine learning models that predict the unknown cost (objective function) coefficients of optimization problems from contextual instance information. Naturally, most of the problems of interest in this space can be cast as integer linear programs. In this work, we focus on binary linear programs (BLPs) and propose a new end-to-end training method to predict-then-optimize. Our method, Cone-aligned Vector Estimation (CaVE), aligns the predicted cost vectors with the normal cone corresponding to the true optimal solution of a training instance. When the predicted cost vector lies inside the cone, the optimal solution to the linear relaxation of the binary problem is optimal. This alignment not only produces decision-aware learning models, but also dramatically reduces training time as it circumvents the need to solve BLPs to compute a loss function with its gradients. Experiments across multiple datasets show that our method exhibits a favorable trade-off between training time and solution quality, particularly with large-scale optimization problems such as vehicle routing, a hard BLP that has yet to benefit from predict-then-optimize methods in the literature due to its difficulty.},
  isbn = {978-3-031-60599-4},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Tang24fastPredictConeBinOptCaVE.pdf}
}

@inproceedings{Mandi22DecisionFocusLearnToRank,
  title = {Decision-Focused Learning: {{Through}} the Lens of Learning to Rank},
  shorttitle = {Decision-Focused Learning},
  booktitle = {International Conference on Machine Learning},
  author = {Mandi, Jayanta and Bucarey, V{\i}ctor and Tchomba, Maxime Mulamba Ke and Guns, Tias},
  year = {2022},
  pages = {14935--14947},
  publisher = {PMLR},
  url = {https://proceedings.mlr.press/v162/mandi22a.html?ref=https://githubhelp.com},
  urldate = {2024-06-25},
  keywords = {obsLitNote},
  file = {/Users/scott/Library/CloudStorage/OneDrive-Personal/share/ref/zotero/Mandi22DecisionFocusLearnToRank.pdf}
}

@misc{Mulamba21ContrastiveLossesSolution,
  title = {Contrastive {{Losses}} and {{Solution Caching}} for {{Predict-and-Optimize}}},
  author = {Mulamba, Maxime and Mandi, Jayanta and Diligenti, Michelangelo and Lombardi, Michele and Bucarey, Victor and Guns, Tias},
  year = {2021},
  month = jul,
  number = {arXiv:2011.05354},
  eprint = {2011.05354},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2011.05354},
  urldate = {2024-06-25},
  abstract = {Many decision-making processes involve solving a combinatorial optimization problem with uncertain input that can be estimated from historic data. Recently, problems in this class have been successfully addressed via end-to-end learning approaches, which rely on solving one optimization problem for each training instance at every epoch. In this context, we provide two distinct contributions. First, we use a Noise Contrastive approach to motivate a family of surrogate loss functions, based on viewing non-optimal solutions as negative examples. Second, we address a major bottleneck of all predict-and-optimize approaches, i.e. the need to frequently recompute optimal solutions at training time. This is done via a solver-agnostic solution caching scheme, and by replacing optimization calls with a lookup in the solution cache. The method is formally based on an inner approximation of the feasible space and, combined with a cache lookup strategy, provides a controllable trade-off between training time and accuracy of the loss approximation. We empirically show that even a very slow growth rate is enough to match the quality of state-of-the-art methods, at a fraction of the computational cost.},
  archiveprefix = {arXiv},
  note = {Comment: Accepted at IJCAI2021},
  file = {/Users/scott/Library/CloudStorage/OneDrive-Personal/share/ref/zotero/Mulamba21contrastiveLossPredAndOpt.pdf}
}

@article{Valdes22repGradBoostBackprop,
  title = {Representational {{Gradient Boosting}}: {{Backpropagation}} in the {{Space}} of {{Functions}}},
  shorttitle = {Representational {{Gradient Boosting}}},
  author = {Valdes, Gilmer and Friedman, Jerome H. and Jiang, Fei and Gennatas, Efstathios D.},
  year = {2022},
  month = dec,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {12},
  pages = {10186--10195},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3137715},
  url = {https://ieeexplore.ieee.org/document/9661330},
  urldate = {2024-06-25},
  abstract = {The estimation of nested functions (i.e., functions of functions) is one of the central reasons for the success and popularity of machine learning. Today, artificial neural networks are the predominant class of algorithms in this area, known as representational learning. Here, we introduce Representational Gradient Boosting (RGB), a nonparametric algorithm that estimates functions with multi-layer architectures obtained using backpropagation in the space of functions. RGB does not need to assume a functional form in the nodes or output (e.g., linear models or rectified linear units), but rather estimates these transformations. RGB can be seen as an optimized stacking procedure where a meta algorithm learns how to combine different classes of functions (e.g., Neural Networks (NN) and Gradient Boosting (GB)), while building and optimizing them jointly in an attempt to compensate each other's weaknesses. This highlights a stark difference with current approaches to meta-learning that combine models only after they have been built independently. We showed that providing optimized stacking is one of the main advantages of RGB over current approaches. Additionally, due to the nested nature of RGB we also showed how it improves over GB in problems that have several high-order interactions. Finally, we investigate both theoretically and in practice the problem of recovering nested functions and the value of prior knowledge.},
  keywords = {obsLitNote},
  file = {/Users/scott/Library/CloudStorage/OneDrive-Personal/share/ref/zotero/Valdes22repGradBoostBackprop.pdf}
}

@misc{Zhang21LearningMultiLayeredGBDT,
  title = {Learning {{Multi-Layered GBDT Via Back Propagation}}},
  author = {Zhang, Zhendong},
  year = {2021},
  month = sep,
  number = {arXiv:2109.11863},
  eprint = {2109.11863},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2109.11863},
  urldate = {2024-06-25},
  abstract = {Deep neural networks are able to learn multi-layered representation via back propagation (BP). Although the gradient boosting decision tree (GBDT) is effective for modeling tabular data, it is non-differentiable with respect to its input, thus suffering from learning multi-layered representation. In this paper, we propose a framework of learning multi-layered GBDT via BP. We approximate the gradient of GBDT based on linear regression. Specifically, we use linear regression to replace the constant value at each leaf ignoring the contribution of individual samples to the tree structure. In this way, we estimate the gradient for intermediate representations, which facilitates BP for multi-layered GBDT. Experiments show the effectiveness of the proposed method in terms of performance and representation ability. To the best of our knowledge, this is the first work of optimizing multi-layered GBDT via BP. This work provides a new possibility of exploring deep tree based learning and combining GBDT with neural networks.},
  archiveprefix = {arXiv},
  keywords = {todo},
  file = {/Users/scott/Library/CloudStorage/OneDrive-Personal/share/ref/zotero/Zhang21LrnMultiLayerBDTbackprop.pdf}
}

@misc{OConnor24OptimizingQuantilebasedTrading,
  title = {Optimizing {{Quantile-based Trading Strategies}} in {{Electricity Arbitrage}}},
  author = {O'Connor, Ciaran and Collins, Joseph and Prestwich, Steven and Visentin, Andrea},
  year = {2024},
  month = jun,
  number = {arXiv:2406.13851},
  eprint = {2406.13851},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.13851},
  url = {http://arxiv.org/abs/2406.13851},
  urldate = {2024-06-25},
  abstract = {Efficiently integrating renewable resources into electricity markets is vital for addressing the challenges of matching real-time supply and demand while reducing the significant energy wastage resulting from curtailments. To address this challenge effectively, the incorporation of storage devices can enhance the reliability and efficiency of the grid, improving market liquidity and reducing price volatility. In short-term electricity markets, participants navigate numerous options, each presenting unique challenges and opportunities, underscoring the critical role of the trading strategy in maximizing profits. This study delves into the optimization of day-ahead and balancing market trading, leveraging quantile-based forecasts. Employing three trading approaches with practical constraints, our research enhances forecast assessment, increases trading frequency, and employs flexible timestamp orders. Our findings underscore the profit potential of simultaneous participation in both day-ahead and balancing markets, especially with larger battery storage systems; despite increased costs and narrower profit margins associated with higher-volume trading, the implementation of high-frequency strategies plays a significant role in maximizing profits and addressing market challenges. Finally, we modelled four commercial battery storage systems and evaluated their economic viability through a scenario analysis, with larger batteries showing a shorter return on investment.},
  archiveprefix = {arXiv},
  file = {/Users/scott/Library/CloudStorage/OneDrive-Personal/share/ref/zotero/OConnor24quantileFrcstElecTradeOpt.pdf}
}

@article{Elmachtoub22SmartPredThenOptSPO+,
  title = {Smart ``{{Predict}}, Then {{Optimize}}''},
  author = {Elmachtoub, Adam N. and Grigas, Paul},
  year = {2022},
  month = jan,
  journal = {Management Science},
  volume = {68},
  number = {1},
  pages = {9--26},
  issn = {0025-1909, 1526-5501},
  doi = {10.1287/mnsc.2020.3922},
  url = {https://pubsonline.informs.org/doi/10.1287/mnsc.2020.3922},
  urldate = {2024-06-25},
  abstract = {Many real-world analytics problems involve two significant challenges: prediction and optimization. Because of the typically complex nature of each challenge, the standard paradigm is predict-then-optimize. By and large, machine learning tools are intended to minimize prediction error and do not account for how the predictions will be used in the downstream optimization problem. In contrast, we propose a new and very general framework, called Smart ``Predict, then Optimize'' (SPO), which directly leverages the optimization problem structure---that is, its objective and constraints---for designing better prediction models. A key component of our framework is the SPO loss function, which measures the decision error induced by a prediction. Training a prediction model with respect to the SPO loss is computationally challenging, and, thus, we derive, using duality theory, a convex surrogate loss function, which we call the SPO+ loss. Most importantly, we prove that the SPO+ loss is statistically consistent with respect to the SPO loss under mild conditions. Our SPO+ loss function can tractably handle any polyhedral, convex, or even mixed-integer optimization problem with a linear objective. Numerical experiments on shortest-path and portfolio-optimization problems show that the SPO framework can lead to significant improvement under the predict-then-optimize paradigm, in particular, when the prediction model being trained is misspecified. We find that linear models trained using SPO+ loss tend to dominate random-forest algorithms, even when the ground truth is highly nonlinear.             This paper was accepted by Yinyu Ye, optimization.             Supplemental Material: Data and the online appendix are available at https://doi.org/10.1287/mnsc.2020.3922},
  langid = {english},
  keywords = {obsLitNote},
  file = {/Users/scott/Library/CloudStorage/OneDrive-Personal/share/ref/zotero/Elmachtoub22SmartPredThenOptSPO+.pdf}
}

@article{Berthet20LrnDiffPerturbOptPFYL,
  title = {Learning with Differentiable Pertubed Optimizers},
  author = {Berthet, Quentin and Blondel, Mathieu and Teboul, Olivier and Cuturi, Marco and Vert, Jean-Philippe and Bach, Francis},
  year = {2020},
  journal = {Advances in neural information processing systems},
  volume = {33},
  pages = {9508--9519},
  url = {https://proceedings.neurips.cc/paper/2020/hash/6bb56208f672af0dd65451f869fedfd9-Abstract.html},
  urldate = {2024-06-26},
  keywords = {obsLitNote},
  file = {/Users/scott/Library/CloudStorage/OneDrive-Personal/share/ref/zotero/Berthet20LrnDiffPerturbOptPFYL.pdf}
}

@misc{23DailyMultivitaminMay,
  title = {Daily Multivitamin May Enhance Memory in Older Adults},
  year = {2023},
  month = jun,
  journal = {National Institutes of Health (NIH)},
  url = {https://www.nih.gov/news-events/nih-research-matters/daily-multivitamin-may-enhance-memory-older-adults},
  urldate = {2024-06-28},
  abstract = {A large clinical trial found that daily multivitamins led to modest improvements in memory over a three-year period in older adults.},
  langid = {english}
}

@misc{Tan24AreLanguageModels,
  title = {Are {{Language Models Actually Useful}} for {{Time Series Forecasting}}?},
  author = {Tan, Mingtian and Merrill, Mike A. and Gupta, Vinayak and Althoff, Tim and Hartvigsen, Thomas},
  year = {2024},
  month = jun,
  number = {arXiv:2406.16964},
  eprint = {2406.16964},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.16964},
  url = {http://arxiv.org/abs/2406.16964},
  urldate = {2024-07-01},
  abstract = {Large language models (LLMs) are being applied to time series tasks, particularly time series forecasting. However, are language models actually useful for time series? After a series of ablation studies on three recent and popular LLM-based time series forecasting methods, we find that removing the LLM component or replacing it with a basic attention layer does not degrade the forecasting results -- in most cases the results even improved. We also find that despite their significant computational cost, pretrained LLMs do no better than models trained from scratch, do not represent the sequential dependencies in time series, and do not assist in few-shot settings. Additionally, we explore time series encoders and reveal that patching and attention structures perform similarly to state-of-the-art LLM-based forecasters.},
  archiveprefix = {arXiv},
  note = {Comment: 25 pages, 8 figures and 20 tables},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Tan24llmUseTSfrcst.pdf}
}

@inproceedings{Salwala22distribIncrmntlMLbigTS,
  title = {Distributed {{Incremental Machine Learning}} for {{Big Time Series Data}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Salwala, Dhaval and Tirupathi, Seshu and Quanz, Brian and Gifford, Wesley M. and Siegel, Stuart and Ekambaram, Vijay and Jati, Arindam},
  year = {2022},
  month = dec,
  pages = {2356--2363},
  doi = {10.1109/BigData55660.2022.10020361},
  url = {https://ieeexplore.ieee.org/document/10020361},
  urldate = {2024-07-09},
  abstract = {Today's highly instrumented systems generate large amounts of time series data from many different domains. In order to create meaningful insights from these data, techniques are needed to handle the collection, processing, and analysis at scale. The high frequency and volume of data that is generated introduces several challenges including data transformation, managing concept drift, the operational cost of model re-training and tracking, and scaling hyperparameter optimization.Incremental machine learning can provide a viable solution to handle these kinds of data. Further, distributed machine learning can be an efficient technique to improve performance, increase accuracy, and scale to larger input sizes.In this paper, we introduce a framework that combines the computational capabilities of Apache Spark and the workflow parallelization of Ray for distributed incremental learning. We conduct an empirical analysis of our framework for time series forecasting using the Walmart M5 dataset. The system can perform a parameter search on streaming data with concept drift producing a robust pipeline that fits high-volume data effectively. The results are encouraging and substantiate system proficiency over traditional big data analysis approaches that exclusively use either offline or online training.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Salwala22distribIncrmntlMLbigTS.pdf}
}

@article{Sidogi23SigTransfrmStockLOB,
  title = {A {{Signature Transform}} of {{Limit Order Book Data}} for {{Stock Price Prediction}}},
  author = {Sidogi, Thendo and Mongwe, Wilson Tsakane and Mbuvha, Rendani and Olukanmi, Peter and Marwala, Tshilidzi},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {70598--70609},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3293064},
  url = {https://ieeexplore.ieee.org/abstract/document/10175387},
  urldate = {2024-07-10},
  abstract = {A novel approach is presented for predicting the mean-mid stock price by utilizing high-frequency and complex limit order book (LOB) data as inputs for machine learning algorithms. Specifically, the proposed approach uses rough path theory to extract signature path features from the LOB data and compresses them for training machine learning models. We compare the performance of this approach to standard autoregression methods and demonstrate its superior performance in terms of model prediction error. In addition, we use deep neural networks (DNN) and random forest (RF) to further test the proposed approach's performance compared to standard approaches using raw LOB data. The findings show that Sig-DNN, DNN with signature features, outperforms DNN with raw LOB data in terms of prediction error and efficiency, while Sig-RF, RF with signature features, underperformed RF with raw LOB data in terms of prediction but not in efficiency. We evaluate the proposed approach on multiple exchanges, including the Johannesburg and New York Stock Exchanges, and the results demonstrate better prediction error and efficiency performance in developed markets compared to emerging markets. Overall, the study highlights the potential of using signature path features with machine learning techniques for predicting the mean-mid stock price.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sidogi23SigTransfrmStockLOB.pdf}
}

@misc{Loning19SktimeUnifiedInterface,
  title = {Sktime: {{A Unified Interface}} for {{Machine Learning}} with {{Time Series}}},
  shorttitle = {Sktime},
  author = {L{\"o}ning, Markus and Bagnall, Anthony and Ganesh, Sajaysurya and Kazakov, Viktor and Lines, Jason and Kir{\'a}ly, Franz J.},
  year = {2019},
  month = sep,
  number = {arXiv:1909.07872},
  eprint = {1909.07872},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1909.07872},
  urldate = {2024-07-10},
  abstract = {We present sktime -- a new scikit-learn compatible Python library with a unified interface for machine learning with time series. Time series data gives rise to various distinct but closely related learning tasks, such as forecasting and time series classification, many of which can be solved by reducing them to related simpler tasks. We discuss the main rationale for creating a unified interface, including reduction, as well as the design of sktime's core API, supported by a clear overview of common time series tasks and reduction approaches.},
  archiveprefix = {arXiv},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Loning19unifMLinterfaceTSsktime.pdf}
}

@misc{Akyildirim22ApplicationsSignatureMethods,
  title = {Applications of {{Signature Methods}} to {{Market Anomaly Detection}}},
  author = {Akyildirim, Erdinc and Gambara, Matteo and Teichmann, Josef and Zhou, Syang},
  year = {2022},
  month = feb,
  number = {arXiv:2201.02441},
  eprint = {2201.02441},
  primaryclass = {cs, q-fin, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.02441},
  url = {http://arxiv.org/abs/2201.02441},
  urldate = {2024-07-10},
  abstract = {Anomaly detection is the process of identifying abnormal instances or events in data sets which deviate from the norm significantly. In this study, we propose a signatures based machine learning algorithm to detect rare or unexpected items in a given data set of time series type. We present applications of signature or randomized signature as feature extractors for anomaly detection algorithms; additionally we provide an easy, representation theoretic justification for the construction of randomized signatures. Our first application is based on synthetic data and aims at distinguishing between real and fake trajectories of stock prices, which are indistinguishable by visual inspection. We also show a real life application by using transaction data from the cryptocurrency market. In this case, we are able to identify pump and dump attempts organized on social networks with F1 scores up to 88\% by means of our unsupervised learning algorithm, thus achieving results that are close to the state-of-the-art in the field based on supervised learning.},
  archiveprefix = {arXiv},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Akyildirim22sigMethodsMktAnmly.pdf}
}

@misc{Buhler20DatadrivenMarketSimulator,
  title = {A {{Data-driven Market Simulator}} for {{Small Data Environments}}},
  author = {B{\"u}hler, Hans and Horvath, Blanka and Lyons, Terry and Arribas, Imanol Perez and Wood, Ben},
  year = {2020},
  month = jun,
  number = {arXiv:2006.14498},
  eprint = {2006.14498},
  primaryclass = {cs, q-fin, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.14498},
  url = {http://arxiv.org/abs/2006.14498},
  urldate = {2024-07-10},
  abstract = {Neural network based data-driven market simulation unveils a new and flexible way of modelling financial time series without imposing assumptions on the underlying stochastic dynamics. Though in this sense generative market simulation is model-free, the concrete modelling choices are nevertheless decisive for the features of the simulated paths. We give a brief overview of currently used generative modelling approaches and performance evaluation metrics for financial time series, and address some of the challenges to achieve good results in the latter. We also contrast some classical approaches of market simulation with simulation based on generative modelling and highlight some advantages and pitfalls of the new approach. While most generative models tend to rely on large amounts of training data, we present here a generative model that works reliably in environments where the amount of available training data is notoriously small. Furthermore, we show how a rough paths perspective combined with a parsimonious Variational Autoencoder framework provides a powerful way for encoding and evaluating financial time series in such environments where available training data is scarce. Finally, we also propose a suitable performance evaluation metric for financial time series and discuss some connections of our Market Generator to deep hedging.},
  archiveprefix = {arXiv},
  note = {Buhler20aatadrivenMktSimSigTransfrm
\par
Comment: 27 pages, 9 figures},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Buhler20aatadrivenMktSimSigTransfrm.pdf}
}

@misc{Pattnaik20LightGBMautoRgrsSktime,
  title = {A {{LightGBM Autoregressor}} --- {{Using Sktime}}},
  author = {Pattnaik, Satya},
  year = {2020},
  month = dec,
  journal = {Medium},
  url = {https://towardsdatascience.com/a-lightgbm-autoregressor-using-sktime-6402726e0e7b},
  urldate = {2024-07-10},
  abstract = {Sklearn for Time Series!},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Pattnaik20LightGBMautoRgrsSktime.pdf}
}

@misc{Morrill21GeneralisedSignatureMethod,
  title = {A {{Generalised Signature Method}} for {{Multivariate Time Series Feature Extraction}}},
  author = {Morrill, James and Fermanian, Adeline and Kidger, Patrick and Lyons, Terry},
  year = {2021},
  month = feb,
  number = {arXiv:2006.00873},
  eprint = {2006.00873},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.00873},
  url = {http://arxiv.org/abs/2006.00873},
  urldate = {2024-07-10},
  abstract = {The 'signature method' refers to a collection of feature extraction techniques for multivariate time series, derived from the theory of controlled differential equations. There is a great deal of flexibility as to how this method can be applied. On the one hand, this flexibility allows the method to be tailored to specific problems, but on the other hand, can make precise application challenging. This paper makes two contributions. First, the variations on the signature method are unified into a general approach, the {\textbackslash}emph\{generalised signature method\}, of which previous variations are special cases. A primary aim of this unifying framework is to make the signature method more accessible to any machine learning practitioner, whereas it is now mostly used by specialists. Second, and within this framework, we derive a canonical collection of choices that provide a domain-agnostic starting point. We derive these choices as a result of an extensive empirical study on 26 datasets and go on to show competitive performance against current benchmarks for multivariate time series classification. Finally, to ease practical application, we make our techniques available as part of the open-source [redacted] project.},
  archiveprefix = {arXiv},
  note = {Comment: 25 pages},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Morrill21GenSignatureTSfeatExtract.pdf}
}

@incollection{Liu23conceptDriftGlobalTSfrcst,
  title = {Handling {{Concept Drift}} in {{Global Time Series Forecasting}}},
  booktitle = {Forecasting with {{Artificial Intelligence}}},
  author = {Liu, Ziyi and Godahewa, Rakshitha and Bandara, Kasun and Bergmeir, Christoph},
  editor = {Hamoudia, Mohsen and Makridakis, Spyros and Spiliotis, Evangelos},
  year = {2023},
  pages = {163--189},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-35879-1_7},
  url = {https://link.springer.com/10.1007/978-3-031-35879-1_7},
  urldate = {2024-07-12},
  abstract = {Machine learning (ML) based time series forecasting models often require and assume certain degrees of stationarity in the data when producing forecasts. However, in many real-world situations, the data distributions are not stationary and they can change over time while reducing the accuracy of the forecasting models, which in the ML literature is known as concept drift. Handling concept drift in forecasting is essential for many ML methods in use nowadays, however, the prior work only proposes methods to handle concept drift in the classification domain. To fill this gap, we explore concept drift handling methods in particular for Global Forecasting Models (GFM) which recently have gained popularity in the forecasting domain. We propose two new concept drift handling methods, namely: Error Contribution Weighting (ECW) and Gradient Descent Weighting (GDW), based on a continuous adaptive weighting concept. These methods use two forecasting models which are separately trained with the most recent series and all series, and finally, the weighted average of the forecasts provided by the two models are considered as the final forecasts. Using LightGBM as the underlying base learner, in our evaluation on three simulated datasets, the proposed models achieve significantly higher accuracy},
  isbn = {978-3-031-35878-4 978-3-031-35879-1},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Liu23conceptDriftGlobalTSfrcst.pdf}
}

@misc{Piao24FredformerFrequencyDebiased,
  title = {Fredformer: {{Frequency Debiased Transformer}} for {{Time Series Forecasting}}},
  shorttitle = {Fredformer},
  author = {Piao, Xihao and Chen, Zheng and Murayama, Taichi and Matsubara, Yasuko and Sakurai, Yasushi},
  year = {2024},
  month = jul,
  eprint = {2406.09009},
  primaryclass = {cs},
  doi = {10.1145/3637528.3671928},
  url = {http://arxiv.org/abs/2406.09009},
  urldate = {2024-07-12},
  abstract = {The Transformer model has shown leading performance in time series forecasting. Nevertheless, in some complex scenarios, it tends to learn low-frequency features in the data and overlook high-frequency features, showing a frequency bias. This bias prevents the model from accurately capturing important high-frequency data features. In this paper, we undertook empirical analyses to understand this bias and discovered that frequency bias results from the model disproportionately focusing on frequency features with higher energy. Based on our analysis, we formulate this bias and propose Fredformer, a Transformer-based framework designed to mitigate frequency bias by learning features equally across different frequency bands. This approach prevents the model from overlooking lower amplitude features important for accurate forecasting. Extensive experiments show the effectiveness of our proposed approach, which can outperform other baselines in different real-world time-series datasets. Furthermore, we introduce a lightweight variant of the Fredformer with an attention matrix approximation, which achieves comparable performance but with much fewer parameters and lower computation costs. The code is available at: https://github.com/chenzRG/Fredformer},
  archiveprefix = {arXiv},
  note = {Piao24transFrmrFreqBiasFredFormer
\par
Standard transformers are biased towards captuing low freq signals; the reason is that they have high energy. ~This model aims to fix that. ~Has code, and a lighter weight version of this algorithm.
\par
Comment: This paper has been accepted by SIGKDD2024},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Piao24transFrmrFreqBiasFredFormer.pdf}
}

@misc{Bamania24graphDeepLrnKANboost,
  title = {Kolmogorov-{{Arnold Networks}} ({{KANs}}) {{Are Being Used To Boost Graph Deep Learning Like Never Before}}},
  author = {Bamania, Dr Ashish},
  year = {2024},
  month = jun,
  journal = {Medium},
  url = {https://levelup.gitconnected.com/kolmogorov-arnold-networks-kans-are-being-used-to-boost-graph-deep-learning-like-never-before-2d39fec7dfc3},
  urldate = {2024-07-13},
  abstract = {A deep dive into how Graph Kolmogorov-Arnold Networks (GKANs) are improving Graph Deep Learning to surpass traditional approaches},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bamania24graphDeepLrnKANboost.pdf}
}

@misc{OSullivan24demandFrcstCausalGraph,
  title = {Safeguarding {{Demand Forecasting}} with {{Causal Graphs}}},
  author = {O'Sullivan, Ryan},
  year = {2024},
  month = jun,
  journal = {Medium},
  url = {https://towardsdatascience.com/safeguarding-demand-forecasting-with-causal-graphs-591511fc8e0e},
  urldate = {2024-07-13},
  abstract = {Causal AI, exploring the integration of causal reasoning into machine learning},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\OSullivan24demandFrcstCausalGraph.pdf}
}

@article{Chen24multiConceptDriftVrblSlideWin,
  title = {Multi-Type Concept Drift Detection under a Dual-Layer Variable Sliding Window in Frequent Pattern Mining with Cloud Computing},
  author = {Chen, Jing and Yang, Shengyi and Gao, Ting and Ying, Yue and Li, Tian and Li, Peng},
  year = {2024},
  month = feb,
  journal = {Journal of Cloud Computing},
  volume = {13},
  number = {1},
  pages = {40},
  issn = {2192-113X},
  doi = {10.1186/s13677-023-00566-9},
  url = {https://doi.org/10.1186/s13677-023-00566-9},
  urldate = {2024-07-16},
  abstract = {The detection of different types of concept drift has wide applications in the fields of cloud computing and security information detection. Concept drift detection can indeed assist in promptly identifying instances where model performance deteriorates or when there are changes in data distribution. This paper focuses on the problem of concept drift detection in order to conduct frequent pattern mining. To address the limitation of fixed sliding windows in adapting to evolving data streams, we propose a variable sliding window frequent pattern mining algorithm, which dynamically adjusts the window size to adapt to new concept drifts and detect them in a timely manner. Furthermore, considering the challenge of existing concept drift detection algorithms that struggle to adapt to different types of drifting data simultaneously, we introduce an additional dual-layer embedded variable sliding window. This approach helps differentiate types of concept drift and incorporates a decay model for drift adaptation. The proposed algorithm can effectively detect different types of concept drift in data streams, perform targeted drift adaptation, and exhibit efficiency in terms of time complexity and memory consumption. Additionally, the algorithm maintains stable performance, avoiding abrupt changes due to window size variations and ensuring overall robustness.},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Chen24multiConceptDriftVrblSlideWin.pdf}
}

@inproceedings{Maduskar23unsuperDriftDetUDDT,
  title = {{{UDDT}}: {{An Unsupervised Drift Detection Method}} for {{Industrial Time Series Data}}},
  shorttitle = {{{UDDT}}},
  booktitle = {2023 {{IEEE}} 2nd {{Industrial Electronics Society Annual On-Line Conference}} ({{ONCON}})},
  author = {Maduskar, Deepti and Sharma, Divyasheel and Chandrika, K. R. and Borrison, Reuben and Manca, Gianluca and Dix, Marcel},
  year = {2023},
  pages = {1--6},
  publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/10431133/},
  urldate = {2024-07-16},
  abstract = {Industrial ML models are primarily data-driven. Therefore, one of the main focus for monitoring the model should be towards identifying the drifts in the data that might affect the performance of the model. The traditional drift detecting methods are usually based on some assumptions related to the underlying data such as no inter-dependence. However industrial sensor data typically consists of time series data, which is collected at regular intervals. Therefore, detecting drift in dependent data where the current readings depend on the previously registered readings demands a different approach. Existing solutions require either the ground truth, a fixed size, or the underlying model details. We propose an Unsupervised Drift Detection method for industrial Time series data or UDDT, a generic approach with no such pre-requisites. In our approach, we can check whether two series belong to the same model. Apart from detecting the drift in the two series, it can also provide the rationale behind the observed drift, i.e., whether the drift is due to a difference in stationarity, correlation structures, or noise distributions. We evaluate the UDDT on two datasets to demonstrate its correctness and the trust regions under various circumstances. We also establish its applicability for the real industrial setting. Keywords---Drift Detection, Distribution Shift, Time Series, Inter-dependence, Stationarity, Correlation Structure, Noise},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Maduskar23unsuperDriftDetUDDT.pdf}
}

@misc{Dancker24dontNeedDprLrnTSfrcst,
  title = {Why {{You}} ({{Currently}}) {{Do Not Need Deep Learning}} for {{Time Series Forecasting}}},
  author = {Dancker, Jonte},
  year = {2024},
  month = jun,
  journal = {Medium},
  url = {https://towardsdatascience.com/why-you-currently-do-not-need-deep-learning-for-time-series-forecasting-0de57f2bc0ed},
  urldate = {2024-07-18},
  abstract = {What you need instead: Learnings from the Makridakis M5 competitions and the 2023 Kaggle AI report},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Dancker24dontNeedDprLrnTSfrcst.pdf}
}

@article{Okabe24virtNodeGraphNNphonon,
  title = {Virtual Node Graph Neural Network for Full Phonon Prediction},
  author = {Okabe, Ryotaro and Chotrattanapituk, Abhijatmedhi and Boonkird, Artittaya and Andrejevic, Nina and Fu, Xiang and Jaakkola, Tommi S. and Song, Qichen and Nguyen, Thanh and Drucker, Nathan and Mu, Sai},
  year = {2024},
  journal = {Nature Computational Science},
  pages = {1--10},
  publisher = {Nature Publishing Group US New York},
  url = {https://www.nature.com/articles/s43588-024-00661-0},
  urldate = {2024-07-21},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Okabe24virtNodeGraphNNphonon.pdf}
}

@misc{Zewe24AthermalPredVgraphNN,
  title = {{{AI}} Method Radically Speeds Predictions of Materials' Thermal Properties},
  author = {Zewe, Adam},
  year = {2024},
  month = jul,
  journal = {MIT News},
  url = {https://news.mit.edu/2024/ai-method-radically-speeds-predictions-materials-thermal-properties-0716},
  urldate = {2024-07-21},
  abstract = {Researchers developed a machine-learning framework that can predict a key property of heat dispersion in materials that is up to 1,000 times faster than other AI methods, and could enable scientists to improve the efficiency of power generation systems and microelectronics.},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zewe24AthermalPredVgraphNN.pdf}
}

@article{Nature24boostGraphNNvirtNode,
  title = {Boosting Graph Neural Networks with Virtual Nodes to Predict Phonon Properties},
  year = {2024},
  month = jul,
  journal = {Nature Computational Science},
  pages = {1--2},
  publisher = {Nature Publishing Group},
  issn = {2662-8457},
  doi = {10.1038/s43588-024-00665-w},
  url = {https://www.nature.com/articles/s43588-024-00665-w},
  urldate = {2024-07-21},
  abstract = {A graph neural network using virtual nodes is proposed to predict the properties of complex materials with variable dimensions or dimensions that depend on the input. The method is used to accurately and quickly predict phonon dispersion relations in complex solids and alloys.},
  copyright = {2024 Springer Nature America, Inc.},
  langid = {english},
  keywords = {obsLitNote}
}

@misc{FERC24understandParticipCAISO,
  title = {Understanding and {{Participating}} in {{California ISO}} ({{CAISO}}) {{Processes}}},
  author = {FERC},
  year = {24},
  month = apr,
  journal = {Federal Energy Regulatory Commission},
  url = {https://www.ferc.gov/understanding-and-participating-california-iso-caiso-processes},
  urldate = {2024-07-23},
  abstract = {Q. What is CAISO and what does it do? A.},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\FERC24understandParticipCAISO.pdf}
}

@misc{Sell24priceFrcstCAISOgithub,
  title = {Caiso-Price-Forecast},
  author = {Sell, Morgan},
  year = {2024},
  month = may,
  address = {GitHub},
  url = {https://github.com/Morgan-Sell/caiso-price-forecast},
  urldate = {2024-07-23},
  abstract = {Predicts the CAISO day-ahead market hourly prices using different forecasting methods including ARIMA and LSTM.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sell24priceFrcstCAISOgithub.pdf}
}

@misc{Bahareh23priceFrcstCAISOidAndDA,
  title = {{{CAISO Intraday}} \& {{Day-ahead Price Forecast Case Study Solution}}},
  author = {Bahareh},
  year = {2023},
  month = aug,
  journal = {QuantRisk},
  url = {https://quantrisk.com/caiso-day-ahead-market-price-forecast/},
  urldate = {2024-07-23},
  abstract = {CAISO intraday \& day-ahead AI machine learning price forecast case study covering LMP, trading hubs and system prices. We used QuantRisk AI Forecasting solution.},
  langid = {american},
  keywords = {obsLitNote},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Bahareh23priceFrcstCAISOidAndDA.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Nelso23ancillarySat_CAISO_ERCOT.pdf}
}

@misc{CAISO24marketsOperationsTrn,
  title = {Markets and Operations Training},
  author = {CAISO},
  year = {2024},
  journal = {California ISO},
  url = {https://www.caiso.com/stakeholder/training/markets-and-operations},
  urldate = {2024-07-23},
  abstract = {The California ISO manages the flow of electricity on high-voltage power lines, operates a wholesale energy market, and oversees infrastructure planning.},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\CAISO24marketsOperationsTrn.pdf}
}

@article{Nyangon24pcaCAISOpriceFrcstDA,
  title = {Principal Component Analysis of Day-ahead Electricity Price Forecasting in {{CAISO}} and Its Implications for Highly Integrated Renewable Energy Markets},
  shorttitle = {Principal Component Analysis of Day-ahead Electricity Price Forecasting In},
  author = {Nyangon, Joseph and Akintunde, Ruth},
  year = {2024},
  month = jan,
  journal = {WIREs Energy and Environment},
  volume = {13},
  number = {1},
  pages = {e504},
  issn = {2041-8396, 2041-840X},
  doi = {10.1002/wene.504},
  url = {https://wires.onlinelibrary.wiley.com/doi/10.1002/wene.504},
  urldate = {2024-07-23},
  abstract = {Abstract             Electricity price forecasting is crucial for grid management, renewable energy integration, power system planning, and price volatility management. However, poor accuracy due to complex generation mix data and heteroskedasticity poses a challenge for utilities and grid operators. This paper evaluates advanced analytics methods that utilize principal component analysis (PCA) to improve forecasting accuracy amidst heteroskedastic noise. Drawing on the experience of the California Independent System Operator (CAISO), a leading producer of renewable electricity, the study analyzes hourly electricity prices and demand data from 2016 to 2021 to assess the impact of day-ahead forecasting on California's evolving generation mix. To enhance data quality, traditional outlier analysis using the interquartile range (IQR) method is first applied, followed by a novel supervised PCA technique called robust PCA (RPCA) for more effective outlier detection and elimination. The combined approach significantly improves data symmetry and reduces skewness. Multiple linear regression models are then constructed to forecast electricity prices using both raw and transformed features obtained through PCA. Results demonstrate that the model utilizing transformed features, after outlier removal using the traditional method and SAS Sparse Matrix method, achieves the highest forecasting performance. Notably, the SAS Sparse Matrix outlier removal method, implemented via proc RPCA, greatly contributes to improved model accuracy. This study highlights that PCA methods enhance electricity price forecasting accuracy, facilitating the integration of renewables like solar and wind, thereby aiding grid management and promoting renewable growth in day-ahead markets.                            This article is categorized under:                                                   Energy and Power Systems {$>$} Energy Management                                                     Energy and Power Systems {$>$} Distributed Generation                                                     Emerging Technologies {$>$} Digitalization},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nyangon24pcaCAISOpriceFrcstDA.pdf}
}

@book{Kavrelishvili23priceFrcstCAISOthesis,
  title = {Forecasting of {{California Market Electricity Prices}}},
  author = {Kavrelishvili, Gurami},
  year = {2023},
  publisher = {University of California, Los Angeles},
  url = {https://search.proquest.com/openview/709b2c9a8f2e97bdd284a2c8b1d7d845/1?pq-origsite=gscholar&cbl=18750&diss=y},
  urldate = {2024-07-23},
  abstract = {The importance of electricity in modern society has become increasingly crucial for everyday life. Along with its importance comes the Monetary aspect of buying and selling power in the 21st century. This thesis focuses on predicting Market Electricity Prices in SCE (Southern California Edison) Territory to help Electric Service Providers (ESPs) maximize profits by analyzing upcoming Market prices to make better business decisions. To forecast Market Electricity prices, this thesis utilizes ARIMA modeling, Time series Regression Modeling, Random Forest Modeling, Exponential Smoothing Techniques and Extreme Gradient Boosting The various models are trained on over two years of data and forecasted for Long-Term, ShortTerm and Day-Ahead Market electricity prices to help ESPs across different time scales. To assess the model performance, Root Mean Square Error (RMSE) is utilized across different models. The overall goal of this thesis is to find the best performing model.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kavrelishvili23priceFrcstCAISOthesis.pdf}
}

@phdthesis{Li24mdlAlgTradeElectMktThesis,
  title = {Data-{{Driven Modeling}} and {{Algorithmic Trading}} in {{Electricity Market}}},
  author = {Li, Yinglun},
  year = {2024},
  url = {https://escholarship.org/uc/item/4qm909wm},
  urldate = {2024-07-23},
  school = {UC Riverside},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Li24mdlAlgTradeElectMktThesis.pdf}
}

@misc{Holm24battInflncPkPriceCAISO,
  title = {Powering {{Stability}}: {{Grid-Connected Batteries Influence}} on {{Peak Electricity Pricing}}},
  shorttitle = {Powering {{Stability}}},
  author = {Holm, Emil and Shayeganfar, Parsa},
  year = {2024},
  url = {https://www.diva-portal.org/smash/record.jsf?pid=diva2:1863152},
  urldate = {2024-07-23},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Holm24battInflncPkPriceCAISO.pdf}
}

@article{Nizharadze23PredictingGapDayAhead,
  title = {Predicting the {{Gap}} in the {{Day-Ahead}} and {{Real-Time Market Prices Leveraging Exogenous Weather Data}}},
  author = {Nizharadze, Nika and Farokhi Soofi, Arash and Manshadi, Saeed},
  year = {2023},
  month = nov,
  journal = {Algorithms},
  volume = {16},
  number = {11},
  pages = {508},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1999-4893},
  doi = {10.3390/a16110508},
  url = {https://www.mdpi.com/1999-4893/16/11/508},
  urldate = {2024-07-23},
  abstract = {Predicting the price gap between the day-ahead Market (DAM) and the real-time Market (RTM) plays a vital role in the convergence bidding mechanism of Independent System Operators (ISOs) in wholesale electricity markets. This paper presents a model to predict the values of the price gap between the DAM and RTM using statistical machine learning algorithms and deep neural networks. In this paper, we seek to answer these questions: What will be the impact of predicting the DAM and RTM price gap directly on the prediction performance of learning methods? How can exogenous weather data affect the price gap prediction? In this paper, several exogenous features are collected, and the impacts of these features are examined to capture the best relations between the features and the target variable. An ensemble learning algorithm, namely the Random Forest (RF), is used to select the most important features. A Long Short-Term Memory (LSTM) network is used to capture long-term dependencies in predicting direct gap values between the markets stated. Moreover, the advantages of directly predicting the gap price rather than subtracting the price predictions of the DAM and RTM are shown. The presented results are based on the California Independent System Operator (CAISO)'s electricity market data for two years. The results show that direct gap prediction using exogenous weather features decreases the error of learning methods by 46\%. Therefore, the presented method mitigates the prediction error of the price gap between the DAM and RTM. Thus, the convergence bidders can increase their profit, and the ISOs can tune their mechanism accordingly.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nizharadze23PredictingGapDayAhead.pdf}
}

@article{Owolabi23RoleVariableRenewable,
  title = {Role of {{Variable Renewable Energy Penetration}} on {{Electricity Price}} and Its {{Volatility}} across {{Independent System Operators}} in the {{United States}}},
  author = {Owolabi, Olukunle O. and Schafer, Toryn L. J. and Smits, Georgia E. and Sengupta, Sanhita and Ryan, Sean E. and Wang, Lan and Matteson, David S. and Getmansky Sherman, Mila and Sunter, Deborah A.},
  year = {2023},
  month = dec,
  journal = {Data Science in Science},
  volume = {2},
  number = {1},
  pages = {2158145},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/26941899.2022.2158145},
  url = {https://doi.org/10.1080/26941899.2022.2158145},
  urldate = {2024-07-23},
  abstract = {The US electrical grid has undergone substantial transformation with increased penetration of wind and solar forms of variable renewable energy (VRE). Despite the benefits of VRE for decarbonization, it has garnered some controversy for inducing unwanted effects in regional electricity markets. In this study, the role of VRE penetration is examined on the system electricity price and price volatility based on hourly, real-time, historical data from six independent system operators (ISOs) in the US using quantile and skew t-distribution regressions. After correcting for temporal effects, we found an increase in VRE penetration is associated with a decrease in system electricity price in all ISOs studied. The increase in VRE penetration is associated with a decrease in temporal price volatility in five out of six ISOs studied. The relationships are nonlinear. These results are consistent with the modern portfolio theory where diverse volatile assets may lead to more stable and less risky portfolios.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Owolabi23RoleVariableRenewable.pdf}
}

@article{Manguin16elecPricePredCAISO,
  title = {Electricity {{Price Prediction}} and {{Nodal Price Analysis}} in {{California ISO}}},
  author = {Manguin, M{\'e}lanie and Porter, Emily and Travacca, Bertrand and Pozdnukhov, Alexey},
  year = {2016},
  journal = {Class Report, CE 263N - Scalable Spatial Analytics},
  url = {https://www.academia.edu/download/51831048/scalable-spatial-analytics-report.pdf},
  urldate = {2024-07-23},
  abstract = {In the United States, an independent system operator (ISO) controls and monitors the operation of the electrical power system within a given perime- ter. In California, CAISO also acts as the electricity market operator. In this project we focus our attention on the day ahead market (DAM) for electric- ity. This market takes place one day prior to the operating day and consists of scheduling the quantities and marginal electricity prices (wholesale) for each hour. We will denote p {$\in$} R24 the DAM price for a given day. The marginal clearing price corresponds to the intersection between the supply and demand curve: these curves are formed by CAISO who receive bids from all the market participants (essentially: electricity retailers, gener- ators, and virtual bidders). Figure 1 gives an overview of the DAM market process, while Figure 2 illustrates the obtainment of the market clearing price for a given hour. CAISO has the specificity of having different clearing prices based on lo- cation, this is done with the purpose of including externalities of electricity losses (around 6\% on the transmission grid [6]), and electricity congestion in the final price [7]. All in all, the clearing price in the DAM is specific to a location (or node), and is referred to as Local Marginal Price (LMP). It is essential for a market participant to be able to predict the DAM price before submitting it's bid to CAISO. That is why in a first step we decided to build a prediction model for the DAM price (average price in the system, not the LMP). It is also important for the market participant to understand the underlying risk when making a bid: that is why we studied the error between our prediction and the realized price. In a second step, we examined and visualized the LMP behavior, which is particularly interesting if we want to understand the local market power an agent can have.},
  keywords = {obsLitNote},
  note = {Manguin16elecPricePredCAISO
\par
Apparently class project report on CAISO price prediciton using simple statistical models. ~Interesting b/c an intro to basic mkt behavior, solves a convex optimization problem, and clusters nodes.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Manguin16elecPricePredCAISO.pdf}
}

@article{Guan23electMktTradingML,
  title = {Data-{{Driven Electricity Trading Models}} and {{Intelligent Solution Using Machine Learning Approaches}}},
  author = {Guan, Jingqi and Afnan, Shamama and Gao, Jerry and Wang, Qin},
  year = {2023},
  month = aug,
  journal = {Submitted: Unspecified Journal},
  volume = {1},
  pages = {0},
  url = {https://www.researchgate.net/profile/Jerry-Gao/publication/373194094_Electricity-Trading-Paper/links/64def93814f8d17338082ebb/Electricity-Trading-Paper.pdf},
  urldate = {2024-07-23},
  abstract = {Renewable sources have placed themselves as the preferred energy source while consciousness is growing about the environment, climate change andhealthimpacts of fossil fuel. In addition to these,renewable energy is also becoming cheaper to generate and store, blessedby the advent of modernmanufacturingmethods and scaleups. Renewable energy sources arebecominga financially viable alternative,not just aniche application. Energy Information Administration (EIA, USA) forecasts that from present to 2050, renewable energy will remain the fastest growing source of energy [1]. The Electric gridmodels are also undergoing changes to address this changed landscape. Many large scale energy farms as well as households are participatingin this complex energy delivery mechanism. This has ushered us into the smart grid. Smart grid allows for more data and insights into energy generation/consumption, andhas opened the door for application of more statistical tools and forecasts. Energy data arenow harvested at more granular levels, and with the proliferation of AI/ML, the gathered data cannow be utilized for forecasting more effectively. When it is viewed from the grid-level transmission and distribution perspectives,the available data and computationalpower allow all stakeholders to re-think the market dynamics.},
  keywords = {obsLitNote},
  note = {Guan23electMktTradingML
\par
Surveys trading algs and data, including CAISO and ERCOT. ~I couldn't find that this was accepted anywhere.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Guan23electMktTradingML.pdf}
}

@techreport{Loutan17reliabSrvcPV300MW,
  title = {Demonstration of Essential Reliability Services by a 300-{{MW}} Solar Photovoltaic Power Plant},
  author = {Loutan, Clyde and Klauer, Peter and Chowdhury, Sirajul and Hall, Stephen and Morjaria, Mahesh and Chadliev, Vladimir and Milam, Nick and Milan, Christopher and Gevorgian, Vahan},
  year = {2017},
  institution = {National Renewable Energy Lab.(NREL), Golden, CO (United States)},
  url = {https://www.osti.gov/biblio/1349211},
  urldate = {2024-07-24},
  keywords = {todo},
  note = {Loutan17reliabSrvcPV300MW
\par
Report Jon Donadee recommended. ~Bare non-storage PV for ancillary, I think.},
  file = {/Users/scott/Library/CloudStorage/OneDrive-Personal/share/ref/zotero/Loutan17reliabSrvcPV300MW.pdf}
}

@article{Zarnikau20determElecPriceMISO,
  title = {Determinants of the Wholesale Prices of Energy and Ancillary Services in the {{U}}.{{S}}. {{Midcontinent}} Electricity Market},
  author = {Zarnikau, J. and Tsai, C. H. and Woo, C. K.},
  year = {2020},
  month = mar,
  journal = {Energy},
  volume = {195},
  pages = {117051},
  issn = {0360-5442},
  doi = {10.1016/j.energy.2020.117051},
  url = {https://www.sciencedirect.com/science/article/pii/S0360544220301584},
  urldate = {2024-07-27},
  abstract = {This paper examines wholesale price behavior of energy and ancillary services (AS) in the day-ahead market (DAM) and real-time market (RTM) of the Midcontinent Independent System Operator (MISO), the second largest regional transmission organization in the U.S. Using hourly data for the period of 12/19/2013 to 12/30/2017, it estimates a system of 16 electricity price regressions that recognizes the interdependence of MISO's electricity products. Its key findings are: (1) MISO's regional DAM energy prices increase with the day-ahead forecasts of natural gas price and MISO's regional loads, PJM's DAM energy price at its MISO interface, and MISO's DAM AS requirements; (2) MISO's regional DAM energy prices decline with DAM schedules for nuclear and wind generation; (3) MISO's regional RTM energy prices increase with, but diverge from, MISO's regional DAM energy prices; (4) MISO's system-average DAM AS prices increase with MISO's system-average DAM energy price and procured amounts of AS; and (5) MISO's system-average RTM AS prices increase with MISO's system-average RTM energy price and system-average DAM AS prices. These findings imply that MISO can reduce its DAM and RTM prices for energy and AS by accelerating wind generation development, expanding demand-side-management, and easing inter-regional transmission congestion.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zarnikau20determElecPriceMISO.pdf}
}

@inproceedings{Edwards21rampFrcstVizRAVIS,
  title = {{{RAVIS}}: {{Resource Forecast}} and {{Ramp Visualization}} for {{Situational Awareness}} of {{Variable Renewable Generation}}},
  shorttitle = {{{RAVIS}}},
  booktitle = {Proceedings of the {{Twelfth ACM International Conference}} on {{Future Energy Systems}}},
  author = {Edwards, Paul and Sky, Haiku and Krishnan, Venkat},
  year = {2021},
  month = jun,
  pages = {345--354},
  publisher = {ACM},
  address = {Virtual Event Italy},
  doi = {10.1145/3447555.3466594},
  url = {https://dl.acm.org/doi/10.1145/3447555.3466594},
  urldate = {2024-07-28},
  isbn = {978-1-4503-8333-2},
  langid = {english},
  keywords = {hasCode,obsLitNote},
  note = {Edwards21rampFrcstVizRAVIS
\par
Some kind of prob ramp forecaster and plotter. ~
\par
Has code: \href{https://github.com/ravis-nrel/ravis?tab=readme-ov-file}{https://github.com/ravis-nrel/ravis?tab=readme-ov-file}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Edwards21RAVISResourceForecast.pdf}
}

@misc{Lee24GrokfastAcceleratedGrokking,
  title = {Grokfast: {{Accelerated Grokking}} by {{Amplifying Slow Gradients}}},
  shorttitle = {Grokfast},
  author = {Lee, Jaerin and Kang, Bong Gyun and Kim, Kihoon and Lee, Kyoung Mu},
  year = {2024},
  month = jun,
  number = {arXiv:2405.20233},
  eprint = {2405.20233},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.20233},
  url = {http://arxiv.org/abs/2405.20233},
  urldate = {2024-07-28},
  abstract = {One puzzling artifact in machine learning dubbed grokking is where delayed generalization is achieved tenfolds of iterations after near perfect overfitting to the training data. Focusing on the long delay itself on behalf of machine learning practitioners, our goal is to accelerate generalization of a model under grokking phenomenon. By regarding a series of gradients of a parameter over training iterations as a random signal over time, we can spectrally decompose the parameter trajectories under gradient descent into two components: the fast-varying, overfitting-yielding component and the slow-varying, generalization-inducing component. This analysis allows us to accelerate the grokking phenomenon more than \${\textbackslash}times 50\$ with only a few lines of code that amplifies the slow-varying components of gradients. The experiments show that our algorithm applies to diverse tasks involving images, languages, and graphs, enabling practical availability of this peculiar artifact of sudden generalization. Our code is available at https://github.com/ironjr/grokfast.},
  archiveprefix = {arXiv},
  note = {Comment: 17 pages, 13 figures. Typo fixed. Project page: https://jaerinlee.com/research/grokfast},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Lee24accelGrokFast.pdf}
}

@misc{Power22GrokkingGeneralizationOverfitting,
  title = {Grokking: {{Generalization Beyond Overfitting}} on {{Small Algorithmic Datasets}}},
  shorttitle = {Grokking},
  author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  year = {2022},
  month = jan,
  number = {arXiv:2201.02177},
  eprint = {2201.02177},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.02177},
  url = {http://arxiv.org/abs/2201.02177},
  urldate = {2024-07-28},
  abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
  archiveprefix = {arXiv},
  note = {Comment: Correspondence to alethea@openai.com. Code available at: https://github.com/openai/grok},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Power22beyondOverfitGrokking.pdf}
}

@misc{Lee24repoGrokfast,
  title = {Ironjr/Grokfast},
  author = {Lee, Jaerin},
  year = {2024},
  month = jul,
  url = {https://github.com/ironjr/grokfast},
  urldate = {2024-07-28},
  abstract = {Official repository for the paper "Grokfast: Accelerated Grokking by Amplifying Slow Gradients"},
  copyright = {MIT},
  keywords = {obsLitNote}
}

@book{SaezGallego17invOptFrcstElecMktPrice,
  title = {Inverse {{Optimization}} and {{Forecasting Techniques Applied}} to {{Decision-making}} in {{Electricity Markets}}},
  author = {Saez Gallego, Javier},
  year = {2017},
  series = {{{DTU Compute PHD-2016}}},
  publisher = {Technical University of Denmark},
  address = {Kgs. Lyngby},
  abstract = {This thesis deals with the development of new mathematical models that support the decision-making processes of market players. It addresses the problems of demand-side bidding, price-responsive load forecasting and reserve determination.  From a methodological point of view, we investigate a novel approach to model the response of aggregate price-responsive load as a constrained optimization model, whose parameters are estimated from data by using inverse optimization techniques.The problems tackled in this dissertation are motivated, on one hand, by the increasing penetration of renewable energy production and smart grid technologies in power systems, that is expected to continue growing in the coming years.  Non-dispatchable electricity generation cannot ensure a certain production at all times, since it depends on meteorological factors. Also, smart grid technologies are affecting the consumption patterns that the load traditionally exhibited. On the other hand, this thesis is motivated by the decision-making processes of market players. In response to these challenges, this thesis provides mathematical models for decision-making under uncertainty in electricity markets.  Demand-side bidding refers to the participation of consumers, often through a retailer, in energy trading. Under the smart-grid paradigm, the demand bids must reflect the elasticity of the consumers to changes in electricity price. Traditional forecasting models are typically not able to reflect this elasticity, hence we propose two novel approaches to estimate market bids. Both approaches are data-driven and take into account the uncertainty of future factors, as, for example, price. In both cases, demand-side bids that comprise a price-energy term decrease the expected imbalances and also increase the profit of retailers participating in electricity markets.In the field of load forecasting, this thesis provides a novel approach to model time series and forecast loads under the real-time pricing setup. The relationship between price and aggregate response of the load is characterized by an optimization problem, which is shaped by a set of unknown parameters. Such parameters are estimated from data by using an inverse optimization framework.  The usability of the proposed method is studied and we conclude that inverseoptimization-based modeling is a computationally attractive method that outperforms the forecasting capabilities of traditional time series models.  Regarding the reserve determination, the special characteristics of the Danish power system do not allow for co-optimizing the unit commitment and reserve requirements. Hence, we propose a probabilistic framework, where the reserve requirements are computed based on scenarios of wind power and load forecast errors and power plant outages. The solution of the stochastic optimization models increases the safety of the overall system while decreases the associated reserve costs, with respect to the method currently used by the Danish TSO.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\SaezGallego17invOptFrcstElecMktPrice.pdf}
}

@inproceedings{Yurdakul21netloadRampCAISOfrcst,
  title = {Forecasting {{Daily Primary Three-Hour Net Load Ramps}} in the {{CAISO System}}},
  booktitle = {2020 52nd {{North American Power Symposium}} ({{NAPS}})},
  author = {Yurdakul, Ogun and Meyer, Andreas and Sivrikaya, Fikret and Albayrak, Sahin},
  year = {2021},
  month = apr,
  pages = {1--6},
  doi = {10.1109/NAPS50074.2021.9449642},
  url = {https://ieeexplore.ieee.org/abstract/document/9449642},
  urldate = {2024-07-28},
  abstract = {The deepening penetration of variable energy resources creates unprecedented challenges for system operators (SOs). An issue that merits special attention is the precipitous net load ramps, which require SOs to have flexible capacity at their disposal so as to maintain the supply-demand balance at all times. In the judicious procurement and deployment of flexible capacity, a tool that forecasts net load ramps may be of great assistance to SOs. To this end, we propose a methodology to forecast the magnitude and start time of daily primary three-hour net load ramps. We perform an extensive analysis so as to identify the factors that influence net load and draw on the identified factors to develop a forecasting methodology that harnesses the long short-term memory model. We demonstrate the effectiveness of the proposed methodology on the CAISO system using comparative assessments with selected benchmarks based on various evaluation metrics.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Yurdakul21ForecastingDailyPrimary.pdf}
}

@article{Hobbs22probPVfrcstRamp,
  title = {Using Probabilistic Solar Power Forecasts to Inform Flexible Ramp Product Procurement for the {{California ISO}}},
  author = {Hobbs, Benjamin F. and Zhang, Jie and Hamann, Hendrik F. and Siebenschuh, Carlo and Zhang, Rui and Li, Binghui and Krad, Ibrahim and Krishnan, Venkat and Spyrou, Evangelia and Wang, Yijiao and Xu, Qingyu and Zhang, Shu},
  year = {2022},
  month = jan,
  journal = {Solar Energy Advances},
  volume = {2},
  pages = {100024},
  issn = {2667-1131},
  doi = {10.1016/j.seja.2022.100024},
  url = {https://www.sciencedirect.com/science/article/pii/S2667113122000122},
  urldate = {2024-07-28},
  abstract = {How can independent system operators (ISOs) take advantage of probabilistic solar forecasts to lower generation costs and improve reliability of power systems? We discuss one three-step approach for doing so, focusing on how such forecasts might help the California Independent System Operator (CAISO) prepare unexpected net load ramps, where net load equals gross demand minus wind and solar production. First, we enhance an existing solar forecasting system to provide well-calibrated hours-ahead probabilistic forecasts. We then relate the degree of uncertainty reflected in the forecasted prediction intervals (independent variables) to error distributions for net load ramp forecasts for the CAISO real-time market (dependent variable) using machine learning and quantile regression. Projected ramp forecast errors conditioned on solar uncertainty are translated into flexible ramp requirements that therefore reflect real-time meteorological and solar conditions, improving on typical ISO procedures. Detailed descriptions are provided on the quantile regression and kth-nearest neighbor categorization methods for accomplishing that translation. Finally, a multiple time-scale look-ahead market simulation model is applied to a 118-bus IEEE Reliability Test System, modified to represent the CAISO generation mix and demand distributions. The model runs quantify how solar-conditioned ramp requirements can, first, decrease operating costs by reducing requirements compared to often conservative unconditional methods and, second, decrease generation scarcity events and consequently improve reliability by increasing flexibility requirements at times when unconditional forecast-based requirements understate actual ramp uncertainty. Solar-conditioned ramp requirements are found to reduce generation operating costs by about 2\% for the test system (which would be equivalent to over \$100 million per year for a CAISO-size system).},
  keywords = {todo},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Hobbs22probPVfrcstRamp.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Hobbs22UsingProbabilisticSolar.pdf}
}

@article{Zhao24uncertInformBilevRESsched,
  title = {Uncertainty-{{Informed Renewable Energy Scheduling}}: {{A Scalable Bilevel Framework}}},
  shorttitle = {Uncertainty-{{Informed Renewable Energy Scheduling}}},
  author = {Zhao, Dongwei and Dvorkin, Vladimir and Delikaraoglou, Stefanos and L., Alberto J. Lamadrid and Botterud, Audun},
  year = {2024},
  month = mar,
  journal = {Policy and Regulation IEEE Transactions on Energy Markets},
  volume = {2},
  number = {1},
  pages = {132--145},
  issn = {2771-9626},
  doi = {10.1109/TEMPR.2023.3344126},
  url = {https://ieeexplore.ieee.org/abstract/document/10365314},
  urldate = {2024-07-28},
  abstract = {This work proposes an uncertainty-informed bid adjustment framework for integrating variable renewable energy sources (VRES) into electricity markets. This framework adopts a bilevel model to compute the optimal VRES day-ahead bids. It aims to minimize the expected system cost across day-ahead and real-time stages and approximate the cost efficiency of the stochastic market design. However, solving the bilevel optimization problem is computationally challenging for large-scale systems. To overcome this challenge, we introduce a novel technique based on strong duality and McCormick envelopes, which relaxes the problem to a linear program, enabling large-scale applications. The proposed bilevel framework is applied to the 1576-bus NYISO system and benchmarked against a myopic strategy, where the VRES bid is the mean value of the probabilistic power forecast. Results demonstrate that, under high VRES penetration levels (e.g., 40\%), our framework can significantly reduce system costs and market-price volatility, by optimizing VRES quantities efficiently in the day-ahead market. Furthermore, we find that when transmission capacity increases, the proposed bilevel model will still reduce the system cost, whereas the myopic strategy may incur a much higher cost due to over-scheduling of VRES in the day-ahead market and the lack of flexible conventional generators in real time.},
  keywords = {obsLitNote},
  note = {Zhao24uncertInformBilevRESsched
\par
JonD said he wanted to do bilevel optimization.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhao24uncertInformBilevRESsched.pdf}
}

@inproceedings{He23estRegRsrvRqtsCAISO,
  title = {Estimation of {{Regulation Reserve Requirements}} in {{California ISO}}: {{A Data-driven Method}}},
  shorttitle = {Estimation of {{Regulation Reserve Requirements}} in {{California ISO}}},
  booktitle = {2023 {{IEEE Power}} \& {{Energy Society General Meeting}} ({{PESGM}})},
  author = {He, Li and Zhang, Jie and Hobbs, Benjamin},
  year = {2023},
  month = jul,
  pages = {1--5},
  issn = {1944-9933},
  doi = {10.1109/PESGM52003.2023.10252917},
  url = {https://ieeexplore.ieee.org/abstract/document/10252917},
  urldate = {2024-07-28},
  abstract = {This paper proposes a data-driven method to estimate real-time regulation reserve requirements in the California Independent System Operator (CAISO) balancing authority area (BAA). The approach is based on the statistical analysis of actual historical area control error (ACE) and regulation procurement in the CAISO system. The CAISO baseline method uses day-ahead (DA) information and does not statistically relate requirements in real time. In this work, we examine the benefit of setting requirements in real time, and the proposed method presents advantages rather than the present DA approach by using additional 1-hour-ahead (1HA) independent variables to estimate requirements. By adopting a time series forecasting technique, the approach estimates regulation reserve requirements on an hourly basis. More specifically, a neural network based upon a nonlinear autoregressive exogenous model (NARX), which accounts for temporal continuity and autocorrelation, is leveraged for regulation requirements estimation. The proposed approach incorporates multiple time-series variables that might affect the system balance (e.g., solar, wind, and load). Two metrics are adopted to evaluate method performance: 1) frequency of shortage, and 2) oversupply of the regulation reserve requirement. By dynamically tuning the parameters of the procurement strategies, system regulation reserve requirements were reduced without compromising system reliability, improving both system reliability and economics.},
  keywords = {obsLitNote},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\He23EstimationRegulationReserve.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\He23estRegRsrvRqtsCAISO.pdf}
}

@inproceedings{Feng20adaptRsrvRqtProbNetLoad,
  title = {A {{Data-driven Method}} for {{Adaptive Reserve Requirement Estimation}} via {{Probabilistic Net Load Forecasting}}},
  booktitle = {2020 {{IEEE Power}} \& {{Energy Society General Meeting}} ({{PESGM}})},
  author = {Feng, Cong and Sun, Mucun and Zhang, Jie and Doubleday, Kate and Hodge, Bri-Mathias and Du, Pengwei},
  year = {2020},
  month = aug,
  pages = {1--5},
  issn = {1944-9933},
  doi = {10.1109/PESGM41954.2020.9282155},
  url = {https://ieeexplore.ieee.org/document/9282155},
  urldate = {2024-07-28},
  abstract = {The University of Colorado Boulder Boulder, CO, 80309, USA With the increasing penetration of renewable energy, power systems are subject to more uncertainty. This makes power system reserve scheduling more challenging. Most of the current reserve requirement determination methods calculate reserve requirements based on historical data, which does not consider the real-time or future system uncertainty. In this paper, a data-driven method is developed to determine the non-spinning reserve requirement (NSRR) in the Electric Reliability Council of Texas (ERCOT) system. The method follows the procedure of the current ERCOT method while adaptively determining the NSRR based on probabilistic net load forecasts. Case studies with two years of ERCOT data show that the developed method significantly reduces the NSRR by introducing an adaptive temporal resolution and update rate. Sensitivity analysis with different forecasting and percentile thresholds indicates the flexibility of the developed method.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Feng20adaptRsrvRqtProbNetLoad.pdf}
}

@inproceedings{Kohansal17supplyBidsElastCAISO,
  title = {A Data-Driven Analysis of Supply Bids in {{California ISO}} Market: {{Price}} Elasticity and Impact of Renewables},
  shorttitle = {A Data-Driven Analysis of Supply Bids in {{California ISO}} Market},
  booktitle = {2017 {{IEEE International Conference}} on {{Smart Grid Communications}} ({{SmartGridComm}})},
  author = {Kohansal, Mahdi and {Sadeghi-Mobarakeh}, Ashkan and {Mohsenian-Rad}, Hamed},
  year = {2017},
  month = oct,
  pages = {58--63},
  doi = {10.1109/SmartGridComm.2017.8340745},
  url = {https://ieeexplore.ieee.org/abstract/document/8340745},
  urldate = {2024-07-28},
  abstract = {One month of supply bids in the California ISO day-ahead energy market are analyzed in this paper. A total of 1.5 million records of bid data are studied. The bids are studied based on their types and their distribution at different hours. The relationship between the market price and the offered supply capacity are investigated. A data-driven estimate is provided for the aggregated supply curve and accordingly the price elasticity of supply is identified for hours that price is highly inelastic. Importantly, this analysis shows the impact of the recent high capacity installations of renewable generation in the state of California on electricity price and price inelasticity. Finally, the undesirable consequences of price inelasticity, such as on creating price spikes and exercising market power, are discussed.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kohansal17supplyBidsElastCAISO.pdf}
}

@misc{Alghumayjan24EnergyStorageArbitrage,
  title = {Energy {{Storage Arbitrage}} in {{Two-settlement Markets}}: {{A Transformer-Based Approach}}},
  shorttitle = {Energy {{Storage Arbitrage}} in {{Two-settlement Markets}}},
  author = {Alghumayjan, Saud and Han, Jiajun and Zheng, Ningkun and Yi, Ming and Xu, Bolun},
  year = {2024},
  month = apr,
  number = {arXiv:2404.17683},
  eprint = {2404.17683},
  primaryclass = {cs, eess, math},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2404.17683},
  urldate = {2024-07-30},
  abstract = {This paper presents an integrated model for bidding energy storage in day-ahead and real-time markets to maximize profits. We show that in integrated two-stage bidding, the real-time bids are independent of day-ahead settlements, while the day-ahead bids should be based on predicted real-time prices. We utilize a transformer-based model for real-time price prediction, which captures complex dynamical patterns of real-time prices, and use the result for day-ahead bidding design. For real-time bidding, we utilize a long short-term memory-dynamic programming hybrid real-time bidding model. We train and test our model with historical data from New York State, and our results showed that the integrated system achieved promising results of almost a 20{\textbackslash}\% increase in profit compared to only bidding in real-time markets, and at the same time reducing the risk in terms of the number of days with negative profits.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control},
  file = {C\:\\Users\\scott\\Zotero\\storage\\VLKXXPLZ\\Alghumayjan24priceFrcstStorageTransfrmr.pdf;C\:\\Users\\scott\\Zotero\\storage\\HWLGSB6E\\2404.html}
}

@misc{RektPunk24nonCrossMQBoostQR,
  title = {{{MQBoost}}: {{Multiple}} Quantiles Estimation Model Maintaining Non-Crossing Condition (or Monotone Quantile Condition) Using {{LightGBM}} and {{XGBoost}}},
  author = {RektPunk},
  year = {2024},
  journal = {GitHub},
  url = {https://github.com/RektPunk/MQBoost},
  urldate = {2024-07-31},
  keywords = {obsLitNote},
  note = {RektPunk24nonCrossMQBoostQR
\par
\begin{itemize}

\item this is the \href{https://github.com/microsoft/LightGBM/issues/5727}{LightGBM issue} where this started, explaining inspiration

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\RektPunk24nonCrossMQBoostQR.pdf}
}

@article{Cannon18nonCrossNeuralQRrainfal,
  title = {Non-Crossing Nonlinear Regression Quantiles by Monotone Composite Quantile Regression Neural Network, with Application to Rainfall Extremes},
  author = {Cannon, Alex J.},
  year = {2018},
  month = nov,
  journal = {Stochastic Environmental Research and Risk Assessment},
  volume = {32},
  number = {11},
  pages = {3207--3225},
  issn = {1436-3259},
  doi = {10.1007/s00477-018-1573-6},
  url = {https://doi.org/10.1007/s00477-018-1573-6},
  urldate = {2024-07-31},
  abstract = {The goal of quantile regression is to estimate conditional quantiles for specified values of quantile probability using linear or nonlinear regression equations. These estimates are prone to ``quantile crossing'', where regression predictions for different quantile probabilities do not increase as probability increases. In the context of the environmental sciences, this could, for example, lead to estimates of the magnitude of a 10-year return period rainstorm that exceed the 20-year storm, or similar nonphysical results. This problem, as well as the potential for overfitting, is exacerbated for small to moderate sample sizes and for nonlinear quantile regression models. As a remedy, this study introduces a novel nonlinear quantile regression model, the monotone composite quantile regression neural network (MCQRNN), that (1) simultaneously estimates multiple non-crossing, nonlinear conditional quantile functions; (2) allows for optional monotonicity, positivity/non-negativity, and generalized additive model constraints; and (3) can be adapted to estimate standard least-squares regression and non-crossing expectile regression functions. First, the MCQRNN model is evaluated on synthetic data from multiple functions and error distributions using Monte Carlo simulations. MCQRNN outperforms the benchmark models, especially for non-normal error distributions. Next, the MCQRNN model is applied to real-world climate data by estimating rainfall Intensity--Duration--Frequency (IDF) curves at locations in Canada. IDF curves summarize the relationship between the intensity and occurrence frequency of extreme rainfall over storm durations ranging from minutes to a day. Because annual maximum rainfall intensity is a non-negative quantity that should increase monotonically as the occurrence frequency and storm duration decrease, monotonicity and non-negativity constraints are key constraints in IDF curve estimation. In comparison to standard QRNN models, the ability of the MCQRNN model to incorporate these constraints, in addition to non-crossing, leads to more robust and realistic estimates of extreme rainfall.},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Cannon18nonCrossNeuralQRrainfal.pdf}
}

@article{Moon21lrnMuliQuantilesNN,
  title = {Learning {{Multiple Quantiles With Neural Networks}}},
  author = {Moon, Sang Jun and Jeon, Jong-June and Lee, Jason Sang Hun and Kim, Yongdai},
  year = {2021},
  month = oct,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {30},
  number = {4},
  pages = {1238--1248},
  publisher = {Taylor \& Francis},
  issn = {1061-8600},
  doi = {10.1080/10618600.2021.1909601},
  url = {https://doi.org/10.1080/10618600.2021.1909601},
  urldate = {2024-07-31},
  abstract = {We present a neural network model for estimation of multiple conditional quantiles that satisfies the noncrossing property. Motivated by linear noncrossing quantile regression, we propose a noncrossing quantile neural network model with inequality constraints. In particular, to use the first-order optimization method, we develop a new algorithm for fitting the proposed model. This algorithm gives a nearly optimal solution without the projected gradient step that requires polynomial computation time. We compare the performance of our proposed model with that of existing neural network models on simulated and real precipitation data. Supplementary materials for this article are available online.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Moon21lrnMuliQuantilesNN.pdf}
}

@misc{Taquet22theoryDescrConformScores,
  title = {Theoretical {{Description}} for {{Conformity Scores}} ({{MAPIE}} Docs)},
  author = {Taquet, Vianney and others},
  year = {2022},
  publisher = {GitHub},
  url = {https://mapie.readthedocs.io/en/latest/theoretical_description_conformity_scores.html#theoretical-description-for-conformity-scores},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Taquet22theoryDescrConformScores.pdf}
}

@article{Kumar24ComparConformQRtoQR,
  title = {Comparing {{Conformal}} and {{Quantile Regression}} for {{Uncertainty Quantification}}: {{An Empirical Investigation}}},
  shorttitle = {Comparing {{Conformal}} and {{Quantile Regression}} for {{Uncertainty Quantification}}},
  author = {Kumar, Bhargava and Kumar, Tejaswini and Nadakuditi, Swapna and Patel, Hitesh and Gupta, Karan},
  year = {2024},
  month = may,
  journal = {International Journal of Computing and Engineering},
  volume = {5},
  number = {5},
  pages = {1--8},
  issn = {2958-7425},
  doi = {10.47941/ijce.1925},
  url = {https://carijournals.org/journals/index.php/IJCE/article/view/1925},
  urldate = {2024-08-01},
  abstract = {Purpose: This research assesses the efficacy of conformal regression and standard quantile regression in uncertainty quantification for predictive modeling. Quantile regression estimates various quantiles within the conditional distribution, while conformal regression constructs prediction intervals with guaranteed coverage. Methodology: By training models on multiple quantile pairs and varying error rates, the analysis evaluates each method's performance. Findings: Results indicate consistent trends in coverage and prediction interval lengths, with no significant differences in performance. Quantile regression intervals lengthen toward the distribution tails, while conformal regression intervals lengthen with higher coverage. Unique contribution to theory, policy and practice: On the tested dataset, both methods perform similarly, but further testing is necessary to validate these findings across diverse datasets and conditions, considering computational efficiency and implementation ease to determine the best method for specific applications.},
  copyright = {Copyright (c) 2024 Bhargava Kumar, Tejaswini Kumar, Swapna Nadakuditi, Hitesh Patel, Karan Gupta},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kumar24ComparConformQRtoQR.pdf}
}

@manual{Bien24evalcastCOVIDfrcst,
  type = {Manual},
  title = {Evalcast: {{Tools}} for Evaluating {{COVID}} Forecasters},
  author = {Bien, Jacob and others},
  year = {2024},
  url = {https://cmu-delphi.github.io/covidcast/evalcastR/},
  keywords = {obsLitNote},
  note = {Bien24evalcastCOVIDfrcst
\par
A COVID forecast R scoring package
\par
Has a decent explanation of the weighted interval score (``Evalcast Docs: weighted\_interval\_score'', 2024)
\par
R package version 0.1.0}
}

@article{Wen22continProbFrcstWind,
  title = {Continuous and {{Distribution-Free Probabilistic Wind Power Forecasting}}: {{A Conditional Normalizing Flow Approach}}},
  shorttitle = {Continuous and {{Distribution-Free Probabilistic Wind Power Forecasting}}},
  author = {Wen, Honglin and Pinson, Pierre and Ma, Jinghuan and Gu, Jie and Jin, Zhijian},
  year = {2022},
  month = oct,
  journal = {IEEE Transactions on Sustainable Energy},
  volume = {13},
  number = {4},
  pages = {2250--2263},
  issn = {1949-3037},
  doi = {10.1109/TSTE.2022.3191330},
  url = {https://ieeexplore.ieee.org/abstract/document/9831039},
  urldate = {2024-08-01},
  abstract = {We present a data-driven approach for probabilistic wind power forecasting based on conditional normalizing flow (CNF). In contrast with the existing, this approach is distribution-free (as for non-parametric and quantile-based approaches) and can directly yield continuous probability densities, hence avoiding quantile crossing. It relies on a base distribution and a set of bijective mappings. Both the shape parameters of the base distribution and the bijective mappings are approximated with neural networks. Spline-based conditional normalizing flow is considered owing to its non-affine characteristics. Over the training phase, the model sequentially maps input examples onto samples of base distribution, given the conditional contexts, where parameters are estimated through maximum likelihood. To issue probabilistic forecasts, one eventually maps samples of the base distribution into samples of a desired distribution. Case studies based on open datasets validate the effectiveness of the proposed model, and allows us to discuss its advantages and caveats with respect to the state of the art.},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wen22continProbFrcstWind.pdf}
}

@misc{Taquet22modelAgnostIntervMAPIE,
  title = {{{MAPIE}}: {{Model}} Agnostic Prediction Interval Estimator},
  author = {Taquet, Vianney and others},
  year = {2022},
  publisher = {GitHub},
  url = {https://github.com/scikit-learn-contrib/MAPIE},
  keywords = {obsLitNote}
}

@misc{KellyDetwiler24tale2CAISO_ERCOT,
  title = {A Tale of Two {{ISOs}}: Energy Market Design in {{Texas}} and {{California}}},
  shorttitle = {A Tale of Two {{ISOs}}},
  author = {{Kelly-Detwiler}, Peter},
  journal = {GE Vernova},
  url = {https://www.gevernova.com/gas-power/resources/articles/2018/a-tale-of-two-isos-energy-market-design-in-Texas-and-California},
  urldate = {2024-08-01},
  abstract = {ERCOT and CAISO have differing market designs, but the same disrupter: renewables. In either case, results to date suggest market reform will be required to keep generators profitable and grids reliable.},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\KellyDetwiler24tale2CAISO_ERCOT.pdf}
}

@misc{Nelson23ancillarySat_CAISO_ERCOT,
  title = {Ancillary {{Market Saturation}} in {{CAISO}} and {{ERCOT}}: {{A Series}} of {{Predictable Events}}},
  shorttitle = {Ancillary {{Market Saturation}} in {{CAISO}} and {{ERCOT}}},
  author = {Nelson, Brent},
  year = {23},
  month = jul,
  journal = {Ascend Analytics},
  url = {https://www.ascendanalytics.com/blog/ancillary-market-saturation-in-caiso-and-ercot-a-series-of-predictable-events},
  urldate = {2024-08-01},
  abstract = {Ancillary services have been a key component of  storage revenue stacks since batteries began coming  online, turbocharging returns for some operating  projects. However, this turbocharger has already  started to wane in CAISO and ERCOT and serves as  a harbinger for ancillary decline in other markets.  This decline is consistent with Ascend Analytics'  forecast of ancillary service prices, highlighting the  growing importance of geographic location and  bidding strategy as value drivers for storage projects  as revenues shift from ancillary services to energy  arbitrage.},
  langid = {english},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nelso23ancillarySat_CAISO_ERCOT.pdf}
}

@misc{Linusson15nonconformistLib,
  title = {Donlnz/Nonconformist},
  author = {Linusson, Henrik},
  year = {2015},
  url = {https://github.com/donlnz/nonconformist},
  urldate = {2024-08-01},
  abstract = {Python implementation of the conformal prediction framework.},
  copyright = {MIT},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Linusson15nonconformistLib.pdf}
}

@article{Diamond16convexOptCVXPY,
  title = {{{CVXPY}}: {{A Python-embedded}} Modeling Language for Convex Optimization},
  shorttitle = {{{CVXPY}}},
  author = {Diamond, Steven and Boyd, Stephen},
  year = {2016},
  journal = {Journal of Machine Learning Research},
  volume = {17},
  number = {83},
  pages = {1--5},
  url = {https://www.jmlr.org/papers/v17/15-408.html},
  urldate = {2024-08-01},
  keywords = {obsLitNote},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Diamond16convexOptCVXPY.pdf}
}

@misc{Saleh21SolutionNonMonotonicityCrossing,
  title = {Solution to the {{Non-Monotonicity}} and {{Crossing Problems}} in {{Quantile Regression}}},
  author = {Saleh, Resve A. and Saleh, A. K. Md Ehsanes},
  year = {2021},
  month = nov,
  number = {arXiv:2111.04805},
  eprint = {2111.04805},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.04805},
  url = {http://arxiv.org/abs/2111.04805},
  urldate = {2024-08-01},
  abstract = {This paper proposes a new method to address the long-standing problem of lack of monotonicity in estimation of the conditional and structural quantile function, also known as quantile crossing problem. Quantile regression is a very powerful tool in data science in general and econometrics in particular. Unfortunately, the crossing problem has been confounding researchers and practitioners alike for over 4 decades. Numerous attempts have been made to find a simple and general solution. This paper describes a unique and elegant solution to the problem based on a flexible check function that is easy to understand and implement in R and Python, while greatly reducing or even eliminating the crossing problem entirely. It will be very important in all areas where quantile regression is routinely used and may also find application in robust regression, especially in the context of machine learning. From this perspective, we also utilize the flexible check function to provide insights into the root causes of the crossing problem.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Comment: 8 pages, 14 figures, IEEE conference format},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Saleh21crossingQuantileSolution.pdf}
}

@article{Bondell10linSplnCnstrnQRnoncross,
  title = {Noncrossing Quantile Regression Curve Estimation},
  author = {Bondell, Howard D. and Reich, Brian J. and Wang, Huixia},
  year = {2010},
  month = dec,
  journal = {Biometrika},
  volume = {97},
  number = {4},
  pages = {825--838},
  issn = {0006-3444},
  doi = {10.1093/biomet/asq048},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3371721/},
  urldate = {2024-08-02},
  abstract = {Since quantile regression curves are estimated individually, the quantile curves can cross, leading to an invalid distribution for the response. A simple constrained version of quantile regression is proposed to avoid the crossing problem for both linear and nonparametric quantile curves. A simulation study and a reanalysis of tropical cyclone intensity data shows the usefulness of the procedure. Asymptotic properties of the estimator are equivalent to the typical approach under standard conditions, and the proposed estimator reduces to the classical one if there is no crossing. The performance of the constrained estimator has shown significant improvement by adding smoothing and stability across the quantile levels.},
  pmcid = {PMC3371721},
  pmid = {22822254},
  keywords = {todo},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Bondell10linSplnCnstrnQRnoncross.pdf}
}

@article{Parzen62estProbDenseModePAVA,
  title = {On Estimation of a Probability Density Function and Mode},
  author = {Parzen, Emanuel},
  year = {1962},
  journal = {The annals of mathematical statistics},
  volume = {33},
  number = {3},
  eprint = {2237880},
  eprinttype = {jstor},
  pages = {1065--1076},
  publisher = {JSTOR},
  url = {https://www.jstor.org/stable/2237880},
  urldate = {2024-08-02},
  keywords = {obsLitNote},
  note = {Parzen62estProbDenseModePAVA
\par
PAVA is an O(n) algorithm for enforcing montonicity on fitted values, e.g. estimated quantiles. ~This can be used as a QR postprocessing step. ~It will be WAY faster than the optimizer calls I used for enforcing montonicity, way back at IWES.
\par
It's an example of isotonic regression, and \href{https://www.perplexity.ai/search/can-conformal-quantile-regress-KOOwapTiTtetET8qC4QXuQ\#6}{Perplexity says} it's implemented in \texttt{sklearn.isotonic.IsotonicRegression}
\par
\href{https://www.perplexity.ai/search/can-conformal-quantile-regress-KOOwapTiTtetET8qC4QXuQ\#5}{Perplexity thinks} this paper is a good refeence for PAVA},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Parzen62estProbDenseModePAVA.pdf}
}

@article{Chernozhukov21DistributionalConformalPrediction,
  title = {Distributional Conformal Prediction},
  author = {Chernozhukov, Victor and W{\"u}thrich, Kaspar and Zhu, Yinchu},
  year = {2021},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {48},
  pages = {e2107794118},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2107794118},
  url = {https://www.pnas.org/doi/10.1073/pnas.2107794118},
  urldate = {2024-08-02},
  abstract = {We propose a robust method for constructing conditionally valid prediction intervals based on models for conditional distributions such as quantile and distribution regression. Our approach can be applied to important prediction problems, including cross-sectional prediction, k--step-ahead forecasts, synthetic controls and counterfactual prediction, and individual treatment effects prediction. Our method exploits the probability integral transform and relies on permuting estimated ranks. Unlike regression residuals, ranks are independent of the predictors, allowing us to construct conditionally valid prediction intervals under heteroskedasticity. We establish approximate conditional validity under consistent estimation and provide approximate unconditional validity under model misspecification, under overfitting, and with time series data. We also propose a simple ``shape'' adjustment of our baseline method that yields optimal prediction intervals.},
  keywords = {todo},
  note = {Chernozhukov21DistribConformPred
\par
Predicts full distribution (cdf, I think), so no crossover problem. ~Is somehow conformal, and is applied to k step ahead forecasts.
\par
\begin{itemize}

\item Supplemental material is also attached.
\item original R code is \href{https://github.com/kwuthrich/Replication_DCP}{here}.
\item python implementation of paper is \href{https://github.com/TaeseongYoon/DCP}{here}
\item \href{https://mapie.readthedocs.io/en/stable/examples_regression/2-advanced-analysis/plot_conformal_predictive_distribution.html}{MAPIE lib an estimate cdf,} but this must be a different algorithm
\item compare to (Marcjasz et al., 2023)

\end{itemize}},
  file = {C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Chernozhukov21DistribConformPred_supp.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Chernozhukov21DistribConformPred.pdf;C\:\\Users\\scott\\OneDrive\\share\\ref\\obsidian\\Obsidian Share Vault\\lit\\lit_sources\\Chernozhukov21DistributionalConformalPrediction.pdf}
}

@article{Zecchin24conformProbPredMPC,
  title = {Forking {{Uncertainties}}: {{Reliable Prediction}} and {{Model Predictive Control With Sequence Models}} via {{Conformal Risk Control}}},
  shorttitle = {Forking {{Uncertainties}}},
  author = {Zecchin, Matteo and Park, Sangwoo and Simeone, Osvaldo},
  year = {2024},
  journal = {IEEE Journal on Selected Areas in Information Theory},
  volume = {5},
  pages = {44--61},
  issn = {2641-8770},
  doi = {10.1109/JSAIT.2024.3368229},
  url = {https://ieeexplore.ieee.org/abstract/document/10445122},
  urldate = {2024-08-03},
  abstract = {In many real-world problems, predictions are leveraged to monitor and control cyber-physical systems, demanding guarantees on the satisfaction of reliability and safety requirements. However, predictions are inherently uncertain, and managing prediction uncertainty presents significant challenges in environments characterized by complex dynamics and forking trajectories. In this work, we assume access to a pre-designed probabilistic implicit or explicit sequence model, which may have been obtained using model-based or model-free methods. We introduce probabilistic time series-conformal risk prediction (PTS-CRC), a novel post-hoc calibration procedure that operates on the predictions produced by any pre-designed probabilistic forecaster to yield reliable error bars. In contrast to existing art, PTS-CRC produces predictive sets based on an ensemble of multiple prototype trajectories sampled from the sequence model, supporting the efficient representation of forking uncertainties. Furthermore, unlike the state of the art, PTS-CRC can satisfy reliability definitions beyond coverage. This property is leveraged to devise a novel model predictive control (MPC) framework that addresses open-loop and closed-loop control problems under general average constraints on the quality or safety of the control policy. We experimentally validate the performance of PTS-CRC prediction and control by studying a number of use cases in the context of wireless networking. Across all the considered tasks, PTS-CRC predictors are shown to provide more informative predictive sets, as well as safe control policies with larger returns.},
  keywords = {todo},
  note = {Zecchin24conformProbPredMPC
\par
Maybe most interesting b/c scenarios are somehow used in a control problem: MPC, or some kind of risk minimizing control. ~I didn't read deep enough to get it, but maybe a hint for computationally tractiable battery control using scenaris -- possible calibrated normal copula scenarios?
\par
Conformal prediction and scenario generation by a kind of time series memory lookup, although in the end, in at least one of the examples, they seem to have used conformal-calibrated DeepAR to calibrate them, or maybe somehow to just give confidence intervals (I didn't get to the end)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zecchin24conformProbPredMPC.pdf}
}

@incollection{Patton13copulaMultivarTSfrcst,
  title = {Chapter 16 - {{Copula Methods}} for {{Forecasting Multivariate Time Series}}},
  booktitle = {Handbook of {{Economic Forecasting}}},
  author = {Patton, Andrew},
  editor = {Elliott, Graham and Timmermann, Allan},
  year = {2013},
  month = jan,
  series = {Handbook of {{Economic Forecasting}}},
  volume = {2},
  pages = {899--960},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-444-62731-5.00016-6},
  url = {https://www.sciencedirect.com/science/article/pii/B9780444627315000166},
  urldate = {2024-08-03},
  abstract = {Copula-based models provide a great deal of flexibility in modeling multivariate distributions, allowing the researcher to specify the models for the marginal distributions separately from the dependence structure (copula) that links them to form a joint distribution. In addition to flexibility, this often also facilitates estimation of the model in stages, reducing the computational burden. This chapter reviews the growing literature on copula-based models for economic and financial time series data, and discusses in detail methods for estimation, inference, goodness-of-fit testing, and model selection that are useful when working with these models. A representative data set of two daily equity index returns is used to illustrate all of the main results.},
  keywords = {todo},
  note = {Patton13copulaMultivarTSfrcst
\par
Basic Copulas, including time varying and conditional, along with tests for fit and tail correlations.
\par
Also
\par
\begin{itemize}

\item (Clarence and Bruno, 2015)

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Patton13copulaMultivarTSfrcst.pdf}
}

@article{Clarence15multiVarCopulaFrcstTS,
  title = {Forecasting Time Series with Multivariate Copulas},
  author = {Clarence, Simard and Bruno, R{\'e}millard},
  year = {2015},
  journal = {Dependence Modeling},
  volume = {3},
  number = {1},
  publisher = {De Gruyter},
  url = {https://econpapers.repec.org/article/vrsdemode/v_3a3_3ay_3a2015_3ai_3a1_3ap_3a24_3an_3a5.htm},
  urldate = {2024-08-04},
  abstract = {In this paper we present a forecasting method for time series using copula-based models for multivariate time series. We study how the performance of the predictions evolves when changing the strength of the different possible dependencies, as well as the structure of the dependence. We also look at the impact of the marginal distributions. The impact of estimation errors on the performance of the predictions is also considered. In all the experiments, we compare predictions from our multivariate method with predictions from the univariate version which has been introduced in the literature recently. To simplify implementation, a test of independence between univariate Markovian time series is proposed. Finally, we illustrate the methodology by a practical implementation with financial data.},
  keywords = {todo},
  note = {Clarence15multiVarCopulaFrcstTS
\par
More on conditional copulas, like conditional student t.
\par
Also covers this: (Patton, 2013)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Clarence15multiVarCopulaFrcstTS.pdf}
}

@article{Fakoor23quantileAggrConformMono,
  title = {Flexible {{Model Aggregation}} for {{Quantile Regression}}},
  author = {Fakoor, Rasool and Kim, Taesup and Mueller, Jonas and Smola, Alexander J. and Tibshirani, Ryan J.},
  year = {2023},
  journal = {Journal of Machine Learning Research},
  volume = {24},
  number = {162},
  pages = {1--45},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v24/22-0799.html},
  urldate = {2024-08-04},
  abstract = {Quantile regression is a fundamental problem in statistical learning motivated by a need to quantify uncertainty in predictions, or to model a diverse population without being overly reductive. For instance, epidemiological forecasts, cost estimates, and revenue predictions all benefit from being able to quantify the range of possible values accurately. As such, many models have been developed for this problem over many years of research in statistics, machine learning, and related fields. Rather than proposing yet another (new) algorithm for quantile regression we adopt a meta viewpoint: we investigate methods for aggregating any number of conditional quantile models, in order to improve accuracy and robustness. We consider weighted ensembles where weights may vary over not only individual models, but also over quantile levels, and feature values. All of the models we consider in this paper can be fit using modern deep learning toolkits, and hence are widely accessible (from an implementation point of view) and scalable. To improve the accuracy of the predicted quantiles (or equivalently, prediction intervals), we develop tools for ensuring that quantiles remain monotonically ordered, and apply conformal calibration methods. These can be used without any modification of the original library of base models. We also review some basic theory surrounding quantile aggregation and related scoring rules, and contribute a few new results to this literature (for example, the fact that post sorting or post isotonic regression can only improve the weighted interval score). Finally, we provide an extensive suite of empirical comparisons across 34 data sets from two different benchmark repositories.},
  keywords = {obsLitNote},
  note = {Fakoor23quantileAggrConformMono
\par
Interesting because:
\par
\begin{itemize}

\item discusses combining separate QR forecasts, improving accuracy
\item how to ensure cdf montonicity (useful for copula if nothing else)
\item isotonic regression
\item some conformal stuff that might be multi-quantile, since they're worried about monotonicity
\item is focused on deep learning but says techniques can be applied on other models.

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Fakoor23quantileAggrConformMono.pdf}
}

@misc{Luo24ConformalThresholdedIntervals,
  title = {Conformal {{Thresholded Intervals}} for {{Efficient Regression}}},
  author = {Luo, Rui and Zhou, Zhixin},
  year = {2024},
  month = jul,
  number = {arXiv:2407.14495},
  eprint = {2407.14495},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2407.14495},
  urldate = {2024-08-04},
  abstract = {This paper introduces Conformal Thresholded Intervals (CTI), a novel conformal regression method that aims to produce the smallest possible prediction set with guaranteed coverage. Unlike existing methods that rely on nested conformal framework and full conditional distribution estimation, CTI estimates the conditional probability density for a new response to fall into each interquantile interval using off-the-shelf multi-output quantile regression. CTI constructs prediction sets by thresholding the estimated conditional interquantile intervals based on their length, which is inversely proportional to the estimated probability density. The threshold is determined using a calibration set to ensure marginal coverage. Experimental results demonstrate that CTI achieves optimal performance across various datasets.},
  archiveprefix = {arXiv},
  keywords = {hasCode,todo},
  note = {Luo24conformThrshIntervRgrsson
\par
I think this is a way to monotonize and calibrate ordinary multiquantile regression outputs.
\par
Has code to test the algorithm and others.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Luo24conformThrshIntervRgrsson.pdf}
}

@misc{Xu24ConformalPredictionMultidimensional,
  title = {Conformal Prediction for Multi-Dimensional Time Series by Ellipsoidal Sets},
  author = {Xu, Chen and Jiang, Hanyang and Xie, Yao},
  year = {2024},
  month = may,
  number = {arXiv:2403.03850},
  eprint = {2403.03850},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.03850},
  url = {http://arxiv.org/abs/2403.03850},
  urldate = {2024-08-04},
  abstract = {Conformal prediction (CP) has been a popular method for uncertainty quantification because it is distribution-free, model-agnostic, and theoretically sound. For forecasting problems in supervised learning, most CP methods focus on building prediction intervals for univariate responses. In this work, we develop a sequential CP method called \${\textbackslash}texttt\{MultiDimSPCI\}\$ that builds prediction \${\textbackslash}textit\{regions\}\$ for a multivariate response, especially in the context of multivariate time series, which are not exchangeable. Theoretically, we estimate \${\textbackslash}textit\{finite-sample\}\$ high-probability bounds on the conditional coverage gap. Empirically, we demonstrate that \${\textbackslash}texttt\{MultiDimSPCI\}\$ maintains valid coverage on a wide range of multivariate time series while producing smaller prediction regions than CP and non-CP baselines.},
  archiveprefix = {arXiv},
  keywords = {obsLitNote},
  note = {Xu24conformPredMultiDimTS
\par
Multivariate forecasting of some kind, but probably not multi quantile. ~Could be a modern equivalent to Pinson's polyhedral forecasts, if the variables multistep forecasts.
\par
Comment: Accepted by the Forty-first International Conference on Machine Learning (ICML 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Xu24conformPredMultiDimTS.pdf}
}

@misc{Zhou24ConformalizedAdaptiveForecasting,
  title = {Conformalized {{Adaptive Forecasting}} of {{Heterogeneous Trajectories}}},
  author = {Zhou, Yanfei and Lindemann, Lars and Sesia, Matteo},
  year = {2024},
  month = may,
  number = {arXiv:2402.09623},
  eprint = {2402.09623},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.09623},
  urldate = {2024-08-04},
  abstract = {This paper presents a new conformal method for generating simultaneous forecasting bands guaranteed to cover the entire path of a new random trajectory with sufficiently high probability. Prompted by the need for dependable uncertainty estimates in motion planning applications where the behavior of diverse objects may be more or less unpredictable, we blend different techniques from online conformal prediction of single and multiple time series, as well as ideas for addressing heteroscedasticity in regression. This solution is both principled, providing precise finite-sample guarantees, and effective, often leading to more informative predictions than prior methods.},
  archiveprefix = {arXiv},
  keywords = {hasCode,todo},
  note = {Zhou24cnfrmAdaptMultiStepHetTrajFrcst
\par
Uses deterministic forecasts to do mult-step ahead forecasts that are adaptive in some way. ~Has code.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zhou24cnfrmAdaptMultiStepHetTrajFrcst.pdf}
}

@misc{Huang24AdaptiveUncertaintyQuantification,
  title = {Adaptive {{Uncertainty Quantification}} for {{Trajectory Prediction Under Distributional Shift}}},
  author = {Huang, Huiqun and He, Sihong and Miao, Fei},
  year = {2024},
  month = jun,
  number = {arXiv:2406.12100},
  eprint = {2406.12100},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.12100},
  url = {http://arxiv.org/abs/2406.12100},
  urldate = {2024-08-04},
  abstract = {Trajectory prediction models that can infer both finite future trajectories and their associated uncertainties of the target vehicles in an online setting (e.g., real-world application scenarios) is crucial for ensuring the safe and robust navigation and path planning of autonomous vehicle motion. However, the majority of existing trajectory prediction models have neither considered reducing the uncertainty as one objective during the training stage nor provided reliable uncertainty quantification during inference stage under potential distribution shift. Therefore, in this paper, we propose the Conformal Uncertainty Quantification under Distribution Shift framework, CUQDS, to quantify the uncertainty of the predicted trajectories of existing trajectory prediction models under potential data distribution shift, while considering improving the prediction accuracy of the models and reducing the estimated uncertainty during the training stage. Specifically, CUQDS includes 1) a learning-based Gaussian process regression module that models the output distribution of the base model (any existing trajectory prediction or time series forecasting neural networks) and reduces the estimated uncertainty by additional loss term, and 2) a statistical-based Conformal P control module to calibrate the estimated uncertainty from the Gaussian process regression module in an online setting under potential distribution shift between training and testing data.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Huang24adaptTrajUncertDistShift
\par
Starts with some kind of GP forecast, and this is adjusted, somehow considering the possibility of concept drift.
\par
Comment: 9 pages, 2 figures},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Huang24adaptTrajUncertDistShift.pdf}
}

@misc{Guha24ConformalPredictionRegressionasClassification,
  title = {Conformal {{Prediction}} via {{Regression-as-Classification}}},
  author = {Guha, Etash and Natarajan, Shlok and M{\"o}llenhoff, Thomas and Khan, Mohammad Emtiyaz and Ndiaye, Eugene},
  year = {2024},
  month = apr,
  number = {arXiv:2404.08168},
  eprint = {2404.08168},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.08168},
  url = {http://arxiv.org/abs/2404.08168},
  urldate = {2024-08-04},
  abstract = {Conformal prediction (CP) for regression can be challenging, especially when the output distribution is heteroscedastic, multimodal, or skewed. Some of the issues can be addressed by estimating a distribution over the output, but in reality, such approaches can be sensitive to estimation error and yield unstable intervals.{\textasciitilde}Here, we circumvent the challenges by converting regression to a classification problem and then use CP for classification to obtain CP sets for regression.{\textasciitilde}To preserve the ordering of the continuous-output space, we design a new loss function and make necessary modifications to the CP classification techniques.{\textasciitilde}Empirical results on many benchmarks shows that this simple approach gives surprisingly good results on many practical problems.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Comment: International Conference of Learning Representations 2024
\par
Guha24conformRgrssnAsClassif
\par
Uses conformal classification techniques to predict distribution (I think) of continuous value. ~Looks pretty simple. Similar to StephenV and my multitask regression thing? ~Or could it be converted into that?
\par
Is compared to top conformal methods inclucing Chernoukov's distribution regression paper, and others.
\par
Comment: International Conference of Learning Representations 2024},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Guha24conformRgrssnAsClassif.pdf}
}

@article{Uniejewski21RegularizedQuantileRegression,
  title = {Regularized Quantile Regression Averaging for Probabilistic Electricity Price Forecasting},
  author = {Uniejewski, Bartosz and Weron, Rafa{\l}},
  year = {2021},
  month = mar,
  journal = {Energy Economics},
  volume = {95},
  pages = {105121},
  issn = {0140-9883},
  doi = {10.1016/j.eneco.2021.105121},
  url = {https://www.sciencedirect.com/science/article/pii/S0140988321000268},
  urldate = {2024-08-04},
  abstract = {Quantile Regression Averaging (QRA) has sparked interest in the electricity price forecasting community after its unprecedented success in the Global Energy Forecasting Competition 2014, where the top two winning teams in the price track used variants of QRA. However, recent studies have reported the method's vulnerability to low quality predictors when the set of regressors is larger than just a few. To address this issue, we consider a regularized variant of QRA, which utilizes the Least Absolute Shrinkage and Selection Operator (LASSO) to automatically select the relevant regressors. We evaluate the introduced technique -- dubbed LASSO QRA or LQRA for short -- using datasets from the Polish and Nordic power markets. By comparing against a number of benchmarks, we provide evidence for its superior predictive performance in terms of the Kupiec test, the pinball score and the test for conditional predictive accuracy, as well as financial profits for a range of trading strategies, especially when the regularization parameter is selected ex-ante using the Bayesian Information Criterion (BIC). As such, we offer an efficient tool that can be used to boost the profitability of energy trading activities, help with bidding in day-ahead markets and improve risk management practices in the power sector.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Uniejewski21qntlAvgPriceFrcst.pdf}
}

@misc{Zakrzewski24ReModelsQuantileRegression,
  title = {{{ReModels}}: {{Quantile Regression Averaging}} Models},
  shorttitle = {{{ReModels}}},
  author = {Zakrzewski, Grzegorz and Skonieczka, Kacper and Ma{\l}ki{\'n}ski, Miko{\l}aj and Ma{\'n}dziuk, Jacek},
  year = {2024},
  month = may,
  number = {arXiv:2405.11372},
  eprint = {2405.11372},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2405.11372},
  urldate = {2024-08-04},
  abstract = {Electricity price forecasts play a crucial role in making key business decisions within the electricity markets. A focal point in this domain are probabilistic predictions, which delineate future price values in a more comprehensive manner than simple point forecasts. The golden standard in probabilistic approaches to predict energy prices is the Quantile Regression Averaging (QRA) method. In this paper, we present a Python package that encompasses the implementation of QRA, along with modifications of this approach that have appeared in the literature over the past few years. The proposed package also facilitates the acquisition and preparation of data related to electricity markets, as well as the evaluation of model predictions.},
  archiveprefix = {arXiv},
  keywords = {hasCode,todo},
  note = {Zakrzewski24elecPriceFrcstQRavgLib
\par
Generates prob forecasts by doing QR (linear? always?) on an esemble of electricity price forecasts. ~This did great in 2014, but how is it doing these days, especially compared to conformal or other things. ~But it's a library so it would be an easy baseline, if I can figure out how forecast horizons, etc. are configured.
\par
A library of recent methods that forecast electricity prices by quantile averaging. ~Is said to be the gold standard of electricity price forecasting, and several other papers laud it.
\par
If nothing else, it could be a QR monotonizer, an input to some copula.
\par
Strange idea to me, although I've heard of QR avg. in other contexts too. ~These guys (Kath and Ziel, 2021) give it credit, as do others.
\par
Compare with (Jiang et al., 2024)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Zakrzewski24elecPriceFrcstQRavgLib.pdf}
}

@article{Jiang24elecPriceFrcstQuantAvgNCregu,
  title = {Electricity Price Forecasting Using Quantile Regression Averaging with Nonconvex Regularization},
  author = {Jiang, He and Dong, Yao and Wang, Jianzhou},
  year = {2024},
  journal = {Journal of Forecasting},
  volume = {43},
  number = {6},
  pages = {1859--1879},
  issn = {1099-131X},
  doi = {10.1002/for.3103},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3103},
  urldate = {2024-08-04},
  abstract = {Electricity price forecasting (EPF) is an emergent research domain that focuses on forecasting the future electricity market price both deterministically and probabilistically. EPF has attracted enormous interest from both practitioners and scholars since the deregulation of the power market and wide applications of renewable energy sources, such as wind and solar energy. However, forecasting the electricity price accurately and efficiently is an extremely challenging task because of its high volatility, randomness, and fluctuation. Although quantile regression averaging (QRA) has been demonstrated to be efficacious in probabilistic EPF since the global energy forecasting competition in 2014 (GEFCom2014), it is sensitive to nuisance variables especially when the number of variables is large. The forecasting accuracy will be negatively affected by these nuisance variables. To address these challenges, this study investigates a nonconvex regularized QRA in probabilistic forecasting. Two types of nonconvex regularized QRA select the important inputs obtained from point forecasting to obtain more accurate forecasting outcomes. To demonstrate the effectiveness of the proposed EPF model, two real datasets from the European power market are considered.},
  copyright = {{\copyright} 2024 John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {todo},
  note = {Jiang24elecPriceFrcstQuantAvgNCregu
\par
Seems to be feature selection for quantile averaging, which I guess is extra touching w/ extra variables. ~
\par
Compare with (Zakrzewski et al., 2024)
\par
Gives quantile averaging credit: (Kath and Ziel, 2021)}
}

@article{Kato23nonconformMeasReview,
  title = {A Review of Nonconformity Measures for Conformal Prediction in Regression},
  author = {Kato, Yuko and Tax, David MJ and Loog, Marco},
  year = {2023},
  journal = {Conformal and Probabilistic Prediction with Applications},
  pages = {369--383},
  publisher = {PMLR},
  url = {https://proceedings.mlr.press/v204/kato23a.html},
  urldate = {2024-08-04},
  keywords = {todo},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kato23nonconformMeasReview.pdf}
}

@misc{Plassier24ConditionallyValidProbabilistic,
  title = {Conditionally Valid {{Probabilistic Conformal Prediction}}},
  author = {Plassier, Vincent and Fishkov, Alexander and Panov, Maxim and Moulines, Eric},
  year = {2024},
  month = jul,
  number = {arXiv:2407.01794},
  eprint = {2407.01794},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.01794},
  url = {http://arxiv.org/abs/2407.01794},
  urldate = {2024-08-04},
  abstract = {We develop a new method for creating prediction sets that combines the flexibility of conformal methods with an estimate of the conditional distribution \$P\_\{Y {\textbackslash}mid X\}\$. Most existing methods, such as conformalized quantile regression and probabilistic conformal prediction, only offer marginal coverage guarantees. Our approach extends these methods to achieve conditional coverage, which is essential for many practical applications. While exact conditional guarantees are impossible without assumptions on the data distribution, we provide non-asymptotic bounds that explicitly depend on the quality of the available estimate of the conditional distribution. Our confidence sets are highly adaptive to the local structure of the data, making them particularly useful in high heteroskedasticity situations. We demonstrate the effectiveness of our approach through extensive simulations, showing that it outperforms existing methods in terms of conditional coverage and improves the reliability of statistical inference in a wide range of applications.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Plassier24condConformPred
\par
Confidence level are conditional. ~Seems like other new algorithms offer this too. ~Maybe that's what this algorithm is being compared with in this paper.
\par
Comment: 23 pages},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Plassier24ConditionallyValidProbabilistic.pdf}
}

@article{Marcjasz23elecPriceFrcstDistribNN,
  title = {Distributional Neural Networks for Electricity Price Forecasting},
  author = {Marcjasz, Grzegorz and Narajewski, Micha{\l} and Weron, Rafa{\l} and Ziel, Florian},
  year = {2023},
  journal = {Energy Economics},
  volume = {125},
  pages = {106843},
  publisher = {Elsevier},
  url = {https://www.sciencedirect.com/science/article/pii/S0140988323003419},
  urldate = {2024-08-04},
  note = {Marcjasz23elecPriceFrcstDistribNN
\par
A NN forecast that predicts a JohnsoSU distribution, like Axel kinda did, way back in the day. ~Works well electricity price forecasting.
\par
Could be a copula marginal transform.
\par
Compare to
\par
\begin{itemize}

\item (Chernozhukov et al., 2021)
\item (Foresi and Peracchi, 1995)
\item 

\end{itemize}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Marcjasz23DistributionalNeuralNetworks.pdf}
}

@misc{Brusaferri24OnlineConformalizedNeural,
  title = {On-Line Conformalized Neural Networks Ensembles for Probabilistic Forecasting of Day-Ahead Electricity Prices},
  author = {Brusaferri, Alessandro and Ballarino, Andrea and Grossi, Luigi and Laurini, Fabrizio},
  year = {2024},
  month = jun,
  number = {arXiv:2404.02722},
  eprint = {2404.02722},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2404.02722},
  urldate = {2024-08-04},
  abstract = {Probabilistic electricity price forecasting (PEPF) is subject of increasing interest, following the demand for proper quantification of prediction uncertainty, to support the operation in complex power markets with increasing share of renewable generation. Distributional neural networks ensembles have been recently shown to outperform state of the art PEPF benchmarks. Still, they require critical reliability enhancements, as fail to pass the coverage tests at various steps on the prediction horizon. In this work, we propose a novel approach to PEPF, extending the state of the art neural networks ensembles based methods through conformal inference based techniques, deployed within an on-line recalibration procedure. Experiments have been conducted on multiple market regions, achieving day-ahead forecasts with improved hourly coverage and stable probabilistic scores.},
  archiveprefix = {arXiv},
  keywords = {hasCode,todo},
  note = {Brusaferri24onlineConformElectPrice
\par
Adaptive price forecast based on conformal methods. ~Has a library implementing several methods, including (Marcjasz et al., 2023). ~
\par
Comment: 48 pages
\par
Comment: 48 pages},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Brusaferri24OnlineConformalizedNeural.pdf}
}

@article{Wang23eqOpptyCovFairRgrssn,
  title = {Equal {{Opportunity}} of {{Coverage}} in {{Fair Regression}}},
  author = {Wang, Fangxin and Cheng, Lu and Guo, Ruocheng and Liu, Kay and Yu, Philip S.},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {7743--7755},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/1849b94ed817ae7043a6b6934ef410c1-Abstract-Conference.html},
  urldate = {2024-08-04},
  langid = {english},
  keywords = {todo},
  note = {Brusaferri24onlineConformElectPrice
\par
I don't quite get this, but is it like the problem I had when CRPS underestimated high wind power because there were fewer samples (I think I concluded, way back at IWES).
\par
Somehow readjusting quantiles to ensure equally tight conformal coverage accross demographic groups? ~Bin by income, for example to get info for correction.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wang23eqOpptyCovFairRgrssn.pdf}
}

@inproceedings{Diamant24conformDeepSplinesNN,
  title = {Conformalized {{Deep Splines}} for {{Optimal}} and {{Efficient Prediction Sets}}},
  booktitle = {Proceedings of {{The}} 27th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Diamant, Nathaniel and Hajiramezanali, Ehsan and Biancalani, Tommaso and Scalia, Gabriele},
  year = {2024},
  month = apr,
  pages = {1657--1665},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v238/diamant24a.html},
  urldate = {2024-08-04},
  abstract = {Uncertainty estimation is critical in high-stakes machine learning applications. One effective way to estimate uncertainty is conformal prediction, which can provide predictive inference with statistical coverage guarantees. We present a new conformal regression method, Spline Prediction Intervals via Conformal Estimation (SPICE), that estimates the conditional density using neural- network-parameterized splines. We prove universal approximation and optimality results for SPICE, which are empirically reflected by our experiments. SPICE is compatible with two different efficient-to- compute conformal scores, one designed for size-efficient marginal coverage (SPICE-ND) and the other for size-efficient conditional coverage (SPICE-HPD). Results on benchmark datasets demonstrate SPICE-ND models achieve the smallest average prediction set sizes, including average size reductions of nearly 50\% for some datasets compared to the next best baseline. SPICE-HPD models achieve the best conditional coverage compared to baselines. The SPICE implementation is made available.},
  langid = {english},
  keywords = {todo},
  note = {Diamant24conformDeepSplinesNN
\par
An NN the predictons a distribution, I think, with splines. ~Compare to (Marcjasz et al., 2023; Chernozhukov et al., 2021; Mikolov et al., 2013)},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Diamant24conformDeepSplinesNN.pdf}
}

@article{Kath21conformPredElecPriceFrcst,
  title = {Conformal Prediction Interval Estimation and Applications to Day-Ahead and Intraday Power Markets},
  author = {Kath, Christopher and Ziel, Florian},
  year = {2021},
  journal = {International Journal of Forecasting},
  volume = {37},
  number = {2},
  pages = {777--799},
  publisher = {Elsevier},
  url = {https://www.sciencedirect.com/science/article/pii/S0169207020301473},
  urldate = {2024-08-04},
  keywords = {todo},
  note = {Kath21conformPredElecPriceFrcst
\par
Conformal forecasting of DA and ID elec price markets. ~Performance metric is interval length (Winkler, I think) rather than quantile score, etc. ~So it doesn't measure how well this would work in a copula. ~Also, the benchmarks its tested against aren't strong e.g. SVM, etc..
\par
Still, it's specifically on price forecasting.
\par
Also gives quantile averaging (Zakrzewski et al., 2024) much credit for elect price forecasting.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kath21conformPredElecPriceFrcst.pdf}
}

@misc{Dutot24AdaptiveProbabilisticForecasting,
  title = {Adaptive Probabilistic Forecasting of {{French}} Electricity Spot Prices},
  author = {Dutot, Gr{\'e}goire and Zaffran, Margaux and F{\'e}ron, Olivier and Goude, Yannig},
  year = {2024},
  month = may,
  number = {arXiv:2405.15359},
  eprint = {2405.15359},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.15359},
  url = {http://arxiv.org/abs/2405.15359},
  urldate = {2024-08-04},
  abstract = {Electricity price forecasting (EPF) plays a major role for electricity companies as a fundamental entry for trading decisions or energy management operations. As electricity can not be stored, electricity prices are highly volatile which make EPF a particularly difficult task. This is all the more true when dramatic fortuitous events disrupt the markets. Trading and more generally energy management decisions require risk management tools which are based on probabilistic EPF (PEPF). In this challenging context, we argue in favor of the deployment of highly adaptive black-boxes strategies allowing to turn any forecasts into a robust adaptive predictive interval, such as conformal prediction and online aggregation, as a fundamental last layer of any operational pipeline. We propose to investigate a novel data set containing the French electricity spot prices during the turbulent 2020-2021 years, and build a new explanatory feature revealing high predictive power, namely the nuclear availability. Benchmarking state-of-the-art PEPF on this data set highlights the difficulty of choosing a given model, as they all behave very differently in practice, and none of them is reliable. However, we propose an adequate conformalisation, OSSCP-horizon, that improves the performances of PEPF methods, even in the most hazardous period of late 2021. Finally, we emphasize that combining it with online aggregation significantly outperforms any other approaches, and should be the preferred pipeline, as it provides trustworthy probabilistic forecasts.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Dutot24elecPriceFrcstCnfrmProbAdaptFrence
\par
A big review of prob forecasts for electricity price forecasting. ~Much is conformal, and specific to forecasting and is adaptive.
\par
\textbf{Probably worth reading early}},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Dutot24elecPriceFrcstCnfrmProbAdaptFrence.pdf}
}

@misc{Cabezas24RegressionTreesFast,
  title = {Regression {{Trees}} for {{Fast}} and {{Adaptive Prediction Intervals}}},
  author = {Cabezas, Luben M. C. and Otto, Mateus P. and Izbicki, Rafael and Stern, Rafael B.},
  year = {2024},
  month = feb,
  number = {arXiv:2402.07357},
  eprint = {2402.07357},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.07357},
  urldate = {2024-08-04},
  abstract = {Predictive models make mistakes. Hence, there is a need to quantify the uncertainty associated with their predictions. Conformal inference has emerged as a powerful tool to create statistically valid prediction regions around point predictions, but its naive application to regression problems yields non-adaptive regions. New conformal scores, often relying upon quantile regressors or conditional density estimators, aim to address this limitation. Although they are useful for creating prediction bands, these scores are detached from the original goal of quantifying the uncertainty around an arbitrary predictive model. This paper presents a new, model-agnostic family of methods to calibrate prediction intervals for regression problems with local coverage guarantees. Our approach is based on pursuing the coarsest partition of the feature space that approximates conditional coverage. We create this partition by training regression trees and Random Forests on conformity scores. Our proposal is versatile, as it applies to various conformity scores and prediction settings and demonstrates superior scalability and performance compared to established baselines in simulated and real-world datasets. We provide a Python package clover that implements our methods using the standard scikit-learn interface.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Cabezas24rgrssnTreeAdaptConform
\par
Does conformal adptation by training regression trees on conformity scores. ~So kind of alike a switching regression confidence model?
\par
The switch seems troublesome, but maybe the regression is smooth? ~Could make it smoother with linear trees, as in LightGBM or XGBoost},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Cabezas24rgrssnTreeAdaptConform.pdf}
}

@misc{Ai24NotAllDistributional,
  title = {Not All Distributional Shifts Are Equal: {{Fine-grained}} Robust Conformal Inference},
  shorttitle = {Not All Distributional Shifts Are Equal},
  author = {Ai, Jiahao and Ren, Zhimei},
  year = {2024},
  month = feb,
  number = {arXiv:2402.13042},
  eprint = {2402.13042},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.13042},
  url = {http://arxiv.org/abs/2402.13042},
  urldate = {2024-08-04},
  abstract = {We introduce a fine-grained framework for uncertainty quantification of predictive models under distributional shifts. This framework distinguishes the shift in covariate distributions from that in the conditional relationship between the outcome (\$Y\$) and the covariates (\$X\$). We propose to reweight the training samples to adjust for an identifiable covariate shift while protecting against worst-case conditional distribution shift bounded in an \$f\$-divergence ball. Based on ideas from conformal inference and distributionally robust learning, we present an algorithm that outputs (approximately) valid and efficient prediction intervals in the presence of distributional shifts. As a use case, we apply the framework to sensitivity analysis of individual treatment effects with hidden confounding. The proposed methods are evaluated in simulation studies and three real data applications, demonstrating superior robustness and efficiency compared with existing benchmarks.},
  archiveprefix = {arXiv},
  note = {Ai24notAllDistShifsConform
\par
Interesting in that tries to distinguish kinds of concept drift: drift on covariates vs. drift in X-Y dependence. ~I like it.
\par
Does (Kasa et al., 2024) dect covariate drift?
\par
Comment: 25 pages, 5 figures},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Ai24notAllDistShifsConform.pdf}
}

@misc{Kasa24adaptConformDriftNoLabels,
  title = {Adapting {{Conformal Prediction}} to {{Distribution Shifts Without Labels}}},
  author = {Kasa, Kevin and Zhang, Zhiyu and Yang, Heng and Taylor, Graham W.},
  year = {2024},
  month = jun,
  number = {arXiv:2406.01416},
  eprint = {2406.01416},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.01416},
  url = {http://arxiv.org/abs/2406.01416},
  urldate = {2024-08-04},
  abstract = {Conformal prediction (CP) enables machine learning models to output prediction sets with guaranteed coverage rate, assuming exchangeable data. Unfortunately, the exchangeability assumption is frequently violated due to distribution shifts in practice, and the challenge is often compounded by the lack of ground truth labels at test time. Focusing on classification in this paper, our goal is to improve the quality of CP-generated prediction sets using only unlabeled data from the test domain. This is achieved by two new methods called ECP and EACP, that adjust the score function in CP according to the base model's uncertainty on the unlabeled test data. Through extensive experiments on a number of large-scale datasets and neural network architectures, we show that our methods provide consistent improvement over existing baselines and nearly match the performance of supervised algorithms.},
  archiveprefix = {arXiv},
  keywords = {obsLitNote},
  note = {Kasa24adaptConformDriftNoLabels
\par
Classification drift adapt. ~Quite interesting as it detects drifts w/o knowing the labels. ~Does it detect the covariate drift discussed in (Ai and Ren, 2024)?},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Kasa24AdaptingConformalPrediction.pdf}
}

@article{Hu24probTSpredmonotoneNN,
  title = {A Novel Time Series Probabilistic Prediction Approach Based on the Monotone Quantile Regression Neural Network},
  author = {Hu, Jianming and Tang, Jingwei and Liu, Zhi},
  year = {2024},
  month = jan,
  journal = {Information Sciences},
  volume = {654},
  pages = {119844},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2023.119844},
  url = {https://www.sciencedirect.com/science/article/pii/S0020025523014299},
  urldate = {2024-08-04},
  abstract = {Quantile regression is widely applied in various fields such as economy, energy, meteorological prediction research in recent years since it does not require distribution assumptions, has relatively loose conditions, and can effectively estimate the uncertainty of time series forecasting. In this paper, a monotone quantile regression neural network (MQRNN) framework is constructed for time series quantile forecasting. The proposed approach takes the monotonicity of quantile into consideration and handles the quantile crossing problem by adding the quantile information into the input structure and using the gradient based point-wise loss function. Aiming at the complex characteristics of time series, such as time-varying and asymmetric heavy-tailed features, a new quantile function is utilized to describe the complete conditional distribution information of data. Under this model framework, non-crossing multiple quantiles can be predicted simultaneously. The proposed approach is implemented based on artificial neural networks, and the constructed model is applied to actual data in different fields. The experimental results demonstrate that the proposed method combined with long short-term memory (LSTM) can provide accurate and reliable multi-quantile prediction, and alleviate the problem of quantile crossing.},
  note = {Hu24probTSpredmonotoneNN
\par
NN learns monotonic cdf (distribution) relationship. ~It's in the training algorithm, rather than a forced output (I think, can't find the paper).
\par
Compare to (Marcjasz et al., 2023), which predicts a JohnsonSU dist}
}

@misc{Pasche24NeuralNetworksExtreme,
  title = {Neural {{Networks}} for {{Extreme Quantile Regression}} with an {{Application}} to {{Forecasting}} of {{Flood Risk}}},
  author = {Pasche, Olivier C. and Engelke, Sebastian},
  year = {2024},
  month = may,
  number = {arXiv:2208.07590},
  eprint = {2208.07590},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.07590},
  url = {http://arxiv.org/abs/2208.07590},
  urldate = {2024-08-04},
  abstract = {Risk assessment for extreme events requires accurate estimation of high quantiles that go beyond the range of historical observations. When the risk depends on the values of observed predictors, regression techniques are used to interpolate in the predictor space. We propose the EQRN model that combines tools from neural networks and extreme value theory into a method capable of extrapolation in the presence of complex predictor dependence. Neural networks can naturally incorporate additional structure in the data. We develop a recurrent version of EQRN that is able to capture complex sequential dependence in time series. We apply this method to forecast flood risk in the Swiss Aare catchment. It exploits information from multiple covariates in space and time to provide one-day-ahead predictions of return levels and exceedance probabilities. This output complements the static return level from a traditional extreme value analysis, and the predictions are able to adapt to distributional shifts as experienced in a changing climate. Our model can help authorities to manage flooding more effectively and to minimize their disastrous impacts through early warning systems.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Pasche24extremQRNNfrcst
\par
how to predict extreme quantiles outside what was seen in the training data. ~Somehow interpolates across input data, and uses extreme value theory.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Pasche24NeuralNetworksExtreme.pdf}
}

@article{Jensen24ensembleConformQRfrcstTS,
  title = {Ensemble {{Conformalized Quantile Regression}} for {{Probabilistic Time Series Forecasting}}},
  author = {Jensen, Vilde and Bianchi, Filippo Maria and Anfinsen, Stian Normann},
  year = {2024},
  month = jul,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {35},
  number = {7},
  pages = {9014--9025},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2022.3217694},
  url = {https://ieeexplore.ieee.org/abstract/document/9940232},
  urldate = {2024-08-04},
  abstract = {This article presents a novel probabilistic forecasting method called ensemble conformalized quantile regression (EnCQR). EnCQR constructs distribution-free and approximately marginally valid prediction intervals (PIs), which are suitable for nonstationary and heteroscedastic time series data. EnCQR can be applied on top of a generic forecasting model, including deep learning architectures. EnCQR exploits a bootstrap ensemble estimator, which enables the use of conformal predictors for time series by removing the requirement of data exchangeability. The ensemble learners are implemented as generic machine learning algorithms performing quantile regression (QR), which allow the length of the PIs to adapt to local variability in the data. In the experiments, we predict time series characterized by a different amount of heteroscedasticity. The results demonstrate that EnCQR outperforms models based only on QR or conformal prediction (CP), and it provides sharper, more informative, and valid PIs.},
  keywords = {todo},
  note = {Jensen24ensembleConformQRfrcstTS
\par
Combines generic time series QR forecasts into a corrected adaptive conformal forecast.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Jensen24EnsembleConformalizedQuantile.pdf}
}

@inproceedings{Salinas23hyperParamOptConformal,
  title = {Optimizing {{Hyperparameters}} with {{Conformal Quantile Regression}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Salinas, David and Golebiowski, Jacek and Klein, Aaron and Seeger, Matthias and Archambeau, Cedric},
  year = {2023},
  month = jul,
  pages = {29876--29893},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/salinas23a.html},
  urldate = {2024-08-04},
  abstract = {Many state-of-the-art hyperparameter optimization (HPO) algorithms rely on model-based optimizers that learn surrogate models of the target function to guide the search. Gaussian processes are the de facto surrogate model due to their ability to capture uncertainty. However, they make strong assumptions about the observation noise, which might not be warranted in practice. In this work, we propose to leverage conformalized quantile regression which makes minimal assumptions about the observation noise and, as a result, models the target function in a more realistic and robust fashion which translates to quicker HPO convergence on empirical benchmarks. To apply our method in a multi-fidelity setting, we propose a simple, yet effective, technique that aggregates observed results across different resource levels and outperforms conventional methods across many empirical tasks.},
  langid = {english},
  keywords = {todo},
  note = {Salinas23hyperParamOptConformal
\par
Instead of using Gaussian Processes in Bayesian (I guess) hyperparameter optimization, use conformal QR instead. ~
\par
Is said to be faster. ~I'd like that. ~Interesting.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Salinas23OptimizingHyperparametersConformal.pdf}
}

@misc{Sebastian24EnhancingReliabilityPrediction,
  title = {Enhancing Reliability in Prediction Intervals Using Point Forecasters: {{Heteroscedastic Quantile Regression}} and {{Width-Adaptive Conformal Inference}}},
  shorttitle = {Enhancing Reliability in Prediction Intervals Using Point Forecasters},
  author = {Sebasti{\'a}n, Carlos and {Gonz{\'a}lez-Guill{\'e}n}, Carlos E. and Juan, Jes{\'u}s},
  year = {2024},
  month = jun,
  number = {arXiv:2406.14904},
  eprint = {2406.14904},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2406.14904},
  urldate = {2024-08-08},
  abstract = {Building prediction intervals for time series forecasting problems presents a complex challenge, particularly when relying solely on point predictors, a common scenario for practitioners in the industry. While research has primarily focused on achieving increasingly efficient valid intervals, we argue that, when evaluating a set of intervals, traditional measures alone are insufficient. There are additional crucial characteristics: the intervals must vary in length, with this variation directly linked to the difficulty of the prediction, and the coverage of the interval must remain independent of the difficulty of the prediction for practical utility. We propose the Heteroscedastic Quantile Regression (HQR) model and the Width-Adaptive Conformal Inference (WACI) method, providing theoretical coverage guarantees, to overcome those issues, respectively. The methodologies are evaluated in the context of Electricity Price Forecasting and Wind Power Forecasting, representing complex scenarios in time series forecasting. The results demonstrate that HQR and WACI not only improve or achieve typical measures of validity and efficiency but also successfully fulfil the commonly ignored mentioned characteristics.},
  archiveprefix = {arXiv},
  keywords = {todo},
  note = {Sebastian24ptFrcstHetQRadaptConform
\par
Somehow used point forecasts + quantile regression + adaptive width coformal to forecast wind and electricity prices.},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sebastian24ptFrcstHetQRadaptConform.pdf}
}

@article{Sabeti14AdditiveModelsConditional,
  title = {Additive Models for Conditional Copulas},
  author = {Sabeti, Avideh and Wei, Mian and Craiu, Radu V.},
  year = {2014},
  month = mar,
  journal = {Stat},
  volume = {3},
  number = {1},
  pages = {300--312},
  issn = {2049-1573, 2049-1573},
  doi = {10.1002/sta4.64},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/sta4.64},
  urldate = {2024-08-17},
  abstract = {Conditional copulas are flexible statistical tools that couple joint conditional and marginal conditional distributions. In a linear regression setting with more than one covariate and two dependent outcomes, we consider additive models for studying the dependence between covariates and the copula parameter. We examine the computation and model selection tools needed for Bayesian inference. The method is illustrated using simulations and a real example. Copyright {\copyright} 2014 John Wiley \& Sons, Ltd.},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Sabeti14AdditiveModelsConditional.pdf}
}

@misc{Uniejewski24ElectricityPriceForecasting,
  title = {Electricity Price Forecasting with {{Smoothing Quantile Regression Averaging}}: {{Quantifying}} Economic Benefits of Probabilistic Forecasts},
  shorttitle = {Electricity Price Forecasting with {{Smoothing Quantile Regression Averaging}}},
  author = {Uniejewski, Bartosz},
  year = {2024},
  month = jan,
  number = {arXiv:2302.00411},
  eprint = {2302.00411},
  primaryclass = {q-fin, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2302.00411},
  urldate = {2024-08-17},
  abstract = {In the world of the complex power market, accurate electricity price forecasting is essential for strategic bidding and affects both daily operations and long-term investments. This article introduce a new method dubbed Smoothing Quantile Regression (SQR) Averaging, that improves on well-performing schemes for probabilistic forecasting. To showcase its utility, a comprehensive study is conducted across four power markets, including recent data encompassing the COVID-19 pandemic and the Russian invasion on Ukraine. The performance of SQR Averaging is evaluated and compared to state-of-the-art benchmark methods in terms of the reliability and sharpness measures. Additionally, an evaluation scheme is introduced to quantify the economic benefits derived from SQR Averaging predictions. This scheme can be applied in any day-ahead electricity market and is based on a trading strategy that leverages battery storage and sets limit orders using selected quantiles of the predictive distribution. The results reveal that, compared to the benchmark strategy, utilizing SQR Averaging leads to average profit increases of up to 14\%. These findings provide strong evidence for the effectiveness of SQR Averaging in improving forecast accuracy and the practical value of utilizing probabilistic forecasts in day-ahead electricity trading, even in the face of challenging events such as the COVID-19 pandemic and geopolitical disruptions.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Uniejewski24ElectricityPriceForecasting.pdf}
}

@misc{Varma23ExplainingGrokkingCircuit,
  title = {Explaining Grokking through Circuit Efficiency},
  author = {Varma, Vikrant and Shah, Rohin and Kenton, Zachary and Kram{\'a}r, J{\'a}nos and Kumar, Ramana},
  year = {2023},
  month = sep,
  number = {arXiv:2309.02390},
  eprint = {2309.02390},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2309.02390},
  urldate = {2024-08-17},
  abstract = {One of the most surprising puzzles in neural network generalisation is grokking: a network with perfect training accuracy but poor generalisation will, upon further training, transition to perfect generalisation. We propose that grokking occurs when the task admits a generalising solution and a memorising solution, where the generalising solution is slower to learn but more efficient, producing larger logits with the same parameter norm. We hypothesise that memorising circuits become more inefficient with larger training datasets while generalising circuits do not, suggesting there is a critical dataset size at which memorisation and generalisation are equally efficient. We make and confirm four novel predictions about grokking, providing significant evidence in favour of our explanation. Most strikingly, we demonstrate two novel and surprising behaviours: ungrokking, in which a network regresses from perfect to low test accuracy, and semi-grokking, in which a network shows delayed generalisation to partial rather than perfect test accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Varma23ExplainingGrokkingCircuit.pdf}
}

@misc{Jeary24VerifiablyRobustConformal,
  title = {Verifiably {{Robust Conformal Prediction}}},
  author = {Jeary, Linus and Kuipers, Tom and Hosseini, Mehran and Paoletti, Nicola},
  year = {2024},
  month = jun,
  number = {arXiv:2405.18942},
  eprint = {2405.18942},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2405.18942},
  urldate = {2024-08-17},
  abstract = {Conformal Prediction (CP) is a popular uncertainty quantification method that provides distribution-free, statistically valid prediction sets, assuming that training and test data are exchangeable. In such a case, CP's prediction sets are guaranteed to cover the (unknown) true test output with a user-specified probability. Nevertheless, this guarantee is violated when the data is subjected to adversarial attacks, which often result in a significant loss of coverage. Recently, several approaches have been put forward to recover CP guarantees in this setting. These approaches leverage variations of randomised smoothing to produce conservative sets which account for the effect of the adversarial perturbations. They are, however, limited in that they only support {$\ell$}2-bounded perturbations and classification tasks. This paper introduces VRCP (Verifiably Robust Conformal Prediction), a new framework that leverages recent neural network verification methods to recover coverage guarantees under adversarial attacks. Our VRCP method is the first to support perturbations bounded by arbitrary norms including {$\ell$}1, {$\ell$}2, and {$\ell\infty$}, as well as regression tasks. We evaluate and compare our approach on image classification tasks (CIFAR10, CIFAR100, and TinyImageNet) and regression tasks for deep reinforcement learning environments. In every case, VRCP achieves above nominal coverage and yields significantly more efficient and informative prediction regions than the SotA.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Jeary24VerifiablyRobustConformal.pdf}
}

@misc{Matulin24ShortTermProbabilisticLoad,
  title = {Short-{{Term Probabilistic Load Forecasting Based}} on {{Conformalized Quantile Regression}}},
  author = {Matulin, Lucija and Capuder, Tomislav and Plav{\v s}i{\'c}, Tomislav},
  year = {2024},
  month = jan,
  doi = {10.46855/energy-proceedings-10898},
  url = {https://www.energy-proceedings.org/?p=10898},
  urldate = {2024-08-17},
  abstract = {Short-term load forecasting is a fundamental task in reliable and secure power system operation, particularly in the current landscape marked by increased integration of renewable energy sources and electric vehicles, which introduces stochasticity and raises uncertainty. To express uncertainty in load predictions in the form of a probabilistic forecast, prediction intervals are generated. The variability in load values exhibits higher volatility during the day due to increased human activities, contrasting with lower variability at night. Classic methods for constructing prediction intervals cannot correctly model the variability in uncertainty leading to overly conservative prediction intervals. In this paper, we propose a novel approach -- conformalized quantile regression -- to create more informative, variable-length prediction intervals. Experimental results, based on a real load dataset from the Croatian Transmission System, showcase the method's superior performance in capturing adaptivelength prediction intervals. This translates to achieving higher coverage with shorter prediction intervals compared to conventional methods.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Matulin24ShortTermProbabilisticLoad.pdf}
}

@article{Simard15ForecastingTimeSeries,
  title = {Forecasting Time Series with Multivariate Copulas},
  author = {Simard, Clarence and R{\'e}millard, Bruno},
  year = {2015},
  month = may,
  journal = {Dependence Modeling},
  volume = {3},
  number = {1},
  pages = {000010151520150005},
  issn = {2300-2298},
  doi = {10.1515/demo-2015-0005},
  url = {https://www.degruyter.com/document/doi/10.1515/demo-2015-0005/html},
  urldate = {2024-08-17},
  abstract = {In this paper we present a forecasting method for time series using copula-based models for multivariate time series. We study how the performance of the predictions evolves when changing the strength of the different possible dependencies, as well as the structure of the dependence. We also look at the impact of the marginal distributions. The impact of estimation errors on the performance of the predictions is also considered. In all the experiments, we compare predictions from our multivariate method with predictions from the univariate version which has been introduced in the literature recently. To simplify implementation, a test of independence between univariate Markovian time series is proposed. Finally, we illustrate the methodology by a practical implementation with financial data.},
  copyright = {http://creativecommons.org/licenses/by-nc-nd/3.0},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Simard15ForecastingTimeSeries.pdf}
}

@article{VallikiviImprovingUncertaintyQuantification,
  title = {Improving {{Uncertainty Quantification}} in {{Regression Problems}} through {{Conformal Training}}},
  author = {Vallikivi, Johannes},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\VallikiviImprovingUncertaintyQuantification.pdf}
}

@article{KohansalDatadrivenAnalysisSupply,
  title = {A {{Data-Driven Analysis}} of {{Supply Bids}} in {{Caiso Market}}: {{Price Elasticity}} and {{Impact}} of {{Renewables}}},
  author = {Kohansal, Mahdi and {Mohsenian-Rad}, Hamed},
  abstract = {One month of supply bids in the California ISO day-ahead energy market are analyzed in this paper. A total of 1.5 million records of bid data are studied. The bids are studied based on their types and their distribution at different hours. The relationship between the market price and the offered supply capacity are investigated. A data-driven estimate is provided for the aggregated supply curve and accordingly the price elasticity of supply is identified for hours that price is highly inelastic. Importantly, this analysis shows the impact of the recent high capacity installations of renewable generation in the state of California on electricity price and price inelasticity. Finally, the undesirable consequences of price inelasticity, such as on creating price spikes and exercising market power, are discussed.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\KohansalDatadrivenAnalysisSupply.pdf}
}

@article{MessoudiEllipsoidalConformalInference,
  title = {Ellipsoidal Conformal Inference for {{Multi-Target Regression}}},
  author = {Messoudi, Soundouss and Messoudi, Soundouss and Destercke, Sebastien and Destercke, Sebastien and Rousseau, Sylvain and Rousseau, Sylvain},
  abstract = {Quantifying the uncertainty of a predictive model output is of essential importance in learning scenarios involving critical applications. As the learning task becomes more complex, so does uncertainty quantification. In this paper, we consider the task of multi-target regression and propose a method to output ellipsoidal confidence regions whose shapes are tailored to each instance to predict. We also guarantee that those confidence regions are well-calibrated, i.e., that they cover the ground truth with a specified probability. To achieve such a feat, we propose a conformal prediction method outputting ellipsoidal prediction regions. Experiments on both simulated and real-world data sets show that our methods outperform existing ones.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\MessoudiEllipsoidalConformalInference.pdf}
}

@article{Alcantara23DeepNeuralNetworks,
  title = {Deep Neural Networks for the Quantile Estimation of Regional Renewable Energy Production},
  author = {Alc{\'a}ntara, Antonio and Galv{\'a}n, In{\'e}s M. and Aler, Ricardo},
  year = {2023},
  month = apr,
  journal = {Applied Intelligence},
  volume = {53},
  number = {7},
  pages = {8318--8353},
  issn = {0924-669X, 1573-7497},
  doi = {10.1007/s10489-022-03958-7},
  url = {https://link.springer.com/10.1007/s10489-022-03958-7},
  urldate = {2024-08-17},
  abstract = {Wind and solar energy forecasting have become crucial for the inclusion of renewable energy in electrical power systems. Although most works have focused on point prediction, it is currently becoming important to also estimate the forecast uncertainty. With regard to forecasting methods, deep neural networks have shown good performance in many fields. However, the use of these networks for comparative studies of probabilistic forecasts of renewable energies, especially for regional forecasts, has not yet received much attention. The aim of this article is to study the performance of deep networks for estimating multiple conditional quantiles on regional renewable electricity production and compare them with widely used quantile regression methods such as the linear, support vector quantile regression, gradient boosting quantile regression, natural gradient boosting and quantile regression forest methods. A grid of numerical weather prediction variables covers the region of interest. These variables act as the predictors of the regional model. In addition to quantiles, prediction intervals are also constructed, and the models are evaluated using different metrics. These prediction intervals are further improved through an adapted conformalized quantile regression methodology. Overall, the results show that deep networks are the best performing method for both solar and wind energy regions, producing narrow prediction intervals with good coverage.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Alcantara23DeepNeuralNetworks.pdf}
}

@misc{Durante22MultivariateDependenceAnalysis,
  title = {A {{Multivariate Dependence Analysis}} for {{Electricity Prices}}, {{Demand}} and {{Renewable Energy Sources}}},
  author = {Durante, Fabrizio and Gianfreda, Angelica and Ravazzolo, Francesco and Rossini, Luca},
  year = {2022},
  month = jan,
  number = {arXiv:2201.01132},
  eprint = {2201.01132},
  primaryclass = {econ, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2201.01132},
  urldate = {2024-08-17},
  abstract = {This paper examines the dependence between electricity prices, demand, and renewable energy sources by means of a multivariate copula model while studying Germany, the widest studied market in Europe. The inter-dependencies are investigated in-depth and monitored over time, with particular emphasis on the tail behavior. To this end, suitable tail dependence measures are introduced to take into account a multivariate extreme scenario appropriately identified through the Kendall's distribution function. The empirical evidence demonstrates a strong association between electricity prices, renewable energy sources, and demand within a day and over the studied years. Hence, this analysis provides guidance for further and different incentives for promoting green energy generation while considering the timevarying dependencies of the involved variables.},
  archiveprefix = {arXiv},
  langid = {english},
  note = {Comment: Forthcoming in "Information Sciences"},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Durante22MultivariateDependenceAnalysis.pdf}
}

@article{GibbsAdaptiveConformalInferencea,
  title = {Adaptive {{Conformal Inference Under Distribution Shift}}},
  author = {Gibbs, Isaac and Cand{\`e}s, Emmanuel J},
  abstract = {We develop methods for forming prediction sets in an online setting where the data generating distribution is allowed to vary over time in an unknown fashion. Our framework builds on ideas from conformal inference to provide a general wrapper that can be combined with any black box method that produces point predictions of the unseen label or estimated quantiles of its distribution. While previous conformal inference methods rely on the assumption that the data points are exchangeable, our adaptive approach provably achieves the desired coverage frequency over long-time intervals irrespective of the true data generating process. We accomplish this by modelling the distribution shift as a learning problem in a single parameter whose optimal value is varying over time and must be continuously re-estimated. We test our method, adaptive conformal inference, on two real world datasets and find that its predictions are robust to visible and significant distribution shifts.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\GibbsAdaptiveConformalInferencea.pdf}
}

@article{LinConformalPredictionTemporal,
  title = {Conformal {{Prediction}} with {{Temporal Quantile Adjustments}}},
  author = {Lin, Zhen and Trivedi, Shubhendu and Sun, Jimeng},
  abstract = {We develop Temporal Quantile Adjustment (TQA), a general method to construct efficient and valid prediction intervals (PIs) for regression on cross-sectional time series data. Such data is common in many domains, including econometrics and healthcare. A canonical example in healthcare is predicting patient outcomes using physiological time-series data, where a population of patients composes a cross-section. Reliable PI estimators in this setting must address two distinct notions of coverage: cross-sectional coverage across a cross-sectional slice, and longitudinal coverage along the temporal dimension for each time series. Recent works have explored adapting Conformal Prediction (CP) to obtain PIs in the time series context. However, none handles both notions of coverage simultaneously. CP methods typically query a pre-specified quantile from the distribution of nonconformity scores on a calibration set. TQA adjusts the quantile to query in CP at each time t, accounting for both cross-sectional and longitudinal coverage in a theoretically-grounded manner. The post-hoc nature of TQA facilitates its use as a general wrapper around any time series regression model. We validate TQA's performance through extensive experimentation: TQA generally obtains efficient PIs and improves longitudinal coverage while preserving cross-sectional coverage. Our code is available at https://github.com/zlin7/TQA.},
  langid = {english},
  keywords = {hasCode},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\LinConformalPredictionTemporal.pdf}
}

@incollection{Patton13CopulaMethodsForecasting,
  title = {Copula {{Methods}} for {{Forecasting Multivariate Time Series}}},
  booktitle = {Handbook of {{Economic Forecasting}}},
  author = {Patton, Andrew},
  year = {2013},
  volume = {2},
  pages = {899--960},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-444-62731-5.00016-6},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9780444627315000166},
  urldate = {2024-08-17},
  abstract = {Copula-based models provide a great deal of {\'a}exibility in modelling multivariate distributions, allowing the researcher to specify the models for the marginal distributions separately from the dependence structure (copula) that links them to form a joint distribution. In addition to {\'a}exibility, this often also facilitates estimation of the model in stages, reducing the computational burden. This chapter reviews the growing literature on copula-based models for economic and {\"O}nancial time series data, and discusses in detail methods for estimation, inference, goodness-of-{\"O}t testing, and model selection that are useful when working with these models. A representative data set of two daily equity index returns is used to illustrate all of the main results.},
  isbn = {978-0-444-62731-5},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Patton13CopulaMethodsForecasting.pdf}
}

@article{GallegoInverseOptimizationForecasting,
  title = {Inverse {{Optimization}} and {{Forecasting Techniques Applied}} to {{Decision-making}} in {{Electricity Markets}}},
  author = {Gallego, Javier S{\'a}ez},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\GallegoInverseOptimizationForecasting.pdf}
}

@article{Nowotarski18RecentAdvancesElectricity,
  title = {Recent Advances in Electricity Price Forecasting: {{A}} Review of Probabilistic Forecasting},
  shorttitle = {Recent Advances in Electricity Price Forecasting},
  author = {Nowotarski, Jakub and Weron, Rafa{\l}},
  year = {2018},
  month = jan,
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {81},
  pages = {1548--1568},
  issn = {13640321},
  doi = {10.1016/j.rser.2017.05.234},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364032117308808},
  urldate = {2024-08-17},
  abstract = {Since the inception of competitive power markets two decades ago, electricity price forecasting (EPF) has gradually become a fundamental process for energy companies' decision making mechanisms. Over the years, the bulk of research has concerned point predictions. However, the recent introduction of smart grids and renewable integration requirements has had the effect of increasing the uncertainty of future supply, demand and prices. Academics and practitioners alike have come to understand that probabilistic electricity price (and load) forecasting is now more important for energy systems planning and operations than ever before. With this paper we offer a tutorial review of probabilistic EPF and present much needed guidelines for the rigorous use of methods, measures and tests, in line with the paradigm of `maximizing sharpness subject to reliability'. The paper can be treated as an update and a further extension of the otherwise comprehensive EPF review of Weron [1] or as a standalone treatment of a fascinating and underdeveloped topic, that has a much broader reach than EPF itself.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Nowotarski18RecentAdvancesElectricity.pdf}
}

@misc{Wachs19ReliabilityRenewablesModeling,
  title = {Reliability versus {{Renewables}}: {{Modeling Decision Making}} in {{CAISO}} and {{PJM}} to {{Identify Market Barriers}} and {{Supports}} to {{Renewable Energy Adoption}} in the {{US}}},
  shorttitle = {Reliability versus {{Renewables}}},
  author = {Wachs, Elizabeth and Engel, Bernard},
  year = {2019},
  month = jun,
  doi = {10.31224/osf.io/vngzq},
  url = {https://engrxiv.org/index.php/engrxiv/preprint/view/556},
  urldate = {2024-08-17},
  abstract = {While electrification has been proposed as a key mechanism for combating global warming, in the US, fossil fuels are still the mainstay of the electricity sector. This work seeks to identify market barriers and supports for the adoption of renewable energy. A framework to represent the evaluation process for generating capacity is proposed and applied to two regions: California (CAISO), where legislation requires most electricity to be supplied by renewables by 2030, and the Mid-Atlantic (PJM), where renewables have barely penetrated the market. Generation technologies are ordered via multiobjective optimization using genetic algorithms to resolve a bounded knapsack problem. Price factors for each technology are established. A net present value is assessed for representative projects in each region and technology. Local sensitivity analysis is performed. Results suggest that capacity payments in PJM offer an incentive to fossil-fuel based plants, which calls into question the suitability of carbon pricing in the region. While solar PV has the lowest costs, profitability was highest for natural gas based plants in PJM. Sensitivity is higher in CAISO than in PJM due to lower profit margins. Renewables were selected most frequently in the cost optimization, with natural gas present in {\textasciitilde}10\% of results.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\Wachs19ReliabilityRenewablesModeling.pdf}
}

@article{XuSequentialPredictiveConformal,
  title = {Sequential {{Predictive Conformal Inference}} for {{Time Series}}},
  author = {Xu, Chen and Xie, Yao},
  abstract = {We present a new distribution-free conformal prediction algorithm for sequential data (e.g., time series), called the sequential predictive conformal inference (SPCI). We specifically account for the nature that time series data are non-exchangeable, and thus many existing conformal prediction algorithms are not applicable. The main idea is to adaptively re-estimate the conditional quantile of non-conformity scores (e.g., prediction residuals), upon exploiting the temporal dependence among them. More precisely, we cast the problem of conformal prediction interval as predicting the quantile of a future residual, given a userspecified point prediction algorithm. Theoretically, we establish asymptotic valid conditional coverage upon extending consistency analyses in quantile regression. Using simulation and realdata experiments, we demonstrate a significant reduction in interval width of SPCI compared to other existing methods under the desired empirical coverage.},
  langid = {english},
  file = {C:\Users\scott\OneDrive\share\ref\obsidian\Obsidian Share Vault\lit\lit_sources\XuSequentialPredictiveConformal.pdf}
}
